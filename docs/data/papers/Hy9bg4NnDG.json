{"id": "Hy9bg4NnDG", "number": 16003, "cdate": 1758258441689, "mdate": 1759897267933, "content": {"title": "Can Jailbreaks Force Regurgitation? An Investigation into Existing Attacks as a Data Extraction Vector", "abstract": "Large Language Models (LLMs) memorize sensitive and copyrighted data, creating legal and ethical risks that threaten the future of generative AI. Jailbreaks, meanwhile, routinely bypass safety guardrails. Prior work has shown only that jailbreaks can surface arbitrary snippets of copyrighted text---academically interesting, but not practically useful. We take the first step further, showing that jailbreaks can be systematically used to extract verbatim memorized data on demand, causing an LLM to regurgitate a target text from its training data. Evaluating across a diverse set of jailbreaks and LLMs, we demonstrate that our attacks can achieve 100% verbatim extraction. Our results reveal an \"architecture over size\" paradox: smaller models leak more than larger ones, challenging common assumptions about memorization. This is the first work to connect jailbreaks to targeted data extraction, exposing a critical failure mode at the core of today's LLM ecosystem.", "tldr": "Jailbreaks can extract memorized training data from LLMs. Testing 9 models, we show jailbreaks boost extraction success to 100% and reveal smaller models sometimes leak more than larger ones, exposing how safety bypasses compromise privacy.", "keywords": ["jailbreak attacks", "data extraction", "memorization", "privacy vulnerabilities", "verbatim regurgitation", "training data leakage", "membership inference attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d3745a41d53e89aa53f0c34fc66ffe3108ee7c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a systematic analysis of how existing jailbreak attacks can be repurposed as a vector for targeted training data extraction. By leveraging 12 different adversarial prompts, the authors show how bypassing the refusal can increase the amount of training data leaked by the language models. The analysis is performed on 9 open-source LLMs, with two different datasets, resulting in an increase in the jailbreaks’ verbatim extraction success rate (58-100%) compared to baseline prompts’ success (18-85%).\nThe authors also report an analysis of how the effectiveness of the data extraction attack changes based on the architecture and parameters of the model."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem the authors address is timely, and there are few works in the literature focused on targeted training data extraction. Hence, the idea the authors propose is interesting and fits well within this gap.\n- The authors present a clear picture of the problem they are trying to solve in section 3.1. It really helps the reader understand the problem scenario and the choices the authors made."}, "weaknesses": {"value": "- Attack selection: the authors considered a few jailbreak techniques.\n- Model selection: although the authors considered different model families, from different vendors, all the selected models are quite small (1B-3.8B parameters).\n- Missing attack details and analysis on jailbreak success."}, "questions": {"value": "Here are all my concerns, as well as some minor questions. \n\n- The authors define the attacker’s access to the system as query-based — they assume a black-box threat model. As a result, I believe their analysis is somewhat limited in the types of attacks they consider, for two main reasons:\n    - The black-box jailbreak methods the authors consider are very limited and all essentially “manual”. Looking at results from the most widely used state of the art benchmarks[1], two of the top-performing black-box attacks are TAP[2] and PAIR[3]. While it might be challenging to adapt those automated jailbreak algorithms to the task proposed here, the authors offer no explanation for why they weren’t considered or whether they would or would not work for this task. \n    Instead, the authors pick jailbreak approaches like role-playing, system-level attacks, instruction-based prompts, and multi-shot prompting. Hence, they produce 12 adversarial prompts that are used as templates to which they attach the seed for the sample they want to extract. The authors frequently treat those 12 prompts as 12 distinct attack techniques, but they give no insight, not even qualitative, into what those techniques actually are or how they differ; they only describe the four categories listed above, leaving the analysis quite shallow. Finally, Challenge 3 (section 3.1) states that these attacks must be adapted for the targeted-extraction task, but the authors provide no insight into how those adaptations were performed.\n    - Personally, I think a black-box threat model is very limiting, and that it reduces the impact and contribution of the proposed work.\n    It would have been interesting to explore whether gradient-based jailbreak methods (e.g., GCG) could be used as a vector for extracting training data. The authors do not discuss why these methods might or might not be useful for that purpose, other than saying it was discarded because they involve a different threat model.\n\n- The selection of victim models is limited to relatively small ones, ranging from 1B to 3.8B parameters. In Section 5.2, the authors discuss how the vulnerability of models to targeted training data extraction depends more on architecture than on model size. In the conclusion, they report: “We revealed that architectural choices matter more than model size—smaller models with certain designs proved more vulnerable than larger ones with different architectures.” \nHowever, results from other studies [4], not considered by the authors, show that larger models tend to memorize more, and therefore make it easier to extract targeted training data. I believe their analysis could be significantly improved if they included larger models from the same families, such as those with 6–7B or 13–14B parameters, which would make their findings more robust.\n\n- A central part of the analysis is checking how jailbreaks can bypass refusals and thus enable the extraction of the target data. Although the paper gives a precise comparison of extraction success with and without jailbreaks, the assessment of the different attacks’ effectiveness is limited.\n\t- Table 3 reports effectiveness for only 4 out of the 12 prompts, and there are no additional details about how the other prompts performed.\n\t- It’s not clear how the “attack success rate” in Table 3 is computed.\n\t- It’s also unclear, when an attack fails, whether it’s because the jailbreak didn’t succeed in bypassing the refusal or because the model simply didn’t produce the target sample.\n\nOverall, to improve the paper, I would suggest the authors:\n- Better justify their choice of the attacks, clearly explain which techniques are used and how, and include automated jailbreak methods as well.\n- Increase the set of victim models to include larger models.\n- Analyze model behavior when a refusal is bypassed, but the model still does not output the target sample.\n\nQuestions:\n- Could you better motivate the choice of jailbreaks you considered? Why were automated jailbreaks like TAP or PAIR not included, and how were the chosen methods adapted to create the 12 adversarial prompts?\n- Do you think including larger models would change the analysis?\n- How was the “attack success rate” in Table 3 measured?\n- How were the two datasets selected? Did you try other datasets that performed worse, for example, because those datasets might not be in the models’ training data?\n\nReference: \n\n[1] Mazeika et al., HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal, 2024\n\n[2] Mehrotra et al., Tree of Attacks: Jailbreaking Black-Box LLMs Automatically, 2023\n\n[3] Chao et al., Jailbreaking Black Box Large Language Models in Twenty Queries, 2023 \n\n[4] Carlini et al., Quantifying Memorization Across Neural Language Models, 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wHMyIeDxOM", "forum": "Hy9bg4NnDG", "replyto": "Hy9bg4NnDG", "signatures": ["ICLR.cc/2026/Conference/Submission16003/Reviewer_ViYU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16003/Reviewer_ViYU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760543150882, "cdate": 1760543150882, "tmdate": 1762926210678, "mdate": 1762926210678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the impact of commonly used jailbreaking techniques for red-teaming, but on extraction attacks (verbatim memorization). It studies 12 different jailbreaking prompts, across 9 different models, showing an increase in extracted information with jailbreaking prompts compared to no such prompts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem statement is interesting, because if it is truly this easy to break any safeguard against regurgitation, it does bring into question the privacy risks associated with it.\n2. The paper is easy to read and follow. The authors also did a good job setting up the experiments across different models, different jailbreaking prompts, different datasets, etc. Although I believe the evaluations themselves can be improved (see weaknesses)."}, "weaknesses": {"value": "1. Firstly, I'm not convinced of the story that the jailbreaking prompts are trying to break some form of security training or fine-tuning. Safety measures don't commonly cover regurgitation of books or Wikipedia articles, and while some models might have also been fine-tuned specifically to not regurgitate training data (for example, Llama), this is not necessarily a common characteristic of all models studied in the paper. A study of what exactly 'failed regurgitations' in a normal setting look like would have been very helpful here. For example, do the models outright refuse to complete the request, or do they produce gibberish?\n2. And secondly, I'm not sure if the increased extraction rates are truly due to the jailbreak prompts or due to simple multiple different prompts. To my understanding (please correct me if this is not the case), the paper considers the jailbreak prompt version to be 'successful' if any one of the 12 chosen jailbreak prompts is able to extract the text. Thus, there are 12 different chances to get the correct output, compared to just 1 for the baseline performance. Recent works have shown that extraction attacks have high prompt sensitivity [1].  Combining this with the low threshold of acceptance into an extraction (I don't believe the 0.3 threshold is good, the qualitative examples do not help, and a proper ablation study should have been done), I believe the increase in extraction attacks is mainly due to these factors and not the jailbreak prompts themselves. I could be wrong, but it's unclear based on the current set of results in the paper.\n\nReferences -\n\n[1] More, Yash, Prakhar Ganesh, and Golnoosh Farnadi. \"Towards more realistic extraction attacks: An adversarial perspective.\" arXiv preprint arXiv:2407.02596 (2024)."}, "questions": {"value": "1. Is there a specific reason why the paper chooses to introduce its own terminology of 'targeted regurgitation', when the term 'discoverable memorization' [2] already exists?\n2. Am I correct in understanding that a jailbreaking extraction is considered 'successful' if any of the 12 jailbreak attacks work? If so, more detailed experimental results are needed on how many jailbreak attacks actually work. For instance, do most extractions only happen with one jailbreak technique, while all else fails, or can most extractions be done by multiple jailbreak attacks?\n3. Why use threshold = 0.3? Two isolated examples in the Appendix are not enough for this reasoning. Moreover, the extraction attack rates are clearly hitting saturation (reaching 100%), so the threshold can clearly be increased.\n4. Why do the authors believe jailbreaks are the reason their technique is working better? Did they find refusals in the model behavior without jailbreaks? Are they aware of safeguards against simply asking 'Complete the following' in the models they are using?\n\nReferences - \n\n[2] Carlini, Nicholas, et al. \"Quantifying memorization across neural language models.\" The Eleventh International Conference on Learning Representations. 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rx4ObVLpNk", "forum": "Hy9bg4NnDG", "replyto": "Hy9bg4NnDG", "signatures": ["ICLR.cc/2026/Conference/Submission16003/Reviewer_kjfh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16003/Reviewer_kjfh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068450469, "cdate": 1762068450469, "tmdate": 1762926210194, "mdate": 1762926210194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines whether existing jailbreaks can be repurposed to extract memorized training data. The work focuses on regurgitating targeted passages rather than arbitrary text, as is common in many prior works on extraction."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The problem setup is clear, so is the empirical framing. And the question is also clear: do jailbreaks make regurgitation worse? The study, as set up, seems easy to replicate, which is also a good thing. The finding that architectural family matters more than raw parameter count is interesting (but not novel, see below)."}, "weaknesses": {"value": "Overall, while this paper is studying an interesting question, I think it contains methodological issues and exhibits significant overclaims with respect to prior work. Please see below for more details.\n\n1. Very small models, which compromise the types of general claims being made\n\nAll models tested are very small. It is well-known that small models memorize relatively little compared to larger ones (Carlini et al. 2023, Hayes et al. 2025). This is also true even if you target specific passages (Cooper et al. 2025). It is unclear if conclusions drawn on the smaller models tested here would translate to larger ones, particularly in light of recent work that surfaces enormous degrees of memorization of specific text snippets in open-weight models (Cooper et al. 2025). While this prior work does not study instruct-tuned models, it does serve as a useful baseline for comparisons to instruct-tuned models; but I don't think studying such small models can make these types of comparisons effectively. \n\n2. Baseline comparison problem\n\nAs noted above, there's no comparison to non-safety-tuned base checkpoints. Without that, it's impossible to tell whether the improvement from 18 to 58% is due to “breaking safety” or just “probing the base model.” I think this kind of thing is essential for supporting the central claim that jailbreaks cause regurgitation rather than merely reveal it.\n\n3. Concerns about inflated success rates\n\nI'm concerned that counting relatively short n-gram overlaps inflates success rates; the task is closer to local next-token recall than to meaningful extraction. Most prior work uses much more stringent success criteria (typically, 50 tokens of verbatim content) for a reason---it needs to be sufficiently long to be valid evidence for _extraction_ of _memorized_ training data. If the goal here is just to study how much models produce content that _resembles_ training data (rather than a claim about memorization), this perhaps would be okay. But then the work needs to be written as such, not connect so directly as continuing the lineage of memorization/extraction literature. (This remains important related work, but the current work would not be extending it, as it is running a very different type of experiment). I am concerned that the experiments, if interpreted as making a contribution to this other literature, have validity issues (more on this below). \n\n4. Validity: No confirmation of training-set membership\n\nRegarding claims about _memorization_ (not relevant if the paper is making a claim about producing similar data that is not memorized, but again, this seems to be conflated in the paper): making a claim for extraction is making a claim about training-data membership. To be extracted, by definition, the text must have been memorized; to be memorized, the text has to have been in the training data. If you do not have ground-truth information about membership (as is the case here with models like Qwen), one needs to do more for validity than just check that it matches a public book or article. See Cooper et al. 2025, which examines this directly for specific snippets. Also see Carlini et al. 2019 for earlier work on this, which studies the arbitrary (rather than specific snippet) setting. \n\n5. Overclaims\n\nThe paper says it is the first large-scale empirical study linking jailbreaks and extraction, but it's actually a small-scale test. It also doesn't engage prior work effectively that has trodden similar landscape, but done so in much greater detail, either on the jailbreaking side (Nasr et al. 2023), on the extraction methodology side (Carlini et al. 2023), on the validity side (Carlini et al. 2019, Cooper et al. 2025), or on the snippet-specific extraction side (Cooper et al. 2025). Further, while the point about model family vs. size is interesting, this is more effectively studied and discussed in prior work (Cooper et al. 2025). \n\n[1] Carlini et al. 2023. Quantifying Memorization Across Neural Language Models. \n\n[2] Hayes et al. 2025. Measuring memorization in language models via probabilistic extraction\n\n[3] Cooper et al. 2025. Extracting memorized pieces of (copyrighted) books from open-weight language models\n\n[4] Carlini et al. 2019. Extracting Training Data from Large Language Models. \n\n[5] Nasr et al. 2023. Scalable Extraction of Training Data from (Production) Language Models"}, "questions": {"value": "Please see the weaknesses above. I don't have questions at this time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VPbhfKUjEE", "forum": "Hy9bg4NnDG", "replyto": "Hy9bg4NnDG", "signatures": ["ICLR.cc/2026/Conference/Submission16003/Reviewer_oh7i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16003/Reviewer_oh7i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197774831, "cdate": 1762197774831, "tmdate": 1762926208711, "mdate": 1762926208711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the effect of jailbreaks on the generation of text memorized verbatim. The authors show that they can make the models generate significantly more memorized text when attacked compared to the regular prefix prompting baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic studied by the paper is interesting (verbatim memorization and the effect of jailbreaks on it).\n\n- The writing is quite straightforward and easy to follow. However, there are some missing details as mentioned below."}, "weaknesses": {"value": "- The paper is missing a significant amount of details. For example, it is not clear how the authors tune the attacks for this setting. Can the authors provide all the details of each attack tuned for the case of verbatim regurgitation? What optimization objective is used for each attack?\n\n- The paper's main contribution is to study the effect of jailbreaks on verbatim text generation. However, it feels like it's missing a deeper contribution. This study would be more valuable as an observation at the beginning of a more complete analysis of jailbreaking and verbatim memorization. How much effort does the attacker need to make the model generate those samples? What are the constraints for the attacker? (Because if the attacker can, for example, inject many adversarial tokens, then it might be possible to make the model generate any text, not necessarily memorized ones.) Other points the authors could study to extend this work include ways to defend against jailbreaks that aim for verbatim regurgitation.\n\n- There is relevant prior work [1] that the authors seem to have overlooked.\n\nMinor comments:\n\n- I would use \"prefix\" instead of \"seed.\"\n\n- Table 2 and Table 1 need to be changed order.\n\n\n[1] https://arxiv.org/pdf/2404.15146"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KJ699FP8dg", "forum": "Hy9bg4NnDG", "replyto": "Hy9bg4NnDG", "signatures": ["ICLR.cc/2026/Conference/Submission16003/Reviewer_kj1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16003/Reviewer_kj1p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762847078033, "cdate": 1762847078033, "tmdate": 1762926208161, "mdate": 1762926208161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}