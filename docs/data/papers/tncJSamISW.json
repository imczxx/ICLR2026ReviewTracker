{"id": "tncJSamISW", "number": 15508, "cdate": 1758252111440, "mdate": 1759897302284, "content": {"title": "Abstractive Red-Teaming of Language Model Character", "abstract": "We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in a large-scale deployment. In this work, we aim to search for such character violations using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search over natural-language query categories, e.g. “The query is in Chinese. The query asks about family roles.” These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries. Across a 12-principle character specification and 7 target models, we find that our algorithms consistently outperform baselines, and generate qualitatively interesting categories: for example, queries which ask Llama-3.1-8B-Instruct to predict the future lead to predictions that AI will dominate humanity, and queries that ask GPT-4.1-Mini for essential prison survival items lead to enthusiastic recommendation of illegal weapons. Overall, we believe our results represent an important step towards realistic pre-deployment auditing of language model character.", "tldr": "We red-team large language model character by searching over categories of user queries likely to appear in deployment.", "keywords": ["character", "alignment", "red-teaming", "safety", "constitutional ai"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d1d6bacad5625172ca390bd9436f3a121285464.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a new red-teaming/jailbreaking approach using a bilevel setup. In the first step, the framework generates high-level categories of questions that could elicit incorrect behavior from a target LLM. In the next step, queries are generated based on the categories in the previous step, which are finally fed to the LLM. The paper proposes two approaches to optimize this framework — one using the leave-one-out REINFORCE algorithm, where the reward is based on whether the LLM is jailbroken, and a second approach uses Monte Carlo estimates to choose a category. The paper conducts experiments on a large number of LLMs and reports the results of each approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths**:\n\n1. The paper presents a natural way of decomposing the red-teaming problem in a bilevel setup. Intuitively, it is easier to search over the categories of problematic queries rather than the query space itself. \n2. The paper proposes two different approaches to optimize the above framework. The first approach trains a category generator using reinforcement learning, where the reward is a binary label indicating whether the LLM was jailbroken. The second one searches over the category space and chooses categories based on the jailbreaking scores obtained using Monte-carlo. \n3. The paper provides extensive details on data collection and reward model training to implement the above framework. The paper also presents a range of experiments on a large number of LLMs."}, "weaknesses": {"value": "**Weaknesses**:\n\n1. The paper doesn’t present any baselines apart from a random sampling. There are a large number of jailbreaking papers; please refer to [1] for an incomplete list. The paper should compare with the state-of-the-art methods to establish the impact of their proposed method.\n2. The paper doesn’t use standard metrics to evaluate its approach. The paper reports the mean rewards obtained using the reward model, but the details of the scoring mechanism aren’t described in the main body of the paper. The paper should report standard metrics like success rate in jailbreaking LLMs. This method should also be evaluated against state-of-the-art LLMs like GPT-5 and Gemini. This would demonstrate the effectiveness of the method in scalable systems.\n\nComments:\n\nLine 168: Minor — the set of all possible strings is finite, therefore the mentioned set is also finite. \n\n[1] A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models, Xu et al., 2024"}, "questions": {"value": "Please respond to the weaknesses and comments in the above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BbMhODRjcp", "forum": "tncJSamISW", "replyto": "tncJSamISW", "signatures": ["ICLR.cc/2026/Conference/Submission15508/Reviewer_adWE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15508/Reviewer_adWE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539564476, "cdate": 1761539564476, "tmdate": 1762925795402, "mdate": 1762925795402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Abstractive Red Teaming (ART), a framework for automatically generating adversarial prompts by abstracting harmful queries through large language models (LLMs). The goal is to generate semantically equivalent but surface-different prompts that bypass safety filters. The authors propose an iterative paraphrasing and abstraction process, guided by a “semantic preservation” objective and a toxicity classifier. ART aims to generate diverse adversarial prompts that remain challenging for LLM safety systems to detect."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Targets a relevant and practical problem aobut automating adversarial prompt discovery.\n2. Provides clear examples and evaluation results that are easy to reproduce.\n3. Attempts to move beyond surface-level perturbations by introducing a “semantic abstraction” step."}, "weaknesses": {"value": "1. Low Novelty and Conceptual Incrementality. The proposed approach primarily reformulates existing red teaming and paraphrasing strategies under a new expression. While the authors frame this as an innovative abstraction-driven method, the actual process about iterative rewording and filtering based on similarity or toxicity classifiers is a minor variation of well-known paraphrase-based adversarial generation. \n2. Template Dependence and Limited Diversity. The system’s performance appears heavily tied to the chosen abstraction templates and instruction patterns used during generation. In practice, the success of the attacks relies on specific prompt wordings and fixed structural cues, which makes the method brittle and less generalizable. When applied to different safety domains or LLM families with distinct guardrail training, the approach may fail to adapt. \n3. Weak Signal Design and Limited Generalization. The “semantic preservation” and “toxicity-guided” objectives used to control abstraction quality are simplistic and loosely connected to actual adversarial effectiveness. Embedding similarity or classifier confidence cannot reliably ensure semantic equivalence or adversarial strength, especially in nuanced safety contexts. As a result, the method risks generating either trivial rephrasings or semantically drifted prompts that no longer test model robustness meaningfully."}, "questions": {"value": "1. How sensitive are results to the chosen abstraction templates and initial seed prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CrguymnuIq", "forum": "tncJSamISW", "replyto": "tncJSamISW", "signatures": ["ICLR.cc/2026/Conference/Submission15508/Reviewer_7tFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15508/Reviewer_7tFL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964368653, "cdate": 1761964368653, "tmdate": 1762925794439, "mdate": 1762925794439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to improve red teaming strategy from individual adversarial prompts to query categories, which is represented as natural language descriptions of classes of user queries that may trigger policy violations. This paper introduces two algorithms: CRL (Category-Level RL), which applies REINFORCE optimization directly in categories space, and QCI (Query–Category Iteration), which alternates between exploration (generating high-risk query examples) and exploitation (summarizing them into new categories).\nExperiments on 4 LLMs show that CRL and QCI outperform random sampling in discovering realistic and high-risk scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Moving from query-level to category-level search is conceptually reasonable and practically relevant.\n2. The two algorithms (CRL and QCI) are well structured.\n3. Rich qualitative findings: The paper provides many concrete and interesting categories as cases, demonstrating the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The comparison is mainly against random sampling; this paper should have more competitive baselines (e.g., some taxonomy-guided methods).\n2. The total query budget is large (100k queries for each model, CRL seems to require large amount of queries), and there’s limited discussion of convergence, sample efficiency, or sensitivity to hyperparameters."}, "questions": {"value": "See weaknesses, and\n\n1. How many preferences data were used to train the reward model?\n2. How did you address the gap from generated to real user data in discovered categories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "guEKmZ9PS4", "forum": "tncJSamISW", "replyto": "tncJSamISW", "signatures": ["ICLR.cc/2026/Conference/Submission15508/Reviewer_kSd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15508/Reviewer_kSd8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969588490, "cdate": 1761969588490, "tmdate": 1762925793905, "mdate": 1762925793905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an interesting approach to uncovering the types of natural queries that cause language models to violate their intended character specifications. Instead of focusing on individual adversarial prompts, it proposes optimizing a category generator via REINFORCE-based reinforcement learning to iteratively discover high-risk categories of harmful queries. The results suggest that different models exhibit distinct vulnerability patterns across query categories, highlighting model-specific behavioral weaknesses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tUnderstanding what kinds of user queries lead to policy violations is crucial for improving the safety of LLMs in real-world applications.\n2.\tShifting focus from specific adversarial queries to learned semantic categories offers a different way to analyze model vulnerabilities - this conceptual move from instances to abstractions is valuable."}, "weaknesses": {"value": "1.\tOverall, the current presentation lacks clarity and structure. A visual illustration (e.g., a diagram showing the interaction between the category generator, reward model, and experience pool) would greatly aid understanding. Additionally:\n- The core algorithm should be moved from the appendix into the main text.\n- The term “a subset of query space” (Section 4.2) is used without formal definition—is this a set of strings, embeddings, or structured attributes?\n- What is the format of the generated categories? Are they free-text descriptions, templates, or structured labels?\n- Lines 262–269: Why sample K/(ℓ+1) subsets? This choice seems arbitrary without justification.\n- It’s unclear which component drives diversity and novelty in the discovered categories. Is it the sampling strategy, the reward signal, or the generator itself?\n2.\tIn Section 4.1.2, why not use off-the-shelf safety classifiers such as Llama Guard or QwenGuard as the reward model? If a custom reward model is preferred, how does its training data relate to the domains and categories present in the experience pool? Any overlap could introduce circularity or overestimation of risk.\n3.\tWhat if one directly optimizes over individual queries instead of categories? This would help isolate the benefit of the categorization framework. Besides, the paper should compare against recent red-teaming and failure-exploration methods, such as: \n\n[1] Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step RL \n\n[2] Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through Failure-Inducing Exploration \n\n[3] LLMs Know Their Vulnerabilities: Uncover Safety Gaps through Natural Distribution Shift  and so on.\n\n4.\tThe experimental setup uses different settings for Llama-3.1-8B-Instruct versus other models. Could the authors provide more justification for these variations? \n4.\tWhile the qualitative analysis in Section 4.3 is insightful and engaging, many figures and tables are placed to the appendix. Given their importance for illustrating the types of discovered categories, key examples should be included in the main paper to improve readability and impact."}, "questions": {"value": "please refer to the above Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ApixKY6pIj", "forum": "tncJSamISW", "replyto": "tncJSamISW", "signatures": ["ICLR.cc/2026/Conference/Submission15508/Reviewer_cSP7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15508/Reviewer_cSP7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155875597, "cdate": 1762155875597, "tmdate": 1762925793473, "mdate": 1762925793473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}