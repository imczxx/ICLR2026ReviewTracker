{"id": "y3073f59bH", "number": 12623, "cdate": 1758209074536, "mdate": 1759897497597, "content": {"title": "DOES THE DEFINITION OF DIFFICULTY MATTER ? SCORING FUNCTIONS AND THEIR ROLE FOR CURRICULUM LEARNING", "abstract": "Curriculum learning (CL) relies on the simple and intuitive assumption that the non-uniform sampling of training instances based on some measure of sample difficulty is beneficial for learning, with the postulated benefits being faster convergence and improved test-set performance. The motivation for CL is oftentimes grounded on anthropomorphisation – humans, it is argued, often rely on curricula for their learning. However, this simple premise hinges on the notion of sample difficulty for which there is no established definition. Previous research on the benefits of CL begins by settling on a specific definition of difficulty, without questioning the potential bias that this a priori definition introduces. In the present contribution, we conduct an extensive experimental study on the robustness and similarity of the most common scoring functions for sample difficulty estimation on two benchmark datasets from the vision and audio domains. We report a strong dependence of scoring functions on the training hyperparameters, including randomness, which can partly be mitigated through ensemble scoring. While we do not find a general advantage of CL over uniform sampling, we observe that the ordering in which data is presented for CL-based training plays an important role in model performance. Furthermore, we find that the robustness of scoring functions across random seeds positively correlates with CL performance. Finally, we uncover that models trained with different CL strategies complement each other by boosting predictive power through late fusion, likely due to differences in the learnt concepts. Alongside our findings, we release a toolkit implementing sample difficulty and CL-based training in a modular fashion.", "tldr": "", "keywords": ["Curriculum Learning", "Sample Difficulty", "Scoring Function Similarity", "Computer Vision", "Computer Audition", "Deep Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/faba4f0b0371e79e193b9d380ee56aa03eaef702.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to study the role of scoring function and pacing function in curriculum learning. It aims to answer several questions such as *How do different SFs affect model performance*?"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of this paper is of great significance to the underlining mechanism of curriculum learning.\n\n2. The paper conducts large number of experiments on the two datasets.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The conclusion of the paper is somewhat vague. On one hand, the core idea of CL is difficulty measurer and learning scheduler, thus obviously the performance of CL will be largely depended on the two components. On the other hand, the theoretical and practical role of CL in deep learning remains largely unresolved. Simply reiterating previous conclusions without offering deeper insights substantially limits the paper’s contribution.\n\n2. For the difficulty measurer:\n\n2.1 The authors do not explain why DSACE2020 does not exhibit similar behavior to CIFAR10; instead, they only vaguely allude to some underlying factors without concrete analysis.\n\n2.2 In addition to ensemble-based approaches, some studies have explored dynamic hardness [1], which can also enhance model robustness. This line of work should be discussed and compared to provide a more comprehensive understanding.\n\n3. For pacing functions: in the main paper, the authors give CL, RCL and ACL as pacing functions (Figure 3). However, only in CL, there are various pacing functions, or learning schedulers, such as predefined ones (baby step, linear, geometric, exp, etc.) and automatic ones(self-paced, transfer teacher, etc.). The discussions of pacing functions and its role to the overall performance are very limited.\n\n4. The authors claim that anti-curriculum learning (ACL) is less preferable, and they attempt to substantiate this point in Appendix A.8. However, given that the experimental settings for curriculum learning (CL) can vary substantially, the presented evidence is not entirely convincing. As demonstrated in several experiments within the paper, conclusions derived from specific datasets cannot be assumed to generalize to others. In fact, ACL has been extensively studied in prior research [2,3], and its role and potential advantages over standard CL remain subjects of active debate.\n\nReferences:\n\n[1] Curriculum Learning by Dynamic Instance Hardness\n\n[2] Unsupervised hard example mining from videos for improved object detection\n\n[3] Training region-based object detectors with online hard example mining"}, "questions": {"value": "Please see weakness. \n\nI respectfully expect deeper insights into the underlying mechanism of curriculum learning (CL) from this paper, rather than straightforward “yes” or “no” answers to the questions posed at the end of Section 1. From the current results, the main takeaway seems to be that multiple scoring functions (SFs) lack robustness across random seeds. However, this observation is rather intuitive, and employing ensemble methods appears to be the most direct remedy. I encourage the authors to further summarize and articulate the technical insights or mechanistic understanding that this paper contributes beyond empirical observations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LOmGUXC6wB", "forum": "y3073f59bH", "replyto": "y3073f59bH", "signatures": ["ICLR.cc/2026/Conference/Submission12623/Reviewer_WhVm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12623/Reviewer_WhVm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760605670016, "cdate": 1760605670016, "tmdate": 1762923471642, "mdate": 1762923471642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the role of scoring functions in curriculum learning, especially the robustness of scoring functions. The authors conduct a series of empirical studies to explore questions that have been less examined in prior work, including: the sensitivity of scoring functions to training settings, model architectures, and random seeds; the relationship between scoring function sensitivity and performance; and the combination of different curriculum strategies. The experiments are conducted mainly on CIFAR-10 and DCASE2020 datasets, using ResNet, EfficientNet, and PANN model architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of scoring functions is very relevant and key in the field of curriculum learning.\n2. The exploration of late fusion of curriculum strategies is novel.\n3. The findings and the proposed training toolkit can benefit researchers in this area."}, "weaknesses": {"value": "1. Although the authors examine several aspects of scoring functions, the connection between these research questions is unclear. Providing a central narrative or through-line connecting these questions could make the paper more coherent.\n2. Despite the number of experiments, the datasets and model architectures used are relatively small and out-dated. Consequently, the results and findings may not generalize to large-scale datasets or modern models, e.g., LLMs.\n3. The proposed late fusion method is promising, but its applicability in real-world scenarios appears limited.\n4. The paper confirms conclusions from prior work and concludes that curriculum learning is not a panacea, which point has already been discussed in earlier studies. Therefore, the significance of this paper is reduced."}, "questions": {"value": "Do the authors think that the conclusions presented in this paper can be generalized to large-scale models? In the authors’ opinion, is it necessary to investigate these research question in the context of current large models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WQtqWLcyao", "forum": "y3073f59bH", "replyto": "y3073f59bH", "signatures": ["ICLR.cc/2026/Conference/Submission12623/Reviewer_Kah7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12623/Reviewer_Kah7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760953422282, "cdate": 1760953422282, "tmdate": 1762923471344, "mdate": 1762923471344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper empirically investigates the role of sample difficulty in the performance of curriculum learning. It focuses on the empirical correlations among scores used in prior work, the reduction in variability obtained by ensembling, and the relationship between this variability and ultimate performance. The study uses two relatively small datasets: CIFAR-10 (vision) and DCASE2020 (audio). The main findings are: (i) difficulty-based rankings are sensitive to training choices, with overall moderate correlation values; (ii) somewhat higher correlation values are observed across different scores, which is possibly a new empirical observation; and (iii) this variability is not necessarily predictive of final performance improvements. The work provides additional rigorous support for empirical correlations and results reported previously."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Clarity: High, the paper is well written."}, "weaknesses": {"value": "Originality: This is incremental work that rigorously evaluates empirical observations noted in earlier studies. In particular (as ineed noted in the paper), sensitivity to scoring functions has been observed before.\nQuality: Given that the paper’s primary contribution is an empirical comparative evaluation, the scope of the evaluation is not sufficient.\nSignificance: In light of the points above about originality and quality, the significance of the work appears low. The hypothesis driving CL is that the ranking order produced by suitable scoring functions is crucial for CL’s success. However, this does not imply that modest correlations between rankings, when varying the training parameters, should necessarily affect final performance - as the paper itself shows. Thus, while the rigorousity of the observation is of some interest, its significance for understanding CL is limited.\nMinor comment: the somewhat lengthy discussion of ERM in the upper half of page 3 (up to Eq (4)) is not necessary for an ICLR submission."}, "questions": {"value": "Did you evaluate the correlation between SF and rate of convergence? prior work has suggested that the main contribution of CL may be speeding up convergence rather than improving ultimate performance, as this is the most robust difference between CL and random SGD traning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "goXF4RdNgA", "forum": "y3073f59bH", "replyto": "y3073f59bH", "signatures": ["ICLR.cc/2026/Conference/Submission12623/Reviewer_GHcE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12623/Reviewer_GHcE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865381757, "cdate": 1761865381757, "tmdate": 1762923470562, "mdate": 1762923470562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a large-scale empirical study on curriculum learning (CL), focusing on the definition and stability of \"sample difficulty\". The authors evaluate a large set of scoring functions (SFs) on vision (CIFAR-10) and audio (DCASE2020) datasets. They find that a) difficulty rankings are highly sensitive to hyperparameters (model, seed, optimizer), though this can be mitigated by ensembling, b) most SFs agree on a similar notion of difficulty, and c) the use of CL does not consistently outperform standard uniform-sampling baselines. The paper's most significant finding is that models trained with different curricula (e.g., easy-to-hard vs. hard-to-easy) learn complementary knowledge, and ensembling them leads to performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Presents the results of  large scale experiments.\n- Provides a critique to the idea that CL is a plug-and-play useful technique. Echoing prior works such as Wu et al. (2021).\n- Finds that different CL schedules, such as easy-to-hard and hard-to-easy, produce models with complementary knowledge."}, "weaknesses": {"value": "- The paper's primary conclusion that curriculum learning is not a \"plug-and-play\" solution is a confirmation of findings already present in the literature (e.g., Wu et al., 2021). Furthermore, the work is purely empirical, employing existing scoring functions and models without introducing novel methodology\n\n- The paper's interesting claim that different curricula produce complementary models is supported only by indirect evidence from the performance of late-fusion ensembles. This conclusion is not convincing. The work would be significantly stronger if this claim were supported with direct evidence from interpretability analyses (e.g., feature visualization, saliency maps) or a detailed error analysis showing that CL and ACL models systematically fail on different subsets of the data. Currently, the performance gains are presented without statistical significance tests, making it difficult to understand the robustness of the ensemble effects.\n\n- The study's conclusions are based on two mid-sized classification datasets. It is not clear how these findings would translate to other domains (e.g., NLP, RL) or to large-scale training regimes where CL is often thought to be most beneficial. Moreover, the paper notes a divergence in key results between CIFAR-10 and DCASE2020 but does not explore the reasons for this. This makes it difficult to form a generalizable conclusion."}, "questions": {"value": "- Regarding the key difference observed in Figure 2 (robustness vs. performance correlation), what are your primary hypotheses for why this trend exists for CIFAR-10 but not DCASE2020? Could this be related to factors like dataset size, inter/intra-class variance, or the signal-to-noise ratio inherent in the different data modalities?\n- Have you performed any qualitative analysis on the kinds of examples that CL-trained models correctly classify versus those that ACL-trained models handle better? This might clarify on the nature of the \"learnt concepts\" you mention and why they are complementary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h70teiSdBl", "forum": "y3073f59bH", "replyto": "y3073f59bH", "signatures": ["ICLR.cc/2026/Conference/Submission12623/Reviewer_ZxcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12623/Reviewer_ZxcY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188394428, "cdate": 1762188394428, "tmdate": 1762923469514, "mdate": 1762923469514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}