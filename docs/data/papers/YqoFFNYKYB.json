{"id": "YqoFFNYKYB", "number": 4024, "cdate": 1757585657800, "mdate": 1759898057425, "content": {"title": "Split Happens (But Your Video Model Can Be Edited)", "abstract": "Recognition models are typically trained on fixed taxonomies. Yet these taxonomies can be too coarse, collapsing distinctions under a single label (e.g. the action “open” may vary by object, manner, or outcome), and they also evolve as new distinctions become relevant. Collecting annotations and retraining to accommodate such changes is costly. We introduce category splitting, a new task where an existing classifier is edited to refine a coarse class into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video models to expose fine-grained distinctions without extra data. We also show that low-shot fine-tuning, though simple, is highly effective, and benefits further from zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest.", "tldr": "", "keywords": ["Video Understanding", "Representation Learning", "Action Recognition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92d71b7e33763090488a2de6f2067900aad9cdae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new task named \"category splitting\", which aims to refine a coarse category into more detailed subcategories through classifier edits. They propose a zero-shot editing method that leverages the latent compositional structure within video models to create these new distinctions without requiring additional data. Besides, the study shows that low-shot fine-tuning is highly effective, and its performance is further enhanced when initialized with the edited weights. Experimental results on new benchmarks demonstrate that this approach significantly outperforms baselines, improving performance on the newly split categories while preserving performance on the others."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* This task introduces a new setting called \"category splitting\" and creates new benchmarks by reorganizing existing ones (SSv2, FineGym).\n* The proposed method is reasonable and performs well on new splits of category, outperforming existing prevalent Vision-Language Models.\n* The analysis section provides a sound analysis of the method's effectiveness, offering readers a deeper understanding of the method."}, "weaknesses": {"value": "* The task setting proposed in this paper is akin to zero-shot classification or continual learning, but it is less challenging since the distributions of the fine-grained subcategories and the original coarse category are relatively close. Besides, a major limitation of this task setting is the requirement that the coarse and fine-grained categories must share the same base name, limiting the task's significance and practicality.\n* The method section is hard to follow. There exist undefined or unclaimed symbols, and some statements are confusing.\n* As stated in line 139, why use the mean of the associated fine-grained weight vectors as the pseudo vector of the coarse category? What if using the real text vector of coarse categories?\n* According to Table 2, despite achieving sound results on the new categories, it still causes a drop on the other categories, whereas VLMs (e.g. CLIP) can avoid the problem."}, "questions": {"value": "* Why not choose the image encoder from a VLM as the base model, as it may possess better latent compositional structure during the pre-training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ACc4HBCmHg", "forum": "YqoFFNYKYB", "replyto": "YqoFFNYKYB", "signatures": ["ICLR.cc/2026/Conference/Submission4024/Reviewer_8rDB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4024/Reviewer_8rDB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655165221, "cdate": 1761655165221, "tmdate": 1762917139405, "mdate": 1762917139405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for zero-shot fine-grained classification by editing classifier weight matrices without retraining backbones. They extract \"modifier vectors\" by averaging coarse category weights from existing fine-grained examples and subtracting to isolate semantic differences, then use these to generate new subcategory weights via w_subcategory = w_coarse + v_modifier. The method is evaluated on SSV2-Split and FineGym-Split benchmarks, expanding classifier matrices from ~100 to ~150 categories while demonstrating improved performance over standard CLIP and VideoCLIP baselines on fine-grained classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-- Practical & Elegant Solution: Addresses real problem of fine-grained classification without expensive retraining - just intelligent matrix manipulation\n\n== Compositional Approach: The weight arithmetic (w_subcategory = w_coarse + v_modifier) is intuitive and enables systematic fine-grained category generation"}, "weaknesses": {"value": "-- Scalability Questions: Method requires existing fine-grained examples to extract modifiers, and matrix growth (100→150 categories) may not scale to truly large taxonomies\n\n-- Clarity and Organization Issues: Paper was slightly difficult to follow on the main contribution and method - would benefit from restructuring for better readability (more intuitive figures perhaps?)\n\n-- Insufficient Baseline Comparisons: Only compares against basic vision-language models (CLIP, VideoCLIP) rather than established fine-grained classification methods (e.g., other compositional approaches)"}, "questions": {"value": "I wonder, isn't it worth testing your model against fine-grained approaches (e.g., compositional approaches or methods that improve CLIP with additional losses for fine-grained evaluations)? There are many such methods, and while I'm not sure which one would be the best fit, I still raise this question. The current evaluation seems limited to basic vision-language baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "auS5Zj1uxj", "forum": "YqoFFNYKYB", "replyto": "YqoFFNYKYB", "signatures": ["ICLR.cc/2026/Conference/Submission4024/Reviewer_duDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4024/Reviewer_duDd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844104067, "cdate": 1761844104067, "tmdate": 1762917139233, "mdate": 1762917139233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the task of category splitting for video recognition: starting from an already trained video classifier with a fixed label set, you later realize a coarse label like “poking something” or “dropping something” actually needs to be broken into several fine-grained subactions, but you don’t want to retrain the whole model or disturb performance on the other, unrelated classes. The paper’s key insight is that existing video models already contain reusable, compositional structure in their classification head: across related actions, the difference between labels often looks like a consistent “modifier” (e.g. direction, manner, target), so they build a modifier dictionary from existing labels and then synthesize new subcategory weights by adding an appropriate modifier vector to the original coarse class weight, producing new labels in a fully zero-shot way; to go beyond seen modifiers, they train a small alignment module that maps modifier text to modifier vectors, and if a few labeled clips for the new sublabels are available, they fine-tune only those new head weights to improve accuracy while keeping the rest of the model fixed, thus preserving locality. Evaluated on split versions of video datasets (like SSv2-Split and FineGym-Split), the method improves recognition of the new, finer labels without degrading performance on untouched classes, showing that you can “edit” a video model’s label space after training by exploiting the structured differences already present in its classifier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Proposes and explores a novel problem of clear practical relevance\n- The work explores different forms of class splitting, both where the modifiers are seen or unseen\n- The proposed solution is tidy and lightweight: they edit only the classifier head, reuse structure already present in the model by building a modifier dictionary from existing labels\n- The authors construct 2 datasets for this problem from SSv2 and FineGym, which pose challenging test cases"}, "weaknesses": {"value": "- Method depends significantly on the assumption that the original label space already contains enough compositional variation to learn good modifier vectors\n\n- Because the edit happens at the classifier head, it also assumes the backbone already captures the visual distinctions the new sublabels require; if the new split introduces visual novelty rather than just semantic refinement, a head-only edit will struggle, and the paper doesn’t really explore that failure mode\n\n- There is some amount of over-claiming going on in this paper, e.g. the title claims \"YOUR VIDEO MODEL CAN BE EDITED\", but this method is fairly limited to a very specific set of situations."}, "questions": {"value": "- You show that adding a retrieved/aligned modifier vector to the coarse class weight works, but how often is the best modifier actually coming from the same base action vs. being borrowed from a semantically different action? I would like to see some statistical analysis of this.\n\n- How robust is the text encoder you use to messy, real-world label names (typos, multi-sentence definitions, multilingual labels etc) ? This choice should ideally be ablated"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ykxcWdT0BC", "forum": "YqoFFNYKYB", "replyto": "YqoFFNYKYB", "signatures": ["ICLR.cc/2026/Conference/Submission4024/Reviewer_XTPn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4024/Reviewer_XTPn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961061133, "cdate": 1761961061133, "tmdate": 1762917138948, "mdate": 1762917138948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for zero-shot adaptation of video-language models to perform fine-grained classification. This is tackled by \"splitting\" existing categories to sub-categories. The method improves over baselines. The paradigm is also extended to a few-shot setting where similar gains are shown."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method shows good improvement over baselines. The authors also provide many ablations such as choice of encoder, pretraining among others. The dataset generated has also been provided fully aiding in transparency. The concept of zero shot adaptation to finer granularity levels is interesting and warrants more attention."}, "weaknesses": {"value": "The paper uses the term \"video model\" very generally to refer specifically to a kind of video-language model. This is misleading as \"video model\" can refer to other concepts such as video generation models.\n\nThe method is very hard to follow as the authors do not provide any preliminary information of the architecture that they are based upon. Eg. it is hard to follow which are the weight vectors that are being referred to as additive.\n\nSubjective: The title of the paper does not convey the problem being tackled."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ohSOKtviiX", "forum": "YqoFFNYKYB", "replyto": "YqoFFNYKYB", "signatures": ["ICLR.cc/2026/Conference/Submission4024/Reviewer_Lq1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4024/Reviewer_Lq1x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972305989, "cdate": 1761972305989, "tmdate": 1762917138778, "mdate": 1762917138778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}