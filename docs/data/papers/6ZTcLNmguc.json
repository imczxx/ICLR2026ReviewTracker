{"id": "6ZTcLNmguc", "number": 12510, "cdate": 1758208304925, "mdate": 1763318492742, "content": {"title": "Gelato: Graph Edit Distance via Autoregressive Neural Combinatorial Optimization", "abstract": "The graph edit distance (GED) is a widely used graph dissimilarity measure that quantifies the minimum cost of the edit operations required to transform one graph into another. Computing it, however, involves solving the associated NP-hard graph matching problem. Indeed, exact solvers already struggle to handle graphs with more than 20 nodes and classical heuristics frequently produce suboptimal solutions. This motivates the development of machine-learning methods that exploit recurring patterns in problem instances to produce high-quality approximate solutions. In this work, we introduce Gelato, a graph neural network model that constructs GED solutions incrementally by predicting a pair of nodes to be matched at each step. By conditioning each prediction autoregressively on the previous choices, it is able to capture complex structural dependencies. Empirically, Gelato achieves state-of-the-art results, even when generalizing to graphs larger than the ones seen during training, and runs orders of magnitude faster than competing ML-based methods. Moreover, it remains effective even under limited or noisy supervision, alleviating the demand for costly ground-truth generation.", "tldr": "", "keywords": ["graph edit distance", "neural combinatorial optimization", "graph matching", "graph neural networks"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed362e7d8c9fee95a7cb3c7bf69f43e428f2d608.pdf", "supplementary_material": "/attachment/b21e9752f5776aa8fa69d8c23e7ff6e3cb13b0a9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method to approximate the graph edit distance ( GED). The major contribution is the use of a neural network to solve the GED sequentially."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is interesting as it allows to construct the edit paths."}, "weaknesses": {"value": "The designation of “autoregressive” seems not to be correct, or at best not well justified. The proposed method can be clearly defined as sequential. At the end, it uses roughly a search-based strategy in the same spirit as A* and related methods.\n\nA major issue is that there are no guarantees of optimality for the proposed method. Moreover, it is well known that sequential optimization is non-optimal in general.\n\nWhile the authors provide some theoretical results, they cannot be explored in practice. For instance, Lemma 1, which motivates the greedy selection, cannot be exploited because the optimal function cannot be computed in practice, making these results intractable.\n\nIt would be relevant to provide a comprehensive ablation study beyond Section 5.3. For instance, it is not clear if the batch normalization and the residual connections are beneficial or not. Moreover, the paper does not justify the choice of the values of most hyperparameters, such as the number of layers of the GIN set to 5, embedding dimension, number of randomly selected pairs, resembling with k=32...\n\nThe computational complexity needs to be studied in depth. The authors mainly present the inference runtime, but not the training runtime.\n\nThe expression “Any state is also a terminal state” is misleading.\n\nThere are several spelling and grammatical errors that can be easily identified and corrected, such as “We define a the set…”, as well as GINE."}, "questions": {"value": "No further comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "N77qBSPxjf", "forum": "6ZTcLNmguc", "replyto": "6ZTcLNmguc", "signatures": ["ICLR.cc/2026/Conference/Submission12510/Reviewer_jqC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12510/Reviewer_jqC1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687407713, "cdate": 1761687407713, "tmdate": 1762923379226, "mdate": 1762923379226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GELATO solves the graph edit distance (GED) problem\nby reformulating it as a sequential decision making process.\nThe graph neural network (GNN) is trained to predict one matching node pair at a time,\ngiven a partially matched pair of graphs,\nwith efficiency and soundness considerations such as\noverlapping state space and automorphisms,\nallowing for autoregressive inference to generate the full matching\nto compute the GED.\nComprehensive experiments show that GELATO achieves high solution quality compared to baselines,\nwhile taking less time, especially compared to approaches with an non-neural matching/search component.\nFurther analyses include generalization on larger graphs, robustness under limited supervision and ablations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper proposes a novel method, with well-motivated and carefully-considered components.\n\nThe experiments are comprehensive and the results are strong.\n\nThe experiments support the main motivations for this research,\ne.g. generalization to larger graphs and faster inference times."}, "weaknesses": {"value": "While the paper is well-written overall,\nsome more-involved parts of the method could be better explained,\nespecially the state-space reduction and automorphism considerations."}, "questions": {"value": "1. What is the basis for selection of 3 baselines out of 8 in Section 5.2 / Figure 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h8qZT3caDF", "forum": "6ZTcLNmguc", "replyto": "6ZTcLNmguc", "signatures": ["ICLR.cc/2026/Conference/Submission12510/Reviewer_eo4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12510/Reviewer_eo4E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953075511, "cdate": 1761953075511, "tmdate": 1762923378868, "mdate": 1762923378868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Graph Edit Distance (GED) estimation by framing it as a sequential decision-making task. The authors propose GELATO, an autoregressive framework built on a GNN backbone, in which a graph matching solution is incrementally constructed step-by-step. In line with classical search-based algorithms for GED, the method introduces a dynamic programming–style decomposition of the problem, where partial matchings and their induced subproblems are reduced while preserving equivalence to the optimal solution set. The model is trained using node-level matching supervision and evaluated on several benchmark datasets, with experiments demonstrating improved generalization to larger graph sizes compared to some previous learning-based GED predictors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths**\n\n- GED prediction often requires resolving symmetries and tie-breaking, further exacerbated by expressivity limitations of standard GNNs. This make the sequential decision framework, in line with  traditional heuristic search methods for GED, well-motivated. I also like the analysis showing that the reduced subproblem maintains equivalence of optimal solutions. \n\n-  Despite relying on explicit node-level matching supervision, which would usually count as a disadvantage, the paper shows robust generalization to larger graphs, which confirms practical utility of the approach.\n\n- By generating incremental matchings, the method offers interpretability advantages compared to one-shot predictors."}, "weaknesses": {"value": "**Weaknesses**\n\n1.  The statement of Theorem 1 differs between the main paper and the appendix, and the notation in the main paper version is unclear. Since the theorem plays a central conceptual role, the paper would benefit from, aligning the statements across sections, clarifying notation, and adding a brief intuitive explanation (2–3 sentences) to the main body to highlight why equivalence is preserved.\n\n2. The architecture section does not clearly specify how partial matchingsare represented internally.  Do cross-graph edges introduced by a partial matching participate in message passing? If so, the graph structure is dynamically modified, which may have unintended representational implications.  If not, how are embeddings of partially matched nodes coupled or tied together?  Similarly, the implementation of the reduce operation in the neural pipeline is not clearly described.\n\n3. The choice of a 128-dimensional embedding for comparatively small graphs is not well justified, since the node embedding matrix is larger than the adjacency matrix!"}, "questions": {"value": "Please refer to the weaknesses for some questions.  \n\n\nIn addition,  I would like to better understand the practical utility and trade-offs of using an autoregressive approach as opposed to existing one-shot GED predictors. In general, one would expect autoregressive inference to incur higher computational cost, due to stepwise decoding and possible sensitivity to early decisions that may require techniques such as parallel beam search. While the ability to produce interpretable edit paths is a meaningful benefit, the paper does not clearly analyze what is gained and what is sacrificed in moving from one-shot estimation to autoregressive decoding. \n\nIn the reported timing comparisons, the authors attribute improved runtime to the fact that GELATO is “GPU-friendly,” whereas several baselines are not. This makes it difficult to disentangle architectural advantages of the autoregressive formulation from implementation differences. It is desirable to have a more direct study of these trade-offs including inference latency, resource usage, prediction accuracy improvements, etc. \n\nAlso, is there any reason why GraphEDX (Jain et al., 2024) has been omitted as a baseline. I would  consider it a decent exemplar of GPU-frienly one-shot predictor for comparison.  I suspect  GraphEDX does not scale well to larger graphs. However, a comparison on in-distribution graphs, highlighting accuracy vs. interpretability vs. inference costm, would be intersting.\n\nMinor comment: The authors highlight Jain et al., 2024 in the dataset isomorphism issue, but perhaps [1] would be a better reference in this regard. \n\n[1] Position: Graph Matching Systems Deserve Better Benchmarks. In Forty-second International Conference on Machine Learning Position Paper Track.\n\n\n\nOverall, I am positive on the motivation and approach of this paper, but have questions about the neural architecture design and trade-offs around the autoregressive design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not have any ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wpoAJ0Y2ow", "forum": "6ZTcLNmguc", "replyto": "6ZTcLNmguc", "signatures": ["ICLR.cc/2026/Conference/Submission12510/Reviewer_F8dj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12510/Reviewer_F8dj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987440129, "cdate": 1761987440129, "tmdate": 1762923377671, "mdate": 1762923377671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present Gelato, an RL agent framework for solving graph edit distance. The authors first formulate the editing problem as a matching problem, and then train an RL agent to assign matched node pairs sequentially. The experimental evaluation shows that the proposed RL framework outperforms existing non-RL learning methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* This paper is well written with a self-contained story.\n* The evaluation on various datasets demonstrates the utility of the proposed RL method, and the empirical improvements over existing methods are impressive."}, "weaknesses": {"value": "* This manuscript overlooks an important prior work, [Liu et al: Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching](https://openreview.net/pdf?id=QjQibO3scV_). Liu et al presented an RL framework for the QAP form of graph matching, which is also used in Gelato. Though the underlying application and datasets are different, the methodologies of both papers seem relevant. I will be more convinced of the contribution and novelty of this paper if the authors can show the technical differences, unique innovations, and insights compared to Liu et al."}, "questions": {"value": "* I will be happy to reconsider this manuscript if the authors could provide more discussion with the relevant work, [Liu et al: Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching](https://openreview.net/pdf?id=QjQibO3scV_)\n* What will the performance look like if the methodology in Liu et al. is directly applied to GED learning?\n* If I understand it correctly, different models are trained on different datasets. What will the accuracy look like if a model is trained on, say, the AIDS dataset and tested on other datasets? Is it possible to plot a confusion matrix to report the generalization ability? This question is important because the agent is learning the QAP, and we should expect it to generalize with such a general form.\n* Is it possible to train a general agent by mixing multiple training datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bsBdwHRPiL", "forum": "6ZTcLNmguc", "replyto": "6ZTcLNmguc", "signatures": ["ICLR.cc/2026/Conference/Submission12510/Reviewer_W2sn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12510/Reviewer_W2sn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762226144362, "cdate": 1762226144362, "tmdate": 1762923377105, "mdate": 1762923377105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear reviewers, we would like to express our appreciation for the several positive comments on our paper, including\n- The novelty of our approach (eo4E)\n- The strong empirical results (W2sn, eo4E)\n- The robust generalization to larger graphs (F8dj, eo4E)\n- The interpretability of results in form a matchings/edit paths (F8dj, jqC1)\n\nWe would like to thank you for your insightful comments and suggestions on how to further improve the paper. We have carefully considered all of your feedback and made revisions accordingly (highlighted in blue in the paper), which we believe have considerably strengthened our paper. In particular, based on your feedback, we included\n- clearer explanations (e.g. for the reduction procedure)\n- an extensive hyperparameter study\n- additional related work\n\nPlease find detailed replies to all comments in the individual answers below."}}, "id": "nlPGcGcaxC", "forum": "6ZTcLNmguc", "replyto": "6ZTcLNmguc", "signatures": ["ICLR.cc/2026/Conference/Submission12510/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12510/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission12510/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763313336767, "cdate": 1763313336767, "tmdate": 1763313336767, "mdate": 1763313336767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}