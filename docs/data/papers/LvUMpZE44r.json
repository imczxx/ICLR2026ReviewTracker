{"id": "LvUMpZE44r", "number": 9458, "cdate": 1758123207366, "mdate": 1763694134000, "content": {"title": "PrefixMemory-Tuning: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for rapidly adapting large language models (LLMs) to downstream tasks. Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability to achieve performance comparable to full fine-tuning with significantly reduced computational and memory overhead. However, despite its earlier success, its effectiveness in training modern state-of-the-art LLMs has been very limited. In this work, we demonstrate empirically that Prefix-Tuning underperforms on LLMs because of an inherent tradeoff between the contribution of input prompt and parameterized prefix within the attention head. This motivates us to introduce PrefixMemory-Tuning, an architecture that generalizes the principles of Prefix-Tuning while addressing its shortcomings by shifting the prefix module out of the attention head itself and improving its expressiveness. Our experiments show that, across diverse benchmarks, PrefixMemory-Tuning consistently outperforms existing Prefix-Tuning methods. Notably, it achieves competitive performance with modern PEFTs on several general benchmarks, highlighting a potential extension of Prefix-Tuning approaches to become state-of-the-art. Our findings suggest that by overcoming its inherent limitations, Prefix-Tuning can remain a competitive and relevant research direction in the landscape of parameter-efficient LLM adaptation.", "tldr": "", "keywords": ["Large Language Model", "Fine-Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c05d2215e3e4583728499ab56576f61d712b3968.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the limitations of Prefix-Tuning (PT), a parameter-efficient fine-tuning (PEFT) method for large language models (LLMs), which struggles with modern LLMs due to a trade-off between input and prefix contributions within the attention head. The authors introduce PrefixMemory-Tuning (PMT), a novel approach that decouples the prefix from the attention head, enhancing expressiveness and performance. Empirical results show PMT outperforms PT and is competitive with state-of-the-art methods like LoRA, achieving an average improvement of 8.1% over LoRA and 29.4% over PT across six benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. PMT addresses a fundamental limitation of PT by relocating the prefix outside the attention head, improving its scalability and expressiveness.\n\n2. Extensive experiments across diverse benchmarks (e.g., preference alignment, math reasoning) demonstrate PMT's competitive performance.\n\n3. The paper provides a clear explanation of PT's underperformance and a unified framework for future context-based PEFT methods."}, "weaknesses": {"value": "1. The study uses simple feature maps (elu, gelu) as a proof of concept, leaving more expressive options unexplored due to implementation complexity.\n\n2. The paper does not deeply address computational cost or scalability for very large LLMs, which is critical for practical deployment."}, "questions": {"value": "1. How would PMT perform with more sophisticated feature maps (e.g., trainable MLPs) compared to the current elu/gelu implementations?\n\n2. Some related work can be theoretically discussed. \"E^ 2vpt: An effective and efficient approach for visual prompt tuning\" ICCV, which adds some prompts in the attention head.\n\n3. What are the computational and memory overheads of PMT compared to LoRA and other PEFT methods at scale?\n\n4. How does PMT handle extremely long input sequences in real-world applications, given the trade-off issues identified in PT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qv5aQFfIyk", "forum": "LvUMpZE44r", "replyto": "LvUMpZE44r", "signatures": ["ICLR.cc/2026/Conference/Submission9458/Reviewer_gihG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9458/Reviewer_gihG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760493849073, "cdate": 1760493849073, "tmdate": 1762921050982, "mdate": 1762921050982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies the performance limitations of Prefix-Tuning in modern large language models (LLMs) and proposes a method called PrefixMemory-Tuning to address these issues."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": ">s1: Shifting the prefix module out of the attention head itself is a reasonable start point.\n\n>s2: Experiments show that, in the **few-shot setting**, PrefixMemory-Tuning is competitive with state-of-the-art approaches (such as LoRA).\n\n>s3: The presentation is clear, and the figures are of high quality."}, "weaknesses": {"value": "> w1: **The discussion of related work contains inaccuracies**. For instance, lines 87-90 state that \"LoRA+ refines this concept further, projecting the model’s weights onto low-dimensional subspaces to achieve efficiency comparable to full fine-tuning at significantly reduced computational cost.\" However, the actual contribution of LoRA+ lies in its theoretical analysis demonstrating that using identical learning rates for matrices A and B in standard LoRA prevents efficient feature learning in large-width networks. The method addresses this limitation by employing differentially scaled learning rates for the adapter matrices with an optimally determined ratio, rather than proposing weight projection onto low-dimensional subspaces.\n>\n>  LoRA+: Efficient Low Rank Adaptation of Large Models. ICML 2024.\n\n>w2: There is **no support** for the claim in Line 144-146 \"Research shows that prefix-tuning excels in low-data or few-shot settings\".\n\n> w3: **Insufficient literature review**: To name a few: (1) For context-based PEFT methods: [1] [2]; (2) Lines 307-315, a good work to show the memory perspective is [3].\n>\n>[1] DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. ICLR 2024.\n>\n>[2] ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. ICLR 2025.\n>\n> [3] Transformer Feed-Forward Layers Are Key-Value Memories. EMNLP 2021.\n\n>w4: The experiments were primarily conducted in a **few-shot setting**. Recent studies (such as [4]) have also found zero-shot approaches to be competitive or even superior in certain scenarios. What are your thoughts on this?\n>\n>[4] Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot. EMNLP Findings 2025."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nPC4pMQIvk", "forum": "LvUMpZE44r", "replyto": "LvUMpZE44r", "signatures": ["ICLR.cc/2026/Conference/Submission9458/Reviewer_GSuw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9458/Reviewer_GSuw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760951204578, "cdate": 1760951204578, "tmdate": 1762921050602, "mdate": 1762921050602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **PMT (PrefixMemory-Tuning)**, a novel parameter-efficient fine-tuning (PEFT) method. The authors build on the evolution of Prefix-Tuning (PT), which was largely abandoned due to its poor scaling properties in large models, as well as more recent methods like LoRA and GaLore. While PT had been likened to prompt-based learning by introducing a set of trainable continuous vectors (prefixes) for each input, early analyses identified its underperformance as stemming from an inability to reshape attention patterns within attention heads. However, the authors argue that the true cause of PT’s degradation lies in the **trade-off between the prefix and the input representations**.\n\nLeveraging this insight, the authors improve upon conventional PT, proposing PMT as a more effective and efficient approach. Their experiments show that PMT surpasses both **LoRA** and even **full fine-tuning (FFT)** in terms of fine-tuning performance, providing a more scalable and efficient method for adapting large models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a strong theoretical analysis of how fine-tuning (FT) impacts the activation values of attention heads, reducing the relationship to a linear function of the original attention values. This simplification reveals the fundamental cause of **Prefix-Tuning (PT)**'s degradation in large-scale models. The authors effectively demonstrate how the trade-off between prefix and input representations negatively affects model performance.\n\n- The paper offers a clear and insightful analysis of the evolution from **Prefix-Tuning (PT)** to **PMT**, highlighting the **coupling problem** between the prefix and input representations in traditional PT. The authors address this issue by introducing effective approximation techniques that successfully decouple the components. Their experimental results consistently show that **PMT** outperforms **LoRA** and even surpasses **full fine-tuning (FFT)** on most tasks, demonstrating the effectiveness of their approach."}, "weaknesses": {"value": "- The experimental models used in the paper, such as **LLaMA2-7B-Chat** and **Qwen2.5-3B-Instruct**, are relatively small in scale. Given that these models are not at the forefront of current large-scale models, the evaluation does not demonstrate PMT’s performance on truly large-scale architectures, where the method’s scalability and effectiveness may vary. Including experiments on larger models would strengthen the claims regarding PMT's applicability to cutting-edge architectures.\n\n- The comparison to other PEFT methods lacks more recent and advanced variants such as **QLoRA** and **LoRA+**, which are gaining traction in the field. This limits the strength of the empirical comparison, as it does not reflect the latest advancements in PEFT techniques."}, "questions": {"value": "- The PMT architecture can be viewed as adding a linear transformation (similar to a mapping based on \\( q_i \\)) on top of the original model output. This raises an interesting question: could the model architecture itself be modified to inherently incorporate such a transformation, thereby achieving PMT-like behavior without the need for additional components? If this approach were viable, it might improve the model's generalization and efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MEyaDYG1Tx", "forum": "LvUMpZE44r", "replyto": "LvUMpZE44r", "signatures": ["ICLR.cc/2026/Conference/Submission9458/Reviewer_JeiT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9458/Reviewer_JeiT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715562335, "cdate": 1761715562335, "tmdate": 1762921050178, "mdate": 1762921050178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, prefix-finetuning in transformer is re-visited and a new prefix-memory module is added to the scaled dot product module in transformer. By revisiting the existing PEFT framework, it's pointed out that the main gap between prefix-tuning and other efficient finetuning approach like LoRa lies in the attention in-balance of learnable prefix and the input context. When the context is long, the contribution of learnable prefix diminished, and vice versa. To resolve this, the prefix-memory module is added directly to the scaled dot prod structure and used to dynamically adjust the learned attention to fix the problem above. Experiments are conducted to show the competitive results on multiple datasets when compared with SFT and other efficient tuning approach like LoRA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The analysis on why prefix tuning failed under extreme cases is convincing and reasonable. E.g in section 4.2, the qualitative analysis makes sense to illustrate the attention balance is broken when either input context or prefix is relatively long."}, "weaknesses": {"value": "- The evaluation is a bit weak. In table 1 the comparisons are made between multiple finetuning approaches, like PMT(proposed), SFT, LoRA, and prefix tuning. However the results are confusing as the SFT results are weaker than other PEFT approaches. This indicates that the dataset used here might not be challenging enough.\n- The module is called \"prefix memory\", but in the paper there is no analysis about what does this memory module learned. It would be better to qualitatively or quantitatively analyze about this to provide insights.\n- Did not mentioned or compare with some related work, like attention sink and Aprompt. Attention sink had some similar observation that the model's attention is usually overindexed to some specific tokens."}, "questions": {"value": "- The evaluation is a bit weak. In table 1 the comparisons are made between multiple finetuning approaches, like PMT(proposed), SFT, LoRA, and prefix tuning. However the results are confusing as the SFT results are weaker than other PEFT approaches. This indicates that the dataset used here might not be challenging enough. $\\rightarrow$ Is it possible to test on some more challenging tasks where the full SFT is needed and the usefulness of PMT can be better highlighted? \n- The module is called \"prefix memory\", but in the paper there is no analysis about what does this memory module learned. It would be better to qualitatively or quantitatively analyze about this to provide insights. $\\rightarrow$ Is it possible to get more intuitive understanding about what did this newly added memory module learn.\n- Did not mentioned or compare with some related work, like attention sink[1] and Aprompt [2]. For 1, seems the attention analysis in this work is similar, and for 2 it's a popular prompt based PEFT approach. \n\n[1] Xiao, Guangxuan, et al. \"Efficient streaming language models with attention sinks.\" arXiv preprint arXiv:2309.17453 (2023).\n[2] Wang, Qifan, et al. \"Aprompt: Attention prompt tuning for efficient adaptation of pre-trained language models.\" Proceedings of the 2023 conference on empirical methods in natural language processing. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IDQJaHoWVt", "forum": "LvUMpZE44r", "replyto": "LvUMpZE44r", "signatures": ["ICLR.cc/2026/Conference/Submission9458/Reviewer_UiQ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9458/Reviewer_UiQ2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957638196, "cdate": 1761957638196, "tmdate": 1762921049709, "mdate": 1762921049709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}