{"id": "8xTDnj39Ti", "number": 4267, "cdate": 1757651501212, "mdate": 1759898042678, "content": {"title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning", "abstract": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing **Vlaser** - a **V**ision-**L**anguage-**A**ction Model with **s**ynergistic **e**mbodied **r**easoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks—including spatial reasoning, embodied grounding, embodied QA, and task planning.\nFurthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark. We will open-source the model weights, data generation pipelines, and the full dataset to support future research.", "tldr": "We introduce Vlaser,  a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents.", "keywords": ["Vision-Language-Model", "Vision-Language-Action-Model", "Embodied Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33e2acf7b0070c006690f1ebcc047eafc23b53c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes VLASER, a VLA model with collaborative embodied inference ability, which aims to solve the domain gap between upstream VLM inference and downstream VLA strategy learning. VLASER is based on InternVL3, including a two-stage training process of \"VLM pre-training + VLA fine-tuning\", and relies on the Vlaser-6M dataset to achieve SOTA performance on 12 embodied inference benchmarks (Vlaser-8B average score of 51.3), while in WidowX Achieve the best results on the robot benchmark (Vlaser-QA average success rate of 64.6%) and perform competitively on the Google Robot benchmark; The study also reveals key findings: out-of-domain embodied inference data has limited VLA performance improvement, while in-domain data based on robot interaction data can accelerate VLA convergence and improve task success rate."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "See Summary"}, "weaknesses": {"value": "1. In Section 3.2, the authors said there is no positive correlation between common embodied reasoning benchmarks and the closed-loop control performance of robot-specific embodied subordinates, but there is a lack of corresponding quantitative analysis or visualization support.\n2. Figure 2 aims to show the two-stage training process of Vlaser, but the data flow and processing logic within each stage are not clear enough, and the relationship between the steps is unclear.\n3. The description of the training process in Section 2.3 is fuzzy. Although the authors describe the two-stage training strategy and the form of the core loss function, there is not enough detail about the mathematical mechanism of key processes such as flow matching action generation. The relevant parameters only give fixed values, but do not explain their physical meaning or selection basis.\n4. The paper mentions that the VLA data in 2 million domains is generated by Qwen2.5VL-7B, but does not explain how to control the data quality after the generation, and suggests supplementary data quality evaluation methods.\n5. The paper points out that \"in-domain data improves VLA performance\", but does not clarify the separate contributions of general QA, grounding QA, and spatial inference QA in in-domain data.\n6. The title of the paper mentions synergistic embodied reasoning, but it is not clear where the synergy is reflected."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dOF2pMTlLr", "forum": "8xTDnj39Ti", "replyto": "8xTDnj39Ti", "signatures": ["ICLR.cc/2026/Conference/Submission4267/Reviewer_HWMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4267/Reviewer_HWMv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535667828, "cdate": 1761535667828, "tmdate": 1762917267788, "mdate": 1762917267788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an embodied vision-language-action model built on InternVL together with the Vlaser-6M data engine. It uses a two-stage recipe, embodied VLM pretraining followed by VLA fine-tuning with a flow-matching action expert for low-level control. The paper provides an analysis of which pretraining data streams transfer best to VLA policy learning, giving practical guidance on building task-aware data mixtures. Vlaser attains state-of-the-art results across embodied reasoning benchmarks and shows competitive closed-loop manipulation performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is clearly written and easy to follow.\n* This paper tackles an important problem: bridging VLMs and VLAs via embodied-data pretraining and in-domain fine-tuning.\n* Results on embodied reasoning benchmarks and SimplerEnv are promising."}, "weaknesses": {"value": "* For a more informative study, I suggest the authors to include ablations on the pretaining dataset, quantifying how each source contributes to embodied reasoning and robot control.\n\n* In the planning data curation, the authors collected data from Habitat. Did you avoid overlap with the EB-Habitat evaluation set? Additionally, please elaborate more on why Habitat was chosen for planning data over alternatives such as ALFRED.\n\n* For in-domain data curation, how are the questions and answers in the QA/Grounding/Spatial pairs generated? How to ensure the quality and diversity of the dataset?\n\n* In the training recipe, the authors adopt action chunks for robot control. An ablation over different chunk sizes would help motivate this design.\n\n* In Table 3, I do not see results for combining all in-domain data, yet line 419 claims gains from this combination. Please add the result or clarify.\n\n* A further limitation is the lack of real-robot experiments. While challenging, this should at least be acknowledged in the limitations section."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rKDsNJzGL6", "forum": "8xTDnj39Ti", "replyto": "8xTDnj39Ti", "signatures": ["ICLR.cc/2026/Conference/Submission4267/Reviewer_ZtrG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4267/Reviewer_ZtrG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553709405, "cdate": 1761553709405, "tmdate": 1762917267582, "mdate": 1762917267582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Vlaser, a vision-language-action model that unifies embodied reasoning and robot control through a large-scale dataset and a dual-stage training pipeline. Built on InternVL3 and Qwen2.5 backbones with a flow-matching action expert, Vlaser achieves state-of-the-art results on 12 embodied reasoning benchmarks and strong performance on real-robot tasks like WidowX and Google Robot. The authors also provide a systematic analysis of data transfer, showing that in-domain embodied data is far more effective for policy learning than generic web data, highlighting a persistent domain gap between reasoning and control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a unified framework for embodied tasks and introduces the Vlaser-6M dataset.\n2. The dataset is large, diverse, and carefully curated, covering grounding, spatial reasoning, planning, and simulation tasks.\n3. The study offers useful observations on the gap between vision-language reasoning and low-level control, providing practical guidance for future model design."}, "weaknesses": {"value": "1. The detailed architecture of the action expert is not presented. In Figure 2, the structure of the action expert is not shown.\n2. I think the authors could provide more explanations or figures to help readers understand the structure of the action expert and the training process.\n3. Since the paper emphasizes the impact of VLM reasoning on VLA performance, I think the authors could add some experiments on real robots or additional simulators to demonstrate the model’s generalization capability."}, "questions": {"value": "1. The paper mentions “low-level control.” Does this refer to directly controlling the robot’s motors? This terminology seems to be commonly used in the robotics domain.\n2. Although the paper describes the training and inference procedures, it is still unclear what exactly the pretrained VLM passes to the VLA.\n3. In Figure 1, why are the closed-source models not included in the ranking?\n4. Explain what is the open-loop inference and closed-loop control?\n5. After adding the reasoning capability, does it affect the runtime efficiency of the VLA? Is this reasoning capability truly cost-effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pT71V7KPd9", "forum": "8xTDnj39Ti", "replyto": "8xTDnj39Ti", "signatures": ["ICLR.cc/2026/Conference/Submission4267/Reviewer_cVAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4267/Reviewer_cVAr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973771471, "cdate": 1761973771471, "tmdate": 1762917266139, "mdate": 1762917266139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Vlaser, a vision-language-action framework that couples a VLM for embodied reasoning with a flow-matching action expert for low-level control generation, supported by the large-scale Vlaser-6M data engine. The approach unifies grounding, spatial reasoning, and planning with closed-loop execution. Evaluations across 12 embodied benchmarks and SimplerEnv on WidowX and Google Robot demonstrate state-of-the-art performance and reveal that in-domain robot-view data drives the largest gains in downstream control tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, with clear figures and detailed explanations that make the work easy to follow.\n\n2. The method introduces a clean end-to-end VLA design that decouples vision-language reasoning from an action expert and leverages flow matching for low-level control, enabling seamless closed-loop execution.\n\n3. The paper demonstrates state-of-the-art performance across 12 embodied-reasoning benchmarks and consistently surpasses strong baselines.\n\n4. It also conducts a systematic analysis of pretraining data effectiveness for VLA transfer and offers scaling insights."}, "weaknesses": {"value": "1. The paper compares only two model scales within a single architecture and concludes that smaller models are better at point-based grounding while larger models excel at multi-step planning and closed-loop simulation; however, this scale–performance relationship may not generalize to other model families, and the work provides little deeper analysis or mechanistic explanation to substantiate it.\n\n2. The policy relies on single-frame observations and fixed-length action sequences, with the action expert employing a fixed 10-step flow-matching sampler. This design limits the evaluation of temporal reasoning and multi-view robustness. The paper does not conduct ablations on these hyperparameters. The training uses an observation history of only one frame, which may constrain long-horizon planning capabilities."}, "questions": {"value": "1. In Table 3, why does Vlaser exhibit lower performance than the InternVL base model? Could the authors include an ablation study that directly fine-tunes the base model on different in-domain datasets, i.e., to produce Vlaser-QA?\n\n2. The evaluation covers only a small set of manipulation tasks on WidowX and Google Robot, making it difficult to claim generality to other embodiments or tasks. I recommend adding the second-part experiments and reporting the corresponding 8B results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tq8I9VDhVw", "forum": "8xTDnj39Ti", "replyto": "8xTDnj39Ti", "signatures": ["ICLR.cc/2026/Conference/Submission4267/Reviewer_QnV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4267/Reviewer_QnV2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990945274, "cdate": 1761990945274, "tmdate": 1762917265064, "mdate": 1762917265064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}