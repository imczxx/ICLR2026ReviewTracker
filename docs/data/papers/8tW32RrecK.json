{"id": "8tW32RrecK", "number": 7666, "cdate": 1758031257141, "mdate": 1763561603959, "content": {"title": "Turning Tabular Foundation Models into Graph Foundation Models", "abstract": "While foundation models have revolutionized such fields as natural language processing and computer vision, their potential in graph machine learning remains largely unexplored. One of the key challenges in designing graph foundation models (GFMs) is handling diverse node features that can vary across different graph datasets. While many works on GFMs have focused exclusively on text-attributed graphs, the problem of handling arbitrary features of other types in GFMs has not been fully addressed. However, this problem is not unique to the graph domain, as it also arises in the field of machine learning for tabular data. In this work, motivated by the recent success of tabular foundation models (TFMs) like TabPFNv2 or LimiX, we propose G2T-FM, a simple framework for turning tabular foundation models into graph foundation models. Specifically, G2T-FM augments the original node features with neighborhood feature aggregation, adds structural embeddings, and then applies a TFM to the constructed node representations. Even in a fully in-context regime, our model achieves strong results, significantly outperforming publicly available GFMs and performing competitively with, and often better than, well-tuned GNNs trained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tuned GNN baselines. In particular, when combined with LimiX, G2T-FM often outperforms the best GNN by a significant margin. In summary, our paper reveals the potential of a previously overlooked direction of utilizing tabular foundation models for graph machine learning tasks.", "tldr": "", "keywords": ["graph foundation models", "tabular foundation models", "TabPFNv2", "LimiX", "graph neural network", "graph machine learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f5c85c42edc2bd15deefca5db303f68c92be483.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces G2T-FM, which turns graphs into enriched tabular data by concatenating (i) neighborhood feature aggregations, (ii) classic structural stats (degree, PageRank, Laplacian eigenvectors), and (iii) PEARL-style structural encodings, then feeds this into a tabular foundation model (TabPFNv2 or LimiX). In in-context and finetuning regimes it reports competitive or superior node-level performance to tuned GNNs and clearly stronger results than publicly available GFMs; the approach also supports regression."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It is a novel approach to apply tabular foundation model to the graph domain.\n2. It unifies both the regression and classification of node property prediction task in one framework."}, "weaknesses": {"value": "1. The method’s graph learning capacity largely relies on the tabular foundation model (TFM) backbone. The paper does not analyze the backbone’s inductive biases or provide any “ability boundary” characterization (e.g., what classes of structural patterns can/can’t be captured), leaving the effective limits of the approach unclear.\n2. Computing Laplacian eigenvectors and PEARL embeddings can be expensive on large graphs, yet there is no end-to-end complexity or runtime/memory analysis.\n3. The framework uses only basic structural processing; there is no cross-graph pretraining or learned multi-hop structural module. As a result, tasks relying on the structures may remain underperforming.\n4. Despite the novel use of TFMs for graphs, experiments are confined to transductive node-level prediction. Without evidence of inductive generalization (new nodes/graphs), cross-graph transfer, and multi-task coverage (edge/graph-level), the “graph foundation model” claim feels premature."}, "questions": {"value": "1. How was the finetuning of the G2T-TabPFNv2/G2T-LimiX backbone of conducted? Can more details be provided?\n2. How does G2T-FM perform when evaluated inductively (new nodes/graphs at test time without transductive access)? Any changes needed?\n3. What are the time/memory costs for NFA, Laplacian eigenvectors, and PEARL as node/edge counts grow? Could you provide wall-clock and peak-memory vs. GNN baselines?\n4.How sensitive are results to the number of Laplacian components, PEARL repeats, and NFA hop/aggregation choices? Can you report per-dataset optima/robustness?\n5. Can the framework be applied to other graph learning tasks? What kinds of changes are needed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eCD6kKxhTF", "forum": "8tW32RrecK", "replyto": "8tW32RrecK", "signatures": ["ICLR.cc/2026/Conference/Submission7666/Reviewer_EACW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7666/Reviewer_EACW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761233434978, "cdate": 1761233434978, "tmdate": 1762919733737, "mdate": 1762919733737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes G2T-FM, a simple framework that adapts tabular foundation models (TFMs) to graph machine learning, addressing the challenge of heterogeneous node feature spaces and target spaces. Experiments on diverse datasets show that G2T-FM in both in-context and finetuning regimes can outperform well-tuned GNN baselines and existing openly available GFMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a clear and compelling analogy between tabular data and heterogeneous graph features, enabling the transfer of advances from tabular foundation models (TFMs) into the graph machine learning domain.\n\n2. It provides a thorough and insightful discussion of the limitations of prior graph foundation models, highlighting gaps in generalization and feature handling.\n\n3. On multiple datasets, the proposed approach achieves competitive or superior results compared to well-tuned traditional GNN baselines and existing GFM implementations, despite its simplicity."}, "weaknesses": {"value": "1. The idea is insightful but the technical realization is relatively minimal. The method consists largely of straightforward one-hop feature aggregation and concatenation followed by application of an existing TFM.\n\n2. The framework aggregates only one-hop neighbor node features, raising concerns about its expressiveness and ability to capture more complex, multi-hop dependencies.\n\n3. Certain experimental settings and results require further justification. Specifically: Can TS-GNN actually be finetuned (line 370)? Why does G2T-LimiX (ICL) outperform G2T-LimiX (FT) on the tolokers-2 dataset (Table 2)? Why were different data splits used (line 415) compared to other GFMs?"}, "questions": {"value": "1. Why not go further and pretrain a dedicated GFM based on your proposed graph-to-tabular framework? For example, converting the large-scale graph datasets typically used to train GFMs into tabular form and performing cross-graph pretraining could yield a more impactful contribution.\n\n2. See Weaknesses 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "30P8QnWCdW", "forum": "8tW32RrecK", "replyto": "8tW32RrecK", "signatures": ["ICLR.cc/2026/Conference/Submission7666/Reviewer_Cmbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7666/Reviewer_Cmbg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652420433, "cdate": 1761652420433, "tmdate": 1762919733285, "mdate": 1762919733285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,\n\nWe thank you for your valuable feedback. This is a general response to all reviews. We addressed the specific comments and questions in individual responses below.\n\nMany of you expressed concerns regarding efficiency, scalability to larger graphs, and applicability to link- and graph-level tasks. While these comments are valid, we believe that **the main contribution of our work was underestimated**, and we want to elaborate on it.\n\nFor a long time, **we believed that graph foundation models were impossible to create** due to several fundamental challenges. Even when focusing only on node-level prediction tasks, graphs are found in extremely diverse domains ranging from social communications and web graphs to transport networks. This makes it difficult to imagine a single model that can handle all of these tasks and achieve knowledge transfer between different datasets. Furthermore, unlike CV or NLP, in the graph ML field there remains an insufficient amount of high-quality, large-scale, diverse pretraining graph datasets, which makes it difficult to cover the vast landscape of potential applications.\n\nWe believe that **previous works have not fully overcome these challenges**. Most progress has been limited to text-attributed graphs, where tasks can be unified via text encoders, or to few-shot scenarios, which are less practical for many real-world applications. As far as we know, **existing GFMs perform significantly worse than well-tuned GNNs** trained from scratch in the general case of node-level tasks with non-fewshot splits and graphs from arbitrary domains and arbitrary features. We would welcome evidence to the contrary, but our evaluation results support this statement and align with our intuition. For example, many prior GFMs are pretrained on at most several dozen graphs, which is orders of magnitude smaller than the amount of data used for pretraining foundation models in other domains. **The similar position was expressed by other researchers**, as outlined in [1], which points to the lack of truly generalizable GFMs.\n\nBy contrast, our proposed **G2T-FM is the first GFM that can reliably compete with and often significantly outperform GNNs trained from scratch on node-level prediction tasks**. This represents a significant advancement over the existing open-source GFMs and, in our opinion, constitutes an important milestone for the field. While G2T-FM is possibly not yet ready for deployment in production settings due to scalability issues, **it provides a valuable proof of concept and a strong baseline for future research in GFMs**.\n\nWe would also like to note that the considered TabPFNv2 and LimiX models are examples of Prior-data Fitted Networks (PFNs, [2]), which is a relatively young approach and the subject of active research now. Scalability to large datasets is a general problem for this approach (rather than a specific issue with our method), but progress in solving this problem will likely be made in the future, and all solutions will be directly applicable to our method. We would like to emphasize that **our work is the first to apply PFNs to graph machine learning tasks and we believe it is a significant contribution**.\n\nIn summary, **the key contribution of our work is that it shows that GFMs are feasible**. While considerable work remains, such as scaling to larger graphs and addressing link- and graph-level tasks, our results show a clear and promising direction forward. We hope that our work will support the future development of GFMs and will inspire further research in this area.\n\nBest regards,\n\nAuthors\n\n***\n\n[1] Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks, ICML 2025\n\n[2] Transformers Can Do Bayesian Inference, ICLR 2022"}, "title": {"value": "General response to all reviews"}}, "id": "itqnPkUCKC", "forum": "8tW32RrecK", "replyto": "8tW32RrecK", "signatures": ["ICLR.cc/2026/Conference/Submission7666/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7666/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7666/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763561757943, "cdate": 1763561757943, "tmdate": 1763565509415, "mdate": 1763565509415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a preprocessing pipeline for graph learning tasks to convert them to tabular form effectively such that tabular foundation models (TFM) can operate on them. Their pipeline consists of a mixture of hand-crafted, heuristic and learnable feature extractors to obtain tabular features, which is fed into a TFM in either an in-context-learning or fine-tuning setting. The authors demonstrate competitive performance across several benchmarks in which they comfortably outperform graph foundation models (GFM) and are competitive against a variety of GNNs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written; the contributions are clearly stated and the logical flow is easy to follow.\n2. The core idea of leveraging well-established tools from graph learning literature to adapt graph tasks to TFMs, which is conceptually simple but effective.\n3. The results clearly back the authors’ claims; ICL results on datasets with text-based features are particularly promising. I am convinced that G2T-FMs work reasonably well even in an ICL setting, but require further convincing on the overall usefulness over existing methods (see Weaknesses)."}, "weaknesses": {"value": "1. I agree that the ability to leverage TFMs for graph tasks is a valuable contribution in itself, but I don’t think the paper’s contributions go much beyond that, resulting in a paper limited in scope and largely relying on the success of its implementation. In relation with this, I think the paper somewhat oversells its contributions -- while I understand the reasoning to associate the resulting framework with graph foundation models (GFM), I think it’s bit of a stretch to argue that the resulting model is a GFM in the conventional sense. Applying the proposed framework to _any_ graph requires pre-computing structure-based features, which comes with a non-trivial computational cost per graph; avoiding such hand-crafted feature engineering is one of the main driving forces of graph representation learning, and in a related manner GFMs in the first place. The hand-crafted features alone mean the learned embeddings are not transferable by themselves without computing these features for unseen graphs first.\n2. Weak benchmarks: The current experimental section needs to be significantly strengthened to make a more convincing argument towards the merits of G2T-FMs. The crux of the paper is that using tools like NFA, structural features and heuristic (LapPE)/learnable (PEARL) positional encodings allow us to apply TFMs in graph data. Note that most if not all of these tools can be directly applied to not just G2T-FMs but also both GNN and GFN benchmarks compared against. Measuring G2T-FMs against benchmark methods that do not also use structural features or positional encodings results in unfair evaluation — whether G2T-FMs can outperform other architectures when the same structural information is provided to all will provide a much healthier signal on the usefulness of TFMs on graph data. I suggest several evaluation settings to the authors in the Questions section.\n3. In relation to the previous point, the authors don’t address how their G2T-FMs compare with the other benchmarks, in particular the GFMs, in terms of efficiency.  What are the parameter counts for the optimal models? How long does pre-training and/or fine-tuning take for each? How fast is downstream inference? Answering these questions will allow evaluating the merits of the proposed model better, but the information simply isn’t there. Similarly, the cost of graph pre-processing is not discussed, which is crucial considering they compare against benchmarks that do not have this pre-processing overhead.\n\n**Additional comments (no effect on score):**\n- I think referring to the Shi et al. (2021) architecture as GT in shorthand in the experiments is confusing since it uses local attention as opposed to global attention over the graph (more akin to GAT in this), which is typically considered the defining characteristic of GT architectures; Shi et al. (2021) themselves refer to their model as UniMP so I suggest reverting to that.\n\n**Conclusion:** I think this work is primarily a method paper with relatively small theoretical component — and this is to an extent fine, with the caveat that the potential impact of the paper will then be largely determined by whether it provides any performance or efficiency gains in competitive scenarios. Thus, my view is that for acceptance this work needs to be _very_ convincing regarding these performance or efficiency gains; with the current evaluations, while promising, I am not fully convinced this is the case (hence my focus on the weaknesses in evaluation and request for additional results, something I try to avoid asking unless well-justified). Therefore I currently recommend rejection, though again with better evaluation and convincing results I may be persuaded."}, "questions": {"value": "1. Re: W2, Here are several setups that I would have liked to see G2T-FMs compete against:\n   - GNNs with (a) structural features, (b) heuristic positional/structural encodings (PSE) like Laplacian PE and andom walk encodings (RWSE), and (c) learnable PSEs like GPSE [1] and PEARL. At the very least, GNN results using identical features & encodings (namely, node degree, PageRank, Laplacian PE, PEARL, _and_ their combinations) are required. I suggest RWSE on the basis that it may capture different structural information than Laplacian PEs (in the sense that they may complement each other); I suggest GPSE because it is a _learnable_ PSEs similar to PEARL, but learns over a large variety of PSEs to arrive at a unified representation and demonstrates generalization capabilities over OOD graphs. \n   - _Global_ graph transformer (GT) architectures with the above structural features & PSEs. With global, I refer to GTs that leverage _non-local_ attention, unlike GAT or the Shi et al. (2021) GT. Of course, quadratic scaling of GTs on large graphs pose a problem here, so sparse GT implementations like Performer [2]/Exphormer [3]/NodeFormer [4] etc. would be more appropriate here. I suggest picking one architecture and focusing on a subset of more heterophilic tasks where GTs are more likely to outperform GNNs.\n   - GFN benchmarks with the above structural features & PSEs. These GFNs should be able to handle arbitrary node features, _and_ at least some of them can likely benefit from such structural information akin to conventional GNN/GTs.\n2. Re: W3, as mentioned in the Weaknesses section, I suggest the authors provide information on (a) model size and pre-training/fine-tuning/evaluation efficiency, (b) pre-processing overhead of the G2T-FM pipeline, and provide an overview of the benefits of G2T-FMs from this standpoint.\n\n[1] Cantürk, S., Liu, R., Lapointe-Gagné, O., Létourneau, V., Wolf, G., Beaini, D., Rampášek, L. (2024). Graph Positional and Structural Encoder. Proceedings of the 41st ICML 2024, PMLR 235:5533-5566.\n\n[2] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlós, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L.J., & Weller, A. (2020). Rethinking Attention with Performers. ArXiv, abs/2009.14794.\n\n[3] Shirzad, H., Velingker, A., Venkatachalam, B., Sutherland, D.J., and Sinop, A.K. (2023). EXPHORMER: sparse transformers for graphs. In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 1310, 31613–31632.\n\n[4] Wu, Q., Zhao, W., Li, Z., Wipf, D.P., & Yan, J. (2023). NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification. ArXiv, abs/2306.08385."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U9V000eqgV", "forum": "8tW32RrecK", "replyto": "8tW32RrecK", "signatures": ["ICLR.cc/2026/Conference/Submission7666/Reviewer_uY6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7666/Reviewer_uY6A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753424365, "cdate": 1761753424365, "tmdate": 1762919732768, "mdate": 1762919732768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes G2T-FM, a framework that transforms tabular foundation models (TFMs) into graph foundation models (GFMs) by incorporating neighborhood aggregation and structural embeddings. It achieves strong in-context and fine-tuned performance, surpassing existing GFMs and even well-tuned GNNs on various graph tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper first studies the potential of tabular foundation models in graph-related applications. \n2. The proposed approach is common and, in general, sound in graph-related applications."}, "weaknesses": {"value": "1. The novelty of this work is limited. Although it claims to be the first attempt at turning TFMs into GFMs, the proposed method is straightforward: only adding structure features to node features and reusing existing TFMs. This approach, though effective, is well-established and there is no surprise that including such side information will lead to improvements. \n2. The efficiency/cost of the proposed method is not discussed. Specifically, it requires additional preprocessing for computing the complementary features, which can be costly for large graphs. It also didn't compare the test-time efficiency with existing methods, neither in the ICL setting nor in the fine-tuning setting. Given the size of TFMs, even fine-tuning them on test datasets could be costly.\n3.  Several details about implementation are missing, e.g., the order of feature aggregation, the steps of fine-tuning.\n4. Limited gains: the authors mentioned that the observed performance gains of their method might come from the inclusion of side information that was never used in baseline models. Table 5 shows that, with enhanced features, simple GNNs perform reasonably well and the gains of the proposed method become less significant. Given the potentially high inference cost, the applicability of the proposed method is challenged."}, "questions": {"value": "What is the time/space complexity of the proposed method? How to extend it to larger graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kPcQAYGvpo", "forum": "8tW32RrecK", "replyto": "8tW32RrecK", "signatures": ["ICLR.cc/2026/Conference/Submission7666/Reviewer_hjSn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7666/Reviewer_hjSn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970866396, "cdate": 1761970866396, "tmdate": 1762919732362, "mdate": 1762919732362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}