{"id": "xTbFeDhdRG", "number": 800, "cdate": 1756818620512, "mdate": 1759898241230, "content": {"title": "RAR: Reversing Visual Attention Re-Sinking for  Unlocking Potential in Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision-language tasks, yet they frequently exhibit suboptimal output layers, where intermediate decoder layers outperform the final ones, signaling underutilized model capacity. In this work, we delve into the root causes and attribute this issue to the Visual Attention Re-sinking phenomenon, precipitated by attention gradient sparsity driven by textual supervision dominance. This degradation causes attention heads to evolve into sink heads that prioritize low-semantic backgrounds, thereby disrupting modality fusion, neglecting visual information, and biasing outputs toward textual priors, ultimately impairing model performance. To mitigate this, we introduce a parameter-free Sink Attention Dynamic Sparsification (SADS) framework that dynamically identifies and retains all vision heads(concentrating visual attention on semantically relevant regions) while sparsifying sink heads, preserving essential global context through a shared head. Integrated into diverse MLLMs, our framework yields substantial performance gains across 20 benchmarks spanning five task categories (visual grounding, general VQA, OCR-related VQA, vision-centric tasks, and visual hallucination mitigation) surpassing supervised fine-tuning while boosting inference speed by 11.7\\%. This approach offers a novel avenue for maximizing MLLMs capabilities.", "tldr": "", "keywords": ["MLLMs", "visual attention sink"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51909d5c1e2bdf814bd3e4077215935deb313e3f.pdf", "supplementary_material": "/attachment/bd20f1331d71adcc1fa5ac0a26be262580278ea3.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents RAR: a parameter-free method to address the problem of re-sinks in the final layers of a vision-language model, that cause the model to underperform due to undesired attentions. Unhealthy attention heads are removed, which improves the performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The observations about the problems in the final layers, the degradation of performance because of that, the illustrations of spurious visual attention, are all interesting. The finding that visual attention is bi-modal and separates the vision heads and sink heads, is interesting too. The performance gains are structural but marginal. The proposed method is effective and simple, which makes the approach feasible for a broader audience."}, "weaknesses": {"value": "The illustrations are sparse (e.g. Figures 3c, 4a and 8a) and sufficient to convey the main idea, but they are redundant and simplistic, as a consequence a better understanding of the separation into visual and attention heads is not provided. More importantly, the distinction between useful and unhealthy sink heads is not illustrated, and details are lacking to understand this better. \n\nThe whole idea of the paper is to remove the relatively small subset of those unhealthy sink heads. A better understanding of that subset, which ones they are, why they hurt the performance; this is fundamental for a paper that builds on those principles."}, "questions": {"value": "Which subset of the attention heads is unhealthy and why is that, why does that hurt the performance of the last layers? Can you provide insights with some illustrations, and with some statistics, e.g. are they mostly on particular areas in the image, or semantic regions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KcrPBpg9SO", "forum": "xTbFeDhdRG", "replyto": "xTbFeDhdRG", "signatures": ["ICLR.cc/2026/Conference/Submission800/Reviewer_VNAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission800/Reviewer_VNAx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761149585815, "cdate": 1761149585815, "tmdate": 1762915607904, "mdate": 1762915607904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on the problem that the intermediate decoder layers outperform the final ones in MLLMs, and attribute this issue to the Visual Attention Re-sinking phenomenon. Then, a parameter-free Sink Attention Dynamic Sparsification framework is proposed. The proposed SADS achieves superior effectiveness and inference efficiency on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It deeply identifies text-only supervision as the cause of suboptimal MLLM output layers.\n2. The SADS framework effectively addresses key issues to optimize output layers.\n3. Comprehensive experiments across 20 benchmarks validate its superiority."}, "weaknesses": {"value": "1. From Figure 2 and Table 1, it can be observed that the performance degradation caused by the re-sinking phenomenon is limited. It can also be seen from the experiments that the improvement brought by the proposed method is limited.\n2. To demonstrate its effectiveness and  generalizability, more models of different sizes (7B, 13B, ...) should be tested.\n3. An ablation experiment on the proportion of sink heads should be added; additionally, is it reasonable to use a fixed proportion?"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7j6W3iJZRT", "forum": "xTbFeDhdRG", "replyto": "xTbFeDhdRG", "signatures": ["ICLR.cc/2026/Conference/Submission800/Reviewer_7PSW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission800/Reviewer_7PSW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795899699, "cdate": 1761795899699, "tmdate": 1762915607783, "mdate": 1762915607783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the root cause of the visual attention resinking phenomenon, and shows that this phenomenon is the reason why output layers of MLLM yield worse performance than middle layers, a frequently observed phenomenon in existing work. The paper finds that the visual attention resinking phenomena is caused by attention gradient sparsity, which makes the gradient distribution over attention heads sparse hence causing sink tokens to concentrate in sink heads. Based on these findings, the paper proposes a \"Sink Attention Dynamic Sparsification\" strategy, which selects vision heads per layer and forces model to focus on vision signals. Experimental results show improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper developed a systematic approach to diagnose a commonly observed but not well-understood phenomenon. The analysis is logical, insightful and convincing, revealing an overlooked aspect in MLLM training and inference."}, "weaknesses": {"value": "The proposed approach can be viewed as a patch on existing models rather than a solution to the root cause of the resinking problem. It would strengthen the paper if it also proposed ways to address the issue more fundamentally (for example, through improved training objectives)."}, "questions": {"value": "- It seems the gradient sparsity problem gets worse as the training steps increases. If this is the case, I wonder whether the MLLM performance (from the last output layer) would be better at an earlier checkpoint?\n- In Fig 5a, how is the gradient sparsity precisely defined? \n- The framework retains the top 25% of sink heads, is this ratio empirically optimized, and how sensitive is the model performance to this threshold?\n- The tested models used are up to only 7B parameters. Would the same sparsification principle hold in larger models? Due to increased capacity of larger models, it's conceivable that the resinking problem would be alleviated.\n- Would it be possible to address the gradient sparsity problem with better training objectives (e.g. adding regularization terms to upweight vision heads), which address the problem at a more fundamental level?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OmrC42l7c4", "forum": "xTbFeDhdRG", "replyto": "xTbFeDhdRG", "signatures": ["ICLR.cc/2026/Conference/Submission800/Reviewer_3kFx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission800/Reviewer_3kFx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972443789, "cdate": 1761972443789, "tmdate": 1762915607671, "mdate": 1762915607671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}