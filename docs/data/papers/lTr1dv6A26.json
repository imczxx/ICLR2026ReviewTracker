{"id": "lTr1dv6A26", "number": 14822, "cdate": 1758244365981, "mdate": 1759897347339, "content": {"title": "Low-Rank Tensor Encoding Models Decompose Natural Speech Comprehension Processes", "abstract": "How does the brain process language over time? Research suggests that natural human language is processed hierarchically across brain regions over time. However, attempts to characterize this computation have thus far been limited to tightly controlled experimental settings that capture only a coarse picture of the brain dynamics underlying human natural language comprehension. The recent emergence of LLM encoding models promises a new avenue to discover and characterize rich semantic information in the brain, yet interpretable methods for linking information in LLMs to language processing over time are limited. In this work, we develop a low-rank tensor regression method to decompose LLM encoding models into interpretable components of semantics, time, and brain region activation, and apply the method to a Magnetoencephalography (MEG) dataset in which subjects listened to narrative stories. With only a few components, we show improved performance compared to a standard ridge regression encoding model, suggesting the low-rank models provide a good inductive bias for language encoding. In addition, our method discovers a diverse spectrum of interpretable response components that are sensitive to a rich set of low-level and semantic language features, showing that our method is able to separate distinct language processing features in neural signals. After controlling for low-level audio and sentence features, we demonstrate better capture of semantic features. Through use of low-rank tensor encoding models we are able to decompose neural responses to language features, showing improved encoding performance and interpretable processing components, suggesting our method as a useful tool for uncovering language processes in naturalistic settings.", "tldr": "Low-rank tensor encoding models beat linear encoding models for MEG prediction and offer improved interpretability.", "keywords": ["MEG", "LLMs", "NeuroAI", "Encoding models"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6281b4492116b8f90157b8d726bfd6050ccfaf2e.pdf", "supplementary_material": "/attachment/e576359d22f97a6abea3ee098a5810429b5768a9.zip"}, "replies": [{"content": {"summary": {"value": "This work is part of a broader research effort to explore the relationship between internal representations of large language models and human brain activity recorded during speech comprehension. Specifically, the authors focus on building a new encoding model that uses low-rank tensor decomposition to interpret which stimulus features that truly predict human brain recordings (MEG) while participants listen to naturalistic The Moth Stories. The evaluation focuses on comparing the encoding performance of the new encoding model against the prior full-rank ridge regression model and demonstrates how the new encoding model effectively disentangles interpretable features (low-level vs. semantic) in language processing over time. The authors regress low-level control-driven MEG activity from original MEG activity, using the residual MEG low-rank tensor approach to examine whether more semantic related activity is observed over time. Overall, low-rank tensor neural encoding models provide an interpretable alternative approach for mapping language model embeddings to brain activity over time.\n\n**Contributions:**\n\n* *Introduction to low-rank tensor decomposition in an encoding model:* The study presents a low-rank tensor decomposition in neural encoding model for predicting MEG brain activity, which is conceptually novel.\n* *Comprehensive evaluation:* The study compares the encoding performance of a low-rank tensor decomposition model and a full-rank ridge regression model. For each encoding model, the authors measure the correlation between actual and predicted brain activity, and later compute normalized predictivity by dividing model correlation with max ceiling for each channel. Further, the authors  interpret the contribution of both low-level and semantic features in language processing in the brain by looking at individual low-rank components over time and across channels. \n\n**Technical summary:**\nThis is primarily an empirical study, and its methodology involves the following components:\n* *Low-rank tensor decomposition:* The authors use low-rank tensor decomposition in a neural encoding model to predict MEG brain activity. Since MEG data can be processed as  (#Words  x Channels x time lags) and the stimulus representations from LLMs as  (#Words x feature dim), the linear mapping between LLM representations and MEG results weight tensor of shape (time lags x feature dim x channels). With the introduction of low-rank decomposition, the weight tensor decompose into small number of  interpretable components (lag kernel x feature kernel x channel kernel), where the number of components R is the chosen Canonical Polyadic rank of the decomposition.\n* *Neural encoding model:* To train neural encoding model, the authors decompose weight tensor and minimise MSE between true and predict MEG brain activity with L2 ridge penalty, and ADAM as optimiser.. For a full-rank model, the authors use ridge regression with channel-specific parameters selected via 6-fold cross-validation.\n* *MEG story listening dataset:* The authors use available MEG dataset, where 3 participants listened to 27 naturalistic stories from Moth Radio Hour podcasts over 5 sessions; one story was repeated across sessions and used as held-out test data.\n\n**Experimental design/evaluation:**\n* *Noise ceiling:* The authors estimate noise ceiling (CC_max) from 5-repetitions of test story by constructing a sample time series via concatenating windows after word onset of 100 ms. To estimate max ceiling over time, authors subtract CC_max across time windows resulting in getting the excess CCmax given by that time window.\n* *Neural encoding performance:* This analysis evaluates whether the low-rank tensor-based neural encoding model shows better encoding performance over full-rank ridge approach. The primary questions are (i) how the encoding performance varies with the increasing rank, and what aspects of language process the low-rank components over time.\n* *Controlling low-level features:* This analysis provides whether regressing low-level, control-driven MEG activity from original MEG, and using the residual activity in the low-rank tensor encoding model to see whether high-level semantic emerges over time after controlling low-level features.\n\n\n**Main findings:**\nAccording to the authors’ interpretation, the main findings are as follows:\n* The low-rank tensor encoding model exhibits high-degree of brain predictivity and significantly outperforms the full-rank ridge model across 3 subjects, except at rank-1 for subjects 2 and 3. Encoding performance saturating at small ranks (R=5 to 10).\n* Not all low-rank components contributed equally, with some components having large effects while some have rare effects over time. \n* A low-rank model with 10 components for all subjects yielded spatial modes over auditory and language areas, while timecourses show peaks around 150–200ms and 300–400ms.\n* Examining both positive and negative contexts in driving the activity across the three factor matrices, the authors find that most factors are activated by low-level language features (e.g., sentence starts and ends), while one or two factors respond to semantic features.\n* After controlling low-level language features, residual activity components result in most-activating contexts becoming more semantic while 1 to 2 factors respond to low-level."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I found this work to have the following strengths:\n* *Clarity:* The manuscript is well written and well structured. The pipeline in Figure 1 is easy to follow, and the low-rank tensor decomposition mathematical formulation is well explained by factorizing the weight tensor into three factor matrices: time delay (lag), embedding dimension and channel dimension. Later, the training details of the neural encoding model, and the validation procedure are presented clearly. The results section presents the effectiveness of the low-rank tensor decomposition approach in neural encoding, and interprets the language features that drive each component  as the rank increases.\n* *Originality:* The idea of using low-rank tensor decomposition in a neural encoding model for predicting MEG brain activity, while simple, is methodologically novel in MEG encoding. Prior studies in MEG encoding typically use either mTRF (modeling temporal receptive field) approach or flatten channels and delays into a single vector and learn a ridge-regression model (similar to fMRI encoding). With the new encoding approach, it learns fewer parameters while still yielding interpretable components.\n* *Significance:* This work is significant in that it contributes to a better understanding of the parallels between language processing in language models and language processing in the human brain over time. It shows that regressing low-level features from MEG activity helps in isolating semantic components, and that low-rank tensor factorization results in learning interpretable components while mapping between model embeddings and brain activity."}, "weaknesses": {"value": "From my perspective, the primary weaknesses of this study arise from the lack of comparison with prior literature, limited evaluation, and limited samples of brain dataset:\n\n* *Lack of comparison with prior literature:* The most significant limitation of current study is the missing direct comparisons and citations of prior works. For instance, Oota et al. 2024 investigated which types of information presented in the text- and speech-based language models truly predict brain activity by regressing low-level textual, speech and visual features from the embeddings and examined the impact of brain alignment before and after removal of low-level features. Further, they showed how both text and speech language models maintain brain-relevant semantics in early sensory and language regions. In contrast, the current study regresses low-level controls from the MEG signal (similar to the residualization approach of Ramakrishna & Deniz on fMRI), but there is no comparison of different low-level features impact, and importantly missing citations.  \n\nOota et al. 2024, Speech language models lack important brain-relevant semantics, ACL-2024\n\nRamakrishna & Deniz 2021,  Noncomplementarity of information in word-embedding and brain representations in distinguishing between concrete and abstract words. CMCL-2021\n\n* *Small sample size:* Another major limitation is the small number of participants (n=3) in the Moth-Radio-Hour MEG dataset. Although there are other publicly available MEG datasets: MEG-MASC dataset (Gwilliams et al. 2023) (27 participants listening to 4 stories), and the Little Prince dataset in French (d'Ascoli et al. 2024) (58 participants listening, and 46 participants reading), the small sample scale limits the generalizability and statistical power of the findings.\n\nGwilliams et al 2023. Introducing MEG-MASC a highquality magneto-encephalography dataset for evaluating natural speech processing. Scientific Data\n\nd'Ascoli et al. 2024, Decoding individual words from non-invasive brain recordings across 723 participants \n\n* *Limited model evaluation and lack of low-level controls:*\n    * The results and learned components reported only on fixed LLaMA-2-7B layer-3. However, the prior literature on LLM-Brain alignment found that middle to later layers show improved degree of brain alignment [Antonello et al. 2023]. A strong empirical evaluation would include a layer-wise analysis with low-rank tensor factoring to see whether the findings remain held across layers or middle-late layers resulting in high-rank components capturing distinct language processing over time.\n    * The procedure for regressing low-level controls and how they perform train-test split is not fully explained in the paper. Furthermore, the low-level controls are regressed from MEG (whereas several studies perform on model features),  which weakens the choice for residual approach on the MEG side.\n* *Fixed regularisation parameter and Optimal rank:*\n    * For low-rank tensor models, the results are reported using fixed ridge penalty λ=0.1 for all channels because the channels are coupled. This weakens the comparison with the full-rank model. For example, similar to the full-rank model, selecting ridge penalty via 6-fold cross-validation would be stronger and generalizable, as fixed ridge penalty may not be a choice for other datasets.\n    * Authors fit a series of low-rank tensor encoding models increasing rank from 1 to 10 and report the performance. The proposed approach should be robust by performing rank-selection via cross-validation approach or validating on more subjects or another MEG dataset.\n\nFor a complete and detailed account of both major and minor issues, please refer to the “Questions” section."}, "questions": {"value": "I would like to thank the authors for the interesting low-rank tensor approach in MEG encoding in this work. However, there are several points that I believe require further attention/work. I have divided these into major issues, which should be prioritized, and minor ones, which should be addressed for a strong version of current work.\n\n**Major Comments:**\n* *Small sample size:* While I fully understand the complexity associated with empirical research involving human participants, the sample size in this study appears to be very limited. There are additional publicly available naturalistic story-listening MEG datasets with word annotations [Gwilliams et al. 2023]. I strongly encourage the authors to consider performing low-rank tensor decomposition on large sample sizes, and compare the findings with prior approaches. If performing a low-rank approach on all 27 subjects is not feasible, I suggest targeting a subset of the dataset and see whether similar results hold using the same ridge penalty and  low rank (1-10) models. Atleast, the authors provide a clear justification for the current dataset selection and its sufficiency for the conclusions drawn.\n* *Comparison with prior works:* Please compare with prior works by using a broader set of low-level textual features (e.g., number of letters, letters, word length+std) and speech features (e.g., MFCC, fbank, powspec, Articulation, number of phonemes, phonemes, and phonological). Both Deniz et al. 2019 and Oota et al. 2024 used the Moth-Radio-Hour podcast (the key difference is fMRI vs. MEG), so these feature sets can be easily transferable. Therefore authors can use all these low-level feature sets and compare the low-level control applied to MEG recordings (compute residuals on brain side) vs.low-level control applied to the stimulus embeddings (residuals from features side). Such a strong empirical validation is crucial for assessing the robustness and reliability of the results. \n* *Clarification on Fig 4 and Fig 5:* Both Figs report positive and negative activating contexts. Fig 4 reports without low-level control and interprets the activity in three 1-dimensional factors, whereas Fig. 5 shows after controlling for low-level features. The experimental results demonstrate that “most factors are activated by low level language features, such as end of sentence and start of sentence”. This is a weak finding and the findings are not surprising. \n     * Authors should clarify, since this is a listening dataset, how were the sentence boundaries (start/end) created? Did the authors use the same GPT-4 model to create sentence boundaries or boundaries derived through a different preprocessing pipeline?\n     * For in-depeth analysis, I recommend the following: Beyond start/end of sentences, punctuations, test part-of-speech categories (nouns, verbs, adjectives), semantic concreteness/abstractness, and other lexical attributes. \n     * Examine any semantic-specific relations (e.g., entity-property: Adjective->Noun, Adverb-> Noun) consistently occur in a particular component?\n* *Reporting variability across LLM layers:* I recommend authors to perform a layer-wise analysis (at least using middle layer) with a low-rank tensor factoring to test whether the findings remain held across layers or middle-to-late layers resulting in high-rank components capturing distinct language processing over time. If analyzing all layers is not feasible, I suggest selecting the best layer (18) based on Antonello et al. (2023) and performing the analysis there.\n\nAntonello et al. 2023, Scaling laws for language encoding models in fMRI, NeurIPS-2023\n\n* *Clarification on Topomaps in Fig 4:*  The topomaps show high-degree of activity over visual regions despite the task being story listening. Please provide a clear justification on current topomaps.\n\n**Minor Comments/Typos:**\nWhile addressing the following points may not be critical to the paper’s core contributions, doing so would enhance the overall quality. \n* Efficiency reporting: I request authors to quantify the efficiency of the low-rank tensor encoding model vs. the full-rank ridge by providing: (i) parameter counts and (ii) compute metrics. A small table would make this clear.\n* Line 107 & 161: Canonical Polydiadic -> Canonical Polyadic\n* Line 56: I would recommend authors to provide a citation for “Thus, it is difficult to determine which language features in an LLM lead to good predictions of brain activity”. For instance, Oota et al. 2024 investigated this in reading and listening fMRI across text- and speech-based language models.\n* Line 90: Authors mentioned that Variance partitioning and Interpreting weights on features are two interpretation methods explored in encoding models\t. I recommend also including the residualization approach (Toneva et al., 2022; Oota et al., 2023), as this method is used in several linguistic brain-encoding studies to control features on the model side.\n* Please include what each symbol denotes in Fig. 1 within the figure caption for better readability of the pipeline.\n* Line 155: N is not defined in that paragraph. Please state that N denotes the feature dimension. Similarly, define n in Eq. 1.\n* Line 154: FIR is not abbreviated: Finite Impulse Response (FIR)\n* Line 193: 4096-d embeddings -> 4096-D or 4096-dimensional embeddings, same in Fig 4: 1-dimensional or 1-D\n* Uniformity is missing: full rank vs. full-rank, low rank vs. low-rank\n\n**General Advice:**\nThe manuscript presents a methodological novelty in MEG encoding models and a range of experimental design choices for interpreting the features driving the activity that are interesting. However, the current version lacks clear comparison with previous approaches, small sample size and limited experimental evaluation. Adding explicit implications and addressing the above mentioned weaknesses and major comments would make the work stronger."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0UNmJFf4uk", "forum": "lTr1dv6A26", "replyto": "lTr1dv6A26", "signatures": ["ICLR.cc/2026/Conference/Submission14822/Reviewer_n1HC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14822/Reviewer_n1HC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425498627, "cdate": 1761425498627, "tmdate": 1762925172857, "mdate": 1762925172857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a low-rank tensor regression model to analyze Magnetoencephalography (MEG) data from subjects listening to naturalistic narrative stories. The core of the method is to model the high-dimensional Finite Impulse Response (FIR) filter—which maps LLM embeddings to MEG channel activity over time—as a low-rank Canonical Polydiadic (CP) tensor. This decomposition factors the model's weights into a small number of components, each comprising separate 1D vectors for time delays, embedding dimensions, and MEG channels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of building a low-rank tensor encoding model that captures low-level features is interesting. The experiments also confirm its effectiveness compared to traditional ridge regression."}, "weaknesses": {"value": "1. The experiments are not satisfying. The authors fall short in investigating low-rank tensor encoding model in the context of other cognitive signals (e.g. fmri). Besides, the study's experiment on MEG data only involves 3 patients. Given the known variability across individual brain response, N=3 is an insufficient sample size to make broad claims. Previous studies used fmri dataset with hundreds of subjects.\n\n2. There lack experiment and analysis of computational cost between traditional ridge regression and the proposed low-rank tensor encoding model (e.g. the time cost comparison).\n\n3. The analysis primarily relies on cases to prove low-level controls improve semantic features. A proper evaluation metric is needed.\n\n4. The performance comparison in Section 3.2 is potentially flawed. The full-rank ridge regression baseline used 6-fold cross-validation to select channel-specific regularization parameters ($\\lambda_c$). In contrast, the proposed low-rank model used a fixed ridge penalty of 0.1 for all channels, justified as \"near the full regression average.\" This is not a fair comparison."}, "questions": {"value": "1. In line 152-157 the authors mention \"Unlike fMRI, whose time course is slower than the usual pace of spoken words, MEG is fast enough to resolve brain activity during the processing of each word.\". I do not understand why the authors mention this, because FIR model is also used to preprocess fmri data. The expression is confusing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5hEjNOqoGx", "forum": "lTr1dv6A26", "replyto": "lTr1dv6A26", "signatures": ["ICLR.cc/2026/Conference/Submission14822/Reviewer_SBZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14822/Reviewer_SBZ9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793651887, "cdate": 1761793651887, "tmdate": 1762925172488, "mdate": 1762925172488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a low-rank tensor regression framework for MEG-based speech encoding models. The method factorizes the encoding weight tensor into interpretable components corresponding to time, language embeddings, and brain channels, enabling both improved predictive performance and interpretability compared to standard ridge regression. Applied to a naturalistic MEG dataset (participants listening to Moth Radio Hour stories), the approach demonstrates that low-rank models not only yield higher encoding accuracy but also reveal distinct temporal and spatial neural components reflecting different stages of speech comprehension (e.g., early auditory vs. later semantic processing). After regressing out low-level acoustic and structural language features, the remaining components exhibit stronger sensitivity to semantic information, suggesting that the method captures hierarchically organized linguistic processing in the brain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The **low-rank tensor formulation** is well-motivated, elegant, and appropriately implemented.\n- Empirical results convincingly show that the low-rank inductive bias improves both **predictive performance** and **interpretability**.\n- The **low-level control analysis** is a novel point, as it helps disentangle acoustic from semantic representations — an important concern in recent brain–LLM alignment studies."}, "weaknesses": {"value": "- **Quantification of interpretability:** The interpretive claims (e.g., that components correspond to distinct linguistic or cognitive processes) rely mostly on qualitative examples and visualizations. Quantitative or systematic metrics for interpretability (e.g., linguistic feature correlations) would strengthen the argument.\n- **LLM comparison :** I think that the analysis of the components and their positive/activating contexts in Figure 4 should also be done on LLM embeddings too. This would ensure that the linguistic feature distinction that we see from low-rank models is not simply due to the fact that the inputs (LLM embeddings) already contain such information.\n- **Model training inconsistency:** It is unclear how the full-rank model was trained, as opposed to the low-rank model, which the author specifies as being trained using SGD. Could the author indicate the training method for the full-rank model? If they differ, then this could confound performance comparisons and should be justified or controlled for.\n- **Reference mismatch:** The citation to *Vattikonda et al. (2025)* as an example of low-rank encoding is misleading — that paper applied LoRA on speech model as a fine-tuning mechanism, not a low-rank encoding structure per se.\n- The current study is **single-subject focused** in analysis; cross-subject generalization or shared subspace modeling would add more weight to the neuroscientific claims."}, "questions": {"value": "1. When performing PCA on LLM embeddings, was this transformation fit on the entire moth radio corpus (including test data)? If so, this could introduce **train-test leakage**.\n2. Could the authors clarify how **excess CCmax** is defined mathematically? Is it simply the difference between windowed and global CCmax values? If so, please clarify the use of “excess” as the name. Also, since the time window CCmax is computed using a smaller number of timepoints (less samples), I believe it is important to also calculate their confidence intervals.\n3. Have the authors considered quantifying the linguistic selectivity of each factor (e.g., correlation with word frequency, surprisal, or syntactic depth)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EFYxsobiLQ", "forum": "lTr1dv6A26", "replyto": "lTr1dv6A26", "signatures": ["ICLR.cc/2026/Conference/Submission14822/Reviewer_67ce"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14822/Reviewer_67ce"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981995549, "cdate": 1761981995549, "tmdate": 1762925172078, "mdate": 1762925172078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a low-rank tensor regression method for building encoding models of MEG brain activity during language comprehension. Core idea is to model the linear filter, which maps LLM embeddings to neural signals, as a 3D tensor with dimensions for time, embedding features, and MEG channels.The authors constrain this tensor using a low-rank canonical polyadic decomposition. This approach models the filter as a sum of rank-1 components, where each component is the outer product of a temporal profile, a spatial map across channels, and a weight vector over the embedding features.\n\nThe authors apply this method to an MEG dataset of subjects listening to narrative stories and report two main contributions. First, they show that the low-rank model achieves higher prediction accuracy than a standard full-rank ridge regression model, suggesting that the low-rank structure is a useful inductive bias. Second, they demonstrate the model's interpretability by analyzing the individual components. They find that the model separates distinct language processing features, with many components capturing low-level features like sentence onsets or punctuation, while others capture more semantic content. To isolate these semantic components, they fit the model to the residual signal after regressing out low-level audio and word-onset features, showing that the resulting components are more clearly semantic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary contribution is the application of low-rank tensor decomposition to model the MEG encoding filter. This decomposition of the filter into separate, interpretable factors for time, channel, and embedding features is elegant and provides a path to understanding the model's components.\n\nThe authors demonstrate that this model outperforms the standard full-rank ridge regression baseline in prediction accuracy, and that it offers a way to disentangle signals present in MEG. The control analysis, where low level audio and linguistic features are regressed out, is interesting. The analysis seems to support the claim that the method can isolate higher order semantic processing, a key goal in the field."}, "weaknesses": {"value": "The choice of a canonical polyadic tensor decomposition is a very strong inductive bias. This model forces each component to be a strict outer product of one time factor, one channel factor, and one embedding factor, assuming a simple one to one mapping that may not be neurally plausible. A justification for this choice over a more flexible tucker decomposition, which can model interactions between factors via a core tensor, is missing.  This issue is amplified by an inconsistent comparison to the baseline. The full rank model benefits from a careful per channel search for its regularization penalty, while the proposed low rank model uses a single fixed penalty. This methodological difference confounds the central claim that the low rank structure itself is the source of the performance gain.\n\nThe approach to isolating semantic features is also a bit problematic. The authors use a two stage process, first fitting a model to low level controls and then fitting their tensor model to the residuals. This stacked regression method is sensitive to the order of operations and can misattribute any shared variance between the control features and the LLM embeddings. Finally, the interpretation of the resulting semantic components is based on an anecdotal inspection of the most activating text snippets. This might be insufficient for characterizing a high dimensional embedding factor."}, "questions": {"value": "1. Can you elaborate on the choice of the canonical polyadic decomposition? This model imposes a strict multiplicative structure on the time, channel, and embedding factors. Did you explore a more flexible tucker decomposition, which might better capture interactions via a core tensor, and how did it compare in performance and interpretability?\n\n2. Could you clarify the rationale for using a fixed ridge penalty for the low-rank model while the full-rank baseline used a per-channel cross-validated penalty? How can we be sure the reported performance gain is due to the low-rank inductive bias itself and not this difference in regularization strategy?\n\n3. The paper uses a two-stage process to control for low-level features by fitting the model to the residuals. This stacked regression can misattribute variance that is shared between the controls and the LLM embeddings. Have you compared this to a joint model that fits all features simultaneously, and do the semantic components that emerge differ?\n\n4. The interpretation of the embedding factors relies on inspecting a few top-activating text snippets. Have you considered quantitatively correlating the learned embedding factor vectors with established linguistic probes to confirm they are capturing specific semantic or syntactic properties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iS4kI7IrLt", "forum": "lTr1dv6A26", "replyto": "lTr1dv6A26", "signatures": ["ICLR.cc/2026/Conference/Submission14822/Reviewer_G5Sg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14822/Reviewer_G5Sg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244828008, "cdate": 1762244828008, "tmdate": 1762925171321, "mdate": 1762925171321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}