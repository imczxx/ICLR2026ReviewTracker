{"id": "YNqZqw4fLT", "number": 17118, "cdate": 1758272424214, "mdate": 1759897195786, "content": {"title": "SliderQuant: Accurate Post-Training Quantization for LLMs", "abstract": "In this paper, we address post-training quantization (PTQ) for large language models (LLMs) from an overlooked perspective: given a pre-trained LLM, the predominant sequential quantization framework treats different layers equally, but this may be not optimal in challenging bit-width settings. We empirically study the quantization impact of different layers on model accuracy, and observe that: (1) shallow/deep layers are usually more sensitive to quantization than intermediate layers; (2) among shallow/deep layers, the most sensitive one is the first/last layer, which exhibits significantly larger quantization error than others. These empirical observations imply that the quantization design for different layers of LLMs is required on multiple levels instead of a single level shared to all layers. Motivated by this, we propose a new PTQ framework termed **Sliding**-lay**er** **Quant**ization (SliderQuant) that relies on a simple adaptive sliding quantization concept facilitated by few learnable parameters. The base component of SliderQuant is called inter-layer sliding quantization, which incorporates three types of sliding window designs tailored for addressing the varying layer sensitivity to quantization. The other component is called intra-layer sliding quantization that leverages an incremental strategy to quantize each window. As a result, SliderQuant has a strong ability to reduce quantization errors across layers. Extensive experiments on various language generation and reasoning tasks with different LLMs show that our method outperforms previous works for both weight-only quantization and weight-activation quantization. Code will be made publicly available.", "tldr": "This paper presents SliderQuant, a new post-training quantization framework for LLMs, which is superior to existing methods.", "keywords": ["Large language models", "post-training quantization", "low-bit neural networks", "model compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1410472309043417beb1fef3798f8a5a68f8e784.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new PTQ framework that accounts for varying layer sensitivities in large language models. The authors observe that the first and last layers are far more quantization-sensitive than intermediate ones and propose SliderQuant, which adaptively applies inter-layer and intra-layer sliding quantization on weights or weights+activations to mitigate these effects. This design tailors window sizes and quantization granularity across layers, effectively reducing accumulated quantization errors. Experiments on several models and benchmarks show its superior performance at low bitwidths."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and well-organized, with intuitive figures and tables that make the methodology and results easy to follow.\n- The experimental evaluation is comprehensive, covering a wide range of model families including Llama, Qwen, and MoE architectures. The authors conduct extensive tests not only on standard language generation and reasoning benchmarks but also on more challenging tasks like mathematical reasoning, providing rich empirical evidence for the method’s effectiveness. The ablation studies are also sufficient for understanding different factors that could affect the performance."}, "weaknesses": {"value": "- The empirical observations and methodological novelty are limited. Specifically:\n  - The finding that different layers of LLMs exhibit varying sensitivity to quantization—particularly that the first and last layers are most sensitive—has already been clearly identified in several prior works (e.g. [[1]](https://arxiv.org/abs/2412.03599?utm_source=chatgpt.com), [[2]](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F5c805adc-b555-499f-9882-5ca35ce674b5.pdf)), so this observation might not be suitable to be regarded as a novel contribution.\n  - The proposed method, while effective and technically sound, is largely incremental. Its design combines previously explored ideas, such as sliding-window quantization (e.g. [[3]](https://arxiv.org/abs/2405.06219), [[4]](https://arxiv.org/abs/2312.07950)) and layer-wise sensitivity-aware quantization (e.g. [[1]](https://arxiv.org/abs/2412.03599?utm_source=chatgpt.com), [[2]](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F5c805adc-b555-499f-9882-5ca35ce674b5.pdf))), and the concept of quantization synergy across successive layers somewhat resembles extensions of the GPTQ framework's concepts.\n- Some experimental results have limited practical significance. For instance, in Table 2, all methods (including SliderQuant) show more than 10% accuracy degradation under W4A4 quantization on almost all the benchmarks, and similar issues appear in Table 5 under W2A16. Although SliderQuant achieves numerically better results than baselines, such large performance drops render these configurations impractical, raising doubts about the meaningfulness of these ultra-low-bit experiments."}, "questions": {"value": "- Could the authors provide more theoretical or intuitive explanations for the observations that the first / last layers are much more sensitive to quantization, beyond the empirical observations? I believe these could provide insights to future studies and add more novelty to this paper.\n- While the bitwidth is set to be the same for all layers in SliderQuant, mixed-precision quantization is also a popular way for LLM quantization that could utilize the characteristics that different layers have different sensitivity to quantization  (e.g. [[2]](https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F5c805adc-b555-499f-9882-5ca35ce674b5.pdf)). Could the authors provide some comparison with such kind of methods, to show whether the sliding-layer mechanism could complement or outperform mixed-precision strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h6Z5kdlV3N", "forum": "YNqZqw4fLT", "replyto": "YNqZqw4fLT", "signatures": ["ICLR.cc/2026/Conference/Submission17118/Reviewer_BEoY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17118/Reviewer_BEoY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761191004042, "cdate": 1761191004042, "tmdate": 1762927117806, "mdate": 1762927117806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discovers the distinct quantization sensitivity among model layers, and proposes SliderQuant, a novel learnable quantization framework which contains two level sliding quantization concept. Experiments indicate that this method becomes SOTA sliding-based PTQ method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors identify varying sensitivities of different layers to quantization and improve the quantization performance of layers with different sensitivities through a sliding-window design, rather than directly adopting a mixed-precision approach. This provides a novel and interesting perspective.\n2. The writing is clear and well-structured, the experiments are thorough, and the figures and tables are elegantly designed."}, "weaknesses": {"value": "1. The description of intra-layer sliding quantization is the main weakness of the paper. As one of the core innovations, its explanation is too brief, which makes it confusing. Does it mean that the weights/activation matrices are also partitioned and quantized sequentially within each layer?\n2. I'm afraid that whether the effectiveness of both learnable low-rank matrices A and B will be influenced after quantization because they have been integrated into weights before quantization during inference.\n3. The authors ignores to describe the training details, such as loss function, supervision information. Is it followed by OmniQuant?\n4. The authors ignores to provide the memory usage and runtime of SliderQuant. As described, SliderQuant requires loading multiple blocks into the GPU simultaneously, which can lead to substantial memory overhead. Based on prior experience, training a 7B LLM with OmniQuant takes approximately 0.8 hours and 15 GB of memory on an A100 GPU, while SliderQuant is likely to demand even more."}, "questions": {"value": "1. More detailed description about intra-layer sliding quantization is needed (W1).\n2. More training details should be provided (W3).\n3. Although the appendix mentions that the training can be completed on a single A6000 GPU, providing explicit comparative data (e.g., against OmniQuant, CBQ, and the more efficient GPTQ) would make the analysis more intuitive. If the training cost is excessively high, the practicality of SliderQuant may be questioned.(W4).\n\nI promise to increase my score if all my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KrEBh5wWfm", "forum": "YNqZqw4fLT", "replyto": "YNqZqw4fLT", "signatures": ["ICLR.cc/2026/Conference/Submission17118/Reviewer_DdkN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17118/Reviewer_DdkN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742516380, "cdate": 1761742516380, "tmdate": 1762927117529, "mdate": 1762927117529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SliderQuant, a quantization framework. It introduces an adaptive sliding-layer strategy that assigns progressively expanded, fixed, and contracted windows to shallow, middle, and deep layers, respectively, addressing the unequal quantization sensitivity across layers—higher at the top and bottom. The method also includes intra-layer sliding quantization to further improve performance. Experiments show that SliderQuant achieves lower perplexity and higher reasoning accuracy than existing PTQ baselines, especially under challenging low-bit configurations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and strong motivation: The paper is motivated by an empirically grounded observation on layer-wise sensitivity to quantization in LLMs. The motivation is clearly presented and addresses an overlooked aspect in post-training quantization.\n\n2. Comprehensive experiments: The evaluation covers multiple model families and various bit-width settings, demonstrating the generality of the proposed framework.\n\n3. Intuitive and well-written method: The proposed sliding-layer quantization framework is easy to follow and clearly described, with both figures and ablation studies supporting the core design choices. The overall paper is well written and organized."}, "weaknesses": {"value": "1. Uneven optimization frequency of middle layers: According to Figure 1 and the default hyperparameter setting, the 4th and 5th layers appear to be quantized only once. This means that some middle layers receive fewer optimization passes than their neighbors. Could this uneven optimization frequency introduce instability or suboptimal performance? In particular, when the middle-layer window size is larger than two, how do you ensure that all middle layers are optimized an equal number of times?\n\n2. Training cost comparison with baselines is unclear: Although Table 7 usefully extends the baseline window size from (s=2) to (s=4), the training (quantization) cost of SliderQuant versus the baselines remains unclear. Please report quantitative efficiency metrics such as GPU hours. Ideally, a main table could compare performance under equal training budgets to ensure fairness.\n\n3. Hyperparameter generalization is under-discussed: The paper introduces several hyperparameters, but the discussion of their sensitivity is limited to a single model (LLaMA-2-7B). It remains unclear how these settings generalize across models of different sizes and depths. Please analyze or at least discuss how to scale or tune these parameters for larger models."}, "questions": {"value": "1. Missing baseline results in Table 1: Some baselines in Table 1 have missing or abnormal values. Could the authors clarify the cause?\n\n2. Source of improvement: sliding vs. repeated optimization: The proposed method combines a sliding-window design and more times of optimization of the first and last layers. Which factor contributes more to the final improvement? An additional ablation that isolates “adaptive sliding” from “frequency of re-quantization” would help clarify the main source of gain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RUvC5E3R72", "forum": "YNqZqw4fLT", "replyto": "YNqZqw4fLT", "signatures": ["ICLR.cc/2026/Conference/Submission17118/Reviewer_R2bM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17118/Reviewer_R2bM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750895895, "cdate": 1761750895895, "tmdate": 1762927117199, "mdate": 1762927117199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper improves post-training quantization by making the sliding-window reconstruction approach depth-aware. Instead of a fixed window, they expand the window in shallow layers, keep it fixed in the middle, and contract it near the end so that the sensitive early/late layers are quantized with enough context. This simple schedule combined with incremental quantization within each window, outperforms common PTQ baselines (SmoothQuant/OmniQuant/CBQ) on several models(e.g. LLaMA/Qwen/MoE) especially at 4-bit and 2-bit. The main value is practical and better results without changing inference while at the same time the limitations are extra calibration cost and only incremental novelty over existing depth/adaptive PTQ ideas."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Depth-aware sliding window actually makes early and late layers easier to quantize, instead of treating all layer depths the same. \n- Inter-layer and intra-layer sliding reinforce each other, so you get denser cross-layer synergy as compared to a fixed window. \n- On MoE (Table 4) it improves over OmniQuant at every bit setting, which helps generalizable, not tuned for one model claim. \n- Generation Table 5 is especially strong, 2-bit OmniQuant nearly collapses on DeepSeek-R1 distilled models, while 2-bit SliderQuant keeps usable pass@1 on math and code. That’s very useful in order to have a useful model in the end. \n- The method stays compatible with standard PTQ methods(scaling, re-quantizing overlaps), so it is easy to slot into existing pipelines."}, "weaknesses": {"value": "- The method adds several schedule knobs (expand depth, contract depth, window size, γ), and robustness to non-ideal choices is not fully presented in the paper. \n- Comparisons are mostly against fixed-window, non-rotated post training quantization techniques. It’s unclear how much of the gains remain vs the strongest rotation/equivalent methods."}, "questions": {"value": "- Results against at least one rotation-based PTQ(e.g. SpinQuant[1]) to contextualize the gains would be very useful.\n- Is there a way to automatically estimate the window schedule from measures e.g. per-layer sensitivity.\n- What is the calibration-time required and memory overhead compared to the simplest fixed-window baseline?\n- Do the improvements still hold with a smaller calibration set or if the calibration set is slightly domain-mismatched?\n\nI'd be happy to increase my score if these questions are answered!\n\n[1] Liu, Z., Zhao, C., Fedorov, I., Soran, B., Choudhary, D., Krishnamoorthi, R., Chandra, V., Yuandong, T., & Blankevoort, T. (2025). SpinQuant: LLM quantization with learned rotations. arXiv:2405.16406."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EbLHWylMoK", "forum": "YNqZqw4fLT", "replyto": "YNqZqw4fLT", "signatures": ["ICLR.cc/2026/Conference/Submission17118/Reviewer_ZBQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17118/Reviewer_ZBQd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968715414, "cdate": 1761968715414, "tmdate": 1762927116867, "mdate": 1762927116867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}