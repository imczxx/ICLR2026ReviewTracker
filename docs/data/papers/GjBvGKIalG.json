{"id": "GjBvGKIalG", "number": 24660, "cdate": 1758359076668, "mdate": 1759896756350, "content": {"title": "EAGLE: Efficient Analytical Gradient LinearEvaluation for Enhanced Recomputation in Large Language Models", "abstract": "Training large language models requires substantial memory to store intermediate activations, often exceeding the capacity of modern accelerators. Gradient checkpointing addresses this challenge by trading computation for memory, but introduces significant overhead due to redundant forward passes during recomputation. In this work, we present EAGLE (Efficient Analytical Gradient Linear Evaluation), a novel recomputation strategy that leverages closed-form gradient expressions to eliminate redundant forward computations in linear transformations. Unlike traditional gradient checkpointing that uniformly applies autograd-based recomputation, EAGLE selectively computes gradients analytically for linear layers while maintaining standard recomputation for nonlinear operations. We demonstrate the effectiveness of EAGLE on large-scale models including DeepSeek-236B, DeepSeek-671B, and Llama3-70B, achieving throughput improvements of 1.8\\% to 12.5\\% compared to full recomputation across models ranging from 70B to 671B parameters. For individual modules, EAGLE delivers substantial recomputation speedups: 6.31× for RMSNorm+Linear, 25\\% FLOP reduction for MLP, and up to 2.19× for attention mechanisms, while maintaining identical memory usage and gradient accuracy. The approach offers a triple advantage—memory efficiency matching selective recomputation, computational efficiency approaching no-recomputation baseline, and superior execution time—making it an optimal solution for memory-constrained large-scale model training.", "tldr": "EAGLE is a novel recomputation strategy that analytically computes gradients for linear layers, reducing redundant computation and accelerating large-scale language model training.", "keywords": ["Large Language Models; Gradient Checkpointing; Recomputation; Analytical Gradients"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abb70505b03a5c5c6bb63c2f9e4fa6480b648216.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents EAGLE, a recomputation strategy that reduces the overhead of gradient checkpointing in large language model training. It computes analytical gradients for linear layers and integrates with FlashAttention’s backward pass to avoid redundant forward computations. Experiments on models such as LLaMA3-70B and DeepSeek-V3 show up to 9.7× module-level speedup and 18–33% improvement in model FLOPs utilization while keeping memory usage unchanged."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a recomputation method that uses analytical gradients for linear layers and integrates with FlashAttention’s backward pass, reducing redundant computation and improving efficiency while keeping memory usage constant."}, "weaknesses": {"value": "The proposed approach offers only a small conceptual improvement over existing gradient checkpointing, mainly replacing autograd recomputation with analytical formulas.\n\nThe experimental evaluation is too limited, covering few configurations and lacking analysis of convergence, training stability, or scalability.\n\nComparisons with recent optimized methods such as CheckMate and Adacc are missing, so the claimed efficiency gains are not well contextualized.\n\nThe paper focuses heavily on large-model benchmarks without deeper investigation into when and why the method performs best."}, "questions": {"value": "The experiments seem limited in scope. Can the authors provide more details on how many runs were conducted and whether the reported improvements are statistically consistent?\n\nHow does the proposed method affect convergence behavior or final model quality compared to standard checkpointing?\n\nWhy were methods such as CheckMate or Adacc not included in the comparison? Would the claimed gains still hold under those baselines?\n\nThe paper focuses heavily on large-model benchmarks without deeper investigation into when and why the method performs best？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lGaH8QcZS3", "forum": "GjBvGKIalG", "replyto": "GjBvGKIalG", "signatures": ["ICLR.cc/2026/Conference/Submission24660/Reviewer_imqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24660/Reviewer_imqf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760479436834, "cdate": 1760479436834, "tmdate": 1762943151763, "mdate": 1762943151763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EAGLE, a recomputation strategy for training Transformers that replaces autograd-based forward recomputation in linear layers with analytical gradients, and invokes FlashAttention’s backward to avoid the attention forward during checkpointed backprop. For a linear layer $y=Wx$, the method uses $\\\\nabla_W=\\\\nabla_y x^\\\\top$ and $\\\\nabla_x=W^\\\\top\\\\nabla_y$ inside the recompute region, skipping one matmul per region.  EAGLE reports FLOP reductions such as RMSNorm+Linear $8bshh_1 \\\\to 6bshh_1$ and MLP $24bshh_1 \\\\to 22bshh_1$, plus attention speedups via direct FlashAttention backward.   Empirically, module-level speedups range from $1.46\\\\times$ to $9.75\\\\times$, and end-to-end MFU gains are $18.18%$ to $33.33%$ with fixed $R$ and $7.69%$ to $20.00%$ with optimized $R$, on LLaMA3-70B, DeepSeek-V2, and DeepSeek-V3 up to 694B parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear derivation and insertion point of analytical gradients within a recompute block. \n* Integration with FlashAttention backward to remove attention forward recomputation while preserving memory. \n* Explicit FLOP accounting with parameter dependence and consistent module-level speedups.  \n* End-to-end MFU gains across diverse architectures and parallelization regimes."}, "weaknesses": {"value": "* No numerical gradient-check or stability analysis to support \"identical gradient accuracy\".\n* Missing system throughput metrics such as tokens per second; only MFU and duration are reported.\n* Limited sensitivity analysis beyond fixed vs optimized $R$; no study of sequence length or batch size effects in microbenchmarks.\n* Scope of end-to-end evaluation excludes mid-scale models; all results are 70B to 694B."}, "questions": {"value": "* On one configuration per model, report tokens per second (tokens/s) and peak memory alongside MFU and iteration duration, and specify the hardware (e.g., A100 80GB) and precision.\n* Provide a short sensitivity sweep over sequence length, for example $s\\\\in\\\\{4\\\\mathrm{k},16\\\\mathrm{k},32\\\\mathrm{k}\\\\}$, showing MFU, iteration duration, and speedup. Use existing codepaths; no retraining needed.\n* Could you run a minimal gradient check comparing EAGLE to standard Autograd? On a toy 2-layer MLP and a single self-attention block, run one forward-backward step with identical weights and RNG, dropout off, deterministic kernels, under both BF16 and FP32. Let $g_T^{\\\\text{EAGLE}}=\\\\partial\\\\mathcal{L}/\\\\partial T$ and $g_T^{\\\\text{AutoFP32}}=\\\\partial\\\\mathcal{L}/\\\\partial T$ computed by Autograd FP32. For each parameter tensor $T$ (flattened, with $n_T$ elements), please report:\n\n  * (i) $\\\\|g_T^{\\\\text{EAGLE}}-g_T^{\\\\text{AutoFP32}}\\\\|_{\\\\infty}$,\n  * (ii) $\\\\tfrac{1}{n_T}\\\\sum_i \\\\bigl|g_{T,i}^{\\\\text{EAGLE}}-g_{T,i}^{\\\\text{AutoFP32}}\\\\bigr|$,\n  * (iii) $\\\\tfrac{1}{n_T}\\\\sum_i \\\\dfrac{\\\\bigl|g_{T,i}^{\\\\text{EAGLE}}-g_{T,i}^{\\\\text{AutoFP32}}\\\\bigr|}{\\\\max\\\\bigl(|g_{T,i}^{\\\\text{AutoFP32}}|,\\\\epsilon\\\\bigr)}$ with a fixed $\\\\epsilon$ (e.g., $10^{-10}$). A single step with batch 1 and sequence length around 1024 is sufficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u48KyXWi7w", "forum": "GjBvGKIalG", "replyto": "GjBvGKIalG", "signatures": ["ICLR.cc/2026/Conference/Submission24660/Reviewer_oBUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24660/Reviewer_oBUC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979275073, "cdate": 1761979275073, "tmdate": 1762943151170, "mdate": 1762943151170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "See below"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "See below"}, "weaknesses": {"value": "See below"}, "questions": {"value": "The paper introduced a new gradient checkpointing-type algorithm called EAGLE. EAGLE eliminates redundant forward passes by computing gradients directly from cached inputs for linear operations. This method achieves speedup over the existing gradient-checkpointing methods without additional memory consumption.  \n\nThe paper is well-written, and the proposed method is clearly presented. According to the literature review in the paper, the proposed idea has not been applied in the existing literature.  The proposed method will be useful for compute-constrained LLM developers. \n\nTo me, this paper starts from a very simple insight and designs a better gradient-accumulation approach. Mathematically, the insight itself is relatively straightforward, but from an engineering perspective, I believe implementing this method and achieving performance improvements requires significant effort. I appreciate the authors' hard work on the execution. \nIn summary, I am leaning towards acceptance.\n\nI  have several presentation suggestions: \n\n1. In Figure 1 and in the summary of the main contribution in Section 1 (~line 100). It would be better to explicitly specify \"achieving recomputation speedups\" over which baseline method.\n\n2. In Section 1 or maybe in Figure 1, it would be better to explicitly show the memory comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pnrsgjgLRm", "forum": "GjBvGKIalG", "replyto": "GjBvGKIalG", "signatures": ["ICLR.cc/2026/Conference/Submission24660/Reviewer_5v8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24660/Reviewer_5v8h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084727505, "cdate": 1762084727505, "tmdate": 1762943150589, "mdate": 1762943150589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}