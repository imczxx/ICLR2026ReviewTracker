{"id": "aUSOQWt9oh", "number": 18355, "cdate": 1758286782380, "mdate": 1763078463581, "content": {"title": "Continual Learning by Reuse, New, Adapt and Skip: A Hierarchical Exploration-Exploitation Approach", "abstract": "To effectively manage the complexities of real-world dynamic environments, continual learning must incrementally acquire, update, and accumulate knowledge from a stream of tasks—without suffering from catastrophic forgetting of prior knowledge. While this capability is innate to human cognition, it remains a significant challenge for modern deep learning systems.\nAt the heart of this challenge lies *the stability-plasticity dilemma*: the need to balance leveraging prior knowledge, integrating novel information, and allocating model capacity adaptively based on task complexity. In this paper, we propose a novel exemplar-free class-incremental continual learning (ExfCCL) framework that addresses these issues through a Hierarchical Exploration-Exploitation (HEE) approach. Our method centers on two key subsystems: (i) a HEE-guided neural architecture search (HEE-NAS) that enables a learning-to-adapt backbone via four primitive operations—reuse, new, adapt, and skip—thereby serving as an internal memory that dynamically updates selected components across tasks; and (ii) a task ID inference mechanism, which utilizes an external memory of task centroids to select the appropriate task-specific backbone and classifier during testing. We term our overall framework **CHEEM** (Continual Hierarchical-Exploration-Exploitation Memory). CHEEM is evaluated on the challenging MTIL and Visual Domain Decathlon (VDD) benchmarks using both Tiny and Base Vision Transformers. It significantly outperforms state-of-the-art prompting-based continual learning methods, closely approaching full fine-tuning upper bounds. Furthermore, it learns adaptive model structures tailored to individual tasks in a semantically meaningful way, demonstrating its effectiveness in exemplar-free continual learning scenarios.", "tldr": "", "keywords": ["Continual Learning", "Adaptive Models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/132eaac2104948d485546d5f659035d1c6b178f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the Exemplar-free Class-incremental Continual Learning (ExfCCL) by identifying the critical \"local vs. global argmax\" issue as a primary failure mode for existing methods. This paper proposes CHEEM, a novel framework that decouples task-ID inference (via an external memory) from model adaptation (via a dynamic Hierarchical Exploration-Exploitation NAS). The method is innovative and achieves SOTA results on two challenging benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper compellingly identifies the \"local vs. global argmax\" discrepancy as a primary failure mode for existing ExfCCL methods. Even a similar viewpoint has been discussed in previous works [1], the clear diagnosis from theoretical analysis is still interesting.\n+ The HEE-NAS mechanism intelligently combines Reuse, New, Adapt (LoRA), and Skip to balance the stability-plasticity dilemma and improves the model efficiency during inference. The similarity-guided HEE sampling strategy (vs. uniform) is well-supported by ablation studies, demonstrating significant parameter savings.\n+ The paper demonstrates state-of-the-art performance on the challenging MTIL and VDD benchmarks. The key ablations and the qualitative analysis of learned, efficient architectures (via the Skip operation) effectively support the method's design choices.\n- - - \n**Reference**   \n[1] A theoretical study on solving continual learning. In NeurIPS, 2022."}, "weaknesses": {"value": "+ The paper focuses on inference FLOPs (which are low due to the Skip operation) but completely omits any discussion of the training cost. NAS methods, particularly those involving supernet training (Sec 2.3) and evolutionary search (Sec 2.4), are notoriously expensive. The claim of \"efficiency\" is misleading without this context. The authors must report training complexity (e.g., GPU-hours per task, relative cost vs. baselines) for a fair assessment of the method's practicality.\n+ Concerns on the External Memory ($F_1$ Reliance): The entire framework hinges on an external memory built using a single, frozen $F_1$ model. This design appears brittle and its robustness is unverified. It assumes $F_1$ (e.g., trained on ImageNet) can provide a separable feature space for all future tasks, even semantically distant ones. The paper provides no analysis of this component's robustness or its accuracy (e.g., task-ID inference accuracy is not reported), which is a critical point of failure.\n+ Section 2.5 states that all New and Adapt modules are randomly re-initialized after the architecture search, discarding the weights learned during the HEE-NAS (Sec 2.3). This appears to contradict the entire purpose of the sophisticated, similarity-guided HEE sampling. If the weights are ultimately discarded, what is the benefit of this complex, guided-sampling-based weight-training process over a simpler search? A critical ablation comparing (A) inheriting supernet weights vs. (B) retraining from scratch is missing.\n+ The paper positions itself as a dynamic architecture method. However, it fails to survey or compare against a highly relevant and concurrent line of work: dynamic adapter methods. Techniques like InfLoRA [2] or SDLoRA [3] also dynamically allocate parameters (e.g., LoRA modules) based on task needs, presenting a different and potentially more efficient approach to the same problem. This omission leaves a significant gap in the related work and experimental comparison.\n+ The paper's core innovation is the 4-op NAS. However, the ablations are insufficient to isolate each component's contribution. The key comparison (Table 1) is CHEEM (all 4 ops) vs. LORA (1 op). This does not quantify the specific impact of the Skip operation. A CHEEM-without-Skip ablation is necessary to measure the precise trade-off between Skip, accuracy, and inference FLOPs.\n- - -\n**Reference**   \n[2] InfLoRA: Interference-free Low-Rank Adaptation for Continual Learning. In CVPR, 2024.    \n[3] SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning. In ICLR, 2025."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xjekI7MjLQ", "forum": "aUSOQWt9oh", "replyto": "aUSOQWt9oh", "signatures": ["ICLR.cc/2026/Conference/Submission18355/Reviewer_QXts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18355/Reviewer_QXts"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529368697, "cdate": 1761529368697, "tmdate": 1762928066140, "mdate": 1762928066140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "JuZdFkQAKL", "forum": "aUSOQWt9oh", "replyto": "aUSOQWt9oh", "signatures": ["ICLR.cc/2026/Conference/Submission18355/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18355/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763078460706, "cdate": 1763078460706, "tmdate": 1763078460706, "mdate": 1763078460706, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a continual learning approach (CHEEM) that uses a hierarchical exploration-exploitation neural architecture search to adapt the architecture using 4 operations (reuse, new, adapt, and skip) and a clustering based task ID inference mechanism. CHEEM is evaluated on MTIL and VDD demonstrating strong performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CHEEM is exemplar-free which helps reduce memory usage and preserve privacy.\n2. CHEEM learns adaptive model structures tailored to individual tasks in a semantically meaningful way, which is shown in Fig 2."}, "weaknesses": {"value": "1. A number of recent architectural baselines are missing, e.g., MoE-adapters, prompt-pool.\n2. Total number of parameters are not included in experiments (Tables 1 and 3).\n3. In most cases, CHEEM only shows modest improvement over LORA, which does not justify the additional steps in CHEEM."}, "questions": {"value": "1. What is the effect of task order on the architecture and performance?\n2. How does task relatedness impact the architecture?\n3. What is novel about the clustering based a task ID inference mechanism (external task-centroid memory)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iXG13Y5BWL", "forum": "aUSOQWt9oh", "replyto": "aUSOQWt9oh", "signatures": ["ICLR.cc/2026/Conference/Submission18355/Reviewer_wAQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18355/Reviewer_wAQH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799541743, "cdate": 1761799541743, "tmdate": 1762928065490, "mdate": 1762928065490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of adapting pretrained vision transformers to the exemplar-free class-incremental learning (ExfCCL) setting. Specifically, it proposes CHEEM, which uses neural architecture search (NAS) to determine, for each task, whether to (a) reuse existing layers, (b) adapt existing layers with task-specific LoRA adapters, (c) add a new layer, or (d) skip a layer. Additionally, it keeps track of task centroids to select task-specific classifiers. The authors empirically demonstrate that the proposed method outperforms baselines on two ExfCCL benchmarks, MTIL and VDD, which consist of images from different domains, both in terms of overall accuracy and FLOPs.\n\nOverall, the paper is technically sound, and the proposed method outperforms baselines in both accuracy and the FLOPs of the NAS-searched final model. My main concerns are related to the motivation of the study and the need for additional analysis on training-time complexity and contributions of each component of the method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is intuitive and empirically shown to outperform existing approaches on challenging benchmarks.\n- The description of the NAS procedure is clear; however, given the multi-stage nature of the algorithm, pseudocode would be helpful for clarity."}, "weaknesses": {"value": "1. The introduction contains too much detail about the method, making it difficult to identify the core issue in existing methods that this paper addresses. It is unclear under what circumstances one would prefer this relatively complex method over simpler alternatives such as LoRA adapters combined with a task predictor.\n2. The task ID inference mechanism---framed as a main contribution---is never explicitly described. Could the authors clarify? Additionally, while the authors note issues with task-agnostic heads, it is unclear why the proposed method performs better than existing approaches that use task predictors or task centroids. Which of the drawbacks listed in Appendix G.4 does the proposed method specifically address?\n3. The NAS procedure seems computationally expensive during training, but only the FLOPs of the final model are reported in comparisons.\n4. An ablation study on the contribution of each of the four operations would be helpful. For instance, it would be interesting to see the effect of adding \"New\" on top of \"Adapt.\""}, "questions": {"value": "1. What is the difference between \"LoRA finetuning\" and \"LoRA\"?\n2. Where is the proposed metric, FoM, used?\n3. L52: \"no restrictive assumptions\": Do the tasks need to have disjoint classes? If not, how does class overlap affect task inference or operation sampling?\n4. L468: What exactly are the \"substantially significant differences\" between the proposed method and L2G, and why is L2G not included in the comparisons?\n\n### Questions/comments that did not impact the score\n5. L102: difficult >> difficulty\n6.  L104: loca >> local\n7. Table 6: VDD-MLP's accuracy is bolded but it is lower than VDD-Attn's."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R9GJunqIQ4", "forum": "aUSOQWt9oh", "replyto": "aUSOQWt9oh", "signatures": ["ICLR.cc/2026/Conference/Submission18355/Reviewer_v8tT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18355/Reviewer_v8tT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924876817, "cdate": 1761924876817, "tmdate": 1762928063401, "mdate": 1762928063401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework for exemplar-free continual learning (ExfCCL) referred to as CHEEM, designed to prevent catastrophic forgetting. It tackles the stability-plasticity dilemma using a Hierarchical Exploration-Exploitation (HEE) approach. A core component, HEE-NAS, acts as an \"internal memory\" that dynamically adapts the model's architecture by reusing, creating, or adapting components for new tasks. An \"external memory\" of task centroids is used to infer the correct task ID during testing. CHEEM significantly outperforms state-of-the-art methods on challenging benchmarks by learning adaptive, task-specific model structures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**A Novel Framework**: The paper introduces a new approach by defining Exemplar-Free Continual Learning (ExfCCL) as the problem of managing two separate memories within a Vision Transformer (ViT): an external memory for task identification (task centroids) and an internal memory for model parameters.\n\n**A Dynamic Architecture Method**: It presents a unique Neural Architecture Search (NAS) method that uses hierarchical exploration-exploitation. This method maintains the internal memory by continually learning dynamic, task-aware models. It uses four distinct operations—Reuse, Adapt, New, and Skip—to selectively update the model, which helps mitigate catastrophic forgetting.\n\n**State-of-the-Art Results**: The method achieves top-tier performance on two difficult benchmarks (MTIL and VDD), as measured by a new Figure of Merits (FoM) metric. Crucially, it also demonstrates the ability to automatically learn logical, customized model structures suited for each individual task."}, "weaknesses": {"value": "**Missing Algorithm Details**: The paper's reproducibility and clarity would be significantly improved by including pseudocode that details the step-by-step execution of the proposed CHEEM algorithm within a continual learning scenario.\n\n**Unverified \"Skip\" Operation**: The contribution of the \"Skip\" operation is not empirically validated. The experimental results should include an analysis (or ablation study) showing its specific impact on the number of parameters, training time, and inference time.\n\n**Incomplete Results in Main Tables**: To properly demonstrate the parameter efficiency of CHEEM, Tables 1 and 3 should be updated to include the total number of learnable parameters for all compared methods.\n\n**Missing Key Baseline Comparisons**: The evaluation is incomplete. It should include a direct comparison against DualPrompt on the standard 10-Split and 20-Split CIFAR-100 and 10-Split ImageNet-R benchmarks. This comparison must report:\n\n - The number of learnable parameters.\n\n - The average accuracy (performance).\n\n - The forgetting metric (Forget / BWT).\n\n - The training and inference (test) times."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "81g4O959MR", "forum": "aUSOQWt9oh", "replyto": "aUSOQWt9oh", "signatures": ["ICLR.cc/2026/Conference/Submission18355/Reviewer_PLw7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18355/Reviewer_PLw7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976086655, "cdate": 1761976086655, "tmdate": 1762928060233, "mdate": 1762928060233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}