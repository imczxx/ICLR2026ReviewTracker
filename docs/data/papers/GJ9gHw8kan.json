{"id": "GJ9gHw8kan", "number": 21111, "cdate": 1758313901521, "mdate": 1759896941485, "content": {"title": "From Transformer to Transponder: Introducing Contextual Modulation Training for Residual Learning  in LLMs", "abstract": "Transformers are the backbone of state-of-the-art systems across language, vision, and multimodal learning tasks, yet the relevance scale of their functional blocks (self-attention and feed-forward networks) is typically constant across inputs and depth. This static design neglects context-sensitive regulation of information flow through residual pathways. We introduce the \\emph{contextual modulator}: a lightweight, input-aware mechanism that can scale the outputs of linear sublayers within a block or the entire block output at token- and channel-level granularity. The modulator is implemented via compact parametric functions and adds negligible parameter overhead. Building on this idea, we propose Transponder, which integrates contextual modulators throughout Transformer blocks to endow functional residual architectures with fine-grained, input-adaptive control. Transponder provides evident improvement over six other scaling or normalization methods across LLaMA backbones ranging from 60M to 250M parameters, yielding consistent perplexity reductions with $<1\\%$ additional parameters. Analysis reveals depth-, module-, and token-specific scaling patterns, indicating that learned modulators act as input-adaptive regulators of residual information flow. Transponder provides a simple, general mechanism to augment Transformer-based models with context-sensitive modulators, providing robust and significant performance improvements without substantial architectural changes.", "tldr": "", "keywords": ["deep learning", "residual connection", "modulation training", "contextual scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa0231b864f982f7b140af12bea42488a0ebeaac.pdf", "supplementary_material": "/attachment/ab66013377759c66fefb127535c7670d7c09e047.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces TRANSPONDER, a method to enhance Transformer-based language models by adding lightweight, context-aware modulators with curvature-controlled sigmoids and low-rank bottlenecks. The approach achieves consistent perplexity reductions across LLaMA backbones (60M–250M parameters) with <1% additional parameters and demonstrates stability in Post-LN settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "It presents a modular framework for input-aware residual pathway control in Transformers.\n\nIt demonstrates consistent perplexity improvements across LLaMA variants (60M–250M)."}, "weaknesses": {"value": "The core idea of attaching lightweight, contextual gates to existing sublayers is a natural extension of existing gating mechanisms, and as a result, the conceptual novelty is incremental.\n\nIt is only evaluated on LLaMA backbones, without exploring its effectiveness on other Transformer variants.\n\nIt lacks theoretical analysis to explain why contextual modulation stabilizes training or improves performance.\n\nThey do not compare this parameter allocation with alternative ways to use the same parameter budget (e.g., widening FFN or adding lightweight adapters). Therefore, it is unclear whether the observed improvement is specific to the proposed mechanism or simply a result of having more capacity."}, "questions": {"value": "Minor Errors\n\nThe baseline perplexity values for the C4 dataset at 130M differ between Table 2 (26.73) and Table 5 (26.07).\n\nThe manuscript uses hyphens and en dashes inconsistently for the same purpose. For example, \"Self-attention\" (Line 128) uses a hyphen, whereas \"self–attention\" (Line 152) uses an en dash.\n\nThe terminology is used inconsistently throughout the manuscript. For example, both “sub-layer” and “sublayer” appear in different sections.\n\nThe manuscript is inconsistent in its capitalization of paragraph or subsection titles. For example, “Sigmoid and Learnable Sigmoid.” uses title case, whereas “Hidden dimensions.” uses sentence case.\n\nLines 156 and 161: Eq. equation 1 -> Eq. 1\n\nLines 236, 272, 325, 334, and 348: Openwebtext -> OpenWebText\n\nLine 303: !1% -> 1%\n\nLine 610: REPORDUCTION -> REPRODUCTION\n\nLines 128 and 129: e.g. -> e.g.,\n\nLine 255: LAuREL -> LAuReL\n\nLine 235: corpus -> corpora\n\nLine 236: For OpenWebText dataset -> For the OpenWebText dataset, for C4 dataset -> for the C4 dataset\n\nLine 241: metrics -> metric\n\nLine 402: comparable)perplexity -> comparable) perplexity\n\nLine 587: Learning rate Decay Method -> Learning Rate Decay Method\n\nLine 589: Layer Number -> Number of Layers\n\nLine 590: Head Number -> Number of Heads"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yOUFffUHEp", "forum": "GJ9gHw8kan", "replyto": "GJ9gHw8kan", "signatures": ["ICLR.cc/2026/Conference/Submission21111/Reviewer_SMQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21111/Reviewer_SMQN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226743765, "cdate": 1761226743765, "tmdate": 1762941281215, "mdate": 1762941281215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TRANSPONDER, a lightweight context-aware modulation framework for Transformers that dynamically scales the outputs of sublayers (e.g., Q/K/V projections, FFN) according to input context. The method separates representation learning (“what to compute”) from residual control (“how much to mix”), introducing compact modulators that operate at token- and channel-level granularity with <1% additional parameters. Experiments on LLaMA models (60M-250M) show consistent 5-15% perplexity reductions on OpenWebText and C4 datasets and improved training stability in the Post-LN setting. The paper includes ablations over placement, granularity, and hidden dimension."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses the static nature of residual scaling in Transformers.\n- Can be integrated into standard architectures with minimal modification.\n- 5-15% perplexity reductions across datasets and model sizes.\n- Prevents divergence in challenging Post-LN setups.\n- Explores modulation placement, granularity, and hidden size.\n- Visual analyses show token- and depth-dependent scaling patterns.\n- Achieves improvements with <1% additional parameters."}, "weaknesses": {"value": "- Results are limited to language modeling on OpenWebText/C4 using 60M-250M models. No evidence on large-scale models (>=1B) or real downstream tasks (QA, reasoning, instruction following).\n- Claims of “lightweight” and “negligible overhead” are not supported by FLOPs, latency, or memory statistics.\n- No analysis of how contextual modulation stabilizes training or enhances representational capacity.\n- No comparison against equal-parameter alternatives such as wider FFNs or more attention heads that could yield similar gains.\n- Partial overlap with LLaMA’s built-in gate projections: A single ablation (“w/o up and gate”) hints at redundancy but lacks a full quantitative study."}, "questions": {"value": "- Could you provide runtime, FLOPs, and memory overhead compared to LLaMA baselines across different sequence lengths and batch sizes?\n- How does TRANSPONDER perform on larger-scale models (>=1B) and downstream tasks such as instruction following or reasoning benchmarks?\n- If the same +1% parameter budget were spent on FFN expansion or additional attention heads, would performance improvements be comparable?\n- Can you offer a theoretical or analytical explanation for why contextual modulation improves optimization and stabilizes Post-LN training?\n- Have you analyzed the interaction or redundancy between your modulators and LLaMA’s gate-proj components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eP3W5WrcG6", "forum": "GJ9gHw8kan", "replyto": "GJ9gHw8kan", "signatures": ["ICLR.cc/2026/Conference/Submission21111/Reviewer_Kpr8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21111/Reviewer_Kpr8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889977623, "cdate": 1761889977623, "tmdate": 1762941280170, "mdate": 1762941280170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Transponder, a lightweight, input-aware modulation mechanism for Transformers. The key idea is to add contextual modulators that scale outputs of linear or functional sublayers (e.g., attention, MLP) at token or channel level. This allows the model to regulate residual information flow dynamically, unlike static scaling methods such as ReZero, DeepNorm, or LAuReL.\nExperiments on LLaMA backbones (60M–250M) trained on OpenWebText and C4 show consistent perplexity (PPL) improvements with less than 1% parameter overhead. The authors also perform ablations on granularity, placement, and modulation strength, and visualize token- and depth-dependent modulator behaviors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of decoupling representation from control via input-dependent modulators is conceptually strong and intuitively motivated.\n2. Experiments are strong. Covers multiple LLaMA scales and extensive ablations (modulator placement, resolution, hidden dimension, and component-wise contribution)."}, "weaknesses": {"value": "1. Similarity to Gating Methods. Perhaps a discussion comparing it with the gating method can be seen during the rebuttal phase.\n2. In Table 1, the first-row results (“Modulator-path-scalar”) show abnormally high PPL (e.g., 1088 for 250M), suggesting instability. The authors should explain why this configuration fails and whether this is due to optimization divergence or implementation bugs.\n3. Table 1 lacks direct comparison with the LLaMA baseline for those variants, making it hard to gauge how much each modulator improves over standard training."}, "questions": {"value": "I would like to ask about the significant reduction in PPL for Table 4. What is the reason behind this performance improvement? The method mentioned in the paper still follows a Transformer-like architecture, so theoretically, with the same number of parameters, there shouldn't be such a substantial performance change."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V4ucfghSJu", "forum": "GJ9gHw8kan", "replyto": "GJ9gHw8kan", "signatures": ["ICLR.cc/2026/Conference/Submission21111/Reviewer_7pnL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21111/Reviewer_7pnL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910805479, "cdate": 1761910805479, "tmdate": 1762941279242, "mdate": 1762941279242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Transponder, which aims to improve the performance of Transformer-based models by introducing contextual modulation training for residual learning in large language models (LLMs). The authors argue that the static design of Transformers neglects context-sensitive regulation of information flow through residual pathways, and propose the use of lightweight, input-aware modulators to scale the outputs of linear sublayers within a block or the entire block output at token- and channel-level granularity.\n\nTransponder provides improvement over six other scaling or normalization methods across LLaMA backbones ranging from 60M to 250M parameters, yielding consistent perplexity reductions with less than 1% additional parameters. Analysis of learned modulator values reveals depth-, module-, and token-specific patterns that adapt layer-wise contributions to input semantics, providing direct evidence that residual functional transformations benefit from adaptive, context-aware scaling. Transponder provides a simple, general mechanism to augment Transformer-based models with context-sensitive modulators, providing robust and significant performance improvements without substantial architectural changes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Well-Motivated Problem**: The authors clearly identify a limitation of the transformer design—static functional scaling across residual pathways—and provide a rationale for introducing input-aware modulation. The problem is convincingly motivated, with emphasis on the need for adaptive regulation to improve representation learning.\n\n* **Clear Writing**: The paper is written in an accessible and structured manner. It carefully explains the core principles of Trasnponder, its mechanism, and its integration into Transformers, making it easy to understand.\n\n* **Strong Empirical Results**: Transponder demonstrates consistent and significant gains in perplexity reduction across LLaMA model variants, with relative improvements reaching as high as 15.3%, underscoring the effectiveness of the approach without substantial computational or parameter overhead.\n\n* **Comprehensive Analysis and Ablations**: The paper includes extensive ablations and experiments, systematically analyzing placement, granularity, hidden sizes, and modulation coverage. This depth of analysis confirms the robustness and adaptability of the design choices."}, "weaknesses": {"value": "* **Evaluation** : It should be possible to train a llama baseline like the one in the paper to achieve less than 22.50 ppl on OWT. Did the authors properly tune the baseline ? What is the experiment setup  ? How many FLOPS are used for the baseline vs the Transponder results ? What is held constant ? It would also be interesting to know if this works beyond ppl and if it works on downstream taks.\n\n* **Efficiency** : How does the Transponder affect the training & inference latency and throughput ? Do these make the models slower ?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PfPd0LXnRV", "forum": "GJ9gHw8kan", "replyto": "GJ9gHw8kan", "signatures": ["ICLR.cc/2026/Conference/Submission21111/Reviewer_SG6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21111/Reviewer_SG6Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982732023, "cdate": 1761982732023, "tmdate": 1762941276931, "mdate": 1762941276931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper the authors produce adding input-aware modulation, otherwise known as gating, to all transformer layers and projections. This is done by using a sigmoid-activated low-rank layer that is multiplied with attention/ffw layer outputs as well as outputs of individual projections. The results show significant perplexity gains over a LLAMA baseline at different model scales, with minimal (1%) parameter overhead. The authors perform extensive ablations of their method, concluding that the rank can be relatively low, and that adding multiple such gating modules improves model quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The experimental setup is sound\n\n* The results show significant perplexity improvements and show improvement over a previous baseline (Laurel)\n\n* The authors' view of adding modulation everywhere is unifying and interesting."}, "weaknesses": {"value": "* There is related work with significant overlap that is not discussed. In particular, all the following papers use input-aware gating and show it improves model quality significantly. The authors should discuss them and describe the differences with their work.\nhttps://arxiv.org/pdf/2409.19606\nhttps://arxiv.org/pdf/2502.06785\nhttps://arxiv.org/pdf/2505.06708\n\n* The authors should show the hyperparameters they used in their final model (which modulators in Fig 1 are scalar vs channel based, what is the rank used in each case etc) are and how the 1% extra params is calculated. (In case it was mentioned and I missed it, it would help to make it more visible in the main body)\n\n* The authors should train a baseline model with +1% extra params to compare the perplexities in a fair way.\n\n* The authors could add some measurements and discussion on training time impact."}, "questions": {"value": "* Were the learning rates of the baseline model tuned? This is especially relevant since the authors use 2 * sigmoid activation, which might artificially increase the learning rate and confound the results.\n\n* How does the approach compare to pervious work referenced above?\n\n* What happens if we ablate the intermediate (low-rank) activation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sBrdx11GFP", "forum": "GJ9gHw8kan", "replyto": "GJ9gHw8kan", "signatures": ["ICLR.cc/2026/Conference/Submission21111/Reviewer_kjNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21111/Reviewer_kjNo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762465096420, "cdate": 1762465096420, "tmdate": 1762941275680, "mdate": 1762941275680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}