{"id": "4zRRnDscqn", "number": 4980, "cdate": 1757825627101, "mdate": 1759898001780, "content": {"title": "GGBall: Graph Generative Model on Poincaré Ball", "abstract": "Generating graphs with hierarchical structures remains a fundamental challenge\ndue to the limitations of Euclidean geometry in capturing exponential complexity.\nHere we introduce GGBall, a novel hyperbolic framework for graph generation\nthat integrates geometric inductive biases with modern generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder (HVQVAE) with a\nRiemannian flow matching prior defined via closed-form geodesics. This design\nenables flow-based priors to model complex latent distributions, while vector quantization helps preserve the curvature-aware structure of the hyperbolic space. We\nfurther develop a suite of hyperbolic GNN and Transformer layers that operate\nentirely within the manifold, ensuring stability and scalability. Empirically, GGBall\nestablishes a new state-of-the-art across diverse benchmarks. On hierarchical graph\ndatasets, it reduces the average generation error by up to 18% compared to the\nstrongest baselines. These results highlight the potential of hyperbolic geometry\nas a powerful foundation for the generative modeling of complex, structured, and\nhierarchical data domains.", "tldr": "The first graph generation framework built upon the Poincaré Ball model of hyperbolic space.", "keywords": ["Hyperbolic Space; Graph Generation; Flow Matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2579975a5e40a7e640532140a313d7886e289eb4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new generative framework for graph data based on a hyperbolic latent space. The authors motivate the use of hyperbolic geometry by highlighting its theoretical advantages for representing hierarchical and complex graph structures compared to standard Euclidean latent spaces.\n\nThe approach embeds graphs into a hyperbolic space through a Vector-Quantized Autoencoder (VQ-VAE) operating within the Poincaré ball model. The paper introduces dedicated architectural components, including a Poincaré Graph Neural Network (GNN), a Poincaré Transformer, and corresponding hyperbolic encoders and decoders that allow mapping graphs to and from the hyperbolic latent space.\n\nThe model is evaluated on three datasets containing relatively small graphs."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to follow, even though it introduces non-trivial concepts from hyperbolic geometry. The theoretical development is solid, clearly presented, and supported by extensive supplementary material.\n\nThe idea of leveraging a hyperbolic latent space for graph generation is both neat and original, and the proposed framework represents a meaningful conceptual step forward in the design of geometry-aware generative models.\n\nI would like to particularly emphasize that the theoretical contribution and its presentation are of very high quality. The mathematical rigor and clarity are exemplary."}, "weaknesses": {"value": "**Empirical Evaluation and Euclidean Baseline**\n\nThe paper’s main claim is the superiority of hyperbolic over Euclidean latent spaces for graph generation. Consequently, it is essential to provide clear empirical evidence supporting this claim.\nIncluding an Euclidean VQ-VAE baseline in Table 1 would greatly strengthen the experimental section, as it would directly demonstrate the empirical advantages of the hyperbolic space. Similarly, presenting detailed results and experimental settings for Figure 1 would help assess the validity of the reported improvements.\nSince the proposed model is theoretically and empirically more complex, it is important to show that it provides a clear and consistent performance advantage over its Euclidean counterpart, ideally across all reported datasets and experiments, including those in Tables 2 and 3.\n\n**Evaluation Metrics and Protocol**\n\nThe evaluation protocol requires clarification.\n\n* *Community-Small*: The results for the HVQVAE+Flow model appear to outperform the training set itself as given in SPECTRE, which should not occur in a properly calibrated evaluation. Moreover, the absence of standard deviations prevents assessment of variability or statistical significance. The paper refers to results from DiGress (which relies on SPECTRE), but the evaluation chain is not made explicit.\n\n* *Ego-Small*: Similar concerns arise here, as the model’s results also systematically outperform the training set as given in DGAE (which seems to be the journal paper of VQGAE) , which appears inconsistent. Clarifying the evaluation setup is crucial to ensure reproducibility and interpretability of the reported findings.\n\n**Evaluation on QM9**\nThe use of the *novelty* metric on QM9 is problematic. QM9 is an exhaustive enumeration of small organic molecules satisfying specific constraints; thus, generating molecules outside this set does not necessarily indicate successful generalization. This metric is therefore not commonly used for evaluation. It would be beneficial for the authors to justify its inclusion or reconsider its use.\nFor the same reason, achieving 100% *uniqueness* is not necessarily desirable. \n\nTherefore, the only meaningful metric in this setting is *validity*, which remains substantially below various baselines.\nTo provide a more comprehensive evaluation, including additional metrics such as FCD (Fréchet ChemNet Distance) or NSPDK similarity would be highly valuable.\n\n**Graph Size and Dataset Diversity**\nThe experiments are limited to datasets with small graphs, with a maximum of around 20 nodes (e.g., Community-Small). Although this limitation is acknowledged in the paper, it remains a major limitation, as many contemporary models handle graphs with up to 200 nodes and report results on larger benchmarks such as Planar, SBM, Zinc250K, or Moses.\n\nIncluding experiments on at least one larger-scale dataset would considerably strengthen the empirical evidence and demonstrate scalability.\nThis is particularly important because the Community-Small and Ego-Small datasets each contain only around 200 instances, making overfitting likely and reducing the statistical robustness of the results.\n\n\n--------\n\n**SPECTRE**: Karolis Martinkus, et al.. SPECTRE:\nSpectral conditioning helps to overcome the expressivity limits of one-shot graph generators. In Proceedings of the 39th International Conference on Machine Learning. PMLR, 17–23 Jul 2022.\nhttps://proceedings.mlr.press/v162/martinkus22a.html.\n\n**DGAE**: Yoann Boget, et al.. Discrete graph auto-encoder. Transactions on Machine Learning Research, 2024. https://openreview.\nnet/forum?id=bZ80b0wb9d."}, "questions": {"value": "The theoretical contribution of the paper is strong and well-founded, but the empirical evaluation does not yet convincingly demonstrate the claimed benefits of hyperbolic latent spaces.\n\nI would encourage the authors to consider the following improvements:\n\n1. Include a systematic ablation comparing hyperbolic and Euclidean latent spaces across all tasks and datasets.\n2. Describe the experimental protocol in detail, including data splits, training configurations, and evaluation procedures (possibly in the supplementary material).\n3. Expand the empirical evaluation by including larger datasets (e.g. Zinc250, Planar, SBM) and additional performance metrics (e.g., FCD, NSPDK).\n\nSuch additions would substantially reinforce the paper’s claims and make the overall contribution more compelling."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OqRj1rQec7", "forum": "4zRRnDscqn", "replyto": "4zRRnDscqn", "signatures": ["ICLR.cc/2026/Conference/Submission4980/Reviewer_BT1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4980/Reviewer_BT1X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761139365538, "cdate": 1761139365538, "tmdate": 1762917806017, "mdate": 1762917806017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a hyperbolic framework for graph generation to address the limitations of Euclidean geometry in capturing hierarchical graph structures. It integrates a Hyperbolic Vector-Quantized Autoencoder with Riemannian flow matching based on closed-form geodesics in the Poincaré Ball model. Empirically, GGBall achieves state-of-the-art performance across benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed model is a fully hyperbolic graph generation framework using the Poincaré Ball, leveraging its exponential volume growth to naturally preserve hierarchical structures.\n\n2. Combining HVQVAE with Riemannian flow matching (for flexible prior modeling) resolves stability issues of continuous hyperbolic VAEs and enhances generative capacity.\n\n3. It outperforms SOTA baselines across abstract graph generation and molecular graph generation."}, "weaknesses": {"value": "1. Experiments focus on small/medium graphs (e.g., QM9, small community graphs); performance on large-scale graphs is unproven.\n\n2. While HVQVAE avoids HVAE’s KL issues, the paper does not explore alternative variational formulations to fully leverage hyperbolic probabilistic modeling. In addition, what is a L_degree in Line 273?\n\n3. The Poincaré Ball’s fixed negative curvature may struggle with heterogeneous graphs (mixing hierarchical and non-hierarchical structures), unlike mixed-curvature alternatives not explored here.\n\n4. The loss function has many hyperparameters. How are their values ​​determined? What are their values ​​for different datasets? What are the sensitivity experiments? Won't so many parameters increase the difficulty of hyperparameter tuning, thereby reducing the practical usability of the method? The inability to answer these questions may cause concern for the reader."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x0RzLPsPOa", "forum": "4zRRnDscqn", "replyto": "4zRRnDscqn", "signatures": ["ICLR.cc/2026/Conference/Submission4980/Reviewer_LTqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4980/Reviewer_LTqy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898574207, "cdate": 1761898574207, "tmdate": 1762917805211, "mdate": 1762917805211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework for generating graphs with hierarchical structure termed “GGBall”, consisting of\n\n1. hyperbolic message passing and DiT layers which use\n    \n    1. message aggregation in the tangent space via log/exp maps (GNN)\n        \n    2. scale and shift values used for FiLM like message modulation derived from hyperbolic distances  (GNN)\n        \n    3. value aggregation using Möbius gyromidpoints (DiT)\n        \n    4. relevancy score calculation leveraging hyperbolic distances on poincare linear layer proejcted q,k values (DiT)\n        \n    5. hyperbolic auxillary operations (diT): layernorm aggregation in the tangent sapce like the GNN message aggregation, residual connections using möbius addition instead of standard addition and multi-head splitting and concatenation in a hyperbolic geometry perserving fashion\n        \n2. a novel graph auto encoder leveraging this hyperbolic latent space parametrization, trained to reconstruct node and edge types, regularized by matching degrees consistently + an l2 norm (evaluated in continuous, standard AE,variational AE and quantized VAE form, termed HGAE, HVAE and HVQVAE respectively, where quantized version was motivated by numerically unstable KL divergence for the HVAE)\n    \n3. a manifold flow matching method taken from [https://openreview.net/pdf?id=g7ohDlTITL](https://openreview.net/pdf?id=g7ohDlTITL)\n    \n\nThe method is evaluated on community-small, ego-small and qm9, with the interpolation properties of the latent state being studied in particular"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. seemingly strong performance\n2. overall very clear exposition of a complex, but theoretically well motivated approach\n\nhitting the guides dimensions:\n\n- originality: hyperbolic embeddings are well established, but creating end to end VQVAE+ flow models I haven't seen yet\n- quality: well written, decent evaluation, proofs appear to be correct after a single close read\n- clarity: fully understandable, with some small nits\n- significance: solid incremental advance, evaluation on larger graphs/trees required to say more"}, "weaknesses": {"value": "1. The density of exposition and hyperbolic-geometry terms can make the paper somewhat difficult to follow (einstein midpoint, möbius gyromidpoint etc). I’d suggest the following two tweaks\n    \n    1. state direction around around 107 that all terms not immediately defined are defined in the appendix for space constraint reasons (to warn the reader some will be just mentioned)\n        \n    2. make use of latex’ glossary feature [https://www.overleaf.com/learn/latex/Glossaries](https://www.overleaf.com/learn/latex/Glossaries) and a reasonable link color/style, to allow hovering over the term to see the definition in modern browsers (I think this will help the definition quite a lot) + enabling backlinks (if readers click through)\n        \n2. should detail how  hyperparameter choices/tuning were performed\n    \n3. nice to have: consider adding a  test on larger graphs (guacamol,moses), as noted in appendix K\n    \n4. nit: shouldn’t it be $\\frac{2}{\\sqrt{c}\\lambda_x^c}$ , see e.g. [https://arxiv.org/pdf/1805.09112](https://arxiv.org/pdf/1805.09112)  eq 12?\n    \n5. interpolation experiment needs reporting of a baseline (in the appendix)\n    \n6. not: I think $\\lambda_{valid}$ is meant to be $\\lambda_{degree}$ on eq 7?"}, "questions": {"value": "1. would it make sense to do do ablations over mechanisms in the poincare parametrization since a lot of them are introduced, or are they an all or nothing operation (I assume the lattern)?\n    \n2. why does HVQVAE change from e.g. 0.002 in table  1 to 0.0071 in table 2? should report mean/std across multiple rounds (at least for their method/closest baseline) for CIs\n    \n3. why no experiment on generating trees? this would seem like the clear “hello world” example for this?\n\n+ address as many weaknesses as possible please"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "15bPc8RWf0", "forum": "4zRRnDscqn", "replyto": "4zRRnDscqn", "signatures": ["ICLR.cc/2026/Conference/Submission4980/Reviewer_tqE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4980/Reviewer_tqE4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982557567, "cdate": 1761982557567, "tmdate": 1762917804821, "mdate": 1762917804821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main goal of the paper is to develop a graph generative model that leverages hyperbolic geometry to naturally capture hierarchical and tree-like structures. Specifically, it aims to show that embedding graphs in the Poincaré ball and generating them through manifold-aware neural components (hyperbolic GNN, geodesic attention, HVQVAE, and Riemannian flow matching) can more effectively represent the relational geometry of graphs compared to traditional Euclidean models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work presents a coherent framework that combines hyperbolic graph neural networks, geodesic attention, vector quantization, and Riemannian flow matching in a single end-to-end model. This integration is technically nontrivial and demonstrates careful engineering of both discrete and continuous latent components.\n\n2. Paper is well written and maintains consistent notation throughout to follow."}, "weaknesses": {"value": "1. Authors assume node labels depend only on their own latent and edges depend only on pairwise hyperbolic relations which I agree cuts decoding complexity, but it also forbids higher-order dependencies (motifs, triads) and makes long-range constraints modelling not accounted. It can miss global combinatorial constraints that are not pairwise-decomposable.\n\n2. Authors add a degree-edge consistency term aligning predicted degree to ground-truth degrees and it improved MMD on degree but might risk over-regularizing towards degree histograms at the expense of other structures (e.g., motif diversity), if authors can comment on it?\n\n3. Authors shared the anonymous repo but all files on the anonymous link are not accessible. It compromises the reproducibility.\n\n4. Hyperbolic geometry’s expressive power critically depends on curvature c. However, methodology fixes c a priori. Curvature effectively controls the “branching factor” of the embedding manifold. Without adaptive curvature learning: 1) Graphs of different hierarchy depths collapse into a single scale OR The model may under- or over-stretch distances, biasing flow priors and reconstruction.\n\n5. No error bounds are shown for the tangent-space linearization (how much curvature is lost per layer). Similarly, is there any theoretical grounding that repeated log–exp projections preserve manifold consistency?\n\n6. The results mainly demonstrate performance on small synthetic or molecular settings, leaving open whether the proposed hyperbolic framework generalizes to large-scale or sparse real-world graphs."}, "questions": {"value": "Look in the Weaknesses Section. I am open to considering answers for concerns mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c0JreixLyD", "forum": "4zRRnDscqn", "replyto": "4zRRnDscqn", "signatures": ["ICLR.cc/2026/Conference/Submission4980/Reviewer_FJcZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4980/Reviewer_FJcZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985520268, "cdate": 1761985520268, "tmdate": 1762917804543, "mdate": 1762917804543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}