{"id": "CEpNboUJyw", "number": 15846, "cdate": 1758256027682, "mdate": 1759897278013, "content": {"title": "CSAttention: Centroid-Scoring Attention for Accelerating LLM Inference", "abstract": "Long-context LLMs increasingly rely on long prefill prompts for agents and domain Q&A, pushing attention and KV-cache to become the dominant decode-time bottlenecks. While sparse attention methods reduce computation and transfer costs, they struggle to simultaneously maintain model accuracy and achieve high inference speed under high sparsity. To address this challenge, we propose Centroid-Scoring Attention (CSAttention), a training-free sparse attention method for efficient LLM inference. CSAttention adopts a storage-for-computation strategy: it leverages query distributions to construct a fixed-size, query-centric lookup table in each subspace during the offline prefill stage, enabling online decoding to perform efficient searches and centroid-score accumulation over regular, GPU-friendly data structures. By combining subspace partitioning with query-centric table construction, CSAttention mitigates distribution shift between queries and keys, and reliably recovers high-scoring keys even under very high sparsity, enabling significant computational savings while maintaining competitive model performance. Extensive experiments demonstrate that CSAttention maintains near-lossless model accuracy while delivering substantial improvements in inference efficiency. Compared to state-of-the-art sparse attention methods, CSAttention achieves superior model accuracy and higher inference speed in high-sparsity (95%) and long-context (32K-128K) scenarios. Notably, CSAttention achieves up to 4.24× speedup over full attention when decoding 128K context length, demonstrating its practical value for scalable long-context inference.", "tldr": "", "keywords": ["LLM Inference", "Sparse Attention"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7b040e65331a3c4917aeac6e738f494a2147a7c.pdf", "supplementary_material": "/attachment/992f76f73265d69208f4484ac1c0be71a6210574.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Centroid-Scoring Attention (CSAttention), a training-free sparse attention method designed to accelerate LLM inference for long-context scenarios. The core problem it addresses is the dual bottleneck of KV cache memory footprint and the $O(N d)$ memory-bound computation in the decoding phase.\n\nThe method is based on a *storage-for-computation* strategy, primarily targeting applications with a static, shared prefill context. In an *offline prefill* stage, it partitions the feature space into $m$ subspaces and clusters the prefill queries into $C$ centroids in each. It then precomputes and stores a Top-L list of partial scores between each centroid and all keys. During the *online decode* phase, a new query finds its nearest centroid in each subspace, fetches the $m$ pre-scored lists, and performs a *gather-and-sum* to find the global Top-K keys. Attention is then computed only on this small $K$ (e.g., 5%) subset.\n\nThe paper demonstrates that CSAttention can achieve near-lossless accuracy at 95% sparsity and significant speedups (up to 4.24x) over full attention in long-context (128K) decoding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. High-Relevance Problem: The paper tackles one of the most significant and practical challenges in LLM serving today: the cost and memory bottleneck of long-context attention, especially in the memory-bound decoding phase.\n2. Novel Query-Centric Indexing: The core idea of clustering queries (Q) rather than keys (K) to build the index is a sound and valuable insight. As shown in Figure 1(d), this directly addresses the Q/K distribution shift.\n3. Strong Empirical Accuracy: The method's ability to maintain near-lossless accuracy (Tables 1 and 2) while operating at very high sparsity (95%) is impressive."}, "weaknesses": {"value": "1. Misleading Claims on Memory and Complexity\n\nNon-Fixed-Size Index: The abstract and introduction repeatedly refer to a fixed-size lookup table. This is factually incorrect. Appendix B.1 clearly states the list length $L = \\alpha N$, making the total index size $O(m C \\alpha N)$. This index is not fixed-size; it scales linearly with the context length $N$, just like the KV cache it is meant to help manage.\n\nExacerbating the Memory Bottleneck: The primary motivation is that the $O(Nd)$ KV cache is too large for HBM. This method introduces a second large data structure of $O(mC\\alpha N)$ bytes. This does not solve the memory capacity bottleneck; it arguably makes it worse by requiring space for both the full KV cache and this new massive index. The All-GPU mode is therefore even less practical at scale than standard attention.\n\nNo Asymptotic Improvement in Decoding: The decoding phase of standard attention is memory-bound with $O(Nd)$ complexity. CSAttention's decoding complexity (Appendix B.2) is $O(m\\alpha N + \\rho Nd)$. This is still $O(N)$ and remains memory-bound. The 4.24x speedup is purely a constant factor improvement, not an asymptotic one, which is a less fundamental contribution.\n\n2. High Prefill Cost:\n\nThe offline prefill is not just a standard prefill. It involves computationally expensive $O(INCd)$ k-means clustering and a massive $O(NCd)$ scoring step to build the index. Figure 4 confirms this, showing CSAttention's prefill latency is substantially (2-5x) higher than all baselines. This cost must be paid once per context. This limits the method's applicability to only a niche set of problems where the long context is truly static, shared, and long-lived (e.g., a fixed set of RAG documents for an entire service). It is unusable for common long-context tasks like summarizing a new document, few-shot learning on a new prompt, or a long, evolving conversation. The paper is not sufficiently transparent about this critical limitation.\n\n3. The Stale Centroids Problem:\n\nThe query-centroids are computed only from the queries in the prefill context. During a long online decoding phase, the distribution of these newly generated queries will inevitably drift from the original prefill query distribution. The static, stale centroids may no longer be representative of the new queries, leading to poor list selection, decreased recall, and a potential degradation in accuracy that is not measured in the current experiments. The paper does not address or evaluate this query-drift phenomenon.\n\n4. Unanalyzed Cost of Streaming Updates:\n\nThe paper claims a streaming-friendly update (Section 3.3) for when a new key is added. This requires computing its partial score against all $m \\times C$ centroids and attempting insertion into $m \\times C$ lists. This is a non-trivial overhead that is added to every single decoding step. The latency of this update step is not analyzed or reported, and it's unclear if it negates a significant portion of the gains from the sparse attention."}, "questions": {"value": "1. Can the authors clarify the total memory footprint (KV Cache + CSAttention Index) compared to baselines? Given the index also scales with $O(N)$, how does this method practically alleviate the memory capacity bottleneck, which is the primary motivation?\n\n2. How does the method's accuracy hold up against query distribution drift? For example, if the model is decoded for thousands of steps, do the stale centroids from the prefill stage lead to a drop in accuracy?\n\n3. What is the measured per-step latency of the Streaming-friendly update (Step 5, Section 3.3)? Please provide a breakdown of the decoding step time (Nearest Centroid, Gather-and-Sum, Streaming Update, Sparse Attention) to clarify the true source of speedup and the cost of this update."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fvsgv5Q3cQ", "forum": "CEpNboUJyw", "replyto": "CEpNboUJyw", "signatures": ["ICLR.cc/2026/Conference/Submission15846/Reviewer_s5u9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15846/Reviewer_s5u9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484933679, "cdate": 1761484933679, "tmdate": 1762926068679, "mdate": 1762926068679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Long-context decoding is challenging due to increased KV-cache pressure. Unlike existing work, **CSAttention** proposes to cluster **Q** vectors and pre-compute the partial attention score between cluster centroids and **K** vectors, recording *m* short lists containing important keys during the prefill phase. During generation, the new query vector is sliced and matched to the closest cluster centroid for each segment. The keys from each list are gathered and reduced to find the top-*k* keys for attention. CSAttention achieves near-optimal accuracy at 5% sparsity and provides good speedup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It is interesting to cluster **Q** vectors instead of **K** vectors to identify important tokens.  \n- The use case covers both CPU–GPU and GPU-only scenarios."}, "weaknesses": {"value": "- The method has high prefill overhead and the end-to-end performance is unclear.  \n- The method may be hard to extend to short-prefill, long-generation scenarios."}, "questions": {"value": "Thanks for submitting to ICLR 2026. The paper introduces an interesting idea of clustering **Q** vectors to identify important tokens. However, I still have some concerns about the paper.\n\n- Firstly, due to GQA, the query vector is much larger than the key vector. In order to perform the clustering, we need to store all the KQV tensors, which is memory intensive. Additionally, the clustering overhead seems significant. Indeed, CSAttention has the slowest prefill throughput and significant overhead compared to other baselines. For long-context prefill, the increased prefill time can offset the speedup during generation. While multi-round conversation and other use cases can amortize this cost, such restrictions may limit the applicability of the method.  \n\n- Secondly, the method seems sensitive to the existing queries. I wonder what would happen if the new query has never been seen before and is far from existing clusters.  \n\n- Additionally, the method seems to be only applicable to long-context prefill, long-generation scenarios. However, for reasoning tasks, most of the KV-cache pressure comes from the generation phase. In this case, how does the method handle new tokens? Does re-clustering cause even more overhead? Or is it even possible to re-cluster the queries if the existing query vectors are not stored?  \n\n- For accuracy evaluation, 5% already represents a considerable amount of tokens. While the baselines perform slightly worse, they still achieve reasonably good performance. It would be more convincing to show results at higher sparsity levels to make the difference more significant.  \n\n- For all-GPU decode speed, it seems that the full-attention baseline is surprisingly slow. I wonder what implementation is used for the full-attention baseline and what type of parallelism is used across the 4 GPUs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CL8I4MjdQ3", "forum": "CEpNboUJyw", "replyto": "CEpNboUJyw", "signatures": ["ICLR.cc/2026/Conference/Submission15846/Reviewer_gZeQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15846/Reviewer_gZeQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987394770, "cdate": 1761987394770, "tmdate": 1762926067970, "mdate": 1762926067970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Centroid-Scoring Attention, a training-free, query-centric sparse attention method that builds fixed-size per-subspace centroid lookup tables during an offline prefill and then performs fast centroid-score accumulation at decode time to recover high-scoring keys under extreme sparsity. The method is motivated by observed Q/K distribution shift and uneven subspace contributions, and it is evaluated on LongBench and LongBench v2 with three 7–8B class models; the authors report near-lossless accuracy at 95 percent sparsity and substantial throughput gains up to about 4.2 times at ultra-long contexts, while offering both All-GPU and CPU-GPU deployment modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The core idea is conceptually clear and practically oriented. The method aligns well with realistic offline-prefill plus online-decode deployment patterns described in the paper. \n\n* The authors evaluate on a diverse LongBench suite, compare against multiple recent sparse attention baselines, report both accuracy and throughput. \n\n* Empirical robustness at extreme sparsity is convincing in many metrics. Results show the approach keeps macro averages very close to full attention (within a fraction of a point on several backbones) while achieving large speedups at long context lengths, which supports the central claim that query-centric centroid scoring improves recall under high sparsity."}, "weaknesses": {"value": "* Method sensitivity and hyperparameter tuning appear underexplored. Key choices such as number of subspaces, centroids per subspace, Top-L scaling, α and ρ, and the middle-dominant schedule materially affect both recall and memory. Provide a short appendix with recommended default hyperparameters and expected index sizes for common model/head configurations can be fine.\n\n* Evaluation leaves some gaps in fairness and workload diversity. baselines are tuned but a few strong alternatives (for example recent learned-index or hybrid retrieval-attention schemes) are not present, and comparisons do not fully characterize cases where CSAttention might degrade, such as highly domain-shifted queries not covered by the prefill or pathological heads where Q/K geometry collapses. Additional failure-mode analyses and latency-percentile measurements would clarify worst-case behavior for latency-sensitive applications.\n\n* One more concern is about the speed of the clustering and searching, which should be analysis. Include percentile tail-latency and memory-usage plots in the main paper to complement average-throughput claims."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ApT2jAA4vw", "forum": "CEpNboUJyw", "replyto": "CEpNboUJyw", "signatures": ["ICLR.cc/2026/Conference/Submission15846/Reviewer_f8Pw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15846/Reviewer_f8Pw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993044660, "cdate": 1761993044660, "tmdate": 1762926067599, "mdate": 1762926067599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CSAttention, a new sparse attention method to speed up LLM inference on long contexts. The main idea is to tackle the Q/K distribution shift. Instead of building an index on keys (like other methods), they cluster the queries during an offline prefill stage. Then they pre-compute scores for these query-centroids against all keys. At decode time, a new query just finds its closest centroid, grabs the pre-computed Top-L lists, and sums them up to find the final Top-K keys. It's a training-free \"storage-for-computation\" approach. They show it's nearly lossless at 95% sparsity and gives a big speedup (e.g., 4.24x at 128K)."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Motivation and Design: The strongest aspect of the paper is the empirical identification of the Q/K distribution shift . The query-centric clustering mechanism is a logical and novel solution to this specific problem.\n\n2. Excellent Results on Long Prefill Tasks: Within its tested domain (long-document Q&A via LongBench), the method demonstrates an impressive combination of near-lossless accuracy at 95% sparsity and high throughput.\n\n3. High-Quality Presentation: The paper is well-written, logically structured, and easy to follow."}, "weaknesses": {"value": "1. Critically Incomplete Benchmarking: The paper's claims to accelerate \"LLM Inference\" are not fully supported, as the evaluation exclusively focuses on long-prefill, short-decode workloads (like LongBench). It completely omits the equally critical long-generation workload (e.g., long-form chain-of-thought reasoning), where the prompt is short and the KV cache grows dynamically with generated tokens. This is a major gap.\n\n\n\n2. Undefined and Potentially Weak Baseline: The headline claim of \"up to 4.24x speedup over full attention\"  is scientifically meaningless without defining the \"Full Attention\" baseline. If this baseline is not a state-of-the-art implementation (e.g., FlashAttention-2/3), the speedup numbers are not representative of real-world gains.\n\n\n\n3. Unclear Generalizability: Because long-generation is not tested, it is unknown how the \"streaming-friendly updates\"  perform. It's unclear how the Q-centroids, built from prefill queries, would generalize to the (potentially different) distribution of generated queries during a long reasoning process."}, "questions": {"value": "My current rating is a weak reject based on the significant gaps in the evaluation. I am willing to raise my score if the authors can provide convincing answers to the following:\n\n1. What specific implementation was used for the \"Full Attention\" baseline in Figure 3? Was this a naive implementation or a SOTA kernel like FlashAttention-2 or FlashAttention-3?\n\n2. How does the throughput of CSAttention (in All-GPU mode) compare directly to a FlashAttention-2/3 baseline at 32K, 64K, and 128K contexts?\n\n3. Can you provide experimental results (both accuracy and throughput) for a long-generation task? For example, a benchmark involving long-form chain-of-thought where the model must generate thousands of tokens, thereby testing the \"streaming-friendly updates\"  in a generation-heavy regime.\n\n4. How does the query-centric clustering  handle the first generated token (which has no prior query), and how does the system adapt if the distribution of generated queries differs from the prefill queries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ggw3LXCqSb", "forum": "CEpNboUJyw", "replyto": "CEpNboUJyw", "signatures": ["ICLR.cc/2026/Conference/Submission15846/Reviewer_WaZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15846/Reviewer_WaZH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140139764, "cdate": 1762140139764, "tmdate": 1762926066944, "mdate": 1762926066944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}