{"id": "MCeM7uRH9U", "number": 20443, "cdate": 1758306224622, "mdate": 1759896977383, "content": {"title": "An Interactive Paradigm for Deep Research", "abstract": "Recent advances in large language models (LLMs) have enabled deep research systems that synthesize comprehensive, report-style answers to open-ended queries by combining retrieval, reasoning, and generation. Yet, most frameworks rely on rigid workflows with one-shot scoping and long autonomous runs, offering little room for course correction if user intent shifts mid-process. We present **SteER**, a framework for steerable deep research that introduces interpretable, mid-process control into long-horizon research workflows. At each decision point, **SteER** uses a cost–benefit formulation to determine whether to pause for user input or proceed autonomously. It combines diversity-aware planning with utility signals that reward alignment, novelty, and coverage, and maintains a live persona model that evolves throughout the session. **SteER** outperforms state-of-the-art open-source and proprietary baselines by up to 22.80\\% on alignment, leads on quality metrics such as breadth and balance, and is preferred by human readers in 85\\%+ of pairwise alignment judgments. We also introduce a persona–query benchmark and data-generation pipeline. To our knowledge, this is the first work to advance deep research with an interactive, interpretable control paradigm, paving the way for controllable, user-aligned agents in long-form tasks.", "tldr": "", "keywords": ["interactive deep research", "LLM agents", "interpretable control", "multi-turn clarification", "long-horizon reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/487b1321a7b90e26d7975a95e69f7a390a858c26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces STEER, a novel framework that brings interactive, user-steerable control to long-form research question answering systems. Current “deep research” pipelines (where an AI agent conducts multi-step web retrieval and reasoning to produce a comprehensive report) typically operate in a largely autonomous way: the system might ask one clarification at the start and then proceed to generate a lengthy answer. This one-shot approach can lead to misalignment with the user’s true needs if those needs are not fully specified upfront or evolve as new information emerges.\n\nSTEER aims to keep the user in the loop in a principled, minimally invasive manner throughout the research process. The framework is built around three key ideas: \n\n(1) Diversity-aware planning: At each step, the system generates several potential sub-questions or directions to investigate, explicitly promoting diversity of topics (using techniques like Maximal Marginal Relevance to avoid redundancy). This ensures a broad exploration of the query’s aspects. \n\n(2) Cost–benefit based pause decision: Rather than blindly pursuing all those directions or arbitrarily asking the user, STEER uses a decision module to determine whether to pause and ask the user to choose the next direction. It computes a utility for expanding each candidate branch (favoring those that align with the user’s interests, add novelty, and improve coverage of the topic) and an approximate “execution cost” for following that branch (e.g. how much work it entails). If the system continues autonomously, it will pick a subset of branches that maximize total utility minus cost. If it pauses, the user can select which branches they care about (and even suggest new ones). The expected benefit of pausing is calculated as the gain in utility from letting the user prune low-value branches and add relevant ones, minus a pause cost that models the burden of bothering the user. The pause cost is personalized: it increases as more questions are asked, according to a user-specific tolerance budget (so the system will only interrupt when it believes the user’s guidance is truly valuable).\n\n3) Live persona modeling: STEER maintains a dynamic model of the user’s persona and preferences. Initially, it takes into account any provided profile or aspect checklist the user wants covered. After each interaction, it updates this persona – for instance, if the user selects certain subtopics or explicitly states new preferences, these are incorporated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In experiments on a constructed persona-query benchmark, STEER outperforms state-of-the-art baselines (including both open-source pipelines and a proprietary system) by a large margin in terms of alignment to the user’s requested aspects (gains of up to 22.8% in alignment metrics are noted). Its answers also maintain strong overall quality, with higher breadth and balance of coverage. \n\nHuman evaluators overwhelmingly prefer STEER’s outputs – in over 85% of cases when asked which answer is more tailored to their interests, and similarly high preference for focus and usefulness. Importantly, the framework allows tuning the trade-off between asking too often and potentially missing what the user wants: by adjusting the pause cost parameters, one can make the agent more cautious or more independent, as suited for the user.\n\nThe contributions of this work are the introduction of an interactive, interpretable decision policy in deep research agents and a demonstration that this leads to measurably more user-aligned results."}, "weaknesses": {"value": "One possible downside is added complexity: STEER’s policy involves many components (planning, utility estimation, persona updates) and could require careful calibration of hyperparameters like the user’s interruption cost. I hope to hear detailed discussions about this matter (In terms of realistic application view). However, the authors provide a clear formulation for these and show that the system can effectively balance asking versus autonomy. \n\nAlso, i want the authors to carefully investigate whether there are any other benchmarkable research application, which enables 1) pause decision or 2) live persona modeling. I evaluate these characteristic as beneficial, but i think that this paper requires extensive comparisons with current web-based research services. \n\nI am quiet convinced that Steer advances the deep research application via various interplay algorithms, but i also think that these are somewhat incremental. So, i would appreciate if the authors provide table-based view on comparisons of techniques. I am leaning toward rejecting this paper, while i could increase the score if above issues are well resolved."}, "questions": {"value": "Discussed in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AupvEWIItx", "forum": "MCeM7uRH9U", "replyto": "MCeM7uRH9U", "signatures": ["ICLR.cc/2026/Conference/Submission20443/Reviewer_a7R1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20443/Reviewer_a7R1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637371666, "cdate": 1761637371666, "tmdate": 1762933883627, "mdate": 1762933883627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduce Steer, which is a framework that incorporate mid-process control into the long-horizon research workflow. They compare their frameowkr with opensource and openAI's o4-mini-deep-research model. THe evaluation is done on persona-tailored quality and general quality of the report. It outperform the proprietary model on the persona alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The task steerable deep research is interesting.\nThe system is well designed"}, "weaknesses": {"value": "1. there is no meta-evaluation on the LLM-as-judge, need to have high correlation with human judgements to backup the usage of LLM-as-judge.\n2. the user study is flawed: first, missing inter-annotator agreement; second, currently the user needs to mimic the persona and query, which are not what the user's own query, it would be better to conduct the user study with user doing their own queries instead of mimicking some one.\n3. the task is not novel, as in chatgpt, people can already do deep research in multi-turns."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EBIId8Uv8v", "forum": "MCeM7uRH9U", "replyto": "MCeM7uRH9U", "signatures": ["ICLR.cc/2026/Conference/Submission20443/Reviewer_fdcx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20443/Reviewer_fdcx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886998397, "cdate": 1761886998397, "tmdate": 1762933882929, "mdate": 1762933882929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SteER, a human-in-the-loop framework designed for deep research. The framework introduces interpretable, mid-process control into long-horizon research workflows, combining diversity-aware planning with utility signals that reward alignment, novelty, and coverage. Experimental results show substantial performance improvements over proprietary baselines, and human evaluators consistently prefer SteER in pairwise alignment judgments."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces an interactive human-in-the-loop framework for deep research, enabling mid-process interpretability and control. Centered around three components: diversity-aware exploration, pause-decision, and persona modeling, the framework systematically explores different solution paths, keeping track of different costs and user personas.\n- SteER demonstrates strong performance on Persona-tailored and Quality metrics.\n- Ablation studies provide valuable insights into the contributions of different components. Removing Explore, InfoGain, and Diversity exploration degrades the performance across metrics except for Depth without InfoGain.\n- User studies validate the framework’s effectiveness and alignment with human preferences. Even though evaluated on only 58 pairwise annotations, SteER is preferred in 86-90% cases with substantial gains in Coverage and Findability."}, "weaknesses": {"value": "- Comparisons are limited to only three frameworks (GPT-Researcher, Open Deep Research, and OpenAI Deep Research model).\n- Including additional baselines such as Gemini-2.5-Pro Deep Research, Perplexity Research, and Grok Deeper Search would strengthen the evaluation.\n- Experiments are conducted solely on DeepResearchGym. Broader benchmarking, including datasets like DeepResearch Bench (Du et al., 2025), would enhance the generalizability of the results."}, "questions": {"value": "Was any analysis conducted on the latency or runtime of SteER compared to the baseline frameworks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i3lMHRHr5B", "forum": "MCeM7uRH9U", "replyto": "MCeM7uRH9U", "signatures": ["ICLR.cc/2026/Conference/Submission20443/Reviewer_FMVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20443/Reviewer_FMVC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970285484, "cdate": 1761970285484, "tmdate": 1762933882389, "mdate": 1762933882389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}