{"id": "QfZ5RqXl8q", "number": 12640, "cdate": 1758209190658, "mdate": 1763114045700, "content": {"title": "Adaptive Guidance Scaling for Posterior Diffusion-based Sampling", "abstract": "Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions.\nIn this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity.\nThe resulting approach, Adaptive Posterior diffusion Sampling (APS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks—including super-resolution, Gaussian deblurring, and motion deblurring—on CelebA-HQ and ImageNet-256 validation sets. APS consistently surpasses existing diffusion-based baselines in perceptual quality without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.", "tldr": "APS: posterior-guided diffusion sampler that sets the likelihood step size in an adaptive, robust manner; obtains state-of-the-art results without task-specific tuning.", "keywords": ["Diffusion models", "Inverse Problems", "Posterior Sampling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9f32154adb12d9bd5121d6d6cea832ca97085dfa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present a hyperparameter-free approach to find the step size for conditional diffusion sampling methods such as DPS [1].\nThey apply their approach \"Adaptive Posterior Diffusion Sampling\" (APS) to several linear inverse problems on both CelebA and ImageNet.\n\n[1] Chung et al. \"Diffusion Posterior Sampling for General Noisy Inverse Problems\" (2023)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is fairly well known that DPS can be unstable, so it is a good idea to develop a better choice for the step size/guidance strength. \n- For specific linear inverse problems the MAP surrogate can be efficiently evaluated \n- In all the experiments in Table 1 APS-DPS performs better than DPS and in most cases APS-$\\Pi$GDM performs better than $\\Pi$GDM"}, "weaknesses": {"value": "- The current presentation is limited to linear inverse problems with additive Gaussian noise. For non-linear forward operators or other noise types the surrogate for $d_t$ (Section 3.2.2) will become computationally expensive. \n- The calculation of $d_t$ requires to solve a linear system involving the forward operator $A$ at every sampling step. This might be feasible for certain types of forward operators (e.g., super-resolution or deblurring), but this \nbecomes expensive is the forward operator itself is more costly to evaluate (e.g., computed tomography)"}, "questions": {"value": "- The difference in the results in Table 2 between $\\xi_t=1$ and your method seem to be minor. Does this mean incorporating the $\\gamma_t$ in (18) has more importance? \n- Why is the bias correction needed? So, where do you introduce the bias? \n- Can you provide a plot of change of the step size $\\| d_t \\|_2$ over sampling iterations?\n- Can you add a comparison of the sampling time (as your method needs to compute $d_t$ at every iteration)?\n- How would this approach translate to latent diffusion models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cr4t2Aog1B", "forum": "QfZ5RqXl8q", "replyto": "QfZ5RqXl8q", "signatures": ["ICLR.cc/2026/Conference/Submission12640/Reviewer_Cc9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12640/Reviewer_Cc9K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639138876, "cdate": 1761639138876, "tmdate": 1762923483291, "mdate": 1762923483291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank the reviewers for the time and effort they dedicated to evaluating our submission. After careful consideration, we have decided to withdraw the paper.\n\nWe appreciate the reviewers’ insights and constructive feedback. At the same time, we feel that the central contribution of our work—introducing a simple, clearly-motivated, and hyperparameter-free approach for image reconstruction that requires neither finetuning nor task-specific modifications—may not have been fully recognized in the current submission and the reviews it got. Therefore, we decided to withdraw the paper and resubmit a strengthened version of the paper that better emphasize its unique and important contribution and also take into account the reviewers’ comments."}}, "id": "Mxde5Cx0Rm", "forum": "QfZ5RqXl8q", "replyto": "QfZ5RqXl8q", "signatures": ["ICLR.cc/2026/Conference/Submission12640/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12640/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763114044674, "cdate": 1763114044674, "tmdate": 1763114044674, "mdate": 1763114044674, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an adaptive likelihood stepsize estimation strategy for DPS and PGDM. The algorithm is tested on superresolution and deblurring tasks. The overall idea is interesting, but execution may be improved."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- I really like the overall idea of adaptively changing the step size based on existing iterates and residuals.\n- The adaptive algorithm shows good performance."}, "weaknesses": {"value": "- Like I said, I like the overall idea. But the main step that went into it is not clearly justified. Why is one trying to align the estimates toward the MMSE estimate of the posterior given x_t, y? This is going to be a bad estimate especially for high values of t (i.e. at higher noise level). There is no clear justification given for the objective in (15). \n- Similarly, there is a hand-wavy argument about doubling the scaling in (18), but this is neither justified nor empirically demonstrated.\n- The MAP surrogate is also not discussed in detail. Why is this a good solution? How does the solution quality change along the trajectory?\n- In terms of experiments, all problems considered are resolution related. Please include results from non-resolution related tasks for random (~90%) and box (128 \\times 128) inpainting.\n- It is unclear how many test datasets were used for the results reported. Usually a 1000 images are reported in these experiments.\n\nMinor:\n- In (1), please use matrix notation, as this idea only works for linear forward operators, and not non-linear ones.\n- It is not fair to say that other methods directly add \\xi_t g(y,x_t), since they define their \\xi_t to be equivalent to this paper's \\gamma_t \\xi_t.\n- LPIPS is typically reported using VGG (e.g. in DPS).\n- I'm not sure why the authors are saying PGDM is not publicly available. This is available in the RED-Diff repository. Also the VP-SDE to VE-SDE conversion is already covered in the PGDM paper Appendix A.1, this is a matter of scaling by \\sqrt{\\bar{\\alpha}_t}.\n- The authors highlight that they incorporate likelihood gradients into a DDIM sampler in a principled way as one of their three main contributions. However, the switch from Eq. 10 to 13 brings an additional factor of \\xi_t, which is not present in Eq. 10. I know this is used in all other works, but the inclusion of this variable hinders the derivation."}, "questions": {"value": "- What is the justification for trying to align the estimates toward the MMSE estimate of the posterior given x_t, y, i.e. the objective in (15)? \n- What is the justification for the scaling in (18)?\n- How good is the MAP solution for different parts of the trajectory?\n- Can you please include other relevant inverse problems that are not focusing on resolution improement, such as random (~90%) and box (128 \\times 128) inpainting?\n- How many test datasets were used in the reported results?\n- Why was LPIPS calculated using AlexNet instead of VGG, as in DPS or PGDM?\n\nI'm willing to increase my score if the authors address these issues sufficiently"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jEEUjmMy6h", "forum": "QfZ5RqXl8q", "replyto": "QfZ5RqXl8q", "signatures": ["ICLR.cc/2026/Conference/Submission12640/Reviewer_n43G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12640/Reviewer_n43G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878827709, "cdate": 1761878827709, "tmdate": 1762923482762, "mdate": 1762923482762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Posterior diffusion Sampling (APS), a method designed to automatically adjust the guidance strength in diffusion-based inverse problem solvers. Instead of using a manually tuned hyperparameter, APS introduces an adaptive scaling factor that depends on the alignment between the denoiser’s prediction and a likelihood-based correction term. The method is positioned as a hyperparameter-free alternative to approaches like DPS and ΠGDM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation to remove manual tuning of the guidance strength is clear and relevant to practical applications.\n2. Writing quality and presentation are good, making the method easy to follow.\n3. The paper provides meaningful ablations and visual examples to support its claims of reduced sensitivity to hyperparameters."}, "weaknesses": {"value": "1. The paper makes several simplifying assumptions that reduce mathematical rigor, the most notable being the elimination of the Jacobian term as discussed in Appendix A.1.2. The authors justify this choice by citing the DDS paper, which also omits Jacobians. However, DDS operates under a different geometric framework where the denoising dynamics are modeled along a locally affine approximation of the data manifold and the Tweedie denoised estimate is updated before each DDIM sampling step. In contrast, APS removes the Jacobian without adopting a comparable manifold assumption or analyzing how this omission affects posterior consistency. It is therefore unclear whether the Jacobian-free simplification remains theoretically valid in the DPS or ΠGDM setting.\n2. No inpainting task is reported, even though inpainting is a standard benchmark in the inverse problems community. Given the reliance on the no-Jacobian assumption, it is plausible that the method may not perform well on tasks such as inpainting, which are typically more challenging without the Jacobian term. Omitting these results raises concerns about the generality and robustness of the proposed approach.\n3. The proposed method is only demonstrated on inverse problems with linear and differentiable measurement operators. It remains unclear how APS would handle non-linear or non-differentiable operators such as JPEG compression, quantization, non-linear deblurring, or phase retrieval.\n4. Although APS defines its adaptive scale as $ξ_t^* = \\frac{<dt,gt>}{||gt||^2}$, the surrogate posterior correction $d_t$ is itself computed using the MAP estimator. Because both $d_t$ and $g_t$ depend on the same residual term ($\\mathbf{A}\\hat{\\mathbf{x}}_0−\\mathbf{y}$), the alignment $<dt,gt>$ effectively measures self-correlation rather than a genuine interaction between prior and likelihood information. Ultimately, it seems to me that the adaptive scale $ξ_t^*$ is not computed from two independent signals.\n5. The authors' claim of achieving “state-of-the-art” results is difficult to justify, as Table 1 shows that the proposed APS-DPS and APS-ΠGDM variants fail to consistently outperform even the relatively outdated baselines, achieving the best results in only about 10–15% of cases. Without comparisons to more recent methods (*e.g.*, RED-Diff, MGPS) or stronger baselines (*e.g.*, DDNM), it remains uncertain whether APS offers any real improvement over the current state of the art.\n6. The literature review does not acknowledge the ECCV 2024 paper (https://doi.org/10.1007/978-3-031-73010-8_26), which pursues the same overarching goal of adaptive posterior sampling but through a different mechanism. Although the strategy differs from APS, both aim to automate the balance between prior and likelihood terms in diffusion-based inverse problem solvers. Failing to mention this work gives the impression that APS is the first to address adaptive posterior sampling, when in fact related efforts already exist with complementary approaches.\n\n**Minor Comments:**\n- Vectors and matrices should be written in boldface to clearly distinguish them from scalar quantities.\n- Providing a structured algorithm in the main text would make the proposed method easier to follow and reproduce.\n- It is important to report the selected heuristic task-specific weighting ($\\omega_t$) used for DPS and ΠGDM in each experimental setup.\n- Although the authors state otherwise, ΠGDM code is publicly available (https://github.com/NVlabs/RED-diff/blob/master/algos/pgdm.py)."}, "questions": {"value": "1. *Regarding weakness 1:* Have the authors evaluated whether including the Jacobian, as in DPS, leads to any difference in adaptive guidance behavior or reconstruction quality?\n2. *Regarding weakness 3:* Since the derivation assumes linearity in the forward operator, how would the MAP surrogate behave when applied to non-linear or non-differentiable operators? Do the authors anticipate that APS would remain stable and effective in these cases?\n3. *Regarding weakness 4:* Can the authors clarify whether any component of $d_t$ is statistically independent of the likelihood gradient $g_t$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6LmuUv0KE0", "forum": "QfZ5RqXl8q", "replyto": "QfZ5RqXl8q", "signatures": ["ICLR.cc/2026/Conference/Submission12640/Reviewer_eyDA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12640/Reviewer_eyDA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938611588, "cdate": 1761938611588, "tmdate": 1762923482156, "mdate": 1762923482156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adaptive Posterior Sampling (APS), a method for dynamically adjusting the guidance scale when incorporating likelihood gradients into diffusion-based inverse problem solvers. The main idea is to adaptively determine the step size of the likelihood update based on the degree of alignment between two surrogate estimates of the posterior gradient: one derived from the denoiser (as in standard score-based models) and another approximated via a MAP formulation.\n\nThe approach is designed to be hyperparameter-free, theoretically consistent with DDIM’s time-scaling, and broadly applicable across different inverse problems. APS is evaluated on standard linear inverse tasks such as super-resolution, Gaussian debluring, and motion debluring, using CelebA-HQ and ImageNet-256 datasets. Experiments show improvements in perceptual quality (LPIPS) with competitive or slightly higher PSNR compared to prior methods such as DPS, ΠGDM, and DDRM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and technically sound. The derivations connecting Tweedie’s formula, MAP inference, and diffusion posterior updates are logically structured and transparent. The idea of adapting the likelihood step based on local posterior geometry is intuitive and elegantly motivated. The use of a Jacobian-free variant for computational efficiency is practical and well justified.\n\n2. The experiments are actively covering multiple datasets, degradation operators, and noise levels. Comparisons are fair, and ablations on the adaptive factor and surrogate estimators provide valuable insight. The reported results suggest consistent improvements in image perceptual quality with similar or lower computational cost."}, "weaknesses": {"value": "The paper is well written and technically correct, but the main idea feels more like a careful refinement of known methods than a new concept. The adaptive scaling rule is based on a simple idea, which is measuring how well two gradient estimates agree and adjusting the step size accordingly. This is elegant, but not deeply new.\n\n1. The method’s key equation is quite straightforward. It computes how much the denoiser gradient and likelihood gradient align, and then rescales the update based on this alignment. The paper claims this makes sampling more stable, but there is no strong analysis or evidence showing why this is the case. The improvement is more observed than explained.\n\n2. Although the experiments are thorough, the performance gains are relatively small. The LPIPS scores improve slightly and PSNR sometimes decreases. The visual results look good, but not dramatically better than existing methods. Without more insight into why the method works, it feels like a well-tuned variant rather than a major advance.\n\n3. The theoretical development assumes the problem is linear and the noise is Gaussian. This makes the math clean, but it limits the method’s generality. Many real inverse problems are nonlinear or learned (for example, when using a neural forward model), and it is not clear how APS could handle those cases."}, "questions": {"value": "1. Can you give more intuition or evidence for why aligning the denoiser and likelihood gradients helps? For example, does it make the sampling path smoother or reduce noise in the posterior updates?\n\n2. The method assumes a linear and Gaussian forward model. How could APS be extended to nonlinear or learned operators? Would the adaptive scaling still work?\n\n3. Could the adaptive scaling be understood as a form of adaptive learning rate control, similar to how optimizers like Adam adjust step sizes? Would that analogy help explain why it improves stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NvvAnDCDMe", "forum": "QfZ5RqXl8q", "replyto": "QfZ5RqXl8q", "signatures": ["ICLR.cc/2026/Conference/Submission12640/Reviewer_g99D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12640/Reviewer_g99D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762754571388, "cdate": 1762754571388, "tmdate": 1762923481873, "mdate": 1762923481873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}