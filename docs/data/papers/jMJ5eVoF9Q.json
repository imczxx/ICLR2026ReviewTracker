{"id": "jMJ5eVoF9Q", "number": 16039, "cdate": 1758258955249, "mdate": 1759897266098, "content": {"title": "DRIFT: Directional Reasoning Injection for Fine-Tuning MLLMs", "abstract": "Multimodal large language models (MLLMs) are rapidly advancing, yet their reasoning ability often lags behind that of strong text-only counterparts. Existing methods to bridge this gap rely on supervised fine-tuning over large-scale multimodal reasoning data or reinforcement learning, both of which are resource-intensive. A promising alternative is \\textit{model merging}, which interpolates parameters between reasoning-enhanced LLMs and multimodal variants. However, our analysis shows that naive merging is not always a ``free lunch'': its effectiveness varies drastically across model families, with some (e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance degradation. To address this, we propose Directional Reasoning Injection for Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning knowledge in the gradient space, without destabilizing multimodal alignment. DRIFT precomputes a reasoning prior as the parameter-space difference between reasoning and multimodal variants, then uses it to bias gradients during multimodal fine-tuning. This approach preserves the simplicity of standard supervised fine-tuning pipelines while enabling efficient reasoning transfer. Extensive experiments on multimodal reasoning benchmarks, including MathVista and MathVerse, demonstrate that DRIFT consistently improves reasoning performance over naive merging and supervised fine-tuning, while matching or surpassing training-heavy methods at a fraction of the cost.", "tldr": "", "keywords": ["reasoning capability", "data efficient", "multimodal large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/975a206d1335a333e88a3d551c828baf488c7b2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the weak reasoning ability of multimodal large language models (MLLMs) and the high cost of existing enhancement methods. It proposes DRIFT, which computes a reasoning prior as the parameter difference between a reasoning-capable LLM and an MLLM, then uses it to guide gradient updates during fine-tuning, effectively injecting reasoning skills while preserving multimodal alignment. Experiments show that DRIFT outperforms standard fine-tuning and naive model merging on benchmarks like MathVista and MathVerse, achieving performance comparable to costly methods at a fraction of the training expense."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is highly efficient in design; the paper provides extensive experiments to demonstrate its effectiveness."}, "weaknesses": {"value": "On Experimental Results:\n\n1. Some of the reported experimental results differ from those originally published in the paper. For instance, the performance of OpenVLThinker-7B on Mathvista has decreased from 72.3 to 65.3. What could be the reason for this discrepancy? It's crucial to clarify whether this is due to differences in experimental setup, dataset versions, or other factors.\n\n2. Experiments should also cover models of varying sizes, including both larger models (e.g., 32B) and smaller ones, to provide a comprehensive understanding of the method's scalability and performance across different capacities.\n\nOn Methodology:\n\n3. Previous works have similarly explored methods based on model gradients for merging. The manuscript should clearly articulate the technical distinctions and innovations of the current approach compared to these prior efforts, emphasizing what makes it unique or advantageous.\n\nOn Presentation Details:\n\n4. In Figure 2, the caption font size is too small, making it difficult to read. It is recommended to increase the font size for better readability.\n\n5. In Figure 4, the legend partially obscures the results, making it hard to discern where the advantages lie. Adjustments should be made to ensure all data points and comparisons are clearly visible and understandable.\n\n6. The formatting of Table 3's title appears incorrect, with abnormal spacing on the second line. This should be corrected to adhere to standard formatting guidelines for clarity and professionalism.\n\n[1] EDITING MODELS WITH TASK ARITHMETIC, 2023.03\n[2] FAST MODEL EDITING AT SCALE, 2022.06"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8MCBgV3VEV", "forum": "jMJ5eVoF9Q", "replyto": "jMJ5eVoF9Q", "signatures": ["ICLR.cc/2026/Conference/Submission16039/Reviewer_QU8a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16039/Reviewer_QU8a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834250302, "cdate": 1761834250302, "tmdate": 1762926238273, "mdate": 1762926238273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to narrow the gap in reasoning performance between MLLMs and their LLM counterparts. A method called DRIFT is proposed which conditions gradient updates during training on the difference in parameter space between the MLLM and the LLM. Experiments are conducted which show that DRIFT achieves better improvements in reasoning performance than alternative methods such as model merging while avoiding degradation in multimodal alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work addresses an important and timely problem, which is mitigating the gap in reasoning capabilities seen in MLLMs relative to the LLMs from which they were derived.\n2. Conditioning gradient updates on the difference in parameter space between MLLMs and LLMs is an interesting and novel approach (to the best of my knowledge).\n3. Several interesting analyses are provided investigating the usefulness of reasoning, the role of merging candidates, and merging strategies."}, "weaknesses": {"value": "1. The discussion of related work contains errors - Ratzlaff et al. (2025) is a training-free model merging technique that merges LLM parameters into an MLLM. L104-106 incorrectly states that it is an instruction tuning technique. \n2. The experimental results focus exclusively on math & logical reasoning tasks. A broader range of reasoning tasks here would help in assessing how well the method generalizes. \n3. DRIFT seems to offer no or only negligible improvement relative to SFT on 2/5 datasets (MathVista and LogicVista).\n4. The authors make the case that model merging methods fail to offer any improvements for MLLMs (Table 2), but it's unclear if any tuning of merging parameters was done for these experiments, which can significantly impact the results."}, "questions": {"value": "1. In collecting the dataset (section 4.1), was any analysis done to verify the accuracy of the reasoning chains?\n2. Did you try tuning merging parameters for any of the evaluated model merging techniques? \n3. L259-260: why exactly is it necessary to store all candidate models in GPU memory when trying to optimize the merging strength? Couldn't different models be merged with different parameter values and evaluated separately on a validate set to choose the optimal value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fvvKl8trVL", "forum": "jMJ5eVoF9Q", "replyto": "jMJ5eVoF9Q", "signatures": ["ICLR.cc/2026/Conference/Submission16039/Reviewer_a7Fe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16039/Reviewer_a7Fe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960525879, "cdate": 1761960525879, "tmdate": 1762926237892, "mdate": 1762926237892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DRIFT, a lightweight method to transfer reasoning capabilities from text-only language models to multimodal large language models (MLLMs) without destabilizing their visual alignment. DRIFT computes a \"reasoning direction\" as the parameter difference between a reasoning-rich text model and a multimodal model, then uses it to bias gradients during fine-tuning. It achieves strong performance on multimodal reasoning benchmarks like MathVista and MathVerse using only ~4K training examples and ~2 hours of training, outperforming other parameter merging and rivaling data-heavy methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Instead of merging parameter, DRIFT injects reasoning priors into gradients space.\n- Clearly motivates the problem, explains the fragility of naive merging, and presents DRIFT as a simple yet effective alternative."}, "weaknesses": {"value": "- Limited generalization: Only tested on Qwen2.5-VL-7B; needs validation on LLaVA, Idefics, InternVL, and larger scales (14B–72B).  \n- Interpretability of Δ: No evidence that parameter difference encodes reasoning, not just task-specific drift.  \n- Avg. gain mismatch: Table 2 claims +6.0 mean vs. max +5.4 per benchmark—please clarify.  \n- Transfer efficiency: DRIFT gains are modest vs. large lift from DeepSeek-R1-Qwen-Distill-7B on text model; how to evaluate the efficiency of transferring reasoning ability?"}, "questions": {"value": "- Can Δ be adapted during training? Have you tried dynamic priors (e.g., updating Δ via exponential moving average) to handle evolving reasoning needs?\n- Scalability to larger models? Any preliminary results on 14B/32B/72B models? Does parameter divergence grow linearly, or does DRIFT need adjustment?\n- Interpretability of injected reasoning? Why the difference between text-only reasoning model and mllms indicates the priors to guide optimization? For example, can you visualize which attention heads are most influenced by Δ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XAFcLUc7L4", "forum": "jMJ5eVoF9Q", "replyto": "jMJ5eVoF9Q", "signatures": ["ICLR.cc/2026/Conference/Submission16039/Reviewer_iacC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16039/Reviewer_iacC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993566562, "cdate": 1761993566562, "tmdate": 1762926237466, "mdate": 1762926237466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles an important problem: MLLMs typically lag behind pure text-based LLMs in reasoning ability. Existing approaches based on SFT or RL are computationally expensive, while seemingly cheaper alternatives such as model merging are highly unstable (as convincingly demonstrated in Section 3.2, especially on the Qwen models).\n\nTo address this, the authors propose DRIFT (Directional Reasoning Injection for Fine-Tuning), a lightweight gradient-space injection method. DRIFT precomputes a reasoning prior vector ($\\Delta = \\phi_{reason} - \\phi_{VL}$), and during SFT, biases the gradient updates using this vector ($\\tilde{g} = g + \\alpha \\cdot \\text{scale}(g, \\Delta)$). The authors claim that this approach is both data- and compute-efficient (requiring only 4K samples and about two hours of training), while achieving state-of-the-art or competitive results on mathematical reasoning benchmarks such as MathVista."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has a clear motivation and addresses a key problem in the MLLM domain. It analyzes the effect of naive model merging on enhancing MLLM reasoning ability and demonstrates the fragility of such merging methods.\n2. The proposed DRIFT method is innovative: instead of performing unstable parameter-space merging, it guides learning in the gradient space, providing a gentler way to inject reasoning capability. The method is also efficient in terms of both data.\n3. DRIFT outperforms naive merging methods on reasoning benchmarks, and the model trained with ThinkLite surpasses its SFT counterpart on mathematical reasoning tasks."}, "weaknesses": {"value": "1. The core motivation of the paper is to demonstrate that model merging fails on the Qwen series but works effectively on LLaMA- and Mistral-based models. However, in the main experiments, the authors only conduct studies based on Qwen models. The authors should at least include experiments on one model family where merging is effective to validate the generality of DRIFT.\n2. The paper evaluates models only on mathematical reasoning tasks. While math is a representative form of reasoning, reasoning is not limited to mathematics. It is recommended to include evaluations on non-mathematical visual reasoning tasks.\n3. The authors claim that DRIFT “transfers reasoning knowledge without destabilizing multimodal alignment”, but this claim lacks direct evidence, as all evaluations are conducted on mathematical tasks. It remains unclear whether the general multimodal perception ability is destabilized.\n4. The training of DRIFT is based on the high-quality ThinkLite dataset, and models trained with ThinkLite alone already achieve near SOTA performance. DRIFT provides only about a 2 point average improvement on top of this, which somewhat weakens its overall contribution."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XCMPLX5wC1", "forum": "jMJ5eVoF9Q", "replyto": "jMJ5eVoF9Q", "signatures": ["ICLR.cc/2026/Conference/Submission16039/Reviewer_9SMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16039/Reviewer_9SMS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163925945, "cdate": 1762163925945, "tmdate": 1762926236905, "mdate": 1762926236905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}