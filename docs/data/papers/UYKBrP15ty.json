{"id": "UYKBrP15ty", "number": 12574, "cdate": 1758208730356, "mdate": 1759897500970, "content": {"title": "Variance-Reduced Reinforcement Learning for Large Reasoning Models via James-Stein Baselines", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is becoming an impactful paradigm in large reasoning model (LRM) post-training.\nTo stabilize training, control variates (baselines) are commonly introduced, canonically chosen to approximate the value function. Popular approaches such as RLOO and GRPO estimate baselines with per-prompt empirical\naverages of generated response, which can exhibit high variance under limited rollout budgets. Recognizing that value functions must be estimated simultaneously across all prompts in a batch, we propose a James–Stein estimator as the baseline. This approach leverages statistical shrinkage to reduce the mean squared error in the overall value function estimation, without additional computational overhead while maintaining the unbiasedness of the policy gradient estimator. We provide theoretical justification for James-Stein baselines and validate it empirically. Across diverse models, tasks, and rollout budgets, our approach consistently outperforms existing baselines, demonstrating robust variance reduction and improved training stability.", "tldr": "By applying James-Stein shrinkage to the baseline, we reduce the variance of policy gradient and improve reinforcement learning of large reasoning models.", "keywords": ["Large Reasoning Model", "Reinforcement Learning", "Variance Reduction"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ad6bd0a792112001d6001c862b2fa1bfb941e77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a new variance-reduced technique for reinforcement learning with verifiable rewards (RLVR), inspired by the James-Stein shrinkage principle. The proposed method, called James-Stein Policy Optimization (JSPO), aims to reduce the variance of policy gradient estimates in RLVR settings without introducing additional computational overhead. By leveraging statistical shrinkage, the JSPO baseline trades a small amount of bias for a significant reduction in mean squared error (MSE), leading to improved training stability and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The introduction of the James-Stein estimator as a baseline for variance reduction is novel, and the approach is easy to implement. The authors provide theoretical justification for the use of the James-Stein estimator, demonstrating its effectiveness in reducing policy gradient variance theoretically and empirically."}, "weaknesses": {"value": "The notation in the paper is overwhelming and can be confusing. It is not always clear which random variable the expectations and variances are being taken with respect to. A table summarizing the key notations and their contexts would be helpful for clarity. For other questions please refer to the following. I would be willing to raise  my  score if the authors address my confusion regarding these points."}, "questions": {"value": "1. What is the exact definition of $b_i^j$ in equ (4) and elsewhere in the paper? It would be beneficial to give a clear expression.\n2. In Line 185, is $\\mathbb{E}_{Y}[b_i^j]=\\mathbb{E}[b_i^j]$ satisfied? In my opinion, the LHS take expectation with respect to the response $y_j$ following the $\\pi(\\cdot|x_i)$, while the RHS appears to take expectation over both the response $y^j_i$ and the prompt $x_i$. Could you clarify if this equality holds?\n3. The James-Stein estimator is derived under assumptions that are closely tied to **Gaussian distributions**. How does this approach behave when reward distributions are skewed or multimodal, which are common in complex reasoning tasks? Would the shrinkage still yield a reduction in mean squared error (MSE) under these conditions ?\n4. RLVR typically involves sparse rewards, with feedback provided at the end of a reasoning process. How do the James-Stein estimator, address the challenges posed by sparse rewards, or how does this impact the learning efficiency and stability of the model in sparse reward settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns are apparent in the paper."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pH0FUwGrbo", "forum": "UYKBrP15ty", "replyto": "UYKBrP15ty", "signatures": ["ICLR.cc/2026/Conference/Submission12574/Reviewer_2gEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12574/Reviewer_2gEZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496967383, "cdate": 1761496967383, "tmdate": 1762923427007, "mdate": 1762923427007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes James–Stein Policy Optimization (JSPO): a new baseline (control-variate) for RL with verifiable rewards (RLVR) used to fine-tune large reasoning models. The key idea is to estimate each prompt’s value not in isolation (per-prompt mean, as in RLOO/GRPO), but jointly across the whole batch via James–Stein shrinkage toward the batch mean, implemented with a two-level leave-one-out construction to keep the policy-gradient estimator unbiased. Theoretical claims include: (i) Proposition 1—unbiasedness of the gradient when the per-sample baseline is independent of the reward; (ii) a bias–variance decomposition showing why prompt-only means are inadmissible under batch MSE; (iii) Proposition 2/Theorem 1—a closed-form optimal shrinkage coefficient that trades off within-prompt noise vs. across-prompt variability and reduces policy-gradient variance while preserving unbiasedness. Empirically, JSPO improves Pass@1 on math/logic benchmarks over RLOO/GRPO and reduces estimated gradient-variance by ~11–32% across several models and rollout budgets (2/4/8 generations). The method is drop-in, has negligible overhead, and no extra hyperparameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear, principled objective: Reduces policy-gradient variance by reframing baseline design as multi-task value estimation with an MSE criterion; the James–Stein angle is elegant and well-motivated. \nUnbiasedness preserved: The two-level leave-one-out construction (per-prompt and per-batch) keeps baselines independent of the held-out reward, matching the REINFORCE unbiasedness requirement. \nConsistent empirical gains: Improvements across tasks (MATH500, OlympiadBench, AMC/logic puzzles; also GSM8K with 2/4/8 rollouts), plus direct gradient-variance tracking that aligns with the theory. \nPracticality: Critic-free, tiny code change (a few lines), batch-only statistics, compatible with existing RLVR stacks"}, "weaknesses": {"value": "Ablations could be deeper: How sensitive are outcomes to batch size n and rollouts m beyond the tested grid? What happens with non-binary, dense, or scale-shifted rewards?\n\nDistributional assumptions not stress-tested: James–Stein style shrinkage helps when prompts share a latent mean structure. If batch prompts are intentionally heterogeneous (mixed tasks/difficulties), shrinkage could over-bias the baseline; the paper largely uses in-distribution batches.\n\nGeneralization beyond reasoning tasks: All experiments target math/logic puzzles; evidence on coding, tool-use, or longer-horizon tasks would strengthen claims of generality."}, "questions": {"value": "Robustness of λ̂: Do you use any clipping or shrinkage-to-zero floors/ceilings for λ̂ to avoid instability with small n or heavy-tailed rewards?\n\nIn Figure 3, JSPO’s improvement over RLOO on Countdown is noticeably smaller than the other three tasks. Could you diagnose why?\n\nTable 1 (fairness across methods): Are decoding/hyperparameters (temperature, top-p, token limits) identical for ReMax, REINFORCE++, GRPO, RLOO, BLOO, and JSPO? If any differ, please list them, since small decoding shifts can change GSM8K accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PIYgRljYpa", "forum": "UYKBrP15ty", "replyto": "UYKBrP15ty", "signatures": ["ICLR.cc/2026/Conference/Submission12574/Reviewer_e9ve"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12574/Reviewer_e9ve"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943442908, "cdate": 1761943442908, "tmdate": 1762923426643, "mdate": 1762923426643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a James–Stein–based baseline for policy-gradient training of large reasoning models (RLVR). By shrinking per-prompt reward means toward a batch mean—via a leave-one-out construction to preserve unbiasedness—the method aims to reduce gradient variance and thereby stabilize training. The authors provide a derivation linking variance of the policy-gradient estimator to MSE of the baseline, prove an optimal shrinkage coefficient, and give an implementation that adds negligible overhead. Experiments on math and logic-puzzle benchmarks show consistent accuracy gains and lower estimated gradient variance compared to RLOO/GRPO-style baselines under multiple rollout budgets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper cleanly reduces policy-gradient variance control to estimating a value-function baseline, motivating James–Stein shrinkage and proving an optimal (data-driven) coefficient with an unbiased leave-one-out construction.\n\nThe baseline is easy to add to existing critic-free RLVR pipelines and is presented with concise pseudo-code."}, "weaknesses": {"value": "While well-executed, the paper extends a long line of “better baseline” work; the novelty is primarily in bringing James–Stein shrinkage to RLVR with careful LOO plumbing, rather than introducing a new learning paradigm.\n\nEvidence is confined to RLVR-style reasoning tasks (math/puzzles); there is no evaluation on broader RL settings where baseline design has also been heavily studied, which limits generality claims."}, "questions": {"value": "When batch prompts are heterogeneous, performance may revert toward LOO means. Can you provide a runtime diagnostic and an automatic rule for disabling or annealing shrinkage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BRw7krlkAj", "forum": "UYKBrP15ty", "replyto": "UYKBrP15ty", "signatures": ["ICLR.cc/2026/Conference/Submission12574/Reviewer_YBJe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12574/Reviewer_YBJe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972204273, "cdate": 1761972204273, "tmdate": 1762923426332, "mdate": 1762923426332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a variance-reduced baseline for policy gradient methods in reinforcement learning with verifiable rewards (RLVR) on large reasoning language models. The central idea is to use a James-Stein-type shrinkage estimator as a control variate in policy gradient updates, adaptively blending per-prompt and global-batch mean reward estimates. The method, termed JSPO, is theoretically motivated to reduce mean squared error and, via a carefully constructed leave-one-out strategy, maintains unbiased gradients. The paper offers both theoretical justification and extensive experiments showing JSPO’s benefits over existing baselines in reducing gradient variance and improving downstream performance on mathematical and logical reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 The utilization of the James-Stein estimator builds directly on well-understood results in statistics, and the paper provides thorough derivations connecting batch-level value function estimation to baseline variance reduction (see especially Section 3.3 and Theorem 1, page 6).\n\n2 The leave-one-out adaptation of the shrinkage estimator to RLVR is both elegant and practically impactful, ensuring unbiased gradients while achieving variance benefits. The mathematical treatment is careful, with Proofs of Proposition 1 and Theorem 1 provided in Appendix B.\n\n3 SPO outperforms RLOO, GRPO, ReMax, and BLOO across all tested rollout regimes, including challenging low-rollout settings where baseline variance is most limiting"}, "weaknesses": {"value": "1 The methodology, though thoughtfully adapted, is essentially a direct application of classical James-Stein shrinkage (as in James et al., 1961 and Stein et al., 1956) to RLVR policy gradient baselines. While the adaptation—especially the unbiased leave-one-out variant—is useful, the paper could more rigorously discuss the theoretical/empirical boundaries between what is gained by the JSPO version versus more general empirical Bayes or shrinkage estimators (see Feldman et al., 2014; Efron & Morris, 1973; Brown, 1971).\nThe related work section does not discuss these key foundational works adequately, which reduces the clarity on conceptual originality (see the \"Potentially Missing Related Work\" section below).\n\n2 While the experiments are broad, the scope is still largely limited to math and logic reasoning tasks. There is insufficient evidence to claim generality across different RLVR domains (e.g., for instructions, summarization, or control tasks).\n\n3 There is no ablation studying what happens if rollouts are highly non-i.i.d. across prompts, nor is there an analysis of the estimator’s robustness to reward sparsity or distributional shift."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "D1vd3kJIu0", "forum": "UYKBrP15ty", "replyto": "UYKBrP15ty", "signatures": ["ICLR.cc/2026/Conference/Submission12574/Reviewer_tdVg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12574/Reviewer_tdVg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977238255, "cdate": 1761977238255, "tmdate": 1762923425940, "mdate": 1762923425940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}