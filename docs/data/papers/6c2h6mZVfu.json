{"id": "6c2h6mZVfu", "number": 16019, "cdate": 1758258667495, "mdate": 1763727796853, "content": {"title": "SF-PE: A Synergistic Fusion of Absolute and Relative Positional Encoding for Spiking Transformers", "abstract": "Positional signals in spiking neural networks (SNNs) suffer distortion due to spike binarization and the nonlinear dynamics of Leaky Integrate-and-Fire (LIF) neurons, which compromises self-attention mechanisms. We introduce Spiking-RoPE, a spiking-friendly relative rotary positional encoding that applies two-dimensional spatiotemporal position-dependent rotations to queries/keys prior to\nbinarization, ensuring that relative phase kernels are preserved in statistical expectation under LIF dynamics while maintaining content integrity. Building on this core, we propose Spiking Fused-PE (SF-PE), a scheme that fuses absolute CPG-based spikes with Spiking-RoPE. The resulting attention score decomposes into complementary row/column (absolute) and diagonal (relative) structures, thereby expanding the representable function space. We validate our method across two diverse domains (time-series forecasting and text classification) on Spikformer, Spike-driven Transformer, and QKFormer backbones. SF-PE consistently improves accuracy and enhances length extrapolation capabilities. Ablations on rotation bases and 1D vs. 2D variants support the design. These results establish rotary encoding as an effective, spiking-friendly relative PE for SNNs and demonstrate that fusing absolute and relative signals yields synergistic benefits under spiking constraints. Code: https://anonymous.4open.science/r/SNN-RoPE-F6DE.", "tldr": "", "keywords": ["Spiking Neural Networks (SNNs)", "Rotary Positional Encoding (RoPE)", "Central Pattern Generator (CPG)", "Neuromorphic Computing", "Time-series Forecasting", "Text Classification"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4020b4bba6af967fd5565797c64abb8a28f8bf73.pdf", "supplementary_material": "/attachment/d5d18b6fd03c97286b73fc8b314d138bb54e008c.pdf"}, "replies": [{"content": {"summary": {"value": "The paper studies how spike binarization and LIF dynamics distort positional signals in spiking Transformers, weakening self-attention. \nIt introduces **Spiking-RoPE**, which applies rotary positional encoding to queries/keys *before* binarization so relative phase information is preserved under LIF. \nA **2-D Spiking-RoPE** decouples sequence and time rotations to explicitly model spatiotemporal relations. \nTo combine absolute and relative cues, the authors fuse CPG-based absolute spikes with Spiking-RoPE into **Spiking Fused-PE (SF-PE)**, yielding complementary row/column vs. diagonal attention patterns. \nA theoretical analysis supports pre-spike rotations as intrinsically compatible with LIF, and experiments across time-series forecasting and text classification on multiple spiking backbones show consistent improvements and better length extrapolation. \nComprehensive ablations on rotation bases and 1-D vs. 2-D variants corroborate the design’s robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper redesigns RoPE specifically for SNNs by applying it pre-spike, extends it to a 2-D spatiotemporal variant, and fuses absolute CPG-PE with relative Spiking-RoPE into SF-PE to capture complementary row/column and diagonal structures. It provides a statistical-expectation analysis showing phase preservation through LIF, includes thorough ablations (1-D vs 2-D; RoPE base), and reports solid results across time-series and text on multiple spiking backbones. The method is clearly motivated, the pre-spike rotation pipeline is well articulated with equations and design rationale, and assumptions are explicitly stated."}, "weaknesses": {"value": "1) The paper motivates Spiking-RoPE as a **pre-spike** operation (before LIF binarization) to justify “phase preservation under LIF.” In the released code (`spikformer_cpg_rope.py`, self-attention forward), Q/K pass through `self.q_lif` / `self.k_lif` **first**, and only then `apply_spiking_rotary_pos_emb(...)` is applied—i.e., RoPE is **post-spike**. Under this code path, the pre-spike theoretical claim does not hold as implemented.\n\n2) The manuscript emphasizes 2D spatiotemporal rotation and fusing absolute CPG with Q/K inputs. The public model instantiates a 1D rotary embedding (`RotaryEmbedding1DSpatial`) and injects CPG via `CPGLinear` on the encoder path rather than at Q/K. If this is intended to be equivalent, that needs justification; otherwise it should be documented as an implementation deviation or the stated 2D variant should be released."}, "questions": {"value": "Q1. The paper argues for **pre-spike** RoPE; the released code implements **post-spike**. Which path produced the reported results? If post-spike was used, please revise the theory accordingly; if pre-spike was used, provide the corresponding implementation and report a direct pre- vs post-spike comparison.\n\nQ2. Current code uses `RotaryEmbedding1DSpatial` and fuses CPG via `CPGLinear` on the encoder path rather than at Q/K. (i) Are these implementations theoretically equivalent to the paper’s formulas? If not, please mark as a deviation. (ii) Release and evaluate the full **2D** variant and the **Q/K-side fusion** described in the manuscript, with side-by-side results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WRJ91II74S", "forum": "6c2h6mZVfu", "replyto": "6c2h6mZVfu", "signatures": ["ICLR.cc/2026/Conference/Submission16019/Reviewer_LdHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16019/Reviewer_LdHq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760932269563, "cdate": 1760932269563, "tmdate": 1762926223971, "mdate": 1762926223971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Spiking Fused-Positional Encoding (SF-PE) for spiking transformers, which integrates absolute positional encoding (CPG-PE) and relative positional encoding (Spiking-RoPE) to resolve the distortion of positional signals in spiking neural networks (SNNs)—a problem caused by spike binarization and the nonlinear dynamics of Leaky Integrate-and-Fire (LIF) neurons. The authors provide theoretical proof that Spiking-RoPE preserves relative phase kernels in statistical expectation under LIF dynamics, and validate SF-PE across two tasks (time-series forecasting and text classification) and three spiking backbones (Spikformer, SDT-V1, QKFormer). Results show consistent accuracy improvements and enhanced length extrapolation, with ablations supporting the design of 2D Spiking-RoPE and rotation bases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It rigorously proves that Spiking-RoPE preserves relative phase kernels in statistical expectation under LIF dynamics, addressing the lack of theoretical analysis for positional information preservation in existing SNN transformers (Gap 1).\n2. The 2D Spiking-RoPE explicitly models spatiotemporal relationships by decoupling sequence and time axes, solving the limitation of most PEs treating position as one-dimensional (Gap 3), and ablation experiments confirm its superiority over 1D variants."}, "weaknesses": {"value": "1. I checked the code and found that the authors merely fused the RoPE code crudely into Spiking Self-Attention without analyzing the properties of spikes; theoretically, RoPE cannot work on binary matrices, so I am skeptical about the performance improvements claimed by the authors.\n2. The theoretical analysis of Spiking-RoPE’s phase preservation under LIF dynamics relies on the strong assumption that the firing probability function of LIF neurons operates almost linearly, yet the paper provides no verification of the validity range or error bounds of this assumption.\n3. In the text classification experiments, the paper shows that neither CPG-PE nor SF-PE improves performance on the RTE task, but it fails to analyze the reasons for this task-specific ineffectiveness (e.g., whether it is related to spatiotemporal modeling or relative position encoding).\n4. The paper claims that SF-PE enhances length extrapolation capabilities, but it only mentions the analysis in Appendix D without presenting key results (e.g., performance degradation trends on sequences longer than training lengths) in the main text."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P9n9dXIKJR", "forum": "6c2h6mZVfu", "replyto": "6c2h6mZVfu", "signatures": ["ICLR.cc/2026/Conference/Submission16019/Reviewer_zk6P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16019/Reviewer_zk6P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825857192, "cdate": 1761825857192, "tmdate": 1762926223552, "mdate": 1762926223552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel method that combines Spiking-RoPE with CPG positional encoding in spiking neural networks, resulting in a fused position encoding mechanism called Spiking Fused-PE. The approach first injects absolute positional information from CPG-PE into token embeddings via linear projection, then applies a two-dimensional Spiking-RoPE rotation to encode relative positions along both spatial (sequence) and temporal dimensions. Following this 2D rotation, the attention computation is decomposed into amplitude terms carrying absolute information and trigonometric terms encoding relative positional differences. The goal of this design is to jointly capture absolute and relative positions across space and time, yielding richer and more structured spatiotemporal representations. Experiments demonstrate the effectiveness of Spiking Fused-PE through quantitative performance gains and ablation studies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper extends the RoPE mechanism to the temporal dimension of spiking neural networks and integrates it with CPG absolute positional encoding to form a new positional encoding scheme. This combination is somewhat novel. Additionally, the paper provides detailed mathematical derivations, and includes visual illustrations of the Spiking-RoPE computation process in the appendix, which together enhance clarity and facilitate understanding."}, "weaknesses": {"value": "1) The paper’s overall innovation appears somewhat limited. It mainly extends RoPE to the time-step dimension and then merges it with CPG positional encoding, which results in a method that integrates existing ideas rather than introducing a fundamentally new mechanism. Although the proposed Spiking Fused-PE achieves good performance, it lacks deeper methodological novelty or theoretical breakthroughs. \n\n2) The paper does not explore other potential fusion strategies between CPG and Spiking-RoPE. For instance, it remains unclear whether the CPG component must be injected only at the input embedding stage or if it could be integrated after the 2D RoPE rotation or within later layers, which might lead to different positional interaction effects.\n\n3) The experimental section lacks sufficient discussion of computational cost. Specifically, Table 2 reports identical inference times across all compared methods, including SF-PE, which theoretically requires additional computation for the 2D RoPE operation. This inconsistency raises concerns about the rigor of the experimental setup and whether the runtime measurements were properly controlled or averaged."}, "questions": {"value": "1) Could the authors explore whether CPG can be applied at other stages of the model. For example, after the 2D RoPE rotation or within later processing blocks rather than being limited to the input x? Would such variations affect the encoding of absolute and relative positional information?\n\n2) In Table 2, why are the inference times completely identical for all three methods? Since SF-PE introduces additional 2D RoPE computations on top of CPG-PE, one would expect a measurable increase in inference time. How were these runtimes obtained and were they averaged over multiple runs, and was the hardware or implementation optimized in a way that masked the computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TVdicOvEU9", "forum": "6c2h6mZVfu", "replyto": "6c2h6mZVfu", "signatures": ["ICLR.cc/2026/Conference/Submission16019/Reviewer_nkFe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16019/Reviewer_nkFe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886573317, "cdate": 1761886573317, "tmdate": 1762926223164, "mdate": 1762926223164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}