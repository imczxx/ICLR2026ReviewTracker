{"id": "1QANBDkeln", "number": 1984, "cdate": 1756974417821, "mdate": 1759898175175, "content": {"title": "UES: An Ultra-expanded Semantic Space for Unsupervised Domain Adaptation", "abstract": "Unsupervised domain adaptation (UDA) provides a promising solution for label \nannotation and dataset bias issues by facilitating knowledge transfer \nfrom a label-rich source domain to a related but unlabeled target \ndomain. FC+Softmax+CE loss is the most commonly used supervised \ncomponent in discriminative models. While it demonstrates unparalleled \nadvantages under IID assumption, it fails to construct a metric \nspace with better generalization performance. Therefore, under \nthe non-IID assumption in UDA, the target domain features often \nexhibit large intra-class variations, which can easily cross decision \nboundaries and lead to inter-class confusion. To tackle this issue, \nwe propose an innovative margin loss, Ultra-expanded (UE) loss, to \nencourage feature to occupy a more expansive space, thereby keeping \nthem farther from decision boundaries. Furthermore, we propose a \nsemantic zoom mechanism that enables the network to preserve key semantics. The combined Ultra-expanded Semantic (UES) loss constructs \nan Ultra-expanded semantic space and can be readily integrated into \nvarious UDA methods as a regularization term. Extensive experiments \ndemonstrate its absolute performance improvements.", "tldr": "In UDA, the farther a feature is from the decision boundary, the better.", "keywords": ["Unsupervised Domain Adaptation", "Transfer Learning", "Margin loss"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f144ac8bf3a4be3ad415953d5fe9b5eb97c8f48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes constructing an ultra-expanded semantic space for unsupervised domain adaptation (UDA) to improve both intraclass compactness and interclass separability. Unlike conventional margin-based methods that risk decision boundary violations and negative transfer, the approach pushes features away from decision boundaries to create a more generalizable metric space. It is easy to implement, compatible with existing feature alignment techniques, and achieves consistent performance improvements and enhanced testing stability."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. It seems that the loss could be combined with any feature distribution alignment technique, showing its high generality and compatibility.\n2. Simple design allows straightforward integration into existing models.\n3. The paper is well-motivated and seems to be reproducible."}, "weaknesses": {"value": "I believe this paper has two main limitations:\n1. Although the authors introduce the proposed Ultra-expanded (UE) loss with good motivation, there seems to be a lack of in-depth theoretical discussion and analysis. As a result, readers may find it difficult to gain a truly insightful understanding. Please refer to [A].\n2. The experiments are based on relatively outdated baselines. Improvements over these older methods may still fall short of the performance achieved by more recent approaches, even including zero-shot methods. It remains unclear whether the proposed methods would remain effective when applied to newer methods, especially those leveraging pretrained or large-scale models.\n\nOther issues include:\n1. The paper uses overly complex and non-standard notation, and contains several typos, which would benefit from careful proofreading.\n2. The comparative experiments are somewhat limited, both in terms of baselines, datasets, and experimental settings.\n\n[A] Xu, Gezheng, et al. \"Revisiting Source-Free Domain Adaptation: a New Perspective via Uncertainty Control.\" The Thirteenth International Conference on Learning Representations. 2024."}, "questions": {"value": "I hope the authors could provide further clarification or responses regarding the theoretical analysis and experimental settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XH9aDuPDMM", "forum": "1QANBDkeln", "replyto": "1QANBDkeln", "signatures": ["ICLR.cc/2026/Conference/Submission1984/Reviewer_pixb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1984/Reviewer_pixb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668514654, "cdate": 1761668514654, "tmdate": 1762915984732, "mdate": 1762915984732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a well-motivated and clearly written contribution to UDA. The idea of expanding classifier weights into an “ultra-expanded semantic space” is elegant and empirically validated. Although the method is simple, it provides notable and consistent gains over classical baselines with minimal complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed UES loss is conceptually simple yet effective. The theoretical motivation and geometric interpretation are clearly articulated, and the visualizations convincingly show its ability to enhance intra-class compactness and inter-class separability across domains.\n2.The method introduces very few hyper-parameters, shows strong robustness to them, and can be easily integrated as a plug-and-play regularizer into existing UDA frameworks.\n3.The Semantic Zoom mechanism is intuitively reasonable: using different temperatures for source and target domains is a neat way to retain semantically meaningful information while mitigating domain bias."}, "weaknesses": {"value": "1.While the chosen alignment backbones (DAN, DANN, DeepCORAL, DAAN) are classic and representative, they are relatively old. The paper lacks comparison with more recent or stronger baselines, making it difficult to assess the broader applicability of UES in modern settings.\n2.The margin-loss comparison focuses on Softmax, ArcFace, and Center Loss, which are also dated. The effectiveness of UES against more advanced discriminative losses remains unclear.\n3.The Geometric Interpretation in Section 4.1 is insightful and explains how Wbasis enlarges the margin and thus enhances robustness. However, the analysis remains primarily at a geometric and empirical level, lacking a formal connection to existing domain generalization bounds or theoretical guarantees. Strengthening this link would substantially improve the paper’s rigor."}, "questions": {"value": "1.The Semantic Zoom module employs different softmax temperatures. How sensitive is the approach to this setting, and would a learnable or adaptive temperature improve performance further?\n2.Since UES constructs an expanded feature space, does it risk over-dispersion or feature instability in high-dimensional domains?\n3.The experiments show that the method consistently improves accuracy and reduces A-distance, but it remains unclear how UES interacts with domain alignment losses (e.g., adversarial or moment matching). Does UES primarily benefit from better intra-domain discrimination, or does it also implicitly reduce inter-domain discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wKLIYrNbbH", "forum": "1QANBDkeln", "replyto": "1QANBDkeln", "signatures": ["ICLR.cc/2026/Conference/Submission1984/Reviewer_kizw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1984/Reviewer_kizw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916132016, "cdate": 1761916132016, "tmdate": 1762915984431, "mdate": 1762915984431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UES (Ultra-Expanded Semantic Space), a new loss formulation for Unsupervised Domain Adaptation (UDA). The authors claim that this loss can serve as a plug-and-play regularizer for existing UDA frameworks such as DANN, DAN, DeepCORAL, and DAAN. Experiments on three benchmark datasets (Digits, Office-31, Office-Home) show improved accuracy compared to ArcFace and Center Loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper attempts to design a simple, general-purpose regularizer applicable across UDA frameworks.\n2. The figures in this paper are easy to understand."}, "weaknesses": {"value": "1. All baselines (DANN, DAN, DeepCORAL, DAAN) are from 2015–2019. Missing modern methods makes the claimed superiority unconvincing.\n2. The related work section is outdated and clearly misses recent advances in UDA from the past few years.\n3. The experiments are limited to relatively simple datasets and lack evaluations on more challenging benchmarks such as DomainNet, which is commonly used in UDA research.\n4. The writing quality requires further improvement. The paper should be reorganized for better readability, and the main experiments should be integrated into the main text rather than placed in the appendix."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mAZ6Xo6s0c", "forum": "1QANBDkeln", "replyto": "1QANBDkeln", "signatures": ["ICLR.cc/2026/Conference/Submission1984/Reviewer_kFiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1984/Reviewer_kFiE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923336127, "cdate": 1761923336127, "tmdate": 1762915983989, "mdate": 1762915983989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an innovative approach for Unsupervised Domain Adaptation (UDA) by introducing the Ultra-expanded Semantic (UES) loss, designed to enhance both intra-class compactness and inter-class separability. The approach aims to mitigate issues caused by the traditional FC + Softmax + Cross-Entropy loss, particularly in non-IID settings where features from the target domain exhibit large intra-class variation, leading to poor generalization. The UES loss incorporates an Ultra-expanded (UE) loss term and a Semantic Zoom mechanism, both of which are used to push features further from decision boundaries while preserving critical semantic information. Extensive experiments on several datasets demonstrate the proposed method's effectiveness, showing consistent improvements over baseline methods, such as ArcFace and Center Loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Innovative approach: proposes a new margin-based UES loss that simultaneously enhances intra-class compactness and inter-class separability for UDA.\n\n- Clear motivation: identifies the weakness of FC+Softmax+CE under non-IID assumptions and provides a reasonable explanation for why UES loss can help.\n\n- Strong experimental validation: consistently outperforms ArcFace and Center Loss across multiple datasets and baselines (DAN, DANN, DeepCORAL, DAAN).\n\n- Good robustness: demonstrates stable convergence and wide tolerance to hyperparameters (e.g., expansion factor e, λ₂).\n\n- Easy integration: the loss is simple and can be added as a regularization term to existing frameworks with minimal code changes."}, "weaknesses": {"value": "- Limited theoretical analysis: the paper lacks a solid theoretical justification for why UES loss performs better than other margin-based losses.\n\n- Insufficient ablation studies: only limited analysis on the contribution of Semantic Zoom vs. UE loss; unclear which component drives most of the gains.\n\n- Missing computational analysis: claims low overhead but provides no quantitative evidence (training time, memory, etc.).\n\n- No discussion on limitations: does not explore scenarios where the method might fail (e.g., extreme domain shift or noisy target data).\n\n- Writing and structure: some sections (especially “Inspirational Discoveries”) read more like extended discussion rather than rigorous analysis, which affects clarity."}, "questions": {"value": "- Could the method be extended to multi-source domain adaptation? How does the UES loss behave when there are multiple source domains?\n\n- How does the choice of hyperparameters (e.g., expansion factor, temperature) influence the results across different tasks? A more thorough analysis of hyperparameter sensitivity would be beneficial.\n\n- Could the method be applied to more complex domains beyond image classification, such as natural language processing or video processing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rmOAsgdvBS", "forum": "1QANBDkeln", "replyto": "1QANBDkeln", "signatures": ["ICLR.cc/2026/Conference/Submission1984/Reviewer_zXJC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1984/Reviewer_zXJC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946132664, "cdate": 1761946132664, "tmdate": 1762915983790, "mdate": 1762915983790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}