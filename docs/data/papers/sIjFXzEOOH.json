{"id": "sIjFXzEOOH", "number": 18116, "cdate": 1758284028694, "mdate": 1759897132492, "content": {"title": "A Fair Bayesian Inference through Matched Gibbs Posterior", "abstract": "With the growing importance of trustworthy AI, algorithmic fairness has emerged as a critical concern. \nAmong various fairness notions, group fairness - which measures the model bias between sensitive groups - has received significant attention. \nWhile many group-fair models have focused on satisfying group fairness constraints, model uncertainty has received relatively little attention, despite its importance for robust and trustworthy decision-making. \nTo address this, we adopt a Bayesian framework to capture model uncertainty in fair model training. \nWe first define group-fair posterior distributions and then introduce a fair variational Bayesian inference. \nThen we propose a novel distribution termed matched Gibbs posterior, as a proxy distribution for the fair variational Bayesian inference by employing a new group fairness measure, the matched deviation. \nA notable feature of matched Gibbs posterior is that it approximates the posterior distribution well under the fairness constraint without requiring heavy computation. \nTheoretically, we show that the matched deviation has a strong relation to existing group fairness measures, highlighting desirable fairness guarantees. \nComputationally, by treating the matching function in the matched deviation as a learnable parameter, we develop an efficient MCMC algorithm.\nExperiments on real-world datasets demonstrates that matched Gibbs posterior outperforms other methods in balancing uncertainty‚Äìfairness and utility‚Äìfairness trade-offs, while also offering additional desirable properties.", "tldr": "", "keywords": ["Algorithmic fairness", "Bayesian inference", "Gibbs posterior"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfc77aa2ee77252b7ca0cbe69e98edeb16d5a4dd.pdf", "supplementary_material": "/attachment/3c114bc2314e29338f0072da5040d8dadaafc3a0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a Bayesian framework for fair machine learning, addressing group fairness and uncertainty quantification  simultaneously. Traditional group-fair models ensure fairness by constraining optimization objectives but rarely account for predictive uncertainty, which is crucial for trustworthy AI. The authors propose a matched Gibbs posterior, derived from a novel fairness-aware penalty called the matched deviation, which upper-bounds the Wasserstein and total-variation fairness measures. This formulation avoids adversarial training and provides a computationally efficient approximation to the fair Bayesian posterior. They design an MCMC algorithm where both model parameters and the matching function ùëá are inferred jointly, ensuring fairness without heavy computation. Experiments on tabular, image, and text datasets show that the matched Gibbs posterior achieves better trade-offs between utility vs fairness and uncertainty vs fairness than baselines such as Reduction, GapReg, and Adversarial fairness methods, while also improving individual fairness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Well written and clearly structured: Despite heavy mathematics, the paper is well organized; intuitions precede theorems, notation is consistent, and experiments visually support claims.\n\n2. Novel combination of fairness and Bayesian inference: The paper is among the first to integrate group fairness constraints directly into Bayesian inference, explicitly addressing both uncertainty quantification and fairness.\n\n3. Avoids adversarial optimization: By replacing adversarial discriminators with a learnable matching function ùëá, the approach sidesteps instability and ùëÇ(ùëõ^2) cost typical in IPM-based fairness.\n\n4. Improved fairness‚Äìutility trade-off: Across datasets, matched Gibbs posterior consistently outperforms prior baselines on accuracy, NLL, Brier, and calibration error (ECE)."}, "weaknesses": {"value": "1. Scalability concerns: Joint inference of ùëì and ùëá may become costly for high-dimensional or non-metric input spaces (e.g., text embeddings). No discussion on large-scale efficiency.\n\n2. Restricted to binary sensitive attributes: The method currently handles only ùëÜ‚àà{0,1}; multi-group or intersectional fairness remains unexplored.\n\n3. Empirical scope and baselines: Experiments are thorough but limited to medium-sized datasets; modern large-scale deep architectures (e.g., BERT, ResNet-50) are absent.\n\n4. Unclear robustness under complex priors: The framework assumes tractable Gaussian priors; how matched Gibbs behaves with non-Gaussian or hierarchical priors is not tested.\n\nMy main concerns focus on scalability and applicability to modern large-scale architectures. Specifically, the paper lacks validation on high-capacity models such as BERT or ResNet-50, and it remains unclear whether the proposed matched Gibbs posterior remains computationally feasible in high-dimensional or non-metric spaces (e.g., text embeddings). In such settings, stability and sensitivity to noise could become significant issues."}, "questions": {"value": "Impact of imperfect matching: How sensitive is fairness performance to suboptimal or noisy matching functions ùëá? Can the authors quantify how deviation from optimal ùëá affects fairness bounds?\n\nScalability: How would the proposed MCMC perform on large neural models (e.g., transformers) or high-dimensional text embeddings?\n\nConnection to uncertainty geometry: Can the authors relate matched Gibbs fairness constraints to curvature of the posterior (e.g., Hessian eigen structure) or persistent-homology-based fairness landscapes? (This is not necessary for the paper, just something that came to mind during the review.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lrLXdCKPuW", "forum": "sIjFXzEOOH", "replyto": "sIjFXzEOOH", "signatures": ["ICLR.cc/2026/Conference/Submission18116/Reviewer_ZbGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18116/Reviewer_ZbGT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670257095, "cdate": 1761670257095, "tmdate": 1762927883464, "mdate": 1762927883464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a Bayesian route to group fairness by defining fairness for posteriors and proposing a matched Gibbs posterior: a Gibbs posterior with a new matched deviation penalty that avoids adversarial IPM/MMD inner loops, comes with bounds relating it to Wasserstein/TV, and is trained via an HMC+MH sampler that jointly infers the predictor and a matching function $T$. Experiments on ADULT, DUTCH, CRIME, CELEBA, and CIVIL suggest improved utility‚Äìfairness and uncertainty‚Äìfairness trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear formulation of fairness for posteriors (average DP to strong DP via rejection) and a practical proxy‚Äîthe matched Gibbs posterior‚Äîthat circumvents adversarial critics and offers $O(n)$ updates with an explicit sampler.\n2. Sound theory + strong experiments. Bounds linking matched deviation to Wasserstein/TV give intuition, and the image/text/tabular results show stronger Pareto fronts."}, "weaknesses": {"value": "1. Fairness target & positioning. The work focuses on demographic parity. Is there any justification for this choice against other metrics (Equal opportunity/equalized odds/calibration)?\n2. Fairness is measured on score distributions (W2(P_{f,0}, P_{f,1})); the relation to thresholded decisions (rate gaps) is unclear. Could you provide a more detailed analysis connecting score-level DP to rate-level DP, with sensitivity to thresholds or post-hoc calibration? \n3. The prior and MH proposal for T (swap k matches) are reasonable, but (i) what distance d is used for images/text (esp. CIVIL)? (ii) How does mixing/acceptance scale with n, class imbalance, and high-dimensional X? \n4. Complexity claims. The paper argues $O(n)$ per update vs. $O(n^2)$ MMD; it would be better if there were some running time/memory tables on large splits and report the cost of the HMC step (leap-frogs, step size) and MH over $T$.\n5. Concerns about the dataset. The empirical studies are very strong. However, the adult dataset has some known issues (https://arxiv.org/abs/2108.04884), it would be better to try out a more robust dataset or mention these caveats.\n6. Limitations. The current proposed method seems only applicable to binary sensitive attributes. Are there any discussions on how to extend to multi-categorical/continuous ones? Also, how could it be generalized to conditional fairness notions like equalized odds?\n\nI'm willing to raise the score if my concerns are discussed and addressed, thank you."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z7cDTcXdqa", "forum": "sIjFXzEOOH", "replyto": "sIjFXzEOOH", "signatures": ["ICLR.cc/2026/Conference/Submission18116/Reviewer_vJDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18116/Reviewer_vJDv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962158348, "cdate": 1761962158348, "tmdate": 1762927882776, "mdate": 1762927882776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the underexplored challenge of integrating model uncertainty into algorithmic fairness. To this end, the authors propose a group-fair posterior distribution and develop a fair variational Bayesian inference framework that embeds fairness constraints into probabilistic learning. To improve computational efficiency, they introduce a novel matched Gibbs posterior, which approximates the fair posterior under fairness constraints using a newly defined metric, matched deviation. This measure is theoretically shown to be closely related to established fairness notions, thereby offering strong fairness guarantees. Empirical evaluations on real-world datasets demonstrate that the proposed approach achieves superior trade-offs between fairness, model uncertainty, and predictive utility compared to baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of group fairness is highly relevant and socially significant, making this study timely and impactful.\n\n2. The authors effectively integrate group fairness and model uncertainty within a Bayesian inference framework and provide solid theoretical justification for their approach.\n\n3. Experiments conducted on three distinct modalities of datasets demonstrate the robustness and practical applicability of the proposed method."}, "weaknesses": {"value": "The paper‚Äôs writing and structure could be improved for clarity. After reading, several conceptual issues remain ambiguous:\n\na. What is the motivation for jointly modeling group fairness and uncertainty? Are there concrete real-world applications that benefit from this combination?\n\nb. What are the main technical challenges in combining fairness and uncertainty? Have there been prior studies exploring this intersection? Why can‚Äôt existing fairness and uncertainty methods simply be combined?\n\nc. Why is the Bayesian inference framework particularly appropriate for this setting? What specific advantages does it offer compared to prior non-Bayesian approaches?\n\nThe work only considers DP as the fairness definition, which is too limited. Other widely used metrics, such as EO and EOdds, should also be discussed. It remains unclear whether the proposed theory and framework can generalize to these metrics.\n\nIn the experimental section, fairness evaluation is restricted to the Wasserstein distance under DP. The empirical validation could be strengthened by including additional fairness metrics to better demonstrate the generalizability and robustness of the proposed method."}, "questions": {"value": "See weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "24DKT0OWme", "forum": "sIjFXzEOOH", "replyto": "sIjFXzEOOH", "signatures": ["ICLR.cc/2026/Conference/Submission18116/Reviewer_26og"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18116/Reviewer_26og"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989103009, "cdate": 1761989103009, "tmdate": 1762927882245, "mdate": 1762927882245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}