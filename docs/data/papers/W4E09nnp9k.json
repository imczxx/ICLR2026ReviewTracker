{"id": "W4E09nnp9k", "number": 3007, "cdate": 1757315920016, "mdate": 1763712524293, "content": {"title": "From Algebraic Structure to Neural Parameters: A Cyclic Codes Perspective on Transformer-Based Decoders", "abstract": "The advent of Transformer architectures has significantly enhanced the performance and flexibility of neural decoders. Meanwhile, cyclic codes continue to play a crucial role in practical communication systems. In this paper, we bridge these two domains by proposing a novel decoding approach that integrates the algebraic structure of cyclic codes into Transformer-based decoders. Leveraging the inherent cyclic properties, we introduce interpretable error correction patterns and inter-node relationship hypotheses that link the structural characteristics of the codes to the model parameters. Building on these insights, we design a plug-and-play, flexibly deployable decoding method tailored for cyclic codes. Experimental results show that our method achieves an average reduction in bit error rate (BER) by an order of magnitude, while also reducing the total number of parameters by approximately 97\\%. Additional comparative experiments validate our proposed conjectures and highlight a promising pathway for bridging classical coding theory and modern Transformer-based decoding architectures.", "tldr": "We investigated from the perspective of cyclic codes, bridging the algebraic structure and the parameters, and proposed a plug-and-play method that enhanced the decoder performance while reducing the parameter count.", "keywords": ["Algebraic Structure", "Cyclic Codes", "Parameter Interpretability", "Error Correction Pattern", "Relationship"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74ccf1468f2557b2c4170a87bb72a67cdfc99be3.pdf", "supplementary_material": "/attachment/9577e251449d5c90d1c80302e3db9f8935bf6093.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes techniques to reduce the parameter count and improve the efficiency of error correction code transformer (ECCT) models by leveraging code-specific properties, specifically focusing on cyclic codes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong empirical results showing a significant reduction in parameter count while maintaining competitive performance. Even though the study is limited to only cyclic codes, it is still impressive. \n\n2. The concept of error correction patterns is novel and interesting. Extending the parity check matrix (PCM) to an $ n \\times n$ space to bring in this angle of cyclically equivariant error patterns is interesting. This expansion further shows equivalence between variable and check node connections, which enables parameter sharing. \n\n3. Sufficient ablations to show that the expansion of PCM to an $ \\ n \\times n$ is only suitable for cyclic codes and not any random linear code, making the code-specific optimization angle convincing."}, "weaknesses": {"value": "1. Overly strong claims, such as \"explainable embedding mechanism\" to choose the dimension $d$, without a clear explanation as to how the relation is derived. \"Given that the embedding vectors represent relationships, adopting a bijective correspondence in dimensionality (i.e., d = 2n) becomes theoretically justified\" is not a clear enough explanation. It is unclear why \"expands each relationship into two distinct values\" is undesirable. Does this mean that the performance achieved with $d = 2n$ is better than both $d=n$ and $d=4n$? If yes, then rigorous empirical evaluation is needed to back up this claim. The results in Figure 11 (Appendix) seem to contradict this, where increasing $d$ seems to improve the performance continuously. \n\n2. Figure 6 is unclear because of the overlapping colors; it should be corrected to reflect the colors from the legend. \n\n3. While the empirical results are impressive, the niche application makes it of limited interest to the general audience at ICLR. Addressing things such as what these insights and techniques mean for codes other than cyclic codes or other graph learning problems would significantly strengthen the paper."}, "questions": {"value": "Suggested in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G2j57Q41Lt", "forum": "W4E09nnp9k", "replyto": "W4E09nnp9k", "signatures": ["ICLR.cc/2026/Conference/Submission3007/Reviewer_gUY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3007/Reviewer_gUY2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769278840, "cdate": 1761769278840, "tmdate": 1762916494558, "mdate": 1762916494558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to Transformer-based decoders for cyclic error-correcting codes by exploiting their algebraic structure. The authors introduce two key concepts: (1) Error Correction Patterns (ECPs) and (2) inter-node relationships encoded in embedding dimensions. By utilizing circulant parity-check matrices (PCMs) and implementing parameter reuse through cyclic shifting, they achieve improvements in both performance (reduction in BER) and efficiency (reduction in parameters) across ECCT, CrossMPT, and MM-ECCT architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tries to provide the systematic explanation of what embedding dimensions represent in Transformer-based decoders, namely, inter-node relationships. The introduction of ECPs is elegant and provides clear intuition for why circulant PCMs simplify the learning problem. The connection between cyclic code structure and inter-node relationships is well-established through Conclusions 1 and 2. And the results overall appear to be convincing."}, "weaknesses": {"value": "The authors have attempted to present an interesting idea; however, the paper lacks overall flow and clarity, making it difficult for readers to understand and apply. I will highlight these issues in my following observations.: \n\n1. Clarity on d≠2n cases. Sections 3.2–3.3 state that the embedding dimension d may be a multiple or a divisor of 2n and cite “segmented cyclic shifting,” but they do not specify the exact shift stride for expansion or the rule for compression. Figure 2 is illustrative rather than algorithmic. A precise construction (with a small numeric example) would improve understanding.\n\n2. Section 3.3 (core contribution) as written is not sufficiently understandable. The parameter-reuse mechanism (Eq. 5) is promising, but the presentation needs more detail. Specifically: formally define the operator sc(⋅)(currently described only as a “segmented cyclic shift” without a mathematical specification), make the shift stride explicit, and include a worked example. In my opinion, the authors should rewrite this section for better clarity and understanding.\n\n3. Section 3.4 mentions “our method can also be used in the feed-forward network” but provides no details or explanation how. Please either provide extensive details for the feed-forward case (the FFN weight matrices W_1∈R^(d×d_ff ) and W_2∈R^(d_ff×d)) or remove this claim.\n\n4. The paper would benefit from thorough proofreading, as both grammar and clarity require significant improvement. For example, Sec. 2.2 states: “which can be describe as the performance is exceptionally poor on not training codewords, but exceptionally strong on training codewords.”\nSuggested rewrite: “which can be described as strong performance on training codewords but poor generalization to unseen (non-training) codewords.”\n\n5. A recent paper published in IEEE Transactions on Communications, titled \"Transformer-Based Decoders for Cyclic Codes: A Tanner Cycle-Equivalent Approach\". This work should be cited, and the unique contributions over this prior work should be clearly articulated."}, "questions": {"value": "I summarized all questions in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AckCT7Uyli", "forum": "W4E09nnp9k", "replyto": "W4E09nnp9k", "signatures": ["ICLR.cc/2026/Conference/Submission3007/Reviewer_jnNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3007/Reviewer_jnNM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908477043, "cdate": 1761908477043, "tmdate": 1762916494102, "mdate": 1762916494102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a ECCT-like decoder for cyclic error-correcting codes, motivated by the observation that extending the parity-check matrix (PCM) into an n×n circulant form makes all variable and check nodes share an identical one-error pattern up to a cyclic shift. This symmetry enables the model to learn a single error-correction rule that generalizes across all positions. This largely simplifies the learning procedure, enabling >95% reduction in the parameter count while maintaining the same performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper identifies that a cyclic extension of the parity check matrix, before application to an ECCT results in all the VNs/CNs having the same one error pattern upto a cyclic shift. This allows the model to focus on learning to decode this specific error correction pattern (as opposed to different error patterns in each row/column in the previous architectures).\n\n2. The symmetry in the attention mask derived from the circulant PCM allows significant parameter reuse - achieving similar/better performance with more than 95% reduction in parameter count.\n\n3. The analysis of the layerwise attention provides good evidence that this method unifies error correction strategies under a cyclic shift inductive bias."}, "weaknesses": {"value": "1. The writing is not super clear. Example - sc(.) is central to the parameter-reuse mechanism (Eq 5) but is never formally defined beyond a short verbal description. Pseudocode or an explicit algorithmic description of the segmentation and cyclic-shift procedure is necessary for reproducibility - especially given that the source code of the implementation has not been provided.\n\n2. The theoretical explanation is not very convincing - The paper repeatedly claims that setting d=2n establishes a “bijective mapping between embedding coordinates and directed cyclic relations” (Sec. 3.2, Eq. (3), Conclusion 3). However, no analytical derivation or probing experiment (or an orthogonality analysis), supports this statement. The evidence is limited to an empirical plateau in performance at d≈2n (App. C, Fig. 11).\nDoes the d≈2n inflection point persist when varying the number of heads, layers, and FF width (holding total params constant)? If the knee moves, the claim is just a capacity artifact.\nIn prior theoretical ML literature, I do not know any proof that such a bijective relation exists (please cite the relevant papers if this has been studied before): the embedding dimension d≈2n being sufficient could just be an artifact of architectural capacity limits. \n\n3. \\phi(i,j) representing relations between nodes i and j restates the standard semantics of the attention mechanism - I may have misunderstood, but I would not count this as a theoretical contribution of this paper."}, "questions": {"value": "See weaknesses.\nWilling to increase score if concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "USbFgh2Z6D", "forum": "W4E09nnp9k", "replyto": "W4E09nnp9k", "signatures": ["ICLR.cc/2026/Conference/Submission3007/Reviewer_qcSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3007/Reviewer_qcSQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957274940, "cdate": 1761957274940, "tmdate": 1762916493738, "mdate": 1762916493738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a ECCT-like decoder for cyclic error-correcting codes, motivated by the observation that extending the parity-check matrix (PCM) into an n×n circulant form makes all variable and check nodes share an identical one-error pattern up to a cyclic shift. This symmetry enables the model to learn a single error-correction rule that generalizes across all positions. This largely simplifies the learning procedure, enabling >95% reduction in the parameter count while maintaining the same performance.\n\nEdit : Updated score after revision"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper identifies that a cyclic extension of the parity check matrix, before application to an ECCT results in all the VNs/CNs having the same one error pattern upto a cyclic shift. This allows the model to focus on learning to decode this specific error correction pattern (as opposed to different error patterns in each row/column in the previous architectures).\n\n2. The symmetry in the attention mask derived from the circulant PCM allows significant parameter reuse - achieving similar/better performance with more than 95% reduction in parameter count.\n\n3. The analysis of the layerwise attention provides good evidence that this method unifies error correction strategies under a cyclic shift inductive bias."}, "weaknesses": {"value": "1. The writing is not super clear. Example - sc(.) is central to the parameter-reuse mechanism (Eq 5) but is never formally defined beyond a short verbal description. Pseudocode or an explicit algorithmic description of the segmentation and cyclic-shift procedure is necessary for reproducibility - especially given that the source code of the implementation has not been provided.\n\n2. The theoretical explanation is not very convincing - The paper repeatedly claims that setting d=2n establishes a “bijective mapping between embedding coordinates and directed cyclic relations” (Sec. 3.2, Eq. (3), Conclusion 3). However, no analytical derivation or probing experiment (or an orthogonality analysis), supports this statement. The evidence is limited to an empirical plateau in performance at d≈2n (App. C, Fig. 11).\nDoes the d≈2n inflection point persist when varying the number of heads, layers, and FF width (holding total params constant)? If the knee moves, the claim is just a capacity artifact.\nIn prior theoretical ML literature, I do not know any proof that such a bijective relation exists (please cite the relevant papers if this has been studied before): the embedding dimension d≈2n being sufficient could just be an artifact of architectural capacity limits. \n\n3. \\phi(i,j) representing relations between nodes i and j restates the standard semantics of the attention mechanism - I may have misunderstood, but I would not count this as a theoretical contribution of this paper."}, "questions": {"value": "See weaknesses.\nWilling to increase score if concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "USbFgh2Z6D", "forum": "W4E09nnp9k", "replyto": "W4E09nnp9k", "signatures": ["ICLR.cc/2026/Conference/Submission3007/Reviewer_qcSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3007/Reviewer_qcSQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957274940, "cdate": 1761957274940, "tmdate": 1763734679389, "mdate": 1763734679389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work integrates the algebraic structure of cyclic codes into recent transformer-based decoders. By leveraging the cyclic properties, the total number of parameters can be significantly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Parameter Efficiency: The paper's main achievement is a huge 97% average reduction in model parameters. \n2. Novel Code-Aware Approach: The idea to use an $n \\times n$ circulant PCM seems to be new in the domain of transformer-based decoders."}, "weaknesses": {"value": "1. Potential Increase in Computational Complexity: The paper focuses heavily on the 97% parameter reduction, which is a saving in model storage. However, it doesn't address the computational cost (FLOPs). Method increases the input sequence length from $L=2n-k$ to $L=2n$. Since the transformer's attention mechanism has a complexity of $O(L^2)$, this longer sequence means more computations are required during training and inference. This important trade-off is not discussed.\n\n2. Novelty of the ECP Concept: The paper claims to have pioneered the concept of Error Correction Patterns (ECP). However, the definition provided (the set of all check nodes connected to a specific variable node) appears identical to the standard graph-theory concept of a '1-hop neighborhood' or a 'check set'. The contribution isn't the concept itself, but rather the analysis of how this pattern becomes unified under their $n \\times n$ circulant PCM\n\n3. The Embedding Theory is a Hypothesis, Not a Proof: The interpretation of \"embedding = $2n$ relationships\" is a compelling hypothesis, but it isn't mathematically proven. The experiments (e.g., Figures 10 & 11) show that the data is consistent with this theory, but they don't prove it's the definitive reason it works. Claiming to have systematically interpreted the meaning of embedding might be an overstatement."}, "questions": {"value": "1. A recent published paper \"Transformer-Based Decoders for Cyclic Codes: A Tanner Cycle-Equivalent Approach\" in IEEE Transactions on Communications (2025), already proposed this paper's core ideas: 'Parameter Cyclic Reuse' and the theoretical basis that 'Embedding = Relationship'. It is required to clarify that this paper's novel contributions compared to this TCOM paper. \n2. Proposed method appears to be disadvantageous in terms of computational complexity from two aspects. First, the input sequence length increases from $L=2n-k$ to $L=2n$. Second, by abandoning the Systematic PCM and using an $n \\times n$ circulant matrix, the attention mask is likely to become significantly more dense. Given this double jeopardy can you claim the model's practical utility based solely on the 97% parameter reduction? A comparison of actual inference time (latency) is necessary.\n3. It was known that the mask matrix from the systematic PCM is better than the mask matrix from the non-systematic PCM. Hence, the original ECCT’s decoding performance is evaluated by using the systematic PCM and the following research on transformer-based decoders adopt the systematic PCM. It is required to clarify whether this paper adopted the systematic PCM or non-systematic PCM. In my understanding, the proposed method can be valid only for non-systematic PCM, then its technical impact could be limited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zNfluXoJAw", "forum": "W4E09nnp9k", "replyto": "W4E09nnp9k", "signatures": ["ICLR.cc/2026/Conference/Submission3007/Reviewer_5Lhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3007/Reviewer_5Lhd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960420755, "cdate": 1761960420755, "tmdate": 1762916492965, "mdate": 1762916492965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}