{"id": "O4nISWGU4F", "number": 19939, "cdate": 1758300822312, "mdate": 1759897011448, "content": {"title": "Quantifying depressive mental states with large language models", "abstract": "Large Language Models (LLMs) may have an important role to play in mental health by facilitating the quantification of verbal expressions used to communicate emotions, feelings and thoughts. While there has been substantial and very promising work in this area, the fundamental limits are uncertain. Here, focusing on depressive symptoms, we outline and evaluate LLM performance on three critical tests. The first test evaluates LLM performance on a novel ground-truth dataset from a large human sample (n=770). This dataset is novel as it contains both standard clinically validated quantifications of depression symptoms and specific verbal descriptions of the thoughts related to each symptom by the same individual. The performance of LLMs on this richly informative data shows an upper bound on the performance in this domain, and allow us to examine the extent to which inference about symptoms generalises. Second, we test to what extent the latent structure in LLMs can capture the clinically observed patterns. We train supervised sparse auto-encoders (sSAE) to predict specific symptoms and symptom patterns within a syndrome. We find that sSAE weights can effectively modify the clinical pattern produced by the model, and thereby capture the latent structure of relevant clinical variation. Third, if LLMs correctly capture and quantify relevant mental states, then these states should respond to changes in emotional states induced by validated emotion induction interventions. We show that this holds in a third experiment with 190 participants. Overall, this work provides foundational insights into the quantification of pathological mental states with LLMs, highlighting hard limits on the requirements of the data underlying LLM-based quantification; but also suggesting LLMs show substantial conceptual alignment.", "tldr": "We demonstrate and validate an approach towards quantifying human depressive mental states from language with LLMs.", "keywords": ["llm", "depression", "mental health", "sparse autoencoders"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d194d9662257f84eb397b8c3b4bcb67541deac5.pdf", "supplementary_material": "/attachment/ca172078a0ab9041efadb3f5b4f75b160327584c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a three-part framework for evaluating the ability of large language models (LLMs) to quantify depressive symptoms from open-ended verbal descriptions. The authors introduce a novel dataset combining open-ended responses with clinically validated depression measures, propose a supervised sparse auto-encoder (sSAE) to extract latent symptom representations, and validate the approach using an emotion induction experiment. While the topic is timely and the experimental design is ambitious, the paper suffers from significant methodological, conceptual, and ethical shortcomings that undermine its contributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The collection of paired open-ended and standardized questionnaire data is a valuable contribution to the field. The three-test framework is conceptually sound and addresses different aspects of model performance.\n\n The use of mood induction to test model sensitivity to emotional change is a strong experimental design choice.\n\n The use of sSAEs and perturbation analyses demonstrates a sophisticated approach to interpreting latent representations."}, "weaknesses": {"value": "The paper conflates verbal expression with mental state. LLMs are trained on language, not mental states, and the leap from text-based prediction to mental state quantification is not sufficiently justified. The authors claim LLMs \"encode a consistent representation akin to a mental state,\" but this is speculative and not supported by causal or theoretical grounding.\n\nIn Study 3, the mood induction stimuli were derived from Study 1 responses, which were already used to train the sSAE. This introduces a risk of overfitting and limits the generalizability of the results.  The use of LLM-predicted scores to select mood induction prompts (Appendix D.3.1) may bias the intervention toward model-friendly examples.\n\nSome new LLM-based mental health methods are not referred, such as mentalllama. The 1.5-minute time limit and 30-word minimum for open-ended responses may have influenced response quality and model performance in unaccounted ways. The authors suggest potential applications in \"therapeutic intervention allocation and delivery\" and suicide risk identification, but provide no evidence that the model is safe, reliable, or clinically valid for such high-stakes use.\n\nModel performance, while \"moderate to strong,\" is evaluated only on a specific online population with depressive symptoms. There is no evidence that results generalize to clinical populations, other cultures, or non-English speakers."}, "questions": {"value": "How do you justify the claim that LLMs capture mental states as opposed to linguistic patterns associated with self-reported symptoms?\n\nHow do you rule out the possibility that the sSAE’s sensitivity to mood induction is an artifact of using LLM-scored prompts from the same dataset used for training?\n\nCan you provide any evidence that the sSAE latent dimensions correspond to clinically meaningful constructs beyond symptom severity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7jjHkOZziT", "forum": "O4nISWGU4F", "replyto": "O4nISWGU4F", "signatures": ["ICLR.cc/2026/Conference/Submission19939/Reviewer_dkn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19939/Reviewer_dkn7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100697398, "cdate": 1761100697398, "tmdate": 1762932107369, "mdate": 1762932107369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework to evaluate the capacity of Large Language Models to quantify depressive mental states. The work tests model performance against human data, extracts a latent representation of symptoms, and assesses the representation's sensitivity to induced changes in emotion, which provides a method for validating language models for mental health applications."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The use of three tests provides a framework for model evaluation. The first test establishes a performance ceiling using a dataset that links verbal descriptions with clinical scores. The second test moves from performance to mechanism by training a sparse auto-encoder to find a latent structure. The perturbation and steering analyses show a connection between the latent space and model output. The third test connects the model's internal states to human psychological states through an emotion induction experiment. this sequence of performance, mechanism, and external validity provides a systematic approach to the problem."}, "weaknesses": {"value": "The open-ended text from participants was constrained by time and word count, which may influence the quality of the language data the models received. The population was recruited from an online platform, which may not represent the general population or clinical populations. The emotion induction focused on one dimension of affect; the sensitivity of the derived measures to other states of emotion is not known. The paper equates model performance on questionnaire prediction with quantification of a mental state, which is a step in reasoning that requires more justification."}, "questions": {"value": "How might the sSAE's latent structure differ if trained on data from a clinical population undergoing treatment, where symptom expression might change over time?\n\nThe steering intervention modifies the hidden state. What is the effect of this modification on the semantic content of text generated from that perturbed state? Does it produce text that reflects the steered symptom severity?\n\nCould the framework be adapted to quantify other mental constructs, such as anxiety or psychosis, and what challenges would be anticipated?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RJaVR98vlW", "forum": "O4nISWGU4F", "replyto": "O4nISWGU4F", "signatures": ["ICLR.cc/2026/Conference/Submission19939/Reviewer_fVTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19939/Reviewer_fVTX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963006322, "cdate": 1761963006322, "tmdate": 1762932106267, "mdate": 1762932106267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to evaluate LLMs performance on 3 tests to quantify depressive mental states. They recruited human participants with diverse levels of depression to gather responses from mental health questionnaires (PHQ, GAD, SDS) as data for the LLMs.\nFor test 1, they attempted to predict item-level multiple-choice question scores based on the participants' item-level open-ended text responses. \nFor test 2, they use ground-truth from test 1 to train a supervised sparse auto-encoders (sSAE) to predict the participants’ z-scored PHQ-9 scores from the hidden states representations of the best-performing LLM from test 1.\nFor test 3, they evaluated whether the sSAE measures from test 2 can capture and quantify specific emotional changes induced through text."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nThe authors recruited participants (770 for test 1 and 190 for test 3) to collect real-life data for their experiments. This inherently separates their analysis and results from other studies in similar applications, which provides new and fresh insights.\nI am not certain if their methodology to evaluate LLMs in this way is novel.\n\n\nQuality:\nA comprehensive study was done on this topic with 3 tests conducted while building upon the previous.\n\n\nClarity:\nThe authors provided detailed explanation of the methods and implementation.\n\n\nSignificance:\nN/A"}, "weaknesses": {"value": "The authors did not provide sufficient explanation for their choice of plots and graphs to demonstrate their evaluation results, such as figures 2 and 3. \nIt is unclear what some of these figures are meant to show and the authors offer limited insights and descriptions from the visualization.\n\nThe authors offer limited insights into the reason behind the LLMs performance in these tasks despite claiming that they aim to describe the LLMs' promises and limitations (i.e. \"The LLMs performed well/poorly in this 'abc' task because of 'xyz' possible reason.\").\n\nThe authors did not offer suggestions as to how the results of their study can translate to a real-life application of LLM in mental welfare.\n\nThey did not discuss much about prior related work that also attempted to use LLMs for depressive symptoms detection for a comparison.\nThis makes it harder to judge whether their work is significant within this area."}, "questions": {"value": "Typos:\nLine 367: \"continuos\"\nLine 1329: \"he\"\nLine 1337: \"partiicpant\"\n\n\n\nFormatting Issues:\nAuthors made LaTeX formatting error. Refer to section 3.1.1. For quotation marks, using regular double quotes \"\" is incorrect. ``'' is needed.\nThe graphs in B.5.2 need to be adjusted. The letters overlap with the top number.\n\n\n\nQuestions:\n1. I do not have expertise on scores such as PHQ, GAD, or SDS but I believe that the statistics shown in Appendix B.1.2 does not indicate that the 770 human sample show a very prominent level of depression. \nIs this a regular population sample where majority of people are not depressed? If so, this could affect the validity of the evaluation for depressive symptoms.\nPlease correct me if my understanding of the scores are incorrect.\n\n2. Are there any overlap between the participants in study 1 and 3? If some participants overlap, then this could create some potential bias in the training data for study 3 and cause the result to appear better.\n\n3. Should the authors perform comparisons between groups such as non-depressed or happy participants and severely depressed participants to see how the LLMs would behave?\n\n4. In Figure 2I, is there a reason why performance is noticeably worse for Q1 and Q8?\nI did not see a possible explanation or hypothesis mentioned for this.\n\n5. For study 3, is the analysis comprehensive enough to only include positive (mood high) and negative (mood low) emotional states from the participants?\n\n6. Are these experiments truly enough to establish a baseline / ceiling / limitation for LLMs ability to quantify depressive mental states?\nIt is difficult to see how significant your results are in this area without a comparison with other studies that also attempted to find evaluation methods for depressive signs with LLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "boUQZE3tdB", "forum": "O4nISWGU4F", "replyto": "O4nISWGU4F", "signatures": ["ICLR.cc/2026/Conference/Submission19939/Reviewer_LQco"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19939/Reviewer_LQco"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985132774, "cdate": 1761985132774, "tmdate": 1762932105378, "mdate": 1762932105378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether off-the-shelf large language models (LLMs) encode clinically meaningful information in their hidden representations that correspond to depressive and emotional mental states. The authors propose a three-part evaluation framework to determine the potential and limitations of LLMs for quantifying mental health symptoms using data from 770 participants who provided open-ended text responses alongside standard depression and anxiety questionnaires (PHQ-9, GAD-7, SDS).\n\nFirst, they show that several instruction-tuned models (Gemma2, Llama3, Mistral) can predict questionnaire scores using the answers to open-ended questions as their context. Second, they train a supervised sparse autoencoder (sSAE) on LLM hidden states to model the latent structure of depressive symptoms, demonstrating selective control over symptom dimensions via latent perturbations. Finally, in a mood-induction experiment with 190 participants, the sSAE latent scores track experimentally induced emotional changes.\n\nBased on these results, the paper concludes that LLMs encode structured representations of depressive mental states that can be extracted and manipulated through supervised latent modeling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a large dataset on mental disorders. The dataset contains rich ground truth for mental health modeling, consisting of matched open-ended Q&As and several questionnaires.\n- The paper presents evidence for all experiments with statistical rigor."}, "weaknesses": {"value": "- The paper contributions lie in its dataset and experimental design related to mental health. However, the paper offers limited novelty in its methods for quantifying or analyzing the LLM states. \n- While the results are promising, the use of a supervised sparse autoencoder (sSAE) in Study 2 (and 3) provides limited theoretical insight into the nature of LLM state representations. The observed predictive performance may primarily reflect the supervised model’s ability to fit symptom scores rather than evidence that the LLM itself encodes meaningful mental-state dimensions. It would be more informative if the authors could demonstrate direct manipulation of the LLM’s own outputs or internal activations based on the discovered sparse coding, rather than relying on a separate probing model.\n- The writing occasionally overstates the contribution or uses vague phrasing. For example, it is unclear what is meant by “further establishing the sensitivity and limits of LLM quantification” (Line 437). Phrases like this imply a theoretical characterization of model capability, but the paper only presents empirical correlations on a specific dataset.  Terms such as “upper bound,” “hard limits,” and “conceptual alignment” are used without precise definition or analytical support.\n- The third experiment aims to test whether LLM-derived latent measures track participants’ emotional changes after a mood induction. However, it is unclear how the emotion information is actually represented or provided to the LLM."}, "questions": {"value": "1. How exactly was the emotional state “input” to the LLM?\n2. In C.1.4, why was the average between the two hidden states appropriate? Why not in C.1.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The authors state that they obtained approval from an ethical committee."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cZi3wsEYsP", "forum": "O4nISWGU4F", "replyto": "O4nISWGU4F", "signatures": ["ICLR.cc/2026/Conference/Submission19939/Reviewer_hNP2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19939/Reviewer_hNP2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064258573, "cdate": 1762064258573, "tmdate": 1762932104353, "mdate": 1762932104353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}