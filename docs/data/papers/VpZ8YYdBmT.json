{"id": "VpZ8YYdBmT", "number": 14232, "cdate": 1758230846404, "mdate": 1763581039666, "content": {"title": "Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations", "abstract": "Large language models (LLMs) demand extensive memory capacity during both fine-tuning and inference. To enable memory-efficient fine-tuning, existing methods apply block-wise quantization techniques, such as NF4 and AF4, to the network weights. We show that these quantization techniques incur suboptimal quantization errors. Therefore, as a first novelty, we propose an optimization approach for block-wise quantization. Using this method, we design a family of quantizers named 4-bit block-wise optimal float (BOF4), which consistently reduces the quantization error compared to both baseline methods. We provide both a theoretical and a data-driven solution for the optimization process and prove their practical equivalence. Secondly, we propose a modification to the employed normalization method based on the signed absolute block maximum (BOF4-S), enabling further reduction of the quantization error and empirically achieving less degradation in language modeling performance. Thirdly, we explore additional variations of block-wise quantization methods applied to LLMs through an experimental study on the importance of accurately representing zero and large-amplitude weights on the one hand, and optimization towards various error metrics on the other hand. Lastly, we introduce a mixed-precision quantization strategy dubbed outlier-preserving quantization (OPQ) to address the distributional mismatch induced by outlier weights in block-wise quantization. By storing outlier weights in 16-bit precision (OPQ) while applying BOF4-S, we achieve top performance among 4-bit block-wise quantization techniques w.r.t. perplexity.", "tldr": "", "keywords": ["quantization", "large language models", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bf643e63afcaa3f64316436415bfe81a7eb66c6.pdf", "supplementary_material": "/attachment/8de26b914fbde3bc89966d55a00c3b3a978ac6a0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a new 4-bit data format called FOF4, which leverages the asymmetric property of maximum/minimum values within a block to reduce the waste of degrees of freedom in the quantization codebook. This method can be regarded as a type of non-uniform quantization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This method leverages the asymmetry of the boundaries in weight quantization to reduce quantization error."}, "weaknesses": {"value": "1. This method can be regarded as a special type of non-uniform quantization method (1D codebook). Additionally, compared with other KMeans-based methods such as GPTVQ and RPTQ, it may not have advantages in terms of accuracy and speed (without hardware support).\n2. This method lacks a comparison with similar basic codebook-based methods like GPTVQ，VPTQ...\n3. Current LLMs can achieve W4A4 quantization with almost no loss, which has greater advantages in reducing computational overhead. This method seems unable to achieve this, and it also requires additional unstructured outlier storage and computation."}, "questions": {"value": "Methods that previously determined normalization constants based on MSE have also achieved good results. Why is the weight based on the maximum absolute value chosen instead? What advantages does this approach offer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mFwmXVOihO", "forum": "VpZ8YYdBmT", "replyto": "VpZ8YYdBmT", "signatures": ["ICLR.cc/2026/Conference/Submission14232/Reviewer_SU8o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14232/Reviewer_SU8o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639514163, "cdate": 1761639514163, "tmdate": 1762924687173, "mdate": 1762924687173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel 4-bit block-wise quantization algorithm called BOF4-S. The proposed algorithm employs an enhanced EM algorithm to obtain the optimal reconstruction levels. Additionally, it normalizes the weights using the signed absolute block maximum, which effectively saves one reconstruction level, thereby adding an extra degree of freedom to the codebook and further reducing quantization error. Moreover, this algorithm excludes outlier weights and store them separately to further enhance the quantization accuracy. Experimental results demonstrate that the proposed algorithm achieves less quantization error and perplexity compared to the conventional methods, while also achieving superior model performance before and after fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a thorough and clear explanation of the method, making it easy for readers to understand. The experimental results are comprehensive, which strongly supports the effectiveness of the proposed method. \n\n2. The method addresses several practical issues in current quantization algorithms, such as the wastage of reconstruction levels and the impact of outliers on quantization accuracy. By cleverly saving reconstruction levels, designing an optimized codebook algorithm, and eliminating outliers, it provides an ingenious approach to optimizing quantization algorithms, further enhancing the quantization accuracy.\n\n3. This method achieves improvements in both quantization error and perplexity with minimal time overhead. Furthermore, the LLMs quantized using this algorithm demonstrate better task performance before and after fine-tuning compared to traditional methods, showcasing the method's strong applicability."}, "weaknesses": {"value": "Although selecting one of the two endpoints as the reconstruction level for the maximum absolute weight provides an additional degree of freedom for the codebooks, it also requires an extra bit to store the sign of this maximum value. Is this overhead justified? Especially since, when using Llama-3.2-3B as the base model, there is no performance improvement on most tasks (Table 2)."}, "questions": {"value": "1. Is it possible for both endpoints of the normalized weights (-1 and 1) to appear at the same time? If so, how is the sign handled?\n2. When outliers are removed, their corresponding values in the tensor are replaced with 0. Is this tensor modified before or after normalization? Does directly replacing values with 0 impact the distribution of weights within the current block, potentially interfering with the selection of the optimal codebooks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TTOGYSrUZ9", "forum": "VpZ8YYdBmT", "replyto": "VpZ8YYdBmT", "signatures": ["ICLR.cc/2026/Conference/Submission14232/Reviewer_dVvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14232/Reviewer_dVvQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750675087, "cdate": 1761750675087, "tmdate": 1762924686748, "mdate": 1762924686748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive study on improving 4-bit block-wise quantization for Large Language Models (LLMs). The authors identify a fundamental issue in existing methods (e.g., NF4, AF4): they optimize the quantization error of the *normalized* weights rather than that of the *original* weights, leading to suboptimal results. To address this, the authors propose an improved Expectation-Maximization (EM) algorithm that directly optimizes the error of the original weights, resulting in a family of quantizers termed BOF4. They also introduce signed absolute maximum normalization, which frees up one codebook entry to enhance representational capacity. Additionally, a mixed-precision scheme is proposed to identify and handle outlier weights. The paper is strongly supported by rigorous mathematical derivations and extensive experiments on multiple LLMs and tasks (both inference and fine-tuning), demonstrating consistent and significant performance improvements over strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Theoretical Soundness and Novelty:** The paper's core insight—optimizing the end-to-end quantization error of the *original* weights rather than that of the *normalized* weights—is profound and well-articulated. The derivation of new centroid update rules for Lloyd's algorithm (applicable to both MSE and MAE) constitutes a solid theoretical contribution. Clear and comprehensive mathematical proofs further solidify the theoretical foundation.\n- **Holistic Methodological Framework:** The paper extends beyond a single idea by introducing a suite of complementary techniques: an optimal codebook based on reconstruction loss (BOF4), a signed normalization scheme (BOF4-S), and a practical outlier-preserving mechanism (OPQ). This integrated approach effectively enhances quantization accuracy from multiple perspectives.\n- **Thorough and Convincing Experiments:** The evaluation is extensive, covering multiple model families (Llama, Qwen, Mistral), both inference and fine-tuning (QLoRA) scenarios, and a wide range of benchmarks (perplexity, NLP tasks, code generation). The results consistently show that the proposed methods, especially BOF4-S with OPQ, outperform the baselines.\n- **Practical Impact and Reproducibility:** The methods are directly applicable for memory-efficient LLM deployment and fine-tuning. The paper provides optimized codebooks in the appendix and discusses integration with data-aware PTQ methods like GPTQ (Appendix I), enhancing its practical utility and reproducibility."}, "weaknesses": {"value": "- **Assumption of Gaussian Weight Distribution:** The optimization of the BOF4 codebook relies on the assumption that network weights are Gaussian-distributed. Although Appendix C provides justification that most blocks are indeed Gaussian, especially after OPQ, the performance on models or layers with significantly non-Gaussian weight distributions remains less explored. This could limit the generalizability to certain architectures.\n\n- **Overhead of OPQ:** Although OPQ is shown to have minimal runtime overhead (Appendix G.3), it introduces additional memory overhead for storing the outlier indices and values. A more detailed analysis of this memory-cost/accuracy trade-off, especially for very large models, would be beneficial."}, "questions": {"value": "- **Q1: Gaussian Distribution Assumption**: The EM algorithm used for BOF4 relies on an assumed Gaussian distribution. How sensitive is the final performance to deviations from this assumption? If a model's weights do not follow a Gaussian distribution, what would be the magnitude of deviation this algorithm might cause?\n\n- **Q2: Selection of Hyperparameter q**: Regarding OPQ, the hyperparameter `q=0.95` was chosen via a limited search. Could you discuss the sensitivity of the results to the choice of `q`? Is this value generally robust across different models and sizes, or does it need tuning?\n\n- **Q3: The Special Sign Bit:** The signed normalization in BOF4-S requires storing the sign of the block maximum if double quantization is applied, as noted in Appendix A. Could you quantify the potential performance degradation if this extra bit is not used and the standard double quantization scheme is applied naively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zALIzhE9rx", "forum": "VpZ8YYdBmT", "replyto": "VpZ8YYdBmT", "signatures": ["ICLR.cc/2026/Conference/Submission14232/Reviewer_sXWd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14232/Reviewer_sXWd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925905298, "cdate": 1761925905298, "tmdate": 1762924686328, "mdate": 1762924686328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel approach to improving block-wise training for large language models (LLMs) by introducing refined optimization and scheduling mechanisms that better capture inter-block dependencies during training. The authors demonstrate that their method enhances both convergence stability and downstream task performance, offering a computationally efficient alternative to full end-to-end fine-tuning. The study is well-motivated, clearly written, and supported by extensive experimental validation across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed method provides an original perspective on block-wise training, addressing the often-overlooked issue of gradient inconsistency across blocks.\n\n2. The formulation is mathematically rigorous, with theoretical justification for the proposed scheduling strategy.\n\n3. The experiments span several model sizes and datasets, showing consistent improvement over baselines such as layer-wise and progressive tuning.\n\n3. The approach maintains efficiency advantages (reduced memory and training cost) while achieving comparable or superior results to full fine-tuning, highly relevant for real-world large-scale model adaptation.\n\n4. The paper is well-structured, with clear motivation, methodology, and ablation analysis that enhances understanding."}, "weaknesses": {"value": "1. While the approach performs well on benchmark datasets, it would be useful to see how well it generalizes to non-language tasks (e.g., multimodal or code models).\n\n2.  The method involves scheduling parameters whose influence is only briefly discussed; more detailed robustness analysis would strengthen the contribution.\n3. Although the method is efficient, the paper could provide clearer quantification of the additional cost introduced by the new scheduling mechanism relative to vanilla block-wise training."}, "questions": {"value": "How sensitive is the performance to the choice of block partitioning (e.g., number of layers per block)?\n\nCould the proposed inter-block dependency modeling be integrated with adapter-based fine-tuning approaches?\n\nHave the authors considered evaluating the method’s stability when applied to continual learning or streaming data scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3uhjJa0v0H", "forum": "VpZ8YYdBmT", "replyto": "VpZ8YYdBmT", "signatures": ["ICLR.cc/2026/Conference/Submission14232/Reviewer_M1dB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14232/Reviewer_M1dB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982783063, "cdate": 1761982783063, "tmdate": 1762924685685, "mdate": 1762924685685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}