{"id": "3cB9243E9i", "number": 15202, "cdate": 1758248936175, "mdate": 1763630734693, "content": {"title": "Rethinking JEPA: Compute‑Efficient Video Self-Supervised Learning with Frozen Teachers", "abstract": "Video Joint Embedding Predictive Architectures (V‑JEPA) learn generalizable off-the-shelf video representations by predicting masked regions in latent space with an exponential moving average (EMA)‑updated teacher.\nWhile EMA prevents representation collapse, it complicates scalable model selection and couples teacher and student architectures. \nWe revisit masked‑latent prediction and show that a frozen teacher suffices. Concretely, we (i) train a target encoder with a simple pixel‑reconstruction objective under V‑JEPA masking, then (ii) freeze it and train a student to predict the teacher’s latents on masked regions. \nThis leads to a two‑stage, unregularized scheme, that we refer to as SALT (Static-teacher Asymmetric Latent Training). SALT decouples optimization into pixel reconstruction (teacher) and masked latent prediction (student), increasing transparency, efficiency, and scalability while preserving the ability of representations to generalize under frozen evaluation. Empirically, our student models outperform recently proposed V-JEPA 2 encoders under frozen backbone evaluation across diverse benchmarks. They are also more compute‑optimal: at matched pretraining FLOPs, our method achieves higher probing accuracy, and its scaling curves dominate V‑JEPA’s accuracy–FLOPs Pareto frontier. \nFinally, we find that student quality is remarkably robust to teacher quality: high-performing students emerge even with small, sub-optimal teachers. This points to a compute budget allocation that should overwhelmingly favor the student.\nThese results position SALT as a simple, scalable, and compute‑efficient alternative to EMA‑based self‑distillation for video representation learning.", "tldr": "SALT: A simple, scalable, and compute‑efficient alternative to EMA‑based self‑distillation for video representation learning.", "keywords": ["SALT", "video", "SSL", "video_representation_learning", "masked_video_modeling", "MAE", "JEPA", "latent_space_ prediction"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a52ec8238aa75a07755d64c20c24f5dbe85eb433.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a modification to the V-JEPA framework, which consists of two stages. In the first stage, a teacher network is pretrained on a dataset—VMAE-v2 in this case. In the second stage, a significantly larger student network is trained to predict the teacher’s features using the V-JEPA loss function. Since the teacher is considerably smaller than the student, the total computational cost is dominated by the student. Nevertheless, this approach achieves higher performance and converges faster than the original V-JEPA framework, which trains the teacher and student jointly via EMA, under the same computational budget."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of employing a smaller teacher than the student is both interesting and computationally efficient.  \n2. The method’s two-stage design simplifies training and is likely to offer greater stability compared to EMA-based teacher frameworks such as V-JEPA.  \n3. The approach is supported by an extensive set of experiments ensuring fair comparisons in terms of total compute and performance.  \n4. The method is evaluated across diverse settings, including image and video datasets as well as intuitive physics understanding, providing a comprehensive assessment of its effectiveness."}, "weaknesses": {"value": "1. The main motivation behind the proposed framework is to address the instability of V-JEPA, which reportedly requires meticulous hyperparameter tuning. However, the paper does not provide empirical evidence to support this claim. Is your model more stable with respect to the training hyperparameters listed in Table 5? It would strengthen the argument to include a comparison between V-JEPA and SALT in terms of stability across those hyperparameters.\n\n2. The paper does not include an ablation study on the impact of teacher pretraining. How sensitive is the proposed method to the choice of the pretrained teacher? What is the rationale for using VMAE-v2 specifically? I encourage the authors to include an ablation for this factor, at least for a ViT-B teacher. For example, a teacher could be selected from one of the following works: [1, 2, 3, 4].\n\n3. The concept of training from a frozen teacher network has been previously explored in [2, 4], although in those works the teacher is an image model. It would be fair to discuss these approaches in the related work section, as the underlying idea is conceptually related to the current paper.\n\n4. Is there a particular reason why the proposed method is only evaluated on video datasets? Was it also tested on images, and if so, did it perform comparably? Including such an analysis could clarify whether the idea of using a smaller teacher than the student is specific to video settings.\n\n---\n\n**References**\n\n1. *Masked Motion Encoding for Self-Supervised Video Representation Learning*, CVPR 2023  \n2. *SIGMA: Sinkhorn-Guided Masked Video Modeling*, ECCV 2024  \n3. *MGMAE: Motion Guided Masking for Video Masked Autoencoding*, ICCV 2023  \n4. *SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning*, CVPR 2025"}, "questions": {"value": "Please refer to weaknesses. I tried to provide clear questions there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gWMfC8JfzM", "forum": "3cB9243E9i", "replyto": "3cB9243E9i", "signatures": ["ICLR.cc/2026/Conference/Submission15202/Reviewer_k4fL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15202/Reviewer_k4fL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676578187, "cdate": 1761676578187, "tmdate": 1762925504021, "mdate": 1762925504021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits video self-supervised learning under the JEPA framework and introduces SALT , which replaces the EMA-updated teacher with a frozen one.\nBy decoupling teacher and student training into pixel reconstruction and masked latent prediction, the method improves efficiency, transparency, and scalability. Experiments show that SALT outperforms V-JEPA2 at matched compute while remaining robust to teacher quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The method consists of two stages: pixel reconstruction and masked latent prediction, allowing small teacher models to train large-scale student models. Each training stage has a clear loss function with physical meaning, enabling SALT to avoid representation collapse.\n2.On multiple mainstream video understanding benchmarks, SALT consistently outperforms V-JEPA2 with the same model architecture, particularly excelling on motion understanding tasks (SSv2), demonstrating stronger temporal modeling capabilities.\n3.SALT can successfully train larger student models using smaller teacher models, improving computational efficiency and scalability."}, "weaknesses": {"value": "1.Although the teacher is frozen, a pixel reconstruction training stage is still required first, introducing an additional pretraining step and making the process slightly longer compared to fully end-to-end methods.\n2.While SALT performs exceptionally on temporal tasks (SSv2), its improvements on appearance-based tasks (such as K400) are modest, and it still lags slightly behind the latest methods (PEcoreG, InternVideo2-1B).\n3.Although the overall computational cost is lower, SALT’s two-stage process still requires training the teacher model first, increasing the overall implementation complexity and making the workflow slightly longer compared to end-to-end EMA-based methods.\n4.Missing code reduces reproducibility."}, "questions": {"value": "1.Although SALT claims lower computational cost than the EMA baseline, it still requires training the teacher before the student. Compared to end-to-end EMA methods, does it truly save overall training time?\n2.Experiments show that SALT achieves significant improvements on SSv2, but only limited gains on K400. Is SALT more suited for temporal modeling? If the task primarily involves appearance or scene recognition, does the method still maintain a clear advantage?\n3.SALT relies on a two-stage teacher-student training, which adds some training complexity. Additionally, its performance gains on the K400 dataset are modest and fall short compared to the latest methods (PEcoreG, InternVideo2-1B)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ogifavZUn", "forum": "3cB9243E9i", "replyto": "3cB9243E9i", "signatures": ["ICLR.cc/2026/Conference/Submission15202/Reviewer_R6kh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15202/Reviewer_R6kh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806325205, "cdate": 1761806325205, "tmdate": 1762925503604, "mdate": 1762925503604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SALT (Static-teacher Asymmetric Latent Training), a two-stage, unregularized framework for video representation pretraining. The authors challenge the conventional approach of using dynamic teachers in self-distillation architectures, such as V-JEPA, and propose that a less optimal, pre-trained teacher is sufficient for distilling a student model. Specifically, they pretrain a teacher model using a pixel reconstruction objective, freeze the teacher model, and then use it to distill the student, demonstrating performance improvements over V-JEPA under fair settings. Extensive experiments validate the effectiveness of SALT and provide justification for its design choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an alternative to traditional V-JEPA, which relies on EMA and stop-gradient operations.\n2. The proposed SALT method demonstrates superior performance and greater efficiency compared to V-JEPA.\n3. The experiments are thorough and well-executed, including ablation studies that effectively validate the contributions of each design element in the proposed method."}, "weaknesses": {"value": "1. The VJEPA framework utilizes EMA-based self-distillation, where performance improvement is driven by the joint optimization of both the teacher and student models. However, it seems counterintuitive that SALT employs a smaller-scale or less capable teacher to distill a more performant student. This discrepancy warrants a more detailed analysis and clarification.\n2. The authors claim that \"a high-performing teacher is not necessary to train a high-quality student.\" This assertion appears to hold under the assumption of fixed total training steps across two stages. However, in an unconstrained setting, does the performance of the teacher model positively correlate with that of the student? Specifically, if teachers are trained for 20k, 40k, and 80k steps, respectively, and students are trained for 80k steps in all cases, would the performance of the models improve sequentially?\n3. The proposed method primarily relies on pretraining and distillation, while it seems that not explicitly present temporal optimization. Recent advancements in image-to-video distillation [1-3] have incorporated temporal relationships into strong representation models during distillation. The authors are encouraged to compare their approach with these recent methods in the related work section, in order to investigate how temporal information can be leveraged in representation distillation.\n\n[1] Liu Y, Xu Q, Wen P, et al. When the future becomes the past: Taming temporal correspondence for self-supervised video representation learning. In CVPR, 2025.\n[2] Li R, Liu D. Spatial-then-temporal self-supervised learning for video correspondence. In CVPR, 2023.\n[3] Hu Y, Wang R, Zhang K, et al. Semantic-aware fine-grained correspondence. In ECCV, 2022."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MjfS46P9zA", "forum": "3cB9243E9i", "replyto": "3cB9243E9i", "signatures": ["ICLR.cc/2026/Conference/Submission15202/Reviewer_ZRSW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15202/Reviewer_ZRSW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911661802, "cdate": 1761911661802, "tmdate": 1762925503142, "mdate": 1762925503142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SALT (Static-teacher Asymmetric Latent Training), a two-stage framework for compute-efficient self-supervised video representation learning. The method challenges the necessity of the complex, online, EMA-updated teacher used in state-of-the-art models like Video Joint Embedding Predictive Architectures (V-JEPA). The core design of this work is borrowed from Masked Video Distillation, using two-stage training: (1) At stage-1, a teacher encoder is trained using a simple pixel-reconstruction objective, similar to VideoMAE; (2) At stage-2, the teacher encoder is frozen, and a student encoder, along with a predictor, is trained to predict the teacher's latent representations for masked regions of the input video. The results of experiments show that high-performing student models can be trained using smaller teachers—models. The main results show that SALT outperforms the strong V-JEPA 2 baseline on multiple video understanding benchmarks (e.g., Something-Something-v2, Kinetics-400), especially when matched for computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental validation is extensive and rigorous. The systematic comparison against strong baselines, including a retrained V-JEPA 2, convincingly demonstrates SALT's superiority in both accuracy and efficiency. The paper includes a comprehensive set of ablations that analyze the impact of key design choices, such as the teacher's training data, masking strategy, model size, and the compute allocation between teacher and student.\n\n2. The demonstration that the student's training loss is highly predictive of downstream performance (Figure 3) is a major practical advantage.\n\n3. This paper is very well-written, clearly structured, and easy to follow."}, "weaknesses": {"value": "1. **Limited Novelty in the Self-Supervised Learning Framework**: The proposed two-stage distillation framework, which uses a frozen teacher to provide target features, shares a significant conceptual overlap with prior work. Specifically, the core idea was proposed and studied in Masked Video Distillation (MVD)[1] two years ago and has been scaled and improved in subsequent works like VideoPrism[2] and InternVideo2[3]. Therefore, the method's design is not an entirely novel concept. The paper's primary contributions are better framed as a large-scale empirical study that (a) provides a fair and direct comparison against a strong momentum-encoder baseline (V-JEPA 2) on larger datasets and models, and (b) uncovers some valuable insights like smaller teachers can produce strong students. Key conclusions related to the method's design (such as the student outperforming the teacher or a frozen teacher being superior to an EMA teacher) are consistent with findings in these earlier studies. It would be beneficial for the authors to more precisely position their contributions in the introduction, emphasizing the scientific insights and empirical discoveries rather than the originality of the algorithmic framework itself.\n2. **Lack of Deeper Analysis on Teacher Model Size Saturation**: The experiments in Figure 6 suggest that, under the paper's settings, increasing the teacher's model size does not yield significant performance gains for the student. However, the paper does not offer a deep theoretical or experimental analysis to explain this phenomenon. For instance, an analysis of the target features provided by teachers of different sizes, or an investigation into how this finding correlates with the student's model size, would provide much greater insight into the dynamics of the teacher-student relationship in this framework.\n3. **Unexplored Performance Bottleneck at Larger Scales**: Table 1 reveals a slight performance degradation on the SSv2 benchmark as the SALT model is scaled from 1B to 2B parameters. The paper does not address or analyze this observation. This trend suggests that SALT may face a performance bottleneck at very large scales, and an investigation into the potential causes would significantly strengthen the paper's contribution to understanding the scalability of this approach.\n\n[1] Wang, et al. \"Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning.\" CVPR 2023.\n\n[2] Zhao, Long, et al. \"Videoprism: A foundational visual encoder for video understanding.\" arXiv preprint arXiv:2402.13217 (2024).\n\n[3] Wang, Yi, et al. \"Internvideo2: Scaling foundation models for multimodal video understanding.\" ECCV 2024."}, "questions": {"value": "1. The V-JEPA paper reported frozen evaluation results using a pretrained model from MVD. Given that this represents a highly relevant baseline for a frozen-teacher approach, the authors should include the results of this baseline in the main results table.\n\n2. Have the authors considered the possibility of iterating the training process? Specifically, could the student model trained in Stage 2 be frozen and used as a new teacher to train a subsequent, potentially stronger, student model? This appears to be a natural extension of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oiYqxuBhHm", "forum": "3cB9243E9i", "replyto": "3cB9243E9i", "signatures": ["ICLR.cc/2026/Conference/Submission15202/Reviewer_3hkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15202/Reviewer_3hkH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993641247, "cdate": 1761993641247, "tmdate": 1762925502600, "mdate": 1762925502600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}