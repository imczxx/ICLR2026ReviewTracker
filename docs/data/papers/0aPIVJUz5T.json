{"id": "0aPIVJUz5T", "number": 21335, "cdate": 1758316354722, "mdate": 1759896927823, "content": {"title": "Benefits and Limitations of Communication in Multi-Agent Reasoning", "abstract": "Chain-of-thought prompting has popularized step-by-step reasoning in large language models, yet model performance still degrades as problem complexity and context length grow. By decomposing difficult tasks with long contexts into shorter, manageable ones, recent multi-agent paradigms offer a promising near-term solution to this problem. However, the fundamental capacities of such systems are poorly understood. In this work, we propose a theoretical framework to analyze the expressivity of multi-agent systems. We apply our framework to three algorithmic families: state tracking, recall, and $k$-hop reasoning. We derive bounds on (i) the number of agents required to solve the task exactly, (ii) the quantity and structure of inter-agent communication, and (iii) the achievable speedups as problem size and context scale. Our results identify regimes where communication is provably beneficial, delineate tradeoffs between agent count and bandwidth, and expose intrinsic limitations when either resource is constrained. We complement our theoretical analysis with a set of experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical outcomes confirm the tradeoffs between key quantities predicted by our theory. Collectively, our analysis offers principled guidance for designing scalable multi-agent reasoning systems.", "tldr": "We introduce a theoretical framework for analyzing the expressivity of multi-agent reasoning systems, derive tradeoffs between agent count, communication, and scalability, and validate our predictions with controlled LLM experiments.", "keywords": ["chain-of-thought prompting", "multi-agent systems", "reasoning", "expressivity", "inter-agent communication", "scalability", "large language models", "algorithmic analysis"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33010373504ce3cdc5b9766db95c8379a275be53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the trade-off between expressive power and communication cost in multi-agent reasoning systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The ternary structure of Size/Depth/Communication is used to characterize the \"cost of scalability\" in multi-agent reasoning, and an \"impossible zone\" is proved: when communication is O(1), it is impossible to reduce the depth to O(Size/w) as the number of agents increases (Proposition 4.2). This has significant implications for system design.\n\n2. Section 3.1 and the discussion section map the three theoretical intervals to existing pipelines (such as CoA, LLM×MapReduce, LongAgent, hierarchical merging, etc.), making them operable."}, "weaknesses": {"value": "1. The dataset is primarily synthetic and lacks specific comparisons on real-world document question answering, complex RAGs, or graph reasoning tasks.\n\n2. The robustness to noise and instruction following failure is only explained in §5.2 as a phenomenon of \"constant extra token overhead\", lacking a systematic analysis.\n\n3. Without integrating these protocols into actual multi-agent systems, it is impossible to observe their true guidance for MAS systems."}, "questions": {"value": "1. How do the three types of protocols degrade under message loss/delay/content error? Can you provide an upper bound for noisy communication (e.g., converting O(k) rounds into a success probability with an error rate ε)?\n\n2. Does the main proposition still maintain optimality under soft attention, finite precision, and fixed width?\n\n3. Can you provide the token overhead required for training and inference in a specific scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ohngNwSNeT", "forum": "0aPIVJUz5T", "replyto": "0aPIVJUz5T", "signatures": ["ICLR.cc/2026/Conference/Submission21335/Reviewer_7TtA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21335/Reviewer_7TtA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810433636, "cdate": 1761810433636, "tmdate": 1762941702888, "mdate": 1762941702888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical framework for analyzing the expressivity of multi-agent systems in LLM reasoning tasks. The authors group multi-agent reasoning approaches in three algorithmic families (associative recall, state tracking, and k-hop reasoning) and propose theoretical claims on the number of agents required, the agents' communication structure, and speedups that can be achieved with respect to a problem and context size. The evaluation on synthetic benchmarks using Llama models provides the experimental evidence to support the theoretical claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The formalization of multi-agent systems as directed acyclic graphs in Definition 3.1provides a rigorous foundation for analysis. The distinction between CoT edges and communication edges is well-defined.\n2. Figure 1 effectively illustrates the three discussed communication protocols making it easier to understand the paper.\n3. The experimental results demonstrate that theoretical statements are closely aligned with practical evaluation results."}, "weaknesses": {"value": "1. The Definition 3.1 states that the agents can only send/receive a single token at each time step. This restriction does not reflect the related work in multi-agent systems, where agents typically are not constrained to communicate through a single-token representation.\n2. While Appendix D provides prompts and hyperparameters, some choices lack justification. For example, why 8 agents are used for majority voting, why the specific chunk sizes were chosen?\n3. The framework explicitly excludes multi-agent debate, voting mechanisms, and self-consistency approaches, which are popular in related work. While the authors justify this by focusing on expressivity rather than stochasticity-based improvements, this limits the framework's applicability."}, "questions": {"value": "1. Justify the hyperparameter choices (see Weakness 2)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uz3ug6pc8h", "forum": "0aPIVJUz5T", "replyto": "0aPIVJUz5T", "signatures": ["ICLR.cc/2026/Conference/Submission21335/Reviewer_6i2W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21335/Reviewer_6i2W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873123366, "cdate": 1761873123366, "tmdate": 1762941702652, "mdate": 1762941702652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to resolve: When and how does inter‑agent communication provably help LLM multi‑agent reasoning as problem size and context scale?  CoT and test‑time compute improve reasoning but degrade with long contexts; recent multi‑agent systems split tasks, yet their expressive capacity and resource tradeoffs lack theory. The work targets principled guidance for scalable designs by formalizing multi‑agent protocols as Transformer‑computable DAGs with size, computation depth (wall‑clock proxy), and communication budget, and asks for achievable/necessary tradeoffs across tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear and actionable formalization (Section 3). The paper models multi-agent systems as Transformer-computable labeled DAGs and defines three concrete metrics: Depth(N), Size(N), and communication budget. Definitions 3.1–3.3 make the theoretical analysis both reproducible and extensible. This formalism is a valuable contribution to the emerging theory of LLM-based multi-agent systems. The approach seems inspired by work on GNN+MARL, such as https://arxiv.org/abs/2210.13148.\n\nProposition 4.1 (Conservation of size) and Proposition 4.2 (bounded communication implies O(1) single-agent CoT) together identify three feasible operating regimes. Importantly, they rule out the appealing but impossible scenario of \"O(1) communication with strong depth reduction.\" The accompanying figure and discussion in Section 4.1 and Figure 2 are clear and immediately applicable to system design.\n\nProtocol designs with matching bounds. The paper demonstrates that O(1) depth and communication is achievable (Proposition 4.3), providing formal justification for simple manager-worker designs on retrieval tasks (page 6). For more complex tasks, prefix-sum-style cascades achieve depth O(log w + N/w) with optimal communication Θ(w) and size Θ(N). The lower bounds in Proposition 4.7 match these upper bounds up to logarithmic factors (pages 6–7).\n\nThe experiments mirror the theoretical predictions well, which strengthens confidence in the framework's soundness. Figures 3–5 and the additional results in Appendix E (Figures 6–9) all align with the theoretical characterizations."}, "weaknesses": {"value": "Idealized communication model. The theoretical model assumes single-token messages and uses a UHAT-style assumption. The proofs in Appendix C.2 rely on these idealizations to obtain the O(1) depth equivalences. It would strengthen the work to discuss robustness when messages span multiple tokens and agents don't perfectly adhere to the protocol. How much do the bounds degrade in more realistic settings?\n\nDepth lower bounds may underestimate practical costs. The authors acknowledge that instruction-following overhead (extra boilerplate tokens) affects depth lower bounds in practice (Section 5.2). However, the cost model doesn't explicitly price this overhead in terms of compute or latency units, which makes it harder to apply the theoretical predictions to real systems.\n\nLimited experimental scope. The experiments focus on synthetic tasks (recall, parity, k-hop reasoning) using Llama-3.* and EXAONE via TogetherAI. While these validate the theory, there's no evaluation on real-world long-context reasoning tasks like complex mathematical problem-solving, program synthesis, or long-document question answering. In these settings, memory management, tool use, and noisy communication patterns would provide a more complete picture of practical applicability (Section 5 & Appendix D).\n\nGap between theoretical metrics and practical deployment. While Definitions 3.1–3.3 are mathematically precise, an explicit mapping from \"edges/tokens\" to actual prompt token budgets and per-round latency would help practitioners. For example, when reading Figure 6, it would be valuable to understand how to translate the theoretical metrics into wall-clock predictions for system design."}, "questions": {"value": "I'd like to discuss a few points with the authors:\n\nMulti-token messages. Your proofs assume single-token communication, and you mention that extension to bounded-length words is \"straightforward\" (Section 3). Could you clarify how the depth and communication lower bounds would change when messages are b-token words? Specifically, how do the results differ when b is constant versus when b = O(log N)?\n\nRobustness to asynchrony and failures. What happens if workers reply at different times or occasionally drop messages? Can the Conservation-of-size argument and Proposition 4.2 be recovered under such conditions, or does this break the O(1) single-agent simulation result? (Related to Appendix C.2)\n\nPractical cost model. The figures show \"computation depth\" versus \"communication edges,\" which are clean theoretical metrics. Could you provide a cost model that maps these to concrete latency (in terms of round-trips) and FLOPs? This would help designers choose w(N) under realistic compute budgets when implementing these protocols. (Related to Figure 6 and Appendix E)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zR2NL4rjWs", "forum": "0aPIVJUz5T", "replyto": "0aPIVJUz5T", "signatures": ["ICLR.cc/2026/Conference/Submission21335/Reviewer_YPeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21335/Reviewer_YPeN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935837178, "cdate": 1761935837178, "tmdate": 1762941702337, "mdate": 1762941702337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a formal framework to study multi-agent reasoning in LLMs, defining clear complexity measures such as width, depth, and communication. It analyzes three representative task families—associative recall, state tracking, and k-hop reasoning—and characterizes their optimal trade-offs. Both upper and lower bounds are derived, showing when communication helps and when it saturates. Empirical simulations with pretrained LLMs on synthetic tasks validate the theoretical predictions and illustrate the regimes where multi-agent protocols are beneficial."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper lays out a clean formal model for multi-agent reasoning in LLM settings, making the resource trade-offs (width/depth/comm) precise.\n-It analyzes three task families with sharp trade-offs, and the table does a good job of clearly summarizing the regimes.\n- Both upper and lower bounds are provided in addition to constructions."}, "weaknesses": {"value": "The paper does not include simulations on realistic tasks such as multi-document summarization or noisy graph question-answering, so it remains unclear how well the proposed framework extends to complex, real-world reasoning settings."}, "questions": {"value": "- Have you considered running simulations on more realistic tasks—such as multi-document summarization or noisy graph QA—to test whether the theoretical trade-offs observed in your framework persist in practical, large-scale reasoning settings?\n- How would your theoretical bounds change (if at all) when agents are allowed to overlap input chunks (i.e., share some context) or when messages can be multi-token (not just one token)? Have you thought about generalising to that regime, and do you expect qualitatively new regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3WxqnEVYDc", "forum": "0aPIVJUz5T", "replyto": "0aPIVJUz5T", "signatures": ["ICLR.cc/2026/Conference/Submission21335/Reviewer_PxAy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21335/Reviewer_PxAy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239513185, "cdate": 1762239513185, "tmdate": 1762941701980, "mdate": 1762941701980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}