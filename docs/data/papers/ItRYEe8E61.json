{"id": "ItRYEe8E61", "number": 18908, "cdate": 1758291893884, "mdate": 1759897074180, "content": {"title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs", "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio–visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question–answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio–visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.", "tldr": "OmniVideoBench is a benchmark for comprehensive evaluation of synergistic audio–visual understanding in Omni MLLMs.", "keywords": ["Multimodal Reasoning", "MLLM Benchmark", "Text", "Audio", "Video"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91d45f0008e1b28d00f8279322f8afad0e4c26b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose OmniVideoBench, a benchmark for audio–visual understanding in videos. It consists of 1000 QA pairs from 628 videos ranging from several seconds to 30 minutes, within 13 question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, etc. They also evaluate multiple MLLMs on OmniVideoBench to performance investigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novelty\n\nCurrently,  the video understanding community mainly focuses on understanding and reasoning from the visual modality. It ignores the fact that video is the combination of visual and audio clues. This OmniVideoBench provides a bench for comprehensively evaluating reasoning capabilities across both modalities. Hence, it somehow shows the value of this bench for multimodal video understanding.\n\n* Clarity\n\nThe paper is well-written with good structure. Hence, the clarity is basically good.\n\n* Significance\n\nThis paper focuses on evaluating audio–visual understanding capacity of MLLMs, which is an important and practical problem for video understanding. Hence, the significance is basically OK for video research community."}, "weaknesses": {"value": "* Question Type \n\nThe authors choose 13 types for QA pairs. Please further explain why to choose these types. Is it sufficient to evaluate audio–visual understanding in videos? \n\n* Method Insight\n\nIt woule be more interesting to investigate or indicate how to design MLLMs to boost the tasks in this benchmark.\n\n* Small Size\n\nThe authors collected only 628 original videos. The small number of videos would restrict the generalization of this benchmark."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NUE3KcDHJh", "forum": "ItRYEe8E61", "replyto": "ItRYEe8E61", "signatures": ["ICLR.cc/2026/Conference/Submission18908/Reviewer_xiBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18908/Reviewer_xiBm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808210494, "cdate": 1761808210494, "tmdate": 1762930898984, "mdate": 1762930898984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes OmniVideoBench, a new benchmark designed to address key limitations in existing video datasets—namely, the lack of systematic evaluation of audio–visual co-reasoning and inconsistencies in logical task composition. The benchmark is constructed from 628 real-world videos ranging from a few seconds to around 30 minutes, covering three types of audio (speech, sound, and music). It includes 1,000 multiple-choice questions spanning 8 major categories, 68 subcategories, and 13 task types, each accompanied by human-annotated step-by-step reasoning chains.\n\nExperimental results show that current models struggle significantly on this benchmark. While the best model, Gemini-2.5-Pro, only achieves 58.90% accuracy, and most open-source models perform near random. Besides, Long video understanding remains a major challenge for most models. Notably, performance drops sharply for music-dominated audio even for the strongest models. \n\nOverall, OmniVideoBench combines long temporal structure with audio–visual complementarity, providing a more realistic and comprehensive testbed for advancing multimodal video reasoning research."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Broad Coverage and Task Diversity。 The dataset spans 8 high-level categories, 68 subcategories, and 13 task types, with video durations ranging from 4 to 1955 seconds. It also explicitly includes an Ultralong category for videos longer than 10 minutes.\n* Step-by-Step Human-Annotated Reasoning Chains. Each question is accompanied by a human-labeled step-by-step reasoning chain, in terms of modality, evidence, and inference triples.\n* Data Quality Control. The authors take multiple steps to ensure data quality, including a three-stage filtering pipeline and the exclusion of videos with large-scale on-screen subtitles that might leak answers or bias model predictions.\n* Comprehensive Evaluation. The benchmark is evaluated using a wide range of models, including both open-source and close-source of varying scales."}, "weaknesses": {"value": "* Limited Per-Task Coverage. The dataset contains 1,000 questions spread across 13 distinct task types, resulting in fewer than 100 examples per task on average. This limited coverage may constrain the robustness of task-specific evaluation and generalization analysis.\n* Unbalanced Audio Category Distribution. The distribution of audio types is heavily skewed—Speech accounts for over three-quarters of the dataset, while Music constitutes only 9.1%. This imbalance may bias models and limit insights into performance under underrepresented audio conditions.\n* Lack of Human Baseline. Although the conclusion emphasizes a large gap between human and model performance, no human baseline is reported in the experiments. \n* Dataset unavailable. The authors did not provide the dataset link in the paper."}, "questions": {"value": "1. What is the core distinction between this work and WorldSense that is mentioned in Table 1?\nTable 1 suggests that the two benchmarks share similar characteristics across several dimensions—including modality coverage, domain diversity, video type, audio type, and answer format. A more explicit comparison would help clarify the novel contributions of OmniVideoBench.\n2. Qwen2.5-Omni performs better on WorldSense than on OmniVideoBench. Does this imply that OmniVideoBench presents a more difficult challenge?\nBeyond video length, are there other factors or task-level distinctions that contribute to the increased difficulty of OmniVideoBench? Providing such an analysis would help justify the benchmark’s added value.\n3. What is the significance of emphasizing long videos?\nTable 1 shows that SOTA model performance is comparable between the 5–10 minute and 10+ minute video subsets. Additionally, WorldSense already includes videos of up to 10 minutes. Could the authors clarify what unique challenges OmniVideoBench introduces with longer videos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IhHSlKhEfk", "forum": "ItRYEe8E61", "replyto": "ItRYEe8E61", "signatures": ["ICLR.cc/2026/Conference/Submission18908/Reviewer_iHcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18908/Reviewer_iHcc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808247343, "cdate": 1761808247343, "tmdate": 1762930898274, "mdate": 1762930898274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniVideoBench, a large-scale benchmark designed to evaluate the collaborative audiovisual reasoning capabilities of multimodal large language models. The benchmark comprises 1,000 manually annotated high-quality question-answer pairs (QA) across 628 videos, each featuring explicit step-by-step reasoning chains indicating modalities and evidence. OmniVideoBench spans 8 primary video genres, 68 subcategories, and 13 distinct task types (e.g., temporal, spatial, causal reasoning), structured to comprehensively evaluate modal complementarity and logical consistency. The paper evaluates both open-source and proprietary MLLMs on OmniVideoBench, revealing that model performance lags significantly behind human capabilities, particularly on tasks requiring genuine multimodal integration."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper ensures the richness and coverage of the dataset, comprising 1,000 distinct QA pairs that span a wide range of real-world scenarios, video durations, and audio types. It also contains 13 different task types covering diverse reasoning skills.\n2. The annotation protocol ensures that all questions require true audio-visual integration and stepwise reasoning, with multi-layered filtering to weed out unimodal or bias-prone items."}, "weaknesses": {"value": "1. The paper does not provide statistical results comparing it with other relevant datasets, which fails to highlight the contribution of the proposed dataset.\n2 From Table 1 and Figure 3, it is evident that the vast majority of QA pairs relate to Speech (76.2%) versus Sound (14.7%) and Music (9.1%), creating considerable class imbalance.\n3 The benchmark's positioning relative to several directly analogous or recently proposed audio-visual (AV) benchmarks is incomplete. Recent works, such as AVHBench (Kim et al., 2025) and DAVE (Radevski et al., 2024), are not cited or discussed, nor are specialized audio-visual QA datasets, including AVQA (Yang et al., 2022) and MusicAVQA (Li et al., 2022).\n4. While human annotation is used in construction, the paper does not report human baseline accuracy or response variability for the main test set.\n5. While Figure 1 gives some specific sample breakdowns, the paper lacks a deeper set of qualitative analyses of successful versus failure cases, especially for (a) long video cases and (b) music understanding tasks."}, "questions": {"value": "1. Can the authors provide quantitative statistics on the efficacy of the automated filtering steps in Section 2.4? Specifically, what is the rejection or retention rate at each filtering stage, and what percentage of QA pairs end up truly requiring both modalities?\n2. Please clarify how \"semantic units\" ($S_i$) are operationalized for the semantic distance metric. Is this manual phrase decomposition, or is some NLP toolchain applied? This is crucial to evaluating distractor design reproducibility.\n3. Will human benchmark results (e.g., accuracy, agreement rates) on OmniVideoBench be reported?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B8gD35LbTb", "forum": "ItRYEe8E61", "replyto": "ItRYEe8E61", "signatures": ["ICLR.cc/2026/Conference/Submission18908/Reviewer_rRon"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18908/Reviewer_rRon"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880415556, "cdate": 1761880415556, "tmdate": 1762930897618, "mdate": 1762930897618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents OmniVideoBench, a large-scale, carefully curated benchmark evaluating synergistic audio-visual reasoning in MLLMs. OmniVideoBench consists of 1,000 manually verified QA pairs with explicit step-by-step reasoning traces, based on 628 diverse long-form videos across 8 major categories and 68 subcategories. The benchmark emphasizes logical consistency, modality complementarity, and covers 13 question types relevant to real-world video understanding. The authors systematically filter and annotate the data to ensure questions demand audio-visual integration. Baseline experiments on both open- and closed-source MLLMs reveal a substantial gap to human-level reasoning, particularly regarding music, long videos, and abstract audio understanding."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark features 1,000 high-quality, manually verified QA examples, each annotated with step-by-step reasoning chains with human proofreading. This explicit annotation supports analysis of both model answers and reasoning processes.\n2. Long Videos span 8 major categories and 68 subcategories, ensuring comprehensive coverage and evaluation of a wide range of real-world scenarios.\n3. The authors conducted extensive and comprehensive evaluations and experiments."}, "weaknesses": {"value": "1. Missing Comparisons with Key Recent Audio-Visual Benchmarks: Several highly relevant, recently released benchmarks should be compared and discussed [1,2,3]. \n2. It will be great to see the error analysis on the properties of the questions/items themselves—e.g., what makes some reasoning chains or audio-visual interactions particularly difficult?\n3. More qualitative examples and error cases could be included in the appendix to provide deeper insights into model behavior and help readers better understand the paper.\n\n\n\n\n\n\n\n\n[1] Sung-Bin, Kim, et al. \"Avhbench: A cross-modal hallucination benchmark for audio-visual large language models.\" arXiv preprint arXiv:2410.18325 (2024).\n[2]Chowdhury, Sanjoy, et al. \"Avtrustbench: Assessing and enhancing reliability and robustness in audio-visual llms.\" arXiv preprint arXiv:2501.02135 (2025).\n[3[ Sakshi, S., et al. \"Mmau: A massive multi-task audio understanding and reasoning benchmark.\" arXiv preprint arXiv:2410.19168 (2024)."}, "questions": {"value": "Please check the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WAP71LavDr", "forum": "ItRYEe8E61", "replyto": "ItRYEe8E61", "signatures": ["ICLR.cc/2026/Conference/Submission18908/Reviewer_adN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18908/Reviewer_adN5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059038823, "cdate": 1762059038823, "tmdate": 1762930882475, "mdate": 1762930882475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}