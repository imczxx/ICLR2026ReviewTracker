{"id": "G2PRg8cDiV", "number": 22938, "cdate": 1758337286836, "mdate": 1759896839457, "content": {"title": "Barron Approximation and locally optimal weight densities for shallow neural networks", "abstract": "Mean field theories provide optimization results for shallow neural networks by analyzing the weight distribution in infinite width limits. Corresponding results for finite sized networks are obtained by particle approximations, for which sharp quantitative bounds are still an open problem. In this paper, we consider a modified mean field loss, which allows a more fine grained control over finite sized networks. We prove convexity and equidistribution properties, which directly lead to Barron type approximation results for networks sampled from  local loss minimizers. We demonstrate that particle approximations of the new loss function naturally lead to gradient descent methods with dropout type regularization.", "tldr": "We provide Barron type approximation bounds for networks sampled from weight distributions that are local optima of a new mean field type loss.", "keywords": ["approximation", "Barron spaces", "mean field theory", "optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3542a1f47becb5e6b1a542c855bce99be3586b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper views the finite-width one-hidden-layer MLP as an approximation of its infinite limit with the integral form. Setting up the target function as an integral, the paper studies the approximation error using a finite-sized network. Based on the specific parameterization, the paper shows several main results: $i)$. the optimization landscape of the probability measure is convex; $ii)$. the local minimizers of the objective satisfies equidistribution property, i.e. the norm of the integrand are independent of the sample; and $iii)$. second-layer weights can be explicitly constructed from the integral form. Lastly, the paper also studied Wasserstein gradient flow and regular gradient descent within the particle approximation, and illustrated the connection with dropout."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provided a detailed study of approximating the target function using the finite-sized network, and derived the corresponding approximation based on a newly defined stable Barron norm.\n2. The paper demonstrated a way to construct the second layer weight given a ideal Radon-Nikodym derivative.\n3. The paper derive the form of gradient descent by approximating the expectations in the objective with a sample mean, and shows the connection between gradient descent and dropout."}, "weaknesses": {"value": "1. By its current form, the paper is hard to be appreciated by a general audience. In particular, the paper starts from a particular parameterization of the neural network in Eq. (1) and does not motivate why this particular parameterization is chosen. In particular, it is unclear why the second layer weight $a_{\\pi}(w)$ need to depend on the first layer weight $w$. Moreover, the paper set up the target function in Eq. (7) and argues that the measure $\\phi(w)$ is hard to estimate because it is a signed measure, but it is unclear whether the target function defined over a signed measure is something that have practical interest. Lastly, the significance of the derived results, including the approximation bound depending on the stable Barron measure, the equidistribution property, and the convexity of the objective, are vague. In general, the paper lacks a motivation of its mathematical setup, a connection with practical scenarios, and a highlight of its significance.\n2. The paper did not include discussion about its relationship with previous work. In particular, how is Eq. (5) fundamentally different from the mean-field parameterization? How does the definition of Eq. (7) relate to the reproducing kernel Hilbert space?\n3. Estimating the second layer weights seems to suffer from practicality issue. The estimation is based on the approximation of the expectation based on sampling, where for each sample a minimization problem needs to be solved.\n4. Connection between Eq. (29) and the dropout algorithm is not clear.\n5. Notations are not explicitly introduced in the main text but deferred to the appendix, making the paper even harder to decode. The definition of $f$ seems to be not clear. In particular, $f$ is defined in Eq. (7), but in Eq. (15) it is not clear why $f$ from Eq. (7) have an explicit dependency on $m$."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qUIPgGsl5X", "forum": "G2PRg8cDiV", "replyto": "G2PRg8cDiV", "signatures": ["ICLR.cc/2026/Conference/Submission22938/Reviewer_xqKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22938/Reviewer_xqKn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581120478, "cdate": 1761581120478, "tmdate": 1762942446418, "mdate": 1762942446418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a modified mean-field objective for shallow ReLU networks that directly optimizes the expected sampling error of finite networks rather than the usual continuum loss. Concretely, the authors (i) introduce the new loss with the expectation taken outside the norm; (ii) prove that this loss is convex over admissible weight distributions; (iii) show that any local minimizer enforces an equidistribution condition implying that each sampled neuron contributes with equal magnitude; (iv) obtain Barron-type approximation rates for networks sampled from these minimizers; and (v) connect particle approximations and gradient-descent training to dropout-like regularization. Two variants are developed: an idealized setting with exact outer weights and a practical construction based on averaging optimal outer coefficients conditioned on one neuron."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The theory is crisp and tight: the paper proves finite‑width approximation theory in the Barron space, and prove equidistribution property of the minimizer as a necessary optimality condition arising directly from the convex objective, which is an interesting observation. The whole theoretical story is complete."}, "weaknesses": {"value": "While the finite particle approximation algorithm is clearly described, the manuscript includes no experiments to verify the equidistribution, or approximation property in practice."}, "questions": {"value": "Could the authors include a small synthetic experiment that visualizes the learned $\\pi$ vs. a classical baseline and measures finite‑width error vs. $m$ under your loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vS9ImmQvHj", "forum": "G2PRg8cDiV", "replyto": "G2PRg8cDiV", "signatures": ["ICLR.cc/2026/Conference/Submission22938/Reviewer_Behz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22938/Reviewer_Behz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875044054, "cdate": 1761875044054, "tmdate": 1762942446067, "mdate": 1762942446067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the approximation theory and optimization landscape of shallow neural networks through a modified mean field perspective. The authors introduce a new loss function that directly optimizes the expected sampling error of finite-width networks, diverging from traditional mean field formulations. The work establishes convexity of the loss with respect to the weight distribution, analyzes the equidistribution of weights at local minima, and demonstrates Barron-type approximation rates under both idealized and practical outer weight choices. The paper further discusses how particle approximations of the new loss naturally result in gradient methods akin to dropout regularization, aiming to bridge mean field insights to practical finite-width training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Discussion about best choice of distribution $\\pi$: By reformulating the mean field loss so that the expectation is taken outside the norm, the article considers how to choose a distribution to minimize the Barron norm on the right side. This is a subtle but important problem for practical applications.\n2. Mathematical rigor: Major theoretical results are clearly stated and appear logically sound. Convexity is proven carefully, and the equidistribution result is both elegant and insightful.\n3. Comprehensive literature review: The introduction situates the work well among mean field theory, Barron approximations, landscape/optimization studies, and recent literature."}, "weaknesses": {"value": "1. Lack of convergence rate improvement: Although the paper considers improvements to the distribution \\pi, its approximation rate with respect to parameter size is not further improved. However, as the paper points out, under some activation functions [1], the approximation rate can actually be improved.\n2. Discussion of completeness issues: The entire paper only focuses on the error in the case of infinite samples. For the case of finite samples, the relationship between approximation error and generalization error is not clear.\n3. The algorithm's motivation is unclear: The algorithm proposed in the paper optimizes the distribution of a parameter. Because the corresponding problem is convex, it is guaranteed to be optimal after convergence and to achieve a controllable convergence rate for the objective function. Subsequently, if a specific implementation of the approximation is desired, it is achieved through specific sampling. However, specific sampling does not guarantee specific error control. Therefore, why consider optimizing this loss instead of directly randomly initializing a neural network and performing gradient descent?\n4. The computational efficiency of the algorithm: Although the article has discretized the expectation, Eq. (28) shows that it has not discretized the space. Will this dual discretization of expectation and space affect the computational efficiency and its operability?\n5. Lack of specific implementation of the algorithm: Although the article proposes a specific algorithm from a theoretical perspective, it does not provide a specific implementation. It would be beneficial if the algorithm's operability and advantages could be demonstrated through experiments.\n\nReference:\n[1] J. W. Siegel and J. Xu. Sharp Bounds on the Approximation Rates, Metric Entropy, and n-Widths\nof Shallow Neural Networks. Foundations of Computational Mathematics,2 4(2):481–537, Apr. 2024."}, "questions": {"value": "1. On approximation-rate gains: Your approach improves $\\pi$, but the approximation efficiency with respect to parameter size does not improve. Which assumptions or settings in your framework hinder this improvement? Can your analysis and algorithm be tweaked to achieve higher approximations? In particular, the efficiency of parameter approximations is related to the curse of dimensionality, which is why people are interested in barron spaces. Could improving the efficiency of approximations with respect to $\\pi$ lead to relevant insights?\n2. Regarding the connection with generalization: The entire article only discusses the impact of approximation rate, and both theoretically and experimentally focuses on $L^2$ loss. For the discretized version with finite parameters, the relationship between approximation error and generalization error needs to be discussed.\n3. On the motivation for optimizing the “distributional” loss: Since the convexity of the loss guarantees optimality, what concrete advantage does optimizing this loss have over directly training a randomly initialized neural network with gradient descent on the original objective? Is there a principled consistency bridge from the convex distributional optimum to a finite network that preserves risk within a provable tolerance rather than in the sense of expectation?\n4. On the discretization and computational efficiency: Eq. (28) discretizes the expectation but keeps the space continuous. In practice, do you also discretize the parameter space (e.g. grid), and if so, how does this affect complexity and error propagation?\n5. On practical implementability and evidence: Could you provide a minimal end-to-end implementation showing how the distribution is optimized and then instantiated as a finite model? Even a small-scale experiment (synthetic or standard benchmarks) demonstrating training time, final risk, and sample efficiency vs. a baseline neural network would clarify the operability and benefits of your approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dZGaqcLhsK", "forum": "G2PRg8cDiV", "replyto": "G2PRg8cDiV", "signatures": ["ICLR.cc/2026/Conference/Submission22938/Reviewer_Wzry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22938/Reviewer_Wzry"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905403110, "cdate": 1761905403110, "tmdate": 1762942445782, "mdate": 1762942445782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies finite shallow models (neural networks with one hidden layer) and their equidistribution properties from the perspective of Barron norms and discrete mean field theory. First, the paper recalls Barron-type convergence bounds for approximations obtained by sampling the neurons, and discusses the equidistribution property associated with optimal sampling measures (section 3.2). Moreover, the paper examines the quadratic network loss as a function of the sampling measure under assumption of exact outer weights, and shows that the loss is convex and its optima have similar equidistribution and convergence properties (section 3.1). After that the paper proposes a replacement of the theoretical exact outer weights by suitable computable alternatives. It is shown that, up to some perturbation terms, these computable alternatives also provide similar equidistribution and convergence properties (section 3.2). Finally, section 4 provides a preliminary informal discussion connecting the proposed constructions to Wasserstein gradient descent and dropout."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Clarity and quality** \n\nThe first half of the paper (sections 1 - 3.1) is very well written. The logical flow is clear, the results are presented as concisely formulated rigorous theorems. However, starting from the averaged outer weights (section 3.2) the exposition was progressively harder to follow for me.    \n\nThe mathematical level of the paper is on the whole quite high. The authors are very familiar with both classical (e.g., Barron's) and state of the art (mean field, Wasserstein flow etc.) approximation theory of shallow models. \n\nThe paper has a large number of appendices containing both background material and proofs of the theorems presented in the main text.  \n\n\n**Originality and significance**\n\nWhile shallow networks is a more or less well-explored topic and its relevance to state-of-the-art deep models is limited, the particular ideas proposed in the paper - e.g., the approximate outer weights $\\\\overline{a}_{\\\\pi}$ - seem to be new and potentially have merit."}, "weaknesses": {"value": "1. The main problem with this work is that it looks unfinished to me. The main new contributions are concentrated in the second half of the paper, but I find the exposition towards the end rather hectic. The results are either not stated as clear theorems, or are stated in terms of some auxiliary quantities (e.g., perturbation terms $\\\\Delta_k$). The importance of these new theorems (lemma 3.4, theorem 3.5) is not clear to me. The last section 4 is particularly vague and hard to appreciate. None of the presented results looks truly useful or memorable.  \n\n2. While the paper aims to address practical finite networks, it presents no actual examples or applications of the developed theory."}, "questions": {"value": "I didn't carefully study the proofs, but there seem to be some minor issues with formulas here and there. \n- It seems that lemma C.1 needs some polishing. The first equality in line 914 is probably meant to be an inequality. On the other hand, it's not very clear why the lemma itself is stated in terms of inequalities: if the norm is euclidean and $X_i, X'_i$ are centered i.i.d., then all three expressions in line 903 seem to be connected by equalities using suitable coefficients.  The proof of the lemma and its applicaton to the proof of theorem 2.1 can be slightly clarified.\n-  The norm $\\\\|f\\\\|_B$ in the seconf part of formula (12) seems to be $|f|_B$; it is identical to the norm in line 231."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jxrj8hVEXm", "forum": "G2PRg8cDiV", "replyto": "G2PRg8cDiV", "signatures": ["ICLR.cc/2026/Conference/Submission22938/Reviewer_CBxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22938/Reviewer_CBxT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017284358, "cdate": 1762017284358, "tmdate": 1762942445468, "mdate": 1762942445468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}