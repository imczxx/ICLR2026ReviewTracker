{"id": "FfedFHs6Tx", "number": 2352, "cdate": 1757062136995, "mdate": 1763689326187, "content": {"title": "AsyncTool: Evaluating the Asynchronous Function Calling Capability under Multi-Task Scenarios", "abstract": "Large language models (LLMs) based agents have demonstrated strong proficiency\nin leveraging external tools to address complex problems. However, existing evalu-\nations largely overlook the temporal dimension of tool invocation, particularly the\npractical impact of inherent tool response latency, and they are typically confined\nto single-task scenarios. In realistic applications, tasks often need to be executed in\nparallel, and overall efficiency critically depends on the ability to utilize idle time\nduring tool response delays. We denote this capability as asynchronous tool calling.\nTo address the lack of evaluation in this area, we propose ASYNCTOOL, which, to\nthe best of our knowledge, is the first benchmark specifically aimed at assessing\nthe asynchronous multitasking abilities of LLM-based agents within interactive\ntool-use contexts. ASYNCTOOL consists of composite tasks with intra-task step\ndependencies that must be executed concurrently while incorporating realistic tool\nresponse delays. Through a hybrid data evolution strategy, we construct a diverse\nand representative asynchronous multitasking dataset that covers multiple scenarios\nand exhibits a wide range of tool use patterns. We further assess performance from\nthree levels, namely Step Level, Sub-Task Level, and Task Level, covering perspec-\ntives from fine-grained to coarse-grained. Extensive experiments on ASYNCTOOL\nshow that even state of the art models experience notable performance degradation\nwhen confronted with complex asynchronous workflows. Our analysis identifies\nthe main failure modes of current tool agents and provides practical guidelines\nfor designing future systems with stronger temporal reasoning and coordination\ncapabilities.", "tldr": "", "keywords": ["asynchronous tool call;benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa9312830f3cbc839493f73f57c0528049ce9042.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "ASYNCTOOL introduces a benchmark for evaluating LLM’ ability to handle asynchronous, multi-task tool use, where tool responses are delayed. The authors construct the dataset by combining 2–3 independent tool-based tasks and simulating one-round response latencies, creating ground-truth trajectories that utilize idle time. Evaluation on top models shows that they struggle with this temporal coordination, exposing a gap in current agentic reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a timely and underexplored gap in agentic LLMs.\n\nThe motivation and novelty are clear: testing agents’ temporal reasoning and their ability to manage idle time.\n\nThe authors conduct a solid set of experiments across diverse models and uncover systematic failure modes."}, "weaknesses": {"value": "1- The benchmark assumes a uniform one-turn delay for all tool calls. In practice, real-world tools have variable latencies. The paper lacks ablations or robustness analysis under different delay distributions, which limits generalizability.\n\n2- Although ASYNCTOOL introduces three levels of evaluation (step, subtask, task-level), these are still based on correctness metrics. There is no explicit metric to quantify how well an agent utilizes idle time or performs efficient scheduling. As a result, true gains in scheduling behavior are not isolated.\n\n3- Presentation issues:\nTable 2 is difficult to interpret; column labels are not clearly defined. Also, the paper does not clearly explain how tool delays are simulated during evaluation (distinct from dataset construction). Qualitative examples showing how delayed or repetitive tool calls from the agent lead to the observed drop in metrics would help readers better connect the evaluation setup to the failure patterns.\n\n4- The benchmark assumes a single reference trajectory that represents the optimal schedule under fixed latency. However, there can be multiple valid ways to use idle time effectively. The evaluation may penalize models that produce alternative but equally efficient schedules, making the metric sensitive to arbitrary ordering choices rather than genuine reasoning quality.\n\n5- The paper does not explore potential solutions. Exploring mitigation strategies, like a basic fine-tuning experiment, would strengthen the contribution."}, "questions": {"value": "See in the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4cahEXpLjw", "forum": "FfedFHs6Tx", "replyto": "FfedFHs6Tx", "signatures": ["ICLR.cc/2026/Conference/Submission2352/Reviewer_adPW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2352/Reviewer_adPW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412104302, "cdate": 1761412104302, "tmdate": 1762916203898, "mdate": 1762916203898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key gap in LLM agent evaluation: most benchmarks ignore tool response latency and focus only on single-task scenarios. The authors propose ASYNCTOOL, which is the first benchmark to evaluate an agent's asynchronous multitasking capabilities by simulating realistic tool response delays and requiring concurrent task execution. A high-quality dataset was also constructed using a four-step pipeline (collection, AI reconstruction, human annotation, multi-task composition). The experiments are conducted at the Step, Sub-Task, and Task levels, and show that even SOTA models struggle significantly with these complex asynchronous workflows, revealing key failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors introduce tool response latency into LLM evaluation, which is critical for real-world deployment but often overlooked in existing benchmarks.\n\n2. The proposed three-level evaluation system (step, sub-task, task) provides a nuanced understanding of model behavior. Including 19 models across different scales strengthens its empirical contribution.\n\n3. The pipeline for building the benchmark is transparent and well-documented, which enhances reproducibility and trustworthiness.\n\n4. The analysis of failure modes (e.g., tool confusion, task neglect) is clear and provides specific guidance for future research."}, "weaknesses": {"value": "1. The use of a fixed \"one-round delay\" is unrealistic. Real-world latency is variable and unpredictable, so the benchmark may only test a simple heuristic rather than true asynchronous management. \n\n2. The reliance on single, deterministic ground-truth paths turns the task into \"plan following\" rather than \"plan generation\". In addition, the benchmark does not take into account resource contention or mutual exclusion (e.g., a tool being locked by one task). All of these shortages limits the benchmark's realism.\n\n2. The current evaluations only focus on correctness (e.g., accuracy, F1), not considering efficiency (such as total task completion time, token usage, or number of tool calls), which is important in real-world multitasking scenarios.\n\n4. The evaluations do not penalize excessive or unnecessary tool calls, which could lead to gaming the system through brute-force switching strategies."}, "questions": {"value": "1. Could you clarify how the latency of each tool call is determined? Is it uniformly set to one round for all tools? If so, what is the justification for this simplification, and do you plan to support heterogeneous or stochastic latency in future versions?\n\n2. Have you conducted the experiments without tool-call latency (i.e., synchronous setting)? Such a comparison would help isolate the impact of asynchrony and better demonstrate the value of the benchmark.\n\n3. If a model is strong at single-task tool use, it might achieve high scores by frequently switching tasks with a carefully crafted prompt. Have you observed such behavior? Are there any mechanisms or penalties in place to ensure that the evaluation rewards strategic scheduling rather than heuristic over-invocation?\n\n4. Can you elaborate on the trade-off between evaluation simplicity (using deterministic paths) and realism? Is ASYNCTOOL evaluating planning or just the ability to follow a complex instruction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FMIqQW7wnQ", "forum": "FfedFHs6Tx", "replyto": "FfedFHs6Tx", "signatures": ["ICLR.cc/2026/Conference/Submission2352/Reviewer_fuiW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2352/Reviewer_fuiW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888267740, "cdate": 1761888267740, "tmdate": 1762916203585, "mdate": 1762916203585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASYNCTOOL, a new benchmark for evaluating LLM agents' capability to handle multiple tasks concurrently through asynchronous tool calls. The authors identify a critical gap in existing benchmarks, which typically assume instantaneous tool responses and focus on single-task scenarios. ASYNCTOOL addresses this by simulating tool response latency, thereby requiring agents to manage idle time by interleaving the execution of different tasks. The paper details the construction of a multi-task dataset via a \"hybrid data-evolution strategy\" and evaluates a wide range of LLMs using a three-level metric system (Step, Sub-Task, Task). The main finding is that even SOTA models struggle significantly with asynchronous workflows, with the paper providing an analysis of common failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Problem Formulation: The paper is the first, to my knowledge, to systematically formalize and create a benchmark for asynchronous multi-task tool use, a critical and under-explored area for LLM agents.\n2. Rigorous Data Curation: The multi-step data construction process, combining LLM-based generation with intensive human verification, is a strong point and likely results in a high-quality, internally consistent dataset.\n3. Comprehensive Analysis: The paper provides a good qualitative analysis of agent failure modes, offering useful insights into why current models struggle with this task."}, "weaknesses": {"value": "1. Unverifiable SOTA Results: The use of unreleased and hypothetical models (GPT-4.1, GPT-5) for key results is a major flaw. It makes the reported SOTA performance impossible to reproduce and sets an unstable target for the research community. This significantly reduces the benchmark's practical utility.\n2. Unrealistic Task Environment: The benchmark's core mechanics—a fixed one-round latency and a single deterministic correct trajectory—do not reflect the variability and flexibility required in real-world scenarios. This limits the generalizability of the findings and risks over-fitting future models to this specific, simplified setup.\n3. Lack of Quantitative Error Breakdown: The paper qualitatively describes failure modes but misses the opportunity to provide a quantitative breakdown in the main text. Quantifying the prevalence of \"tool confusion\" vs. \"task neglect\" across different models would provide much stronger and more actionable evidence."}, "questions": {"value": "1. Justification for Unreleased Models: Could the authors justify the decision to benchmark and prominently feature hypothetical or private preview models? Given the negative impact on reproducibility and the benchmark's utility, would it not be more scientifically sound to re-frame the results around the best-performing publicly accessible models as the primary baseline?\n2. Defense of Deterministic Trajectories: Please provide a stronger defense for enforcing single, deterministic solution paths. Have you analyzed how many tasks in your dataset could have alternative valid paths? How does this design choice not risk penalizing more advanced, flexible reasoning agents in favor of those better at sequence imitation?\n3. Impact of the Latency Model: Can you provide any ablation or theoretical argument on how the results might change with a more realistic, variable latency model? Does the current fixed-delay model truly test \"temporal reasoning,\" or does it primarily test context-switching and short-term memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ok7YQ3MNYG", "forum": "FfedFHs6Tx", "replyto": "FfedFHs6Tx", "signatures": ["ICLR.cc/2026/Conference/Submission2352/Reviewer_9RxN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2352/Reviewer_9RxN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938717954, "cdate": 1761938717954, "tmdate": 1762916203390, "mdate": 1762916203390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}