{"id": "ZMzha5gbnF", "number": 10605, "cdate": 1758177111287, "mdate": 1759897641129, "content": {"title": "Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability", "abstract": "Diffusion language models (DLMs) generate tokens in parallel through iterative denoising, which can reduce latency and enable bidirectional conditioning.\nHowever, the safety risks posed by jailbreak attacks that exploit this inference mechanism are not well understood. In this paper, we reveal that DLMs have a critical vulnerability stemming from their iterative denoising process and propose a countermeasure. Specifically, our investigation identifies that if an affirmative token for a harmful query appears at an intermediate step, subsequent denoising can be steered toward a harmful response even in aligned models.\nFurthermore, we demonstrate that the vulnerability enables existing optimization-based jailbreak attacks to be applied to MDLMs.\nBuilding on this analysis, we propose a novel safety alignment method tailored to DLMs that trains models to generate safe responses from contaminated intermediate denoising steps containing affirmative tokens. Our experiments indicate that the proposed method significantly mitigates the vulnerability with minimal impact on task performance. Furthermore, our method also improves robustness against conventional jailbreak attacks. Our work underscores the need for DLM-specific safety research.", "tldr": "We identify a ciritical vulnerability of diffusion language models, and we also propose a countermeasure for mitigations.", "keywords": ["safety", "jailbreak", "diffusion language models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14dd62dedcc0b3777c6b75b1b39c94fa7d21f4d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigate the  priming vulnerability in diffusion language models , which affirmative tokens could be used during intermediate denoising steps to achieve attacks.. To address this, the authors propose Recovery Alignment which teach DLMs to recover safe responses from contaminated intermediate states. Experiments show the proposed methods can substantially reduces jailbreak  attack success rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides the systematic safety analysis of diffusion-based language models and highlights a vulnerability unique to their parallel denoising mechanism. \n2. The proposed alignment performance is strong which substantially reduces jailbreak  attack success rates.\n3. The evaluation is comprehensive which covers multiple models and datasets."}, "weaknesses": {"value": "1. The LLM usage is not described in the paper.  The paper frequently contains long compound sentences composed of multiple short sentences, which is a typical LLM type writing. \n2. Despite the differences in form, the priming vulnerability is discussed by several prior works regarding ARM[1,2], the author should mentioned these work in the paper and discuss the difference.\n3. The presentation need to be improved. Some sentences are confusing such as\"Moreover, they do not discuss how a more realistic attacker, who cannot intervene in the denoising process, could exploit this vulnerability. In this work, we design an attack that intervenes in the denoising process for comprehensive evaluation. \" Even though with the following sentences i understand what author what to say, but this is confusing.\n4. The ablation is not comprehensive. The author should conduct counterfactual experiments (e.g., replacing non-affirmative tokens or injecting random noise tokens) to verify whether the safety failures specifically depend on token semantics. Meanwhile, the sampling parameters already prove the play a import role in jailbreak success rate[3],  it would be good if related ablation is included.\n5. Theorem 4.1 assume a monotonicity condition which is overly strong, if this condition satisfied that means that as denoising proceeds, the likelihood of generating the target response r should never decrease. In diffusion model, each step involves stochastic re-masking and resampling such the monotonicity condition is impossible.  The authors should thoroughly revise the theoretical part.\n\n\n\n[1]Huang, Yuyi, et al. \"Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models.\" arXiv preprint arXiv:2502.16491 (2025).\n\n[2]Miao, Ziqi, et al. \"Response attack: Exploiting contextual priming to jailbreak large language models.\" arXiv preprint arXiv:2507.05248 (2025).\n\n[3]Huang, Yangsibo, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. \"Catastrophic jailbreak of open-source llms via exploiting generation"}, "questions": {"value": "1. Does recovery alignment need to manually crafted the  contaminated intermediate states? If so, Would this increase the training cost and make it difficult to scale?\n\nOthers see weakness."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KFcHEsHqvI", "forum": "ZMzha5gbnF", "replyto": "ZMzha5gbnF", "signatures": ["ICLR.cc/2026/Conference/Submission10605/Reviewer_GBUt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10605/Reviewer_GBUt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760944128775, "cdate": 1760944128775, "tmdate": 1762921870259, "mdate": 1762921870259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate safety vulnerabilities in Masked Diffusion Language Models (MDLMs), a class of parallel, iterative denoising-based text generators. They identify a new risk priming vulnerability: inserting or generating affirmative tokens during intermediate denoising steps can steer an otherwise aligned MDLM toward producing harmful content. To mitigate this issue, the authors design Recovery Alignment (RA) trains MDLMs to “recover” safe responses via RLHF-style optimization. Experiments on LLaDA, LLaDA 1.5, and MMaDA models show that RA significantly reduces attack success rates while maintaining general performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors study an interesting and important problem given the fast-growing of DLMs.\n2. The identified risk is unique to DLM and is interesting.\n3. The proposed attack has high ASR. The proposed alignment method is empirically useful.\n4. The paper provides a theoretical analysis of the identified risk."}, "weaknesses": {"value": "1. Unclear definition of \"affirmative tokens\" and it is unknown how the authors find all such tokens in the model full vocabulary.\n2. The theory proposed lacks empirical validation.\n3. The experiments are not conducted on larger DLMs, lacking experiments on generalization."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "61jCPHhk7N", "forum": "ZMzha5gbnF", "replyto": "ZMzha5gbnF", "signatures": ["ICLR.cc/2026/Conference/Submission10605/Reviewer_ztRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10605/Reviewer_ztRH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883980877, "cdate": 1761883980877, "tmdate": 1762921869811, "mdate": 1762921869811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the robustness of DLM, discovers and tries to mitigate the priming vulnerability of such models. Priming vulnerability, detailedly discussed in Section 4, refers to a jailbroken behavior of the DLMs that when affirmative tokens appear at an intermediate step of denoising, subsequent generation would have greater probability of producing harmful response. Experiments with two threat models are presented, which demonstrate clear evidence of the priming vulnerability. To mitigate this vulnerability, this paper proposes the recovery alignment. According to Tables 2-4, RA exhibits clear advantage comparing to exsiting alignments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. (Novelty and soundness) This paper focus on the robustness of diffusion models, which is an important topic that is left less studied. This paper not only discover the safety risk of DLM (called the priming vulnerability), but also propose to mitigate this risk by proposing recovery alignment. I believe this paper would help improve the overall robustness of DLMs.\n2. The experimental results are significant."}, "weaknesses": {"value": "1. I cannot find a discussion on the related works on the metrics of jailbreak attacks (i.e., how to judge whether a model's response is \ncontroversial). Since this is directly related to the final ASR and is rapidly developing overtime, I suggest include a brief discussion on the metrics, in addition to the methods."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "59xtH0zP1t", "forum": "ZMzha5gbnF", "replyto": "ZMzha5gbnF", "signatures": ["ICLR.cc/2026/Conference/Submission10605/Reviewer_zsC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10605/Reviewer_zsC5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963304784, "cdate": 1761963304784, "tmdate": 1762921869447, "mdate": 1762921869447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and rigorously analyzes a novel security vulnerability, termed \"priming vulnerability,\" in Diffusion Language Models (DLMs). This vulnerability stems from the iterative, parallel denoising process, where the presence of an affirmative token in an intermediate state can steer the subsequent generation toward a harmful response, bypassing safety guardrails.\nThe authors propose two main contributions:\n\nThey introduce the Anchoring Attack (a hypothetical intervention) to characterize the vulnerability and the First-Step GCG (a non-interventional, realistic attack) to exploit it efficiently.\n\nThey propose Recovery Alignment, a DLM-specific safety alignment method that trains the model to recover from adversarially \"contaminated\" intermediate states back to a safe response, utilizing an RLHF-style objective with a linear curriculum."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Although I am not familiar with the DLMs, I believe there are still many Pros:\n* **Novel and Timely Focus**: The work is highly relevant and timely, focusing on the unique safety challenges of an emerging model class (DLMs) whose non-causal inference fundamentally differs from Autoregressive Models. This addresses a critical gap in current LLM safety research.\n* **Rigorous Vulnerability Analysis**: The paper provides compelling, quantitative evidence for the \"priming vulnerability.\", including:\n    * Anchoring Attack: It demonstrates that even a minimal intervention at t =1 can significantly increase the Attack Success Rate (ASR increases from 2% to 21% on LLaDA Instruct). This clearly demonstrates the sensitivity of the intermediate states.\n    * First-Step GCG: The authors cleverly leverage the vulnerability to derive a tractable lower bound on the intractable GCG objective, resulting in an attack that is ∼20× faster and significantly more effective than Monte Carlo-based GCG. This provides a strong, efficient evaluation benchmark.\n* Well-Motivated Defense: Recovery Alignment directly addresses the root cause of the vulnerability: the failure of traditional alignment to account for contaminated intermediate states. The use of a linear schedule for the intervention step is a sound curriculum learning approach for stabilization.\n* Practicality of Defense: The RA framework is instantiated in an RLHF-style that leverages existing harmful query datasets and reward models, suggesting a practical and scalable solution without requiring bespoke data construction."}, "weaknesses": {"value": "* The theoretical justification in 4.1 for this monotonic increase in the log-likelihood of the target response r over the denoising steps needs to be more robustly discussed, especially for different harmful responses r. Does this assumption hold true when r is a diverse set of harmful responses?\n* Complexity of RA Training: Although RA is practical in terms of data, the training process involves generating intermediate states, running the partial denoising process, and then using a potentially expensive reward model to score the output. \n* Generality of Priming Token: The paper focuses on \"affirmative tokens.\" It would be valuable to discuss if other types of tokens (e.g., specific harmful jargon, boundary tokens) could similarly \"prime\" the model, or if the mechanism is uniquely linked to affirmation."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1GcN246FdZ", "forum": "ZMzha5gbnF", "replyto": "ZMzha5gbnF", "signatures": ["ICLR.cc/2026/Conference/Submission10605/Reviewer_hNjZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10605/Reviewer_hNjZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971657439, "cdate": 1761971657439, "tmdate": 1762921868377, "mdate": 1762921868377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}