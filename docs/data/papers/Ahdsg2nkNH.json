{"id": "Ahdsg2nkNH", "number": 7916, "cdate": 1758043134262, "mdate": 1763648939243, "content": {"title": "Multilevel Control Functional", "abstract": "Control variates are variance reduction techniques for Monte Carlo estimators. They play a critical role in improving Monte Carlo estimators in scientific and machine learning applications that involve computationally expensive integrals. We introduce \\emph{multilevel control functionals} (MLCFs), a novel and widely applicable extension of control variates that combines non-parametric Stein-based control variates with multi-fidelity methods. We show that when the integrand and the density are smooth, and when the dimensionality is not very high, MLCFs enjoy a faster convergence rate. We provide both theoretical analysis and empirical assessments on differential equation examples, including Bayesian inference for ecological models, to demonstrate the effectiveness of our proposed approach. Furthermore, we extend MLCFs for variational inference, and demonstrate improved performance empirically through Bayesian neural network examples.", "tldr": "", "keywords": ["Variance Reduction", "Monte Carlo"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/554cb4a4f912d2e3b69ed236a778594e2cc4036a.pdf", "supplementary_material": "/attachment/cb0a98d8c62b127b6d6f9ea71fcc01210e4b0522.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel variance-reduction method for computing expectations using Markov chains and variational inference. The proposed approach, Multi-Level Control Functionals (MLCF), integrates multi-fidelity modeling with non-parametric control variates based on the kernelized Stein discrepancy. The authors prove that the estimator is unbiased, establish theoretical bounds on its variance, and derive the level-wise optimal sample allocation. The method is empirically validated on inference tasks involving dynamical systems and Bayesian neural networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel and well-motivated approach to variance reduction, a key challenge in Bayesian inference.  \n- The method is theoretically rigorous, providing explicit variance bounds and optimal sample allocation formulas.  \n- The empirical results are convincing, demonstrating that MLCF consistently yields lower-variance estimators across the tested scenarios."}, "weaknesses": {"value": "- The impact of varying the fidelity level $L$ is not explored, making it difficult to assess its practical importance.  \n  *(See related questions below.)*\n\n### Minor\n- In the BNN example, it is not immediately clear from the main text that variance reduction is applied to  \n  the gradient estimator of the ELBO. This is mentioned in the appendix; consider moving or referencing it earlier for ease of reading.  \n  (A single clarifying sentence would suffice.)"}, "questions": {"value": "- Please provide the converged training ELBO and test log-likelihood values corresponding to Figures 5 and 6.  \n- At what dimensionality does the integrand cease to be considered “moderate”?  \n- What can be said about the relationship between the magnitude of $L$ and the extent of variance reduction?  \n- How does $L$ affect computational cost?  \n- How should practitioners choose or tune $L$ in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qOZi0BBv9g", "forum": "Ahdsg2nkNH", "replyto": "Ahdsg2nkNH", "signatures": ["ICLR.cc/2026/Conference/Submission7916/Reviewer_NiZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7916/Reviewer_NiZU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761305601845, "cdate": 1761305601845, "tmdate": 1762919940183, "mdate": 1762919940183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all reviewers"}, "comment": {"value": "We would like to thank all reviewers for their time and careful consideration of our paper. We are delighted with the strong support for our paper, with all three reviewers recommending ``accept'' (8/8/8) with high confidence(4/4/3). The feedback is overwhelmingly positive with reviewers emphasizing the following strengths:\n\n+ **Novelty/Significance:** Reviewer 8axS: \"... novel and highly intuitive\". Reviewer u3Vy: \"... is innovative\". Reviewer NiZU: \"... a novel and well-motivated approach to variance reduction\".\n+ **Theoretical Analysis:** Reviewer 8axS: \"The authors provide a solid theoretical analysis ... is supported by a strong theoretical foundation ... a clear\nvariance bound ...derivation of the optimal sample allocation ...\". Reviewer u3Vy: \"convergence rate analysis is a key strength\".  Reviewer NiZU: \"... is theoretically rigorous, providing explicit variance bounds and optimal sample allocation formulas\".\n+ **Quality of Experiments:** Reviewer 8axS: \"The method's effectiveness is demonstrated empirically on ...In all cases, the proposed MLCF and MLCFRG methods outperform their respective baselines ... \". Reviewer u3Vy: \"... empirical evaluations are well-chosen and cover diverse use cases\". Reviewer NiZU: \"The empirical results are convincing...\".\n\nWe are also thankful to reviewers for their additional suggestions which will further strengthen the paper. These are addressed in detail in our individual responses for each reviewer."}}, "id": "odEt3ExBuQ", "forum": "Ahdsg2nkNH", "replyto": "Ahdsg2nkNH", "signatures": ["ICLR.cc/2026/Conference/Submission7916/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7916/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7916/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763645569782, "cdate": 1763645569782, "tmdate": 1763645569782, "mdate": 1763645569782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Multilevel Control Functionals (MLCFs), a novel variance reduction technique for Monte Carlo estimators. MLCFs extend traditional control variates by combining two powerful ideas: non-parametric Stein-based control variates and multi-fidelity methods . The authors provide theoretical analysis showing that MLCFs achieve faster convergence rates when integrands and densities are smooth and dimensionality is moderate. They validate MLCFs empirically on differential equation (DE) tasks (e.g., Bayesian inference for ecological models) and extend the framework to variational inference (VI), demonstrating improved performance on Bayesian neural network (BNN) benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The combination of Stein-based control variates with multi-level methods is innovative.\n2. The convergence rate analysis is a key strength, even though I did not validate the details of mathematical theory.\n3. The empirical evaluations are well-chosen and cover diverse use cases:"}, "weaknesses": {"value": "1. The abstract and theory section reference MLCFs working when “dimensionality is not very high,” but this is underspecified. What is the upper bound of dimensionality for MLCFs to remain competitive? \n2. Maybe more baseline methods (mentioned in related works) could be included in the experiments."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6gRYxWOMcQ", "forum": "Ahdsg2nkNH", "replyto": "Ahdsg2nkNH", "signatures": ["ICLR.cc/2026/Conference/Submission7916/Reviewer_u3Vy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7916/Reviewer_u3Vy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838461337, "cdate": 1761838461337, "tmdate": 1762919939579, "mdate": 1762919939579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper proposes Multilevel Control Functionals (MLCFs), a novel variance reduction technique for efficiently estimating computationally expensive integrals. The core idea is an elegant combination of two powerful variance reduction strategies: Multilevel Monte Carlo (MLMC) and non-parametric, Stein-based Control Functionals (CFs).\n\nMLMC methods accelerate estimation by using a telescoping sum of a hierarchy of low-fidelity, cheap approximations to the expensive, high-fidelity integrand. The variance is reduced by estimating the small differences between levels.\n\nThis paper's key insight is to treat the MLMC difference terms themselves as integrands that can be further variance-reduced. The authors propose applying a non-parametric Stein-based Control Functional to each level of the MLMC estimator. This results in an estimator for the sum of these variance-reduced differences, which has significantly lower variance.\n\nThe authors provide a solid theoretical analysis, including a variance bound that demonstrates a faster convergence rate than standard MLMC, particularly for smooth integrands and densities in low-to-moderate dimensions. They also derive the optimal sample allocation across levels to minimize this variance bound under a fixed computational budget.\n\nFurthermore, the paper extends this framework to variational inference (VI) by proposing the Multilevel Control Functional Re-parameterized Gradient (MLCFRG) estimator. This estimator applies the MLCF idea to the multilevel gradient estimator for the ELBO (MLRG), and a practical, efficient recursive update is provided.\n\nThe method's effectiveness is demonstrated empirically on a synthetic example, a boundary-value ODE, a Bayesian inference problem (Lotka-Volterra system) using MCMC samples, and a variational inference problem (Bayesian Neural Network). In all cases, the proposed MLCF and MLCFRG methods outperform their respective baselines (MLMC, CF, MLMCRG)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novelty and Significance: The paper's main contribution—hybridizing MLMC with Stein-based Control Functionals—is both novel and highly intuitive. It directly addresses a practical and important problem: the high computational cost of integration in many scientific and machine learning applications. This combination is powerful, as it leverages the \"divide-and-conquer\" strength of MLMC and the non-parametric variance-reducing power of CFs.\n\nTheoretical Soundness: The method is supported by a strong theoretical foundation. The paper provides a clear variance bound that explains why and when MLCF should be effective (dependence on smoothness and dimensionality). The derivation of the optimal sample allocation adds to the method's practical utility."}, "weaknesses": {"value": "Computational Cost of Control Functionals: The primary weakness, acknowledged by the authors, is the cubic computational cost of inverting the kernel Gram matrix to construct the control functional, where the cost scales with the number of design points. This cost is incurred per level for the standard MLCF estimator and per iteration for the MLCFRG estimator. While the authors argue this is negligible if the integrand is sufficiently expensive, this scaling severely limits the number of points that can be used to build the CF, which in turn limits the achievable variance reduction.\n\nPractical Complexity: The method adds a new layer of complexity compared to the relatively simple MLMC. A user must now select an appropriate kernel (e.g., Mateŕn, SE) and its hyperparameters (e.g., length-scale) for each level. The paper suggests maximizing the marginal likelihood (in the Appendix), but this is a non-trivial, costly optimization problem in itself, adding to the overall computational burden."}, "questions": {"value": "Hyperparameter Sensitivity: How sensitive is MLCF's performance to the choice of kernel and its hyperparameters? The paper uses several different kernels. How much of the performance gain is attributable to careful, and potentially expensive, hyperparameter tuning via marginal likelihood maximization at each level?\n\nRelation to Multi-Fidelity BQ: How does MLCF compare to other methods that combine multilevel/multi-fidelity approaches with kernel-based methods, such as the multilevel Bayesian Quadrature (Li et al., 2023) you cite? A direct empirical comparison, even on the ODE problem, would be very insightful to position MLCF in the literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FCHsqH0vCc", "forum": "Ahdsg2nkNH", "replyto": "Ahdsg2nkNH", "signatures": ["ICLR.cc/2026/Conference/Submission7916/Reviewer_8axS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7916/Reviewer_8axS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983809704, "cdate": 1761983809704, "tmdate": 1762919939225, "mdate": 1762919939225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}