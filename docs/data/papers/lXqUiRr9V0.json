{"id": "lXqUiRr9V0", "number": 24428, "cdate": 1758356842461, "mdate": 1763731267585, "content": {"title": "Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations", "abstract": "Despite their impressive capabilities, large language models (LLMs) frequently generate hallucinations. Previous work shows that their internal states encode rich signals of truthfulness, yet the origins and mechanisms of these signals remain unclear.\nIn this paper, we demonstrate that truthfulness cues arise from two distinct information pathways: (1) a Question-Anchored pathway that depends on question–answer information flow, and (2) an Answer-Anchored pathway that derives self-contained evidence from the generated answer itself.\nFirst, we validate and disentangle these pathways through attention knockout and token patching.\nAfterwards, we uncover notable and intriguing properties of these two mechanisms and investigate the factors underlying their distinct behaviors.\nFurther experiments reveal that (1) the two mechanisms are closely associated with LLM knowledge boundaries; (2) internal representations are aware of their distinctions; and (3) there is a clear misalignment between truthfulness encoding and language modeling.\nFinally, building on these insightful findings, two applications are proposed to enhance hallucination detection performance.\nOverall, our work provides new insight into how LLMs internally encode truthfulness, offering directions for more reliable and self-aware generative systems.", "tldr": "The paper explores how large language models intrinsically encode truthfulness signals through two distinct pathways—Question-Anchored and Answer-Anchored—to detect hallucinations, revealing their properties and proposing enhanced detection methods.", "keywords": ["Interpretability", "Hallucination Detection", "Hallucinations", "Truthfulness", "Large Language Models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65d0a96d7215581ed48c2a08cb53a2043b2432ff.pdf", "supplementary_material": "/attachment/7c13d94bf5c150ddfee9a3e93e9018d20e97f42a.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates how large language models internally encode truthfulness signals to detect their own hallucinations in question-answering tasks. Through saliency analysis on attention flows, the authors identify a bimodal distribution suggesting two separate pathways: a Question-Anchored pathway that relies on information flowing from exact question tokens to the answer, and an Answer-Anchored pathway that uses self-contained cues within the generated answer. They validate this distinction using attention knockout to block question-to-answer flows and token patching to inject hallucinatory question cues, showing sharp behavioral splits across Llama-3 and Mistral models on PopQA, TriviaQA, HotpotQA, and Natural Questions datasets. Further analyses link Question-Anchored encoding to high-confidence known facts and Answer-Anchored to uncertain or extrapolated responses, demonstrate that models can predict which pathway is active from internal states, and reveal that pathway selection misaligns with uniform question-answer attention during standard generation. The insights enable two new detection methods—Mixture-of-Probes that routes to pathway-specific classifiers and Pathway Reweighting that amplifies salient signals—yielding up to 10% AUC gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core contribution of disentangling two truthfulness pathways is novel and rigorously substantiated; attention knockout produces clean, statistically robust bifurcations in prediction shifts, while token patching elegantly confirms differential sensitivity to question-derived hallucinations. The large-scale scope across six model sizes from two families and four diverse datasets strengthens generalizations beyond prior probing studies that treat truthfulness signals as monolithic. \n\nLinking pathways to knowledge boundaries via answer accuracy and generation entropy provides interpretable ties to model capabilities, and the self-awareness probe achieving high accuracy in predicting pathway usage reveals sophisticated internal monitoring. The proposed Mixture-of-Probes and Pathway Reweighting directly exploit these mechanisms for tangible gains in hallucination detection, outperforming layer-wise probing baselines consistently. Focus on exact question and answer tokens grounded in semantic frame theory sharpens analysis compared to whole-sequence averaging."}, "weaknesses": {"value": "Attention knockout is applied cumulatively up to layer k without isolating per-layer contributions or controlling for indirect flows through intermediate tokens, potentially conflating direct and propagated effects. Token patching experiments restrict to correct-answer contexts with non-hallucination predictions, limiting insight into how pathways behave under native hallucinations rather than injected ones. Knowledge boundary associations rely on answer accuracy and entropy but ignore finer-grained metrics like memorization versus reasoning or entity frequency in pretraining, leaving the \"boundary\" definition coarse. \n\nSelf-awareness results train the pathway predictor on the same probing layers used for hallucination detection, risking circularity since the predictor may simply rediscover the knockout intervention signal. \n\nThe applications evaluate only on the same four datasets used for discovery, with no held-out tasks like long-form generation or multi-hop reasoning where pathway dynamics might differ. Ablations for Mixture-of-Probes do not compare against simpler ensembles, and Pathway Reweighting lacks clarity on how reweighting coefficients are derived beyond \"information intensity.\""}, "questions": {"value": "How sensitive are the bimodal saliency distributions to the choice of exact token identification—does switching to dependency-parse heads or named entities alter the peaks? \n\nIn attention knockout, why does prediction stability for Answer-Anchored instances hold even in early layers where question context is still accessible, and what fraction of information reroutes through non-exact question tokens? \n\nFor token patching, what happens when patching hallucinatory cues into Answer-Anchored instances that are already hallucinations—does the probe flip less or more than in correct cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9Q3R9TjoSW", "forum": "lXqUiRr9V0", "replyto": "lXqUiRr9V0", "signatures": ["ICLR.cc/2026/Conference/Submission24428/Reviewer_YZZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24428/Reviewer_YZZQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750899896, "cdate": 1761750899896, "tmdate": 1762943079889, "mdate": 1762943079889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the intrinsic truthfulness encoding mechanisms of large language models (LLMs), uncovering two distinct information pathways: Question-Anchored (relying on question-answer information flow) and Answer-Anchored (deriving evidence from generated answers themselves). Through attention knockout, token patching, and large-scale experiments across four datasets and multiple model architectures (Llama-3, Mistral-7B), the paper identifies three key properties of these pathways—association with knowledge boundaries, intrinsic self-awareness, and misalignment with language modeling objectives. Building on these findings, two pathway-aware hallucination detection methods (Mixture-of-Probes and Pathway Reweighting) are proposed, achieving up to 10% AUC gain compared to baselines. The work deepens the understanding of LLM hallucinations and provides practical directions for building more reliable generative systems."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel Mechanistic Insight**: The idea of decomposing truthfulness encoding into two interpretable pathways is original and provides a new conceptual framework for understanding hallucinations.\n* **Methodological Rigor**: The use of attention knockout, token patching, and multi-model, multi-dataset validation is impressive and methodologically sound.\n* **Empirical Significance**: The consistent performance gains of MoP and PR demonstrate strong practical value beyond theoretical contributions.\n* **Clarity and Presentation**: The paper is clearly written, with well-structured figures and a logical flow from hypothesis to validation to application."}, "weaknesses": {"value": "* The paper could further **clarify the causal direction** between pathway activation and hallucination occurrence. While associations are well-demonstrated, causal inference remains somewhat implicit.\n* The **interpretability of “self-awareness”** could be discussed with more nuance — e.g., whether this phenomenon indicates metacognitive capability or simply separable representational clusters.\n* **Comparative analysis** with existing mechanistic interpretability frameworks (e.g., Transformer Circuits, causal tracing) is limited; positioning the work within that literature would strengthen its conceptual grounding.\n* The **applications section (Section 5)**, though interesting, feels condensed compared to the theoretical parts; additional ablations on gating accuracy or scalability would improve completeness."}, "questions": {"value": "1. How sensitive are the identified pathways to model size or instruction tuning—would finetuning for factual QA alter the pathway balance?\n2. Could the authors elaborate on whether “A-Anchored” signals are related to self-consistency or internal entailment mechanisms seen in reasoning tasks?\n3. How does the MoP framework generalize to non-QA settings (e.g., open-ended generation or summarization)?\n4. Would integrating external knowledge retrieval alter or reinforce the two-pathway distinction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xYrpyCZrid", "forum": "lXqUiRr9V0", "replyto": "lXqUiRr9V0", "signatures": ["ICLR.cc/2026/Conference/Submission24428/Reviewer_vwQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24428/Reviewer_vwQd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893516264, "cdate": 1761893516264, "tmdate": 1762943079552, "mdate": 1762943079552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their careful reading of our manuscript and for the constructive feedback. We have provided detailed, point-by-point responses to every question and suggestion raised. **Because many of the newly added analyses and experiments are best communicated visually, the corresponding detailed results are presented in figures in the revised appendix. All modifications are clearly marked in red for ease of reference.**"}}, "id": "3vqQezTBIH", "forum": "lXqUiRr9V0", "replyto": "lXqUiRr9V0", "signatures": ["ICLR.cc/2026/Conference/Submission24428/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24428/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24428/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763730654321, "cdate": 1763730654321, "tmdate": 1763730654321, "mdate": 1763730654321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines how LLMs internally encodes truthfulness, aiming to uncover the mechanisms underlying intrinsic hallucination-detection signals in their representations. The authors propose that two distinct internal “truthfulness pathways” underlie these signals: the Question-Anchored Pathway and the Answer-Anchored Pathway. To identify and disentangle two pathways, the paper blocks question–answer attention and token patching. The experiments, conducted on multiple datasets (PopQA, TriviaQA, HotpotQA, NQ) and models (Llama-3 1B, 3B, 8B, 70B, Mistral-7B), reveal a bimodal distribution of dependency on question–answer interactions—supporting the existence of two separate truthfulness mechanisms.\n\nThe author further applied their experiment findings to hallucination detection in two ways:  1) Mixture-of-Probes (MoP): Specialized classifiers for each pathway combined via a gating network predicting pathway usage. 2) Pathway Reweighting (PR): Adjusts attention flows to amplify pathway-relevant signals for hallucination detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation and findings are interesting. First, the author found that Q-Anchored signals align with knowledge boundaries and they dominate when the model is confident and knowledgeable. Second, LLMs exhibit intrinsic self-awareness that internal representations can predict which pathway is active.\n\n2. The author uses multiple interpretability methods (attention knockout, token patching, answer-only experiments) to triangulate evidence.  The author also tested in various model families and parameter size (Llama-3 and Mistral, from 1B to 70B), four QA datasets, and multiple scales, demonstrating the generalizability of the findings.\n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The entire experiment analysis relies on identifying \"exact question\" and \"exact answer\" tokens using GPT-4o. This step relies on a proprietary model and lacks a detailed explanation of the experiment setup here. That may change over time. Also, no further ablation study on sensitivity to token selection choices.\n\n2. Insufficient explanation of the pathway selection mechanism. Although the paper shows the model can predict which pathway is used, I still have several questions about this step. Is selection deterministic or probabilistic? And how early in the forward pass is pathway selection determined?\n\n3. For the patching, how does the author ensure the patch sample is comparable in difficulty? Random selection could introduce noise if patch questions are easier or harder."}, "questions": {"value": "See in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fBM8NwG1r1", "forum": "lXqUiRr9V0", "replyto": "lXqUiRr9V0", "signatures": ["ICLR.cc/2026/Conference/Submission24428/Reviewer_5BX9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24428/Reviewer_5BX9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972350226, "cdate": 1761972350226, "tmdate": 1762943079258, "mdate": 1762943079258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanisms underlying truthfulness encoding in large language models (LLMs) during hallucination detection. The authors discover two distinct information pathways: (1) a Question-Anchored (Q-Anchored) pathway that relies on question-answer information flow, and (2) an Answer-Anchored (A-Anchored) pathway that derives evidence from the generated answer itself. Through attention knockout and token patching experiments across multiple models, the authors present evidence for these pathways. Building on these findings, the authors further propose two hallucination detection methods that leverage the pathway information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The presentation of the paper is good. The authors present their findings in a clear manner and include necessary details and supporting evidence.\n\n- The paper illustrates the practical value of the findings through two hallucination detection algorithms guided by the pathway insights.\n\n- The structure of the paper is good. The authors present a complete research process: initial observation, hypothesis formulation, experimental investigation, and derivation of practical applications."}, "weaknesses": {"value": "- Suggestion for additional analysis: In Section 3.1, the authors present saliency score distributions aggregated across all samples, revealing a bimodal pattern. As a manipulation check, it would be valuable to stratify this analysis by the actual presence of hallucination (z=1 vs. z=0). Specifically, showing saliency patterns separately for hallucinated versus truthful responses could reveal whether different pathways are preferentially engaged depending on whether hallucination is actually occurring. This could provide deeper insights into the relationship between the detection mechanisms and the phenomenon being detected, and potentially uncover additional patterns about when and why each pathway is utilized.\n\n- The token patching experiment (Section 3.2.3) would benefit from a control condition where random (non-exact) tokens are replaced instead of exact question tokens. This would rule out the alternative explanation that prediction changes result from general disruption due to token replacement rather than specifically from the semantic content of exact question tokens. Such a control would strengthen the causal claim that exact question tokens drive the Q-Anchored pathway.\n\n- The authors claim \"misalignment between truthfulness encoding and language modeling\" based on homogeneous aggregated attention patterns across Q-Anchored and A-Anchored pathways (Figure 6). This conclusion is premature for several reasons:\n  - Aggregation masks important differences: Averaging attention across all heads may obscure head-specific patterns. Prior work demonstrates that different attention heads serve distinct functions.  Head-level analysis is needed to determine whether specific heads show differential patterns between pathways.\n  - Attention may not be the relevant mechanism: The pathway distinction might manifest in other components rather than in attention weights. The authors should analyze whether these other mechanisms show pathway-dependent differences.\n  - Mechanism vs. artifact: Fundamentally, it's unclear whether these pathways represent actual computational mechanisms used during generation, or merely patterns visible through the probe's lens. That attention patterns (which drive generation) don't distinguish pathways, while probe predictions (which don't affect generation) do raise the question: are these pathways genuinely mechanistic, or post-hoc observational artifacts?"}, "questions": {"value": "- In Section 3.2.4, please clarify whether the hidden states analyzed are re-obtained by running a new forward pass with only the answer as input, or whether the original hidden states (from processing the full question+answer) are used. This distinction is crucial for interpreting the 'answer-only' condition, as original hidden states would contain implicit question information through the attention mechanism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YA4Gtad3px", "forum": "lXqUiRr9V0", "replyto": "lXqUiRr9V0", "signatures": ["ICLR.cc/2026/Conference/Submission24428/Reviewer_4YZJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24428/Reviewer_4YZJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996910529, "cdate": 1761996910529, "tmdate": 1762943078917, "mdate": 1762943078917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}