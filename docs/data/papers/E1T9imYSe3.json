{"id": "E1T9imYSe3", "number": 3476, "cdate": 1757440999033, "mdate": 1759898087836, "content": {"title": "QuBS: Combinatorial Quantum Brain Surgeon on Quantum Annealers", "abstract": "Quantum Neural Networks (QNNs) are parameterized quantum circuits trained for machine learning tasks. Sparse QNNs are desired since: 1) their limited entanglement topologies are less likely to approximate 2-design unitaries and affected by training issues such as barren plateaus; and 2) they reduce susceptibility to decoherence and cumulative operational errors---a critical requirement for high-fidelity execution on near-term quantum hardware. This necessitates a method to prune QNNs with minimal fidelity loss. Existing techniques predominantly rely on Euclidean-space heuristics, such as Pauli rotation strengths or their gradients, to serve as proxies for QNN operation importance. However, these approaches are misaligned with the Hilbert-space dynamics and fundamentally ignore the non-commutativity of quantum operations (i.e. inter-parameter correlations). In response to existing limitations, we propose *QuBS*, a novel framework that explicitly accounts for the QNN's Hilbert-space evolution. *QuBS* approximates the non-commutativity effects of QNN pruning up to 2nd order. The core of *QuBS* involves searching for an optimal sparse subnetwork by solving a Quadratic Unconstrained Binary Optimization (QUBO) problem. Given the NP-hard nature of QUBOs (unsolvable in polynomial time classically), which become intractable for classical solvers as problem size grows, we leverage quantum annealers---an emerging machine leveraging adiabatic Hamiltonian evolution to seek high-quality solutions for QUBOs; this evolution is performed on the order of microseconds per anneal, enabling rapid sampling of the solution space. We further propose an iterative variant of *QuBS* to enable pruning large-scale QNNs with constrained computational resources provided by the annealer. We evaluate *QuBS* on pre-trained QNNs designed for image classification and benchmark it against established protocols. We show that *QuBS* consistently outperforms existing baselines with up to 30 percent improvement in retained classification accuracy at high sparsity levels.", "tldr": "", "keywords": ["QUBO form", "quantum machine learning", "Hessian", "quantum optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b72e9762a4f337332ba25aef9f4c75ef4c5b85a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel framework, called Combinatorial Quantum Brain Surgeon, to prune QNNs. They benchmark their approach against state-of-the-art on small, feasible instances, showcasing improvement over them. Further, they utilize QUBO problems that can be solved on quantum annealers.\n\nWhile the approach is interesting, I do think that there is an inconsistency in the problem setting, in particular, one of their primary motivations of pruning QNNs is to avoid trainability issues due to excessive randomness in the models (2-designs), however, pruning takes place after training. The method aims to minimize the increased loss resulting from removal of gates, thus, it is a precondition to work with an already trained model. This would leave the only reason to do QNN pruning to reducing decoherence and operational errors, which already occur during training anyway (i.e., I train my model on a noisy loss, so ideally the model is already trained to be robust against that). That leaves it an open question to me about what the whole motivation of doing QNN pruning is. In classical ML this is usually done afterward to minimize HW requirements, energy, etc., but this seems to not translate into (today's) QML.\n\nIt is not clear what the difference between Figure 3 and Appendix P (which should be referenced in the main text where appropriate) is, however, Appendix P shows that the proposed architecture (classical feature extraction + QNN) works better (or for later epochs equally well) when removing the quantum components, which raises the question about the appropriateness of using said architecture. If adding a QNN to the problem setup does not lead to an increase in performance, it seems pointless to even add it.\n\nFurther, I think the paper has structural problems. A lot of text has been moved to the Appendix, sometimes, even without reference where necessary (i.e., introduction to adiabatic quantum computing, QUBO) and there are missing links to literature. I want to highlight that the Limitation section should definitely not be hidden (for sure not without even a reference) in the Appendix, as it is a fundamental part of the approach. I think the main paper could at least outline proof sketches, not outsource everything to the Appendix (it should be possible to follow the propositions etc. only from reading the paper) and the proofs in the Appendix could benefit from explanations and elaborations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors have equipped their approach with a sound theoretical framework\n- Comparison to state-of-the-art shows an improvement"}, "weaknesses": {"value": "- Their motivation for QNN pruning is inconsistent with the approach (outlined above)\n- The experimental setup is not clear to me (see comment above)\n- Missing comments and explanations for the proofs\n- Claims that are not justified: i.e., \"Euclidean-space heuristics are misaligned with the dynamics of quantum circuit\". This is highlighted many times, however, a justification, in particular, for the relevance to QNN pruning is never given.\n- Notation inconsistency. N is sometimes used for qubits, other times for the number of gates. The interaction graph and chromatic number are hardly explained. Eq.13 is not clear (what is $N_p$ and $b$?). App.C. why is $\\delta L(w)$ a real number? $\\delta$ is a vector of perturbations for every parameter, no? Shouldn't it be $L(\\delta w)$? Please restate the propositions in the appendix before proving them and comment/explain on the transformations being done, this would greatly improve readability (also the proofs in matrix form are hardly possible to follow).  Adjust indices in Eq.56 - mathematical notation usually includes lower and upper bound.\n- Please add the performance of the full model to the plots comparing the pruning approaches\n- The paper does not add evidence to the existence of lottery tickets in QNNs. Either discuss and provide evidence in the main part of the paper, but this is a strong claim to add to the conclusion.\n- Add references to the AQC and QUBO introductions\n- It is a bit difficult for me to understand why the authors made the decision to only give an introduction to QNNs (from a very basic perspective) in the main part of the paper, and not do the same for QUBO and AQC. I'd suggest giving a very brief introduction there as well, or at least, link to the Appendix. Further, I think the introduction to QNNs does not clearly draw a distinction between their setup and the general definition, i.e., QNNs (at least hardware-efficient) often do not use CR operations (your setup), but work with CNOT, CZ. Further, other types of feature encoding (e.g., re-uploading) can be used too. Lastly, the last sentence in Section 3 needs elaboration, it is not clear to me what this should mean.\n- (minor) The cost function and expectation value are used interchangeably, it should be made clear that this is not the case (in particular, if you're training against ground truth labels)."}, "questions": {"value": "- Why is the removal and parameter nullification protocol a result of the Lie group framework? This just follows from not rotating being equal to identity. \n- What does remark 4 mean?\n- Where does the 1/2 factor in Eq.46 go?\n- What is the difference between Appendix P and Figure 3?\n- Figure 8: What does this figure depict? What is the x-axis, and why is the discrepancy between original and pruned networks so big, even for hardly any components removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M0srvlf73U", "forum": "E1T9imYSe3", "replyto": "E1T9imYSe3", "signatures": ["ICLR.cc/2026/Conference/Submission3476/Reviewer_iudq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3476/Reviewer_iudq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761145941919, "cdate": 1761145941919, "tmdate": 1762916744101, "mdate": 1762916744101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a pruning algorithm for quantum neural networks (QNNs) that leverages the Hessian matrix of the QNN. They formulate the problem of selecting a pruning mask as a QUBO problem, which is then solved using quantum annealing. The authors aim to achieve a computational advantage in solving the QUBO problem."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The figure and the presentation look good."}, "weaknesses": {"value": "Weakness:\n- There is a fundamental issue with the results of Proposition 1 (Eqn. (7)), which serves as the theoretical foundation of this work.  Specifically, in Eqn. (40), which is a crucial step in deriving Eqn. (7), the third equality only holds if all the generators $G_i$ and $G_j$ commute. However, the authors consider a general quantum circuit without making this strong assumption. As a result, the conclusions in Eqn. (7) are incorrect.\n- Some notations in Equation (9) are not well defined. For example, the authors should provide explanations for the terms $\\delta w_{pr}$\n and $w_0$.\n- There is an inconsistency in the notation used throughout the manuscript. For instance, the parameter is denoted as $\\theta$ in Section 4, but as $w$ in Section 5. This lack of consistency should be addressed for clarity."}, "questions": {"value": "The questions are included in the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FX67lG36iu", "forum": "E1T9imYSe3", "replyto": "E1T9imYSe3", "signatures": ["ICLR.cc/2026/Conference/Submission3476/Reviewer_ZbsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3476/Reviewer_ZbsX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651442845, "cdate": 1761651442845, "tmdate": 1762916743843, "mdate": 1762916743843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript presents a quantum circuit pruning method named QuBS. It formulates the search for an optimal sub-circuit that maintains high fidelity as a Quadratic Unconstrained Binary Optimization (QUBO) problem, solvable via quantum annealers. Experimental results demonstrate the method’s performance under different sparsity ratios and its advantages over two existing algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The manuscript clearly describes the proposed method, providing both theoretical derivations and algorithmic details.\n\n- The method includes two versions—gate-level pruning and depth-level pruning—as well as an iterative variant for large-scale circuits, making the work more comprehensive."}, "weaknesses": {"value": "- The overall organization of the manuscript is weak; too many sections are created, and the logical connections between them are unclear.\n\n- The writing quality needs improvement. For example, the exact definition of the objective function $\\mathcal{L}$ is missing.\n\n- The motivation is questionable. The paper claims to address the misalignment between Euclidean-space heuristics and Hilbert-space dynamics in quantum circuits, but this statement is confusing. A more detailed discussion of these differences and the limitations of existing methods is needed.\n\n- The experimental comparison is insufficient. Only two existing methods are considered, while other pruning approaches, such as symmetric pruning, should also be included. In addition, the circuit scales used in the experiments are too small to convincingly demonstrate scalability.\n\n- Experimental details are missing and should be clearly reported."}, "questions": {"value": "What is the computational complexity of estimating the Hessian matrix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZBbPF6qp8w", "forum": "E1T9imYSe3", "replyto": "E1T9imYSe3", "signatures": ["ICLR.cc/2026/Conference/Submission3476/Reviewer_y9p5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3476/Reviewer_y9p5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897324158, "cdate": 1761897324158, "tmdate": 1762916743635, "mdate": 1762916743635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript provides a pruning framework for variational quantum circuits that models  non-commutativity among gates. It uses second order Taylor expansion which gives a QUBO with gradients and Hessian. The authors solve the QUBO on a D-Wave annealer. The theory is complete, and the experiments on a small hybrid model for CIFAR-10, QuBS give more accurate results then other approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript provide a good theory framework\n2. Hessian has a clean commutator–anticommutator form. \n3. The experiments shows consistent gains at high sparsity."}, "weaknesses": {"value": "1. Comparisons are limited to NR-QNN and QAdaPrune. Maybe it should include all methods receive equivalent post-pruning fine-tuning or hyperparameter sweeps.\n2. The paper does not report wall-clock costs or budget vs. accuracy trade-offs for this step on larger $d$; exact forward-only Hessians require $O(d^2)$ circuit evaluations.\n3. The paper does not compare against strong classical QUBO solvers beyond simulated annealing.\n4. The paper works with a small 5 qubit anstz. It is not clear how it scales to larger circuits."}, "questions": {"value": "1. The Lagrangian penalty enforces sparsity but also adds a rank-1 perturbation to the coupling matrix. would there be a strong penalty?\n2. Can authors make comments more on \"lottery tickets in QNN settings\" in the summary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5zAijoL2Np", "forum": "E1T9imYSe3", "replyto": "E1T9imYSe3", "signatures": ["ICLR.cc/2026/Conference/Submission3476/Reviewer_KSaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3476/Reviewer_KSaG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971077656, "cdate": 1761971077656, "tmdate": 1762916743439, "mdate": 1762916743439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}