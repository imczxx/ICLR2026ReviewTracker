{"id": "67o9fTKYZJ", "number": 8707, "cdate": 1758095570057, "mdate": 1763555065052, "content": {"title": "On the Evolution of Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation", "abstract": "Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. \nExisting self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity.\nTo address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation–selection principle: selection prevents drift, while novelty prevents collapse.\nEvaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH).", "tldr": "EVOL_RL prevents self-learning AIs from entropy collapse by rewarding both stability and novelty, boosting their reasoning skills.", "keywords": ["LLM", "Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a99ea2602d96da07df824953e2c79c1387f68b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EVOL-RL, a label-free reinforcement learning framework that helps language models improve themselves without external rewards or ground-truth labels. The authors show that existing self-training methods relying on confidence or majority voting often cause entropy collapse—models become over-confident, lose diversity, and generalize poorly. EVOL-RL addresses this by combining a majority-based stability signal with a novelty-aware reward that values reasoning diversity, mirroring the evolutionary balance between selection and variation. On GRPO, the method consistently boosts pass@1 and pass@16 accuracy across math and reasoning benchmarks and improves out-of-domain generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has several clear strengths. It’s very well-motivated — the authors clearly identify why current label-free learning approaches tend to collapse in diversity, and the diagnosis of “entropy collapse” feels both intuitive and empirically grounded. The proposed solution, combining majority-based stability with a novelty-driven exploration term, is conceptually straightfoward and surprisingly simple to implement. The experiments are strong and consistent across datasets and model sizes, showing meaningful gains in both pass@1 and pass@16 accuracy, which indicates that the method improves reasoning reliability without sacrificing diversity. I also appreciated the inclusion of detailed training-dynamics plots and ablations, which make it much easier to understand why the approach works rather than just showing numerical improvements."}, "weaknesses": {"value": "Although the paper is strong overall, several aspects could be clearer or more complete.  The evaluation is solid but somewhat narrow — almost all experiments are done with the Qwen3 family, so it’s not clear whether the proposed reward structure would transfer cleanly to other models of different base model reasoning capabilities like Llama. \n\nIt would also help to include additional baselines beyond TTRL, such as entropy-regularized self-training, to better situate the improvement. \n\nFinally, while the results and visualizations are convincing, the paper does not quantify the compute or runtime cost of computing similarities for each prompt, which would matter for scaling up."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7nvUNiGBLO", "forum": "67o9fTKYZJ", "replyto": "67o9fTKYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission8707/Reviewer_5QAG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8707/Reviewer_5QAG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963017942, "cdate": 1761963017942, "tmdate": 1762920512056, "mdate": 1762920512056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EVOL-RL, a RL framework that integrates majority voting and semantic novelty into the reward function. The core idea is to encourage the model to produce semantically novel yet self-consistent responses, extending majority voting to mitigate diversity collapse and mode collapse. Experimental results on Qwen3-4B-Base and Qwen3-8B-Base models demonstrate the effectiveness and robustness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is clearly present and easy to follow.\n- The experimental results show that EVOL-RL actually achieves notably improvement in pass@16 accuracy, which is a big plus."}, "weaknesses": {"value": "- EVOL-RL primarily builds upon self-consistency-based reward, adding an auxiliary objective to promote response diversity. However, the method remains fundamentally dominated by self-consistency. In my view, this contributes little to improving the model's reasoning capability. For example, when tackling high-difficulty problems, most LLM responses are incorrect, meaning the reward signal derived from majority voting is unreliable. Consequently, the method cannot effectively guide the model toward the correct reasoning trajectory. \n- Using cosine similarity in the embedding space to measure semantic novelty is a rather coarse approach. For high-dimensional embeddings, cosine similarity may not provide enough discrimination. In addition, it only reflects novelty at the semantic level, not the logical novelty of the reasoning process, and the latter is what truly matters for reasoning exploration.\n- Validation on small-scale models introduces considerable uncertainty. I suggest that the authors include experiments on larger models (e.g., 32B parameters), which would strengthen the reliability of the results and better demonstrate the method’s effectiveness.\n- The proposed Novelty Reward could likely be integrated into the RLVR framework (using GT reward instead of major voting reward) as well. I recommend adding an additional experiment combining RLVR + Novelty Reward. If this setup also yields a clear improvement in pass@16, it would further support the generality and effectiveness of the proposed novelty-based reward design.\n\nOverall, I take a cautious approach and give a borderline reject recommendation. I look forward to the authors' response."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6DTsSwOf47", "forum": "67o9fTKYZJ", "replyto": "67o9fTKYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission8707/Reviewer_UJzh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8707/Reviewer_UJzh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163548210, "cdate": 1762163548210, "tmdate": 1762920511661, "mdate": 1762920511661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a straightforward idea regarding label free RL training. It solves the entropy collapse issue from previous label free training approaches. \n\nInspired by biological evolution, they propose a novel label free RL training design which balances two aspects of the RL training: selection (picking good answers) and variation (trying new things). \n\nBased on this intuition, they designed EVOL-RL, a practical system that complements the majority-vote reward with a \"novelty-aware reward\" for new reasoning paths. This approach proved highly effective, avoiding performance decline and enabling models to maintain longer, more informative chains of thought. Empirically, this design brings state-of-the-art results, and can improve generalization to unseen problems where prior methods fail."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper exposes and directly fixes \"entropy collapse,\" a critical limitation of previous label-free RL methods that causes models to converge on a single, repetitive solution and lose reasoning diversity.\n\n- The EVOL-RL method can improve non-trivially over previous TTRL by rewarding additional \"novelty,\" which encourages to maintain longer, more informative, and varied chains of thought instead of just finding the shortest, most common answer. \n\n- It also shows strong generalization, improving on unseen tasks it wasn't trained on."}, "weaknesses": {"value": "- Will this design work on other base series? There are concerns that the Qwen base model has trained on massive reasoning data during its annealing and SFT stage which could potentially have \"evaluation leakage\". It makes it hard to judge if the current Evol-RL design will only work on a Qwen kind of base model or it can have broader generalization implications for other base models. \n\n- The \"novelty-aware reward\" isn't free. It requires a separate process to calculate the semantic difference between solutions, adding higher computational cost and additional engineering complexity to the training setup.\n\n- There are other ways to improve a model's entropy, for example changing the temperature to encourage more diverse and stochastic output and etc. The authors didn't address other existing solutions to address the entropy collapse issue [1] - [6]. \n\n- The system measures novelty using semantic similarity in an embedding space. This is a proxy for real, human-like creativity and may not always capture what makes a reasoning path genuinely inventive.\n\n\n[1] Dai et al. CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models. 2025\n\n[2] Li et al. Jointly Reinforcing Diversity and Quality in Language Model Generations. 2025\n\n[3] Cui et al. The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models. 2025\n\n[4] Park et al. Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models. 2025\n\n[5] Shen, H. On Entropy Control in LLM-RL Algorithms. 2025\n\n[6] Zhao et al. The Majority is not always right: RL training for solution aggregation. 2025"}, "questions": {"value": "See the above Weaknesses. Mainly including:\n\nValidating the design on other base models.\n\nMore detailed computation cost analysis.\n\nBetter justifying the position of this work compared to other existing work that attempts to resolve entropy collapse and novelty encouragement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mCRhg0P8oq", "forum": "67o9fTKYZJ", "replyto": "67o9fTKYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission8707/Reviewer_zE94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8707/Reviewer_zE94"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762703080756, "cdate": 1762703080756, "tmdate": 1762920511327, "mdate": 1762920511327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies label‑free self‑improvement of large language models (LLMs). The authors argue that naïve self-training tends to collapse entropy (shorter, less diverse outputs). They propose EVOL-RL, combining (1) majority-voted selection for stability and (2) novelty-aware rewards to encourage variation. The method uses a semantic-space novelty reward with an entropy regularizer and asymmetric clipping in a generalized RL framework. Experiments on reasoning tasks (AIME24→AIME25) show substantial gains in Pass@1 and Pass@k over majority-only baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Combining majority selection and novelty reward is intuitive, easy to implement, and empirically effective.\n* Significant empirical improvements on challenging reasoning tasks, with out-of-domain transfer tested.\n* Clear presentation and reproducible implementation"}, "weaknesses": {"value": "* Majority selection + novelty/diversity reward is conceptually straightforward and has analogues in prior work (self-consistency, diversity-encouraging RL). The paper should clarify distinctions and contributions relative to prior diversity-aware self-training approaches. [1][2]\n* Embedding-based novelty reward may require more computation load. Long-term behavior over many self-improvement iterations is unclear; does novelty continue to help or collapse eventually?\n* The current reward, while balancing selection and variation, may overly penalize semantically similar reasoning paths that produce an incorrect final answer. For example, a reasoning trajectory almost identical to a majority-correct solution but failing on a minor step would receive a low reward. This strict focus on final correctness could discourage exploration of \"almost correct\" reasoning strategies, limiting the model’s ability to refine nuanced problem-solving behaviors.\n* The paper appears to use test-set examples during self-training (e.g., AIME24→AIME25 transfer).\n\n[1] Pang J C, Wang P, Li K, et al. Language model self-improvement by reinforcement learning contemplation[J]. arXiv preprint arXiv:2305.14483, 2023.\n[2] Shuvendu Roy, Hossein Hajimirsadeghi, ta al. You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models[J]. arXiv preprint arXiv:2511.04902, 2025."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SGGxBdvxvO", "forum": "67o9fTKYZJ", "replyto": "67o9fTKYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission8707/Reviewer_zAS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8707/Reviewer_zAS9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762777567064, "cdate": 1762777567064, "tmdate": 1762920510994, "mdate": 1762920510994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revision Changes"}, "comment": {"value": "To address the reviewers' feedback and improve the paper’s clarity and robustness, we made the following updates:\n\n1. **Extended Training Dynamics (Figure 3 & Figure 5):**\n\n   * We extended the training duration on **MATH-500** to ensure stability.\n   * The results show that EVOL-RL remains stable for up to 300 steps, while the baseline models degrade or plateau.\n\n2. **Expanded Related Work (Section 2):**\n\n   * We added a more detailed comparison with concurrent entropy-minimization and label-free methods.\n   * This clarifies how our directional novelty reward differs from non-directional exploration or generic entropy bonuses.\n\n3. **Theoretical Analysis of Reward Formulation (Appendix C.1):**\n\n   * We added an analysis explaining how the reward handles **“almost correct”** solutions.\n   * The update clarifies how the method protects rare exploratory mistakes while penalizing common failure modes.\n\n4. **Generalization to Different Architectures (Appendix B.1):**\n\n   * We introduced experiments on **OctoThinker-8B-Hybrid-Base** to demonstrate transferability beyond the Qwen series.\n   * EVOL-RL achieves a **+19.5%** pass@16 improvement on AIME24 over TTRL, confirming robust generalization.\n\n5. **Comprehensive Baseline Comparisons (Appendix B.2):**\n\n   * We added comparisons against **Self-Consistency**, **EM-RL-Token**, and **EM-RL-Sequence**.\n   * Results show that simple entropy regularization fails to improve pass@16 on challenging benchmarks like AIME24.\n\n6. **Runtime and Computational Cost Analysis (Appendix B.5):**\n\n   * We added wall-clock analysis (Figure 7) comparing Novelty Reward Calculation to the training process.\n   * The overhead of reward computation is minor relative to generating long reasoning chains.\n\n7. **Validation of Semantic Novelty Metric (Appendix B.6):**\n\n   * We included a case study visualizing the cosine similarity matrix of reasoning paths (Figure 8).\n   * This demonstrates that the embedding-based metric effectively distinguishes different logical paths and serves as a valid proxy for reasoning novelty."}}, "id": "rWfJWG6ENJ", "forum": "67o9fTKYZJ", "replyto": "67o9fTKYZJ", "signatures": ["ICLR.cc/2026/Conference/Submission8707/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8707/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission8707/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763555705211, "cdate": 1763555705211, "tmdate": 1763555705211, "mdate": 1763555705211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}