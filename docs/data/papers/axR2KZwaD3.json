{"id": "axR2KZwaD3", "number": 1173, "cdate": 1756858678193, "mdate": 1763432532767, "content": {"title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility", "abstract": "Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data remarkably differ from those of text or vision. Time-series embeddings, unlike text or vision, exhibit sharply decaying singular spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of *flow-of-ranks*, a mechanism by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why rank schedules should grow with depth. Guided by these results, we compress Chronos, a large time series foundation model, achieving a reduction of $65\\\\%$ in inference time and $81\\\\%$ in memory without loss of accuracy. These findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility.", "tldr": "", "keywords": ["time series", "foundation models", "rank structure", "attention", "embedding"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6140bd7b582c89967eb287db01c9764c3f68210d.pdf", "supplementary_material": "/attachment/720e46323abad9ebb48e3570f80e60f7b66ac3d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes Transformers for time-series data through the lens of rank structure. The authors show that time-series embeddings are inherently low-rank, unlike those in text or vision. They prove that this structure induces low-rank attention matrices, introducing the concept of flow-of-ranks to describe how rank gradually increases with layer depth due to nonlinear mixing. They demonstrate that time-series foundation models, such as Chronos, can be compressed by up to 65% in inference time and 81% in memory without accuracy loss."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper combines linear-algebraic theory with well-designed experiments confirming the predicted low-rank behavior in TSFMs.\n2. The authors introduce a new analytical lens (flow-of-ranks) that connects data modality to model design. The findings have real design implications for TSFMs."}, "weaknesses": {"value": "1. The main validation focuses on Chronos. Testing on other TSFMs (like TimesFM and Time-MoE) would strengthen the generality claim.\n2. The comparison to prior compression methods (LoRA) is missing, which makes it unclear how much gain stems from modality vs. technique."}, "questions": {"value": "1. How does the proposed compression compare quantitatively with existing low-rank or sparse-attention baselines (e.g., LoRA, Linformer)?\n2. Does the low-rank structure persist after fine-tuning a compressed model on downstream tasks?\n3. Can the flow-of-ranks pattern be empirically confirmed on other TSFMs beyond Chronos?\n4. How would the theory extend to multivariate or irregularly sampled time series?\n5. Could you provide a simple practical guideline (e.g., rank schedule formula) for designing TSFMs from scratch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xsT0RyL6Xl", "forum": "axR2KZwaD3", "replyto": "axR2KZwaD3", "signatures": ["ICLR.cc/2026/Conference/Submission1173/Reviewer_u1Ge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1173/Reviewer_u1Ge"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761336337402, "cdate": 1761336337402, "tmdate": 1762915697608, "mdate": 1762915697608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A Question about Theorem 2"}, "comment": {"value": "Consider the input embedding defined by a two-layer residual MLP. For any $\\varepsilon > 0$, the formula indicates that the maximum rank is $(1 + \\varepsilon^{-2})k$. Since $\\varepsilon$ is usually a very small positive number (typically less than 1), it seems that when $\\varepsilon$ is less than $\\sqrt{k/(d - k)}, (1 + \\varepsilon^{-2})k$ is much larger than $d$. For example, when $k = 16$ and $d = 768$, $\\varepsilon$ only needs to be less than $0.1458$ for this to hold (Apparently, $\\varepsilon\\$ is usually smaller than 0.1458). I wonder if I might be missing something here, as it seems counterintuitive that the $\\varepsilon$-numerical rank of the MLP embedding is constrained by the patch size $k$ rather than the hidden dimension $d$ in such cases. Could you kindly shed some light on how to reconcile this apparent discrepancy? I would truly appreciate your insights."}}, "id": "JggnUtQYsk", "forum": "axR2KZwaD3", "replyto": "axR2KZwaD3", "signatures": ["~Peiwang_Tang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Peiwang_Tang1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1173/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119001361, "cdate": 1763119001361, "tmdate": 1763119343961, "mdate": 1763119343961, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We appreciate the reviewers' valuable feedback to help improve the clarity and generality of our manuscript. We are glad that all the reviewers found our work mathematically rigorous and an important contribution to understanding TSFMs. To summarize:\n * **Reviewer Qkww** acknowledges that our \"theories provide novel insights to guide time series foundation model design and that the translation from theory to practice is seamless and compelling.\"\n * **Reviewer eiE9** highlights that our paper is \"easy to follow\" with \"intuitive results\" that serve \"as a good entry point to understand how time series data differs from other modalities\". \n * **Reviewer uH8K** acknowledges that our work \"provides the first general theoretical results (Theorem 3) connecting low-rank input embeddings to the compressibility of the internal Attention matrices\" and \"introduces and quantifies the concept of \"flow-of-ranks.\" \n * **Reviewer u1Ge** importantly highlights that our \"findings have real design implications for TSFMs\". \n\nWe provide a common response below, and address minor, reviewer-specific comments in our individual reviewer responses. We have also updated the manuscript with the changes highlighted in red in the rebuttal version.\n\n\n## **Applicability to TSFMs beyond the Chronos family**\n We have added additional experiments on compressing Moirai-1.0-R-base in Table 3, which we also show in the below table:\n\n | Size Ratio | $\\tilde{d}\\_0$ | $\\alpha$ | WQL | MASE | $\\tilde{d}\\_0$ | $\\alpha$ | WQL | MASE |\n |:-----------:|:----------:|:---------:|:------:|:-------:|:-------------:|:---------:|:------:|:-------:|\n | | **w/ flow-of-ranks**  |  |  |  | **w/o flow-of-ranks** |  |  |  |\n | 0.250 | 8 | 0.34 | 1.001 | 1.014 | 16 | 0.00 | 1.069 | 1.050 |\n | 0.500 | 10 | 0.58 | 0.996 | 1.007 | 32 | 0.00 | 1.038 | 1.036 |\n | 1.000 | – | – | – | – | 64 | 0.00 | 1.000 | 1.000 |\n\n For Moirai-1.0-R-base, we show that one can also compress the attention matrices by at least $75%$ percent without affecting the performance. The results show that our method is applicable more broadly than just to Chronos and Chronos-Bolt, and is more generally observed on other TSFMs, e.g., Moirai. The results also show an ablation of the flow-of-ranks design proposed in our paper: given the same compression ratio, a model compressed with flow-of-ranks performs significantly better than a model that does not.\n\n\n## **Flow-of-Ranks holds on Various TSFMs** \nWe have done some additional experiments that reveal the ranks of attention matrices in more TSFMs, including Moirai, WaveToken, and VisionTS in Figure 6. Our results reveal the following:\n\n  * Moirai, which uses a patch-based input embedding still has low-rank structures. There is also a clear flow-of-rank pattern in the weight matrices throughout layers.\n\n  * WaveToken, which uses a wavelet transform to decompose the context into different frequency-modes, inputs a time series (i.e., the concatenation of wavelets in the time domain) rather than the wavelet coefficients. The input domain is a one-dimensional space, and is quantized for embedding. We also observe low-rank structures. (See Figure 2(a)) (More empirical analysis can also be found in Appendix H.)\n\n  * VisionTS first transforms a time series into an image and operates on the patches. Since the patch space usually has a higher dimension, we observe that the attention matrices are closer to full-rank than those in other TSFMs.\n\n  These results show that many TSFMs indeed have low-rank structures in their attention matrices, and our flow-of-ranks is a general phenomenon.\n\n\n## **Multivariate TSFMs** \nWe have revised footnote 1 to indicate how the analysis in our paper seamlessly applies to multivariate TSFMs. Note that our central argument in Section 2 is that the embedding $\\boldsymbol{\\Phi}: \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$ maps a $k$-dimensional patch space onto a low-dimensional submanifold $\\boldsymbol{\\Phi}(\\mathbb{R}^k)$ in the hidden space because we usually have that $k \\ll d$. For multivariate time series, the input embedding is $\\boldsymbol{\\Phi}: \\mathbb{R}^{k \\times c} \\rightarrow \\mathbb{R}^d$, where $c$ is the number of variates. As long as $k\\cdot c \\ll d$, the central argument still applies.\n\n\n## **Autocorrelation-agnostic** \nWe added clarifications in Sections 3 and 4 by separating the temporal domain from the embedding domain and showing that the low-rank assumption follows from the low-dimensional patch embedding, which is independent of temporal structure, stationarity, or any specific autocorrelation pattern."}}, "id": "5GOX5kPn2F", "forum": "axR2KZwaD3", "replyto": "axR2KZwaD3", "signatures": ["ICLR.cc/2026/Conference/Submission1173/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1173/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1173/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763229343437, "cdate": 1763229343437, "tmdate": 1763229343437, "mdate": 1763229343437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper titled “UNDERSTANDING TRANSFORMERS FOR TIME SERIES: RANK STRUCTURE, FLOW-OF-RANKS, AND COMPRESSIBILITY” analyzes Transformer models for specifically time series (TSFMs). They show that in this scenario, the transformer models ( as a consequence of the data being passed to it) possess a uniquely low-rank structure compared to similar architectures for text or vision. They postulate thus due to the continuous nature of time-series embeddings. This low-rank input structure leads to that the Attention layer matrices being highly compressible. The authors introduce the concept of \"flow-of-ranks,\" which describes how the numerical rank of a representation gradually increases with model depth due to nonlinear mixing, explaining why earlier layers are more amenable to compression. By leveraging these insights, the researchers demonstrate that large TSFMs like Chronos are severely over-parameterized and can be significantly compressed, achieving up to a 65% reduction in inference time and 81% in memory without losing predictive accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The low rank property of time-series may prove very useful in their application using transformers, in terms of designing models with fewer parameters. Theoretically, this low-rank property is proven for continuous embeddings, with guaranteed polynomial or exponential decay of singular values for smooth or analytic functions, respectively (Theorems 1 and 2).\nThe work provides the first general theoretical results (Theorem 3) connecting low-rank input embeddings to the compressibility of the internal Attention matrices (W_Q, W_K, W_V)\nThe paper introduces and quantifies the concept of \"flow-of-ranks,\" which explains how  non-linear components (like activations, residual connections, and normalization) across deep layers gradually increase the rank of a representation (Theorem 4)"}, "weaknesses": {"value": "The majority of the empirical validation and compression experiments focus almost exclusively on the Chronos family of Time Series Foundation Models. While there are references to other models the core compression techniques and deep rank analysis (flow-of-ranks, impact of heads) are primarily demonstrated on Chronos. This limits the generality of the practical findings and compression results to other TSFM architectures\nWhile the paper's core claims about rank structure are presented as a modality-dependent framework, the empirical evidence is often constrained to a small number of specialized experiments. How do we know this structure will prevail in other transformer architectures or other datasets.\nThe core theoretical analysis (Theorems 1 and 2) is derived for the univariate time series case. The authors mention in passing that this is extendible to few-variate time series but a detailed discussion is lacking"}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FNM5ozfmA2", "forum": "axR2KZwaD3", "replyto": "axR2KZwaD3", "signatures": ["ICLR.cc/2026/Conference/Submission1173/Reviewer_uH8K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1173/Reviewer_uH8K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793145702, "cdate": 1761793145702, "tmdate": 1762915697443, "mdate": 1762915697443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why Transformers trained on time series data behave differently from those trained on text or vision. The authors analyze the rank structure of embeddings and attention matrices to explain why time-series foundation models (TSFMs)—such as Chronos are highly compressible without losing much accuracy.\nSeveral key points include: time series are naturally low-rank, and flow-of-rank perspective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To my understanding, transformer theories in time series are generally lacking or provides limited practical guidance. This paper serves as a good entry point to understand how time series data differs from other modalities.\n2. The results are intuitive to me, making this paper easy to follow."}, "weaknesses": {"value": "1. This paper considers univariate time series. which is limited as several TSFMs can handle any-variate inputs.\n2. The paper assumes input data X is 1-rank (or low-rank). I think this is a pretty strong assumption, which may not hold in many high-dimensional data."}, "questions": {"value": "1. Can the authors explain footnote 1? Does it mean that if we have $n$-variate data, $x$ is then rank-$m$, where $n = m$? Is it possible that $m < n$?\n2. I wonder how naive Thm 1 and Thm 2 are? Since this paper mainly shows existence proof, if the input data is low-rank, it seems like its straightforward that we can find low-rank weights to model it. Are there any counterexamples?\n3. Are there any other features in the data assumption that makes it a time series? Or would the result hold for all low-rank input data?\n\nOverall, I think this paper will be a good contribution to the field and am happy to adjust my score if the authors address my concerns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qmvMRtBcKv", "forum": "axR2KZwaD3", "replyto": "axR2KZwaD3", "signatures": ["ICLR.cc/2026/Conference/Submission1173/Reviewer_eiE9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1173/Reviewer_eiE9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961038649, "cdate": 1761961038649, "tmdate": 1762915697250, "mdate": 1762915697250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a rigorous linear-algebraic analysis of time series Transformer models. It first examines the ranking structure of time series embeddings, revealing unique low-rank characteristics that distinguish them from other modalities. The authors then theoretically infer the potential low-rank nature of the attention matrices in time series Transformers. Meantime, the authors introduce the concept of “flow-of-ranks” to describe how representation ranks evolve and increase across Transformer layers due to nonlinear transformations. Finally, leveraging these insights, the paper proposes two effective compression strategies for time series foundation models, achieving up to 65% reduction in inference time and 81% reduction in memory usage on the Chronos model, without compromising accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong Theoretical Grounding: Theorems 1 and 2 formally connect patch size and embedding smoothness to low-rank structure. Theorem 3 crucially links low-rank inputs to compressible attention layers, while Theorem 4 quantifies the \"flow-of-ranks.\" These theories provide novel insights to guide time series foundation model design.\n\n2.  The translation from theory to practice is seamless and compelling. Each theoretical claim is supported by corresponding empirical evidence, making the overall theory framework more convincing and practically relevant. And the results are striking, showing that TSFMs are significantly more over-parameterized than LLMs and can be compressed dramatically. The layer-dependent rank schedule is a simple yet powerful idea derived directly from the \"flow-of-ranks.\"\n\n3. Clarity and Organization: Despite the complex mathematical content, the paper is well-structured and readable. The flow from data structure to single-layer analysis to depth-dependent phenomena and finally to applications is logical and easy to follow."}, "weaknesses": {"value": "1. The core experiments only focus on a single architecture family. All experiments are conducted on Chronos and Chronos-Bolt, which are based on the T5 architecture. While the principles are argued to be general, validation on other TSFM architectures (e.g., TimesFM, Moirai) would have further strengthened the claim of universality.\n\n2. The paper provides elegant existence proofs—such as the low-rank properties of time series embeddings and the W_Q/K/V matrices—based on the core assumption that time series embeddings are intrinsically low-rank. However, this assumption may be an artifact of current TSFM design choices (e.g., small patch sizes and simple MLP-based embedding layers). As more tokenization methods emerge (e.g., VisionTS [1], Wavelet-based Tokenization [2]), it remains unclear whether these conclusions will still hold.\n\n[1] Chen M, Shen L, Li Z, et al. Visionts: Visual masked autoencoders are free-lunch zero-shot time series forecasters[J]. arXiv preprint arXiv:2408.17253, 2024.\n\n[2] Masserano L, Ansari A F, Han B, et al. Enhancing foundation models for time series forecasting via Wavelet-based tokenization[J]. arXiv preprint arXiv:2412.05244, 2024."}, "questions": {"value": "1. Could the observed low-rank and compressibility properties be interpreted as universal characteristics of time series Transformer models? If so, does this imply that current architectures are inherently low-rank and may therefore lack sufficient expressiveness to capture more complex temporal dynamics?\n\n2. The success of pre-trained compressed model suggests that standard TSFMs are severely over-parameterized. Does this imply that the common practice of scaling up model size for time series is misguided ?  Is the low rank of TSFMs a inherant feature, or is it a sign that we are not yet challenging them with tasks of sufficient complexity?\n\n3. Theorem 3 suggests that the attention matrix may be less compressible when dealing with more complex or higher-rank input time series (e.g., noisy financial data). Are there datasets of this nature that could be used to empirically test the boundaries of the proposed low-rank assumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DFZLKVs7FC", "forum": "axR2KZwaD3", "replyto": "axR2KZwaD3", "signatures": ["ICLR.cc/2026/Conference/Submission1173/Reviewer_Qkww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1173/Reviewer_Qkww"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997642821, "cdate": 1761997642821, "tmdate": 1762915697117, "mdate": 1762915697117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}