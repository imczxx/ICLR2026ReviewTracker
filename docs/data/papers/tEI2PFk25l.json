{"id": "tEI2PFk25l", "number": 23068, "cdate": 1758339074068, "mdate": 1759896833557, "content": {"title": "Human-Prior Correction: Scalable Post-hoc Calibration that Aligns Vision Models with Human Uncertainty", "abstract": "Deep vision models achieve high accuracy but produce poorly-calibrated predictions that misalign with human uncertainty, limiting their reliability in safety-critical applications. We propose Human-Prior Correction (HPC), a post-hoc calibration method that aligns model confidence with human perceptual uncertainty without retraining. HPC solves a principled Bayesian objective $\\min_p \\text{KL}(p||\\text{human}) + \\lambda \\text{KL}(p||\\text{model})$ yielding the closed-form solution $p^* \\propto \\text{human}^\\alpha \\cdot \\text{model}^{1-\\alpha}$, where the human confusion prior captures systematic perceptual similarities (e.g., cat$\\leftrightarrow$dog). Our key insight is that foundation models like CLIP encode human-like confusion patterns that serve as proxy priors, eliminating the need for expensive human annotations. Across CIFAR-10/100 and ImageNet, HPC achieves (1) 19.7\\% improvement in human alignment (NLL$_\\text{human}$), (2) 23.9\\% ECE reduction while maintaining accuracy, (3) increasing benefits under distribution shift (21\\% improvement at maximum corruption), and (4) 45\\% reduction in worst-group calibration disparity. The method adds negligible computational overhead (less than 0.1\\% of forward pass), combines synergistically with existing calibration techniques, and improves conformal prediction sets by yielding tighter intervals at fixed coverage. By incorporating structured human confusions into predictions, HPC bridges the gap between statistical calibration and human-aligned uncertainty, a critical step toward trustworthy AI deployment.", "tldr": "HPC aligns model confidence with human uncertainty by log-linear pooling of model probabilities with human or CLIP-derived confusion priors, improving calibration, robustness under shift, and conformal set tightness without retraining.", "keywords": ["uncertainty calibration", "post-hoc calibration", "human uncertainty", "human priors", "log-linear pooling", "foundation model priors (CLIP/DINO)", "conformal prediction", "distribution shift robustness", "subgroup fairness", "vision classification"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebcb25031ca04a98243490008cd07d310b31c0ee.pdf", "supplementary_material": "/attachment/44d8ed2e05bfa9919dd14e0c011747b0e2ba681a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Human-Prior Correction (HPC), a post-hoc calibration method that incorporates human confusion patterns into model predictions. The key innovation is using foundation models (CLIP, DINO) as proxy sources for human-like confusions, eliminating expensive annotation requirements. The method is evaluated on CIFAR-10/100 and ImageNet variants, showing improvements in human alignment (NLL_human) and calibration (ECE) while maintaining accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Using foundation models (CLIP, DINO) as proxy sources for human confusion patterns is creative and addresses a genuine practical bottleneck—expensive human annotations.\n- The paper considers multiple important aspects of calibration (human alignment, robustness, fairness, conformal prediction), but the results are poorly organized and inconsistently reported, making it hard to extract clear conclusions from the main text."}, "weaknesses": {"value": "**Errors in references**\n\n- Non-existent reference: \"Correcting the calibration: A novel framework for aligning classifier predictions with human judgement\" \n- Wrong authors: Galil et al. (2023) citation lists \"Itay Galil, Shiran Dori-Hacohen, and Alon Shachaf\" but actual authors are \"Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom Goldstein\"\n\nThese errors raise serious concerns about the paper's rigor.\n\n**Major writing and presentation issues**\n\n- Lines 254-260 (Ablations section): States results but doesn't reference any table or figure—where should readers verify these claimed observations?\n- Lines 372-377: Discusses results without referencing specific figures or tables\n- Table 4: Caption appears both above and below the table \n- Table 5: Uses completely different format compared to other tables\n- Table 2: Doesn't mention which model/architecture is used\n- Figure 3 caption: Mentions \"BCTS, Dirichlet\" but these methods are not shown in the figure\n- Table captions don't include main conclusions or takeaways\n\nThese issues make verification of claims impossible.\n\n**Inconsistent reporting**\n\nThe arbitrary selection of which proxy results to show suggests cherry-picking and does not instill confidence.\nTable 1: Reports Human, DINO, CLIP\nTable 2: Reports Human, CLIP (why not DINO?)\nTable 3: Reports CLIP, SimCLR for CIFAR-100; \nCLIP, DINO for ImageNet-200 \n\n**LLM usage is not reported**"}, "questions": {"value": "Why do different tables report different subsets of proxy priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ORJtgjlE9y", "forum": "tEI2PFk25l", "replyto": "tEI2PFk25l", "signatures": ["ICLR.cc/2026/Conference/Submission23068/Reviewer_bvL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23068/Reviewer_bvL3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951257009, "cdate": 1761951257009, "tmdate": 1762942500129, "mdate": 1762942500129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a confidence calibration method for classification networks. The calibration is done by combining the image prediction distribution with the marginal class confusion distribution of the predicted class.   The combining is done by applying the posterior formula."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Confidence calibration is an important problem and there is always room for improvement."}, "weaknesses": {"value": "The paper is not well written. The abstract is too technical.   Notation is not defined before it is used (e.g. p_0  in eq. 4). What is beta in section 3. 4.?  Adding an algorithm box to the paper can increase its readability.  \n\nThe temperature scaling method is very simple, robust and works well. The proposed method requires setting many parameters (class conditional distributions and lambda)  and there is no significant improvement that justifies using the proposed method."}, "questions": {"value": "I didn't find the paper Li & Zhang (2025). I guess it hasn't been published yet.  \nWhat is beta in section 3.4?\n\nWhy not use the class confusion matrix computed on the calibration dataset instead of the clip-based distribution?\n\nIn the case that the predicted class y_pred is wrong, you are using the wrong class conditional distribution C[y_pred,:]. How does this affect your method?\n\nThe method is closely related to performing temperature scaling, where temperature is selected for each predicted class separately.\nYou should compare your method with this simple version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I didn't find the paper Li & Zhang (2025). I guess it hasn't been published yet."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iW8REJ5zb3", "forum": "tEI2PFk25l", "replyto": "tEI2PFk25l", "signatures": ["ICLR.cc/2026/Conference/Submission23068/Reviewer_2gf5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23068/Reviewer_2gf5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154165069, "cdate": 1762154165069, "tmdate": 1762942499870, "mdate": 1762942499870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Human-Prior Correction (HPC), a post-hoc calibration framework that adjusts model confidence to align with human perceptual uncertainty without retraining. The key idea is to blend model predictions with a human confusion prior through a Bayesian objective, yielding a closed-form correction. This prior encodes structured human confusion patterns, such as cat↔dog similarities, which are either derived from annotation datasets like CIFAR-10H or approximated using proxy priors from foundation models (CLIP, DINO, SimCLR). The paper claims HPC improves human alignment (NLL_\\text{human}) by about 20%, reduces Expected Calibration Error by nearly 24%, and enhances robustness under distribution shift while maintaining accuracy. The method incurs negligible computational overhead and complements existing techniques like temperature scaling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach to infuse human confusion information is well-motivated. To scalably define the prior, they approximate human confusion through models like CLIP and SSL.  They conducted experiments across multiple datasets and demonstrate consistent gains across architectures, robustness to prior misspecification. The integration of HPC with conformal prediction is also well-motivated, offering tighter yet semantically coherent prediction sets. \nThe method’s practicality without requiring  retraining, and adding a small computational overhead is interesting."}, "weaknesses": {"value": "While the method is presented as post-hoc, it depends on access to a class-conditional confusion prior that itself must be estimated, either from human annotations or proxies derived from other models. The assumption that CLIP or DINO capture human-like confusion patterns is plausible but not theoretically justified. While theoretical justification might be difficult, atleast there should be a discussion on when this assumption fails. For example, consider a fine-grained classification which is a practically relevant classification tasks. Say CUB-200. Wouldn't the human confusion be much larger than the CLIP/DINO when they are trained on it. Consequently, given the mismatch, the result would not align with human-aware calibration. Highlighting this is important.  \nRelated to this, if a FM like CLIP is assumed to be available, the paper should discuss the reasons behind utilizing a Resnet-18 or Resnet-50 instead of directly deploying CLIP (with/ without finetuning) for the classification task itself.? The main novelty is primarily from approximating human-confusion with these foundation models. \nAlso, a lot of references are non-existent, making it very difficult to ascertain the merits as well as placing trust in the results of the paper. \n\nQ. Li and M. Zhang. Correcting the calibration: A novel framework for aligning classifier predictions with\nhuman judgement. Journal of Machine Learning Research, 26(5):775–800, 2025. doi: 10.5555/12345678.\n12345679. 1, 2, 4\nYifan Zhang, Qiang Li, Bolei Zhou, Andrea Vedaldi, and Philip H.S. Torr. Human-aligned uncertainty quantifi-\ncation for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 8234–8243, 2024. 2\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. On calibration of mixture-of-experts\nneural networks. In International Conference on Learning Representations (ICLR), 2020. 2"}, "questions": {"value": "Please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "References seem to be hallucinated- possibly through an LLM."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B7yyGDCBHS", "forum": "tEI2PFk25l", "replyto": "tEI2PFk25l", "signatures": ["ICLR.cc/2026/Conference/Submission23068/Reviewer_4sGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23068/Reviewer_4sGm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762355730020, "cdate": 1762355730020, "tmdate": 1762942499354, "mdate": 1762942499354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}