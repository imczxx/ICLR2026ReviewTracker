{"id": "9pZhYkf80k", "number": 19177, "cdate": 1758294149296, "mdate": 1759897053987, "content": {"title": "Achieving Ultra-Low Latency and Lossless ANN-SNN Conversion through Optimal Elimination of Unevenness Error", "abstract": "Spiking Neural Networks (SNNs) are a promising approach for neuromorphic hardware deployment due to high energy efficiency and biological plausibility. However, existing ANN–SNN conversion methods suffer notable accuracy degradation under low-latency inference, primarily caused by the $\\textbf{unevenness error}$. \nTo mitigate this error, prior works commonly adopt trade-off strategies at the cost of higher latency and energy consumption, such as longer time-steps, more complex spiking neuron models, or two-stage inference mechanisms. In this paper, we present a principled and efficient solution to the unevenness error. Specifically, we first develop a unified framework to quantify the unevenness error and then derive a sufficient condition for eliminating it: under an approximately constant input current, matching the ANN quantization function ($\\operatorname{floor}$, $\\operatorname{round}$, $\\operatorname{ceil}$) with the SNN’s initial membrane potential ($0$, $\\frac{\\theta}{2}$, $\\theta$), where $\\theta$ is the firing threshold, and setting the quantization level $L$ equals to the number of time-steps $T$, which ensures exact ANN–SNN correspondence.\nThis finding challenges the prevailing belief that more time-steps always yield better accuracy; instead, it reveals that there exists an optimal time-step that matches the ANN’s quantization characteristics, avoiding redundant inference latency from excessive time-steps. \nExtensive experiments on CIFAR-100, ImageNet-1K, CIFAR10-DVS, and DVS-Gesture validate our theory. For example, our method achieves a state-of-the-art 74.74\\% top-1 accuracy on ImageNet-1K using ResNet-34 with only 8 time-steps, demonstrating the effectiveness of our approach in low-latency SNN inference.", "tldr": "This paper solves unevenness error in low-latency ANN–SNN conversion via a quantification framework and elimination condition validated by experiments (e.g., 74.74% ImageNet-1K accuracy with ResNet-34 in 8 time steps)", "keywords": ["Spiking Neural Networks; ANN-SNN Conversion;Unevenness Error"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59c2ef6eedebcf807d5fed80659e0f7b2e67e04c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a theoretical analysis of unevenness error and proposes a unified framework for optimal elimination of the error."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This work discusses how to eliminate error in three specific cases of the quantization function (floor, ceil, round)."}, "weaknesses": {"value": "1. The key point of this work is not actually about eliminating unevenness error, the so-called error elimination is just a packaging story background. As shown in Algorithm 1, line 19, the authors set $\\forall t, q_t^l=\\frac{W^l\\sum_{t=1}^Ts_t^{l-1}\\theta^{l-1}}{T}$, which means that the input current at each time-step is exactly the same. This is essentially equivalent to replacing the result of an $L$-level threshold function at one time-step with the result of a single-threshold function at $L$ consecutive time-steps, and the entire process is completely equivalent. Therefore, I tend to think that the contribution of this work to the SNN community is very poor.\n\n2. Traditional ANN-SNN Conversion cannot be directly applied to time-series datasets such as CIFAR10-DVS. Therefore, it is curious how this work deals with the specific details, which do not seem to be discussed in the main text. I tend to think that this work may have adopted a scheme similar to the multi-threshold spiking model, followed by equivalent conversion in SNN inference stage. This idea has already been proposed in previous works."}, "questions": {"value": "See Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GTEOCZnPpZ", "forum": "9pZhYkf80k", "replyto": "9pZhYkf80k", "signatures": ["ICLR.cc/2026/Conference/Submission19177/Reviewer_EnBu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19177/Reviewer_EnBu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795347631, "cdate": 1761795347631, "tmdate": 1762931181776, "mdate": 1762931181776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a fundamental bottleneck in ANN2SNN conversion: performance degradation under low-latency inference due to the unevenness error. The authors propose a Quantization-Voltage Matching (QVM) framework that provides a theoretical and practical method to eliminate this error completely. QVM achieves lossless conversion by aligning the ANN quantization function (floor, round, or ceil) with the initial membrane potential of the SNN neuron and by setting the quantization level L equal to the number of time-steps T. This configuration ensures that the spike count in the SNN exactly matches the quantized activations in the ANN, thus achieving theoretically zero conversion error. Extensive experiments on CIFAR-100, ImageNet-1K, CIFAR10-DVS, and DVS-Gesture show that QVM achieves state-of-the-art accuracy with drastically reduced latency with only 8 time-steps, surpassing all prior methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a mathematically rigorous derivation for the sufficient conditions eliminating unevenness error (Theorem 3) and bridges the gap between quantization theory and membrane potential dynamics.\n2. This paper achieves ultra-low latency (T=8) while maintaining accuracy comparable to full-precision ANN baselines.\n3. Comprehensive empirical validation includes detailed ablation on quantization functions, membrane potentials, and quantization levels. Figures and ablation convincingly validate the theoretical claims."}, "weaknesses": {"value": "1. The theory is derived for Integrate-and-Fire (IF) neurons; extension to Leaky IF (LIF) or adaptive threshold models is not shown, as LIF is more frequently used in recent research.\n2. While energy efficiency is implied via reduced time-steps, no measured power or latency-on-hardware benchmarks are presented.\n3. The paper references algorithmic pseudocode, but training configurations and implementation details are minimal (e.g., hyperparameters for quantization)."}, "questions": {"value": "1. Could the QVM framework extend to LIF neurons or temporal coding schemes beyond rate coding?\n2. Is there a measurable energy efficiency improvement on neuromorphic chips or FPGA?\n3. Could this principle generalize to quantized transformer-based or other architectures in SNNs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4THu9iK6Ha", "forum": "9pZhYkf80k", "replyto": "9pZhYkf80k", "signatures": ["ICLR.cc/2026/Conference/Submission19177/Reviewer_Ac34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19177/Reviewer_Ac34"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811102809, "cdate": 1761811102809, "tmdate": 1762931181402, "mdate": 1762931181402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Quantization–Voltage Matching (QVM) framework to address the unevenness error in ANN–SNN conversion. The authors derive sufficient conditions for eliminating this error and prove that aligning ANN quantization functions with corresponding SNN initial membrane potentials can achieve theoretically lossless conversion. Experiments on CIFAR-10/100, ImageNet, and DVS datasets show state-of-the-art accuracy under very low-latency inference."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Extensive experiments across multiple benchmarks (CIFAR, ImageNet, DVS) convincingly support the theoretical claims. Achieving near-lossless accuracy at only 8 time-steps on large-scale datasets highlights the real-world applicability of QVM."}, "weaknesses": {"value": "The major weakness of this paper lies in its organization and presentation. In the *Method* section, the authors present several theorems and derive the sufficient condition for eliminating the unevenness error. However, the detailed description or implementation procedure of the proposed QVM framework, which might be one of the most important part of the paper, is missing. Furthermore, the paper introduces a large number of mathematical symbols and notations without providing a summary or notation table, which significantly hinders readability. I strongly recommend that the authors reorganize the paper to improve logical flow, move the algorithmic details of QVM into the main body, and include a comprehensive table summarizing the symbols and their meanings. Therefore, I suggest resubmission after substantial revision and improvement of structure and clarity.\n\nAlthough the motivation of ANN–SNN conversion is energy efficiency, the paper does not evaluate or discuss the computational overhead, energy consumption, or neuromorphic hardware compatibility of QVM."}, "questions": {"value": "Since Theorem 3 claims that unevenness error can be theoretically eliminated, why does a small accuracy gap still remain between quantized ANN and converted SNN in practice (e.g., Table 1)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Zerd93ruly", "forum": "9pZhYkf80k", "replyto": "9pZhYkf80k", "signatures": ["ICLR.cc/2026/Conference/Submission19177/Reviewer_KkAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19177/Reviewer_KkAx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892849449, "cdate": 1761892849449, "tmdate": 1762931181010, "mdate": 1762931181010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the construction of ultra-low-latency SNNs under the ANN-to-SNN conversion framework. The authors systematically identify and formalize three key conversion errors including quantization, clipping, and unevenness and propose a new strategy named QVM that sets the number of time steps T equal to the quantization level L. Experiments show competitive accuracy on event-based datasets like CIFAR10-DVS and DVS-Gesture even at extremely low latency T=4."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper clearly dissects and discusses the origins and impacts of three error sources, particularly providing nuanced insights into the unevenness error.\n\n2.The evaluation include both conventional vision datasets and event-based neuromorphic datasets, demonstrating strong generalization and positioning the method favorably against existing ANN-conversion-based SNNs."}, "weaknesses": {"value": "1.The core mechanism of QVM, how ANN activations are mapped to initial membrane potentials, threshold settings, or whether calibration/fine-tuning is used, is not clearly described. No pseudocode is provided.\n\n2.The paper reports no energy consumption, energy efficiency, or hardware simulation results.\n\n3.Modern vision and language models heavily rely on ViTs or their spiking variants. The work only validates on CNN backbones and provides no evidence of applicability to Transformer-based architectures."}, "questions": {"value": "As in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OdFJdAS5s2", "forum": "9pZhYkf80k", "replyto": "9pZhYkf80k", "signatures": ["ICLR.cc/2026/Conference/Submission19177/Reviewer_mu5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19177/Reviewer_mu5E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762569422383, "cdate": 1762569422383, "tmdate": 1762931180495, "mdate": 1762931180495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}