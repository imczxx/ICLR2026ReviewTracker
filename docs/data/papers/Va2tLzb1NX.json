{"id": "Va2tLzb1NX", "number": 4558, "cdate": 1757706451063, "mdate": 1759898026603, "content": {"title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers", "abstract": "Transformers have achieved state-of-the-art performance across diverse language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron (MLP) neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multi-lingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing “safety” and improve performance on the GSM8K benchmark (+1.6%) by amplifying “reasoning”. Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.", "tldr": "", "keywords": ["transformers; language models; multi-head self-attention; interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25d2796e42551c44eba09b3e581f4bd2885d0a94.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Scalable Attention Module Discovery (SAMD), a method for identifying attention heads associated with abstract “concepts” in transformers via cosine similarity, and Scalar Attention Module Intervention (SAMI), a one-parameter mechanism to amplify or suppress their influence. The approach is demonstrated across LLMs and ViTs, with qualitative interpretability results and modest quantitative effects on reasoning, safety, and vision benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Introducing attention-head–level concept attribution is an original direction that is computationally light and easily applicable to diverse transformer architectures.\n\n - The same pipeline is used across text and vision models, suggesting potential generality and extensibility.\n\n - The paper contributes to ongoing efforts to connect internal transformer components to semantic behaviors, particularly through sparse, interpretable “modules.”"}, "weaknesses": {"value": "- The evaluation is dominated by qualitative visualizations and anecdotal examples. There are no robust statistical analyses, reproducible metrics, or causal validation to confirm that the discovered modules truly mediate the claimed concepts.\n\n - The use of cosine similarity as a proxy for conceptual alignment is not theoretically or empirically justified; results may reflect correlation, not causation.\n\n - Choices of K (number of heads) and s (scaling factor) appear arbitrary, tuned via small grid searches without sensitivity analysis.\n\n - The paper never clearly defines in what sense the approach is “concept-agnostic.” This weakens the interpretive and theoretical clarity of the contribution."}, "questions": {"value": "How is a “concept” defined in this framework, and how can the method be considered “concept-agnostic” if it depends on concept-specific vectors?\n\nCan the identified attention modules be causally validated (e.g., via path patching or feature ablation)?\n\nHow robust are the discovered modules to randomization, different seeds, or model variants?\n\nCould quantitative measures (e.g., mutual information or probing accuracy) strengthen the claims?\n\nThe heatmap figures are difficult to distinguish between colors, which are somehow important (e.g., with bold borders) and which are not.\nPlease, change the color."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xGZE3X9ulP", "forum": "Va2tLzb1NX", "replyto": "Va2tLzb1NX", "signatures": ["ICLR.cc/2026/Conference/Submission4558/Reviewer_WBy6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4558/Reviewer_WBy6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226449883, "cdate": 1761226449883, "tmdate": 1762917438572, "mdate": 1762917438572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an attribution method (of model components, not the input) for explaining transformer-based models. Specifically, it identifies the most important attention heads across the model which are relevant to a concept of interest i.e. the French language. To do so, they identify the maximally close (in terms of cosine similarity) attention heads to concept vectors captured from a positive dataset. They then propose that the identified components of the model can be up- or downscaled to change model behavior, thus verifying their component extraction, and providing a real use case for the method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Very interesting method with good novelty. Unlike many previous MLP neuron attribution approaches, this is the first I have seen which identifies attention concepts. This is an interesting and critical result with the stronger push for mechanistic interpretability and adjacent approaches in the modern XAI literature. \n\nWell written with extensive experimental results.  \n\nI think the simplicity of the approach is a benefit to its usability. I feel that I could replicate this with a few hours of work if I had access to the datasets."}, "weaknesses": {"value": "Minor – plainly calling this an attribution method feels misaligned with the literature. Attribution methods often refer to input (feature) attribution. This is more aligned with neuron attribution. Perhaps it should be attention attribution but not to be confused with input attribution using attention weights/gradients. \n\nThere are not any true comparisons against other methods. It is hard to tell if this should be negative because it may be challenging to create a fair comparison against a similar MLP based method. I think it could have been possible to use knowledge editing benchmark."}, "questions": {"value": "Did the authors consider a knowledge editing style benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "As6sAuOEL6", "forum": "Va2tLzb1NX", "replyto": "Va2tLzb1NX", "signatures": ["ICLR.cc/2026/Conference/Submission4558/Reviewer_HhFz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4558/Reviewer_HhFz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602924852, "cdate": 1761602924852, "tmdate": 1762917438150, "mdate": 1762917438150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SAMD and SAMI - Scalable Attention Module Discovery and Intervention. Given a feature vector $v_c$, SAMD discovers a set of $K$ attention heads whose output $a_{l,h}$ has on average high cosine similarity to $v_c$. They call this set of attention heads (which is a circuit in a way) a module.  Given a the circuit of $K$ attention heads, SAMI amplifies or suppresses the module by scaling the attention heads output $a_{l,h}$ by a scaling factor $s$ which they choose on a per problem basis using grid search. They demonstrate the effectiveness of their methods in 4 different experiments:\na) They find and steer modules that correspond to SAE features. This motivates the name - as they choose concepts, and search for relevant modules related to the concept. This is done by generating a dataset $D_p$ for which an SAE feature $v_c$ is highly activated and then running SAMD with $v_c$ and $D_p$.\nb) They find and steer a module that corresponds to reasoning capabilities in the model. They report improved scores on the GSM8K reasoning benchmark.\nc) They find and steer a module that corresponds to a refusal direction. The report improved attack success rate over orthogonalization of the refusal direction in two out of three cases. \nd) They find and steer a module that corresponds the classification of a given ImageNet target on ViT-B/32 21k, showcasing effectiveness of targeted unlearning of a single class."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novel and elegant method for circuit discovery\n* Clear presentation of the findings\n* Demonstration of effectiveness of method on a broad variety of applications over two different modalities. Especially in the vision literature this is addressing a research gap, as vision-circuit discovery remains under-explored."}, "weaknesses": {"value": "* 4.2 the construction of $D_p$ is unclear from just reading the main body of the paper. \n* Concept figure should be improved\n\t* Font way to small\n\t* No order of panels provided\n\t* SAMI is not explained in the rightmost panel\n* Comparison to baseline such as e.g. difference in means is missing for 4.1, 4.2 and 4.4. If this concern is addressed appropriately I will improve my score.\n* In 4.2 the authors only report evals on the dataset that they used for construction. An OOD reasoning benchmark eval would be useful to evaluate the generality of the reasoning-module.\n* Only ViT-B 32 evaluated in 4.4). Experiments with at least ViT-L would be recommended as ViT-B often shows different behaviors from its bigger counterparts. If this concern is addressed I will improve my score."}, "questions": {"value": "* Did the authors explore including highly negative cosine sim attention heads (e.g. Fig 24.) and flipping the $s$ value for these heads? If so, that did they find? Especially in Fig 24 gemma 7b one head seems to have the highest absolute alignment with -0.4 similarity while the highest positive alignment is only 0.3.\n* What is the rational behind the $D_p$ construction via the test samples of GSM8K? Is it that the prompts are explicitly encouraging to reason?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mG01iFYFQD", "forum": "Va2tLzb1NX", "replyto": "Va2tLzb1NX", "signatures": ["ICLR.cc/2026/Conference/Submission4558/Reviewer_BQcd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4558/Reviewer_BQcd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917228026, "cdate": 1761917228026, "tmdate": 1762917437861, "mdate": 1762917437861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}