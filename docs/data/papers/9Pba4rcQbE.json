{"id": "9Pba4rcQbE", "number": 17895, "cdate": 1758281707745, "mdate": 1759897147368, "content": {"title": "MoDr: Mixture-of-Depth-Recurrent Transformers for Test-Time Reasoning", "abstract": "Large Language Models have demonstrated superior reasoning capabilities by generating step-by-step reasoning in natural language before deriving the final answer. Recently, Geiping et al. introduced 3.5B-Huginn as an alternative to this paradigm, a depth-recurrent Transformer that increases computational depth per token by reusing a recurrent block in latent space. Despite its performance gains with increasing recurrences, this approach is inadequate for tasks demanding exploration and adaptivity, a limitation arising from its single, chain-like propagation mechanism. To address this, we propose a novel dynamic multi-branches routing approach for Huginn, termed as Mixture-of-Depth-Recurrent (MoDr) Transformer, which enables effective exploration of the solution space by shifting chain-like latent reasoning into a LoRA-based multi-branch dynamic relay mode with a learnable hard-gate routing. Meanwhile, we introduce an auxiliary-loss-free load balancing strategy to mitigate the potential routing collapse. Our empirical results reveal that MoDr achieves average accuracy improvements of +7.2% and +2.48% over the original Huginn model and its fine-tuned variant, respectively, across various mathematical reasoning benchmarks and improvements of +21.21% and +1.52% on commonsense reasoning benchmarks.", "tldr": "Mixture-of-Depth-Recurrent Transformers for Test-Time Reasoning", "keywords": ["large language models", "mixture-of-depth-recurrent transformer", "latent space", "test-time reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c22f73acd75584de9a7e550195c8025af7e2d630.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, authors propose the Mixture-of-Depth-Recurrent Transformer (MoDr), which replaces the single recurrent loop in the Huginn model with multiple LoRA-based branches. To switch between branches authors propose a hard-gated dynamic routing mechanism that adaptively selects the best branch for each reasoning step. This allows the model to explore diverse reasoning paths in latent space efficiently. A loss-free load balancing mechanism ensures that routing remains stable and evenly distributed across branches.\nAuthors test their model on several math and commonsense reasoning tasks and show improvements over vanilla model and LoRA finetuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper in well organized and easy to follow\n2. Authors propose a novel architecture, combining MoE with dynamic routing, LoRA-based adapters, and recurrent nature of the Huginn model. \n3. Authors explore an interesting paradigm for “dynamic latent exploration” during test-time reasoning, bridging the gap between latent reasoning and mixture-of-experts routing.\n4. Authors show improvements on six mathematical and six commonsense reasoning datasets, covering both in-domain and out-of-domain settings."}, "weaknesses": {"value": "1. While overall work is novel, all its components - Depth recurrence, MoE, and auxiliary-loss-free load balancing, - are assembled from existing paradigms rather than introducing a fundamentally new principle\n2. The comparison set is limited mostly to Huginn and LoRA-Huginn models:\n2a. For fair comparison, authors should specify additional compute budget, specifically number of additional parameters, total number of trainable parameters, spent FLOPS, and change in inference time, if any. Since MoDr introduces routing and multiple LoRA branches, empirical runtime and memory cost comparisons with Huginn are essential.\n2b. Simple Chain-of-Thought prompting, majority voting, and full-SFT baseline are missing, while all being popular and cheap techniques widely used in literature and on practice.\n3. The paper only reports Top-1 routing; Top-k routing or soft gating could reveal trade-offs between performance and stability.\n4. The paper’s claim that routing improves reasoning exploration is plausible but largely untested. While the authors include a qualitative case study, no quantitative evidence shows that different branches develop distinct reasoning skills or that routing adapts to task complexity."}, "questions": {"value": "See \"Weaknesses\" section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KwTPPFpbiS", "forum": "9Pba4rcQbE", "replyto": "9Pba4rcQbE", "signatures": ["ICLR.cc/2026/Conference/Submission17895/Reviewer_WUhc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17895/Reviewer_WUhc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761406322635, "cdate": 1761406322635, "tmdate": 1762927718750, "mdate": 1762927718750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mixture-of-Depth-Recurrent (MoDr) Transformer, a novel extension of the depth-recurrent Huginn model. The key idea is to enhance the adaptability and exploration capability of latent-space reasoning by transforming Huginn’s single recurrent reasoning path into a multi-branch, dynamically routed system. Specifically, the authors:\n\nIntroduce multiple LoRA-based recurrent branches that share the Huginn loop weights, providing lightweight diversity in reasoning behavior.\n\nEmploy a hard-gate dynamic router to select the most suitable branch per token based on hidden-state context.\n\nDesign an auxiliary-loss-free load balancing strategy to prevent routing collapse during training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Novel extension of depth-recurrent architectures — The work builds directly on Huginn and introduces a meaningful architectural innovation: turning the single-chain recurrent loop into a mixture-of-recurrent-paths framework.\n\nDynamic routing in latent reasoning — The proposed hard-gate router for token-level branch selection is conceptually elegant and empirically validated, pushing the field toward adaptive latent reasoning rather than verbal CoT.\n\nParameter-efficient design — The use of LoRA branches preserves the efficiency of the original model while expanding reasoning capacity.\n\nComprehensive experiments — Results on both mathematical and commonsense reasoning tasks are convincing, and ablations (router removal, branch count, load balancing) are thorough and insightful.\n\nStrong empirical gains with negligible overhead, making the approach practical for future LLM test-time reasoning research."}, "weaknesses": {"value": "I am not very familiar with the previous work (i.e., Huginn), but I find this paper to be quite intuitive and well-motivated. The Huginn model repeatedly reuses a single core module for latent reasoning, which inherently limits the model’s exploration depth. In contrast, this paper introduces a gated routing mechanism combined with LoRA adapters to merge multiple parallel recurrent branches, thereby enhancing the reasoning capability of LLMs. Overall, this is an elegant and valuable piece of work.\n\nHowever, I still have some critical questions:\n\nThe paper only reports results on Qwen/Qwen2.5-Math-7B-Instruct, which is a relatively small model. Have the authors attempted to test larger LLMs (e.g., Qwen3, Qwen2.5-72B)? Since the proposed approach seems computationally lightweight, restricting the experiments to a single small model weakens the empirical support for the paper’s claims.\n\nThe evaluation benchmark is not sufficiently comprehensive. Some challenging reasoning benchmarks, especially in mathematics and science (e.g., AIME, GPQA, Super-GPQA), are missing, which limits the generality of the results."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q7t3AAbVkP", "forum": "9Pba4rcQbE", "replyto": "9Pba4rcQbE", "signatures": ["ICLR.cc/2026/Conference/Submission17895/Reviewer_kgDC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17895/Reviewer_kgDC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873534073, "cdate": 1761873534073, "tmdate": 1762927717826, "mdate": 1762927717826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MoDr, a Mixture-of-Depth-Recurrent Transformer that improves test-time reasoning by replacing Huginn's single recurrent block with multiple LoRA-based branches and a hard-gate router."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed idea is both novel and compelling. The authors introduce an innovative multi-branch dynamic routing mechanism for recurrent transformers, leveraging LoRA adapters to create lightweight and diverse reasoning branches, all while sharing the backbone weights for efficiency.\n\n2. The method demonstrates strong performance, outperforming baseline models not only on in-domain tasks but also showing robust generalization to out-of-domain benchmarks.\n\n3. The paper presents a comprehensive analysis, including detailed ablation studies, router effectiveness evaluations, branch specialization insights, and load balancing assessments. These provide robust empirical evidence supporting the model’s effectiveness, adaptability, and practical value."}, "weaknesses": {"value": "1. It would be helpful to include more details about the trained model, such as training duration, detailed parameter count, etc.\n\n2. A comparison with a standard Transformer model of similar size would strengthen the evaluation and better highlight the effectiveness of the proposed approach.\n\n3. Including memory usage and FLOPs would provide a clearer picture of the model’s efficiency and practical deployment cost."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AODJbMg4N5", "forum": "9Pba4rcQbE", "replyto": "9Pba4rcQbE", "signatures": ["ICLR.cc/2026/Conference/Submission17895/Reviewer_o7Ko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17895/Reviewer_o7Ko"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990291668, "cdate": 1761990291668, "tmdate": 1762927717365, "mdate": 1762927717365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a modification to the recurrent-depth model Huginn aimed at improving performance on reasoning tasks. Specifically, it introduces MoDr: a set of multiple LoRA parameter groups and a routing mechanism that determines which LoRA parameter set will be active for a given token. After supervised finetuning on reasoning traces, MoDr outperforms the baseline model and the model with a single LoRA parameter set on mathematical and commonsense reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**  The submission shows that the multiple branches of a MoDr model specialize in different areas, and the routing mechanism appropriately sends data to the right specialist to obtain high performance (Table 3). This analysis of MoDr's LoRA+routing approach supports this general direction's potential for impact. Moreover, the performance benefits on reasoning tasks are notable. \n\n**Quality**  The baselines facilitate testing of the proposed approach’s components (e.g., the contribution of the router). The method's generalization to OOD tasks is well established.\n\n**Clarity**  The paper is mostly well written and clearly communicates key points. \n\n**Significance**  The overall idea to equip latent reasoning with different modes that a router can send tokens through is very interesting. As the authors point out, the potential for more efficient reasoning models makes this area of study significant."}, "weaknesses": {"value": "I would be happy to raise my score if the following can be addressed:\n- There might be a weakness regarding the baselines' training durations. Please see \"Questions\" for details.\n\n- Important related work is not discussed in the main text and appears to be missing in the appendix. Clear discussion of prior work that uses MoE/routing + LoRA would help clarify the methodological novelty of the submission. Applying such an approach to Huginn-like models seems new, and the submission should clarify other novel aspects through such discussion.\n\nA minor weakness is that some points are made multiple times (e.g. chain-like reasoning and loss-free balancing are mentioned >5 times each in the first few pages), and more efficient writing could make space for clearer discussion of related work in the main text. Currently, related work is primarily discussed in the appendix."}, "questions": {"value": "- Please find and discuss related works to address the contextualization weakness above. Some examples might include StructMoE or Lottery Ticket Adaptation.\n\n- Figure 1: Could this be modified to illustrate the proposed approach more clearly? In contrast, Figure 2 is very helpful and clear. \n\n- Line 246: The “relay race” analogy is unclear. Could you please clarify what is meant? Modifying Figure 1 could help address this point, too.\n\n- How is the introduced load balancing different from the approach proposed in Wang et al. (2024)?\n\n- Line 316: If the number of training epochs is greater for models with recurrent branches than it is for baseline models, should you have an ablation study that trains baseline models for more epochs? Relatedly, Figure 5 shows that performance improves with the number of branches, but the number of epochs is also changing, so it's unclear how much of the benefit is from branch count (as opposed to epoch count)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E98E1oBRoK", "forum": "9Pba4rcQbE", "replyto": "9Pba4rcQbE", "signatures": ["ICLR.cc/2026/Conference/Submission17895/Reviewer_eRKJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17895/Reviewer_eRKJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153890938, "cdate": 1762153890938, "tmdate": 1762927716911, "mdate": 1762927716911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}