{"id": "IwQz03tJ24", "number": 23477, "cdate": 1758344381474, "mdate": 1759896812648, "content": {"title": "UnifiedVerifier: Unifying Paradigms in Automated LLM Evaluation", "abstract": "The current landscape of Large Language Model (LLM) evaluation is fragmented, with bespoke models for objective verification (e.g., answer\\&process-verify, fact-checking) and subjective judgment (e.g., response quality ranking) operating in isolation. These models are often trained under specific task paradigms or fixed prompts, lacking versatility and failing to accommodate user needs for customizable evaluation criteria, input forms, and output formats. To address these challenges, this paper introduces UnifiedVerifier, an innovative framework designed to achieve comprehensive, general-purpose, and customizable verification capabilities within a single model. The core contributions of UnifiedVerifier are twofold: first, we present Evolutionary Verification Data Synthesis (Evo-Verify), a multi-stage, evolution-inspired automated pipeline that systematically generates a large-scale, high-fidelity training dataset. This dataset spans an extensive array of verification dimensions, intricate judgment criteria, and varied output formats, thereby fostering unprecedented versatility. Second, we propose an alignment technique called \"Core-Anchored Reinforcement Learning\" (CARL), which effectively mitigates the pervasive issue of reward hacking in conventional reinforcement learning by anchoring a majority of the reward signal to verifiable, objective ground truths, ensuring robust and reliable model alignment. Experimental results show that our UnifiedVerifier, trained on a 4-billion-parameter model, not only surpasses its base model across a suite of benchmarks covering both objective and subjective tasks but also outperforms larger thinking models on key objective and subjective verification tasks at only one-tenth the inference cost compare to the base thinking model. This demonstrates that the UnifiedVerifier framework achieves an exceptional balance between generality, performance, and efficiency, offering a new paradigm for building the next generation of LLM evaluation tools.", "tldr": "We introduce UnifiedVerifier, a unified LLM evaluation framework that uses innovative data generation and training methods to achieve comprehensive, customizable evaluation in a single model, outperforming much larger models with greater efficiency.", "keywords": ["LLM", "LLM Evaluation", "LLM-as-a-Judge", "Customizable Evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4a21d58dd659a7506fa09e2704e039634a1382a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UnifiedVerifier, a novel framework designed to address the fragmented and inflexible nature of current LLM evaluation. The primary goal is to unify objective verification and subjective judgment into a single, efficient, and customizable model. The authors propose two core contributions: (1) the Evo-Verify pipeline, an automated and evolutionary method for generating a large-scale, diverse, and complex dataset for training evaluation models, and (2) Core-Anchored Reinforcement Learning (CARL), an alignment technique designed to enhance model reliability by anchoring the reward signal to verifiable ground truths, thereby mitigating reward hacking. Experiments show that a 4B UnifiedVerifier model achieves state-of-the-art performance on both objective and subjective benchmarks, outperforming much larger models with significantly higher inference efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Well-Motivated Problem: The paper addresses a critical and timely problem in LLM research. The current evaluation landscape is indeed fragmented, and the pursuit of a unified, general-purpose, and flexible evaluation model is a valuable and important research direction.\n\n- Innovative Data Generation Framework (Evo-Verify): The Evo-Verify pipeline is a key strength of this work. Instead of merely rewriting instructions, it systematically evolves the \"verification task\" itself, resulting in a high-quality dataset with unprecedented diversity in task types, judgment criteria, and output formats. This provides a solid foundation for training a truly general-purpose evaluator.\n\n- New RL Alignment Technique (CARL): CARL offers a well-reasoned solution to the pervasive issue of reward hacking in RL-based alignment. By anchoring the majority of the reward signal to objective, programmatically verifiable facts, it provides a robust mechanism to ensure the model's alignment is stable and truthful, which is a significant improvement over relying solely on a fallible, learned reward model.\n\n- Strong Empirical Results and High Efficiency: The experimental results are impressive. The fact that a 4B parameter model can outperform models that are orders of magnitude larger is a strong testament to the effectiveness of the proposed framework. Furthermore, the demonstrated gains in inference efficiency make this approach highly practical for real-world applications."}, "weaknesses": {"value": "- Limited Empirical Justification for the CARL Stage: While CARL is theoretically well-motivated to improve reliability, its empirical contribution appears marginal in the main results (e.g., Table 2 shows an accuracy improvement from 89.8% to 90.1%). This small gain raises questions about the necessity of the complex RL stage, especially given the strong performance of the SFT model alone. The paper would be strengthened by including experiments on an adversarial test set designed specifically to induce reward-hacking behaviors, which would better demonstrate CARL's unique value in ensuring model robustness.\n\n\n- Inherent Contradiction in CARL's Scope and the Paper's Goal of Generality: The paper's stated ambition is to create a \"unified\" verifier for both objective and subjective tasks. However, the CARL framework fundamentally depends on the existence of \"verifiable answers\" for its core anchoring mechanism. This makes it highly effective for objective tasks but limits its applicability to purely subjective domains (e.g., evaluating creativity, persuasiveness, or empathy) where no such ground truth exists. This creates a tension between the framework's goal of generality and the narrow scope of its proposed reliability mechanism.\n\n\n- Unclear Process for Ensuring \"Correctness\" in the Evo-Verify Pipeline: The paper explains that the Evo-Verify process generates new, complex verification tasks. However, the method for generating the corresponding \"correct answers\" (i.e., the labels for SFT) is only briefly mentioned. It relies on a powerful teacher model (GPT-OSS-120B) to both generate and self-select the best response. This process hinges on the strong assumption that the teacher model is consistently accurate. The paper would benefit from a more detailed explanation of this critical step and a discussion of the potential for propagating teacher model errors into the training data.\n\n\n- Concerns about the Quality Control of the Initial Seed Data Pool: The methodology starts with 2 million QA pairs from 45 benchmarks. The only filtering step described is based on \"difficulty\" (retaining questions where model pass rates are low), not on \"correctness.\" This approach fails to address potential label noise in the source benchmarks. Critically, this filtering method could inadvertently enrich the dataset with contaminated samples, as questions with incorrect ground-truth answers would likely cause high-performing models to \"fail,\" thus being misclassified as \"difficult\" and retained. The paper lacks a sufficient discussion of the quality of its source data and any steps taken to ensure its cleanliness.\n\n\n- The Evaluation of UnifiedVerifier as a Reward Model is Insufficient and Lacks Critical Baselines: A central claim of the paper is the utility of UnifiedVerifier as a superior reward model. However, the experiment designed to validate this (Table 5) is insufficient as it fails to convincingly demonstrate the added value of its complex, generated rewards.\n\t- Omission of the Most Direct Baseline: For the chosen downstream tasks (AIME and GPQA), both of which have programmatically verifiable answers, the most direct and critical baseline is a simple, rule-based reward model (i.e., a standard RLVR setup where reward=1 for a correct answer and reward=0 for an incorrect one). The paper does not include this comparison. Without it, it is impossible to know if the nuanced, rubric-based rewards generated by UnifiedVerifier offer any real benefit over a simple correctness signal for these objective reasoning tasks. If a basic rule-based reward performs comparably, it would seriously question the necessity of generating complex feedback for this class of problems.\n\t- Failure to Test on Appropriate Benchmarks: Furthermore, the very choice of AIME and GPQA as evaluation benchmarks is misaligned with the paper's broader goal of creating a \"unified\" verifier. These benchmarks do not test the model's advertised strength in handling subjective or multi-faceted tasks where a simple rule-based reward is not feasible. To truly substantiate its \"unified\" claim, the model must be evaluated as a reward model on more appropriate benchmarks, such as Arena-Hard or MT-Bench, which require the nuanced judgment that UnifiedVerifier is supposedly designed to provide. The current experiment leaves the model's core promise in its most unique application area completely unverified."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mm3AsQRE1M", "forum": "IwQz03tJ24", "replyto": "IwQz03tJ24", "signatures": ["ICLR.cc/2026/Conference/Submission23477/Reviewer_tJxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23477/Reviewer_tJxx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554875875, "cdate": 1761554875875, "tmdate": 1762942676401, "mdate": 1762942676401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to address an important problem – how to build more robust verifiers that can generalize to many domains. They train a verifier using their recipe and compare it to existing judge models on two verification benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The trained verifier model appears to be stronger than other larger models and other judge models trained for verification on two verification benchmarks. They compare against several other judge models."}, "weaknesses": {"value": "The writing of the paper is a bit convoluted and difficult to parse clearly. Several details of how their dataset was generated and specifics of their training algorithm feel underspecified to both reproduce the work and clearly understand the contributions. In particular the CARL approach for training the verifier feels quite underdefined. Please see questions for specific notes on where additional clarity is needed. The authors compare against several other judge models but it would be valuable to clearly outline how the training data and algorithm differ from that used for training the other models. For example, is their model stronger because it was trained for longer, with more data, or due to the specific algorithm used? They evaluate their model on different verification benchmarks — it would be good to clearly define what these benchmarks are checking for. A more realistic and convincing evaluation would be to see if using their verifier when RL tuning a model for a specific task leads to better results than training a task-specific critic (e.g. PPO), using a pre-existing judge model, or using no critic model at all (GRPO). The authors mention some initial results around this, but the details of the training setup are lacking. While the premise of the paper is interesting, the work feels like a first-draft and would benefit from significant rewrites for better clarity, better positioning and detailing their approach, and from more realistic evaluations (as mentioned above)."}, "questions": {"value": "- starting quotes are reversed throughout, please fix\n- section 1 line 105: which base model was used? which 120B param model are you comparing against? This is mentioned later in the text, but useful to specify it early on.\n- line 162-167: the language here can be simplified – it seems that the main difference is that Evol-Instruct generates a broad set of instructions whereas Evo-Verify generates verification related instructions?\n- line 180: If out of distribution generalization is a problem for verifiers, wouldn’t it be better to build task-specific verifiers or co-train task-specific verifiers w/ generators?\n- line 216: what domains are these benchmarks covering? what is their format – MCQ, short answer, long-form reasoning? If possible, explicitly listing out the benchmarks would be best. The format would determine how verification would take place.\n- line 220: please mention which LLMs are used for this process.\n- line 243: is the filtering done using a different LLM?\n- line 245: were there existing answer choices in the selected questions? are those discarded and replaced by model generated answers?\n- line 246: is the best option selected by comparing against the ground truth answer? perhaps the same model should not be used for answer generation & selection as it may be biased toward particular answer choices.\n- line 246: what is the final format of the training instances – is there a separate instance to very each generated answer choice for each question?\n- line 261: Are these 30,000 questions a subset of the 200,000 instances generated in the previous section? How was the subset selected?\n- section 4.2, line 264: The motivation for CARL is unclear. If the goal is to train a verifier, I assume the data is pairs of (question, candidate response) or (question, candidate response, ground truth) and the model’s output would be some CoT + a correct or not assessment for the candidate response? If this is the case, then what do you mean by “forcing the model to generate the verifiable response without seeing the solution”? Please describe the motivation and data inputs and expected model outputs more clearly.\n- section 4.3: Currently these rewards are not well motivated as the goal of training with CARL is not clearly explained. Is CARL simply GRPO with specific rewards?\n- line 313: How is the data / training strategy of these judge models different from what is used for UnifiedVerifier?\n- line 358: What is the process of training UnifiedVerifier-4B w/o CARL? Is this simply an SFT with the generated data?\n- line 429: What algorithm did you use to tune Qwen 2.5 7B with the UnifiedVerifier? Please specify more details of the training setup. How does this compare to PPO where a critic is trained in conjunction with the policy, or vanilla GRPO where there is no critic model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "erJgdHtPdM", "forum": "IwQz03tJ24", "replyto": "IwQz03tJ24", "signatures": ["ICLR.cc/2026/Conference/Submission23477/Reviewer_Ymp7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23477/Reviewer_Ymp7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962194766, "cdate": 1761962194766, "tmdate": 1762942676150, "mdate": 1762942676150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UnifiedVerifier, an innovative framework aimed at providing comprehensive, general-purpose, and customizable verification capabilities within a 4B model. The authors introduce an automated data generation methodology based on the Evol-Instruct method, which systematically synthesizes the \"verification task\" across various formats and difficulty levels. To enhance the reward model, they propose a Core-Anchored Reinforcement Learning approach, optimized with multi-dimensional signals such as objectiveness, quality, and format. While this study holds significant value for the community, there are several areas for improvement regarding the methodology and experimental design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe Evo-Verify method is a promising approach for automatically generating training data, which enhances the development of general and controllable reward models.\n2.\tThe adaptation of a reinforcement learning algorithm for verification tasks yields positive results, indicating potential for further exploration."}, "weaknesses": {"value": "1.\t**Reward Hacking Concerns**: The authors emphasize the challenge of \"reward hacking\" multiple times. Are there empirical results demonstrating UnifiedVerifier's effectiveness in mitigating this issue?\n2.\tIn lines 227-229, the authors reference tasks that fall under type 3, which retain only the question. Could they provide specific examples or clarify this classification?\n3.\t**Filtering Process Details**tering process based on three axes: question difficulty, verification complexity, and instruction strictness. What exactly does this filtering entail? Additionally, why is there a dual filtering for question difficulty? How is the quality of GPT-OSS assessed in selecting the best response?\n4.\tThe relationship between the 30,000 challenging SFT examples (line 261) and the 200,000 high-quality instances for supervised fine-tuning (line 244) needs clarification. How do these datasets interact?\n5.\t**Evaluation Datasets**: Why did the authors choose not to utilize the new RewardBench2 dataset for evaluation?\n6.\t**Baseline Comparison**: For a fairer evaluation, the baseline models should be changed to Qwen3-4B as well. This would allow for a more accurate assessment of the effectiveness of Evo-Verify and CARL.\n7.\tIn line 372, the authors assert UnifiedVerifier's superior \"fine-grained\" verification capability for challenging problems. What evidence supports this claim?"}, "questions": {"value": "Please refer to the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6pvo90qjFx", "forum": "IwQz03tJ24", "replyto": "IwQz03tJ24", "signatures": ["ICLR.cc/2026/Conference/Submission23477/Reviewer_t5we"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23477/Reviewer_t5we"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984076025, "cdate": 1761984076025, "tmdate": 1762942675933, "mdate": 1762942675933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}