{"id": "YoEFNqMNmG", "number": 13222, "cdate": 1758215257473, "mdate": 1759897455031, "content": {"title": "Advancing the understanding of fixed point iterations in deep neural networks: a detailed analytical study", "abstract": "Recent empirical studies have identified fixed point iteration phenomena in deep neural networks, where the hidden state tends to stabilize after several layers, showing minimal change in subsequent layers. This observation has spurred the development of practical methodologies, such as accelerating inference by bypassing certain layers once the hidden state stabilizes, selectively fine-tuning layers to modify the iteration process, and implementing loops of specific layers to maintain fixed point iterations. Despite these advancements, the understanding of fixed point iterations remains superficial, particularly in high-dimensional spaces, due to the inadequacy of current analytical tools. In this study, we conduct a detailed analysis of fixed point iterations in a vector-valued function modeled by neural networks. We establish a sufficient condition for the existence of multiple fixed points in looped neural networks with varying input regions. Additionally, we expand our examination to include a robust version of fixed point iterations. To demonstrate the effectiveness and insights provided by our approach, we provide case studies that in looped neural networks, there may exist $2^d$ number of robust fixed points under exponentiation or polynomial activation functions, where $d$ is the feature dimension. Furthermore, our preliminary empirical results support our theoretical findings. Our methodology enriches the toolkit available for analyzing fixed-point iterations of loop neural networks and may enhance our comprehension of neural network mechanisms.", "tldr": "", "keywords": ["Looped Neural Networks", "Fixed Point Iterations"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d3d2e5a22832f1c7118decc9f398d01a47d0330.pdf", "supplementary_material": "/attachment/50ee6879f00762bb869a8cad33fa1cfe27da497d.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes the fixed points of \"looped neural networks,\" which are feed-forward networks where weights are shared across layers.  Looped neural networks can be viewed as a type of recurrent neural network whose hidden state at t=0 is initialized to the input data, and whose final hidden state at t=T is treated as the output prediction, without any separate connections for sequential input/output. This paper is motivated by recent looped neural network architectures where the shared layer is a transformer block, and various useful computations are performed by iterating the block until convergence to a fixed point.\n\nThe paper focuses on a simpler and more classical looped neural network architecture, where the shared layer is a linear transformation with Lipschitz constant less than 1, followed by an element-wise non-linear activation function.  The paper proves that two specific instances of this architecture - one whose activation function is a polynomial, another whose activation is a polynomial composed with an exponential - can have a number of fixed points that is exponential in the size of the hidden layer.  They also prove that convergence to a fixed point is relatively stable under small perturbations of the activation vector at each time-step.  Some small-scale simulation experiments corroborate the theoretical results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This paper focuses on an interesting, (re-)emerging area of recurrent neural networks (in this case, looped transformers) and analysis of their fixed points.  Improving our theoretical understanding of RNN fixed points may be helpful in their mechanistic interpretation and design.\n\n- The proofs appear to be sound and are supported empirically, which is a plus.\n\n- Examining the case of convergence under noise is interesting and relevant to adversarial robustness.\n\n- Complete proofs and experimental code are included in appendix and supplementary material, which promotes reproducibility."}, "weaknesses": {"value": "- Experimental results are very limited - just a single paragraph and some figures - and only applies to synthetic data.  Including real-world tasks and data would be more compelling and relevant to the looped neural network research community.\n\n- I question the significance of the theoretical results, for the following reasons:\n    - The results only show that many fixed points exist; it is unclear exactly if/how this would inform neural network design.\n    - The results only apply to a classical type of layer, namely linear composed with element-wise activation function.  Fixed points of this layer have already been well-studied (e.g. [1,2,3,4] below), and they are not relevant to the looped transformer networks that are cited as motivation for this paper.\n    - The activation functions considered (exponential of/polynomial) are not common in practice. In fact, similar results hold for a more common activation function (hyperbolic tanh) when the weight matrix is an identity matrix scaled by a number larger than 1; such an example might make for a more relevant case study.\n    - Theorem 4.1 seems to be a direct application of Banach's fixed point theorem, limiting its novelty. \n\n- There is some imprecise word choice that made it difficult for me to understand exactly what was being claimed:\n    - In 3.2, \"we first introduce the fixed point iteration problem\" is followed by the definition of a fixed point, not the definition of an iterative process\n    - The \"noise term\" h(x) in Theorem 4.2 appears to be a deterministic function of x.\n    - Theorem 5.1 describes W and b as \"noisy parameters\" but they do not appear to be random variables.\n\n[1] Katz GE, Reggia JA. Using directional fibers to locate fixed points of recurrent neural networks. IEEE transactions on neural networks and learning systems. 2017 Aug 24;29(8):3636-46.\n\n[2] Eisenmann L, Monfared Z, Göring N, Durstewitz D. Bifurcations and loss jumps in RNN training. Advances in Neural Information Processing Systems. 2023 Dec 15;36:70511-47.\n\n[3] Sussillo D, Barak O. Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks. Neural computation. 2013 Mar 1;25(3):626-49.\n\n[4] Golub MD, Sussillo D. FixedPointFinder: A Tensorflow toolbox for identifying and characterizing fixed points in recurrent neural networks. Journal of open source software. 2018 Nov 1;3(31):1003."}, "questions": {"value": "- Some theorems involve unexplained constants, such as 0.95 and 20 in Theorem 4.2 (likewise in Theorem 5.1).  Are these constants essential, or would similar results hold for other values of these constants?\n- Theorem 5.1 says K_i in [0,.9) but layer states K_j = 0.92.  Is this a contradiction, or do these variables denote different quantities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tvs0mBDyxS", "forum": "YoEFNqMNmG", "replyto": "YoEFNqMNmG", "signatures": ["ICLR.cc/2026/Conference/Submission13222/Reviewer_53pi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13222/Reviewer_53pi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761323937208, "cdate": 1761323937208, "tmdate": 1762923911910, "mdate": 1762923911910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submitted work studies fixed points in a special type of neural networks called looped networks. Looped networks are neural networks but with the same weights in every layer. The motivation here is to understand how repeated application of a layer can result in a fixed point: $f(x) = x$ (simplified for intuition). The authors provide conditions under which looped networks exhibit fixed points (the activations do not change in subsequent layers). They then extend their analysis to the case of noise perturbations at each layer and other specific cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The authors introduce a novel mathematical analysis for analyzing fixed points in Looped networks\n* The paper is clearly written and provides intuitions behind the main results using simple examples (Figure 1 and 2)"}, "weaknesses": {"value": "**Practical implications**: While the authors give examples of relevance of fixed point iterations, such as\n* Adjacent layers may perform identical operations\n* Skipping layers during inference,\n\nthey are not directly related to the fixed point analysis performed in this work. I would request the authors to help me understand the connection better.\n\n**No Real Network Experiments**: The experiments are limited to toy synthetic setups and its unclear if they have any real-world implications."}, "questions": {"value": "* Can you comment on how strong is the derivative assumption in Theorem 4.1? I am unsure if its expected to holds generally. \n* Do trained neural networks exhibit fixed points? Can we take individual layers of a LLM and study the fixed points? I believe the authors can study the recurrent blocks of recurrent Transformers [2]\n* Is there a way to use the fixed point analysis to skip layers during inference?\n* Can the authors suggest improvements in existing networks to avoid fixed point collapse?\n\n\n[1] Exponential expressivity in deep neural networks through transient chaos, 2016\n\n[2] Scaling up test-time compute with latent reasoning: A recurrent depth approach, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CLh5yS3sxR", "forum": "YoEFNqMNmG", "replyto": "YoEFNqMNmG", "signatures": ["ICLR.cc/2026/Conference/Submission13222/Reviewer_cLiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13222/Reviewer_cLiS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419826901, "cdate": 1761419826901, "tmdate": 1762923911518, "mdate": 1762923911518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies fixed-point iterations in looped neural networks, establishing a sufficient condition for the existence of multiple fixed points (Theorem 4.1) and their variant under noise perturbations (Theorem 4.2). The authors also present illustrative case studies using polynomial and exponential activation functions, and simple simulations validating their theory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and well structured. It offers a solid theoretical contribution by generalizing Banach fixed-point analysis to multiple fixed points and robustness under noise. The related work section provides a coherent overview and motivation for studying fixed points in neural networks."}, "weaknesses": {"value": "The empirical section relies on toy examples with artificial activation functions. It would be much stronger to include experiments with more practical architectures or common nonlinearities (e.g., ReLU) to show broader applicability. The discussion of the potential implications for real-world or large-scale networks remains somewhat abstract."}, "questions": {"value": "I can see from the paper that fixed-point analysis is an emerging subfield, but being non-expert in this subfield, I personally feel it is still a niche direction in deep learning theory. Could the authors clarify what concrete algorithmic benefits or new analytical tools their framework could bring for mainstream architectures (e.g., recurrent, equilibrium, or transformer models)? More discussion on why multiple fixed points matter for understanding or improving deep models would make the paper’s impact clearer to a broader audience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dwtz5GUarM", "forum": "YoEFNqMNmG", "replyto": "YoEFNqMNmG", "signatures": ["ICLR.cc/2026/Conference/Submission13222/Reviewer_3H88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13222/Reviewer_3H88"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851622889, "cdate": 1761851622889, "tmdate": 1762923911011, "mdate": 1762923911011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of fixed-point iterations in looped neural networks (LNNs) — architectures where the same layer (with shared weights) is repeatedly applied. The study formalizes conditions for the existence and robustness of multiple fixed points and relates these to practical phenomena observed in deep or residual-like networks.\n\nCore contributions:\n\nGeneral Theorem (Theorem 4.1):\nEstablishes sufficient conditions for multiple fixed points in LNNs based on Banach’s fixed-point theorem, via contractivity within disjoint input domains D_i.\n\nRobust Fixed-Point Theorem (Theorem 4.2):\nExtends Banach’s theorem to include perturbations/noise, modeling the effect of residual connections. Provides quantitative error bounds showing that iteration remains convergent if |f (x) | < 0.95 even with additive noise up to 1/m.\n\nThe paper links looped neural dynamics to fixed-point iteration theory, establishing mathematical foundations for phenomena observed in Deep Equilibrium Models (DEQ) and looped Transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear mathematical framing:\nThe use of Banach’s contraction principle and its noisy variant gives a solid analytical grounding for convergence of LNNs.\nProofs are rigorous and fully detailed in Appendix A–C.\n\nNovel sufficient conditions:\nTheorem 4.1’s condition using disjoint contractive regions is an elegant way to guarantee multiple attractors in high-dimensional networks—extending traditional single-fixed-point analyses.\n\nRobustness analysis (Theorem 4.2):\nModelling small perturbations as noise provides a bridge between theoretical analysis and residual or perturbed networks, which is practically meaningful.\n\nComprehensive case studies:\nDemonstrating 2^d fixed points under polynomial/exponential activations is conceptually neat and mathematically clean. Figure 3’s experiment (p. 8) shows convergence of 1024 trajectories to distinct attractors, visually confirming the results."}, "weaknesses": {"value": "Restrictive examples:\nThe polynomial and exponential activations are synthetic, designed specifically to satisfy the theorems. It remains unclear whether common activations (ReLU, tanh, GELU) exhibit similar multi-fixed-point structures.\n\nGenerality of disjoint-region assumption:\nTheorem 4.1 requires explicit partitioning of the input space into disjoint D_i where contractivity holds. For most realistic networks, such partitioning with constants K_i < 1 are hard to verify or may fail.\n\nExponential-growth interpretation:\nWhile the existence of 2^d fixed points is mathematically correct for specially chosen g (x) in practical networks such exponentially many stable equilibria may not exist; the relevance to more realistic networks is not discussed."}, "questions": {"value": "1) Can you provide examples using standard activations (ReLU, tanh) where contractivity and multiple fixed points approximately hold?\n\n2) How do your sufficient conditions relate to the Jacobian spectral radius used in DEQ stability analyses?\n\n3) The theorems are proved for constant weights. Can you comment on how these results could be lifted to weights if they were updated across iterations?\n\n4) Does the 2^d fixed-point structure improves performance or learning efficiency in LNNs?\n\n5) How could these results be lifted to DEQ analysis in Looped Transformers as suggested in the tex?\n\nThe reason why I did not put a higher score is because it's not clear to me what is the impact of these results so if you could provide arguments towards this, I would be willing to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MICMEihikG", "forum": "YoEFNqMNmG", "replyto": "YoEFNqMNmG", "signatures": ["ICLR.cc/2026/Conference/Submission13222/Reviewer_DZqY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13222/Reviewer_DZqY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954268697, "cdate": 1761954268697, "tmdate": 1762923910600, "mdate": 1762923910600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}