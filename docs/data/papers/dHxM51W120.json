{"id": "dHxM51W120", "number": 16128, "cdate": 1758260347852, "mdate": 1763689368809, "content": {"title": "Towards Simple and Provable Parameter-Free Adaptive Gradient Methods", "abstract": "Optimization algorithms such as AdaGrad and Adam have significantly advanced the training of deep models by dynamically adjusting the learning rate during the optimization process. However, ad-hoc tuning of learning rates poses a challenge, leading to inefficiencies in practice. To address this issue, recent research has focused on developing ``parameter-free'' algorithms that operate effectively without the need for learning rate tuning. Despite these efforts, existing parameter-free variants of AdaGrad and Adam tend to be overly complex and/or lack formal convergence guarantees. In this paper, we present AdaGrad++ and Adam++, novel and simple parameter-free variants of AdaGrad and Adam with convergence guarantees. We prove that AdaGrad++ achieves comparable convergence rates to AdaGrad in convex optimization without predefined learning rate assumptions. Similarly, Adam++ matches the convergence rate of Adam without relying on any conditions on the learning rates. Experimental results across various deep learning tasks validate the competitive performance of Adam++.", "tldr": "", "keywords": ["Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65d2283c5ef45b4c0d8ecb180aae521fc1648feb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel parameter-free adaptive gradient method, scaling the distance over gradient by $1/\\sqrt{d}$ for AdaGrad++ and by $1/\\sqrt{d(t+1)}$ for Adam++. Theoretical convergence results, along with empirical studies, are presented."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea is simple.\n\n2. Theoretical convergence results, with generalized assumptions, are presented."}, "weaknesses": {"value": "1. The paper is easy to follow since the idea is simple. However, the writing is rough and lacks careful checking and polishing.\n\n2. It should be explained why $\\eta_t$ should involve a max operation to ensure its nondecreasing.\n\n3. Though it is a common issue, this method still requires a base learning rate, which needs to be tuned.\n\n4. The results in Figures 1 and 2 indicate that the improvement in test accuracy achieved by Adam++ primarily stems from its reduced overfitting -- the baseline models exhibit overfitting; the level of overfitting and underfitting can be adjusted by modifying the (base) learning rate; this makes the results less convincing."}, "questions": {"value": "1. Why is a nondecreasing $\\eta_t$ preferred?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G9rVwLsjVH", "forum": "dHxM51W120", "replyto": "dHxM51W120", "signatures": ["ICLR.cc/2026/Conference/Submission16128/Reviewer_6s59"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16128/Reviewer_6s59"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747384788, "cdate": 1761747384788, "tmdate": 1762926299104, "mdate": 1762926299104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes new optimizers named AdaGrad++ and Adam++, which are parameter-free variants of AdaGrad and Adam, respectively.\nThey can be implemented with a small modification to AdaGrad and Adam, whereas existing parameter-free methods require complicated implementations.\nThe authors also provide theoretical proofs that ensure the convergence of their optimizers in the convex optimization setting.\nIn the experiments, they show that Adam++ can achieve competitive performance with Adam without the need of learning rate tuning in image classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Their proposed optimizers can be implemented very easily by slightly modifying the original AdaGrad and Adam.\n- Presentation of the paper is clear and easy to read.\n- Although existing parameter-free optimizers tend to underperform well-tuned Adam, Adam++ seems to work well, showing competitive performance with the original Adam without learning-rate tuning.\n- As far as I checked, there were no mathematical flaws in the proofs of their theorems."}, "weaknesses": {"value": "### Experimental settings are limited to image classification\n\nI would like to see results in other settings, such as language modeling, reinforcement learning, generative modeling (e.g., diffusion models), etc.\n\n### Lack of Theoretical Results in Non-convex settings\n\nTheoretical analysis is only provided for convex settings, but there are a lot of convergence analysis of Adam/AdaGrad in smooth non-convex settings.\nThese optimizers are mainly used for deep learning models, whose objective functions are highly non-convex, so it is desirable that the analysis in non-convex settings is included (see [1, 2, 3] for example).\n\n### Gap between theory and practice\n\nIn the derivation of Adam++, the authors use AMSGrad's update rule to ensure the theoretical convergence, but they omit it and use a simplified version in the practical implementation, which degrades the soundness of theoretical results.\n\n### Additional memory costs\n\nIn the proposed method, the initial parameter $mathbf{x}_0$ has to be stored to calculate $r_t$, which requires additional memory in addition to the other optimizer states (e.g., $\\mathbf{m}_t$ and $\\mathbf{v}_t$ in Adam).\nThis can be a disadvantage in the era of large-scale models.\n\n### Minor comments\n\n- The authors should cite [4] in Section 3, which also proposes a method to guarantee the convergence of Adam with a small modification.\n- The authors do not include \"LLM Usage\" statements in the paper, which is mandatory as described in the official guideline.\n\n[1] Chen, Xiangyi, et al. \"On the convergence of a class of adam-type algorithms for non-convex optimization.\" arXiv preprint arXiv:1808.02941 (2018).\n\n[2] DÃ©fossez, Alexandre, et al. \"A simple convergence proof of adam and adagrad.\" arXiv preprint arXiv:2003.02395 (2020).\n\n[3] Zhang, Yushun, et al. \"Adam can converge without any modification on update rules.\" Advances in neural information processing systems 35 (2022): 28386-28399.\n\n[4] Taniguchi, Shohei, et al. \"ADOPT: Modified Adam Can Converge with Any $\\beta_2 $ with the Optimal Rate.\" Advances in Neural Information Processing Systems 37 (2024): 72438-72474."}, "questions": {"value": "- In my understanding, Case 1 of Adam++ is identical to AdaGrad++. Is this correct? If not, what is the difference between them?\n- The authors use AMSGrad modification to ensure the convergence of Adam++, but AMSGrad tends to slow down the convergence in practice. How about using the technique of ADOPT [4], which can also ensure the theoretical convergence by slightly modifying the original Adam. To my knowledge, ADOPT works better than AMSGrad and Adam in many cases, so it may help bridge the gap between theory and practice.\n\n[4] Taniguchi, Shohei, et al. \"ADOPT: Modified Adam Can Converge with Any $\\beta_2 $ with the Optimal Rate.\" Advances in Neural Information Processing Systems 37 (2024): 72438-72474."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QpeU0eLQSa", "forum": "dHxM51W120", "replyto": "dHxM51W120", "signatures": ["ICLR.cc/2026/Conference/Submission16128/Reviewer_hSug"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16128/Reviewer_hSug"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815887856, "cdate": 1761815887856, "tmdate": 1762926298740, "mdate": 1762926298740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two \"parameter-free\" optimization algorithms named AdaGrad++ and Adam++.  Parameter-free here means that one does not need to sweat the implicit or explicit parameters of the algorithm because the convergence is robust to their values. AdaGrad++ can be seen as a slight extension of the algorithm of Ivgi. Adam++ adapts Ivgi's idea to the Adam algorithm.  \n\nThe authors provide convergence proofs for both algorithms. Although providing such proof is better than what certain competing algorithms can offer, these proofs give very little insight about the parameter-free nature of the algorithms, that is, about of the robustness of their performance across the parameter range (unlike Ward's paper, for instance).  \n\nInstead one has to rely on empirical evidence to convince oneself that parameter searching is a thing of the past.  I would have liked experiments showing how robust the result when one multiplies the suggested learning rates by some constants. Nevertheless, the experiments are insightful because they show the evolution of both the training and testing costs for diverse algorithms, making clear that this is not just a matter of optimization, but also of implicit regularization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- clear statement and valuable proofs\n- generally good empirical results"}, "weaknesses": {"value": "- no empirical assessment of the robustness of the proposed algorithms against parameter changes (or rescalings).\n- proofs say very little about the parameter free nature of the proposed algorithms (too hard maybe?).\n- no clear discussion of the optimization effects vs the implicit regularization effects.\n\n- (minor) lots of distracting typos \"entry-wisely\" \"AdaGard\""}, "questions": {"value": "- did you run experiments to assess the robustness of the proposed algorithms against parameter changes (or rescalings)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l0po6iEuob", "forum": "dHxM51W120", "replyto": "dHxM51W120", "signatures": ["ICLR.cc/2026/Conference/Submission16128/Reviewer_66vf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16128/Reviewer_66vf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848541691, "cdate": 1761848541691, "tmdate": 1762926298360, "mdate": 1762926298360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission presents a combination of parameter-free methods based on DOG (small dependence on initial step-size) and adaptive methods like AdaGrad and Adam (the AMSGrad variant) to obtain new variants, AdaGrad++ and Adam++.\nThe submission provides convergence rates for the proposed methods and shows that they perform similarly to Adam on standard benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The problem of finding optimizers that work out of the box and do not require tuning is important and relevant to the ICLR community. The resulting algorithm is indeed simple, and the presentation of the algorithms is clear."}, "weaknesses": {"value": "My main issues with the submissions \nare that the theoretical guarantees appear weak, \nand that the motivation for the proposed method is unclear.\n\n**Theoretical guarantees are weak,**\nor I have a very hard time understanding them.\nThe submission claims that the rates in Thm 4.2 and 5.1 are at the worst-case sublinear rate because $\\vert s \\vert$ grows in $\\sqrt{T}$ in the worst case, \nbut this seems to ignore the dependence on other quantities that depend on $t$, \nsuch as the maximum distance to the optimum $D$\nand the step-size $\\eta_T$, as the step-size is set as a function of $x_t - x_0$.\nThe submission does not discuss what happens to those quantities, \nand as a result I do not see how the rates can be interpreted as the claimed $1/\\sqrt{T}$ rates.\n\n**Improvement over prior work is unclear.**\nThere is a large body of work on parameter-free, adaptive optimizers and their combination, and it is not clear to me what the main advantages of the proposed method are. \nThe related work section cites many papers, \nbut does not build a compelling case for a hole in the literature that the submission fills. \nThe combination of D-Adaptation and AdaGrad already provided a parameter-free with coordinate-wise adaptive method, and the submission does not make a strong case for why the proposed AdaGrad++ is better.\nAlthough this might be partially due to the \nfact that I do not understand the theoretical guarantees.\n\n\nMinor issues (no response required):\n\n- The font in the figures is very small and not readable.\n- The figure showing the sensitivity to initial step-size for Adam++ in Figure 5 is not very informative without a comparison with plain Adam \n- Please fix the references. \n  Many are duplicated, refer to arxiv versions instead of published versions, don't have years,"}, "questions": {"value": "Why are the theoretical guarantees valid convergence rates that give a $O(1/\\sqrt{T})$ rate? \n\nHow do those rates compare to the actual rates on problems of interest, say convex Lipschitz? What do we loose from not having to specifiy $G$ and $D$ in advance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IJESuWiuK1", "forum": "dHxM51W120", "replyto": "dHxM51W120", "signatures": ["ICLR.cc/2026/Conference/Submission16128/Reviewer_J4sP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16128/Reviewer_J4sP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762557099793, "cdate": 1762557099793, "tmdate": 1762926297928, "mdate": 1762926297928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers and Area Chairs"}, "comment": {"value": "We sincerely thank the reviewers for their careful reading of our work and their detailed, constructive comments. In this revision, we have thoroughly proofread the manuscript to correct typos, fix the references (Q5 of Reviewer J4sP), and have enlarged the figure legends for better readability (Q4 of Reviewer J4sP). Furthermore, we have expanded the explanations of our proposed theories, conducted experiments on hyperparameter robustness, investigated Case 2 of Adam++ in greater depth, and analyzed the optimization and implicit regularization effects. All new materials are marked in blue within the revised paper.\n\n### **1. Theoretical Clarifications**\n\nWe have enhanced the discussion regarding convergence rates in Section 4.1 and Section 5.2 (Q1 of Reviewer J4sP). Additionally, we clarified the distinctions between the AMSGrad-style and non-AMSGrad-style variants of Adam++ Case 2 in Section 5.1 and Appendix H.2.3 (Q3 of Reviewer hSug). Finally, we provided additional explanatory details for the proofs located in Appendices B, C, and D (Q2 of Reviewer 66vf).\n\n### **2. Extended Experimental Evaluation**\n\nWe implemented additional ablation studies to empirically validate the optimization and implicit regularization effects in Appendix H.2.2 (Q3 of Reviewer 66vf), as well as experiments comparing Case 2 of Adam++ with and without the AMSGrad-style mechanism in Appendix H.2.3 (Q3 of Reviewer hSug).\n\nTo further illustrate the robustness of our proposed method, we added baseline figures for AdamW across different step sizes in Figure 7 (Q4 of Reviewer J4sP and Q1 of Reviewer 66vf).\n\nAdditionally, we relocated the language model experiments from Appendix H to Section 6.2 to present a comprehensive empirical validation of the proposed optimizers (Q1 of Reviewer hSug).\n\n### **3. Reference Updates**\n\nWe have added ADOPT (Taniguchi et al., 2024) to the Related Work section (Q5 of Reviewer hSug).\n\nWe believe these revisions address the reviewers' main concerns and provide a clearer demonstration of the strengths of our proposed methods. Please let us know if you have any further questions or suggestions."}}, "id": "bE7w7cIXeT", "forum": "dHxM51W120", "replyto": "dHxM51W120", "signatures": ["ICLR.cc/2026/Conference/Submission16128/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16128/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission16128/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763763997537, "cdate": 1763763997537, "tmdate": 1763763997537, "mdate": 1763763997537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}