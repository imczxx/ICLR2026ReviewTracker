{"id": "TT3gmYaqyc", "number": 5187, "cdate": 1757862905720, "mdate": 1759897989939, "content": {"title": "CameraNoise: Learning Precise Camera Control with Video Diffusion in Noise Space", "abstract": "Controlling camera pose in video diffusion models is essential for generating realistic videos, yet existing approaches struggle to achieve precise control. Methods that directly inject numerical camera parameters into the diffusion backbone often fail to capture subtle viewpoint variations and lead to structural distortions or visual artifacts. To overcome these limitations, we propose CameraNoise, a temporally coherent stochastic representation warped from camera intrinsic and extrinsic parameters. Unlike conventional approaches, CameraNoise embeds camera poses directly into the noise space. This makes our approach independent of scene appearance while faithfully encoding camera motion. Specifically, we introduce a novel Geometry-guided Reprojection Flow along with a CameraNoise warping algorithm, which jointly preserves the Gaussian prior of diffusion and ensures consistent noise propagation under camera transformations. By integrating CameraNoise into the diffusion process, our framework delivers stable and high-quality videos with precise camera control. Extensive experiments on the RealEstate10K benchmark demonstrate that our approach significantly outperforms prior methods in both fidelity and controllability.", "tldr": "We propose CameraNoise, a stochastic noise-space representation derived from camera parameters that preserves the Gaussian prior and enables temporally coherent, appearance-independent, and camera-controllable video generation with diffusion model.", "keywords": ["Diffusion Model", "Video Generation", "CameraNoise"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc33672edf48a4c4d1fdfb586705ec8728754df4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CameraNoise, a method for camera-controlled video diffusion. CameraNoise embeds camera poses directly into the diffusion noise instead of using a conditioning signal within the model blocks, e.g., based on Plucker coordinates. For this, the paper proposes Geometry-guided Reprojection Flow (GRFlow), an alternative to using optical flow for noise warping, where the main motivation is to disentangle camera motion from visual content."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Disentangled camera motion and visual content: Constructing warped noise where the camera motion is disentangled from the visual content is sound and a good idea compared to previous optical flow-based methods."}, "weaknesses": {"value": "- Missing comparisons: Previous works such as Go-with-the-Flow [1] also use noise warping for camera-controlled video modelling. However, there are no comparisons with that line of work. Moreover, there is no comparison with GEN3C [2], a method that uses depth-based warping for 3D inductive bias.\n- Lack of video results: The supplementary website is good but it is missing some more videos. First, it would be great to show generalization beyond RealEstate10K. So ideally the paper can show multiple OOD videos with the same trajectory to show that the model is stable across scenes and consistent w.r.t. camera control precision. Moreover, side-by-side comparisons with baselines would be good. Lastly, it would be great to show if noise from an input video can be transferred to other video generations.\n\n[1] Burgert et al., Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise, CVPR 2025 \\\n[2] Ren et al., GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control, CVPR 2025"}, "questions": {"value": "The paper seems sounds and I like the method. However, there is some lack of comparisons and visual results.\nI would like authors to address following questions:\n- How does the method compare to recent methods such as Go-with-the-Flow or GEN3C?\n- How about some additional visual results such as results on scenes with dynamic objects, and comparisons with previous works?\n\nI currently rate the work below the acceptance threshold but would be happy to consider my rating depending on the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NFYfuXCeC5", "forum": "TT3gmYaqyc", "replyto": "TT3gmYaqyc", "signatures": ["ICLR.cc/2026/Conference/Submission5187/Reviewer_c7T5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5187/Reviewer_c7T5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760653567555, "cdate": 1760653567555, "tmdate": 1762917934797, "mdate": 1762917934797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CameraNoise, a novel approach for camera-controllable video generation that embeds camera pose information directly into the noise space of diffusion models. The key contributions include: (1) a Geometry-guided Reprojection Flow (GRFlow) that captures camera motion independently of scene appearance, (2) a PDE-based warping algorithm that preserves Gaussian priors while propagating temporal correlations, and (3) a dynamic scaling training strategy to improve robustness. Experiments on RealEstate10K demonstrate improvements in both generation quality and camera control precision compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel formulation and theoretical motivation: Embedding camera control in noise space is a conceptually clean departure from feature injection methods, with theoretical motivation and corresponding formulations.\n2. Appearance-agnostic design: GRFlow successfully decouples camera motion from scene appearance (Figure 3), addressing a key limitation of optical flow-based approaches.\n3. Comprehensive validation: Ablation studies (Tables 2-4) systematically justify design choices, and the method shows consistent improvements across multiple metrics."}, "weaknesses": {"value": "1. Single-dataset evaluation: All experiments use only RealEstate10K (indoor scenes). Performance on diverse scenarios (outdoor, dynamic objects, varying depth) is unknown, limiting generalizability claims.\n2. Missing computational analysis: Total training/inference time and overhead from PDE solving are not reported, making practical feasibility unclear compared to baselines."}, "questions": {"value": "1. Failure cases. Under what conditions does the method fail? Can you provide examples where CameraNoise does not improve over baselines? \n2. Scale to longer videos. Table entries show 49-frame videos. How does temporal coherence degrade for longer sequences (100+ frames)?\n3. Performance on dynamic scenes. The current evaluation focuses on static scenes where pixel displacement is primarily caused by camera motion. How does the method handle videos with significant object motion (e.g., people walking, cars moving)? Can you provide more generated samples under such challenging scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fr8TfZtXjS", "forum": "TT3gmYaqyc", "replyto": "TT3gmYaqyc", "signatures": ["ICLR.cc/2026/Conference/Submission5187/Reviewer_GXRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5187/Reviewer_GXRP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879766567, "cdate": 1761879766567, "tmdate": 1762917934517, "mdate": 1762917934517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CameraNoise, a way to encode camera pose control directly in the noise space of a video diffusion model. Instead of injecting camera parameters as features into the backbone, this paper computes an appearance-agnostic Geometry‑guided Reprojection Flow (GRFlow), which relies solely on camera parameters to characterize pixel displacements across frames. Then this paper \nformulates noise warping as a partial differential equation problem and solve it via a bipartite graph. To further enhance inference robustness, this paper introduces dynamic perturbations to camera extrinsics during training. On RealEstate10K, the method reportedly improves camera controllability (lower TransErr/RotErr) and overall video quality (lower FVD), with extensive ablations on GRFlow smoothing (α), λ, and DST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Conditioning on noise rather than intermediate activations is well motivated and addresses the issue when directly injecting numerical camera parameters into the diffusion backbone fails to capture subtle viewpoint variations and leads to structural distortions or\nvisual artifacts.\n2. The design of formulating noise propagation as the discrete solution of advection Partial Differential Equations (PDEs) is well derived and discussed. \n3. Strong empirical results on camera-control benchmarks on RealEstate 10k. \n4. Detailed ablations demonstrating the impact of each hyperparameter in the modeling process."}, "weaknesses": {"value": "1. Experiments are confined to RealEstate10K, which contains mostly indoor, quasi‑static scenes with modest motion. The method’s generalization to outdoor/large‑scale scenes, heavy roll/pitch/yaw, or dynamic objects remains unclear.\n2. Besides comparing to MotionCtrl, CameraCtrl, and AC3D, maybe also compare with other optical-flow derived noise warping in video diffusion (e.g., Go‑with‑the‑Flow: Motion‑Controllable Video Diffusion Models Using Real‑Time Warped Noise)"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VwNC7yBtvW", "forum": "TT3gmYaqyc", "replyto": "TT3gmYaqyc", "signatures": ["ICLR.cc/2026/Conference/Submission5187/Reviewer_UEET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5187/Reviewer_UEET"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980951631, "cdate": 1761980951631, "tmdate": 1762917934120, "mdate": 1762917934120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CameraNoise, camera pose control framework for video diffusion models, addressing the structural distortions caused by directly injecting numerical parameters. CameraNoise embeds camera poses into the noise space using a temporally coherent stochastic representation, which is achieved via a Geometry-guided Reprojection Flow and a novel warping algorithm. This approach ensures consistent noise propagation while preserving the diffusion model's Gaussian prior, resulting in stable, high-quality videos with superior fidelity and controllability over prior methods on benchmarks like RealEstate10K."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It's a good paper. Specifically, the motivation is clear and solution is intuitive and reasonable. \n\n- Unlike previous camera controllable video generation methods, the paper tackles initial noise representation that contains camera poses while keeping gaussianity. Although the method has similar philosophy as previous two works (How I warepd your nosie and Go-with-the-flow), CameraNoise does not require a source video or a reference video which is a strong merit.\n\n- The experiments are comprehensive and presented clearly."}, "weaknesses": {"value": "- In L193-194, how is the pseudo-depth $d$ computed? The pseudo-depth $d$ requires more description since if it's estimated using RGB pixels, the GRFlow can't be characterized as 'appearance-agnostic'. If the depth $d$ deivates too much from the g.t. depth, it would rather incur structural or camera errors.\n\n- Another major weakness is that the method is experimented on RE10K only. Experiments on more benchmark dataset would add value to the paper. Moreover, is GRFlow and CameraNoise framework applicable to dynamic scenes (i.e., can it generate videos with objects with dynamic motion and also dynamic camera pose)?\n\n- Can the authors clairfy how the proposed warping mechanism differs from Go-With-The-Flow warping and HIWYN?\n\n- What is the computation complexity for the GRFlow construction and warping process, respectively?\n\n- What is the base video model (t2v, i2v) for the training?"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B2PxplWRhl", "forum": "TT3gmYaqyc", "replyto": "TT3gmYaqyc", "signatures": ["ICLR.cc/2026/Conference/Submission5187/Reviewer_a2Ah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5187/Reviewer_a2Ah"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181966639, "cdate": 1762181966639, "tmdate": 1762917933712, "mdate": 1762917933712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}