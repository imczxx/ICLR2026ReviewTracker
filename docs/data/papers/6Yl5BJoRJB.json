{"id": "6Yl5BJoRJB", "number": 15497, "cdate": 1758251972610, "mdate": 1759897302901, "content": {"title": "Generation-Augmented Multimodal Retrieval in Personal LLM Agents", "abstract": "With the increasing use of smartphones, users often take photos to quickly capture and store information. This personal multimodal data offers a promising research direction for developing smartphone AI assistants. In this paper, we introduce a new task called personalized multimodal retrieval (PMR) within this context. The PMR task takes a user’s text query as input and retrieves images that match the user’s search intent. The task presents three key challenges: 1) effective management of personal data, 2) handling the quality of user queries, and 3) the need for a lightweight model architecture that can operate on personal devices. To address them, we propose GAMR, which enhances multimodal retrieval by leveraging LLM-driven query refinement and RLHF to optimize end-to-end performance. Extensive experiments demonstrate a 13.2% improvement over SOTA baselines. Moreover, GAMR has been deployed in real products, resulting in an improved user experience.", "tldr": "", "keywords": ["Large Language Models", "Multimodal Models", "Personal LLM Agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/827c653a65eac40abfda02fdf19862d80d35f5e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a personalized multimodal retrieval task for the scenario where a smartphone AI assistant retrieves images from a personal album based on a user's text query. The authors analyze three challenges: managing personal multimodal data, user query quality, and lightweight model architectures. They propose a GAMR approach, which includes a three-layer MSTS index structure for processing spatiotemporal metadata, semantic graphs, and image vectors, and a multimodal model consisting of a CLIP encoder, a dynamic projection expert, and a small LLM. Retrieval performance is optimized through RLHF. Experiments are conducted on the Flickr and YFCC100M datasets, and the results show that GAMR outperforms the best baseline by 13.2% and 5.6%, respectively. The paper claims that the approach has been deployed in real-world products."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The task definition is targeted at real-world user scenarios, the personal photo retrieval problem has practical application value, and the problem analysis is relatively clear.\n2. The three-layer index structure design is reasonable, and the progressive architecture from spatiotemporal filtering to semantic matching and then vector retrieval aligns with retrieval logic.\n3. The experimental setup is relatively complete, including multiple baseline comparisons, ablation experiments, and parameter analysis. The paper is well-written."}, "weaknesses": {"value": "1. The dataset selection is fundamentally flawed. The Flickr and YFCC100M datasets used are public social media datasets, and images come fully annotated with titles and descriptions. This completely contradicts the paper's claim of using private photos on personal devices. Real personal photos often lack these structured annotations. The entire experimental setup is disconnected from real-world application scenarios, completely invalidating the validity of the experimental conclusions.\n2. The complete lack of public access to the code seriously questions the authenticity of the experiment. The paper clearly states that the code is withheld for commercial reasons and is only planned for future release. However, the description in the paper contains numerous problems, leading one to suspect that the authors have not actually implemented the method at all, or that their implementation is so poor that they are afraid to release it publicly. All reported performance data is unverifiable, and the credibility of the experimental results is extremely low. I completely doubt that such an implementation can be commercially viable.\n3. The claim of lightweightness completely lacks on-device verification. The paper's core goal is to run the method on personal devices, but all experiments were conducted on a server with eight RTX3090 GPUs. No real mobile device training or inference data is provided. The feasibility of the 2.1B parameter model and 2.8-second latency on mobile phones is completely unproven, and the claim is seriously disconnected from actual verification. \n4. The method's innovation is seriously lacking. The core technology is simply a simple stacking of mature technologies such as K-D trees, knowledge graphs, and vector retrieval. The LLM query reconstruction is also an existing method, lacking any theoretical innovation or technical breakthroughs. Experimental results show that the dynamic expert module improves performance by less than 1% while increasing complexity. Overall, it is an engineering patchwork of existing technologies, with almost no academic contribution.\n5. The baseline selection has obvious problems. The paper primarily compares general multimodal retrieval models and text models, lacking specialized comparisons with methods in directly related fields such as personalized retrieval, lifelog retrieval, and episodic memory systems. The selected baselines are not designed for personalized scenarios. This comparison fails to demonstrate the method's true advantages in the target task and may be a selective comparison to improve performance.\n6. The method's generalization and applicability are extremely poor. The entire method relies heavily on fully annotated image data, requiring various information such as spatiotemporal metadata and semantic descriptions. However, these annotations are rarely found in real personal photos, making the method completely unapplicable in real-world scenarios. The three-layer index design is overly complex and highly targeted, making it difficult to generalize to other retrieval tasks. \n7. Personalization verification is completely missing. The core concept of the paper is personalized retrieval, but it only uses the general Recall and MAP indicators. There is no evaluation method for personalized features, no real user research, and no way to prove that the retrieval results truly meet the user's personalized needs. The so-called personalization is more like a gimmick than a verified feature."}, "questions": {"value": "1. Regarding the choice of datasets, I noticed that the paper uses Flickr and YFCC100M, both public social media datasets. These images typically include user-generated captions and descriptions. However, the paper emphasizes the use case of private photo retrieval on personal devices, where such photos typically lack structured annotations. How do the authors explain the discrepancy between these datasets and real-world applications? Could they provide experimental results on real personal photo data to validate the effectiveness of their approach?\n2. The paper mentions that the implementation code is currently under internal review for commercial reasons and cannot be made public. Considering that code reproducibility is a fundamental requirement of academic research, could the authors at least provide pseudocode for the core modules or a detailed algorithm description? Furthermore, could they provide some intermediate results or experimental logs to help verify the authenticity of the experiments? The question is well-intentioned, but I find it difficult to believe that this level of implementation is commercially viable.\n3. The paper focuses on a lightweight method that runs on personal mobile devices, but all experiments were conducted on a server equipped with eight RTX3090 GPUs. Could the authors provide additional experimental results on real mobile devices, specifically including key metrics such as training time, inference latency, memory usage, and battery consumption on Android or iOS devices? How feasible is the 2.1B parameter model on real-world mobile phones?\n4. Regarding the innovative nature of the method, I noticed that the core technology is primarily a combination of existing technologies such as K-D trees, knowledge graphs, and vector retrieval. What do the authors consider to be the main technical innovation of this paper? Compared to directly using existing multi-layer index structures or RAG systems, what are the fundamental technical breakthroughs of the MSTS index and overall architecture proposed in this paper? The performance improvement brought by the dynamic expert module seems to be relatively limited. How is the necessity of this design justified?\n5. Regarding the choice of baseline, the paper primarily compares general multimodal retrieval models and text models, but seems to lack comparisons with specialized methods in the field of personalized retrieval. Have the authors considered comparing methods in related fields such as personalized search, lifelog retrieval, and episodic memory systems? These methods may be more closely aligned with the task set in this paper, and the comparison results would be more convincing.\n6. Regarding the generalization and practical applicability of the method, the method proposed in the paper relies heavily on annotations such as spatiotemporal metadata and semantic descriptions. How will the method work in real-world scenarios when most user photos lack these annotations? In Appendix M.1, the authors mention three data collection methods. Can you provide feasibility analysis and user acceptance data for these methods? Can the methods be generalized to other personalized search tasks?\n7. The paper emphasizes personalized search, but only uses generalized Recall and MAP evaluation metrics. How do the authors specifically evaluate the degree of personalization of search results? Have they conducted real-world user research to verify that the search results truly meet the personalized needs of different users? Can you design some evaluation metrics specifically targeting personalized features, such as user satisfaction and query intent understanding accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sqSmwC3OqK", "forum": "6Yl5BJoRJB", "replyto": "6Yl5BJoRJB", "signatures": ["ICLR.cc/2026/Conference/Submission15497/Reviewer_xDv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15497/Reviewer_xDv7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394552035, "cdate": 1761394552035, "tmdate": 1762925787583, "mdate": 1762925787583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces personalized multimodal retrieval (PMR), a personalized text-to-image retrieval task that first collects data via user-assistant conversations, and then retrieve images from a personal album via a textual query that reflects conversational context. \nAlongside the introduced task, the authors propose a system named Generation-Augmented Multimodal Retrieval (GAMR) to tackle.\nSpecifically, GAMR consists of query refinement, a three-layer Multimodal Spatio-Temporal Semantic (MSTS) Index, and RLHF fine-tuning for personalization.\nEvaluations demonstrate strong performance of GAMR while maintaining low computational cost (~10% lower FLOPs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The PMR task is practically meaningful. It compasses privacy, query vagueness and lightweight constraints which make the task practically significant.  \n2. Based on the provided results, the proposed GAMR system performs well on Flickr and YFCC100M compared to the baselines. The transferability on unseen users (Table 2) also supports its robustness.\n3. This paper is complete in its structure, rigorously formulates the problem. The methodology is technically complete, including explicit layer definitions, formal loss equations, and detailed ablations."}, "weaknesses": {"value": "1. Despite introducing a practically meaningful task, the overall contributions of this paper remain limited due to the modest novelty of the proposed PMR task, the model’s sensitivity to hyperparameters (e.g., $\\rho$), and the limited methodological novelty of the hierarchical indexing design.\n2. The authors first introduce the PMR task as a task that involves person-album conversation. However, it seems like the benchmarks used in the paper are not realistic to simulate the introduced PMR setting, where the datasets approximate PMR using public captions rather than genuine personal dialogues, limiting ecological validity despite its scale.\n3. This paper would benefit with a clearer. Some parts of the sections are hard to interpret, such as L183-L185 -- how exactly do you partition an album into spatio-temporal cubes. Figure 2c is also hard to understand which component corresponds to which layer in the illustration. \n4. The authors ablate different layers and the RLHF component of GAMR. The ablation only isolates structural components, but omits loss-level analysis (e.g., L_layer1 loss). Including these would clarify the writing.\n5. L214: What evidence do you use to draw the claim that \"From the above three steps, we observe that retrieval performance relies on the quality of query Q\"? Clarifying this would improve the readability."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4xRSh40Du0", "forum": "6Yl5BJoRJB", "replyto": "6Yl5BJoRJB", "signatures": ["ICLR.cc/2026/Conference/Submission15497/Reviewer_cVwc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15497/Reviewer_cVwc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868398524, "cdate": 1761868398524, "tmdate": 1762925787065, "mdate": 1762925787065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new task called Personalized Multimodal Retrieval, which aims to retrieve a user’s personal images given a natural language query. To address challenges in managing multimodal personal data, vague user queries, and limited on-device resources, the authors propose Generation-Augmented Multimodal Retrieval, which integrates three components: (1) a Multimodal Spatio-Temporal Semantic (MSTS) Index for organizing data; (2) LLM-based query refinement using RLHF to improve retrieval quality; and (3) a lightweight multimodal model with dynamic projection experts for efficient on-device inference. Experiments on Flickr and YFCC100M datasets demonstrates its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel and practical task setting. The paper identifies an important real-world use case: retrieving personal multimodal data via text queries which is underexplored and highly relevant for personal AI assistants.\n- Well designed architecture. The proposed MSTS index and dynamic projection expert module form a coherent and technically sound framework that balances personalization, efficiency, and retrieval accuracy.\n- Comprehensive experiments. Extensive evaluation on two large-scale datasets with multiple baselines convincingly demonstrates the superiority of GAMR."}, "weaknesses": {"value": "- Task definition. I do think it is a common scene that users will actively ask the AI assistant to store the description of an image (Fig.1 (a)). In most case, the user just take a photo (without any description) and want the AI assistant to recall it later. Evan though the user provide a description, it may be a short and semantically ambiguous expression, such as lovely chair, the cute dog etc. Therefore, I think the system should have an automatic collection and annotation process to extract description of a user image.\n- Base models. The small LLMs used here are GPT-2, FLAN-T5 and Qwen2 with sizes ranging from 1.5B to 3B. As a model running on mobile device, I think some frontier small LLMs should be considered, such as Qwen2.5‑0.5B and Gemma 3 (1B) etc.\n- The ablation experiments in Tab.4 show that none of the component except layer-2, makes minor contribution to the final model."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rDOS4M4Wxn", "forum": "6Yl5BJoRJB", "replyto": "6Yl5BJoRJB", "signatures": ["ICLR.cc/2026/Conference/Submission15497/Reviewer_BrmK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15497/Reviewer_BrmK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966107399, "cdate": 1761966107399, "tmdate": 1762925785780, "mdate": 1762925785780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel task of personalized multimodal retrieval, which focuses on retrieving images from a user's personal device based on their text queries to enhance the personalized experience of smartphone AI assistants. The paper proposes a framework named GAMR that utilizes Large Language Models through Reinforcement Learning with Human Feedback (RLHF) to refine user queries and improve retrieval performance. The paper also presents the Multimodal Spatio-Temporal Semantic (MSTS) Index for personal data management. Extensive experiments demonstrate that GAMR outperforms existing baseline methods, showing its potential for real-world applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated and tackles an important and practically relevant problem of personalized multimodal retrieval for smartphone AI assistants.\n2. The proposed GAMR framework effectively leverages LLMs and RLHF to refine user queries, which achieves improved retrieval performance.\n3. The proposed framework is a systematic approach that combines data management, query refinement, and efficient retrieval, making it a comprehensive solution for the task.\n4. The paper is well-written and easy to follow, with clear explanations of the proposed methods and experimental results."}, "weaknesses": {"value": "1. Personalized multimodal retrieval has been studied in prior works (e.g., Cross-Modality Personalization for Retrieval, Meta-Personalizing Vision-Language Models, Personalized Image Retrieval with Sparse Graph Representation Learning). The authors should clarify how their PMR formulation differs from these paradigms. In addition, the title \"Personal LLM Agents\" is somewhat misleading, as the work mainly focuses on image retrieval rather than developing a full LLM-based agent.\n2. The baseline comparison could be expanded. Since GAMR builds upon query reformulation, it would be informative to compare against recent advanced query refinement methods, such as QuARI: Query Adaptive Retrieval Improvement, RaFe: Ranking Feedback Improves Query Rewriting for RAG, and Query Rewriting for Retrieval-Augmented Large Language Models.\n3. Some implementation details remain unclear. In particular, more explanation is needed on how generative VLMs (e.g., Qwen2.5-VL or LLaVA) are incorporated into the personalized retrieval process.\n4. The evaluation setup differs somewhat from the claimed personallization setting. The paper would benefit from discussing the practical gap between experiments on public datasets (e.g., Flickr, YFCC100M) and real-world personalized smartphone use cases."}, "questions": {"value": "1. How are Generative VLMs utilized in the retrieval process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YyORsUTewC", "forum": "6Yl5BJoRJB", "replyto": "6Yl5BJoRJB", "signatures": ["ICLR.cc/2026/Conference/Submission15497/Reviewer_6aB8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15497/Reviewer_6aB8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966769987, "cdate": 1761966769987, "tmdate": 1762925785281, "mdate": 1762925785281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}