{"id": "KHIeyfEqZE", "number": 11736, "cdate": 1758203389082, "mdate": 1759897557780, "content": {"title": "Collaborative Dual-Size Large Language Models with Dual-Stage Deferral Risk Control", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet ensuring their safe deployment remains challenging. Existing safety mechanisms, while effective against malicious inputs, often degrade performance on benign queries due to over-conservative strategies. We propose the \\textbf{D}ual-size LLM collaborative framework with \\textbf{D}ual-stage deferral risk contro\\textbf{L} (\\textbf{DDL}), which integrates lightweight and heavyweight models with calibrated deferral mechanisms. Our approach formalizes the safety–efficiency trade-off as a constrained optimization problem that jointly considers prediction accuracy, computational cost, and safety risk. We provide theoretical guarantees showing that our mechanism achieves distribution-free risk control while minimizing unnecessary heavyweight computation. Extensive experiments on three datasets demonstrate that DDL effectively balances safety and efficiency, achieving performance and safety metrics comparable to state-of-the-art safety-aligned models while reducing average inference time by more than 65\\%.", "tldr": "", "keywords": ["Risk Control", "Model Collaboration", "Deferral Mechanism", "Computational Efficiency", "Safety-Efficiency Trade-off", "Dual-size Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d982d25bccf7ba5c0a264f301f0186344ff7be0e.pdf", "supplementary_material": "/attachment/9c4f069a888eba9cf06f0f1339ae17efd2a08759.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DDL (Dual-size Large Language Model framework with Dual-stage Deferral Risk Control), a collaborative architecture that integrates a lightweight and a heavyweight LLM to balance safety and efficiency during inference. The key insight is that safety–efficiency trade-offs in LLMs form a Pareto frontier, where improving one often degrades the other."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of combining dual-size models in a risk-calibrated deferral pipeline is conceptually novel and theoretically grounded.\n2. The contribution addresses a key bottleneck in scalable safe deployment: unnecessary use of large models for benign inputs.\n3. The writing is professional, structured, and easy to follow."}, "weaknesses": {"value": "1. Limited analysis of failure cases. While DDL provides strong average performance, the paper does not report worst-case or outlier failure scenarios.\n\n2. Token-based classifier generality. The safety classifier’s reliance on a single special token embedding may not scale to more complex multi-turn or multilingual scenarios. An analysis of embedding robustness or potential expansion (e.g., multi-token attention pooling) would enhance credibility."}, "questions": {"value": "See Weaknesses.\n\nBesides, how does DDL behave under distribution shifts (e.g., new unsafe categories not in calibration data)? Would threshold guarantees still hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3XV5wd9Rhm", "forum": "KHIeyfEqZE", "replyto": "KHIeyfEqZE", "signatures": ["ICLR.cc/2026/Conference/Submission11736/Reviewer_rNqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11736/Reviewer_rNqP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760837679404, "cdate": 1760837679404, "tmdate": 1762922771699, "mdate": 1762922771699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper talks about building a multi-LLM framework with a classifier that scores the harmfulness of inputs. It is very difficult to make sense of the model size, dataset size, classifier architecture, etc because of multiple contradictions within the literature. The paper spent quite a few pages on providing theoretical proofs of the two thresholds for the classification, but in practice, the thresholds seem to be identical after automatic calibration on three different datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I apologize that I cannot say any good words about this paper."}, "weaknesses": {"value": "There are multiple fatal errors or inconsistencies in the paper.\n\nThe paper contains multiple fatal and naive errors that might indicate that either the author is extremely negligent or the LLM is hallucinating. \n\nFor example, \n\n1) \n\nOn line 218, the author introduced their work as a \"Privacy Detection Framework\". However, the paper is never about privacy preservation, and the term \"Privacy Detection\" simply does not make sense. \n\n2) \n\nIn multiple cases (line 374, Table 1, Figure 3, line 1127, line 1130, line 1132), the paper mentions a model called \"Qwen-2.5-1B\" or \"Qwen-1B\" which does not actually exist. In Table 1, I can see Qwen-2.5 0.5B, 1B, and 1.5B at the same time, which makes it hard to believe that it is a human error. \n\n3) \n\nThe dataset size does not add up. \n\nIn lines 307 and 309, the paper claims that there are 20,000 training samples using Claude that contain 10k safe and 10k unsafe queries.\n\nHowever, in line 1000 and Table 4, the number has changed to 30,000 in total and 15,000 safe and 15,000 unsafe. And they now claim that these samples are not produced by Claude, instead those are 5,000 from MMLU, 5,000 from BoolQ, and 5,000 from WikiQA. \n\nThe most ridiculous number appears in line 1065, where the authors claim that the classifier is trained with 230K samples. \n\n4) \n\nThe Classifier architecture keeps changing\n\nOn lines 145-147 the paper clearly stated that a simple linear classifier is used with only two parameters (W and b). \n\nThen, in lines 1058-1059 the authors say \"For the trainable token-based safety classifier, we utilize a base encoder with 24 transformer layers.\" I could only assume that they might be referring to Qwen-2.5-0.5B? Because Qwen-2.5-1.5B already has 28 layers. \n\nThen, in the next few lines (line 1059-1060), the authors mentioned that the model architecture actually consists of three fully-connected layers (2048 → 1024 → 512) with GELU activation, which is completely different from what is described in the equation in lines 145-147. \n\nThe plot twist is not over yet. In line 1172, Table 6, when performing the efficiency analysis, I can see linear and 2-layer MLP again. This 2-layer MLP is never mentioned until the 22nd page of this paper. \n\n5) \n\nCitation Error\n\nIn line 168, when citing Deepseek-V3, the author first cited Guo et. al, which was the citation for DeepSeek-Coder. Then in line 323 the authors cited Liu et al. for Deepseek-V3, which is the correct citation for Deepseek-V3. Such inconsistency might still be a human error? \n\n6) \n\nIdentical auto-calibrated thresholds for three different datasets\n\nIn line 407, Table 2, it appears that the optimal thresholds set by auto calibration are all 0.73 and 0.35 for three different datasets. Through literature (line 1089), I believe they might be doing a grid search over 6,000 uniformly spaced threshold candidates in [0.1, 0.9] and ∆ = 0.01? (BTW, it is also mathematically the wrong number of thresholds) Overall, it is implausible to find the exact same threshold for all three datasets through a grid search. \n\n\nAt this point, I believe it is highly likely that no one actually conducted the experiments or wrote the paper themselves. I may find more issues if I keep digging. For example, the bounding of the loss in the equation seems off and it can go beyond [0,1], and the numbers in many tables are suspicious, but there is no point in looking into it anymore."}, "questions": {"value": "**I respectfully request clarification:** \n\nWere these experiments genuinely performed as described?\nIf large language models were used to draft portions of this manuscript, I would kindly ask the authors to acknowledge this and ensure that all technical details have been carefully verified against the actual implementation.\n\nIf the authors did not perform the experiment or write the paper themselves, I kindly request that the authors apologize for the unprofessional behavior, as it wasted reviewers' hours of time and might have made them question their own sanity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Iarx9QOfXI", "forum": "KHIeyfEqZE", "replyto": "KHIeyfEqZE", "signatures": ["ICLR.cc/2026/Conference/Submission11736/Reviewer_xikC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11736/Reviewer_xikC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563097748, "cdate": 1761563097748, "tmdate": 1762922771309, "mdate": 1762922771309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose DDL, a Dual-size LLM collaborative framework with Dual-stage deferral risk controL, including a trainable token-based safety classifier based on hidden state of LLMs and a optimal threshold selection method to expand the Pareto frontier between response latency and classification accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing and presentation of the paper are excellent, with a clear and logical flow.\n- The paper provides a finite-sample theoretical guarantee for its proposed method."}, "weaknesses": {"value": "- **Threat model.** The author assume a white-box scenarios, which can access to the hidden representation of LLMs. However, many commercial scenarios are not open-resourced, such as GPT and Claude. Only considering the white-box scenarios restricts the applicability of the method in real world.\n- **Experiment setting.** The token classification dataset is generated by Claude, which may not reflect real adversarial attacks. Could the author show some examples of the generated unsafe prompt to show that they are stealthy and compare them with the real world unsafe prompts to show that they have the same distribution.\n- **Experiment results.** The core contribution of this method is the token embedding classifier. The author only compare classifier architectures in Appendix I, but no comparison with other classifier-based defense methods, such as the Moderation API by OpenAI. The author should further specify the advantages of the proposed white-box defense methods over the Moderation API."}, "questions": {"value": "- Would the unsafe prompts generated by Claude can be directly refused by the target LLMs? It is unclear that the author accounted for this when generating the unsafe prompts. How can we ensure that the generated unsafe prompts are stealthy enough so that they are not immediately refused by the first LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BJ2dbcMHpV", "forum": "KHIeyfEqZE", "replyto": "KHIeyfEqZE", "signatures": ["ICLR.cc/2026/Conference/Submission11736/Reviewer_QqSw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11736/Reviewer_QqSw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652492623, "cdate": 1761652492623, "tmdate": 1762922770962, "mdate": 1762922770962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DDL, a system designed to ensure LLM safety while maintaining computational efficiency.\n\nThe framework employs two models of different sizes: the smaller model first judges whether a query is safe. If the confidence is high, it directly answers; otherwise, the query is deferred to a larger model. For highly uncertain cases, a third model (or a human verifier) conducts the final verification.\n\nExperiments are conducted on modified “safety-critical” versions of QA datasets, demonstrating that the proposed method outperforms single-model baselines in both safety and performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed framework is conceptually simple for safe LLM deployment.\n- The paper provides theoretical justifications, offering distribution-free risk control guarantees for the dual-stage decision process.\n- The design of a token-based safety classifier is efficient, avoiding full model fine-tuning."}, "weaknesses": {"value": "- Unconvincing experimental results (Table 1): The DDL system uses the small model first and defers to the large model when the confidence is low. Therefore, its overall performance should intuitively fall between the small and large model results. However, Table 1 shows that DDL even outperforms the larger model, which seems implausible. This raises questions about the reliability of the reported results. If this gain is due to the additional verification by DeepSeek V3 (the M3 model), it is not a fair comparison, because the pure models cannot use such external verification. The authors should report how often M3 was invoked, and the results without this external verification. Additionally, efficiency should be measured in terms of FLOPs or total model computation cost, rather than response time, which is highly dependent on infrastructure.\n- Lack of comparison with guard models: The paper does not compare DDL against common guard-model approaches (e.g., using a safety classifier before model inference). Small guard models like [1] can efficiently improve safety without changing the main model's output distribution, providing a fairer baseline.\n- Potential degradation for safe but complex queries: Since delegation depends on the safety classifier's uncertainty, DDL might incorrectly classify safe but difficult questions (e.g., college-level math) as \"safe\" and let the small model answer, leading to significant performance drops on such cases. A more nuanced delegation policy (e.g., combining difficulty estimation with safety) may be necessary.\n\n[1] Lee et al., HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models, ICLR 2025"}, "questions": {"value": "- Clarify in Table 1 which metrics are better when higher vs. lower values (use $\\uparrow$/$\\downarrow$ symbols).\n- Report the percentage of samples handled by each model (M1, M2, M3).\n- Add an ablation comparing DDL with standard guard-model pipelines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D89tHupJvI", "forum": "KHIeyfEqZE", "replyto": "KHIeyfEqZE", "signatures": ["ICLR.cc/2026/Conference/Submission11736/Reviewer_TwQU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11736/Reviewer_TwQU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917599622, "cdate": 1761917599622, "tmdate": 1762922770553, "mdate": 1762922770553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}