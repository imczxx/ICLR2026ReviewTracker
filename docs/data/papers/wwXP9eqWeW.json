{"id": "wwXP9eqWeW", "number": 20985, "cdate": 1758312381200, "mdate": 1759896948378, "content": {"title": "MURMUR: Using cross-user chatter to break collaborative language agents", "abstract": "Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability.", "tldr": "We explore a new attack vector on Language agents communicating with multiple users and solving multiple tasks", "keywords": ["Language Agents", "Security", "LLM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ca714339dda173a236b48a657cca6764fdce771.pdf", "supplementary_material": "/attachment/21f270463a066e34cdc05e6d2603d774c27337eb.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces cross-user poisoning (CUP)—a new attack vector unique to multi-user LLM agents that operate in shared workspaces (e.g., Slack/Discord). In CUP, an attacker posts seemingly legitimate instructions that become part of the persistent shared state, causing the agent to later execute the attacker’s intent while serving benign users. The authors show CUP succeeds on popular multi-user agents in the wild and persists across tasks—unlike classic prompt injection that targets untrusted data sources rather than other users’ instructions. They then propose MURMUR, a framework that uplifts single-user tasks into concurrent, persona-driven multi-user scenarios to stress-test agents for CUP; they report high attack success and long-lived propagation, and explore a first-step defense via task-based clustering to contain cross-task leakage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper crisply distinguishes CUP from both direct and indirect prompt injection by highlighting the user-vs-user instruction conflict.\n\n+ The authors successfully attack two deployed multi-user agents (Continua, ElizaOS), including a step-by-step demonstration where an instruction persists and later triggers a malicious reminder link;"}, "weaknesses": {"value": "- I cannot find the practicality for the proposed attack as shown in Figure 2. To me, this is just another malicious user's prompt instruction, and the LLM follows it. I acknowledge the story is interesting, but the threat model is unclear. Why does a user do this in a collaborative environment? Suspicious behaviors can be easily found by others. Have you done a user study?\n\n- Experiments span three adapted environments and four model backends; however, the landscape of multi-user agent deployments is broader (productivity suites, CRM, code-assist in chat). The narrow environment sets risks of overfitting conclusions to chosen tool APIs and dialog dynamics. \n\n- The defense focuses on containment via clustering; the paper acknowledges that in-cluster attacks remain unsolved and clustering accuracy is a linchpin."}, "questions": {"value": "1. Beyond containment, what mechanisms (e.g., per-user provenance tags, signed-instruction checks, scoped memories) best mitigate in-cluster CUP, where attackers are part of the same task? Any preliminary results?\n\n2. How do ASR/APR curves shift with attacker budget  B, different interleavings (front-loaded vs. late attacks), or stealth (short, policy-like phrases vs. verbose templates)?\n\n3. Since users do not see tool traces but the agent does, how much of the persistence is due to tool-trace conditioning vs. message history alone? Can filtering/siloing tool traces reduce persistence without large TSR drops?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9FH7j95iYg", "forum": "wwXP9eqWeW", "replyto": "wwXP9eqWeW", "signatures": ["ICLR.cc/2026/Conference/Submission20985/Reviewer_Xu6N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20985/Reviewer_Xu6N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699444064, "cdate": 1761699444064, "tmdate": 1762999978739, "mdate": 1762999978739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the security of language model (LM) agents operating in a setup where they serve multiple users and have shared memory. The paper suggests that such agents are vulnerable to a specific type of attack, where a malicious actor \"plants\" the instructions into the memory of the LM, such that these instructions are only executed at a later time point, when the LM is operating on behalf of another user. The paper poses that such delayed execution is not covered by the standard prompt injection framework and thus remains an open vulnerability.\n\nTo study this issue and propose remedies, the paper introduces MURMUR, a multiagent interaction framework where the agent, the users, and the attacker are simulated by LMs. MURMUR setups adapt single-user agentic tasks from other benchmarks (Tau-bench and AgentDojo) into the desired form. The paper then investigates the success rate of the planted cross-user attacks. It finds that the agents are highly susceptible to such attacks and that the malicious instructions persists strongly as the agent continues to work on future tasks. Furthermore, traditional prompt injections are found to not be as effective. The paper also finds that shared context, until a certain number of concurrent users, is beneficial to the agents for successful and more efficient completion of user requests. Finally, the paper proposes a mitigation of the cross-user attacks based on limiting the context of the agent when attending to individual users. This mitigation is shown to drive the ASR to zero across all studied domains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The setting of an agent serving multiple users with shared memory is highly relevant. Clearly, such agents will be deployed more broadly, and security vulnerabilities will be more critical when one attacker can \"poison\" the agent so that many other users are harmed.\n- The paper provides examples of subverting real LLM-based agents Continua and ElizaOS. The case of having the agent move cryptocurrency to a wrong address when poisoned across platforms (in Appendix B) is especially compelling.\n- MURMUR framework allows incorporation of existing single-user benchmarks, making it more scalable."}, "weaknesses": {"value": "## Competing hypotheses for success of CUP\nMy main concern with the paper is that it is not clear to me that the attacks it proposes necessarily succeed because of the setup being _multi-user_. In my understanding, from the results of the paper, two natural competing hypotheses cannot be refuted, namely that the attacks succeed because they are _multi-turn_ or because the prompt injections used in the paper are better than the outdated baselines. Below I discuss these hypotheses in more detail.\n\n1. The results might stem from the attack being _multi-turn_. In L77-79, it is written about Fig. 1 (emphasis mine):\n> However, in a multi-user environment (the right panel), a malicious instruction can be established as a persistent rule under the guise of a helpful policy. As a result, the agent, unable to **attribute** the persistent rule solely to the attacker, incorrectly applies it to benign users. \n\nIn my understanding, attribution is not an issue in the example in Figure 1. The agent knows who made the request to include the phishing link. Rather, the agent fails because it does not recognize the request as malicious, which is the same failure mode as in the traditional jailbreaking. To see that having multiple users is crucial, a useful ablation would be to have Mallory (the attacker) to set the reminders for Carlos and Dana. This way, the multi-turn structure of the dialogue would be preserved, with all the cluttering context that could prevent the agent from seeing the malicious intent, while the multi-user aspect of it would be removed. Looking at the example attack in Appendix G, I have the same concern. In the example, the attacker (Jason Yu) sets up the environment such that the agent will later execute the actions of sending malicious emails. Then, a benign user (Ryan Lee) requests an unrelated task and the agent sends the emails before executing the task. What would've happened if Jason Yu himself requested this unrelated task?\n\n2. An even simpler hypothesis could be that the large ASR comes from a newer prompt injection template. The paper compares cross-user poisoning (CUP) attacks against prompt injections in Table 3. However, from L449-450, I get the impression that the original prompt injections from AgentDojo have been used. If that is the case, of course the LLMs that have been introduced after AgentDojo would incorporate defenses (all of the LLMs in table 3 have been released after AgentDojo). Thus, the PI baseline in Table 3 seems inadequate. Instead, some variation of the template used for CUP (Figure 8 in the Appendix) should be tried for a single-step attack as well.\n\n\n## Statistical significance\nI also suspect that the results in the paper might be statistically underpowered. In particular, Figure 6 reports the success of GPT-4.1 as the agent when varying the number of tasks it does concurrently. In my understanding, these were generated from a total of 111 tasks. The 95% Wilson interval for the first number on the left (approx. 66%) would then be approx. [57%, 74%]. This includes more than the entire y-axis range of the plot. I suspect that similar underpowering is happening with the tool call count averaging. Taking this into account, I cannot currently trust the discussion in Section 5.3.\n\nTo confirm or reject this concern, it would be useful to see confidence intervals reported for at least some of the results in the paper.\n\n## The defense\nI think the defense proposed in the paper is overfitted to the particular design choices of MURMUR and cannot be applied in a scalable way. The whole point of multi-user collaboration with an agent is that the users can influence the agent in a way that propagates to everyone else. The example in Fig. 1 right could also have been a benign interaction, where Mallory just notifies the agent about the (legitimate) website of the workshop. In that case, this information should be correctly propagated by the agent to others in the reminders. If I understand it correctly, the groups-based defense proposed in Section 6 would make such an interaction impossible. Correspondingly, the reason why TSR doesn't change strongly in Table 4 with the defense is that the tasks that the users request are mostly isolated (indeed, they were adapted from _single-user_ benchmarks), hence the shared memory, a key element of the MURMUR setup, is actually barely useful to the agent.\n\nThe defense also looks like it could be compromised if one targets the \"lightweight classifier\" assigning agents to groups.\n\n## Minor points\n- Fig. 5 is counterintuitive / borderline misleading. The text does mention that this is _conditioned on at least one task being compromised_, and the caption should too. From a cursory glance it reads as if the first task is just always compromised by the attack vector in question.\n- The proposed defense and results should not be in the Discussion section but rather stand as its own section or incorporated into the previous section.\n \n- L449: missing number for PI success"}, "questions": {"value": "Why does the user only see the messages corresponding to their task (L249)? It looks like the discord agent, for example, is added to a specific channel and all interactions with it are public. I think this might actually be an issue if the attacker has to put their prompt injection into the shared channel for everyone to see."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ODWYl6nRpu", "forum": "wwXP9eqWeW", "replyto": "wwXP9eqWeW", "signatures": ["ICLR.cc/2026/Conference/Submission20985/Reviewer_dvZ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20985/Reviewer_dvZ8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959307404, "cdate": 1761959307404, "tmdate": 1762999979334, "mdate": 1762999979334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cross-user poisoning (CUP) attack targeting multi-user agentic collaboration scenarios. The CUP attack exploits the inability of LLM agents to distinguish instructions from different users within a shared session, injecting seemingly benign malicious instructions into the shared context to compromise other users' tasks. The authors also introduce a benchmark framework named MURMUR to systematically observe the consequences of CUP and evaluate the performance of LLM agents under this threat."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper identifies a significant threat in multi-user agentic collaboration scenarios.\n\nS2. The proposed benchmark framework is well defined mathematically.\n\nS3. Through experimental analysis, the authors identify the fundamental reason behind the success of CUP attacks as the LLM agents' difficulty in disentangling contexts."}, "weaknesses": {"value": "W1. MURMUR assumes that users only see messages corresponding to their own tasks. The authors justify this assumption by arguing that real humans also ignore messages from unrelated users and isolate the necessary context for themselves. However, in real-world scenarios, a small portion of users may still attend to unrelated messages addressed to them. Completely disregarding the existence of such users may undermine the reliability of the simulation results.\n\nW2. Similarly, MURMUR assumes that users only see the per-group view and are not influenced in their interactions by cross-talk. However, in the real world, a small subset of users may not conform to this assumption.\n\nW3. I cannot fully understand the task-based clustering defense strategy proposed in Section 6. This strategy dynamically clusters users into different task groups to restrict their context access. However, in Section 4.1, the authors have already assumed that users are assigned to different task groups by a mapping function g. It is unclear how these two grouping operations differ or relate to each other.\n\nW4. The following paper [a] proposes a security challenge named Cross-domain Context Bypass (C6), which appears to be similar to the CUP attack. I believe this paper should discuss the differences and connections between these two threats.\n\n[a] Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems."}, "questions": {"value": "Q1. Can MURMUR be easily extended to remove the two assumptions mentioned in W1 and W2?\n\nQ2. Please address the questions raised in W3 and W4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7ACAcZX0rJ", "forum": "wwXP9eqWeW", "replyto": "wwXP9eqWeW", "signatures": ["ICLR.cc/2026/Conference/Submission20985/Reviewer_NWdc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20985/Reviewer_NWdc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095515554, "cdate": 1762095515554, "tmdate": 1762999978851, "mdate": 1762999978851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}