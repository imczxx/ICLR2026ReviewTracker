{"id": "HqLrZ1YYdb", "number": 3727, "cdate": 1757507028623, "mdate": 1763629149131, "content": {"title": "ProCoSA: Probabilistic Concept Learning with Spatial Alignment", "abstract": "Concepts are human-interpretable semantic units that enable intervenable intermediate representations in vision models. However, acquiring concept annotations is expensive and typically incomplete, limiting scalable interpretability. We propose \\textbf{ProCoSA}, a probabilistic framework that treats missing concepts as latent variables and jointly infers concept posteriors and task predictions under partial supervision. To enhance spatial coherence and reduce pseudo-label bias, \\textbf{ProCoSA} introduces a spatial alignment prior that encourages concept activations to align with salient image regions, yielding more calibrated concept probabilities for downstream reasoning. The framework integrates seamlessly into existing concept-to-task pipelines without relying on any specific bottleneck architecture. Experiments on four benchmark datasets under low concept supervision show that \\textbf{ProCoSA} consistently matches or surpasses state-of-the-art methods on both concept and task performance under identical evaluation protocols. The code will be released upon acceptance.", "tldr": "ProCoSA probabilistically infers missing concepts via spatial alignment, producing calibrated concept signals that improve interpretability and downstream accuracy under sparse supervision.", "keywords": ["human–machine interaction，interpretable AI，concept bottleneck models，expectation–maximization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f30e4e1395be8fcceb660e00130a5cb5b978d57c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of learning interpretable concept representations under incomplete or missing concept supervision. To this end, the authors propose ProCoSA, a probabilistic framework that models missing concepts as latent variables and jointly infers concept posteriors and task predictions through an EM procedure. To ensure semantic grounding and reduce pseudo-label bias, ProCoSA introduces a spatial alignment prior that guides inferred concepts to align with salient image regions, supported by lightweight alignment and spatial-consistency regularization. Extensive experiments demonstrate the effectiveness of the proposed ProCoSA."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Compared with previous heuristic pseudo-label propagation approaches (e.g., SSCBM), ProCoSA models concept uncertainty in a Bayesian inference framework, enabling more robust and principled learning of the concept space.\n* The authors discuss the related literature in considerable detail."}, "weaknesses": {"value": "1. Interpretability: Although the proposed method shows accuracy improvements over existing approaches, the authors lack sufficient evaluation of the method’s interpretability. Only a few qualitative visualizations are provided. The authors should conduct both qualitative and quantitative analyses to verify that the learned concepts consistently and accurately correspond to the intended semantic regions.\n2. Method: The paper uses only ResNet as the feature extraction backbone. The authors should include additional architectures such as ViTs to further demonstrate the generality of the proposed method.\n3. Written: This paper is poorly written, with incorrect citation formatting and an unclear presentation of Figure 1.\n4. Code: The authors do not provide code for reproducibility check."}, "questions": {"value": "My questions and concerns are in Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zXswOAp6l8", "forum": "HqLrZ1YYdb", "replyto": "HqLrZ1YYdb", "signatures": ["ICLR.cc/2026/Conference/Submission3727/Reviewer_nBqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3727/Reviewer_nBqg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735061706, "cdate": 1761735061706, "tmdate": 1762916949467, "mdate": 1762916949467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ProCoSA, a method for CBMsthat addresses sparse concept annotations. The core idea is to treat missing concepts as latent variables and infer them using an EM framework. This process is guided by a spatial alignment prior, derived from the cosine similarity between spatial features and concept embeddings, and is supported by two simple regularizers. The model is trained end-to-end, using the posterior mean from the E-step to update the model in the M-step. On four standard CBM datasets, ProCoSA demonstrates improved performance over existing methods in low-label regimes and shows a clear, monotonic improvement in concept intervention curves."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、The paper's use of EM to handle missing labels is a clean, probabilistic alternative to prior heuristic methods like pseudo-labeling.\n\n2、he gating mechanism for the spatial prior is a smart design choice, effectively mitigating noise by applying constraints only when confidence is high.\n\n3、The method shows clear performance gains in low-data regimes and achieves the monotonic intervention curves expected of a well-formed CBM."}, "weaknesses": {"value": "1、Limited Novelty. The paper's core contribution is swapping the k-NN pseudo-labeling from SSCBM with an EM framework. This feels like an incremental methodological refinement rather than a significant conceptual leap.Self-Referential Prior. \n\n2、The spatial alignment prior is not independent, as it shares the same feature backbone with the concept head. This creates a circular problem where a feature is used to generate a prior that in turn constrains itself.\n\n3、The central claim of providing \"more reliable posterior uncertainty\" is unsubstantiated：1）The paper is missing key experiments on confidence calibration (e.g., ECE, Brier score) to actually prove this；2）the motivation relies on aligning concepts to visual evidence, but the paper lacks any quantitative localization metrics to show this is happening effectively beyond a few qualitative examples.\n\n3）the evaluation is restricted to standard benchmarks,to properly test the method's stability, the analysis should include robustness tests (e.g., against input noise/occlusions) or a small cross-domain experiment."}, "questions": {"value": "1、Key Comparison Details Buried in Appendix. The paper claims a direct comparison with SSCBM under a consistent protocol, but crucial experimental details are relegated to the appendix, making this claim difficult to verify from the main text alone.\n\n2、Central Claim of \"Better Uncertainty\" is Unproven. The core selling point is that the method produces more reliable uncertainty, yet the paper provides no quantitative evidence. Key metrics like Expected Calibration Error (ECE) or a selective risk analysis are completely missing.\n\n3、Self-Referential Prior. The alignment prior is derived from the same backbone features it is meant to guide. This is a significant methodological flaw, as the prior provides no external information and risks simply amplifying the model's own biases.\n\n4、Localization Claim Lacks Quantitative Support. The assertion that the spatial prior improves concept localization is backed only by qualitative heatmaps. A strong claim like this requires quantitative validation (e.g., pointing accuracy or other localization metrics).\n\n5、Limited Robustness Evaluation. The experiments are confined to standard CBM benchmarks. The paper is missing any analysis of the model's robustness to domain shifts or input perturbations (e.g., noise, occlusions), making its real-world stability unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6h8vix2nvH", "forum": "HqLrZ1YYdb", "replyto": "HqLrZ1YYdb", "signatures": ["ICLR.cc/2026/Conference/Submission3727/Reviewer_53yr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3727/Reviewer_53yr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952177555, "cdate": 1761952177555, "tmdate": 1762916948759, "mdate": 1762916948759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ProCoSA, a probabilistic framework for concept-based interpretable learning under partial concept supervision. The motivation is to treat missing concept labels as latent variables and jointly infer concept probabilities and task predictions using an Expectation-Maximization (EM) approach. To improve spatial consistency and reduce pseudo-label bias, ProCoSA introduces a spatial alignment prior that encourages concept embeddings to align with salient image regions. The method is evaluated on 4 datasets (CUB, AwA2, WBCatt, Derm7pt) under low concept supervision ratios (5%–20%), showing consistent improvements in both concept and task accuracy over existing methods like CBM, CEM, and SSCBM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. ProCoSA treats missing concepts as latent variables and uses variational inference within an EM framework, providing a principled alternative to heuristic pseudo-labeling. Besides, it explicitly models concept uncertainty, which is often overlooked in existing concept bottleneck models (CBMs)\n\n2. ProCoSA provides concept-level saliency maps that align well with human intuition. Besides, it also supports test-time intervention by allowing concept correction, demonstrating causal alignment between concepts and task predictions."}, "weaknesses": {"value": "1. Missing analysis on the computational overhead: the EM loop with multiple fixed-point iterations per E-step may increase training time compared to simpler pseudo-labeling methods, though this is not quantified.\n\n2. While ablations show the impact of alignment and spatial losses, more analysis on the sensitivity to hyperparameters would be useful;\n\n3. ProCoSA lacks a theoretical justification for the convergence of the truncated EM algorithm or the quality of the variational approximations"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lij2D9iPTI", "forum": "HqLrZ1YYdb", "replyto": "HqLrZ1YYdb", "signatures": ["ICLR.cc/2026/Conference/Submission3727/Reviewer_5mwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3727/Reviewer_5mwT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972596483, "cdate": 1761972596483, "tmdate": 1762916948263, "mdate": 1762916948263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}