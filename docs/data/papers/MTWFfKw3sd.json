{"id": "MTWFfKw3sd", "number": 3408, "cdate": 1757419548177, "mdate": 1763559171874, "content": {"title": "Unsupervised Invariant Risk Minimization", "abstract": "We propose a novel unsupervised framework for Invariant Risk Minimization (IRM), extending the concept of invariance to settings where labels are unavailable. Traditional IRM methods rely on labeled data to learn representations that are robust to distributional shifts across environments. In contrast, our approach redefines invariance through feature distribution alignment, enabling robust representation learning from unlabeled data. We introduce two methods within this framework: Principal Invariant Component Analysis (PICA), a linear method that extracts invariant directions under Gaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep generative model that disentangles environment-invariant and environment-dependent latent factors. Our approach is grounded in a novel class of unsupervised structural causal models and supports environment-conditioned sample generation and transfer. Empirical evaluations on synthetic benchmarks, modified versions of MNIST and CelebA demonstrate the effectiveness of our methods in capturing invariant structure, preserving relevant information, and generalizing across environments without access to labels.", "tldr": "We propose and study Invariant Risk Minimization (IRM) in the context of unsupervised learning", "keywords": ["Unsupervised Learning", "Invariant Risk Minimization", "Variational Autoencoder", "Principal Components Analysis"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc56b9dc50e6f86b4f2e193c9d3cf6e5cd30e5bd.pdf", "supplementary_material": "/attachment/08fc466211d6ecbce42ace3a08179f00e0bedc17.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes two unsupervised algorithms inspired by the invariant risk minimization (IRM) paper.  PICA (principal invariant component analysis) is in fact quite constrained and therefore less interesting than VIAE (variational invariant auto-encoder) which works by splitting the latent into an invariant component and an environment dependent component.  VIAE can also be used for environment transfer and to possibly recover IRM"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The notion of unsupervised invariant algorithms is compelling\n- The VIAE approach seems promising (as illustrated by the environment transfer algorithm.)."}, "weaknesses": {"value": "- Some notations are confused (see below)\n- PICA is very constrained because the difference of two empirical covariance matrices $\\Sigma_1-\\Sigma_2$ is likely to have a null kernel.  VIAE is far more satisfactory.\n- Experiments in section 4.2.1 are insufficient. For instance, you could construct a CMNIST problem with labels that only depend on the shape of the digit, and a RevCMNIST problem using the same patterns but with labels that only depend on the color. The invariant features in the sense of IRM would be the shape for CMNST and the color for RevCMNIST. But since both datasets have the same input patterns, the unsupervised approach cannot make this difference."}, "questions": {"value": "Imprecise notations: \n- line 88. $(X,e)\\sim P_X^e(X)$.  If $P^e_X$ is a distribution over $X$, then it does not generate pairs $(X,e)$\n- line 91. What's the relation between $P^e_X$ and $P_X$ ?\n- line 98. Should there be an additional sum or an empirical average on the data for each environment.\n- line 102. If the definition of $P(X,Y)$ implicitly depends on $\\Phi$, why write $P(X,\\Phi(X))$ in line 98?\n\nQuestions:\n- What is the relation of PICA and CCA (canonical correlation analysis, Hoteling 1936)?\n  Hoteling is also the inventor of PCA btw.\n- Isn't PICA very constrained as the difference of two empirical convariance matrices  $\\Sigma_1-\\Sigma_2$ can easily have a null kernel.\n- Line 286. Why not the other direction $P_e(Z_{inv},Z_e|X)=P(Z_{inv}|X) P_e(Z_e|Z_{inv},X)$ ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pjyuNeaFzn", "forum": "MTWFfKw3sd", "replyto": "MTWFfKw3sd", "signatures": ["ICLR.cc/2026/Conference/Submission3408/Reviewer_XTfo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3408/Reviewer_XTfo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760644605366, "cdate": 1760644605366, "tmdate": 1762916710099, "mdate": 1762916710099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes unsupervised Invariant Risk Minimization (IRM), extending the IRM framework to settings without labels by redefining invariance through feature distribution alignment across environments. The authors introduce two methods: Principal Invariant Component Analysis (PICA), a linear Gaussian approach that identifies invariant directions via null-space projections, and Variational Invariant Autoencoder (VIAE), a deep generative model that factorizes latent representations into environment-invariant (Z_inv) and environment-specific (Z_e) components. The framework is evaluated on synthetic data, modified MNIST variants (SMNIST, SCMNIST), and preliminary experiments on CelebA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The problem formulation--extending IRM to unlabeled multi-environment data--appears to be novel.\n- The two-method approach (linear PICA + nonlinear VIAE) provides complementary perspectives with clear mathematical exposition.\n- The fairness application demonstrates a natural use case where environment-invariant features correspond to removing sensitive attributes."}, "weaknesses": {"value": "- One of the core weaknesses is that the objective is conceptually ill-defined. The paper redefines invariance as matching the marginals of the representations across environments but does not justify how this invariance serves IRM's goal of robust prediction. For instance, the learnt invariant features could be useless for any feasible downstream tasks. \n- Connecting to the previous point, there is no identifiability analysis of the learned representation; the model could learn arbitrary rotations of the invariant features [see 1] and there is no theoretical or empirical justification along those lines. I am also unsure if the learned representation would be coherent in all cases, for instance when data distribution is unbalanced across environments (as an extreme case, consider all 3's in one environment, all 1's in another). \n- The paper is missing extremely relevant prior literature on learning disentangled representations [1, 2, 3, 4] and fairness via disentanglement [11, 12, 13], as well as less critical but appropriate citations works on causal representation learning, IRM and domain generalization [5, 6, 7, 8, 9, 10], among others. Similarly, links between section 4.2 and references in domain adaptation should be fleshed out.\n- Empirical justification is very limited: (1) MNIST-based simple datasets, no baselines - zero comparison to disentangled representations such as $ \\beta $-VAE, or supervised IRM methods (2) No quantitative disentanglement metrics, evaluation relies on visual inspection; (3) Section 4.3's claim that 84% linear probe accuracy \"validates\" the approach lacks context without supervised baselines showing achievable performance.\n\nOverall: The paper explores an interesting question but suffers from fundamental conceptual ambiguity (what is \"invariance\" without labels?) and insufficient empirical rigor (simplistic datasets, no baselines, no disentanglement metrics, no supervised comparisons). \n\nReferences:\n\n[1] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations - Locatello et al. 2019  \n[2] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework - Higgins et al., 2017  \n[3] Isolating Sources of Disentanglement in Variational Autoencoders - Chen et al, 2018  \n[4] Disentangling by Factorising - Kim et al, 2018  \n[5] Towards Causal Representation Learning - Schölkopf et al., 2021  \n[6] Weakly Supervised Disentangled Generative Causal Representation Learning - Shen et al, 2022  \n[7] On Learning Invariant Representations for Domain Adaptation - Zhao et al, 2019  \n[8] Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization - Sagawa et al, 2020  \n[9] Learning Optimal Features via Partial Invariance - Choraria et al., 2023  \n[10] Context is Environment - Gupta et al, 2023  \n[11] Learning Fair Representations - Zemel et al., 2013  \n[12] Flexibly Fair Representation Learning by Disentanglement - Creager et al., 2019   \n[13] On the Fairness of Disentangled Representations - Locatello et al., 2019"}, "questions": {"value": "- Can the authors provide some grounded justification for distributional equivalence?\n- Can the authors demonstrate a practical use-case, backed with results, of the presented method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5meKVoEK4Z", "forum": "MTWFfKw3sd", "replyto": "MTWFfKw3sd", "signatures": ["ICLR.cc/2026/Conference/Submission3408/Reviewer_wMwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3408/Reviewer_wMwF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686610500, "cdate": 1761686610500, "tmdate": 1762916709936, "mdate": 1762916709936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an invariance principle for unsupervised learning, where an optimal reconstruction is sought from a latent representation that is constrained to be identically distributed along the training environments. The principle is demonstrated on Gaussian data, then a tailored VAE architecture for the problem is presented and demonstrated on MNIST variants and CelebA.\nThe empirical results show the model learns latent representations that hold domain invariant features, and another latent representation that holds domain dependent features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall I liked the topic of the paper, the discussion presented and the developed methods.\nI think there are some original contributions that are presented clearly, and the work could attract some interest from crowd interested in these topics."}, "weaknesses": {"value": "There are some apparent weaknesses that I think the authors should take into account when revising the paper:\n1. The motivation for the problem is not entirely clear. \"Risk minimization\" is a term mostly used in the context of a prediction problem, and I think the unsupervised setting is inherently different, hence a more suitable name for the work or solution might be something like an Invariant autoencoder/Environment-Invariant autoencoder etc. Now given this framing, it is not entirely clear what one has to gain from invariance in the unsupervised case, and what is the spurious correlation present at training time. Are we expecting the reconstruction error to be stable or min-max optimal on new environments, or some other form of robustness? The unclarity about motivation is especially apparent in section 4.2 where the authors link the problem back to supervised learning. I was not fully able to follow the claims in this section, and I think some more conceptual clarity is required either via math (as suggested in point 3 below), or a more convincing empirical problem. \n2. The idea seems quite similar to works from the domain adaptation and generalization literature, like DICA, DANN, CORAL, and Domain Separation Networks [1, 2, 3, 4]. Some of them are more closely related than others to the work under review, but it seems important that the authors refer to these works and explain the conceptual differences. Other than the presence of a label, the invariance principle where one wishes to learn a representation $\\Phi(X)$ such that $P_{e}(\\Phi(X)) = P_{e'}(\\Phi(X))$ for each $e,e'\\in{\\mathcal{E}_{train}}$ is shared across these works.\n3. The paper could benefit from some additions like a mathematical result demonstrating that the approach achieves some well-motivated desirable property on the linear-gaussian setting. Another interesting aspect could be to discuss the causal version of the \"unsupervised\" graph in Fig. 1, i.e. $Z_e \\leftarrow X \\rightarrow Z_{inv}$ and derive the corresponding architecture.\n4. Finally, a clear drawback is that experiments are performed in rather small datasets and carefully designed problems that follow the assumptions of the method.\n\nSmall comments: It might be worthwhile enlarging Figure 1 and explaining the terms FIIF and PIIF formally. There are several places that use terms like \"recover the causal structure\" (line 248), \"causality constraints\" etc. I think it's better to keep the word invariance rather than causality, because recovering the causal structure might allude some readers to think the work tried to do causal discovery or some sort of structure learning, which is not the case. \n\nOverall, while I have several reservations about the paper, I gave an overall score of borderline accept and will reconsider it upon the authors' response. \n\n[1] Bousmalis, Konstantinos, et al. \"Domain separation networks.\" Advances in neural information processing systems 29 (2016).\n[2] Muandet, Krikamol, David Balduzzi, and Bernhard Schölkopf. \"Domain generalization via invariant feature representation.\" International conference on machine learning. PMLR, 2013.\n[3] Sun, Baochen, Jiashi Feng, and Kate Saenko. \"Correlation alignment for unsupervised domain adaptation.\" Domain adaptation in computer vision applications. Cham: Springer International Publishing, 2017. 153-171.\n[4] Sicilia, Anthony, Xingchen Zhao, and Seong Jae Hwang. \"Domain adversarial neural networks for domain generalization: When it works and how to improve.\" Machine Learning 112.7 (2023): 2685-2721."}, "questions": {"value": "What is the conceptual difference from methods mentioned in the \"weaknesses\" part? The presence of a label is a technical difference, but the motivation for replacing the label with a reconstruction loss seems somewhat weak.\n\nIt is mentioned that a separate encoder is trained for each environment. Is it actually a separate set of weights being trained, and if so then why? It seems more natural to train an encode that takes a one hot encoding of the environment, and will leverage the larger combined dataset to train its weights, while also possible learning similarities between the domains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EzpbfBh59J", "forum": "MTWFfKw3sd", "replyto": "MTWFfKw3sd", "signatures": ["ICLR.cc/2026/Conference/Submission3408/Reviewer_vC27"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3408/Reviewer_vC27"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872133344, "cdate": 1761872133344, "tmdate": 1762916709734, "mdate": 1762916709734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}