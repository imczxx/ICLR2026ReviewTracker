{"id": "404TzkOCUD", "number": 21911, "cdate": 1758323480547, "mdate": 1759896897191, "content": {"title": "WARP: Weight Teleportation for Attack-Resilient Unlearning Protocols", "abstract": "Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to both the original and unlearned models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forgotten samples and the close proximity of the unlearned model to the original. To demonstrate their severity, we design unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (such as NGP and SCRUB) remain vulnerable.\n\nTo mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce gradient energy of forgotten samples and increase parameter dispersion while preserving accuracy. This reparameterization hides the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or to recover them through reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage by up to 64% in black-box settings and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for improving privacy in approximate unlearning.", "tldr": "A plug-and-play weight-teleportation step reduces alignment with forget-set gradients while preserving predictions, cutting MIA/DRA success across unlearning methods without harming retain-set accuracy.", "keywords": ["Machine unlearning", "Approximate unlearning", "Neural teleportation", "Weight-space symmetries", "Privacy attacks", "Membership inference", "Model inversion", "Data reconstruction"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4aa296c007af7a75a38619b5828516c42428c0b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces WARP, a defense against privacy attacks on machine unlearning. The core contribution is a method to mitigate the risk of an attacker discovering what data was forgotten by comparing a model before and after the unlearning process. WARP accomplishes this by using inherent symmetries in neural networks to teleport the model's weights to a functionally identical but distant point in the parameter space after unlearning. This engineered change effectively masks the information-rich updates from the unlearning process, empirically demonstrating a strong defense against both black-box and white-box reconstruction attacks while preserving the model's utility."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a new defense mechanism for machine unlearning by using neural network symmetries.\n- Demonstrates strong empirical success in thwarting both black-box and white-box privacy attacks.\n- Effectively secures the unlearning process without causing a significant drop in model accuracy."}, "weaknesses": {"value": "The authors correctly acknowledge that the SVD step introduces computational overhead but suggest this cost can be amortized. However, for truly large-scale models with very wide layers (e.g., modern LLMs), this cost can be prohibitive even if amortized. Could the authors provide a more concrete analysis of the method's scalability, perhaps commenting on the practical wall-clock time for models with billions of parameters, and elaborate on whether the proposed amortization strategies are sufficient to make WARP truly practical for industrial-scale applications?\n\nThe paper's privacy analysis is based on strong empirical results against specific attacks. An alternative approach to privacy is using a formal framework like Differential Privacy (DP), for instance, by adding calibrated noise to the unlearning gradients. Could the authors elaborate on the trade-offs between their empirical defense and a formal one? Specifically, if one were to apply DP to achieve a similarly low Attack Success Rate, what would be the anticipated cost in model utility compared to the high utility maintained by WARP?\n\nThe paper claims WARP is a \"plug-and-play\" module, yet only validates this with gradient-based unlearning. How does WARP's performance and compatibility extend to other unlearning paradigms, such as those based on influence functions or data partitioning methods (e.g., SISA)?\n\nThe experiments are conducted using relatively small forget-sets (e.g., ~1% of a class). While this is a common scenario, how would the method perform under a more large-scale unlearning request where, for instance, 20% or 50% of the training data must be forgotten? Does the clear distinction between the retain subspace and its null-space still hold when the retain set shrinks significantly?"}, "questions": {"value": "* How does the SVD's computational overhead scale for industrial-sized models, and is the proposed amortization sufficient for practical application?\n* What is the anticipated model utility cost of using formal Differential Privacy to achieve the same low attack success rate that WARP provides?\n* How does WARP's performance and compatibility extend to non-gradient-based unlearning paradigms like those using influence functions or data partitioning?\n* How does WARP's effectiveness hold up under large-scale unlearning requests where a significant fraction of the training data must be forgotten?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pd8BIUTgo6", "forum": "404TzkOCUD", "replyto": "404TzkOCUD", "signatures": ["ICLR.cc/2026/Conference/Submission21911/Reviewer_puub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21911/Reviewer_puub"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761219734534, "cdate": 1761219734534, "tmdate": 1762941978030, "mdate": 1762941978030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A machine unlearning algorithm that provides defense for reconstruction attacks is introduced in the paper. By applying layer-wise scaling and permutation transformations that preserve the model’s function but relocate its parameters in space, WARP ensures that the parameter difference between the pre-unlearning and post-unlearning models no longer aligns with the forgotten data’s gradients. Experiment results shows that across membership inference and gradient reconstruction attacks, the proposed algorithm reduces privacy leakage with negligible accuracy loss and minimal computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed algorithm uses inherent network symmetries to randomize parameter space without retraining or noise injection. It is easy to integrate into any unlearning pipeline, introduces negligible computational costs, and preserves model accuracy. \n\n2. The authors evaluated their algorithm across multiple datasets, unlearning algorithms, and attack types. The inclusion of detailed ablations on transformation modes and frequencies supports the robustness and generality of the proposed defense."}, "weaknesses": {"value": "1. The described process involves per-layer SVDs and null-space projections, which can be expensive for larger models.\n\n2. The work does not provide a theoretical explanation of how teleportation changes the information relationship between parameters and training data. A formal analysis would make the contribution more rigorous. \n\n3. Symmetries are by design invertible. If an attacker can recreate or approximate the teleportation transform (which they probably can given the strong threat model described in the paper), is it possible that they can work their way up and subtract out it's influence to recover the residual forget gradient?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Ct9oCbnX8", "forum": "404TzkOCUD", "replyto": "404TzkOCUD", "signatures": ["ICLR.cc/2026/Conference/Submission21911/Reviewer_5bBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21911/Reviewer_5bBC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983626140, "cdate": 1761983626140, "tmdate": 1762941977827, "mdate": 1762941977827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a privacy defense method, WARP, for machine unlearning. Despite being efficient and effective, approximate unlearning methods can leak information about the unlearned instance when an adversary analyzes the difference between models before and after unlearning. To tackle this issue, WARP leverages neural network symmetries to keep updates near a loss-invariant set for the retain data, and to suppress gradients from the forget set. It then applies layer-wise projections to teleport parameters in function-preserving directions. Experiments combining WARP with unlearning methods showing reduced attack success under both black-box and white-box settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tWARP can be applied to both CNNs and ViTs, showing that it does not rely on any particular neural network architecture.\n2.\tWARP is designed to be a plug-in method and can be adopted with many unlearning methods, as demonstrated in the experiments.\n3.\tThe improvements of WARP are consistent across all the tasks and unlearning methods."}, "weaknesses": {"value": "1.\tEach layer’s retain subspace is built from a randomly sampled retain minibatch, where this small batch can misrepresent the full retain set when the retain set is highly diverse. This could result in an inaccurate retain subspace and hinder defense effectiveness.\n2.\tThe paper describes teleportation as “leaving the task loss unchanged up to numerical error”, which is in fact an approximation of a loss-invariant transformation on the retain set. Yet there is no analytical worst-case bound on the retain loss drift. Also, the claim “loss-invariant” may not be accurate since it is not strictly unchanged.\n3.\tThe evaluation in this paper centers around with and without WARP but does not compare against any privacy defenses. Since the contribution of this paper is privacy defense in machine unlearning, it would be better to show how WARP outperforms other privacy defense methods, rather than only showing gains when plug WARP into many unlearning methods."}, "questions": {"value": "Can WARP be applied to popular NLP tasks with LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7qkuPtPYKB", "forum": "404TzkOCUD", "replyto": "404TzkOCUD", "signatures": ["ICLR.cc/2026/Conference/Submission21911/Reviewer_NyvK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21911/Reviewer_NyvK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014778168, "cdate": 1762014778168, "tmdate": 1762941977567, "mdate": 1762941977567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the privacy risk in approximate machine unlearning (MU), where adversaries can exploit differences between pre- and post-unlearning models to reconstruct forgotten data. To mitigate this, the authors propose WARP, a plug-and-play defense that leverages neural network symmetries to reduce the forget-set's gradient signal and increase parameter distance, thus obfuscating the unlearning process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tConnecting neural network symmetry with unlearning privacy is a novel conceptual contribution.\n2.\tThe evaluation uses strong, adaptive attacks (U-LiRA, and a custom white-box attack), making the defense's evaluation robust."}, "weaknesses": {"value": "1.\tSignificant Computational Overhead is the most significant drawback. The core of this method (see Algorithm 3) requires computing the SVD for the retainer activation matrix $R_l$ of each layer in each transport step. SVD is a computationally intensive operation. The analysis in Appendix J (Figure 8) shows that this adds an average of +27% to the runtime overhead.\n2.\tThis method introduces a large number of new and seemingly highly sensitive hyperparameters: transmission step size $\\eta_{tel}$, tradeoff coefficient $\\beta$, rank $k$ preserved by SVD, transmission frequency $S$, etc. This is further evidenced by the discussion of the privacy-utility tradeoff in Appendix I (as shown in Figure 7).\n3.\tIn Table 1, for most-memorized samples, NGP+WARP still has an AUC of 0.598, and BT+WARP has an AUC as high as 0.865, which is far from reaching the \"immune\" level.\n4.\tThe entire method is heuristic. It provides no formal privacy guarantees (e.g., in relation to Differential Privacy). It is unclear if a stronger, adaptive attacker aware of WARP could defeat it.\n5.\tThe reported 92% white-box AUC reduction appears to be a best-case scenario. For other baselines (like NGP or Salun), the improvement is marginal. The paper also admits the method can worsen privacy for SCRUB under certain conditions"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RTwYuBeNTV", "forum": "404TzkOCUD", "replyto": "404TzkOCUD", "signatures": ["ICLR.cc/2026/Conference/Submission21911/Reviewer_J66i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21911/Reviewer_J66i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120727351, "cdate": 1762120727351, "tmdate": 1762941977331, "mdate": 1762941977331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}