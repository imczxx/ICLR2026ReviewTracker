{"id": "ChfIF8UZ6X", "number": 17808, "cdate": 1758280765177, "mdate": 1759897152346, "content": {"title": "Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors", "abstract": "Existing data poisoning attacks on retrieval-augmented generation (RAG) systems scale poorly because they require costly optimization of poisoned documents for each target phrase. We introduce Eyes-on-Me, a modular attack that decomposes an adversarial document into reusable **Attention Attractors** and **Focus Regions**. Attractors are optimized to direct attention to the Focus Region. Attackers can then insert semantic baits for the retriever or malicious instructions for the generator, adapting to new targets at near zero cost. This is achieved by steering a small subset of attention heads that we empirically identify as strongly correlated with attack success. Across 18 end-to-end RAG settings (3 datasets $\\times$ 2 retrievers $\\times$ 3 generators), Eyes-on-Me raises average attack success rates from 21.9 to 57.8 (+35.9 points, 2.6$\\times$ over prior work). A single optimized attractor transfers to unseen black box retrievers and generators without retraining. Our findings establish a scalable paradigm for RAG data poisoning and show that modular, reusable components pose a practical threat to modern AI systems. They also reveal a strong link between attention concentration and model outputs, informing interpretability research.", "tldr": "We introduce EYES‑ON‑ME, a modular RAG‑poisoning attack that learns reusable Attention Attractors to steer a small set of high‑correlation attention heads toward a swappable payload Focus Region, enabling near‑zero‑cost adaptation to new baits.", "keywords": ["RAG", "database poisoning", "adversarial attack", "LLM safety", "attention-steering"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c2c8e7c9ab0efae6a516c2a5a32e101c3615dee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EYES-ON-ME, a novel and modular framework for data poisoning attacks on Retrieval-Augmented Generation (RAG) systems. The core idea is to decouple the adversarial document into a reusable \"Attention Attractor\" and a swappable \"Focus Region\" (containing the payload). Instead of optimizing for a final log probability, the authors propose a proxy objective: maximizing the attention scores from a pre-identified subset of \"influential\" heads onto this Focus Region. The paper claims this modular approach is more effective, scalable, and transferable than previous end-to-end optimization methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of this paper is the novelty of its proposed attack vector.\n\n1. **Modular, Reusable Framework**: The decomposition of the attack into a payload-agnostic \"Attention Attractor\" and a separate \"Focus Region\" is a clever idea. It increases transferability and addresses a key limitation of prior work, which required costly re-optimization for each new attack payload.\n\n2. **Attention-Based Proxy Objective**: The proposal to optimize for attention concentration from specific, influential heads, rather than a simple end-to-end log probability, is a well-motivated and interesting contribution.\n\n3. **Position-Independence**: As shown in Section 6.3 (Table 5), this new objective results in an attack that is significantly more robust to the poisoned document's position in the context (0.39% G-ASR variance) compared to log-prob-based methods like Phantom (3.46% variance). This stability is a notable improvement."}, "weaknesses": {"value": "1. **Critical Inconsistency in Main Results**: There is a contradiction between the paper's main results (Table 1) and its ablation study (Table 4b).\n- Section 5.1 states that the Table 1 experiments use triggers with a corpus frequency of 0.5-1%.\n- Table 4(b) analyzes the effect of trigger frequency. It shows that R-ASR (retrieval success) drops precipitously as frequency increases. For the 0.1%-0.5% range, R-ASR is 30.09%, and for 1%-5%, it is 3.0%.\n- This implies the R-ASR for the 0.5-1% range used in Table 1 can't be higher than 30.09%.\n- However, Table 1 reports End-to-End ASR (E2E-ASR) values as high as 82.04%, 64.90%, and 87.50%. By definition, E2E-ASR cannot be higher than R-ASR. It is impossible for the retrieval to succeed <30.09% of the time while the entire attack succeeds 82% of the time. Can you explain this contradiction?\n2. **Lack of Reranker in Threat Model**: The paper's threat model, consisting only of a retriever and a generator, is not representative of modern, realistic RAG systems. Most production-grade RAG pipelines use a reranker model between the retriever and generator to improve the quality of the top-k documents. A strong reranker may easily filter out the adversarial documents.\n3. **Limited Scope of Evaluation**: The main results appear to be based on only five trigger phrases (per dataset), as mentioned in Section 5.1. Given the extremely high variance in transferability shown in Table 2(c) (ranging from 28% to 100%), an evaluation on only five triggers is insufficient to make broad claims of generalizability. It is possible these five triggers were cherry-picked for their high performance.\n4. **Limited Applicability (Trigger Rarity)**: Linked to Weakness 1, Table 4(b) shows that the attack is only truly effective for exceptionally rare triggers. The R-ASR drops from 85.35% for triggers in the <0.05% frequency range to just 40.4% for the 0.05%-0.1% range. This suggests the attack's applicability is limited to triggers that are very rare, weakens its practical threat level."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ielkS3KVVc", "forum": "ChfIF8UZ6X", "replyto": "ChfIF8UZ6X", "signatures": ["ICLR.cc/2026/Conference/Submission17808/Reviewer_CLDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17808/Reviewer_CLDF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790821454, "cdate": 1761790821454, "tmdate": 1762927652309, "mdate": 1762927652309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors studied poisoning attacks for RAG. They proposed a new attack algorithm that generates adversarial documents to attract both the retriever’s and generator’s “attention” so that the final output contains the attacker’s desired content. In particular, they proposed to decompose the adversarial documents into two parts: 1) attention attractors and 2) focus regions. The attention attractor is a series of tokens that wrap the focus region, and the focus region are the slots that contains the actual malicious content, which the attackers want the RAG to output. They performed a few experiments to demonstrate the effectiveness of their proposed attacks and also a few ablation studies to understand how different factors, such as, attention attractor initialization, attractor length and malicious instruction, will affect the ASR of the proposed attack."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "They proposed a seemly novel poisoning attack for RAG and the numerical results look good. They also validated their attack against a few existing defense algorithms."}, "weaknesses": {"value": "I listed a few weaknesses as the follows:\n1. It is a bit hard to follow and understand how the proposed attack is actually work. It will be good to provide a more detailed overview of the framework than just Fig. 2 with only an example. Maybe a pseudocode can explain the framework in a more rigorous way? \n2. Since there is not really a detailed overview of the proposed framework, it is a bit hard to understand a few key steps of the framework. For example, 1) how to compute Eq. (4)? 2) What is $l$ and $h$? 3) For $d_m$, which part of the tokens are generated/computed first? Retriever first? Prefix first? Does the order matter? \n3. I am still not sure when using HotFlip to optimize the tokens, how large is the candidate pool? Why using random initialization, one will have a larger search space than structured tokens/national language?\n4. When looking at the examples of malicious documents generated by the proposed attack framework from Appendix E.4. It looks like the prefix and suffix are words that does not mean anything or random characters, such as, ` This barg kla important meas`. In this case, paraphrasing with a good LLM should correctly rewrite those parts? Wouldn’t that affect the attack’s performance? If not, then why do prefix and suffix matter?"}, "questions": {"value": "1. As $C_{mal}(r)$ is defined in Eq. (2), on Page 3, Equation (2) should be before line 142? \n2. The attack successful rates range presented on Table 3/4  looks much lower than those in Table 1/5. Why is that?\n3. It looks like the trigger frequency affects the ASR quite a bit, however, this is out of the attacker’s control because the attacker cannot access all the benign documents. Then, it is hard for the attackers to understand how many malicious documents they need to insert?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zspN3IvWJg", "forum": "ChfIF8UZ6X", "replyto": "ChfIF8UZ6X", "signatures": ["ICLR.cc/2026/Conference/Submission17808/Reviewer_NR6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17808/Reviewer_NR6W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947186089, "cdate": 1761947186089, "tmdate": 1762927651887, "mdate": 1762927651887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Eyes-on-Me, a novel modular data poisoning attack targeting retrieval-augmented generation (RAG) systems. Unlike traditional end-to-end optimization approaches, Eyes-on-Me decomposes each poisoned document into two reusable components: Attention Attractors and Focus Regions. The Attention Attractors contain the triggers that lure the retriever’s attention, while the Focus Regions embed the malicious instructions intended to manipulate the generator’s output. This modular design enables flexible combinations of triggers and malicious instructions, allowing attackers to adapt or repurpose poisoning strategies without retraining either the retriever or the generator. Extensive experiments and ablation studies demonstrate that Eyes-on-Me achieves superior effectiveness compared to state-of-the-art poisoning attack methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important and practically relevant problem—data poisoning attacks on retrieval-augmented generation (RAG) systems.\n\n2. The proposed idea of decomposing a malicious document into Attention Attractors and Focus Regions is particularly novel and insightful. This modular design not only enhances flexibility in composing new triggers and malicious instructions but also improves the scalability and adaptability of poisoning attacks.\n\n3. The experimental evaluation is thorough and well-executed.  The paper provides end to end, retriever specific and generator specific attack success rate, complemented by extensive ablation studies. These analyses systematically explore factors influencing attack success, efficiency, scalability, and performance under existing defenses. \n\n4. The results are also interesting. Although the primary experiments assume a white-box setting, where model parameters are known, the paper also demonstrates promising transferability of poisoned documents to black-box settings."}, "weaknesses": {"value": "1. Some aspects of the paper’s presentation could be clarified to improve readability. For example, the setup of the motivating experiment in Section 4.2 would benefit from a more detailed description. In addition, Equation (4) should explicitly specify the variables being optimized. For readers unfamiliar with the HotFlip method, providing a brief introduction or summary of this prior approach would also help contextualize the optimization procedure described in Section 4.3.\n\n2. In Section 5, which presents the main experimental results, the paper reports only the attack success rate (ASR). However, it does not include the model’s performance on benign (non-poisoned) queries or compare this performance to that of baseline methods. Since attack stealthiness, i.e., maintaining normal performance on benign inputs while achieving high ASR, is a critical aspect of data poisoning evaluation, including such analysis would strengthen the empirical validation of the proposed approach."}, "questions": {"value": "1. In section 4.2 on the proposed attack approach, what are tok(s_ret) and tok(s_gen)  in Eq.(4)? Since the method emphasizes flexibility in choosing triggers and instructions, are they some placeholder strings used in the optimization?\n\n2. Also in section 4.2, how is the number of influential heads determined? Is there a way for the attacker to control this number?\n\n3. Appendix E.2 shows that the produced attractors from the proposed approach include multiple Unicode strings, e.g., “\\u0626g”. How to make sure such attractors would survive simple pre-deployment filters or grammar/encoding checks, to ensure launching successful attacks in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nEnVQ9S6Z3", "forum": "ChfIF8UZ6X", "replyto": "ChfIF8UZ6X", "signatures": ["ICLR.cc/2026/Conference/Submission17808/Reviewer_EKSt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17808/Reviewer_EKSt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208494174, "cdate": 1762208494174, "tmdate": 1762927651415, "mdate": 1762927651415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present an attack on RAG pipelines which, given a trigger phrase, derives attention attractors for the retriever and generation model which increase attention to a target region and, combined with simple baseline payloads, form a strong attack on the RAG system."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The attack shows strong results, exceeding baselines. The retriever and generator attractors show transferability across models.\n- The attack is well-motivated; attention is known to be associated with strong attacks [1, 2].\n\n[1] Choudhary et al. Through the Stealth Lens: Rethinking Attacks and Defenses in RAG. 2025.\n\n[2] Hung et al. Attention tracker: Detecting prompt injection attacks in llms. 2024."}, "weaknesses": {"value": "- The universality of the attractors is limited; while the authors state that the payload is swappable, the results in 5.3 (c) indicate that the attractors have limited transferability across triggers.\n- While the attack is effective, it explicitly aims to create outliers in attention, which directly compromises stealth; there is existing work on attention-aware defenses [1, 2], and some discussion would be helpful.\n- In addition to GCG [3], AutoDAN [4] should be evaluated as a baseline attack.\n\n\n[3] Zou et al. Universal and Transferable Adversarial Attacks on Aligned Language Models. 2023.\n\n[4] Liu et al. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. ICLR 2024."}, "questions": {"value": "Could you clarify the differences in setting which explains the reduced performance of baselines such as GCG [3] and Phantom [5] relative to results presented in the original works. Section 5.2 notes that retrieval ranking may be critical; is this optimized for? What objective and hyperparameters were used?\n\nMore detail regarding the adversary's objective and the malicious instructions injected is needed for the results in Table 1.\n\nGiven the limited transferability across triggers of the retrieval attractor, additional experiments regarding the effect of payload variation on retrieval behavior would be useful to validate that the attractor operates purely through attention to the payload, rather than by promoting relevance to the trigger directly even in the presence of an unrelated payload.\n\n[5] Chaudhari et al. Phantom: General Backdoor Attacks on Retrieval Augmented Language Generation. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5vT6MsTzN", "forum": "ChfIF8UZ6X", "replyto": "ChfIF8UZ6X", "signatures": ["ICLR.cc/2026/Conference/Submission17808/Reviewer_bxvR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17808/Reviewer_bxvR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216499294, "cdate": 1762216499294, "tmdate": 1762927650890, "mdate": 1762927650890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}