{"id": "SzXDuBN8M1", "number": 10851, "cdate": 1758183330378, "mdate": 1759897624726, "content": {"title": "TD-JEPA: Latent-predictive Representations for Zero-Shot Reinforcement Learning", "abstract": "Latent prediction–where agents learn by predicting their own latents–has emerged as a powerful paradigm for training general representations in machine learning. In reinforcement learning (RL), this approach has been explored to define auxiliary losses for a variety of settings, including reward-based and unsupervised RL, behavior cloning, and world modeling. While existing methods are typically limited to single-task learning, one-step prediction, or on-policy trajectory data, we show that temporal difference (TD) learning enables learning representations predictive of long-term latent dynamics across multiple policies from offline, reward-free transitions. Building on this, we introduce TD-JEPA, which leverages TD-based latent-predictive representations into unsupervised RL. TD-JEPA trains explicit state and task encoders, a policy-conditioned multi-step predictor, and a set of parameterized policies directly in latent space. This enables zero-shot optimization of any reward function at test time. Theoretically, we show that an idealized variant of TD-JEPA avoids collapse with proper initialization, and learns encoders that capture a low-rank factorization of long-term policy dynamics, while the predictor recovers their successor features in latent space. Empirically, TD-JEPA matches or outperforms state-of-the-art baselines on locomotion, navigation, and manipulation tasks across 13 datasets in ExoRL and OGBench, especially in the challenging setting of zero-shot RL from pixels.", "tldr": "We propose a temporal-difference latent-predictive method for zero-shot unsupervised RL.", "keywords": ["zero-shot reinforcement learning", "unsupervised reinforcement learning", "self-predictive representations", "joint embedding predictive architecture"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db89b3aec874556ef9d950b3b42a03e6ddb1dab1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new state representation learning method called TD-JEPA. The main idea of TD-JEPA is to use TD learning to learn BYOL/JEPA-like self-predictive representations, where positives are sampled from geometrically distributed future states (in the MC case). Importantly, instead of learning representations w.r.t. the behavioral policy (like BYOL-$\\gamma$), the authors simultaneously learn a latent task embedding $z$ and train a $z$-conditioned policy and the corresponding representations, somewhat similarly to FB representations. The authors show that TD-JEPA representations enable zero-shot RL, outperforming previous zero-shot RL approaches on ExORL and OGBench. They also demonstrate that these representations can be effectively fine-tuned for offline-to-online RL."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a well-written paper with solid theoretical and empirical results. The proposed method is (to my knowledge) novel, even though there are a number of closely related (but different) works, such as FB and BYOL-$\\gamma$. The authors compare their method with previous methods across diverse categories, and convincingly demonstrate its effectiveness on a wide array of tasks and settings.\n\nAnother strength is that the authors provide a solid theoretical analysis of their method. They theoretically show that their method (more or less) low-rank approximates successor measures. With this connection, they are able to relate downstream performance to their loss. While I didn't exhaustively check the correctness of the proofs in Appendix, they appear to be solid and have some new, intriguing aspects of their own."}, "weaknesses": {"value": "I don't see any major weaknesses in this work. While I believe the current results are already enough for an ICLR publication, I think they could be further strengthened by demonstrating the performance on even more complex, \"new\" environments that go beyond the standard benchmarks used in previous work (Motivo is a good example of this, and I think this could also be done in a separate follow-up work). Another nitpick is that the related work section is placed in Appendix. Especially given that this area is (relatively) dense, I believe discussions about related work are essential to understanding this method, and would like to encourage the authors to move this section to the main paper in the final version."}, "questions": {"value": "- In Table 4, why do some of the settings have zero hidden layers?\n- While I understood that the TD-JEPA representation approximates the successor measure of $\\pi^z$ in a low-rank manner ($F_z^\\top B$) for a **fixed** $z$, can the authors explain what kind of behavior **set** this method would learn? In other words, are there any explicit descriptions of learned skills? For example, for HILP, one can describe its skills as those that maximally span the isometric latent embedding space. Do the authors have a similar intuitive (yet \"correct\") description of skills learned by TD-JEPA? (I guess this question directly applies to FB representations as well.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ULd0AqA1O", "forum": "SzXDuBN8M1", "replyto": "SzXDuBN8M1", "signatures": ["ICLR.cc/2026/Conference/Submission10851/Reviewer_ghUc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10851/Reviewer_ghUc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761276701833, "cdate": 1761276701833, "tmdate": 1762922051100, "mdate": 1762922051100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose TD-JEPA, a method for learning policy-dependent representations of long-term dynamics that can be used for zero-shot RL. They learn the representations by combining temporal difference methods with the self-predictive framework of JEPA. Across a number of experimental benchmarks, they show that TD-JEPA matches or outperforms existing baselines, especially on pixel-based environments. The authors also discuss a number of theoretical properties of the proposed algorithm."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written and clearly motivates the proposed algorithm. The experiments are comprehensive and show convincing performance. The proofs justify the algorithm theoretically and build on techniques used in previous works."}, "weaknesses": {"value": "I do not have any major weaknesses for this work, but I point out things I found interesting which could benefit from more detailed discussion (although I understand some of these points might be outside of the scope of this work).\n\n1. What is the computational cost of the different proposed methods? I know many of the considered methods train multiple function approximators, but it would be nice to have at least a brief discussion of the training speed for each method.\n2. I found the comparison between BYOL and TD-JEPA interesting. In particular, the contrast between modeling expert policies vs. policy conditional measures. I wonder if there are other simple analyses/visualizations that can pinpoint this down further, beyond performance on benchmarks. For example, is one representation more robust to noise / generalizes better (perhaps since TD-JEPA is better on pixel environments, this would be the case)?  \n3. For pixel-based environments, how do the visual features compare from using a purely visual pre-training strategy (MAE for example) to learning the encoders with TD-JEPA (or another RL method)?\n\nIn general, there seem to be a number of different \"representation learning\" methods that are useful for RL in the community. The authors already briefly do this in the discussion section, but continued conversation about the long-term desiderata for a representation, beyond performance on zero-shot benchmarks would be interesting for the community."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4vuNlVgxsA", "forum": "SzXDuBN8M1", "replyto": "SzXDuBN8M1", "signatures": ["ICLR.cc/2026/Conference/Submission10851/Reviewer_FzbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10851/Reviewer_FzbF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761440221821, "cdate": 1761440221821, "tmdate": 1762922050555, "mdate": 1762922050555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TD-JEPA targets learning latent policy dynamics models through successor features from a collection of reward-free off-policy data. Unlike prior work often limited to one-step predictions, this TD loss enables the model to learn representations predictive of long-term, policy-conditioned latent dynamics. The system trains four components directly in latent space: a state encoder ($\\phi$), a task encoder ($\\psi$), a policy-conditioned multi-step predictor ($T_\\phi$), and a set of policies ($\\pi_z$). The predictor learns to approximate successor features, which allows the agent to perform zero-shot optimization of any new reward function at test time."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "TD-JEPA introduces a method for learning long-term transition dynamics rather than single-step dynamics. There is extensive experimentation in simulated benchmarks across DMC and OGBench showing general improvement over other zero-shot RL baselines."}, "weaknesses": {"value": "Perhaps it is because I am not in the immediate area, but it was challenging for me to determine what the real-world impact of this method is. It certainly makes sense in relation to recent latent prediction models, but the introduction may benefit from some framing that takes a step back. Is there more application-facing work that has called for learning from large reward-free multi-task offline datasets that could be cited? And for zero-shot RL? If this doesn’t exist, why not yet?"}, "questions": {"value": "In the abstract, it is written “(TD) learning enables learning representations predictive of long-term latent dynamics across multiple policies…” . Would it be more accurate to say “long-term latent task/policy dynamics” instead?\n\nFor me, the notational assignments were difficult to maintain as I was reading through the relatively terse section 3. I wonder if there is a more distinguishing notation between \\psi and \\phi?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dT0XUdXS2N", "forum": "SzXDuBN8M1", "replyto": "SzXDuBN8M1", "signatures": ["ICLR.cc/2026/Conference/Submission10851/Reviewer_u6m8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10851/Reviewer_u6m8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762067741, "cdate": 1761762067741, "tmdate": 1762922049595, "mdate": 1762922049595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TD-JEPA, a latent-predictive representation learning method for zero-shot reinforcement learning (RL) that uses temporal difference (TD) learning to capture long-term multi-policy dynamics from offline, reward-free data. By training separate state/task encoders, policy-conditioned predictors, and parameterized policies in latent space, TD-JEPA enables zero-shot optimization of arbitrary reward functions—even for challenging pixel-based inputs. Theoretical guarantees and extensive experiments across 13 datasets (ExoRL/OGBench) validate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sound Motivation and Novel Method: The work addresses a key limitation of existing latent-predictive methods (single-task/one-step prediction, on-policy dependence) by leveraging TD learning for offline, multi-step, multi-policy dynamics modeling. Its design—separating state (low-level dynamics) and task (high-level context) encoders, plus TD-based losses and regularization—balances innovation and practicality, avoiding representation collapse while enabling offline training.\n2. Comprehensive and Convincing Experiments: TD-JEPA is evaluated across 65 tasks (locomotion, navigation, manipulation) with proprioceptive/pixel inputs. It matches or outperforms SOTA baselines. Ablations confirm the value of multi-step prediction and separate encoders, while fast adaptation results show pre-trained representations boost sample efficiency for fine-tuning.\n3. Rigorous Theoretical Foundations:\nThe paper provides solid theoretical support."}, "weaknesses": {"value": "TD-JEPA relies on FlowQ-like behavioral cloning regularization to handle low-coverage datasets in OGBench, but its performance under more extreme data scarcity (e.g., critical action gaps, sparse trajectories) is not fully validated. It would help to add analysis on how TD-JEPA’s performance decays as data coverage decreases, and compare it to methods specifically designed for low-quality offline data. This would better demonstrate its practical applicability to real-world scenarios where data is often incomplete."}, "questions": {"value": "In terms of methodology, could you further compare the approach proposed in this paper with that proposed by Motivo [1]?\n\n[1] Andrea Tirinzoni, Ahmed Touati, Jesse Farebrother, Mateusz Guzek, Anssi Kanervisto, Yingchen Xu, Alessandro Lazaric, and Matteo Pirotta. Zero-shot whole-body humanoid control via behavioral foundation models. ICLR, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8iQzosHaWT", "forum": "SzXDuBN8M1", "replyto": "SzXDuBN8M1", "signatures": ["ICLR.cc/2026/Conference/Submission10851/Reviewer_WTAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10851/Reviewer_WTAu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988942819, "cdate": 1761988942819, "tmdate": 1762922048794, "mdate": 1762922048794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}