{"id": "8iGclsodrJ", "number": 3547, "cdate": 1757471869791, "mdate": 1759898081904, "content": {"title": "PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation", "abstract": "Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals.\nIn this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences.\nExtensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.", "tldr": "", "keywords": ["User Preference Modeling", "Multimodal Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bb4de7c2876ef11071f290a357a574a486a2c9f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PREFGEN, a multimodal framework for preference-conditioned image generation that leverages a Multimodal Large Language Model (MLLM) to extract rich user representations from a few liked and disliked images. Its primary contribution is a novel approach that disentangles preferences into stable \"core identity\" and context-dependent \"semantic\" features through systematic probing tasks. To bridge the modality gap, the framework innovatively uses a Maximum Mean Discrepancy (MMD) based loss to align the semantic preference distribution with the diffusion model's text space. Experiments show that PREFGEN significantly outperforms strong baselines in both preference alignment and image quality, and the authors further contribute a new benchmark, PREFBENCH, for evaluating this task."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed PREFGEN framework is novel, technically sound, and intuitively motivated. The key insight of disentangling user preferences into a stable \"core identity\" (ecore) and a context-dependent \"semantic preference\" (esem) is particularly strong. Furthermore, the use of a Maximum Mean Discrepancy (MMD) loss for distributional alignment is a clever and well-justified choice.\n2. A major contribution of this work is the creation of a large-scale dataset and a dedicated benchmark. By generating a dataset of nearly one million images from over 50,000 simulated users, the authors enable robust training and evaluation at a scale. The creation of the PREFBENCH benchmark is also highly valuable to the community, as it provides a standardized testbed for evaluating and comparing future methods.\n3. The empirical evaluation is thorough and convincing. This includes \n- 1) benchmarking against a comprehensive set of recent and relevant baselines (e.g., IP-Adapter, ViPer, EasyRef); \n- 2) ablation study on the effect of MLLM embeddings (Table 3, Figure 6), the impact of distribution alignment (Figure 7), the choice of alignment loss (Figure 10), and different fusion strategies (Figure 11); and \n- 3) insightful analyses, such as the layer-wise probing to identify where different preference signals emerge in the MLLM (Figure 2, Table 1) and the t-SNE visualizations of the learned embeddings (Figures 8, 9).\n4. The paper is exceptionally well-written and presented. The motivation is clear, the methodology is explained in a step-by-step manner, and the figures and tables are of high quality."}, "weaknesses": {"value": "1. The proposed PREFGEN framework, while effective, is a complex, multi-stage pipeline that involves several large, distinct models. Replicating the results from scratch would require significant computational resources and expertise to manage and integrate these different components. While the system is trained on a large dataset, the complexity of the pipeline itself might be a bottleneck for scaling up further.\n2. The core of the training data (nearly 1M images) is generated by agents whose preferences are defined by a set of textual attributes. While this is a clever approach to achieve scale, it raises a question about the ecological validity of these preferences. Real human aesthetic taste is often nuanced, inconsistent, and difficult to articulate, whereas an AI agent might adhere to its programmed \"profile\" with unnatural consistency.\n3. The paper provides a comprehensive evaluation of image quality and preference alignment but does not discuss the computational overhead at inference time."}, "questions": {"value": "1. The PREFGEN framework is quite complex. Could you comment on its modularity? Specifically, how critical are the specific choices of IDEFICS2 for feature extraction and the proprietary Claude-3.5 for data generation? Have you explored if more accessible, open-source models could be substituted without a major performance degradation, thereby improving the framework's reproducibility?\n2. The model is predominantly trained on data from simulated agents. While this is a pragmatic solution for scaling, how confident are you that the learned preference patterns generalize well to the often inconsistent and nuanced tastes of real humans? Have you performed any small-scale user studies where real users provide their own organic image histories to test the model's performance in a more naturalistic setting?\n3. The paper does not include an analysis of the computational overhead at inference time. Could you please provide a comparison of the inference latency (e.g., wall-clock time to generate one image) and VRAM usage against key baselines like the standard SDXL and IP-Adapter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5XBz46oz3g", "forum": "8iGclsodrJ", "replyto": "8iGclsodrJ", "signatures": ["ICLR.cc/2026/Conference/Submission3547/Reviewer_P4d3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3547/Reviewer_P4d3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760569546977, "cdate": 1760569546977, "tmdate": 1762916810840, "mdate": 1762916810840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PrefGen, a method for personalizing text-to-image generation models at the individual user level. The approach leverages multimodal large language models to extract user-specific preference embeddings. These embeddings are then aligned with diffusion model latent spaces, enabling the diffusion model to be conditioned on user preference vectors. Experimental results demonstrate that this method improves both image quality and alignment with user preferences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Fig 4 has nice examples of potential use cases i.e., showing that PrefGen can be used for product design or character design, that can beyond purely image generation.\n- The paper was well written and easy to follow."}, "weaknesses": {"value": "- Generalization to new (OOD) users/preferred images: The results evaluates on unseens users, from the same distribution as those in the training data. It would be interesting to also see how well the method does with user preferences and their corresponding liked/disliked images that are different from what was used during training (generated images), i.e., real images, photographs, sketches etc. \n- Preference history: One way to make the paper stronger could be to consider each user having a preference history i.e., more diverse time-vary preferences, rather than static ones.\n- Preference interpretation skewed toward style/color: The generated results tend to reflect style and color preferences more strongly than semantic preferences. It may be helpful to constrain or normalize style/color attributes (e.g., fixed color palette or uniform art style) to better reveal how well PrefGen captures semantic preferences.\n- There could be more thorough motivation and clarifications on design choices, otherwise they seem arbitrary:\n    - Size and composition of H_u: The paper does not clearly discuss how to choose the size of the user preference set H_u. Does its size depend on the diversity of the userâ€™s interests? Should the number of positive and negative examples be balanced?\n    - Necessity of separate embeddings e_sem,e_core,e_img: The paper states that e_sem captures semantic alignment, e_core represents stable identity traits, and e_img provides fine-grained visual cues. However, it remains unclear why this decomposition is necessary. Furthermore, e_img is computed based on an image the user likes. Wont this overfit to that image? What happens if the user has diverse \npreferences?\n    - Rationale for multi-user identification: The paper suggests learning embeddings jointly across multiple users. It would be helpful to clarify why this is necessary. How does performance differ if embeddings are learned independently for each user?\n\n- Minor suggestions.\n    - It could be useful to add a table that summarizes the input requirements of the methods compared in the paper. For example: Whether a method uses a single reference image (e.g., IP-Adapter) vs. multiple user images (e.g., ViPer) or wether the method uses only positive examples or both positive and negative examples. Such a table would make trade-offs across methods easier to compare at a glance.\n    - Fig 1 shows what happens during training, it could also be useful to show what happens at inferences time, inputs needed, if or when the linear classifiers need to be retrained."}, "questions": {"value": "- How well does the method generalize to users with different preferences and images inputs seen during training?\n- Why are the seperate embeddings (e_sem, ...) necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BriciGhDOd", "forum": "8iGclsodrJ", "replyto": "8iGclsodrJ", "signatures": ["ICLR.cc/2026/Conference/Submission3547/Reviewer_n3n1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3547/Reviewer_n3n1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560293721, "cdate": 1761560293721, "tmdate": 1762916810432, "mdate": 1762916810432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PREFGEN, a personalized multimodal generation framework that learns individual aesthetic preferences by disentangling stable user traits from context-specific semantics. It leverages dual-branch preference embeddings and distribution-level alignment to condition diffusion models, achieving superior visual quality and personalization over baselines such as IP-Adapter, StyleAligned, and ViPer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem definition is clear.\n2. It uses distribution-alignment losses (MMD) for robust embedding learning.\n3. It demonstrates comprehensive comparisons across six personalization baselines.\n4. It introduces the large-scale PREFBench dataset with synthetic and real user clusters"}, "weaknesses": {"value": "1. From an overall perspective, this paper presents an engineering-oriented work, employing rather straightforward methodologies such as MMD. The motivation behind the study lacks clarity.\n2. The dataset over-relies on virtual \"user clusters,\" which may inflate controllability and impact real human diversity. The ratio between synthetic and real data should be adjusted to validate the efficacy.\n3. The ablation studies on MMD and disentanglement stability should be strengthened.\n4. Aesthetic analysis is a subjective task. More human evaluation should be introduced."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3fxYeeRGt5", "forum": "8iGclsodrJ", "replyto": "8iGclsodrJ", "signatures": ["ICLR.cc/2026/Conference/Submission3547/Reviewer_YgBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3547/Reviewer_YgBn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572442738, "cdate": 1761572442738, "tmdate": 1762916810107, "mdate": 1762916810107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PREFGEN, a multimodal framework for preference-conditioned image generation. The key idea is to leverage multimodal large language models (MLLMs) to learn rich, user-specific embeddings that capture both aesthetic style and semantic preferences, and then integrate these into diffusion-based generative models. The model performs hierarchical analysis of MLLM layers to disentangle user identity traits and semantic preferences, followed by a distribution alignment module based on Maximum Mean Discrepancy (MMD) to map representations into the text embedding space of diffusion backbones. Experiments on both synthetic and real-user datasets demonstrate that PREFGEN achieves strong improvements in image quality and preference alignment, outperforming multiple personalization baselines like IP-Adapter, ViPer, and EasyRef."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces an elegant multimodal framework that systematically disentangles and aligns user-specific preference signals from different layers of an MLLM, providing conceptual clarity and technical novelty.\n* The proposed MMD-based alignment is a well-motivated alternative to rigid point-wise alignment losses, leading to more stable and generalizable conditioning across diffusion backbones.\n* Extensive experiments, including a new benchmark (PREFBENCH) and human evaluations, show consistent, significant improvements over strong baselines, with convincing qualitative and quantitative evidence."}, "weaknesses": {"value": "* The reliance on a large synthetic agent-generated dataset raises concerns about ecological validity and generalization to truly diverse human preferences, which is only partially addressed by the smaller real-user subset.\n* While the method demonstrates strong results, the added complexity of multimodal probing, dual discrimination tasks, and distribution alignment increases implementation burden and may limit reproducibility.\n* The paper lacks a deeper theoretical or ablation-based explanation of why the hierarchical decomposition into ecore and esem works beyond empirical observation, leaving interpretability somewhat heuristic."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VwHcOdBnB1", "forum": "8iGclsodrJ", "replyto": "8iGclsodrJ", "signatures": ["ICLR.cc/2026/Conference/Submission3547/Reviewer_ovRe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3547/Reviewer_ovRe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996334735, "cdate": 1761996334735, "tmdate": 1762916809626, "mdate": 1762916809626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}