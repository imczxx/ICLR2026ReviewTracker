{"id": "mJSL9qU8DW", "number": 19158, "cdate": 1758293957991, "mdate": 1759897055945, "content": {"title": "SecureLLM: Using Inference-time Compositionality to Build Secure Language Models for Private, Sensitive, and Secret Data", "abstract": "As Large Language Models (LLMs) increasingly support critical sectors such as healthcare, finance, and public governance, ensuring data confidentiality and robust access control is a pressing societal challenge. Traditional security mechanisms isolate sensitive resources from unauthorized users, yet existing LLM safety approaches often fail to enforce strict segregation of confidential data. In this work, we introduce SecureLLM, a novel compositional framework for building provably secure large language models (LLMs) that integrates fine-tuning with traditional access security measures to protect private information. By fine-tuning LLMs on segregated, “siloed” training data and composing their outputs at inference time based solely on a user’s verified credentials, SecureLLM not only prevents unauthorized data leakage but also enables accurate responses for complex queries spanning multiple data silos. Our method is demonstrated on a challenging natural-language-to-SQL translation task and is designed with real-world applications in mind—supporting sectors where protecting sensitive information is paramount.", "tldr": "We introduce an information security method for LLMs that relies on inference-time composition to protect private data without the need for probabilistic guardrails", "keywords": ["Generative Models", "Adapter Composition", "Model Privacy", "Model Security"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35a4352613155f53c9b25b485ea37695304b7780.pdf", "supplementary_material": "/attachment/d259f2113319ab71d76c7a02d601d9da24ee4ffc.zip"}, "replies": [{"content": {"summary": {"value": "To tackle security vulnerability of LLMs, this paper proposes SecureLLM which is a compositional access-control framework. In SecureLLM separate LoRA-tuned models are trained on disjoint siloed datasets and then composed only at inference according to a user’s verified credentials. The main aim of paper is to guarantee that responses cannot incorporate information from unauthorized silos while still answering cross-silo queries. By embedding security directly into the LLM’s architecture, SecureLLM aims to provide a scalable solution for sectors such as healthcare, finance, and public governance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper formalizes a security objective tailored to specific settings in which the composed model must answer only from authorized silos and support cross-silo queries without ever training on unauthorized entities. The Logit Composition rule requires no extra training and potentially outperforms LoraHub and PEM Addition on cross-silo NL2SQL."}, "weaknesses": {"value": "While the paper frequently states provably secure, it provides no cryptographic, information-theoretic, or formal security proof that composition prevents leakage beyond credentialed silos. The security argument mainly relies on data isolation at training time and runtime selection, not on formal noninterference properties. Empirically, there is no test of membership-inference or training-data extraction on individual silos or the composed system. \nThe paper claims several application domains such as healthcare, finance, and public governance can benefit from this study. However, the benchmark is largely synthetic and does not even slightly examine real-world datasets split into silos, nor other tasks that can support realistic ambiguity, distribution shift, and noisy access policies. \nThe comparisons include LoraHub, PEM Addition, and Adapter averaging, but omit more recent studies on model/adapter merging and collaborative LLMs. It is crucial to atleast provide an expanded empirical position vs. current research studies on merging/ensembling would strengthen claims of novelty.\nOverall, this paper aimed to tackle a very interesting problem. However, the solution is not well-founded and is not theoretically supported."}, "questions": {"value": "-Can you specify an explicit threat model (adversary capabilities, query access, collusion across users), a noninterference-style property for authorized vs. unauthorized silos, and a leakage metric (e.g., information flow)? Can you show that, under your inference-time routing and per-silo training, the composed distribution over outputs is independent of unauthorized silos? If not, provide detailed list of assumptions under which your claims hold.\n-Can you explore and include real NL2SQL (e.g., Spider) partitioned into legal silos, and evaluate end-to-end execution accuracy? \n-Can you include recent model-merging and adapter-collaboration baselines (e.g., task arithmetic variants, sparse/TIES merging, ensemble-with-router methods, and strong baselines recommended in recent studies)?\n-Can you provide latency/throughput analyses as the number of silos grows? \n-Can you elaborate more on the distinction of used terms “Private, Sensitive, and Secret”? They have different meaning in the security context and have been used without sufficient context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LAVja2THEE", "forum": "mJSL9qU8DW", "replyto": "mJSL9qU8DW", "signatures": ["ICLR.cc/2026/Conference/Submission19158/Reviewer_jmMA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19158/Reviewer_jmMA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761091145493, "cdate": 1761091145493, "tmdate": 1762931168627, "mdate": 1762931168627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the SecureLLM framework, which constructs provably secure large language models through compositional fine-tuning. To enable cross-silo reasoning while preserving privacy, the research team introduced two novel composition methods: Logit Composition and Maximum Difference. Experimental results demonstrate that this approach achieves performance close to insecure baselines on cross-silo query tasks while maintaining strict access control, validating the effectiveness of the compositional security framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a novel and practical approach to LLM security by combining compositional fine-tuning with access control. The idea of composing silo-specific models at inference is original and well-motivated. The Secure-NL2SQL task is a valuable benchmark for compositional reasoning. Experiments are thorough and demonstrate clear advantages over existing composition methods. The framework is scalable and has high potential impact in privacy-sensitive domains."}, "weaknesses": {"value": "1. The evaluation is limited to NL2SQL, with no validation on broader tasks like QA or dialogue. \n\n2. The claim of \"provable security\" lacks formal proof or threat modeling. Scalability to larger models and real-time inference costs are not analyzed."}, "questions": {"value": "1. How does Logit Composition handle cases where multiple silos produce high-confidence but conflicting outputs?\n\n2. How does inference time and memory usage scale as the number of accessible silos increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OX8zwfJ3jx", "forum": "mJSL9qU8DW", "replyto": "mJSL9qU8DW", "signatures": ["ICLR.cc/2026/Conference/Submission19158/Reviewer_cHke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19158/Reviewer_cHke"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879069126, "cdate": 1761879069126, "tmdate": 1762931168157, "mdate": 1762931168157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SecureLLM, a compositional framework for building provably secure language models that integrate access control into model design. It fine-tunes separate LLMs on isolated data silos and composes them at inference time based on user credentials, ensuring authorized access while supporting cross-silo queries. Using a natural-language-to-SQL task, the authors show that their proposed Maximum Difference and Logit Composition methods outperform prior approaches like LoRAHub and PEM Addition, achieving strong compositional generalization and data security."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Studies an interesting and timely problem with reasonable novelty.\n\n2. Designs a challenging NL2SQL task to evaluate compositionality across disjoint data silos."}, "weaknesses": {"value": "1. The proposed methods are highly heuristic with no theoretical or empirical justification for why they should work; even the underlying intuition is unclear.\n\n2. The experimental setup is limited to a single synthetic NL2SQL task and one Llama-2-7B model, lacking real-world relevance and generality.\n\n3. The results are confusing: theoretically, different composition methods should only affect cross-silo performance (e.g., $S_{1∪2}$, $S_{1∪3}$), since each individual silo model is fine-tuned separately. However, Table 1 shows varying results even on single silos ($S_1$, $S_2$, $S_3$), which should remain identical across methods. This inconsistency requires further explanations.\n\n4. The writing quality is poor: (1) Section 3.2’s method descriptions are unclear and could be formalized mathematically; (2) the experimental setup omits important details (See questions below); (3) several typos and presentation issues reduce readability.\n\nAdditionally, the paper contains several issues in presentation and tone, with some degree of overstatement. The use of the term “provably” is not appropriate, as the paper does not provide any formal proof or theoretical guarantee of security—it merely demonstrates empirical isolation through compositional inference. Moreover, the claim in the final paragraph that the proposed method is “generic and can be applied to numerous other domains” is overly strong and unsupported, given that no experiments beyond the synthetic NL2SQL task are presented."}, "questions": {"value": "1. In Section 4, please clarify the structure of each database: how many columns do the 2–3 tables have, and what kind of data do they contain? The data generation process for the 100,000 pairs is also unclear—how were these generated, and how do they differ from the 300 “realistic” pairs?\n\n2. In Table 1, why are accuracy values reported only for the first two methods, while the others only show tree edit distance?\n\n3. (Minor) There are several typos, e.g., L37 “LLM can convinced” → “LLM can be convinced”; L320 “it's knowledge” → “its knowledge.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xSRBJ3ntMx", "forum": "mJSL9qU8DW", "replyto": "mJSL9qU8DW", "signatures": ["ICLR.cc/2026/Conference/Submission19158/Reviewer_kiko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19158/Reviewer_kiko"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891136710, "cdate": 1761891136710, "tmdate": 1762931167614, "mdate": 1762931167614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed SecureLLM, a framework that enables provably secure LLMs by integrating traditional access control with fine-tuned model composition. It allows LLMs trained on separate data silos to be securely combined at inference time based on user credentials, preventing unauthorized data leakage while maintaining accuracy on complex, cross-silo queries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper introduces a compositional mechanism that uses fine-tuning LLMs on access-controlled data silos, which are to be combined at inference time to prevent unauthorized data leakage.\n2.\tIntegrates the traditional concept of securing the confidential data in the LLM security domain to protect the privacy of the personally identifiable information in the LLM systems."}, "weaknesses": {"value": "1.\tSince the fine-tuning depends on the pre-defined sources, how does it generalize to the other domains that do not exist in the sources?\n2.\tAs lines 77-84 describe its deployments, how can the SecureLLM be employed in real-world scenarios for generalized cases?\n3.\tAs per the paper's position, how does the proposed framework perform against any of the privacy leakage attacks that reveal personal information? The evaluation should have been illustrated under some real scenarios of attacks. \n4.\tWhy was llama-2-7 b chosen to fine-tune? What justifies its use, given that there are many other advanced models after LLaMA-2 with more capabilities? \n5.\tHow does the integration of the proposed method impact the inference time?"}, "questions": {"value": "Please follow the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sscnpuzpFa", "forum": "mJSL9qU8DW", "replyto": "mJSL9qU8DW", "signatures": ["ICLR.cc/2026/Conference/Submission19158/Reviewer_syhN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19158/Reviewer_syhN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940044712, "cdate": 1761940044712, "tmdate": 1762931167090, "mdate": 1762931167090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}