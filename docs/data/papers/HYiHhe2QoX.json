{"id": "HYiHhe2QoX", "number": 12540, "cdate": 1758208472060, "mdate": 1759897503019, "content": {"title": "Generalization in Monitored Markov Decision Processes (Mon-MDPs)", "abstract": "Reinforcement learning (RL) typically models the interaction between the agent and environment as a Markov decision process (MDP), assuming fully observable rewards. In many real-world settings, this assumption fails, motivating the monitored Markov decision process (Mon-MDP), where rewards may be unobserved. Prior work on Mon-MDPs has been limited to simple, tabular cases, restricting their applicability to real-world problems. This work explores Mon-MDPs using function approximation and investigates the challenges. We show that combining function approximation with a learned reward model enables agents to generalize from monitored states with observable rewards to unmonitored states with unobservable rewards. Therefore, we demonstrate that such generalization with a reward model achieves near-optimal policies in environments formally defined as unsolvable. However, we also uncover a critical limitation: agents may incorrectly extrapolate rewards due to *overgeneralization*, which can lead to undesirable behaviors. To mitigate overgeneralization, we propose a cautious policy optimization method leveraging reward uncertainty. This work serves as a step towards bridging the gap between Mon-MDP theory and real-world applications.", "tldr": "Extend Monitored-MDPs to non-tabular environments using function approximation and investigates the associated challenges.", "keywords": ["Reinforcement learning", "Monitored-MDPs", "Partially observable rewards", "Generalization", "Function approximation", "Caution", "Partial Observability", "RL Safety", "DQN", "CVaR", "Reward Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3cd00407762a03ada6ab8b619778a27a3e2e37b.pdf", "supplementary_material": "/attachment/bb01f20f5a6b7f33a4fa4f25f6ae51be896ab712.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the integration of function approximators into traditional tabular (monitored) MDPs to enhance scalability. The authors effectively demonstrate how value function approximators can improve an agent‚Äôs performance in various scenarios within a plant-watering environment. However, there are notable conceptual and methodological gaps in the proposed approach. In fact, the authors contradict several of their claims, as outlined in the weaknesses section."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "‚Ä¢\tIntegrating traditional tabular MDPs with function approximators such as deep Q-networks.\n‚Ä¢\tExploring several scenarios in the plant irrigation example."}, "weaknesses": {"value": "‚Ä¢\tThere are multiple statements that sound contradictory to each other, which I point out in Question section. \n‚Ä¢\tIn Section 4.3 (line 366), the authors claim the agent never observes the penalty. However, in the next paragraph, they state that the agent learns to avoid it. If the agent learns from the penalty, it must have been exposed to it, implying the overall reward is observable.\n‚Ä¢\tThe authors state that traditional monitored MDPs operate in the tabular setting and that their function approximator generalizes the method to non-tabular settings. While this may hold theoretically, all presented experiments are still conducted in large but finite tabular domains. If the approach is intended for non-tabular MDPs, experiments in continuous state‚Äìaction spaces should have been included.\n‚Ä¢\tThe authors claim that traditional methods are limited to finite state and action spaces (Conclusion section). However, their own experiments are restricted to such finite spaces, which weakens this argument.\n‚Ä¢\tThe paper asserts that the proposed approach is near-optimal, yet no theoretical guarantees or performance bounds are provided. Given that it is tested on a single application, its generalizability to other domains remains uncertain."}, "questions": {"value": "1. In the main contributions on page 2, the authors claim that their approach can achieve near-optimal performance for unsolvable monitored MDPs. However, on page 3, they state that there is no optimal policy for unsolvable MDPs. If no optimal policy exists, it is unclear how their approach can be characterized as near-optimal.\n2. In Section 3, the reward function is said to be trained on observable states. However, it is unclear how this trained model can accurately estimate rewards for unobservable states, particularly when their rewards differ substantially from those of observed states.\n3. The reward function $r^E$ follows a simple deterministic rule in Section 4. How would your approach handle a stochastic reward function?\n4. Is the optimal policy known in your examples? On what basis do you evaluate near-optimality?\n5. For training the reward model, what happens when only a limited number of states are observable? Would the performance remain stable under sparse observability?\n6. You mention robustness to overgeneralization. Could you clarify this? Does introducing a penalty bias the agent toward conservative, worst-case behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CTGJSc3Muu", "forum": "HYiHhe2QoX", "replyto": "HYiHhe2QoX", "signatures": ["ICLR.cc/2026/Conference/Submission12540/Reviewer_PqK8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12540/Reviewer_PqK8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705338478, "cdate": 1761705338478, "tmdate": 1762923403550, "mdate": 1762923403550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the Monitored MDP framework (Parisi et al., AAMAS 2024) beyond the tabular regime into function approximation settings, thereby improving its practical applicability. The framework considered in this paper allows partially unobservable rewards. For this, the paper proposes to combine a neural reward model based on DQN function approximation. The paper empirically investigates the proposed approach. The authors found that function approximation with reward modeling may recover near-optimal policies even under unobserved rewards. In addition, interestingly, they also discovered that function approximation may induce an issue of overgeneralization, where agents incorrectly extrapolate rewards. Motivated by this, they extend the \"learning to be cautious\" originally designed for traditional MDPs to the setting of Mon MDPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper extends the concept of Mon-MDPs to practically relevant settings based on function approximation, e.g., DQN. \n- The paper discovers the issue of overgeneralization when applying the framework. It provides a quantitative definition of overgeneralization and analyzes it based on numerical experiments.\n- The paper establishes empirical demonstrations that combine function approximation and reward modeling. The result is very interesting in that the framework can find a near-optimal policy even for instances that are formally considered to be unsolvable.\n- Experiments are well-designed, and risk-averse RL, along with the \"learning to be cautious\" techniques, are carefully adopted."}, "weaknesses": {"value": "- The paper has no new theoretical results, although it may not be the main scope of this paper. \n- Experiments are illustrative, but they are confined to grid-worlds. Perhaps further validations in continuous or high-dimensional control domains would be desired. However, to be fair, the numerical experiments consider instances with a larger scale, in comparison with the AAMAS paper."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c7agHX8MKo", "forum": "HYiHhe2QoX", "replyto": "HYiHhe2QoX", "signatures": ["ICLR.cc/2026/Conference/Submission12540/Reviewer_AuFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12540/Reviewer_AuFX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983498704, "cdate": 1761983498704, "tmdate": 1762923402917, "mdate": 1762923402917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Monitored MDPs (Mon-MDPs), where the environment is endowed with a separate monitoring (also a MDP) process \nThe prior work only dealt with tabular settings of the Mon-MDPs; this paper extends prior tabular treatments by pairing function approximation (FA) with a learned reward model. The paper has a number of simple-to-explain experiments to show generalization (i.e. positive effect of reward extrapolation to unmonitored regions of the environment ) and also a failure mode (where rewards when extrapolated to unmonitored regions lead to unintended consequences). As a remedy, they propose risk-averse policy optimization to act cautiously."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The Mon-MDP setting is clearly defined. Although I do not clearly follow the impact, utility of the same specially in the example and experiments of the authors (more on this below)\n\n2) The set of experiments ,while preliminary ,seems to be a good starting point in exploring this line of research\n\n3)The paper is well written in the sense, at many places it explains the scope of its results and settings and does not seem to overclaim results."}, "weaknesses": {"value": "1) Disconnect between the model and the experiments\n\nWhile the Mon-MDP formalism is general, the experiments instantiate only degenerate monitors, either deterministic spatial gating of observability or a binary ask/no-ask with fixed cost. These settings could be modeled without a stateful monitor MDP, and there are some papers that do just that. To justify the general model, I recommend adding scenarios with nontrivial monitor dynamics (at least giving examples where stateful monitor MDP makes sense, e.g., even the watering robot example in the introduction is vague and not clear what the monitor MDP is in that example) or, alternatively, narrowing the claims to the studied special cases.\n\n2) Role of $r_M$ is very non-intuitive\n\nBecause the objective maximizes $r_ùëÄ+r_E$ , the monitor reward acts as an arbitrary scalarization of two objectives. \n=0; the paper gives no guidance for selecting $r_M$. (e.g. Binary monitor example or no-monitor experiments can be really different with different $r_M$ selection, again in the watering robot example, what is $r_M$ ) . For now, I think it is just reflective ofthe  cost of observation or something similar\n\n3) About Contribution 3\n\n The ‚Äòovergeneralization‚Äô result (watering cacti unseen during training) is unsurprising given that rewards for cacti are never revealed. As framed, it reads more like an instance of standard distribution shift than a novel Mon-MDP phenomenon. The paper does add a metric and a cautious policy mitigation, but the underlying failure case is expected. One way to circumvent it will be to make the failure non-obvious. e.g., create unmonitored states that are feature-similar to monitored ones (not a new class), show the ratio spikes anyway, then show the mitigation helps; or show a case where FA surprisingly doesn‚Äôt fail due to representation structure.\n\n\n4)What is not tested is the reward misspecification under the mon-MDP setting. In all the experiments, whenever the agent can observe the $r_E$, it observes it accurately."}, "questions": {"value": "1) Please see the weaknesses too\n\n2) The parametrization is kind of implicit, i.e. architecture implicitly defines the function classes. It is unclear whether the results are artifacts of the architecture or the chosen parameterization. Can you confirm whether an architecture search or ablation study is required to provide evidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o0W169q6cU", "forum": "HYiHhe2QoX", "replyto": "HYiHhe2QoX", "signatures": ["ICLR.cc/2026/Conference/Submission12540/Reviewer_5Zm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12540/Reviewer_5Zm1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027142118, "cdate": 1762027142118, "tmdate": 1762923402301, "mdate": 1762923402301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the use of nonlinear function approximation coupled with a learned reward model for monitored Markov decision processes (Mon-MDPs). The authors argue that their approach improves generalization, as it allows agents to generalize from monitored states with observable rewards to unmonitored states with unobservable rewards. The caveat is that agents may sometimes overgeneralize, as they inaccurately extrapolate rewards. The authors corroborate these findings with empirical evaluation"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The Mon-MDP framework is relatively recent, and has not been investigated in adequate depth. This paper makes a first effort to shed light on some critical challenges and limitations of Mon-MDPs, while also discussing ways to mitigate them (e.g., by learning a reward model).\n- The experimental study contains targeted experiments that are able to confirm the authors' findings."}, "weaknesses": {"value": "- In my view, the novelty is limited. The authors essentially explore the use of function approximation and a learned reward model for Mon-MDPs. This is a straightforward idea without substantial innovation. Function approximation has been known for a long time to improve the RL performance and is nowadays an almost indispensable component of modern reinforcement learning systems. Adding it to Mon-MDPs makes a lot of sense, but is otherwise straightforward. Likewise, the learned reward model makes a lot of sense in this setting, but it's otherwise not a substantial innovation.\n- There is no theory, e.g., with respect to the learned reward model. The authors try to corroborate their findings through their empirical evaluation, but the paper lacks formal theory.\n- The experiments mainly study relatively simple MDPs. It was not clear why the authors did not try more complex MDPs. Perhaps the Mon-MDP setting is not so broad?\n- The mitigation strategy discussed in the paper for overgeneralization is in my view one of the most interesting parts of this work, but it is only covered rather superficially."}, "questions": {"value": "- Have the authors considered more complex MDPs, where they may even need to apply more involved RL algorithms? Perhaps somehow more realistic MDPS?\n- The authors could have explored mitigation strategies in more detail. No details for instance are provided regarding CVaR and k-of-N CFR. Why the authors decide to use an approximation of CVaR and not the actual CVaR, and also what motivated their use of k-of-N CFR? None of that is covered in the paper. I personally feel the paper would have benefited a lot from a more careful treatment of the phenomenon of overgeneralization and the possible mitigation strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CsPqJRrigZ", "forum": "HYiHhe2QoX", "replyto": "HYiHhe2QoX", "signatures": ["ICLR.cc/2026/Conference/Submission12540/Reviewer_5gM9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12540/Reviewer_5gM9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188963374, "cdate": 1762188963374, "tmdate": 1762923401925, "mdate": 1762923401925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies function approximation for monitor Mon-MDPs and evaluates its performance across several environments based on the plant-watering setup. The experimental results show that Mon-MDP performs better than several baselines that do not incorporate the Mon-MDP structure with FA. The results also indicate that FA enables better generalization to unseen states when the reward function remains unchanged, but it may lead to over-generalization when the underlying reward function changes. The paper further demonstrates that employing a curiosity-driven learning method can help mitigate over-generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper effectively demonstrates the usefulness of function approximation in the context of Mon-MDPs.\n\n- It shows that incorporating robust policy optimization can help handle the over-generalization problem caused by the epistemic uncertainty of the environments."}, "weaknesses": {"value": "- My main concern lies in the novelty and significance of the proposed approach. The algorithm appears to be a combination of several existing techniques, including Mon-MDP, function approximation, and the \"learning to be curious\" framework to handle out-of-distribution data. While the paper provides extensive experimental results in the plant-watering environment, the findings are not particularly surprising. The advantages of Mon-MDP over baselines have already been shown in the tabular setting. Moreover, the generalization benefits of function approximation are well-known and acknowledged by the authors (line 165). Regarding over-generalization, it is unclear whether this issue is unique to Mon-MDPs or is a general phenomenon associated with FA in standard MDPs. In my understanding, the observed over-generalization likely results from the entirely new patterns not captured in the observed data, a problem typically addressed by robust policy optimization methods. In this sense, it is not evident what new insights or contributions this paper provides to the community.\n\n- The experiments were conducted exclusively in the plant-watering environments. To strengthen the empirical findings, it would be beneficial to include results from more diverse and complex environments."}, "questions": {"value": "- What is the novelty of this work beyond combining Mon-MDP, function approximation, and curiosity-driven learning?\n\n- What unique insights does this study offer about the Mon-MDP problem? (Please refer to the first point of weakness for more details)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2f15VcZ3x8", "forum": "HYiHhe2QoX", "replyto": "HYiHhe2QoX", "signatures": ["ICLR.cc/2026/Conference/Submission12540/Reviewer_ZiWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12540/Reviewer_ZiWN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762325385010, "cdate": 1762325385010, "tmdate": 1762923401544, "mdate": 1762923401544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}