{"id": "uUghQYGVKO", "number": 14974, "cdate": 1758246396791, "mdate": 1759897338132, "content": {"title": "Symbal: Detecting Systematic Misalignments in Model-Generated Captions", "abstract": "Multimodal large language models (MLLMs) often introduce errors when generating image captions, resulting in misaligned image-text pairs. Our work focuses on a class of captioning errors that we refer to as systematic misalignments, where a recurring error in MLLM-generated captions is closely associated with the presence of a specific visual feature in the paired image. Given a vision-language dataset with MLLM-generated captions, our aim in this work is to detect such errors, a task we refer to as systematic misalignment detection. As our first key contribution, we introduce SymbalBench, the first benchmark designed to evaluate automated methods for identifying systematic misalignments. SymbalBench consists of 420 vision-language datasets from two domains (natural images and medical images) with annotated systematic misalignments. As our second key contribution, we present Symbal, which utilizes a structured, dual-stage setup with off-the-shelf foundation models to identify such errors and summarize results in natural language. Symbal exhibits strong performance on SymbalBench, correctly identifying systematic misalignments in 63.8% of datasets, a nearly 4x improvement over the closest baseline. We supplement our evaluations on SymbalBench with real-world evaluations, showing that Symbal can identify systematic misalignments in captions generated by an off-the-shelf MLLM. Ultimately, our novel task, benchmark, and method can aid users in auditing MLLM-generated captions and identifying critical failure modes, without requiring access to the underlying MLLM.", "tldr": "We introduce a benchmark and method for detecting a challenging class of captioning errors termed systematic misalignments.", "keywords": ["vision-language", "robustness", "caption", "benchmark", "mllm"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fee258d87c9cefc5e9afd855dc76d042ced7f41f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new task called systematic misalignment detection, which aims to find recurring caption errors in MLLM-generated image captions that are systematically tied to specific visual features (e.g., “cardiomegaly” errors co-occurring with “pacemaker” presence). It contributes (1) **SYMBALBENCH**, a benchmark of 420 synthetic vision-language datasets with planted (f, g) misalignments spanning natural and medical images, and (2) **SYMBAL**, a two-stage pipeline that first discovers the erroneous textual fact ( $\\hat f$ ) from captions and then the associated visual feature ( $\\hat g$ ) from images, each via clustering, alignment scoring, and summarization. The benchmark is created by injecting textual facts with controlled association strength (Cramer’s V) to selected visual features; evaluation uses Accuracy@K and, for open-ended outputs, LLM-as-a-Judge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The two-stage structure is intuitive and defensible, with transparent sub-steps (grouping/score/summarize). This factorization plausibly explains gains over single-shot prompting. \n2. The paper compares multiple text/image embedding backbones and alignment scorers for each stage and reports Accuracy@1/5 across reference-free and reference-based variants, highlighting where VL scorers help and where medical-domain models are needed. \n3. Appendix examples suggest the system can surface plausible spurious ties in off-the-shelf MLLM captions.\n4. The problem identified in this paper holds significant value, and the method solving it is well motivated."}, "weaknesses": {"value": "1. Although the benchmark is large and controllable, its errors are *injected*. This risks overfitting to the injection mechanism or distributional artifacts.\n2. Visual-feature detection in medical images remains modest, which could constrain practical utility in safety-critical use cases where the visual cue is subtle or device-like features are variable.\n3. Add analyses where (a) f and g are *correlated but not causal*, (b) g is *tiny/occluded*, and (c) multiple candidate g’s co-occur, to probe failure modes beyond Cramer’s-V control. \n4. Stage-wise pipelines use powerful VL models (e.g., Qwen2.5-72B) both to *score alignment* and to *summarize clusters*. If those models encode similar biases to the caption generator, they might either mask or hallucinate misalignments. Controls isolating scorer influence (e.g., cross-model scoring) are only partially addressed."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A7Zbe3R5pt", "forum": "uUghQYGVKO", "replyto": "uUghQYGVKO", "signatures": ["ICLR.cc/2026/Conference/Submission14974/Reviewer_LmQg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14974/Reviewer_LmQg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795433555, "cdate": 1761795433555, "tmdate": 1762925307626, "mdate": 1762925307626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"systematic misalignment detection,\" a new task for finding recurring errors in captions generated by Multimodal Large Language Models (MLLMs). A systematic misalignment is when a model repeatedly hallucinates a specific textual concept (e.g., \"cardiomegaly\") when a certain visual feature is present (e.g., \"pacemaker\").\n\n\nI am a research conference reviewer. I will write a review for you, keeping it simple and human-like.\n\nSummary This paper introduces \"systematic misalignment detection,\" a new task for finding recurring errors in captions generated by Multimodal Large Language Models (MLLMs). A systematic misalignment is when a model repeatedly hallucinates a specific textual concept (e.g., \"cardiomegaly\") when a certain visual feature is present (e.g., \"pacemaker\").\n\nTo support this task, the authors create SymbalBENCH, a benchmark with 420 datasets (natural and medical images) containing known, injected systematic errors. They also propose Symbal, an automated two-stage method to detect these errors. Stage 1 groups and scores textual facts to find the likely error, and Stage 2 does the same for images to find the associated visual trigger."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Defining \"systematic misalignment\" as a distinct, detectable type of error is a great idea. It moves beyond just individual hallucinations to finding deeper model biases, which is very useful for the community.\n\n2. The two-stage SYMBAL method (detect text error first, then visual trigger) is logical and effective. Breaking it down into grouping, scoring, and summarizing subtasks makes it modular and easy to improve.\n\n3. Showing it can find actual systematic errors in Llava-1.5 (like hallucinations of \"handbags\" near \"buses\") validates the practical utility of the method."}, "weaknesses": {"value": "1. Synthetic Benchmark: While necessary for a new task, the main evaluation is on synthetically injected errors. Real-world systematic errors might be subtler or more complex than the injected \"object A causes hallucination of object B\" pattern.\n\n2. The method involves multiple steps of embedding, clustering, and calling large LLMs/MLLMs (like Qwen-72B) for every dataset. This might be slow or expensive for very large real-world datasets.\n\n3. The statement that \"SYMBALBENCH consists of a total of 420 vision-language datasets\" is repeated frequently throughout the paper. This seems like an overstatement, as the methodology clarifies: \"We repeat this procedure across a range of possible options for f and g, yielding 420 vision-language datasets with annotated systematic misalignments.\" (L232) In reality, only 2 base datasets (COCO and MIMIC-CXR) are used to create different versions with synthetic errors injected, which is far from the initial promise of 420 distinct datasets.\n\n4. It is not clear to me, given a dataset, how many (f,g) pairs can be derived from it. A VLM can have several visual biases that result in wrong textual descriptions."}, "questions": {"value": "1. How does the method perform if there are multiple different systematic misalignments in the same dataset? Can it detect more than one, or does it just find the strongest signal?\n\n2. Have you tried SYMBAL on datasets where the visual trigger is more abstract (e.g., a specific lighting condition or camera angle causing a textual error) rather than just another object?\n\n3. What is the typical runtime and cost to run the full SYMBAL pipeline on one of the benchmark datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TyVVRCQ3MV", "forum": "uUghQYGVKO", "replyto": "uUghQYGVKO", "signatures": ["ICLR.cc/2026/Conference/Submission14974/Reviewer_6qV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14974/Reviewer_6qV7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832188211, "cdate": 1761832188211, "tmdate": 1762925306952, "mdate": 1762925306952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new task called \"systematic misalignment detection\". This task aims to identify recurring errors in MLLM-generated captions that are strongly associated with the presence of a specific visual feature in the paired image. The authors present two key contributions: (1) SYMBALBENCH, the first benchmark for this task, which consists of 420 vision-language datasets with synthetically injected, annotated systematic misalignments across natural (COCO) and medical (MIMIC-CXR) domains. (2) SYMBAL, a novel, structured, dual-stage method to solve this task. The paper reports that SYMBAL correctly identifies misalignments in 63.8% of datasets (Acc@5, reference-free, open-ended), claiming this is a nearly 4x improvement over direct-prompting baselines."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The primary strength is the introduction of the \"systematic misalignment detection\" task. This is a new, well-motivated, and critical problem for the community as it addresses the need to audit MLLM-generated data at scale , particularly in high-stakes domains like medicine.\n2. The paper contributes SYMBALBENCH, the first benchmark specifically designed for this task. This is a valuable resource for the community.\n3. The paper is exceptionally clear and well-written. The problem is motivated compellingly, and the method is described in unambiguous detail."}, "weaknesses": {"value": "1. The method's strong quantitative results (63.8% Acc@5) are derived entirely from the synthetic SYMBALBENCH, where errors were injected by the authors using a known procedure. This evaluation on a \"clean\" problem provides no guarantee of performance on the \"messy\" and subtle misalignments that may occur organically in the wild.\n2. The \"real-world\" case study in Appendix E is insufficient. It consists of only three qualitative examples of errors found in LLaVA 1.5 . This is anecdotal evidence, not rigorous validation. There is no quantitative analysis (e.g., precision/recall on a human-annotated set of real errors) to support the claim of real-world utility.\n3. The method appears fragile. It relies on K-Means with Silhouette distance to automatically select K, a notoriously unstable process. Furthermore, the \"summarizing the top-ranked group\" step  suggests a \"winner-take-all\" approach that can likely only detect one systematic error per dataset, which seems like a major limitation for real-world auditing."}, "questions": {"value": "Please respond to the weaknesses I mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1l7WkV0JdN", "forum": "uUghQYGVKO", "replyto": "uUghQYGVKO", "signatures": ["ICLR.cc/2026/Conference/Submission14974/Reviewer_VsQa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14974/Reviewer_VsQa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925642363, "cdate": 1761925642363, "tmdate": 1762925306405, "mdate": 1762925306405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on detecting systematic misalignments (that is hallucinations or inaccuracies) in generated captions. First, the paper proposes the synthetic benchmark, “SymbalBench”, to analyze the different methods to detect such hallucinations. Authors found that existing MLLMs fail on hallucination detection. And they propose Symbal as a hallucination detection method for this task which outperforms the selected baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper focuses on the hallucination (systematic misalignments) for the vision-laguage models, which is a very important problem statement. \n* SymbalBench shows that existing LLMs/VLMs are poor at detecting such hallucinations (likely because they are the ones creating hallucinations). \n* Symbal is an empirical low-compute (potentially) method that outperforms the baselines.\n* Additionally, benchmark focuses  on the real and medical domains."}, "weaknesses": {"value": "* The paper is poorly written and hard to follow. For instance, it is unclear if there are 420 datasets or 420 examples in the SymbalBench!\n* Additionally, Symbal benchmark seems illposed. Specifically, the goal is to evaluate the MLLM generated captions. While the proposed benchmark creates the synthetic task pairs. \n* Missing human evaluations. It is clear that Symbal outperforms the zero-shot baselines. However, it is not clear whether it will generalize to real-life scenarios. I would advise to conduct the human evaluations and compare with Symbal (and baselines) on generated captions from recent advance VLMs."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ajGKrbXkh8", "forum": "uUghQYGVKO", "replyto": "uUghQYGVKO", "signatures": ["ICLR.cc/2026/Conference/Submission14974/Reviewer_QdbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14974/Reviewer_QdbW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143350996, "cdate": 1762143350996, "tmdate": 1762925306086, "mdate": 1762925306086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}