{"id": "eslxxopXTF", "number": 3709, "cdate": 1757501745867, "mdate": 1759898074020, "content": {"title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models", "abstract": "Reinforcement learning with verifiable rewards (RLVR) typically adopts *Pass@1 as the reward*, causing policies to prefer conservative and similar actions. The RLVR algorithm has faced issues in balancing exploration and exploitation. \nThus, identifying an appropriate reward metric is crucial. To investigate this, inspired by the Pass@k metric that is used to evaluate a model's ability boundary, we first adopt Pass@k as the reward to train a policy (i.e., **Pass@k Training**), and observe the improvement in its exploration ability. Next, we derive an analytical solution for the advantage estimation process of Pass@k Training, reducing the resource overhead and estimation variance introduced by the sampling process. Our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. \nMoreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we explore the *implicit reward design* in RLVR, showing promising results and highlighting a potential future direction.", "tldr": "", "keywords": ["Reinforcement learning with verifiable rewards", "Large Reasoning Models", "Exploration and Exploitation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57726a9b9d46b0dcfa858fa736807e054d0a6273.pdf", "supplementary_material": "/attachment/272dfe7df94fa01bf27b87fd0a77427607e58204.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes \"Pass@k Training\" within the RLVR framework for LLM reasoning. The key idea is to use Pass@k metrics (the probability of success within k generated samples) as the optimization objective, rather than the typical Pass@1 focus. The paper introduces three variants of Pass@K training: full sampling, bootstrap sampling, and an analytical derivation method. An extensive empirical investigation explores the effects of Pass@k on model exploration and exploitation, policy entropy, answer diversity, robustness, transferability across domains and model sizes, and synergy with other RLVR strategies. Additional theoretical analysis motivates implicit reward design, suggesting new directions for adaptive RLVR objectives."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses the Timely and Impactful Problem for improving the performance of RLVR for LLMs.\n- The presentation is clear and easy to follow\n- Thorough experimental results are presented, including explicit ablation across all variants, comparisons on multiple tasks and domains, diverse LLM architectures/scales, and transfer/generalization studies.\n- The paper demonstrated competitive experiment results in RLVR."}, "weaknesses": {"value": "- The paper appears to be largely built upon *\"Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems\"*, which introduces PKPO for general RL tasks and provides a more thorough and theoretically grounded analysis. In contrast, this work reads more like an application of PKPO to large language models (LLMs), with limited discussion on what distinguishes it from the original PKPO or what its unique contributions are.\n\n- Both the analytical derivation (AD) in this paper and the analyses in PKPO suggest that Pass@K primarily rescales the advantage function. This rescaling seems equivalent to reducing the penalty for negative responses, as they may still be selected in the positive group. If AD alone is sufficient to characterize the method, it’s unclear why experiments like FS (full sampling) and BS (biased sampling) are necessary, especially since they introduce statistical variability without clear justification.\n\n- I understand that the paper aims to demonstrate the value of Pass@K beyond simply modifying reward signals or adding entropy regularization (as in Section 3.1). However, the strategy of randomly flipping a portion of negative responses to positive seems quite different from Pass@K’s original intent. One could ask: what if we instead directly adjust the reward of negative samples from 0 to some other rewards such as 0.5? I suspect there exists a negative reward setting that could theoretically replicate the effect of Pass@K (which is mentioned as the \"implicit reward design?\"). Moreover, entropy regularization with a coefficient of 0.001 appears to be a strong baseline. I also suspect that tuning the entropy coefficient around this value could yield competitive results, potentially marginalizing the claimed benefits of Pass@K."}, "questions": {"value": "- The paper states that *“the explicit design of advantage can be regarded as the implicit reward design.”* However, it remains unclear what the implicit reward is actually changed to. While the paper mentions implicit reward design, Sections 4.1 and 4.2 focus primarily on the *“adaptive switch between pass@1 and pass@K.”* It is not clear how this switching mechanism relates to or implements the idea of implicit reward design. A more explicit connection or interpretation would be helpful.\n\n- In Table 3, the analytical derivation (AD) significantly outperforms both FS and BS, despite the fact that these three methods should theoretically behave very similarly. This large gap is surprising and warrants further explanation. What causes AD to perform so much better, and why do FS and BS underperform to such an extent?\n\n- Given that FS, BS, AD and implicit reward design are theoretically expected to be equivalent, a deeper theoretical analysis of their relationships would be more valuable than empirical comparisons among them. Instead, it would be more informative to focus on empirical analyses that offer insight into the actual behavior of Pass@K. For example: 1. A comparison of the advantage function distributions under Pass@1 and Pass@K; 2. A more detailed examination of the probability assigned to top tokens under each method;3. Case studies that directly compare Pass@1 and Pass@K to better understand what Pass@K is effectively doing in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wej0zh6gum", "forum": "eslxxopXTF", "replyto": "eslxxopXTF", "signatures": ["ICLR.cc/2026/Conference/Submission3709/Reviewer_wDyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3709/Reviewer_wDyE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405831774, "cdate": 1761405831774, "tmdate": 1762916938735, "mdate": 1762916938735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current RLVR training primarily relies on the Pass@1 reward which causes models to prefer conservative and similar actions, suppressing exploration and leading to local optima. To solve this, the paper introduces Pass@k Training, a method that adopts the Pass@k metric as the reward signal. This approach has a higher tolerance for incorrect responses and incentivizes the model to generate diverse solutions to enhance exploration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This method achieves significant performance improvements with a relatively lightweight modification to the RLVR training process. The gains are not only in  performance but also in sample diversity, as measured by answer diversity and policy entropy .\n- The final \"Analytical Derivation\" method is technically solid, supported by a clear theoretical derivation.\n- The experiments are conducted across a wide range of domains, which demonstrates the generalizability and robustness of the method."}, "weaknesses": {"value": "- The novelty of this paper seems to be insufficient. The core motivation and the primary solution appear to be similar with existing research, namely Pass@K Policy Optimization. Besides the effective \"annealing\" strategy of starting with k>1 and decaying to k=1 is also explored in Pass@K Policy Optimization.\n- The implement only adopts GRPO as its training algorithm, the generalizability of the proposed method to other classic RLVR algorithm is not verified."}, "questions": {"value": "- In Table 4, the performance gains are sometimes exaggerated on specific benchmarks with specific base model. For example, in 7B-model experiment Pass@k Training shows a massive improvement on Crypto compared to Pass@1 Training, and P@k T. + P@1 T. shows even much more massive improvement than others. Could the authors provide more analysis on why the benefits of this method are so model-specific and task-specific in these cases?\n- What inspired the Pass@k -> Pass@1 transfer strategy? The paper frames this as an exploration-to-exploitation transition, but how did the authors determine the optimal timing or switch-point for this transition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XudvUQTpCo", "forum": "eslxxopXTF", "replyto": "eslxxopXTF", "signatures": ["ICLR.cc/2026/Conference/Submission3709/Reviewer_nShJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3709/Reviewer_nShJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896520373, "cdate": 1761896520373, "tmdate": 1762916937956, "mdate": 1762916937956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a key limitation of current RLVR methods, which typically optimize for Pass@1, causing models to converge to conservative local optima rather than fully exploring the reasoning space. To mitigate this, the authors propose Pass@k Training, a framework designed to achieve a more adaptive balance between exploration and exploitation in LRMs.\n\nThey first demonstrate that using Pass@k as the reward effectively enhances the model's exploration ability. Then, to improve efficiency and stability, they introduce bootstrap sampling and an analytical derivation for advantage estimation. Furthermore, the paper analyzes why Pass@k Training works and what benefits it brings from four perspectives, and finally extends the idea to an implicit reward design framework that generalizes the analytical formulation to broader RLVR settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on an important issue in reinforcement learning for large reasoning models—the exploration–exploitation trade-off under verifiable rewards. Its motivation is clear, and the proposed Pass@k Training framework provides a systematic three-stage implementation (full sampling → bootstrap sampling → analytical derivation). The presentation of ablations on different k values and learning rates demonstrates a good empirical sense of robustness.\nFrom a methodological perspective, the analytical form of the group advantage reduces sampling variance and clarifies the relationship between exploration and implicit reward shaping, which can be a useful pedagogical contribution for the RLVR community. The experiments are extensive across multiple reasoning benchmarks and clearly report Pass@1 and Pass@k metrics."}, "weaknesses": {"value": "1. The paper shows strong similarity to Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems (DeepMind, 2025) in terms of research motivation (limitations of Pass@1 exploration), core idea (training with Pass@k as the reward), key formulation (Eq. 11), and experimental validation (different tasks but similar evaluation goals and performance trends). However, the prior work is only briefly mentioned in the related-work section without any detailed analytical or empirical comparison.\n\n2. The paper never demonstrates that $\\mathbb{E}[A_{\\text{pos}}]$ or $\\mathbb{E}[A_{\\text{neg}}]$ are equivalent (or linearly related) to the true gradient of the Pass@k objective $\\nabla_\\theta \\mathbb{E}[\\max_i R_i]$. The authors simply substitute group-level reward statistics into the GRPO advantage term, assuming it introduces no bias. In contrast, Pass@K Policy Optimization (PKPO) rigorously proves unbiasedness of its estimator $\\hat{\\nabla}$ via the policy gradient theorem (Theorems 2–4), establishing theoretical consistency with $\\nabla_\\theta \\text{pass@k}$. This paper skips that derivation entirely and only claims variance reduction without verifying the potential bias.\n\n3. Appendix C defines the group-level variance as $\\sigma_{\\text{group}} = \\sqrt{\\bar{R}(1 - \\bar{R})}$, which corresponds to a binomial assumption and represents only the *empirical variance* of sampled rewards, not the variance of the estimator itself. The paper does not prove that $\\mathrm{Var}[A_{\\text{pos}}]$ or $\\mathrm{Var}[\\nabla_\\theta J(\\theta)]$ is bounded or convergent as $k$ increases. In contrast, PKPO Section 4 provides formal variance-reduction results using leave-one-out and maxg@(k−1) baselines. Such theoretical guarantees are entirely missing here.\n\n4. The experimental section lacks direct comparisons with related XPO methods such as PKPO and Best-of-N, and does not report variance or significance tests, making it difficult to verify whether the claimed \"improvement in exploration capability\" and \"outperformance over larger models\" are statistically reliable.\n\n5. Figure 1 introduces Enigmata scores on the first page without prior explanation of what Enigmata is. The benchmark and its evaluation protocol are only described several pages later, which disrupts the logical flow and makes the early figure hard to interpret for readers unfamiliar with the dataset."}, "questions": {"value": "1. In Section 3 and Appendix C, the derivation of Pass@k training lacks a formal proof of unbiasedness or variance bounds. Could the authors clarify whether the proposed advantage estimator $A_{pos}$ and $A_{neg}$ are theoretically consistent with the true gradient of the Pass@k objective? How does the analysis differ from that of PKPO (Theorem 2–4 in Pass@K Policy Optimization)?\n\n2. Why does the experimental section not include a direct comparison with PKPO or other XPO-style methods (e.g., DAPO, Dr. GRPO)? Since these baselines also optimize preference-based or set-level rewards, such a comparison would clarify whether the proposed method yields distinct benefits or merely reproduces known trends."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hVFLSfwsDh", "forum": "eslxxopXTF", "replyto": "eslxxopXTF", "signatures": ["ICLR.cc/2026/Conference/Submission3709/Reviewer_rZxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3709/Reviewer_rZxo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997817317, "cdate": 1761997817317, "tmdate": 1762916936048, "mdate": 1762916936048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Pass@k Training for large language models using reinforcement learning with verifiable rewards (RLVR) to improve exploration. It argues that existing RLVR methods use Pass@1 as the reward metric, leading to conservative policies that prefer safe, similar actions and limit exploration. The authors adopt Pass@k, a metric that measures whether correct responses appear within k attempts, as the reward signal to encourage diverse solution generation. The paper presents three implementations: full sampling, bootstrap sampling for efficiency, and an analytical derivation that provides closed-form advantage estimates. Experiments are presented across maze navigation, mathematical reasoning (AIME, OlymMATH), and synthetic puzzles (Enigmata). Results show that Pass@k Training improves Pass@k performance while maintaining or improving Pass@1 scores. The paper further introduces implicit reward design, suggesting that combining the advantage values of Pass@1 and Pass@k Training, and an adaptive policy entropy guidance based Pass@k Training may be more effective than Pass@k Training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\n-  The analytical derivation of advantage estimates (Equations 8-9) is an original technical contribution, providing closed-form solutions that eliminate sampling variance and reduce computational overhead compared to full sampling.\n\nQuality\n\n- Experiments have been presented across diverse domains, including maze navigation with varying sizes, mathematical reasoning tasks, and synthetic puzzles.\n- The paper presents diversity metrics and policy entropy measurements showing Pass@k Training maintains high entropy and generates diverse negative responses while Pass@1 Training causes entropy collapse.\n\nClarity\n\n- The paper is generally well-written with clear motivation and effective visualizations (Figure 2 comparing training procedures).\n\nSignificance\n\n-  Pass@k Training results in a 7B model achieving 30.8% overall Pass@1 accuracy on Enigmata after Pass@k followed by Pass@1 training, surpassing the performance of GPT-4o and approaching that of Claude 3.7, and improvements transferring across model scales (7B, 32B) and task domains."}, "weaknesses": {"value": "1. The connection between the advantage function design presented in Section 4 and optimality guarantees is not established.\n2. All experiments use outcome-based verification with binary rewards, and applicability to process-based rewards, partial credit, or continuous reward signals remains unexplored.\n3. The computational cost analysis is incomplete. Actual wall-clock time comparisons across all methods and scales are not provided.  Depending on k, Pass@k may significantly increase the wall-clock time, making training impractical for certain tasks.\n4. The comparison with baseline methods is limited, missing recent RLVR approaches beyond noise rewards and entropy regularization, and there is no comparison with other exploration methods, such as curiosity-driven learning or count-based bonuses.\n5. The hyperparameter selection for k and learning rates appears task-specific without any proposed principled guidance to select them.\n6. The reported results do not report the number of runs/seeds, variance, or significance tests, making the claims weak.\n\nMinor \n\n7. The implicit reward design section (Section 4) feels somewhat disconnected from the main narrative and could be better integrated or expanded into a separate contribution."}, "questions": {"value": "1. Can the authors provide formal convergence guarantees for Pass@k Training? Under what conditions does the method converge to optimal or near-optimal policies, and what is the sample complexity compared to Pass@1 Training?\n2. How does Pass@k Training perform with non-binary rewards or process-based verification where intermediate steps receive partial credit? Would the analytical derivation extend to these settings?\n3. What is the actual wall-clock time and computational cost (GPU hours, memory usage) for all three variants (full sampling, bootstrap sampling, analytical derivation) across different model scales and tasks?\n4. Can the authors provide principled guidance/heuristics for selecting k and learning rates based on task characteristics? For instance, do task difficulty, solution space size, or reward sparsity predict optimal hyperparameter settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GBGS3lgycS", "forum": "eslxxopXTF", "replyto": "eslxxopXTF", "signatures": ["ICLR.cc/2026/Conference/Submission3709/Reviewer_A5FH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3709/Reviewer_A5FH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3709/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762932058168, "cdate": 1762932058168, "tmdate": 1762932058168, "mdate": 1762932058168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}