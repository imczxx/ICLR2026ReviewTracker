{"id": "IAuK8al1UI", "number": 20765, "cdate": 1758309867399, "mdate": 1759896959785, "content": {"title": "Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards", "abstract": "Automating clinical documentation with large language models requires precise\nalignment with priorities such as completeness and factual grounding. We present\nan evaluation-integrated reinforcement learning framework for long-form clini-\ncal text generation that couples Group Relative Policy Optimization (GRPO) with\nDocLens, a claim-level evaluator that provides deterministic, dialogue-grounded\nrewards. Our method directly optimizes factual grounding and completeness with-\nout training a separate reward model or relying on human-authored references.\nEmpirically, the approach improves clinical note quality and reduces training cost\nvia a simple reward-gating strategy. An independent GPT-5 qualitative evaluation\nfurther supports these gains, showing higher preference for GRPO outputs in factuality, completeness, and brevity, with fewer omissions and hallucinations. Because the benchmarks are relatively clean and the base model already well aligned, these improvements likely represent a conservative lower bound. The framework\nis scalable to real-world settings and can incorporate custom objectives such as\nguideline adherence or billing preferences.", "tldr": "This work presents an evaluation-integrated reinforcement learning framework for long-form clinical text generation using Group Relative Policy Optimization (GRPO) with DocLens claim-level rewards.", "keywords": ["Clinical SOAP Note Generation", "Reinforcement Learning", "GRPO", "Reward Modeling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a90ebb05aff7b2ef4b0cd3bb34966b6fa053ee58.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an evaluation-integrated reinforcement learning framework for clinical SOAP note generation that combines Group Relative Policy Optimization (GRPO) with DocLens, a claim-based evaluation tool. The key contribution is using DocLens directly as the reward signal, eliminating the need for training a separate reward model as in traditional RL with human feedback approaches. Reference claims are extracted using GPT-4o and used to optimize the policy model to maximize claim-level precision and recall through GRPO. A reward gating strategy is also introduced to accelerate convergence. Empirical validation of the dataset is performed on a medical-dialogue-to-SOAP dataset (in-domain) and an ACI-Bench dataset (out-of-domain) with qualitative evaluation using LLM as a judge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A RL framework that does not require the traditional costly reward model training and instead uses the DocLens evaluation to serve as a deterministic reward signal. \n* Performance evaluated on an in-domain dataset and an out-of-domain evaluation (ACI-Bench) to assess generalization.\n* Honest empirical assessment of the performance improvement on two different datasets with the notion that performance is already quite good and thus expectation of larger gains is impractical.\n* Well-structured paper that was clear to understand and motivates the problem well."}, "weaknesses": {"value": "* There are some experimental setup issues that limit the impact of the results.\n  - The paper only compares a single instruction-tuned base model (Llama-3.1-8B-Instruct) against the GRPO and GRPO + reward gating without comparison against other LLM backbones. It's unclear if the framework generalizes broadly to other LLMs (or even potentially some of the medical-oriented variants like PMC-LLama, MedGemma, etc.)\n  - There are no comparisons with other RL methods using PPO, DPO, or DistillDirect (mentioned in related work).\n  - The paper uses DocLens both as a training objective _and_ the primary evaluation metric. This means the optimized model should perform better, but it's not clear if it means clinical utility is improved. For example, how is hallucination calculated. Is it when there's a medical claim that exists in the note but wasn't in the extracted one using GPT-4o?\n  - There are no ablation studies for some of the key choices in the model. For example, why is the reward gating using a threshold of 0.6? How much does that impact convergence? What about group size of 3? \n* The experiments rely on LLM as a judge and as the oracle. There are no discussion as to whether there is error or biases from GPT-4o that may affect the final model (there is no analysis of GPT-4o's reliability for medical claim verification in fact). Similarly, a single LLM is used as a judge for the ACI Bench. Why isn't there any analysis to compare with human expert judgements? At the very least, a different LLM backbone should've been used for evaluation (Prometheus-Eval is an example of a dedicated LLM-As-A-Judge framework that might also potentially make sense).\n * DocLens is used as a crucial component of the framework but doesn't validate the quality (e.g., is the claim extraction from GPT-4o consistent as defined in the deterministic rewards, does it match with human expert assessments of note quality as the original DocLens paper does not benchmark on this dataset)?  \n* There is no training convergence comparison even though it is one of the major claims of the benefts of the model. The proposed model uses only 3 epochs (2 with reward gating) but are not benchmarked against other RL approaches? Is this due to the deterministic nature or that it overfits? \n\nMinor\n* It's unclear why there is a need to repeatedly state results are a conservative lower bound. Is this because the improvement is small?\n* Please introduce acronyms once and use them consistently throughout the test (e.g., GRPO is introduced twice, but PPO and DPO aren't ever introduced)."}, "questions": {"value": "1. Why were no other RL methods included in the comparison?\n2. Does your framework generalize to other LLM backbones (both generalist and medical-domain specific)? \n3. Does optimizing DocLens F1 improve clinical utility or does it teach the model to match GPT-4o's? \n4. How does convergence compare with other RL methods and what evidence do you have that your model has converged within 2 or 3 epochs?\n5. Is the 0.6 threshold used for reward gating optimal? What about the sensitivity to the group size?\n6. Do human experts agree on the evaluation(e.g., is GRPO better than base notes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pAZq7yDrzx", "forum": "IAuK8al1UI", "replyto": "IAuK8al1UI", "signatures": ["ICLR.cc/2026/Conference/Submission20765/Reviewer_e6DB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20765/Reviewer_e6DB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593777845, "cdate": 1761593777845, "tmdate": 1762934195146, "mdate": 1762934195146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an evaluation-integrated RL pipeline for clinical note generation that combines GRPO with DocLens as tool that extracts claim sets from the source dialogue and from generated SOAP notes to calculate reward (F1-over claims). The results show that on the med-dialogue-to-SOAP test subset and ACI-Bench with Llama-3.1-8B-Instruct, the approach improves F1 performance against instruction-tuned base model baseline. The reward gating variant also proposed in the work achives the same performance more efficiently by using less number of epochs. The paper also provides with GPT-5 based qualitative eval reports to demonstrate effectiveness of the approach over hallucination rate, completeness, organisation, brevity."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Simple pipeline: Uses GRPO with a single claim-based reward without the need for a separate reward model to train is efficient and interesting for the specific clinical generation task.\n- The results show small yet steady improvements on both in-domain and ACI-Bench (out-of-domain).\n- The qualitative results discussed in the paper provides interpretability to effectiveness of the proposed approach in tackling hallucination and improving factuality, completeness and brevity."}, "weaknesses": {"value": "- Weak baseline for claiming superior performance: Missing comparisons to (1) rerank-only with DocLens (no RL), (2) standard PPO/DPO with reward model, (3) supervised training on extracted claims. Also the baseline in the paper is the Llama-3.1-8B-Instruct and not Llama-3.1-8B-Instruct with SFT on this data, which should be the first and most important comparison against the proposed GRPO. It is obvious that the base model will perform poorly against a model trained on this dataset. The results are only interesting if they improve over the SFT model trained on the same dataset.\n- Claiming the DocLens based rewards are “Deterministic rewards” is misleading: DocLens relies on an LLM (GPT-4o) for claim extraction/entailment. Without strict decoding control, rewards are not strictly deterministic.\n- Small scale experiments to support reliability of the claims in larger scale training scenarios. It is unsure if the proposed approach can be still superior to SFT models on similar data when trained for longer. Although I acknowledge the low resource nature of clinical domain, right now the the training dataset size ~800 samples for 2/3 epochs, which is too small. (Although I can understand if the authors had resource constraints)\n- Lack ablations: No sensitivity study for choosing the 0.6 gate threshold, reward scaling to (0-10] (why not 0-1?), or group size k [L262-238].\n- The discussion on qualitative evaluations using GPT-5E lacks reliability and confidence reporting. Although GPT-5 is used as an annotator, the resulting error analysis is hard to trust without inter-annotator agreement or multiple human/LLM annotators."}, "questions": {"value": "- \" What is SOAP?\" should be defined explicitly before it is first mentioned in L59-60.\n- Why was Base model w SFT not considered as a baseline?\n- Why were experiments conducted on small setting? any results on how the claims stand with longer training?\n- How the models perform when only precision or recall is used as reward?\n- Why scale rewards to [0,10] instead of [0,1]? Did you observe stability differences with GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9q8I3AxMAr", "forum": "IAuK8al1UI", "replyto": "IAuK8al1UI", "signatures": ["ICLR.cc/2026/Conference/Submission20765/Reviewer_sot1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20765/Reviewer_sot1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863486051, "cdate": 1761863486051, "tmdate": 1762934194395, "mdate": 1762934194395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors seek to demonstrate a novel evaluation-integrated reinforcement learning for clinical text generation in documentation contexts. They include the DocLens framework in a GRPO loop, and find improved overall performance on ACI-Bench. They complete the paper by offering qualitative evaluation performed via an instance of GPT-5."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors offer a reasonable summary of the existing literature regarding document summarization. \n\t- Their use of the atomic claims framework as applied to document summarization is reasonably novel - particularly their described integration directly into the reward loop. They show reasonably convincingly that this improves performance on the ACI-BENCH overall, which offers reasonable proof-of-concept assessment of this method. \n\t- In particular, the use of non-LLM, traditional NLP methods to show that the inclusion of LLM-as-judge in the training loop is useful is both reasonable and interesting, clearly establishing the promise of the doclens atomic claims method in this space. While this is not as robust as the use of a final human validation, and far from sufficient to rely upon, it remains useful as a directional signal. I do have very substantial concerns about the GPT-5 section (as outlined below), however, as discussed below. \n\t- The diagrams are reasonably clear, and the overall work is straightforward to follow. \nThe appendix is well-documented and clear."}, "weaknesses": {"value": "- I strongly dislike the authors descriptions of \"GPT-5 as an external judge\" in this paper, without further human validation. Use of LLM-as-judge in this way is inappropriate without reference to whether it performs this task equivalently to a human. This is especially true for complex, clinically relevant text such as that in the notes that the authors are seeking to evaluate. If the errors are so subtle as to be invisible to the first LLM, what is the guarantee that they will be visible to the second? It might be countered that GPT-5 is substantially larger and more powerful overall than the LLAMA model tested, but this then would obviously heavily limit the scalability of this approach as extended to the frontier.  \n\t- To further extend the point above, while the DocLens framework was itself validated against clinical context, this human validation remains a core metric for relying on it in any capacity (see https://ai.nejm.org/doi/full/10.1056/AIe2500143). You cannot extend this to a prompted model without similar evaluation or at least. \n\t- To make claims calling this \"external expert-level evaluation\" is misleading to the point of absurdity here, without validation against actual experts. The inclusion of this section at all essentially leads to noise in the paper. LLMs clearly have stylistic preferences which may not align with those of human authors, the so-called \"slop\" phenomenon (also see https://www.nejm.org/doi/full/10.1056/NEJMp2405999). An LLM-LLM loop integrating solely LLM preferences cannot and must not become acceptable as a metric and risks leading to runaway flaws in note quality. \n\t- There is a clear difference between the use of independently validated systems (or those using more deterministic, non-LLM methods like ACI-bench) in conjunction, and creating novel unvalidated systems from whole cloth. Even these conjunctions require repeated validation before clinical deployment, although this is not necessarily essential at this level of conceptual evaluation. \n\t- I must firmly recommend rejection of this paper while this GPT-5 section remains. I defer to the AC as to whether the GRPO-->ACI-Bench improvement work (which is reasonable in its own right) is sufficient to stand as an acceptably novel contribution to the literature."}, "questions": {"value": "1. Was any validation whatsoever performed on this GPT-5 based \"qualitative assessment\" system? If so, what?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MVHI7oU6Uc", "forum": "IAuK8al1UI", "replyto": "IAuK8al1UI", "signatures": ["ICLR.cc/2026/Conference/Submission20765/Reviewer_8ZfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20765/Reviewer_8ZfN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087056380, "cdate": 1762087056380, "tmdate": 1762934193083, "mdate": 1762934193083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies a reinforcement learning framework for long-form clinical text generation to the “doctor–patient dialogue → SOAP note” task. The method uses GRPO for policy updates and employs DocLens as a claim-level reward. DocLens computes completeness (recall) and factual grounding (precision), combining them into an F1 reward that eliminates the need for a separate reward model. During training, reference claims from each dialogue are cached; for each generated note, new claims are extracted and compared with the reference to compute the reward used for GRPO updates. The authors also include a qualitative pairwise evaluation using GPT-5. Experiments focus mainly on this dialogue-to-SOAP task and report one generalization test on ACI-Bench.\n\n**Initial Recommendation:** Reject.\nThe baselines and experimental design are too simplistic. The paper does not convincingly show the necessity of RL, nor that the GRPO + DocLens combination outperforms simpler training and reward schemes. The reporting and reproducibility are incomplete. If these issues are addressed during the rebuttal, I would be open to revisiting my evaluation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Directly integrates a reproducible claim-level metric into RL, with a clear design. The reward based on interpretable precision and recall aligns well with the task objectives.\n* Avoids training a separate reward model, so it's simple and computationally efficient, as the paper emphasizes.\n* The application task itself is practically meaningful: generating SOAP notes from medical dialogues is a relevant real-world clinical scenario."}, "weaknesses": {"value": "* **Lack of motivation for using RL.** DocLens can also serve as a data filter or scorer to build datasets for SFT or DPO. The paper does not provide a fair comparison against SFT and DPO baselines under equal data and compute budgets. It is unclear why RL is necessary.\n* **Overly narrow reward design.** The paper claims that using DocLens as a reward is advantageous, but does not compare against simpler reward functions, e.g., ROUGE-F1, a lightweight LLM-judge score (G-eval style), or rank-based rewards for candidates ranking during rollout. These alternatives are cheaper and could be equally effective. The design’s transferability to other long-form clinical tasks is not demonstrated.\n* **Metric–evaluation coupling risk.** DocLens is both the reward and the primary evaluation metric. Since it is imperfect (no evaluation metric is pefect!), the observed improvements may stem from metric overfitting. No blinded human evaluation is provided, making the clinical significance uncertain.\n* **Insufficient reporting of training cost and settings.** Missing details include candidate count (k) sensitivity, reward gating threshold ablation, token usage, runtime, and cost per example.\n* **Reproducibility concerns.** The appendix contains only prompts, without additional important details for community reproduction.\n* **Incomplete disclosure of LLM usage.** ICLR requires authors to disclose significant use of large language models; this information is currently missing."}, "questions": {"value": "1. Why is RL necessary? Please provide a fair comparison under identical budgets:\n\n   * **SFT:** Use DocLens to select high-quality instruction pairs.\n   * **DPO:** Use DocLens to construct preference pairs.\n   * **GRPO:** Use DocLens as the reward function.\n   * Report their performance under the same model, data, candidate number (k), and training steps.\n2. Add at least three lightweight alternatives: ROUGE-F1, a G-Eval/LLM-judge scalar score, and a multi-rollout rank-based reward. Compare their effectiveness and computational cost to DocLens-F1.\n3. I believe that experimental results for this task, without expert evaluation, have a limited real impact. Perhaps it's just a reward hack of the DOCLENS metrics; without human evaluation and qualitative analysis/case study, it doesn't convince me.\n4. Report candidate number per case, token usage, training steps, GPU memory/time, inference latency, and overall cost. Analyze the effects of candidate count (k), reward gating threshold, and claim extraction errors on final performance. Include detailed data processing steps, hyperparameter tables, random seeds, and released code/version information.\n5. Add a clear LLM usage disclosure section and an ethics statement describing data privacy and compliance procedures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2Ssfkh13eF", "forum": "IAuK8al1UI", "replyto": "IAuK8al1UI", "signatures": ["ICLR.cc/2026/Conference/Submission20765/Reviewer_Srmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20765/Reviewer_Srmz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136978531, "cdate": 1762136978531, "tmdate": 1762934191582, "mdate": 1762934191582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}