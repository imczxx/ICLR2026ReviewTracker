{"id": "IccLTTXjHE", "number": 22019, "cdate": 1758324953128, "mdate": 1759896890858, "content": {"title": "Warped Latent Spaces and Traversal for Chemical Deep Generative Models", "abstract": "We propose a generative framework for interpretable and property-aware molecular design by learning warped subspaces within the latent space of a chemical variational autoencoder (VAE) trained on a sequential representation of small molecules. Instead of directly regularizing latent coordinates, our approach works by creating low dimensional subspaces that are smoothly warped to align with molecular property variation using a novel alignment loss. This warping provides a flexible mechanism to capture nonlinear structure in property–latent relationships while retaining interpretability. This framework enables property optimisation and traversal within a low-dimensional subspace, where directions correspond to meaningful variations in molecular properties and decode back into valid molecules in the original space. We evaluate the method on various tasks related to conditional molecular generation on standard benchmarks used in literature like QM9, ZINC250K and the Pubchem drug datasets demonstrating strong generative quality, validity, uniqueness and novelty alongside a more controllable approach molecular generation.", "tldr": "Learning and traveral in chemically informed latent spaces of deep generative models for small molecule generation", "keywords": ["deep generative models", "chemical deep generative models", "latent variable models", "variational autoencoders", "property prediction", "drug design"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d87adfeb68b0da75344f8c029500a4092ac8cb1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "`method`\n1 a transformer VAE.\n2 non-linear warping function, achieved with an alignment loss over all pairs of molecules as eq 6 (L align).\n\n\n`results`\nMethod evaluated on ZINC250k, on validity, novelty.\nAlso evaluated on property prediction."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "the task of generative models for molecule is an important issue in ML"}, "weaknesses": {"value": "lack of novelty in the method.\n\nLack of suitable evaluation, all baselines are from before 2019 .\n\nThe results show no or minor improvements."}, "questions": {"value": "not sure why we need pairs? in eq 6? is this motivated? could this be clarified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IBTTg2tact", "forum": "IccLTTXjHE", "replyto": "IccLTTXjHE", "signatures": ["ICLR.cc/2026/Conference/Submission22019/Reviewer_i4AG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22019/Reviewer_i4AG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575993446, "cdate": 1761575993446, "tmdate": 1762942022139, "mdate": 1762942022139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a way to make a pretrained molecular VAE’s latent space easier to control for specific chemical properties (like QED, logP, SAS). After training a Transformer-VAE on SELFIES strings, the authors learn a tiny “warping” network for each property that maps the original latent codes into a small, property-aware subspace. In that warped space, the property varies mostly along a single direction, so you can move the latent point in a simple, controlled way to raise or lower the property. A trust-region step keeps moves near the data manifold, and an “inverse lifting” step brings the edited point back to the original latent space to decode valid molecules."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Framing property control as a lightweight warping learned on top of a pretrained VAE is a novel way. It avoids re-training the generator or baking properties into the decoder, which many prior methods require.\n2. Using pairwise distance alignment and covariance whitening to keep the warped subspace well-conditioned is a sensible way to prevent degenerate mappings and encourages smooth traversals.\n3. Single-direction “dials” per property are easy to reason about and integrate into interactive tools or multi-objective workflows."}, "weaknesses": {"value": "1. A single linear direction per property may work only locally; globally the property landscape can be multi-modal or curved.\n2. Many original latents can map to the same warped point; the inverse step may fail or land off-manifold, leading to invalid or degenerate molecules."}, "questions": {"value": "1. Do you detect and swap direction when monotonicity breaks?\n2. How large is the average drift in non-optimized properties after lifting and decoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5CkmMUZhBC", "forum": "IccLTTXjHE", "replyto": "IccLTTXjHE", "signatures": ["ICLR.cc/2026/Conference/Submission22019/Reviewer_bNpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22019/Reviewer_bNpf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869827184, "cdate": 1761869827184, "tmdate": 1762942021694, "mdate": 1762942021694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for molecular property optimization, which operates within the latent space of a pre-trained Variational AutoEncoder (VAE). By employing the SELFIES representation, the VAE's latent space is reportedly free from the \"dead zones\" that typically plague models trained on traditional SMILES representations. This property is highly advantageous for optimization tasks. To render navigation of this high-dimensional latent space tractable, the authors propose learning property-specific transformations that project the latent vectors into a much lower-dimensional space. A covariance whitening regularizer is introduced to simplify the downstream optimization. Although the resulting optimization problem is non-convex, the authors employ a multi-restart strategy to mitigate the impact of local minima."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The central idea of performing property optimization within a learned, low-dimensional subspace—rather than in the VAE's full native latent space—is interesting. This approach to dimensionality reduction for targeted optimization represents the key contribution of the paper."}, "weaknesses": {"value": "While the proposed low-dimensional projection is intriguing, the paper seems to concede (e.g., in the \"Decoding Protocol\" section) that the fundamental challenges of non-convexity and high-dimensionality persist.\n\nA critical omission is a comparison against a baseline that predicts properties directly from the full VAE latent space (e.g., a simple regression model). Such a baseline would not only contextualize the benefit of the proposed projection but also highlight a significant weakness of the current method: its apparent inability to handle multi-property optimization, a capability that a direct-prediction model would inherently possess. The lack of multi-property optimization support is a major limitation.\n\nFurthermore, a significant weakness of the paper is its lack of thorough contextualization. The \"Related Work\" section remains at a high level of abstraction, presenting lists of citations without specific discussion of how those works relate to the paper's novel contributions or limitations. Consequently, the paper fails to compare against any relevant, modern baselines. Even the VAE component's evaluation relies on a baseline from 2019, which is outdated given the rapid progress in the field.\n\nTo address this, the authors should position their work more clearly relative to:\n- General structured prediction (have a look at papers like [1,2]).\n- Modern representation learning for molecules, especially SELFIES-based models (e.g., [3] offers an encoder-decoder, [4] an encoder-only).\n- Other contemporary methods for molecular property optimization.\n\n[1] Amos, Brandon, Lei Xu, and J. Zico Kolter. \"Input convex neural networks.\" International conference on machine learning. PMLR, 2017.\n[2] LeCun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato, M,\nand Huang, F. A tutorial on energy-based learning. Pre-\ndicting structured data, 1:0, 2006.\n[3] Priyadarsini, Indra, et al. \"Self-bart: A transformer-based molecular representation model using selfies.\" arXiv preprint arXiv:2410.12348 (2024).\n[4] Yüksel, Atakan, et al. \"SELFormer: molecular representation learning via SELFIES language models.\" Machine Learning: Science and Technology 4.2 (2023): 025035.\n\n# Typos:\n1. L260: “itt” -> “it”"}, "questions": {"value": "1. Causal Decoder: The rationale for using a causal decoder is unclear. This architecture is typically employed for auto-regressive next-token prediction, which does not appear to be required by your non-autoregressive setup. How does the performance of this causal decoder compare to a standard (non-causal) self-attention mechanism?\n2. Decoder Architecture: Could the authors please provide a diagram or pseudo-code for the decoder architecture? The description of how the property vector $m$ is repeated $T$ times and utilized within the model is currently difficult to follow.\n3. Decoding Protocol Novelty: Is the \"Decoding Protocol\" a novel contribution of this work or standard practice? If it is novel, the justification for presenting it solely in the narrow context of molecular optimization, rather than as a general method for structured prediction, is missing. If it is standard practice, please provide relevant citations.\n4. Tokenizer: What specific tokenizer is used for the SELFIES strings? Is it character-level, or does it utilize a specific SELFIES vocabulary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VYhDj5HOrA", "forum": "IccLTTXjHE", "replyto": "IccLTTXjHE", "signatures": ["ICLR.cc/2026/Conference/Submission22019/Reviewer_CoBp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22019/Reviewer_CoBp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923109672, "cdate": 1761923109672, "tmdate": 1762942021238, "mdate": 1762942021238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a VAE framework to learn property-aligned subspace via an auxiliary loss. Results show that the proposed method can learn a subspace where molecules with similar property values have similar latent representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The presentation is generally clear."}, "weaknesses": {"value": "- Generally, this paper introduces nothing new to the community. The overall problem, using VAE latent space traversal for property optimization, is something that this community was exploring four or five years ago. Many more effective optimization methods have been proposed over the past five years; unfortunately, none of them are compared with or even mentioned in this paper.\n- Even for vanilla molecule generation without property optimization, all the comparison methods in Table 1 are from 2020 or earlier, despite the rapid progress in this field over recent years. The lack of up-to-date baselines makes the evaluation unconvincing.\n- The idea of using a Transformer-based VAE is also not new. Simply replacing SMILES with SELFIES does not contribute meaningful novelty either. The only new component, the latent space contrastive loss (Eq. 6), still does not address a valid problem. In the introduction, the authors criticize latent space disentanglement methods for suffering from property entanglement, but this issue has already been studied and mitigated by a series of works addressing latent factor correlation in VAEs.\n- The properties used for evaluation, including QED and LogP, are considered toy properties with limited connection to real-world molecular design tasks, which further weakens the practical significance of the reported results."}, "questions": {"value": "I don't have more questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uOZZJTIltZ", "forum": "IccLTTXjHE", "replyto": "IccLTTXjHE", "signatures": ["ICLR.cc/2026/Conference/Submission22019/Reviewer_fxRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22019/Reviewer_fxRp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961352453, "cdate": 1761961352453, "tmdate": 1762942020920, "mdate": 1762942020920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}