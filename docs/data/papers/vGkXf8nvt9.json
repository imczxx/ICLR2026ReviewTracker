{"id": "vGkXf8nvt9", "number": 2718, "cdate": 1757221640846, "mdate": 1763472489686, "content": {"title": "Forget-to-Focus: Can unlearning Improve Domain Specialization in LLMs?", "abstract": "Standard fine-tuning of Large Language Models for domain-specific tasks is often suboptimal due to interference from vast, pre-existing general knowledge from pretraining, leading to issues like negative knowledge transfer and the reinforcement of spurious correlations. We study whether removing parts of a pretrained model’s pre-existing general knowledge before adaptation can make downstream learning easier. We propose and analyze Forget-to-Focus: a two-stage protocol that first performs targeted unlearning on a “forget” set (with an optional retain set for stability, then fine-tunes on a domain-specific dataset. Through rigorous experiments on different domains such as medical, mathematics, and coding benchmarks, we analyze whether this preparatory unlearning can lead to improved domain specialization. Our findings show that this protocol consistently outperforms standard fine-tuning e.g., it improves HumanEval pass@1 by 32.5\\% on Qwen3-0.6B and 11.95\\% on Qwen 72B model compared to standard fine-tuning. Beyond accuracy, we observe that F2F reshapes representational geometry as measured by centered kernel alignment, shifting models away from generalist initialization toward structures more conducive to in-domain specialization. Furthermore, unlearning prior fine-tuning helps improved calibration on medical QA tasks, reducing overconfidence and mitigating reliability issues that persist under standard fine-tuning. These findings establish unlearning not merely as a privacy tool but as a principled intervention for domain adaptation. By strategically suppressing irrelevant pretraining knowledge, Forget-to-Focus helps more stable optimization dynamics, better calibrated predictions, and consistently stronger downstream performance. The code is available at anonymous github : \\href{https://anonymous.4open.science/r/D-1545/README.md}{https://anonymous.4open.science/r/D-1545/README.md}", "tldr": "", "keywords": ["llm unlearning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1ea276edf7410df03d82c60ecc143350fbc5fb8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces \"Forget-to-Focus,\" a novel approach that first employs unlearning methods and then fine-tunes LLMs on specific domains, outperforming traditional SFT. The authors support their method with both a mathematical proof and a latent space analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "（1）The story, and motivation is well presented. The writing is easy to follow.\n（2）The findings are meaningful, which shapes unlearning as a way to improve domain specification.\n（3）The experiments cover models with different sizes, architectures, and different domains."}, "weaknesses": {"value": "（1）The choice of base model seems weird and needs justification. Firstly, the chosen model includes different versions (e.g. Qwen2 72B Instruct vs Qwen3-0.6B, Llama2-13B and Llama3.1-8B-Instruct). Can the authors try Qwen2.5 series with different model sizes? Moreover, Qwen3-0.6B is not an instruct model.\n（2）Why in Table1, finetuned Gemma-2B-Instruct is worse then the base model? All the finetuning methods, namely SFT, DAPT, LORA, CurlLora, makes the performance worse.  I doubt the correctness of finetuning process, or the hyperparam choice.\n（3）The authors do not give a automated method to choose forget set, which limits the value of proposed method. For example, given a new domain, how to choose the forget set?"}, "questions": {"value": "(1)\tTable1 is confusing. The authors should use more clear notations other than label of (1) and (2). Same problem exists for Table3.\n(2)\tIn Table2, is it Llama3.1-8B-Instruct instead of “LLaMA 8B”? (also for L288, L327, L351, etc.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HEjO8APhcJ", "forum": "vGkXf8nvt9", "replyto": "vGkXf8nvt9", "signatures": ["ICLR.cc/2026/Conference/Submission2718/Reviewer_P5Ts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2718/Reviewer_P5Ts"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899679399, "cdate": 1761899679399, "tmdate": 1762916342915, "mdate": 1762916342915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Forget-to-Focus (F2F) proposes a two-stage fine-tuning protocol where a large language model (LLM) first undergoes targeted unlearning of irrelevant general-domain knowledge before domain-specific fine-tuning. The approach aims to mitigate negative transfer from pretraining priors that hinder specialization, reframing unlearning as a capacity reallocation mechanism rather than a privacy safeguard. Formally, F2F minimizes an objective combining gradient ascent on a forget set and gradient descent on a retain set, producing a new initialization that lies closer to the downstream optimum. Theoretical analysis shows contraction of irrelevant components in parameter space, while empirical results demonstrate consistent improvements across coding, mathematical, and medical domains. Representational analyses (CKA, SVCCA, Fisher information) confirm that unlearning reshapes the model’s geometry toward domain-relevant subspaces and improves calibration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a conceptually novel perspective—using unlearning not for data privacy, but as a preparatory step for specialization. This reframing is both original and intuitively appealing, addressing a key limitation of conventional fine-tuning.\n- Provides a mathematical formulation of how unlearning alters optimization dynamics, supported by a contraction analysis that theoretically guarantees reduced distance to domain optima.\n- Empirical validation is comprehensive: spanning multiple LLM families (Qwen, LLaMA, Gemma) and domains (medical, math, coding) up to 72B parameters, ensuring robustness of findings. Also, authors incorporates representational diagnostics (CKA, SVCCA, Fisher analysis, PCA-shift) that offer interpretable empirical evidence of capacity redistribution and geometry realignment after unlearning.\n- Quantitative performance gains—up to +32.5% in coding (HumanEval) and +11.9% on large models (Qwen-72B)—as well as improved calibration and reduced overconfidence on sensitive QA tasks."}, "weaknesses": {"value": "1. **Dependence on forget/retain set quality.**\nPerformance improvements are sensitive to how cleanly the forget set separates general and domain-relevant data. The method assumes such partitioning is feasible, but in real-world setups, this boundary is often ambiguous. The paper lacks guidelines for constructing forget sets in open-domain contexts.\n2. **Simplistic convex surrogate assumption.**\nThe theoretical guarantees rely on convexity and orthogonal subspace decomposition, which may not hold in deep nonlinear LLMs. While insightful, the analysis may not fully capture realistic optimization dynamics.\n3. **Limited diversity of domain evaluation.**\nDespite testing on three domains, most benchmarks are standard (PubMedQA, MATH, HumanEval) and measure only accuracy. The paper does not probe harder or more diverse domain shifts (e.g., adversarial or multimodal settings), which limits generalizability claims.\n4. **Computational trade-offs unquantified.**\nAlthough the authors mention unlearning adds modest cost (~1k steps), the paper does not analyze scaling behavior or potential instability when applied to trillion-parameter LLMs or multi-domain settings."}, "questions": {"value": "1. How robust is the proposed benefit of unlearning to imperfect or noisy forget/retain set selection? Could overlap between general and domain-relevant data negate the observed gains?\n\n2. The theoretical analysis relies on convex surrogate assumptions and orthogonal subspace decomposition. How well do these simplifications capture the optimization dynamics of non-convex LLM training?\n\n3. Could the unlearning–retuning pipeline be extended to multi-domain or continual adaptation, where domains overlap or evolve over time, without catastrophic interference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DdEsFFchcK", "forum": "vGkXf8nvt9", "replyto": "vGkXf8nvt9", "signatures": ["ICLR.cc/2026/Conference/Submission2718/Reviewer_LotK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2718/Reviewer_LotK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925630806, "cdate": 1761925630806, "tmdate": 1762916342774, "mdate": 1762916342774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Forget-to-Focus (F2F): first “unlearn” with a forget set (and small retain set), then fine-tune on the target domain. They report gains on coding (MBPP/HumanEval), medical QA (PubMedQA/MedMCQA), and math (GSM8K/MATH), and show representation-level shifts after unlearning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It is an interesting idea and the paper presents clear, simple two-stage protocol and motivation around negative transfer. The paper try to evaluate broad set of domains and model sizes."}, "weaknesses": {"value": "- Feasibility of “forgetting general data.”\nThe paper itself admits it’s hard to decide what to forget without hurting useful priors. \n\n\n- Risk to general capabilities/communication.\nThey only audit multiple-choice commonsense tasks, not instruction-following or dialog quality. So we can’t tell how chatty/instructional behavior survives F2F—even though the base models are “Instruct” variants. The appendix “broad skills” audit doesn’t answer that. \nInstruction following is unreported, such as  MT-Bench / AlpacaEval / Arena-Hard-style evaluation.\n\n- SFT sensitivity / insufficient controls.\nF2F’s headline gains could be sensitive to SFT choices (epochs, LR, batch, data subsampling). The paper provides a single-recipe config and some ablations (GA/GD weights, retain size), but no multi-seed variance/error bars and no systematic hyperparameter sweeps to bound the effect size."}, "questions": {"value": "- Forget-set design: How would you construct scalable, principled forget sets without manual filtering or referencing eval distributions (e.g., HumanEval)? Can you provide automated selection criteria?\n\n- Instruction following: Please report MT-Bench, AlpacaEval, or similar pre/post-F2F to quantify conversational and instruction adherence"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AGKonGUQHU", "forum": "vGkXf8nvt9", "replyto": "vGkXf8nvt9", "signatures": ["ICLR.cc/2026/Conference/Submission2718/Reviewer_N1qK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2718/Reviewer_N1qK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012088266, "cdate": 1762012088266, "tmdate": 1762916342182, "mdate": 1762916342182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}