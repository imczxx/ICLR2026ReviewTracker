{"id": "Y4xzgpLrWc", "number": 16276, "cdate": 1758262579868, "mdate": 1759897250608, "content": {"title": "Online-Optimized RAG for Tool Use and Function Calling", "abstract": "In many applications, retrieval-augmented generation (RAG) drives tool use and function calling by embedding the (user) queries and matching them to pre-specified tool/function descriptions. In this paper, we address an embedding misalignment issue that often arises in practical applications due to imperfect embedding models or noisy descriptions; such misalignment may lead to incorrect retrieval and task failure. We introduce Online-Optimized RAG, a deployment-time framework that continually adapts retrieval embeddings from live interactions using minimal feedback (e.g., task success). Online-Optimized RAG applies lightweight online gradient updates with negligible per-query latency and requires no changes to the underlying LLM. The method is plug-and-play: it supports both single- and multi-hop tool use, dynamic tool inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent theoretical analysis that quantifies how the method's performance depends on the initialization quality of the embeddings and other related quantities. Across diverse tool-use and document-retrieval scenarios, our Online-Optimized RAG consistently improves tool selection accuracy and end-task success, thus providing a simple, practical path to robust, self-improving RAG systems.", "tldr": "Propose a plug-and-play, deployment-time online optimization framework to improve retrieval performance for agentic LLM systems.", "keywords": ["retrieval augmented generation", "tool use", "agentic AI", "online optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eea46b0ba89954a9b3414f4c16e2bc334906a06a.pdf", "supplementary_material": "/attachment/041cabbb3687dcbee825b86445ae0c90f331e4e6.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Online-Optimized RAG (ORAG), a deployment-time framework to correct \"embedding misalignment\" in RAG systems used for tool and function calling. The authors identify that errors at retrieval time, often caused by noisy documentation or suboptimal embedding models, lead to incorrect retrieval and task failure. ORAG addresses this by continuously adapting the tool embeddings—not the LLM or query model—using minimal, real-time feedback (e.g., task success/failure) from live user interactions. This \"plug-and-play\" method applies lightweight online gradient updates with negligible latency.\n\nThe update mechanism is intuitive.When a selected tool succeeds, its embedding is moved toward the query embedding to increase similarity; when it fails, it is moved away. Embeddings of unselected tools are also pushed away to refine the embedding space. The authors provide a theoretical regret analysis and show empirically that ORAG consistently improves tool selection accuracy and task success across diverse benchmarks like UltraTool and ToolRet. The method is also shown to be versatile, successfully integrating with multi-hop retrieval, dynamic tool databases, and rerankers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a robust theoretical analysis to ground the proposed online learning mechanism, while the empirical results consistently demonstrate its effectiveness in improving retrieval accuracy and end-to-end task success over multiple base embedding models.\n\n2. The authors provide a comprehensive evaluation across multiple realistic use cases, including integration with K-retrieval rerankers, handling time-varying dynamic tool databases, and applying the method to multi-hop reasoning. This broad experimentation, spanning several datasets, strongly validates the \"plug-and-play\" and flexible nature of the ORAG framework for real-world deployment."}, "weaknesses": {"value": "1. Limited Comparison to Fine-Tuning Baselines: While ORAG is an online learning algorithm designed to fine-tune embeddings, the empirical evaluation primarily compares its performance against static base embedding models. The absence of a direct comparison against established or contemporary online/offline fine-tuning algorithms (e.g., in-batch negative sampling on the tool descriptions or approaches like DynamicRAG/FLAIR/NUDGE) makes it difficult to quantify the performance benefit and computational advantages of the proposed approach over existing state-of-the-art methods in the literature.\n\n2. Unclear Differentiation from Related Work: The paper does not sufficiently clarify the benefits of ORAG against existing related work, such as DynamicRAG, FLAIR, or NUDGE, outside of the Appendix. While the method's novelty lies in its deployment-time, bandit-style updates to tool embeddings, a more explicit discussion in the main body is needed to highlight the specific practical and theoretical advantages this approach offers compared to these previous efforts, particularly those involving dynamic retrieval or online adaptation. \n\n3. Sensitivity to Feedback: The effectiveness of ORAG relies entirely on the quality and timeliness of the bandit feedback (success/failure). The paper would benefit from a deeper analysis of the impact of noisy or delayed feedback on the stability and convergence rate of the online updates."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MykZ1lnILb", "forum": "Y4xzgpLrWc", "replyto": "Y4xzgpLrWc", "signatures": ["ICLR.cc/2026/Conference/Submission16276/Reviewer_H1oD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16276/Reviewer_H1oD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581621227, "cdate": 1761581621227, "tmdate": 1762926426403, "mdate": 1762926426403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an online update strategy tailored for RAG with tool use and function calling. The high-level idea is that, at each request that comes at time step $t$, if the task was not correctly accomplished, a small gradient update is performed on the database/toolbase embeddings according to the back-propagation rule of the error. The paper has shown that the proposed paradigm is compatible with various aspects of a RAG system, including rerankers, time-varying databases, as well as multi-hop retrieval. Experimental results show that with approximately 50ms of extra latency, the method is able to achieve significant performance improvement over vanilla embedding-based retrieval, pushing beyond existing pareto frontier regarding the trade-off of retrieval latency and performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Given the minor latency introduced by the online update mechanism, the performance improvement achieved by the proposal is quite solid.\n2. The paper did a good job to demonstrate the compatibility of the paradigm with commonly-used components of a modern RAG system."}, "weaknesses": {"value": "1. I have some concerns regarding the comparisons made in the experiments -- there are existing work that conducts online updates of parameters for RAG, for example Fu et al. (2024) (https://arxiv.org/abs/2406.19251). Since none of the baselines incorporates any sort of test-time adaptive mechanism, I think some extra comparison along that line will be able to further strengthen the benefit of the proposal.\n2. I have some issues with the presentation, listed below:\n\n* 2a. L76 \"requires no privileged access to model internals\" -> I'm not sure if I fully understood what you mean here, but I think something like \"can operate under a black-box setting\" would be clearer.\n* 2b. L107 reads a bit weird, here is what I think may work better: \"In this section, we first develop the reader's intuition for our algorithm. We then introduce the rigorous terminology and notation used for its formal description.\"\n* 2c. L169 \"motivated by the sequentially arriving nature\" of the user queries -> \"motivated by the sequential arrival of user queries\"\n* 2d. Your Figure 4 and 5 should have roughly similar scale on the Y-axis for a clear visual understanding. For example, for left subfigure of Figure 5, the range of Y-axis is only 0.8%. I think this figure greatly exaggerated the impact of static vs. dynamic DB.\n* 2e. L434 \"The aim is not so much for a ...\" -> colloquial language"}, "questions": {"value": "1. Can you talk a bit more about the \"success criteria\" for $1(i_t = i_t^*)$? I understand this could be user feedback or heuristic for real-world application deployment, but I'm not sure how they look like in your experiments. Are they also different for each of the test set?\n2. I'm not sure if I follow the reason (b) spanning L177-179 -- why do you think that's might be the case for query embeddings but not for database/toolbase embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t4j17uOPqq", "forum": "Y4xzgpLrWc", "replyto": "Y4xzgpLrWc", "signatures": ["ICLR.cc/2026/Conference/Submission16276/Reviewer_iDTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16276/Reviewer_iDTV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921168208, "cdate": 1761921168208, "tmdate": 1762926425795, "mdate": 1762926425795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an online learning framework that updates retrieval embeddings during deployment to improve tool selection in RAG."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach is lightweight and avoids LLM finetuning.\n2. The plug-and-play online embedding update mechanism requires only binary signals.\n3. The effectiveness of the proposed method is supported by empirical evaluations across multiple datasets, and it shows consistent improvements on Recall@10 and task success.\n4. The paper is clearly written, grounded in online convex optimization theory, providing rigor beyond empirical work."}, "weaknesses": {"value": "1. It would be useful to analyze robustness to continuous signals.\n2. Continual online updates may gradually distort embedding space, more discussions on its stabilization would be helpful.\n3. A deeper analysis on memory for large tool catalogs, and effect of update frequency are lacking."}, "questions": {"value": "1. Is there catastrophic drift during online training? Are resets or replay buffers needed?\n2. How does performance compare to offline embedding finetuning methods such as RAFT?\n3. How sensitive is performance to the learning rate schedule?\n4. What happens under cold-start / unseen tools where p is near zero?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pg8TqicUKL", "forum": "Y4xzgpLrWc", "replyto": "Y4xzgpLrWc", "signatures": ["ICLR.cc/2026/Conference/Submission16276/Reviewer_EW7w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16276/Reviewer_EW7w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039685568, "cdate": 1762039685568, "tmdate": 1762926424941, "mdate": 1762926424941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Online-Optimized RAG, a deployment-time procedure that updates tool embeddings online from minimal feedback (task success/failure), without touching the LLM. The retriever is modeled as a softmax over inner products; after each interaction, the method applies a lightweight online gradient update to the item embeddings, pushing the selected tool toward the query if it succeeded and away if it failed, while pushing unselected items away due to the softmax coupling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Updating only the item embeddings from success/failure, no LLM changes; works for tools and general document retrieval. \n\n2. Stochastic gradient view with an unbiased estimator and a standard sublinear regret analysis."}, "weaknesses": {"value": "1. The novelty is limited. The formulation (softmax retriever + online gradient with bandit feedback) is incremental relative to known online retrieval/adaptation ideas.\n\n2. Main comparisons are dense encoders, BM25, and two rerankers; missing hybrid/metadata recall, supervised tool retrievers, learned rerankers, and online alternatives (e.g., query-side adapters). \n\n3. The paper doesn’t report significance, and there’s little analysis of sensitivity to learning-rate or resilience to noisy/incorrect feedback.\n\n4. When a reranker is used, it’s ambiguous how much of a success is due to the sampler’s updated embeddings vs. the reranker’s scoring. The current estimator doesn’t separate these effects, so the online updates may be biased toward whatever the reranker already prefers."}, "questions": {"value": "1. Can you add non-encoder baselines for tool recall (e.g., BM25/hybrid with metadata,  supervised dense retriever + reranker, a tag/graph-aware recall)?\n\n2. Include at least one online alternative (e.g., a query-side online adapter) in the tool-use setting.\n\n3. When a reranker is used, how do you attribute success to the sampler vs. the reranker? Any bias correction beyond the current estimator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QyquLubC92", "forum": "Y4xzgpLrWc", "replyto": "Y4xzgpLrWc", "signatures": ["ICLR.cc/2026/Conference/Submission16276/Reviewer_SoPM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16276/Reviewer_SoPM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148326799, "cdate": 1762148326799, "tmdate": 1762926424041, "mdate": 1762926424041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Online-Optimized RAG, a framework that continuously adapts retrieval embeddings during deployment using minimal feedback (success/failure signals). The method treats tool retrieval as online multi-class classification and applies lightweight gradient updates to tool embeddings after each interaction. While the motivation is compelling and the approach theoretically sound, I have significant concerns about the experimental evaluation (see below)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated problem: Addresses a genuine practical challenge where embedding models become misaligned with operational environments due to outdated models, or distribution shifts.\n- Practical approach: The method requires only binary feedback (success/failure) and adds minimal computational overhead.\nSolid theoretical foundation\n- Broad applicability: Demonstrates extensions to K-retrieval with reranking, dynamic databases, and multi-hop scenarios"}, "weaknesses": {"value": "- Unclear experimental evaluation: The primary concern is whether the method learns from test data while baselines remain static. The paper reports results \"after applying the method with an average of 3000 updates\" - but it's unclear if these updates come from test interactions, creating an unfair advantage over baseline embeddings. There appears to be no clear temporal split where the method trains on earlier interactions and is evaluated on later ones. This is critical for validating online learning approaches and ensuring fair comparison.\n- Baselines: The comparison is primarily against static embedding models that cannot adapt. Comparisons could include: Other online learning methods, oracle baselines trained offline on the same feedback data, or other adaptive versions of the baseline embedding models\n- Limited realistic evaluation: While Table 2 shows \"single exposure\" results with modest improvements (1-2%), the main results rely on \"multiple exposure\" settings that may not reflect realistic deployment scenarios."}, "questions": {"value": "Do you perform temporal evaluation where the method learns from earlier time periods and is tested on later periods? If not, how do you ensure fair comparison when the method adapts to the test distribution?\n\nWhen you report performance metrics, are the embedding updates derived from the same queries being evaluated? \n\nOverall, the details about the feedback collection process should be clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8DVe3pVB2P", "forum": "Y4xzgpLrWc", "replyto": "Y4xzgpLrWc", "signatures": ["ICLR.cc/2026/Conference/Submission16276/Reviewer_6KJn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16276/Reviewer_6KJn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762281654294, "cdate": 1762281654294, "tmdate": 1762926423674, "mdate": 1762926423674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}