{"id": "aVXXZAp41g", "number": 19400, "cdate": 1758295924456, "mdate": 1759897041240, "content": {"title": "The Ensemble Inverse Problem: Applications and Methods", "abstract": "We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call  ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements.  Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward operator at inference time via training across several sets of truth-observation pairs that are consistent with the same forward operator, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in HEP and inverse imaging.", "tldr": "We introduce the ensemble inverse problem and propose a posterior sampling method based on generative models to solve it.", "keywords": ["Inverse problems", "conditional generative models", "posterior sampling", "permutation invariant neural network"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/662f75de2c400e08ed23ab42ffd5a5cf8069d823.pdf", "supplementary_material": "/attachment/c25721b5053dc87abe458c387415015bf452f63c.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces the Ensemble Inverse Problem (EIP) framework, in which the goal is to infer an unknown variable from a measurement without explicit access to the forward model or prior distribution. The authors describe two settings, EIP-I (recover prior) and EIP-II (recover posterior). The paper focuses on EIP-II and proposes ensemble inverse generative models (EI-DDPM and EI-FM) that condition posterior sampling on both a single observation and an observation ensemble. To extract information from the observation set, the method uses a permutation-invariant set encoder, such as Deep Sets or the Set Transformer. Experiments include 2-D Gaussian toy distributions, high-energy physics unfolding, and an MNIST image inversion benchmark. The authors claim improved generalization to unseen priors and avoidance of iterative inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper introduces a clear formalization of EIP and distinguishes EIP-I and EIP-II, clarifying the problem space and highlighting a setting where the forward operator is unknown. \n- The method uses established conditional generative models (DDPM and flow matching) and extends them to incorporate set-based ensemble conditioning. \n- A permutation-invariant representation is included to encode the ensemble of observations, aligning with prior set-modeling work. \n- Experiments across multiple synthetic and real-world tasks demonstrate feasibility (2-D Gaussian, HEP unfolding, MNIST inversion)."}, "weaknesses": {"value": "- The method is largely a conditional generative model with an added set encoder. The paper frames this as a new problem, but the algorithmic contribution is incremental, in my opinion.\n- The practical relevance of EIP is not strongly justified. Real-world cases where forward operators are truly unavailable yet training observations exist remain unclear. \n- Toy Gaussian and MNIST blur/noise settings do not convincingly demonstrate scalability or relevance for complex inversion tasks. \n- No comparison to stronger modern inference baselines (e.g., existing diffusion-based inverse samplers outside EIP literature).\n- Duplicating samples when $N’<N$ is a rather ad-hoc approach. \n- Claims of generalization to unseen priors are qualitative and lack theoretical justification.\n- No ablations isolating the benefit of set conditioning versus simply conditioning on summary statistics (moments)."}, "questions": {"value": "- Can you provide some concrete, real-world settings where the forward operator is unknown, but large paired datasets exist for training?\n- How does performance scale with dimensionality beyond MNIST? Are there plans to test the proposed algorithms on natural images or scientific imaging?\n- What specific features of the set encoder contribute to generalization to unseen priors beyond simple moment-based approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xiZsmvNdXI", "forum": "aVXXZAp41g", "replyto": "aVXXZAp41g", "signatures": ["ICLR.cc/2026/Conference/Submission19400/Reviewer_2G23"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19400/Reviewer_2G23"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891240717, "cdate": 1761891240717, "tmdate": 1762931319596, "mdate": 1762931319596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the ensemble inverse problem (EIP) which is described as follows. For training, we are given access to several collections $\\mathcal{D}_m$ of (x, y) pairs, where the y are observations of a noisy forward process on the ground truth variable x. In each collection, the ground truth variables x are sampled from a different prior distribution $p_m$. At test time, we are given a new collection $\\mathcal{Y}$ of only the observations y with x sampled from an unknown, different prior distribution $p$. The goal is to approximate either the prior distribution $p$ on x, or the posterior on x conditional on the observed y, i.e. $p(x|y)$.\n\nThe authors propose two algorithms for solving this problem. These algorithms can be viewed as a modification of the standard ODE and PDE diffusion model procedures, where one trains a neural network to predict the noise component of the target x + Gaussian noise, and progressively remove the noise over many steps to recover the original x. Their modification to this approach is to provide two additional inputs to the denoising network. The first is the observation y from the forward process corresponding to the x which is to be denoised. The second is an encoding of the *collection* of observations $\\mathcal{Y}$ corresponding to the point to be denoised, to help give the denoising network some information about the prior.\n\nThe authors then test their algorithms on a synthetic Gaussian example, simulated particle physics data, and MNIST, and show that their method generally performs comparably or better than relevant baselines. In particular, it is often able to generalize to recover priors which were not seen during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "For the most part, the paper is clearly written and easy to follow.\n\nThe experiments cover a diversity of settings. The synthetic Gaussian experiment is easy to visualize and provides some good intuition, and also shows explicitly how the method can generalize beyond priors not seen during training. The particle physics experiments are helpful as the authors use problems from this field to motivate EIP. The MNIST experiment shows that the proposed method may also have applicability in other realistic settings, with the added difficulty of high dimensionality. Finally, the numerical results are generally favorable for the proposed methods.\n\nThe intuition for the proposed method is clear and it should be easy to incorporate it into general diffusion model training pipelines."}, "weaknesses": {"value": "The methodological novelty is somewhat limited. As described in the summary, the method amounts to providing two additional inputs to the denoising network for a standard diffusion model setup. While it is intuitive that the inclusion of these two quantities can encode information about the prior to be reconstructed in a form which is usable by the denoising network, the paper also does not attempt to place this intuition on more rigorous footing, e.g. with theoretical results or even informal mathematical derivations.\n\nIn the first line of the abstract, the authors claim to introduce a novel inverse problem in EIP. However, on lines 96-97, the authors acknowledge that EIP-II has been directly considered by Pazos et al. (2025). The methods the authors introduce are also developed specifically for EIP-II (line 174, beginning of the Methods section), which seems to contradict the claim in the abstract.\n\nOn the topic of related work, the work is closely connected to the extensive literature on using diffusion models for Bayesian inverse problems, and many methods from this literature could be used as baselines especially for the image denoising experiment. See Appendix A of the following paper for a recent extensive overview:\n\n>Chen, Haoxuan, et al. \"Solving inverse problems via diffusion-based priors: An approximation-free ensemble sampling approach.\" arXiv preprint arXiv:2506.03979 (2025).\n\nFinally, while the experiments are generally favorable for the proposed methods, this is not always the case. For example, in Fig. 3, the authors claim that EI-FM has superior performance compared to all other realistic baselines across the full range of $\\gamma$. However, it actually looks like both Omnifold-best and Omnifold-combine perform better in the $|\\gamma|\\geq 0.75$ (highest and lowest) ranges. Confidence values for Fig. 3 and the numerical results in Table 2 are also not provided."}, "questions": {"value": "Are there any rigorous results which can be given on the ability of the proposed methods to recover the ground truth?\n\nOn line 323 (bottom of pg. 6), the cFM-$\\gamma$ method is mentioned presumably as an \"unrealistically good\" baseline method, as it has direct access to the prior information. However, I didn't see a full description of this method. How exactly does this method work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jBWWPBZNb6", "forum": "aVXXZAp41g", "replyto": "aVXXZAp41g", "signatures": ["ICLR.cc/2026/Conference/Submission19400/Reviewer_i4bK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19400/Reviewer_i4bK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934563044, "cdate": 1761934563044, "tmdate": 1762931319182, "mdate": 1762931319182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes a new inverse problem termed the Ensemble Inverse Problem (EIP), where an additional set of observations is incorporated into the inversion process. The authors propose to address EIP using conditional generative models, specifically diffusion and flow-matching–based approaches (EI-DDPM and EI-FM). The model is evaluated on three tasks—synthetic 2D Gaussian problem, particle-physics data unfolding, and MNIST image inversion—and demonstrates superior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides a clean formalization of the ensemble inverse problem, emphasizing inference across multiple priors with a shared but unknown forward operator."}, "weaknesses": {"value": "- While the EIP formulation is interesting conceptually, its practical relevance to real-world inverse problems is not clear. The authors mention applications in high-energy physics and inverse imaging, but for readers unfamiliar with high-energy physics, the motivation in that domain is difficult to assess. For inverse imaging problems, it is unclear to me how the EIP problem setting arises in practice. \n- Despite the new terminology, Algorithms 1 and 2 follow standard conditional diffusion training and sampling procedures. The only notable modification is the addition of a Set Transformer encoder for $\\mathcal{Y}$, and even this encoder is used only in one experiment (the particle-physics data unfolding). \n- The \"high-dimensional\" inverse imaging experiment only uses MNIST (28x28 dimensions), which is far too simple to demonstrate the proposed model’s utility for serious inverse imaging problems. \n- The proposed method requires retraining for each new forward model, as it depends on paired truth–observation datasets. In many practical settings, generating such datasets or retraining large diffusion models may be infeasible."}, "questions": {"value": "- Can authors provide more concrete examples of how EIP corresponds to a real imaging scenarios (e.g. MRI[1], deblurring[2], etc) as well as experimental evidence? \n- Why do the authors tailor the algorithms specifically for DDPM and FM? It's known that diffusion model is equivalent to FM up to a simple reparameterization for Gaussian prior setting [3]. Are there specific empirical motivations for retaining both?\n\n[1]: Sriram, Anuroop, et al. \"End-to-end variational networks for accelerated MRI reconstruction.\" _International conference on medical image computing and computer-assisted intervention_. Cham: Springer International Publishing, 2020.\n\n[2]: Mardani, Morteza, et al. \"A variational perspective on solving inverse problems with diffusion models.\" _arXiv preprint arXiv:2305.04391_ (2023).\n\n[3]: [Diffusion Models and Gaussian Flow Matching: Two Sides of the Same Coin](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-diffusion-flow-173/blog/diffusion-flow/)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nYbpJcoxp0", "forum": "aVXXZAp41g", "replyto": "aVXXZAp41g", "signatures": ["ICLR.cc/2026/Conference/Submission19400/Reviewer_RwgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19400/Reviewer_RwgK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131433415, "cdate": 1762131433415, "tmdate": 1762931318626, "mdate": 1762931318626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a formulation for inverse problems, which the authors term the \"Ensemble Inverse Problem\" (EIP). The goal of EIP is to recover a \"truth\" distribution, $p(x)$, given a set of observations, $\\mathcal{Y}$, which are generated by feeding samples from $p(x)$ through a fixed but unknown forward model and adding noise to them to build a likelihood distribution $p(y|x)$. This problem is common in science (e.g., \"unfolding\" in particle physics) and imaging, especially when the prior $p(x)$ is unknown.\nThe authors distinguish two versions of the problem:\n1. EIP-I (Prior): Recover the prior $p(x)$ from the ensemble $\\mathcal{Y}$.\n2. EIP-II (Posterior): Recover the posterior $p(x|y)$ for a single observation $y$, using the entire ensemble $\\mathcal{Y}$ as context.\n\nThe core methodological contribution is what the authors call \"ensemble inverse generative models,\" that are trained to approximate $p(x|y, \\mathcal{Y})$. This is achieved by conditioning a generative model (e.g., DDPM or Flow Matching) on both the single measurement $y$ and a learned, permutation-invariant embedding of the entire observation set $\\mathcal{Y}$, $\\phi_w(\\mathcal{Y})$. To enable generalization, the model is trained on a collection of datasets from different priors, all passed through the same unknown forward operator. \nThe authors demonstrate through experiments on synthetic data, a 7D particle physics unfolding problem, and a synthetic image inversion task that their method achieves state-of-the-art results. They show it can successfully generalize to unseen priors that were not included in the training data, a task where naive conditional models fail."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The formalization of the \"Ensemble Inverse Problem\" (EIP) is a significant contribution. This is essentially a new \"in-context learning\" or \"meta-learning\" framework for scientific inverse problems, which is potentially impactful. The proposed solution—conditioning on a learned, permutation-invariant embedding of the observation set $\\mathcal{Y}$—is an elegant and general way to solve the problem. Instead of using hard-coded statistics (like moments), it allows the model to learn the most relevant features of the observation set that define the prior, and does not require knowledge of the forward model. \n2. The experiments are designed to test the paper's central claim.\n    * In the synthetic 2D Gaussian and MNIST-mixture experiments, the model is explicitly trained on a disjoint set of priors and tested on the \"in-between\" region (e.g.,  interpolation between seen regions).\n    * The results in Figures 3 and 6 are unambiguous . The proposed method (EI-FM/EI-DDPM) successfully generalizes and solves the problem in the unseen regions, while the baseline (cFM/cDDPM) that lacks the ensemble information completely fails. This is a very powerful demonstration.\n3. The method achieves SOTA results on a real world HEP unfolding task, outperforming multiple baselines (including the similar GDDPM) on 4 unseen physics processes. This shows the approach scales and is practical for complex scientific data."}, "weaknesses": {"value": "1. Ambiguous Experimental Setups: A few points in the paper would need to be clarified, specifically about the experimental setups. \n    - For the HEP experiment:  My understanding was that the paper's premise is that the forward model and the noise model that allow building $p(y|x)$ need to be fixed. However, the data description mentions using \"various parton distribution functions and parton shower models\". This sounds like the underlying physics, and thus $p(y|x)$, is changing across the different datasets in M. This would really need clarification. \n\n    - For the MNIST digit mixture problem, it’s not clear what is the goal of the task. It is to recover one of the deblended digits (or both), or is it to recover the blended mixture at some given time (making it much more like an inpainting+denoising task)? From the text I understood it was the former, but Figure 6 seems to suggest the latter. This needs to be clarified in the text.   \n2. Limited baseline comparisons: The paper positions itself as a general solution for inference, but I was surprised to see that baseline comparisons were limited. While it’s true that most inference methods that have been developed with scientific applications in mind rely on knowledge of a forward model (except on naive conditioning on observation with diffusion and flow matching), with the paired dataset that is assumed to be available, it would be natural to train an emulator for the forward model, which could then be used in a standard inference framework like SBI. This is the natural baseline to compare the proposed method against (and, as I understand, a standard method for HEP applications as well). This would potentially be impactful since SBI methods are known to struggle with covariate shifts, so it’d be natural to show the strength on the proposed method against this specific baseline, and not including it is a clear lack in the paper in its current state (7dimension is very low-dimensional, definitely within the applicability of SBI methods). Moreover, once an emulator has been trained, it would be natural to compare against population-level inference frameworks to empirically adapt or learn to correct misspecified priors, such as 2402.07808, or more general expectation-maximization methods like those proposed in 2405.13712 or 2407.17667. Seeing how the proposed methods compare to these more recent proposed approaches is a natural question for a practitioner.  \n\n3. Limited Evaluation Metrics: The paper evaluates the quality of their inference using metrics like SWD, WD, MSE, and SSIM. These metrics compare bulk properties, means, or medians, and these can miss key differences between high-dimensional distributions.  Coverage tests like TARP (2302.03026) or accuracy scores like PQMass (2402.04355) would be needed to assess if the inferred posteriors/priors here accurate in terms of the entire distribution. This is especially a problem for the image-space application with MNIST, since just the MSE or the SSIM of the mean of the posterior is assessed. The mean, specifically in image space, could look nothing like any individual sample and this could affect the results specifically for structural integrity. It could also favor methods that are overconfident.  \n\n4. Unclear Scaling: The paper claims scalability, but the \"high-dim\" MNIST test is only $28 \\times 28$. A real study of how this method would scale in real high-dimensional settings (e.g., ImageNet-scale), is missing. \n\n5. Limited scientific applications: The performance of the method is demonstrated in a single scientific context (HEP), but the generals claims are that the method is useful for general scientific applications. Applications to other scientific contexts like those in the benchmark in 2503.11043 would really strengthen the paper. Other domains of applications where a forward model is generally not known are, for example, weather forecasting, climate modelling, and cosmology."}, "questions": {"value": "1. About the fact that the physical model seems to be required to be fixed across M: In most scientific applications I’ve seen, when the true physical model is unknown, what is available is typically a whole menu of physical simulators that approximate this underlying physical model to various degrees of fidelity (and various levels of compute cost). It’s those approximate simulators that allows building the mapping from x to y. But in that context, 1- it’s never guaranteed that the true mapping in within the set of simulations we can access, 2- the physical model would be changing between simulations and real experiment where we want to apply the trained model. This is related to point 1 in the weaknesses, can you clarify the assumptions you are working under, and if the physical model does need to be fixed, I’d like it to be explained more clearly how this is supposed to applied to a realistic real-world experimental setup.\n\n2. Similarly, it’s not clear from the paper if the proposed method would work is the different priors were over a heterogenous parameter space, meaning that $p_m(x)$ have different dimensionalities for $x$ across different $m$? This is another context where I could see this applied in a scientific context where one would like to do Bayesian model comparison/fit different model with varying number of parameters. Is that something that could be possible?  \n3.  I was surprised to see that GDDPM-v (with more information) performs worse than GDDPM (with less). This suggests potential overfitting or non-optimal training, which weakens the claim of SOTA performance. Adding information should not impair recovery if the model is well-trained, do you have an explanation for why this is not the case?\n4. Could you provide more details on how the experiments were set up? For example, for Fig 2, how was the prior obtained from observations for the conditional diffusion model and flow matching? Was it by getting the MAP or the mean of the posterior samples for multiple observations? Also, could you provide more details on how the conditioning was done for the other models for the MNIST experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yQtqi84a3F", "forum": "aVXXZAp41g", "replyto": "aVXXZAp41g", "signatures": ["ICLR.cc/2026/Conference/Submission19400/Reviewer_Ay8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19400/Reviewer_Ay8h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762269798659, "cdate": 1762269798659, "tmdate": 1762931318290, "mdate": 1762931318290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}