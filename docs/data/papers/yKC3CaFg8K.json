{"id": "yKC3CaFg8K", "number": 17469, "cdate": 1758276384590, "mdate": 1763657656620, "content": {"title": "Reconciling Visual Perception and Generation in Diffusion Models", "abstract": "We present \\textsc{GenRep}, a unified image understanding and synthesis model that jointly conducts  discriminative learning and generative modeling in one training session. By leveraging  Monte Carlo approximation, \\textsc{GenRep} distills distributional knowledge embedded in diffusion models to guide the discriminative learning for visual perception tasks. Simultaneously, a semantic-driven image generation process is established, where  high-level semantics learned from perception tasks can be used to inform image synthesis, creating a positive feedback loop for mutual boosts. Moreover, to reconcile the learning process for both tasks, a gradient alignment strategy is proposed to symmetrically modify the optimization directions of perception and generation losses. These designs empower \\textsc{GenRep} to be a versatile and powerful model that achieves top-leading performance on both image understanding and generation benchmarks. Code will be released after acceptance.", "tldr": "We present GenRep, a unified image understanding and synthesis model that jointly conducts discriminative learning and generative modeling in one training session.", "keywords": ["Visual Perception", "Image Classification", "Object Detection", "Semantic Segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41fe1d704aafbdb94774ba571eb99b0171f09c6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a unified framework, **GENREP**, that jointly trains discriminative visual understanding and generative image synthesis in a single run:\n(1) It uses a *Monte Carlo approximation along reverse diffusion trajectories* to distill distributional knowledge from a diffusion model, treating it as Bayesian-posterior evidence to aid downstream perception;\n(2) On the generative side, it introduces *semantics-driven generation*, where semantic embeddings from the perception branch jointly modulate both the mean and the variance to steer the reverse process toward target semantics; \n(3) It applies *symmetric gradient alignment* between perception and generation losses to mitigate multi-objective conflicts, forming a mutually reinforcing loop. \nThe authors report strong results on classification, depth estimation, open-vocabulary segmentation/detection, and class-conditional generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation to unify and mutually promote visual perception and visual generation tasks by leveraging their different modeling characteristics is excellent.\n2. In terms of implementation, the generative capability is effectively used to construct a sample distribution to guide perception, while the feature extraction capability from the understanding branch is used to assist denoising-based generation. This approach makes good use of the valuable information from both tasks.\n3. The experiments are comprehensive. The paper provides detailed comparisons for both perception and generation, along with multi-level ablation studies that successfully demonstrate the effectiveness of the proposed solution."}, "weaknesses": {"value": "1. Using the generative prior `p(x|y)` to estimate the posterior `p(y|x)` is inaccurate. The diffusion loss primarily learns the distribution of `x` given a specific condition `y`, and the vast majority of the learning signal is dominated by the mean, not the variance. (As demonstrated in \"Improved Denoising Diffusion Probabilistic Models,\" the supervision signal for the variance must be small, otherwise, the generation quality degrades). This strongly suggests that the variance modeling is inaccurate. More intuitively, the mean is far more important than the variance in diffusion-based generative modeling. Furthermore, the assumption of a uniform distribution for `y` itself is questionable. The pseudocode in the appendix indicates that during training, `p(y'|x)` for all classes `y'` is treated as equally probable, rather than being reestimated from `p(x|y')`. This is clearly inaccurate in cases where there are multiple most-likely labels `y`, and thus it cannot serve as an effective distributional supervision signal.\n2. The perception task is trained only on clean samples. It is uncertain whether the model can maintain good representation capabilities when applied to noised latents."}, "questions": {"value": "1. How does the generative distillation perform on samples where there are multiple (or ambiguous) most-likely labels `y`? Is the estimated distribution accurate in such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1WKUZCpJJO", "forum": "yKC3CaFg8K", "replyto": "yKC3CaFg8K", "signatures": ["ICLR.cc/2026/Conference/Submission17469/Reviewer_poem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17469/Reviewer_poem"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617003864, "cdate": 1761617003864, "tmdate": 1762927352084, "mdate": 1762927352084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified model using GenRep pipeline that jointly learns discriminative and generative modeling with a single diffusion model. The paper aims for mutual enhancement and achieves this by jointly optimizing the two objectives. For example, for perception, the model distills distributional knowledge from the generative diffusion process using Monte Carlo approximation to better approximate conditional distributions. For generation, learned high-level representations are used to guide the reverse diffusion process. Then gradient alignment mechanism is used to resolve conflicts in the updates."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- High-performing framework: GENREP successfully combines discriminative and generative paradigms without sacrificing either performance, achieving state-of-the-art or highly competitive results on both perception and generation, across various datasets\n- Mutual learning synergy: The dual-branch setup intuitively builds a feedback loop — discriminative features improve the quality and faithfulness of generated images, while generative modeling provides richer distributional supervision for perception tasks. This is supported by the improved performance on downstream tasks \n- Generalization across domains: The model exhibits strong out-of-distribution performance (e.g., ObjectNet) and robustness across task types, indicating that the shared latent space supports better transferability and adaptability.\n- Clean optimization design: The proposed gradient alignment mechanism provides a clear way to balance conflicting gradients, promoting stable joint optimization."}, "weaknesses": {"value": "- Computational overhead: While GENREP achieves impressive performance, integrating a diffusion model for perception tasks increases computational cost and reduces inference efficiency (e.g., 12.6 FPS vs. MaskFormer’s 19.6 FPS). This may limit its applicability in latency-sensitive or large-scale deployment scenarios.\n- Approximation sensitivity and bias–variance trade-off: The performance of the generative visual perception component depends on the strided sampling interval (k) used in the Monte Carlo approximation. While a small k (e.g., 1) leads to correlated samples, increasing k beyond 2 reduces correlation but introduces higher variance and less accurate distributional estimates. The authors observe that performance peaks at k = 2 and degrades for larger k values, which is unintuitive because larger strides should, in theory, produce more independent samples and better approximate the true distribution.\n- Multimodal Scalability: GENREP focuses on purely visual integration, but extending this framework to text-conditioned or cross-modal setups could further test its generality. The model currently does not explore alignment with language-conditioned diffusion models."}, "questions": {"value": "- Training stability and sensitivity: While the paper reports stable convergence, the model jointly optimizes several interdependent losses (perception, generation, distillation, and alignment). How sensitive is GENREP to hyperparameter choices and architectural balance between these objectives? Did the authors observe instability or mode collapse when tuning these components across different datasets or backbones?\n- Sampling interval (k) trade-off: Table 9 suggests performance peaks at k = 2 but declines when k increases further. Could the authors elaborate on the underlying cause of this bias–variance trade-off? Intuitively, higher k should yield more independent samples — is the degradation due to sparse coverage of the diffusion trajectory or reduced effective sample size?\n- Could the GENREP architecture be extended to incorporate text-conditioned or open-vocabulary supervision (e.g., via CLIP embeddings)? How might the gradient alignment mechanism adapt in that scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ovvmc7VozR", "forum": "yKC3CaFg8K", "replyto": "yKC3CaFg8K", "signatures": ["ICLR.cc/2026/Conference/Submission17469/Reviewer_aqJQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17469/Reviewer_aqJQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984052869, "cdate": 1761984052869, "tmdate": 1762927351581, "mdate": 1762927351581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GENREP, a framework aiming to unify image understanding (perception) and image synthesis (generation). The method comprises three main components: \n1) Generative Visual Perception Learning: Using Monte Carlo approximation (a mature approach from Generative Classifier) on intermediate (noisy) samples from the reverse diffusion process to estimate the class posterior $p(y|x)$, which serves as an auxiliary distillation loss $\\mathcal{L}_{gen \\underline{ } distil}$ to guide the perception task. \n2) Semantic-Driven Generation: Using high-level semantic representations ($x_{sem}$) from the perception task to dynamically modulate the mean ($\\mu_\\theta$) and variance ($\\sigma_\\theta$) of each step in the generative process. \n3) Gradient Alignment: Using a gradient projection strategy from multi-task learning to reconcile conflicts between the perception and generation objectives. \n\nThe authors claim this unified model achieves SOTA performance on numerous perception and generation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Meaningful Goal: The attempt to unify discriminative learning (perception) and generative modeling (generation) within a single framework that allows for mutual benefits is a valuable research direction.\n2. Interesting Core Idea: The paper links generation and perception of a same Diffusion model via Bayes' theorem, which is a reasonable and natural synergy.\n3. Extensive Empirical Evaluation: The paper validates its approach across a wide range of benchmarks, including classification, segmentation, etc, achieving competitive results."}, "weaknesses": {"value": "1.  The Necessity of Framework: The paper claims its novelty lies in adding \"high-level semantics\" to overcome the \"low-level reconstruction\" limitations of other unified models. However, the proposed mechanisms (MC estimation of $p(y|x)$ and explicit $\\mu_\\theta, \\sigma_\\theta$ modulation) are exceptionally complex (especially for computation), and I am not aware of how the so-called \"high-level semantics\" are contributing or whether they are necessary. In contrast, other unified frameworks (e.g., Diff-2-in-1 [1]) appear to achieve a similar unified goal via a much simpler self-training/mean-teacher loop, even without explicit \"high-level semantics\". This severely questions the necessity of GENREP's convoluted approach (too much cost for limited improvement, or meaningless innovation). Also, please conduct a more comprehensive literature review and a more detailed comparison of similar unified diffusion models, like [1].\n2.  Theoretically Flawed MC Approximation: The \"Generative Visual Perception Learning\" relies on an MC approximation of $p(x|y)$. This estimation is theoretically flawed in several major ways:\n    * Correlated Samples: It uses samples from a Markov chain (the reverse diffusion process). The claim that strided sampling ($k=2$) is sufficient to \"break\" this correlation is unsubstantiated and highly unlikely.\n    * Biased Samples: It uses *intermediate, partially-denoised* samples to approximate the *final, clean* data distribution $p(x|y)$. This is a fundamentally biased estimator.\n    * Computational Overhead: As shown in Algorithm 1, this approximation must be computed *inside every training iteration*, adding massive, unanalyzed computational overhead. Can this approximation be further accelerated?\n\n\n[1] Zheng, S., Bao, Z., Zhao, R., Hebert, M., & Wang, Y. X. Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models. In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1.  Validity of MC Approximation: Can you provide stronger evidence that strided sampling ($k=2$) is sufficient to yield (near) independent samples from the reverse diffusion process? Furthermore, can you quantify the bias introduced by using noisy, intermediate samples to estimate the final, clean distribution $p(x|y)$?\n2.  Question on \"Positive Feedback Loop\": I am curious to see from Table 8 (rows #1, #3), it shows that  adding Semantic-Driven Generation (Percept -> Gen) *hurts* perception (Top-1/mIoU drops). Some explanations or investigations? This is helpful for the claims of \"positive feedback loop\".\n3.  Quantification of Training Cost: Compared to a standard perception baseline or a simpler unified framework (e.g., Diff-2-in-1), please quantify the extra training cost (e.g., in GPU-hours) introduced by the iterative Monte Carlo sampling required by your generative perception loss (Algorithm 1 in Appendix A.7)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LOu4nUbccP", "forum": "yKC3CaFg8K", "replyto": "yKC3CaFg8K", "signatures": ["ICLR.cc/2026/Conference/Submission17469/Reviewer_fxcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17469/Reviewer_fxcY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984457282, "cdate": 1761984457282, "tmdate": 1762927351189, "mdate": 1762927351189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present GenRep, a unified image understanding and synthesis model that jointly conducts discriminative learning and generative modeling in one training session. This paper adopts a novel gradient alignment strategy to guide the joint optimization of perception and generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the idea to distill distributional knowledge embedded in diffusion models to guide the discriminative learning for visual perception tasks is novel\n- the experimental results are comprehensive \n- the paper is well-written \n- the derivation is clear"}, "weaknesses": {"value": "- the gradient alignment may introduce extra computational overhead during training."}, "questions": {"value": "- can the proposed methods be extended to more visual understanding tasks, such as OCR, captioning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v5ucfrlZGl", "forum": "yKC3CaFg8K", "replyto": "yKC3CaFg8K", "signatures": ["ICLR.cc/2026/Conference/Submission17469/Reviewer_8E75"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17469/Reviewer_8E75"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988200389, "cdate": 1761988200389, "tmdate": 1762927350393, "mdate": 1762927350393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}