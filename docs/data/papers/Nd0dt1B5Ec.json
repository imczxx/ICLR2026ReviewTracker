{"id": "Nd0dt1B5Ec", "number": 17391, "cdate": 1758275350722, "mdate": 1763740080341, "content": {"title": "A Novel Architecture for Integrating Shape Constraints in Neural Networks", "abstract": "This research proposes COMONet (Convex-Concave and Monotonicity-Constrained Neural Networks), a novel neural network architecture designed to embed inductive biases as shape constraints—specifically, monotonicity, convexity, concavity, and their combinations—into neural network training. Unlike previous models addressing only a subset of constraints, COMONet can comprehensively integrate and enforce eight distinct shape constraints: monotonic increasing, monotonic decreasing, convex, concave, convex increasing, convex decreasing, concave increasing, and concave decreasing. This integration is achieved through a unique partially connected structure, wherein inputs are grouped and selectively connected to specialized neural units employing either exponentiated or normal weights, combined with appropriate activation functions. Depending on the shape constraint required by each input, COMONet dynamically utilizes its full architecture or a partial configuration, providing significant flexibility. We further provide theoretical guarantees ensuring the strict enforcement of these constraints, while demonstrating that COMONet achieves performance comparable to existing benchmark methods. Moreover, our numerical experiments confirm that COMONet remains robust even under noisy conditions. Together, these results underscore COMONet’s potential to advance constrained neural network training as a practical and theoretically grounded approach.", "tldr": "", "keywords": ["Shape constraint", "Convexity", "Monotonicity", "Neural network", "Regularization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76fb4198c6b5d4e9a30a21efe6b3d8a186c60579.pdf", "supplementary_material": "/attachment/394ab52be131113696c7e9dba4ab3f06ed2f20ff.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes COMONet, a neural architecture designed to enforce combinations of convexity, concavity, and monotonicity constraints by partitioning inputs into subnetworks specialized for each constraint type (convex, concave, monotonic, ReLU, and mirrored ReLU). The authors aim to guarantee constraint satisfaction by design and demonstrate empirical performance on synthetic and real-world datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1) The paper is well written and attempts to generalize known apporaches to design convex neural nets.\n2) The proposed model attempts to generalize existing architectures for convex and monotonic neural networks.\n3) The experimental section includes both synthetic and real datasets, which helps illustrate potential applicability."}, "weaknesses": {"value": "1. Unverified mathematical claims:\na) Equations (6)- (15) are introduced without derivation or citation to standard convex analysis texts (see Boyd & Vandenberghe, Convex Optimization, Section 3.2).\nb) Theorem 4.9 is incorrect. The claim that any twice-differentiable function that is convex in one variable and concave in another must admit only a bilinear cross term is false. For instance, $f(x1,x2)=5x_1^2 - 5x_2^2+sin(x_1)sin(x_2)$ satisfies the local convex-concave property but violates the claimed decomposition. The pariwise interaction terms can be more complex than bilinear, it is however, possible that nonlinear combinations of bilinear terms can approximate such functions. \nc) In equation 19, $x_1$ is not a monotonically increasing feature. \n\n2. There is no discussion about expressivity of the proposed architecture.\nConstraint enforcing architectures often trade off expressivity for guaranteed properties. The paper does not discuss whether COMONet can approximate a wide class of functions under the imposed constraints, or if the constraints severely limit the function space. \n\n3. Lack of ablation studies.\nThe experimental section lacks ablation studies to isolate the contributions of different components of the architecture. As it stands, it is possible that the only reason for good empirical performance is the ReLU subnetwork, or overparameterization compared to the baselines. Ablation studies removing subnetworks or varying their sizes would help clarify this.\n\nOverall, I do not recommend acceptance in the current form, as a lot of mathematical claims are unverified, and some are incorrect. The paper also lacks a convincing application where such a generalization of prior works that enforce convexity or monotonicity is necessary. Finally, I would also encouraage the authors to include a formal study on the exprressivity of the proposed architecture."}, "questions": {"value": "1) Can you provide a more detailed motivation for the architectureincluding an example?\n\n2) Can you provide proofs or references for equations 6-15?\n\n3) Can you clarify the proof of Theorem 4.9?\n\n4) Can you clarify the monotonicity of $x_1$ in equation 19\n\n5) can you clarify what you mean by \"The LLM was specifically employed to refine expressions and to check the clarity and\ncorrectness of mathematical formulations authored by us.\"  First, I am concerned about the word \"correctness\".\nSecond, there appears to be many loose mathematical statements which are typical of chatgpt (i.e. they are not completely wrong, but not precise either)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xe8wfwwFZr", "forum": "Nd0dt1B5Ec", "replyto": "Nd0dt1B5Ec", "signatures": ["ICLR.cc/2026/Conference/Submission17391/Reviewer_BzaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17391/Reviewer_BzaC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803305782, "cdate": 1761803305782, "tmdate": 1762927297996, "mdate": 1762927297996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a new network architecture that uses partial connections impose monotonicity and convexity/concavity shape constraints. Experimental results are good but not impressive."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a practical problem that warrants more research, and their approach seems reasonable and not overly complex. \n\nNice intro, but I highly recommend also including a simple real-work example in the intro to help readers grasp the issue and why you might want a constraint like this. \n\nOn Motivation: Shape constraints are a terrific regularizer, and they aren’t just for specialized needs like healthcare, shape constraints (via the lattice models approach of Gupta et al.) have been widely used at Google in all sorts of ML just for the regularization, interpretability, and reduced churn, there are some great real-world case studies in their 2016 JMLR paper, Monotonic Calibrated Interpolated Look-Up Tables as well as some of the experiments in their later shape constraint papers (some of which you already cite).  Shape constraints are also useful for for fairness (Wang, AI Stats 2020)\n\nI thought the coverage of related work was good (but the details about the related work were not suffciently accurate - see Weaknesses)\n\nThe visualization design of Fig 1 was great (but not sufficiently accurate - see Weaknesses)."}, "weaknesses": {"value": "It is a good paper, the main negative is just that the proposal doesn't seem super significant in terms of what they propose or the experimental achievements, and the bar for NeurIPS is just so high that I'd have trouble championing this paper to get in, but I definitely think there's some nice work here and would like to see it published somewhere. \n\nI have some kvetching about their description of the related work in lattice models, which have been widely used at Google for shape-constrained ML.\n\nRe: Related work - Authors say “PenDer (Gupta et al., 2021)\nappears to be the only existing method capable of incorporating all eight shape constraints depicted in\nFig. 1(a).” but Google’s lattice models can incorporate all of those constraints as well.  In fact the Gupta 2018 paper you cite says in the abstract “We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of concavity/convexity and monotonicity constraints on any subsets of features”. It is incorrect that your chart in Fig 1 b has X’s (no’s) for Shape-constrained Lattice for only-monotonic increasing and monotonic decreasing, but in fact that’s a standard use case of shape-constrained lattices, see the 2016 JMLR paper \nMonotonic Calibrated Interpolated Look-Up Tables.  However, Table Fig 1.b is is correct for SCNN.\n\nre: “Due to this limitation, there remains a need for\nthe development of a method that can integrate all possible shape constraints while strictly guarantee\ntheir satisfaction.”  Lattice models can do that. \n\nApart from thinking you are incorrect about what can be done with Lattice models, I really liked the information presentation of Fig. 1, thanks!\n\nRe: “However, the former relies on a lattice structure to enforce shape constraints,\nresulting in significantly increased computational complexity as the input dimension grows, making it non-scalable.”  While that’s true of a single lattice, note that the papers you cite use an ensemble of such lattices to handle large numbers of features and as their papers prove, the ensemble satisfies these shape constraints as long as the base models do. So it’s not reasonable to say that work doesn’t scale when it does.  \n\nRe: “We introduce three\nclasses of local shape constraints—partial convexity, partial concavity and partial monotonixity—that\napply to subsets of the input coordinates.”  You aren’t “introducing” any of those, those are all known concepts (and typo in monotonicity).\n\nMINOR: \n- Seems weird to cite a 2010 paper for ReLU, I mean they date to Householder’s 1941 work, or maybe cite some later paper that digs into them in a more neural-nety usage but that would be some paper from the 60’s, or citing any ML textbook is fine, but also I don’t think one needs to cite anything for standard ReLU at this point.  \n\n- Use {} around titles in your bib tex to get them to capitalize correctly and consistently please."}, "questions": {"value": "For multiple inputs, do they handle/impose joint convexity (like Amos) or ceterus paribus convexity like lattice models (see the discussion and examples on this in Section 1 of Diminishing Returns Shape Constraints, Gupta et al. 2018). Your definition of partial convexity looks like a ceterus paribus definition, but your architecture is so similar to SCNN that I think you will get joint convexity (which is a more restrictive constraint). \n\nIs the proposal to use RELU units necessary, that is, is that choice that key to achieving the shape constraints, or would any convex activation function work fine? \n\nIn Table 3 the Puzzle Sales Test MSE for SCNN concave and increasing is listed as 9258 +/319, but in the Gupta et al. 2018 Diminishing Returns paper they give it the much better value of Test MSE: 6927, and the dataset has a fixed, real-world non-iid Train/Val/Test split. Did you use this dataset differently and that’s why the results are different? Can you provide the results for their real-world non-iid Train/Val/Test split?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wZ6w71r5em", "forum": "Nd0dt1B5Ec", "replyto": "Nd0dt1B5Ec", "signatures": ["ICLR.cc/2026/Conference/Submission17391/Reviewer_1yni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17391/Reviewer_1yni"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873098407, "cdate": 1761873098407, "tmdate": 1762927297630, "mdate": 1762927297630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new architecture that is able to enforce multiple shape constraints simultaneously, such as monotonicity, convexity, concavity, and their combinations. Existing methods either support only a subset of these constraints or do not guarantee compliance. On the other hand, the proposed approach is able to enforce shape constraint types at the same time and offers theoretical guarantees.\nThis is achieved by routing the features through modules with specific behavior, masking the connectivity of the network units to maintain the desired shape constraints. \nThe method is evalutaed both on synthetic and real-world datasets showing better or comparable predictive performance compared to state-of-the-art."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well structured and very clear. The idea is interesting and well executed. The experimental evaluation covers its basis and seems convincing and exhaustive."}, "weaknesses": {"value": "The proposed network mostly feels like a combination of previous architectures, which makes the novelty of the contribution a bit limited. Also, I am unsure that the combination of multiple shape constraints in a single interconnected network is particularly useful in itself."}, "questions": {"value": "Can you provide more concrete examples of application where satisfying multiple constrains simultaneously is critical?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pQznXDzL7u", "forum": "Nd0dt1B5Ec", "replyto": "Nd0dt1B5Ec", "signatures": ["ICLR.cc/2026/Conference/Submission17391/Reviewer_3grh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17391/Reviewer_3grh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941073684, "cdate": 1761941073684, "tmdate": 1762927296981, "mdate": 1762927296981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces COMONet (Convex–Concave and Monotonicity-Constrained Neural Networks), a novel architecture that embeds shape constraints. Unlike prior approaches that handle only subsets of these constraints, COMONet can enforce all eight possible combinations (monotonic increasing/decreasing × convex/concave). The model achieves this using a partially connected modular design with five specialized unit types (convex, concave, monotonic, ReLU, and reflected-ReLU units) that guarantee constraint satisfaction by design. The authors provide theoretical proofs of convexity, concavity, and monotonicity preservation, along with experimental validation on both synthetic and real-world datasets. Empirical results show that COMONet achieves competitive predictive performance while adhering to shape constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a clear, well-motivated approach that unifies convex, concave, and monotonic constraints under one architecture. Each unit’s properties (convexity, concavity, monotonicity) are formally proven, and the composition of layers preserves these guarantees."}, "weaknesses": {"value": "My biggest concern with the proposed method is that it assumes a fixed, a priori assignment of features to constraint types (e.g., monotonic, convex, concave). This means that each input variable can only satisfy one type of constraint at a time. In practical applications, however, a single feature may exhibit different shape behaviors depending on its interaction with other features (e.g., jointly convex with some, concave with others). The current formulation of COMONet—and its theoretical guarantees in Theorems 4.6–4.8—do not appear to extend to such mixed or conditional relationships. Could the authors clarify whether COMONet can represent or guarantee such cross-constraint interactions?"}, "questions": {"value": "See the weakness above. \n\n\nMinor:\n1. Line 137: \"monotonixity\"\n2. Line 146: \"represents that set...\" -->\" represents the set...\"\n3.  Line 216: \"selectively\"\n4. Line 353: \"as shown in the table 5\" -->\"as shown in table 5\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "edqZHv6j8c", "forum": "Nd0dt1B5Ec", "replyto": "Nd0dt1B5Ec", "signatures": ["ICLR.cc/2026/Conference/Submission17391/Reviewer_pS2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17391/Reviewer_pS2q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087066762, "cdate": 1762087066762, "tmdate": 1762927296548, "mdate": 1762927296548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}