{"id": "vznIYSnv9J", "number": 15477, "cdate": 1758251739643, "mdate": 1759897304400, "content": {"title": "GenDR: Lighten Generative Detail Restoration", "abstract": "Although recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable progress, the misalignment of their targets leads to a suboptimal trade-off between inference speed and detail fidelity. Specifically, the T2I task requires multiple inference steps to synthesize images matching to prompts and reduces the latent dimension to lower generating difficulty. Contrariwise, SR can restore high-frequency details in fewer inference steps, but it necessitates a more reliable variational auto-encoder (VAE) to preserve input information. However, most diffusion-based SRs are multistep and use 4-channel VAEs, while existing models with 16-channel VAEs are overqualified diffusion transformers, e.g., FLUX (12B). To align the target, we present a one-step diffusion model for generative detail restoration, GenDR, distilled from a tailored diffusion model with a larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via representation alignment to expand the latent space without increasing the model size. Regarding step distillation, we propose consistent score identity distillation (CiD) that incorporates SR task-specific loss into score distillation to leverage more SR priors and align the training target. Furthermore, we extend CiD with adversarial learning and representation alignment (CiDA) to enhance perceptual quality and accelerate training. We also polish the pipeline to achieve a more efficient inference. Experimental results demonstrate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity.", "tldr": "", "keywords": ["Diffusion", "Super-Resolution", "Score distillation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc5a32e3601ecb06e4628dec3b92763cb13cc9dc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GenDR for better tradeoff between detail enhancement and inference efficiency. By utilizing a pre-trained 16-channel VAE, it expands the restoration capacity of the proposed SR model with a 0.9B UNet backbone. To reduce computational overhead, it introduces consistent score identity distillation technique to effectively train a one-step model while preserving its ability to generate vivid details. Extensive experiments demonstrate its ability on diverse scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* By utilizing a pre-trained 16-channel VAE, GenDR effectively expands the capacity of the proposed SR model.\n* It proposes a SR-tailored step distillation technique CiDA that restore vivid details and stabilize training.\n* Extensive experiments demonstrate its effectiveness on various benchmark."}, "weaknesses": {"value": "* Since one of the main contributions of this paper is expanding SR capacity by a large channel VAE, more analysis about this component should be added, including the choice of the latent channel, training difficulty and reconstruction ability. \n* Results of multi-step GenDR model are absent. Although authors provide comparison between various distillation methods, comparing with the teacher model can further validate the performance of the proposed CiDA.\n* While replacing the text encoder by a fixed positive embedding reduces inference overhead with slight performance degradation, results on Tab. 7 shows that slightly increasing CFG scale leads to worse performance. I am concerned that the weak textual guidance might come from using a fixed embedding. The authors should consider testing on input images that require high semantic control, such as portraits or text-containing inputs."}, "questions": {"value": "* The proposed method utilizes a generator and two trainable score networks to perform the score distillation process. Thus, quantitative comparison of memory usage should be provided.\n* The authors should consider providing more image results as supplemental material."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T9M3SOzvyW", "forum": "vznIYSnv9J", "replyto": "vznIYSnv9J", "signatures": ["ICLR.cc/2026/Conference/Submission15477/Reviewer_kCw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15477/Reviewer_kCw1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761039727455, "cdate": 1761039727455, "tmdate": 1762925768120, "mdate": 1762925768120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GenDR, a method for efficient super-resolution (SR) that combines VAE-16 and small latent diffusion models (LDMs) using score distillation. The authors claim their approach enhances restoration quality while maintaining speed, achieving superior performance over existing methods. The paper’s main focus appears to be on improving the balance between fidelity and inference time in diffusion-based SR models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The combination of VAE-16 with latent diffusion models, coupled with score distillation, is an interesting approach for super-resolution.\n2. The experimental results demonstrate that GenDR outperforms existing models in terms of both quality and efficiency, with strong quantitative and qualitative performance across multiple benchmarks.\n3. The method is designed to be efficient, offering improved restoration speed without compromising visual fidelity, which is a significant advantage in real-world applications."}, "weaknesses": {"value": "1. The paper struggles with clarity and coherence, especially in how it connects its contributions. While the abstract and introduction spend significant time discussing the advantages of using VAE-16 for SR tasks, the method section shifts focus entirely to the proposed CiDA loss. This abrupt shift makes it difficult to understand the relationship between the two contributions, as they don’t seem to be closely tied. \n2. While the paper introduces CiDA as a novel technique for distilling scores, its relevance to the task of detail restoration in SR is not sufficiently discussed. The paper does not clearly explain why this loss function is needed for SR or how it helps improve the restoration of fine details. The overall discussion of CiDA feels more aligned with general diffusion model training rather than specifically addressing the SR challenge.\n3. The methodology section is dense and difficult to follow. The authors jump between various components like VAE-16, CiD, and adversarial learning without fully explaining their relationships or how they work together to address the SR problem. Additionally, the notations used in the equations make the explanation harder to follow (e.g, $f(\\cdot)$ and $\\epsilon(\\cdot)$ seem to both refer to the scores)."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QMuNK8QJhv", "forum": "vznIYSnv9J", "replyto": "vznIYSnv9J", "signatures": ["ICLR.cc/2026/Conference/Submission15477/Reviewer_5Lfg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15477/Reviewer_5Lfg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827706699, "cdate": 1761827706699, "tmdate": 1762925767762, "mdate": 1762925767762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GenDR, a diffusion-based model for real-world image super-resolution (SR) that aims to address the long-standing trade-off between detail fidelity and inference efficiency. The authors identify that most text-to-image (T2I) diffusion models are suboptimal for SR tasks because (1) they use low-dimensional latent spaces (typically 4-channel VAEs), and (2) they require multi-step inference to synthesize images. To overcome this, the authors develop an SD2.1 model with a 16-channel VAE backbone and a consistent score identity distillation strategy.  Empirically, GenDR achieves competitive or superior performance over state-of-the-art (SOTA) models such as DiffBIR, OSEDiff, and DreamClear, with notable improvements in LIQE, MUSIQ, and Q-Align scores and a 77 ms runtime."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I agree with the claim on the disadvantages of the 16-channel VAE, which probably limits the performance upper bound of diffusion-based SR methods. As illustrated in Table 2, such a modification indeed improves the SR results."}, "weaknesses": {"value": "1. My main concern mainly focuses on the experimental performance. The authors claim that the proposed method is able to enhance the detail fidelity. The quantitative comparison results in Table 1 cannot support such a claim, in which GenDR does not show obvious improvements regarding the fidelity metric, such as PSNR, LPIPS. \n\n2. As for the visual results, I personally don't think GenDR is better than other SoTA methods, particularly in the first example in Fig. 1 and the second example in Fig. 5.\n\n3. As for the efficiency, GenDR shows slight advantages compared with InvSR and OSEDiff. I guess such improvement is due to the removal of the text encoder. However,  this trick is also suitable for other methods (e.g., InvSR) that do not rely on dynamic textual information."}, "questions": {"value": "I wonder about the effectiveness of RAPE regularization in SR task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hENre89iZx", "forum": "vznIYSnv9J", "replyto": "vznIYSnv9J", "signatures": ["ICLR.cc/2026/Conference/Submission15477/Reviewer_Pt8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15477/Reviewer_Pt8i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890178089, "cdate": 1761890178089, "tmdate": 1762925767113, "mdate": 1762925767113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GenDR, a framework designed to convert a pre-trained text-to-image diffusion model (such as Stable Diffusion) into a compact model for real-world image super-resolution. The approach uses a 16-channel VAE to retain more detailed image representations and applies CiDA  to align the training objective with the super-resolution task. GenDR produces results in a single inference step and shows improved performance over several baseline methods in terms of perceptual quality and runtime efficiency on benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a practical limitation of the commonly used f8c4 VAE in SR tasks by expanding it to 16 channels, which helps preserve more details. \n\n- The proposed CiDA framework is a well-engineered solution that enables stable training and efficient inference even in a higher-dimensional latent space.\n\n- The experiments are thorough, covering model size, inference speed, ablations on CiDA, prompt simplification, and performance after SR-specific fine-tuning. These results support both the effectiveness and practical relevance of the method."}, "weaknesses": {"value": "- In Table 2, the paper only reports changes in no-reference metrics, which do reflect perceptual quality improvements, but it lacks reference-based metrics like LPIPS. Including trends in LPIPS (even if degraded) would provide a more complete picture, especially considering the known trade-offs between no-reference and reference-based measures. Visualization comparisons under different loss functions would also strengthen the argument.\n\n- It remains unclear why the model, after being fine-tuned for SR on a 512-resolution UNet, can still generate coherent 1024-resolution T2I results without typical artifacts (e.g., extra limbs). Some clarification on how the SR tuning impacts generalization to higher-resolution T2I tasks would be helpful."}, "questions": {"value": "I suggest the authors include a brief comparison with TVTSR [1] (ICCV 2025) in the main text. While the motivation is aligned, the two methods adopt different and complementary strategies: the proposed approach keeps the spatial compression ratio fixed but increases the channel dimension by 4×, whereas TVTSR maintains the channel size and compresses the spatial resolution by 2×. A short discussion of these orthogonal design choices would help clarify the novelty and positioning of the proposed method.\n\n[1] https://arxiv.org/pdf/2507.20291"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h4OKnl2PsI", "forum": "vznIYSnv9J", "replyto": "vznIYSnv9J", "signatures": ["ICLR.cc/2026/Conference/Submission15477/Reviewer_8VTQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15477/Reviewer_8VTQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988189391, "cdate": 1761988189391, "tmdate": 1762925766632, "mdate": 1762925766632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}