{"id": "htvcTg32pj", "number": 17503, "cdate": 1758276873452, "mdate": 1759897170730, "content": {"title": "pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data", "abstract": "Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,\na personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class\nimbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.", "tldr": "We propose pFedBBN, a personalized federated test-time adaptation framework that mitigates class imbalance and domain shifts using balanced batch normalization and domain-aware client collaboration.", "keywords": ["Federated Learning", "Test-Time Adaptation", "Class Imbalance", "Balanced Batch Normalization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1db557556b9113e87371572aa98c631d7b8a0890.pdf", "supplementary_material": "/attachment/1bdcb8da848aeb2698e6b014e4fdaf8a7d9f0ca9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes pFedBBN, a personalized federated testing-time adaptation (FTTA) framework that combines Balanced Batch Normalization (BBN) and class-wise adaptive normalization (CWAN) to mitigate the negative effects of class imbalance under distribution shifts. It further introduces a BBN-statistics-based client similarity to guide personalized aggregation, with the goal of improving robustness and minority-class performance in federated environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work tackles the intersection of test-time adaptation + class imbalance + personalization, a topic of growing interest.\n\n2. Using class-wise BBN statistics estimated via pseudo labels, and leveraging them for similarity-aware aggregation, is conceptually clear."}, "weaknesses": {"value": "1. Potential contradiction in stating ‚Äúonly affine parameters are updated‚Äù while simultaneously updating and using class-wise BBN running statistics. The interaction between the two must be clarified.\n\n2. Dividing by feature dimension d in Eq.(2)/(3) is atypical for BN statistics. The promised ‚Äúinterpolation with batch statistics‚Äù in contributions is not visible in Eq.(4), which is a uniform mean over classes.\n\n3. Eq.(7) averages raw L2 distances across layers without normalization, which may be unstable across architectures. No sensitivity analysis provided.\n\n4. In Table 1, BBN alone sometimes outperforms pFedBBN (e.g., CIFAR-10-C IID: 72.61 vs. 70.11), suggesting personalized aggregation can hurt when data are IID. Assertions should be toned down.\n\n5. Claims about improving minority-class performance require macro-F1 / balanced accuracy / per-class recall in the main table, currently insufficient evidence.\n\n6. FedCTTA (2025) and other recent FTTA methods are mentioned but not compared.\n\n7. The paper claims class-balanced BN statistics and similarity-based aggregation drive the gains, but lacks direct interpretability to verify this (e.g., similarity matrix visualization, minority-class metrics, sensitivity analysis).\n\n8. Experiments are conducted only on small-scale datasets, lacking validation on larger and more realistic federated benchmarks. This raises concerns about the reliability and scalability of the reported performance."}, "questions": {"value": "1. Are BBN running statistics also updated during TTA, or are they frozen while only Œ≥/Œ≤ are optimized?\n\n2. How is the ‚Äúinterpolation with batch statistics‚Äù implemented? Why does Eq.(4) show uniform class averaging?\n\n3. What is the granularity/frequency of sharing BN stats (per-channel? per-layer? every round?) and the bandwidth?\n\n4. Why does pFedBBN sometimes underperform local BBN in IID settings?\n\n5. What concrete privacy guarantees are assumed when sharing high-dimensional BN statistics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pjkNP7iVIq", "forum": "htvcTg32pj", "replyto": "htvcTg32pj", "signatures": ["ICLR.cc/2026/Conference/Submission17503/Reviewer_mNWi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17503/Reviewer_mNWi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447038282, "cdate": 1761447038282, "tmdate": 1762927386407, "mdate": 1762927386407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes pFedBBN, a personalized federated test-time adaptation framework that aims to handle class imbalance and domain shifts in federated learning without labeled data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The studied topic is interesting and important. The experiment results seem to be encouraging."}, "weaknesses": {"value": "1. The literature review is not comprehensive. The data heterogeneity, particularly as a form of data imbalance and/or long-tail, has been extensively studied by the community, e.g., [1] and [2].\n\n\n2. The presentation of the methodology section is very sloppy making it very hard to accurately understand the details. The experiemnt sections failed to disclose necessary details. Therefore, it's hard to assess the contribution and novelty. Please see the `Question` sections for details.\n\n[1] Shang, Xinyi, et al. \"Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features.\"\n\n[2] Dai, Yutong, Zeyuan Chen, Junnan Li, Shelby Heinecke, Lichao Sun, and Ran Xu. \"Tackling data heterogeneity in federated learning with class prototypes.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 6, pp. 7314-7322. 2023."}, "questions": {"value": "1. What is Balanced Batch Normalization (BBN)? Is it the same as proposed CWAN? Some parts of the paper mention \"each client performs unsupervised local adaptation using class-wise balanced batch normalization (BBN)\" and some parts mention \"Prior to adaptation, the student model is equipped with CWAN by replacing standard batch normalization layers with class-aware normalization layers, enabling balanced statistics across classes during inference\". \n\n2. How is the pseudo-label $\\hat c_i$ computed (section 3.1.1)? Is it computed from the pre-trained source model? What is the difference between the source model and the teacher model? What is the augmented counterpart $\\tilde x$ defined in section 3.1.2?\n\n3. CWAN employs momentum-based running updates (Eqs. 1‚Äì3).\n    * What is the rationale for using running batch updates instead of computing statistics directly using the client's data? I guess these are intended to be used in the local client adaptation, i.e., in each step of updating BBN or CWAN paramters. In other words,  But I'm not sure since Section 3.1 is poorly presented and I can only make some guess. \n    * The updating rule  Eq. (2) needs more explanations. It seems to be related to Welford online variance update formula. \n    * How are absent classes handled in Eqs. (1‚Äì4)? Are their statistics frozen, interpolated, or reinitialized?\n    * How sensitive is CWAN to pseudo-label noise, especially under heavy class imbalance?\n    * Both Eq. (2) and Eq. (3) divide the update terms by the feature dimension $d$, which is nonstandard in batch normalization. What is the theoretical or empirical motivation for dividing by $d$?\n\n4. What is the final loss function used for local client adaptation (section 3.1.2)? It only introduced $L_{KD}$ and  $L_{CR}$.\n\n5. The matrix $W$ in Eq. (8)is described as row-stochastic, and later described as symmetric distance matrix in the Section 4.2.  row-stochastic and symmetric are not equivalent.\n\n6. Does the model aggregation happen on the server side or the client side?\n\n7. It's unclear how the hyperparameters are chosen and how they impact the results, e.g., $\\eta$ in Eq. (3), $\\delta$ in Eq. (5) (which conflicts the parameter used for the Dirichlet distribution), and $\\tau$ used in Eq. (8). It's unclear how the source model and teacher models are obtained / constructed for each client. \n\n\n**Minor Comments**\n1. Failed to disclose the usage of large language model as required. \n2. Please use  \\citep and \\citet Tex commands properly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JLlRLpbQhi", "forum": "htvcTg32pj", "replyto": "htvcTg32pj", "signatures": ["ICLR.cc/2026/Conference/Submission17503/Reviewer_uDPV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17503/Reviewer_uDPV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548505483, "cdate": 1761548505483, "tmdate": 1762927386035, "mdate": 1762927386035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces pFedBBN, a novel personalized federated test-time adaptation (FTTA) framework designed to address the combined challenges of domain shifts and severe class imbalance (CI). The core issue identified is that standard TTA methods fail in federated, non-IID settings because conventional batch normalization statistics become biased by dominant classes, degrading performance on rare classes. pFedBBN tackles this via a two-part, privacy-preserving approach: first, clients perform fully unsupervised local adaptation using a Class-Wise Adaptive Normalization (CWAN) module. This module employs Balanced Batch Normalization (BBN) to track per-class feature statistics using pseudo-labels, thereby mitigating majority-class bias. This local update is stabilized using a confidence-filtered knowledge distillation process to reduce noise from pseudo-labels. Second, clients exchange only their BBN statistics, which act as privacy-preserving domain descriptors. A server then computes an inter-client similarity matrix from these statistics to perform a personalized, weighted aggregation, ensuring clients collaborate primarily with domain-similar peers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. First to address the combination of Class Imbalance and Federated Test-time Adaptation\n2. Part of the proposed method, the balanced batch normalization, can be integrated with prior works for improved performance.\n3. Experiment shows improved results compared to baseline methods."}, "weaknesses": {"value": "1. The work lacks an explanation of the concrete design of the baseline methods; thus, it is not clear how the proposed method is novel compared to prior methods.\n2. The idea of managing class-wise statistics and dealing with batch normalization is similar to several more recent related works dealing with class imbalance problems [1,2,3,4,5], but the relevant works are not cited or discussed.\n3. The algorithm pseudo-code is not provided.\n4. The experiment is conducted on limited datasets of CIFAR 10 and 100, but is not conducted on the more challenging Digits-5 and PACS\n\n[1] Rebalancing batch normalization for exemplar-based class-incremental learning\n[2] FedBN: Federated Learning on Non-IID Features via Local Batch Normalization.\n[3] FedGCR: Achieving Performance and Fairness for Federated Learning with Distinct Client Types via Group Customization and Reweighting\n[4] TTA-FedDG: Leveraging Test-Time Adaptation to Address Federated Domain Generalization\n[5] Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning\n\nOther issues:\n- The citation format is incorrect throughout the paper"}, "questions": {"value": "1. What is the novel contribution of the proposed method compared to more recent class imbalance works?\n2. Can you provide the concrete pseudo-code of your proposed method?\n3. Does the proposed method work on class imbalance settings with more distinct classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OclPGb7f3Q", "forum": "htvcTg32pj", "replyto": "htvcTg32pj", "signatures": ["ICLR.cc/2026/Conference/Submission17503/Reviewer_Wmpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17503/Reviewer_Wmpd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592404564, "cdate": 1761592404564, "tmdate": 1762927385207, "mdate": 1762927385207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper ‚ÄúpFedBBN: Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data‚Äù addresses the challenge of adapting federated models to unseen, unlabeled, and class-imbalanced domains. Standard BN layers bias toward head classes, harming tail performance. The authors propose pFedBBN, which combines Balanced Batch Normalization (BBN)‚Äîmaintaining class-wise BN statistics via pseudo-labels‚Äîwith a teacher‚Äìstudent distillation for unsupervised local adaptation. Only BN affine parameters are updated for efficiency. Clients then exchange BN statistics to compute domain similarity and perform personalized, similarity-weighted aggregation instead of uniform averaging. Experiments on CIFAR-10/100-C show pFedBBN outperforms prior FL and TTA methods, especially for minority classes, offering a lightweight, privacy-preserving solution to non-IID and imbalanced federated test-time scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nThe paper introduces a novel perspective on federated test-time adaptation (FTTA) by explicitly tackling class imbalance, a rarely addressed challenge in this context. The proposed Balanced Batch Normalization (BBN) and class-wise adaptive normalization represent an original and conceptually elegant modification to standard BN, ensuring fair adaptation across majority and minority classes. Integrating this with a similarity-based personalized aggregation using BN statistics is a creative and privacy-preserving innovation that bridges personalization and adaptation in FL.\n\nQuality:\nThe methodology is well-grounded, combining sound theoretical intuition (balanced statistics to mitigate bias) with a practical, lightweight implementation (updating only BN affine parameters). The paper offers strong empirical validation across multiple datasets and imbalance levels, showing consistent gains over state-of-the-art FTTA and FL baselines. The use of pseudo-label filtering and teacher‚Äìstudent regularization further strengthens the robustness of the approach.\n\nClarity:\nThe paper is clearly written and logically structured, with each component‚Äîlocal adaptation, normalization, and aggregation‚Äîintroduced and motivated in a coherent flow. Equations are well-formulated, and visual illustrations effectively convey the model pipeline. The balance between technical depth and readability is maintained throughout.\n\nSignificance:\nThis work makes an important step toward practical deployment of FL models under realistic conditions‚Äîwhere class imbalance, domain shift, and unlabeled test data coexist. By reducing head-class bias and enabling personalized test-time adaptation, pFedBBN advances the frontier of robust, generalizable, and privacy-aware federated learning. Its modular design also makes it extendable to broader multimodal or resource-constrained FL applications."}, "weaknesses": {"value": "Limited empirical scope:\nThe experiments are confined to CIFAR-10-C and CIFAR-100-C‚Äîsynthetic corruption benchmarks that only approximate domain shifts. This leaves uncertainty about the framework‚Äôs robustness in real-world federated settings (e.g., medical imaging, speech, or sensor data). Evaluations on larger or more heterogeneous datasets would better validate generality.\n\nDependence on pseudo-labels:\nThe class-wise BN statistics rely on pseudo-labels generated by the teacher. Under severe class imbalance, tail classes may receive few confident samples, leading to inaccurate statistics and residual bias. The paper could strengthen its approach by proposing confidence calibration or reweighted statistic estimation for low-frequency classes.\n\nLimited adaptation capacity:\nOnly BN affine parameters are updated during test-time adaptation. While efficient, this design may be insufficient for large domain shifts. Incorporating adapter modules or low-rank parameter tuning (e.g., LoRA) could enhance flexibility without heavy computation.\n\nSimilarity metric simplicity:\nThe inter-client similarity uses Euclidean distance on flattened BN statistics, which might not fully capture semantic or feature-level domain divergence. A learned or information-theoretic similarity metric could provide more meaningful personalization.\n\nAblation and sensitivity gaps:\nThe paper would benefit from ablation studies isolating the effects of BBN, pseudo-label confidence thresholding, and the aggregation strategy. Sensitivity analyses on the temperature parameter \nùúè\nœÑ in similarity weighting are also missing.\n\nTheoretical justification:\nWhile empirically motivated, the paper lacks formal analysis connecting class-wise normalization to reduced bias or improved convergence in federated adaptation. Including theoretical insights or convergence bounds would reinforce the method‚Äôs credibility."}, "questions": {"value": "How robust is pFedBBN to noisy pseudo-labels, especially for tail classes‚Äîdid you explore adaptive confidence thresholds or uncertainty calibration? Updating only BN affine parameters may limit adaptation; could lightweight adapters (e.g., LoRA) improve flexibility? Why use Euclidean distance for similarity‚Äîhave you compared cosine or learned metrics? Does personalized aggregation risk bias toward large or clean clients? Please provide ablations for each module and sensitivity to œÑ. Have you tested on real-world non-IID datasets beyond CIFAR-C? Finally, any theoretical or interpretive explanation for how balanced BN reduces head-class bias would strengthen the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XngRKeNGD1", "forum": "htvcTg32pj", "replyto": "htvcTg32pj", "signatures": ["ICLR.cc/2026/Conference/Submission17503/Reviewer_5Gvb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17503/Reviewer_5Gvb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327144707, "cdate": 1762327144707, "tmdate": 1762927384725, "mdate": 1762927384725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}