{"id": "xnQTMZAbZ0", "number": 7831, "cdate": 1758038026848, "mdate": 1759897828789, "content": {"title": "MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance", "abstract": "Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps—3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel–point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.", "tldr": "Solving 3D promotable segmentation using pointmap guidance.", "keywords": ["SAM", "3D Vision", "VGGT"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83b9d59346628a26d4b0b573256c089474d3ffe1.pdf", "supplementary_material": "/attachment/7dd4f5ac63ed88927409dcc581cceec5f45101bb.zip"}, "replies": [{"content": {"summary": {"value": "Promptable segmentation becomes a more and more popular way for object cutout from images and videos. This work aims to promtable segmentation for multi-view images. More specifically, the user can only provide a prompt, like a click, on one of the views, then we hope to output the mask on all views. How to reach view consistency is the primary challenge. To address this, this work leverage the recent VGGT to obtain a point cloud first, then transfer the 2D segmentation features of SAM into 3D space to produce view-consistent results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The motivation is clear and method designs make sense"}, "weaknesses": {"value": "- My major concern is the limited noverlty. From Fig 2, it seems the key difference is just the modify 2D postional encoder to be 3D version with the help of VGGT. The major contribution comes from existing VGGT. \n- It also lacks comparison with a simple baseline：Project the prompt into point cloud and project it onto other views, and do SAM for each view."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P3sDskXP74", "forum": "xnQTMZAbZ0", "replyto": "xnQTMZAbZ0", "signatures": ["ICLR.cc/2026/Conference/Submission7831/Reviewer_zxfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7831/Reviewer_zxfL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908569766, "cdate": 1761908569766, "tmdate": 1762919877896, "mdate": 1762919877896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MV-SAM, a model for multi-view promptable segmentation. The objective is to take a set of unposed images with segmentation prompts (points, boxes, scribbles) for a subset of those images, and output the segmentation mask of the prompted objects for all of the unposed images. While it is possible to apply video-based promptable segmentation models to this task, these models lack explicit 3D understanding, which leads to inconsistent predictions across images. To solve this, MV-SAM uses explicit 3D information from pre-trained 3D reconstruction model $\\pi^3$. Specifically, it uses $\\pi^3$ to obtain 3D point coordinates for all pixels of the unposed images, and obtains sinusoidal positional embeddings from these 3D coordinates for each pixel and for each prompt, yielding per-pixel 3D positional embeddings and 3D prompts. The pixel-level 3D positional embeddings are then added to extracted image features, and fed to a mask decoder together with the 3D prompts, which finally outputs a segmentation mask for each prompt for each input image. With experiments, MV-SAM is shown outperform video-based promptable segmentation method SAM2 across various datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an original and well-motivated idea. By leveraging the power of 3D reconstruction methods and obtaining a 3D point coordinate for each pixel, it is possible to make the segmentation model aware of the 3D structure of the scene, enabling consistency between the segmentation masks predicted for different images of the same scene.\n\n2. The effectiveness of the proposed method that leverages this idea, MV-SAM, is properly demonstrated through experiments. Across various datasets, MV-SAM outperforms video-based promptable segmentation method SAM2 (see Tab. 1 and Tab. 2), validating the effectiveness of using explicit 3D information. MV-SAM is also shown to perform almost on par with methods that require optimization per individual scene, while MV-SAM generalizes to these same scenes without having seen them during training and is thus much more useful in practice.\n\n3. Overall, the paper is very well written. This makes it easy to understand the contributions and their significance.\n\n4. With the control experiments in Sec. 4.3, the paper properly evaluates the effectiveness of individual contributions and components. For instance, it shows that using confidence embeddings to inform the model of pixels with unconfident 3D reconstruction predictions significantly boosts the performance. \n\n5. A considerable strength of the proposed method is that it can be trained on single-view images, but still performs well when applied to multiple images during inference. This way, it does not depend on multi-view datasets and can be trained on large-scale single-view datasets, for which there is more availability."}, "weaknesses": {"value": "1. This is not a major weakness, but it is not clear what the efficiency is of the proposed MV-SAM method compared to existing method SAM2. I can imagine that running $\\pi^3$ for each scene introduces a significant computational overhead. The paper would be stronger if it provided insights into the runtime, number of parameters, and number of FLOPs for both MV-SAM and SAM2. MV-SAM would still be valuable if it were less efficient than SAM2, but information about their relative efficiency would provide insights into the usefulness of each method in practice, and could prompt future research into improving efficiency if necessary.\n\n2. In Sec. 3.3, the paper states: \n    > In SAM2-Video, 2D positional embeddings are independently assigned to every frame, making it necessary to use view-wise attention to associate masks and prompts across different views (referred to as ‘full-view’ in Table 3a).\n     \n    After also checking the SAM2 paper and architecture, it is not clear what the authors mean by this 'view-wise'  or 'full-view' attention. In what way does the 'single-view' attention by MV-SAM - where there is attention between the features of each individual frame and all prompt embeddings - differ from the attention that is used in the SAM2 mask decoder? This is currently not clear. The paper would be stronger if it better explained - or even better, visualized - what the differences are between these two types of attention, especially because this 'single-view' attention is one of the key components of the mask decoder.\n\nThere are some other minor weaknesses, which do not significantly impact my rating:\n\n* There is some inconsistency in Sec. 3.2. Specifically, L240 says that the bottom 15% of the confidence scores are considered 'low-confidence', but L247 says that it is the bottom 20%. Which of the two is correct? This inconsistency should be corrected.\n* Typo in L470: 'prdocue' should be 'produce'."}, "questions": {"value": "I believe this is a strong paper, with an original and effective idea and only some minor weaknesses. Therefore, I recommend to accept this paper. Still, there are some things that could be done to improve the paper. Specifically, the paper would be stronger if it included a comparison of the efficiency of MV-SAM and SAM2, if it better explained or visualized what the difference is between the 'single-view' attention by MV-SAM and the 'full-view' attention by SAM2, and if it fixed some minor textual errors. I would recommend the authors to make these changes and provide these explanations in the rebuttal and the revised version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W6AeRbb4Xv", "forum": "xnQTMZAbZ0", "replyto": "xnQTMZAbZ0", "signatures": ["ICLR.cc/2026/Conference/Submission7831/Reviewer_5RcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7831/Reviewer_5RcK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925426038, "cdate": 1761925426038, "tmdate": 1762919877165, "mdate": 1762919877165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work targets promptable segmentation across multi-view images or videos with 3D consistency.\nPrior methods like SAM2-Video lack 3D awareness and produce inconsistent masks across views, while optimization-based methods (SA3D, OmniSeg3D) require costly per-scene fitting.\n\nTheir approach uses Pi3 to produce pointmaps (pixel-to-3D correspondences) from unposed images.\nThe key insight is that pointmaps naturally bridge 2D prompts and 3D geometry without rendering or projection.\nTheir pipeline\n(1) runs Pi3 to get pointmaps with confidences, extracts image embeddings using frozen SAM2-Video.\n(2) They then encode the 3d information into positional embeddings for the prompt embedding and their proposed \"confidence embedding\".\n(4) Finally they feed the enhanced embeddings through a mask decoder (standard)\n\nFor training, they use only the SA-1B (single-view images) dataset, with no multi-view data or 3D annotations required.\nThey evaluate on their method on the NVOS and SPIn-NeRF benchmarks, measuring mIoU and mean accuracy for segmentation.\nResults show consistent improvement over SAM2-Video (generalization / \"online\" baseline), while achieving competitive performance with per-scene optimization methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The technical idea is well-motivated and easy to follow, building off existing work.\nEnhancing SAM2-Video embeddings with 3D positional information improves consistency across views.\n\nResults show significant improvement over SAM2-Video across NVOS and SPIn-NeRF benchmarks.\nThey achieve competitive performance with optimization-based methods without requiring per-scene fitting.\n\nThe ablations are informative, particularly Table 3a which evaluates key decoder design choices.\nThey systematically compare attention scope, positional embeddings, and confidence embeddings.\n\nThe Appendix is thorough and I found many questions answered in there."}, "weaknesses": {"value": "The comparison against generalization baselines is limited to SAM2-Video alone.\nIt would strengthen the paper to include other video or multi-view segmentation methods that don't require per-scene optimization.\n\nThe method relies heavily on Pi3 for pointmap generation, but the technical sections provide limited detail on how Pi3 works.\nGiven that Pi3 appears to do much of the heavy lifting, it's unclear how much of the contribution is genuinely novel versus simply combining existing components (Pi3 + SAM2-Video).\nIt would be helpful to either describe Pi3 in a preliminary section or compare it against other visual geometry models for generating pointmaps to understand the design choices.\n\nGiven that the proposed method slightly underperforms optimization-based methods, it would be useful to show an extra column or so that measures the inference time / number of frames to let their method shine more.\n\n### Minor Weaknesses\n\nThe model is trained on single-view object-image pairs from SA-1B (Section 3.4), which seems mismatched with the multi-view/video task at test time.\nA bit of justification/analysis on why not just train with multi-view would be helpful.\n\nIn general, some of the formatting and the placement of the results could be improved to better help flow for the reader.\nThis is very minor, but some of the tables are often quite \"far\" from the reference text.\n\nTable 3b is useful but could be better motivated by introducing some of those methods earlier in the related work."}, "questions": {"value": "Figure 5 compares single-view vs full-view attention, but to my understanding the model was never trained with this full-view attention scenario.\nCould the authors elaborate on why this is informative?\n\nIs there any difference between the training/evaluation settings in the ablation studies (Table 3) versus the main results (Table 1)?\nI'm trying to make sure I understand the slight difference in results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M5kyyaTew8", "forum": "xnQTMZAbZ0", "replyto": "xnQTMZAbZ0", "signatures": ["ICLR.cc/2026/Conference/Submission7831/Reviewer_nx6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7831/Reviewer_nx6A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009303333, "cdate": 1762009303333, "tmdate": 1762919876600, "mdate": 1762919876600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for multi-view promptable segmentation, MV-SAM, using pointmaps based on unposed images using a visual geometry model. The method aims to locate all image regions that correspond to a user prompt in the form of an object mask in a given image. One of the main challenges for this task are 3D awareness and challenges from occlusion, lighting, and textures on the objects. The method creates a pointmap using a pretrained visual geometry model for each image in the given set of unposed images along with image features for each point. This 3D representation maps each pixel to a 3D point along with a confidence map to map the 2D user prompts and 2D image features into a shared 3D space. A transformer-based mask decoder predicts view-consistent masks by attending to the relationship between the 3D image features and the 3D prompts. The model was trained on the SA-1B dataset with single-view object-image pairs using the focal loss and dice loss. Evaluation includes comparisons to state-of-the-art methods such as SAM2-Video and other per-scene optimized methods on diverse real-world datasets covering both indoor and outdoor scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the paper is clear and descriptive about the different components of the proposed method. The problem statement is well-scoped and the method section includes all details and motivations behind the design choices. The core contribution of the method is enabling 3D awareness without 3D supervision by using the pointmap representation. It requires no per-scene optimization allowing for generalization to various datasets. Results show considerable improvement over SAM2-Video and being comparable to per-scene optimized methods. The paper includes ablations for different 3D networks, positional encoding choices, and image encoders to validate the claimed impact of the design choices."}, "weaknesses": {"value": "- The method relies on a pretrained visual geometry model, making it dependent on the accuracy of the pointmap reconstruction. The error in the pointmap reconstruction can propagate to the final mask prediction.\n- In Figure 4, I suggest reducing the opacity of the truck in the reference image to make it more visible and easier to interpret.\n- The discussion on the limitations is not included in the main text. I recommend that the authors include a discussion with some examples where the method fails. This would help the reader to better understand the contributions of the paper."}, "questions": {"value": "- How well does the model perform if the reference view is occluded but the target view shows the object fully or a different view of the object? I would also like to see the result of the method where the target view is the reference view and the target view is the current reference view with the entire truck visible.\n- An analysis on the noise in pointmap reconstruction at inference time would be valuable addition to quantify the dependence on the accuracy of the pointmap reconstruction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2YJb8Bg7B7", "forum": "xnQTMZAbZ0", "replyto": "xnQTMZAbZ0", "signatures": ["ICLR.cc/2026/Conference/Submission7831/Reviewer_Ff9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7831/Reviewer_Ff9Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048898862, "cdate": 1762048898862, "tmdate": 1762919875635, "mdate": 1762919875635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}