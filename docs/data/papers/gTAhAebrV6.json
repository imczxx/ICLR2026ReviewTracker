{"id": "gTAhAebrV6", "number": 19055, "cdate": 1758293167557, "mdate": 1759897063595, "content": {"title": "From Attention to Diffusion: A Unified Entropic Optimal Transport View", "abstract": "We show that transformer attention and diffusion models are discretizations of the same entropy-regularized optimal transport (OT) flow. A single attention layer is a KL-proximal (JKO/mirror) step in an OT potential; stacking layers yields probability paths that converge to a probability–flow ODE (PF–ODE) on the simplex. Our construction uses a causal, semi-relaxed EOT that preserves attention masking while retaining OT geometry. We derive a finite-depth error bound controlled by a budget ΞL (quantifying continuum validity) and prove that stacked attention weakly approximates time-inhomogeneous, anisotropic reverse diffusions with an error that separates time discretization, logit variation, and optional degeneracy regularization. Geometrically, we characterize exact Schrödinger Bridge (SB) alignment via a rotational energy ℛ that vanishes if and only if the path is SB, and serves as a practical diagnostic otherwise.\n\nThe framework yields testable predictions: (i) the continuum approximation is accurate when ΞL is small; (ii) depth exhibits diminishing returns beyond a threshold set by contraction and step size; and (iii) lower ℛ correlates with improved generations. We validate these predictions with a diagnostic suite (P0–P4): BV/continuity gating (with abstention on failure), PF–ODE adequacy, curvature/locking geometry, and SB energy. Evidence spans three tracks—Transformers (core diagnostics), diffusion LLMs (dLLM; late-window stability certificate), and a compact image diffusion model (parity and first-order weak-error behavior). These insights motivate mobility-aware temperature scheduling and certified early exit, conserving depth while preserving transport geometry.", "tldr": "We prove attention and diffusion are discretizations of the same entropy-regularized OT flow—yielding a unified theory, finite-depth bounds, and actionable diagnostics", "keywords": ["Optimal Transport", "Transformer Attention", "Diffusion LLM", "Probability-flow ODE", "Schrödinger Bridge alignment", "Neural ODE"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55266b4b016ff9b57d9f06b025be8d9bcf46c13d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper establishes a connection between the attention mechanism in transformers and diffusion-based generative models that are formulated via conditional entropy minimization. First, the authors show that the row-wise softmax in attention can be naturally interpreted as a local optimization step in entropic optimal transport problems. Second, considering the successive application of such steps across layers and assuming that inter-layer changes are sufficiently small, they interpret network depth as a time discretization and obtain a limiting continuous evolution of distributions on the simplex — a probability-flow dynamics. Third, the authors show that the same probabilistic dynamics can be seen as the marginal flow of a stochastic differential equation which is constructed explicitly from the deterministic dynamics. Finally, the authors relate the obtained continuous flow to the Schrödinger bridge problem: the constructed continuous flow is a Schrödinger bridge with respect to a given reference process if and only if the extra part of the velocity is a gradient of some potential."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors pursue the ambitious goal of establishing a connection between the transformer architecture and modern diffusion models. The concept is reasonable, and the presented implementation sketch appears promising."}, "weaknesses": {"value": "The first thing that strikes the reader is that the paper is mathematically hard to follow and contains significant structural deficiencies.\n- The text is fragmented: much remains unclear when reading the main body without constantly consulting the appendices. The flow of the manuscript is not supported by relevant bibliographical references. The proof of Theorem 5.6 is not provided.\n- Proposition 2.1. is formulated unclearly, as  $p^+$ is not defined. Moreover, if the authors intend $p^+$ to denote the softmax, then this result is already well known [1]. \n- Although the proofs in the paper follow standard lines of reasoning at their core, they are presented only as sketches: many steps and explanations are omitted.\n- The authors handle convergences carelessly (e.g. “weak convergence in $L^1$”) without clearly specifying what exactly they mean by these statements.\n- The proof of Theorem 5.1 depends on Theorem J.1 (Lemma 5.4 seems to be useless for the following text). The latter theorem has several problems in its proof. First, (lines 1074-1077) the Arzelà–Ascoli theorem for functions of bounded variation yields almost-everywhere convergence (and not uniform convergence) — uniform convergence only follows in the continuous setting. Moreover, (lines 1079-1081) the parameter $\\alpha$ is introduced empirically and requires a more detailed justification. Second, (lines 1082-1084) the invocation of Banach–Alaoglu theorem is unjustified: boundedness in $L^1$ does not imply compactness in the dual space in the way the authors seem to use it. In the part about “architectural consistency” (lines 1085-1095) the connection between the sequence $D_L(\\cdot)$ and $\\hat b(\\cdot)$ is not explained; without that connection the assertion is meaningless.\nIn view of the above, Section 5, which relies entirely on Theorem J.1, loses its theoretical force and requires substantial revision. It is also important to underline that the proof of Theorem 5.1 itself needs significant expansion and clarification of intermediate steps — at present it is a rather rough sketch.\n- The deficiencies in Section 5 make the later sections much harder to read; I therefore mention only a few apparent  issues: (1) I would like to see more explicit statements about which function spaces the functions $a(\\cdot)$ and $p_H(\\cdot)$ belong to (are they smooth? or Sobolev functions?). (Lemma 6.1.). (2) Conditions guaranteeing existence of a solution to the Fokker–Planck equation are not stated. (Lemma 6.2.). (3) Instead of a full proof of Theorem 6.6, only a sketch is provided. (lines 855-897). (4) The Hodge decomposition is not defined formally. (Theorem 7.2.). (5) Poincaré constant is introduced without a clear source — it is unclear what it can be equal to or under which conditions it is finite. (lines 920-925)\n- The source code is not presented. \n\nReferences: \n\n[1] Marco Cuturi, “Sinkhorn distances: lightspeed computation of optimal transport distances”\n\n**Minor issues:**\n\n- It is unclear what the authors mean by $L^1_{\\mathrm{loc}}([0,1],\\mathbb R^d)$, given that the interval [0,1] is compact. (line 153)\n- Typos: “Remark (sharp mobility bound)” and “Remark 2” appear to be the same remark. (lines 096, 214, 683)\n- Section R collects auxiliary statements that are not accompanied by references to the literature."}, "questions": {"value": "- Given the problems in Section 5 outlined above, are the authors confident that Theorem 5.6 remains valid in its current formulation and therefore does not require a full proof? Judging by the proof strategy in Theorem J.1., the function $b(\\cdot)$ (Section 5) should be interpreted as a distribution; consequently, Theorem 5.6.  that involves the naive product $J_{sm}$ and $b(\\cdot)$ must be reformulated.\n- What precisely is meant by the equality in Theorem 7.1 between the Schrödinger bridge and the “transformer’s flow”? Please clarify the exact mathematical sense of this equality.\n- Could you explain in more detail the condition “Degeneracy handling (regularization)” (line 305) and how it is used in the proofs? The brief comments in the paper do not make it clear whether the limiting transition is fully justified and whether the limiting distribution preserves the required properties.\n\nSuggestions:\n\n- I suggest that the authors substantially rewrite the main part of the paper to make it more readable and supplement the proofs of the main theorems that suffer from the serious issues described above.\n- Clean up the text to remove inaccuracies and typos and improve readability. Add the necessary bibliographic references that contain precise formulations of the theorems the authors use.\n- The authors should use the classical theorems they cite (Arzelà–Ascoli, Banach–Alaoglu, the Poincaré inequality, Grönwall’s inequality) correctly: explicitly state in which spaces the objects live and in which topologies the convergences are meant to hold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MxjOBLdPFF", "forum": "gTAhAebrV6", "replyto": "gTAhAebrV6", "signatures": ["ICLR.cc/2026/Conference/Submission19055/Reviewer_Apk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19055/Reviewer_Apk7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813760852, "cdate": 1761813760852, "tmdate": 1762931087427, "mdate": 1762931087427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NOTE: Due to problems regarding the clarity of the paper (which I mention in weaknesses), it is probable that the summary below may be misleading in some aspects.\n\nThe paper aims to develop a framework that links transformers and diffusion models to entropy regularized optimal transport. Their core idea is to consider that a single transformer’s attention layer performs a KL-proximal step in an optimal transport potential, which is similar to the Sinkhorn algorithm. Therefore, a stack of attention layers correspond to an evolving induced probability flow that can be seen as a diffusion process. This result is formalized in Theorem 6.6. The authors provide constraints on the evolution of the layers to ensure the induced probability flow accurately captures the layers’ behavior. The authors conduct three tracks of experiments: transformers, diffusion LLM and image diffusion. They report different metrics for each of the tracks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The general idea of the paper sounds interesting; however, it is difficult to determine if the authors succeeded or not due to the aspects I will mention below."}, "weaknesses": {"value": "I have serious concerns regarding the structure and writing of the paper. My major concerns are listed below:\n\n- The initial part of the paper needs a deep revision. The Introduction section does not provide the required information to understand the general idea of the paper and it is difficult to determine what problem the paper is actually addressing. The lack of references and explanatory text makes this even harder. \n\n- Additionally, the list of contributions that follows becomes difficult to interpret as it introduces terminology and even mathematical notation that have not been defined or motivated earlier in the paper.\n\n- There are too many references to the Appendices, giving the impression that much of the essential information has been relegated there rather than included in the main text. This makes the paper even more challenging to read as it is necessary to always switch between the Appendix and the Main Text. Therefore, I believe the authors should work on properly dividing the essential and non-essential content.\n\n- Most of the Theorems, Propositions and Lemmas introduce notation that is not properly defined. The paper includes Appendix Q for notation; however, this leads to the problem I mentioned in the previous point above. Additionally, the origin and meaning of some of the variables are never explained, this is the case of $q$ and $k_j$ in Proposition 2.1, this situation repeats in other places. Therefore, I believe the authors should be more careful when introducing the notation.\n\n- The experimental setup is not clear enough to understand what experiments are being conducted or what type of data is being used. Additionally, it is not clear what metrics are being computed and why they are relevant.\n\nThere are other minor and more specific aspects that I list below:\n\n- It is not clear what the “Predictions and implications” subtitle refers to or why it is necessary. \n\n- It is needed to properly formulate or reference the semi-relaxed entopic OT. I checked Appendix B as well, but I could not find a proper formulation or definition. This is needed to understand the so-called causal structure.\n\n- I humbly believe that the entire ‘Conceptual Overview’section should be placed after properly introducing the problem to solve. I think this is important since the mathematical terminology for Jacobian ($J^{\\tau}_{\\text{sm}}(z)$) and finite-depth budget lacks a meaning. I understand they appear in Section 2; but they are rawly introduced without giving context about how they are involved in the problem.\n\nOverall, I think the paper would benefit from a major revision that improves the overall structure and writing. Therefore, I recommend a score of 0 or strong rejection."}, "questions": {"value": "I do not have questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cjRq2iRu5b", "forum": "gTAhAebrV6", "replyto": "gTAhAebrV6", "signatures": ["ICLR.cc/2026/Conference/Submission19055/Reviewer_7XmH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19055/Reviewer_7XmH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928051551, "cdate": 1761928051551, "tmdate": 1762931087103, "mdate": 1762931087103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that Transformer attention and diffusion models are not fundamentally different. Instead, they can both be viewed as two ways of discretizing the same entropy-regularized optimal transport (OT) flow.\n\nThe key idea is:\n\n1. One attention layer works like a single OT update step (a KL-regularized, mirror-descent / JKO step).\n\n2. Stacking many layers approximates a continuous probability flow ODE (PF-ODE) over the probability simplex—meaning depth in a Transformer plays a similar role to time in diffusion models.\n\nThe paper also proposes a causal, semi-relaxed EOT version that keeps the autoregressive mask intact, fixing a key issue in earlier OT-based attention methods, which required doubly-stochastic couplings that are incompatible with causal language model attention."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a unified view by formally connecting attention and diffusion models via entropic optimal transport. This conceptual bridge is novel and **intellectually interesting**: it reframes two dominant generative modeling paradigms as different discretizations of the same continuous-time transport process. The introduction of a causal, semi-relaxed EOT formulation that preserves autoregressive masking is also a meaningful innovation, addressing a key limitation of prior OT-based attention methods."}, "weaknesses": {"value": "Writing is bad and incomplete.\n\n1. No related works in the introduction.\n\n2. The scope is too big and ambitious. The terms such as attention, transformer, diffusion models, Schrodinger bridge, diffusion LLM, are all quite popular in the research community. It is hard for now to use one unified theory to explain them clearly in one framework.\n\n3. To be honest, as an applied mathematician with extensive experience in OT and diffusion models, I don't fully understand the theorems and proofs. All the proofs are less than 3 lines."}, "questions": {"value": "Major revision is needed on the writing.\n\nMath/ Proof re-presentation is also needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A7u2ahU18L", "forum": "gTAhAebrV6", "replyto": "gTAhAebrV6", "signatures": ["ICLR.cc/2026/Conference/Submission19055/Reviewer_bpVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19055/Reviewer_bpVw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007854000, "cdate": 1762007854000, "tmdate": 1762931086697, "mdate": 1762931086697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}