{"id": "6B2zZ8EjHG", "number": 22962, "cdate": 1758337572380, "mdate": 1763705459209, "content": {"title": "Bayesian Optimization by Minimum Filling Distance Search", "abstract": "Bayesian Optimization sequentially queries objective function evaluations, often focusing on the expected utility of evaluating corresponding candidates under uncertainty with a learned probabilistic model of underlying true objective functions. We propose a new filling distance based acquisition function, termed Minimum Filling Distance Search (MFDS), to explicitly takes into account the location of the previous queried observations so that acquisition iterations can avoid oversampling and therefore explore the whole design space more efficiently. For multi-objective optimization, in addition to efficiently approaching the Pareto front, the queried candidates by MFDS are well spread over the entire Pareto set. We provide an asymptotical convergence proof and empirically evaluate MFDS performances, demonstrating the improvement over existing methods using other acquisition functions.", "tldr": "", "keywords": ["Minimum Filling Distance Search (MFDS)", "Multi-objective optimization", "Pareto front"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff29300e603600ce87f1b9ac302f91affe88b511.pdf", "supplementary_material": "/attachment/a770bd2df0c20a38681e360bfb1591857ee5442b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new acquisition function for Single-Objective (SO) and Multi-Objective (MO) Bayesian Optimization (BO), designed to minimize the expected closest distance between an optimal input and a corresponding reference set. In both the SO and MO settings, the acquisition function is formulated as a one-step look-ahead approach. The proposed method has been evaluated on multiple synthetic benchmark functions, demonstrating its effectiveness in improving solution quality and Pareto front coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposal of leveraging the average minimum distance as a metric for formulating the acquisition function is, to the best of my knowledge, novel and interesting.\n- The paper considers both the SO and MO settings and demonstrates that a similar formulation can be applied, with experiments cover both settings."}, "weaknesses": {"value": "- Limited synthetic benchmarks: The paper benchmarks on both single-objective functions (Gaussian mixture, Shekel, Forrester) and multi-objective problems (2-dimensional GMM, DTLZ2, and one real-world problem RE3-5-4) which is plausible. However, the benchmark complexity is more of proof of concept level, and the claim could be much more strength with more complicated empirical comparison.\n- Unclear attribution of performance gain: One unclear aspect is that the authors directly start with a one-step look-ahead formulation (Eq. 14, Eq. 17). Since one-step look-ahead methods already provide a performance benefit, it is not clear whether the performance gain of this algorithm stems from the new acquisition function formulation or from the look-ahead strategy itself. To ensure fair comparison, the standard acquisition functions should also be evaluated with one-step look-ahead. If the authors believe the look-ahead strategy is a core contribution, an ablation study should be included.\n- The complexity and scalability: the algorithm has its fundamental complexity and scalability issue, which would restrict its apllicability in moderate-high input dimensionalities."}, "questions": {"value": "- How is the  $X_n$ is set if it is different from $X$ ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ruwB5eM0Qo", "forum": "6B2zZ8EjHG", "replyto": "6B2zZ8EjHG", "signatures": ["ICLR.cc/2026/Conference/Submission22962/Reviewer_XTdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22962/Reviewer_XTdq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606499783, "cdate": 1761606499783, "tmdate": 1762942453287, "mdate": 1762942453287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to use as an acquisition function for Bayesian optimization the expected minimum distance between the design points and the true minimizer.\nThey develop a convergence proof and demonstrate this method on both single and multi-objective problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It's nice to have some theoretical results.\nNamely, the authors show that the acquisition function converges to zero.\nIt would be more ideal to have a bound/rate on regret or something like this but since there are also fairly extensive numerical results I think this is enough.\n\nThe numerical results seem adequate, with a decent number of benchmark comparators, uncertainty quantification on the iteration number, and five benchmark applications, which is perhaps sufficient."}, "weaknesses": {"value": "I found some of the discussion about exisiing BO methods surprising, and I think readers would appreciate more context for these claims:\n\ni) \"As a result, when an inappropriate prior is set for the probabilistic model, BO may fall in local optima (Wang & de Freitas, 2014).\" What part of that article are you specifically referring to when you say that inappropriate priors lead to local optima?\n\nii) \"[Entropy Search] design works well for objective functions with a single global optimum. For objective functions in real-world applications, however, there is often not just one single optimum.\" I think I unerstand why, conceptually, ES assumes a single global optimum. But have we actually observed any failure cases empirically in the multi-optimum case? Since it is well defined even if one of the local optima is only epsilon worse than the other, it's not obvious to me that it would actually fail in practice. Furthermore, the subsquent Max-Value entropy search of Wang and Jegelka would seemingly solve this anyways unless I'm issing something.\n\nI like some of the intuition built in the intro about why this method is useful under model misspecification, but I ultimately did not fully understand the derivation of the acquisition function. It would be helpful to in particular say a bit more about how the second term in the definition was obtained.\n\nThough a reasonable idea conceptually, there does not appear to be much in the way of analyitcal tractability for this acquisition function, and the authors' experiments are on small iteration counts. This will limit the applicability of the method."}, "questions": {"value": "1) I'm a bit confused with the main definition of the acquisition function u at the bottom of page 4.  The first term does not seem to depend on x. Is it supposed to be D_n\\cup\\{x,y\\} instead of just D_n?\n\n2) At the end of the day, X_N is going to be the set of previously observed design points at time N, or not?\n\n3) Doesn't this method suffer from a similar assumption of a unique global optimum as you mention in your criticism of ES?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2eHGtICWdE", "forum": "6B2zZ8EjHG", "replyto": "6B2zZ8EjHG", "signatures": ["ICLR.cc/2026/Conference/Submission22962/Reviewer_p3DZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22962/Reviewer_p3DZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946650746, "cdate": 1761946650746, "tmdate": 1762942453103, "mdate": 1762942453103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new acquisition function for Bayesian Optimization, Minimum Filling Distance Search (MFDS). MFDS chooses the next point by minimizing the expected distance between the sequence of sampled locations and (i) the posterior distribution of the global minimizer (single‑objective) or (ii) the posterior distribution of the Pareto set (multi‑objective). This explicitly leverages where past samples are, aiming to avoid oversampling and to cover regions with high probability of optimality. The authors prove an asymptotic convergence result for the single‑objective case (under specific assumptions) and empirically compare MFDS to other popular acquisition functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Nice and intuitive geometric representation of the objective. This acquisition function makes sense.\n- Theoretical proof of the convergence. \n- Results showing that this method outperforms others (though more tests should be performed)."}, "weaknesses": {"value": "- Theory is limited just to 1D. \n- Claiming theoretical proof of convergence and then mentioning the \"almost surely\" converges should be clarified better.\n- Very limited tests are performed. The authors should use more test functions and average results across different seeds."}, "questions": {"value": "- Fig.1 why don't you show comparison between different acquisition functions of the next selection point where the set of all the current points is the same? Current comparison seems to be unfair. \n- why are n-1 iterations done in one way and the last iteration is a greedy approach? This seems like a heuristic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o7J5XsBFRt", "forum": "6B2zZ8EjHG", "replyto": "6B2zZ8EjHG", "signatures": ["ICLR.cc/2026/Conference/Submission22962/Reviewer_Lh5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22962/Reviewer_Lh5F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961406835, "cdate": 1761961406835, "tmdate": 1762942452789, "mdate": 1762942452789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an acquisition function for Bayesian optimization. The proposed acquisition function considers the minimum distance between the sampling path and the set of optimal solutions. This acquisition function can also be used for multi-objective optimization. The authors prove the convergence of the proposed method and demonstrate its superior performance through experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is well-motivated, and considering the past observations is intuitively important for Bayesian optimization.\n- Experimental results demonstrate better performance of the proposed method than existing baseline methods."}, "weaknesses": {"value": "I am not an expert in Bayesian optimization, so I could not fully understand the advantage of the theoretical result over existing theoretical guarantees in this field. In particular, I did not fully understand what benefit we obtain from Theorem 1. Does $\\lim_{n\\to \\infty} \\min_{x'} G(x' \\cup X_n, \\mathcal{D}_n) = 0$ indicate that $x_n$ converges to the optimal solution(s)? Is there a similar convergence guarantee for other acquisition functions?"}, "questions": {"value": "My main concerns and questions are outlined in the Weaknesses section. Additionally, I have the following questions:\n\n- In Eq. (8), what is the definition of $P$?\n- In Eq. (12), it seems difficult to compute the integral involving $\\mu$ exactly. How can we compute this integral? What is the computational complexity?\n- In Section 5.1, what does the equation $f_{GM}(x) = -0.5,\\mathcal{N}(-\\mu, 2\\sigma^2) - 0.55,\\mathcal{N}(\\mu, \\sigma^2) + 1$ mean? Does it mean that the probability density functions of the Gaussian distributions $\\mathcal{N}(-\\mu, 2\\sigma^2)$ and $\\mathcal{N}(\\mu, \\sigma^2)$ are used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "sbMmUIKhw6", "forum": "6B2zZ8EjHG", "replyto": "6B2zZ8EjHG", "signatures": ["ICLR.cc/2026/Conference/Submission22962/Reviewer_9e8V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22962/Reviewer_9e8V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088606138, "cdate": 1762088606138, "tmdate": 1762942452603, "mdate": 1762942452603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}