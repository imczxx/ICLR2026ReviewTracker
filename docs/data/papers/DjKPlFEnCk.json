{"id": "DjKPlFEnCk", "number": 11033, "cdate": 1758187427062, "mdate": 1759897613159, "content": {"title": "Bag of Tricks for Subverting Reasoning-based Safety Guardrails", "abstract": "Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks.\nBy leveraging LRMs’ reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results.\nSpecifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. \nTo explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization.\nAlong with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services).\nEvaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse.", "tldr": "We developed four methods to jailbreak reasoning-based safety guardrails such as deliberative alignment, achieving a success rate exceeding 90%.", "keywords": ["Large Reasoning Models", "Large Language Models", "Red-teaming", "Jailbreaks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d751ea9cd85fd8a13bfd431465983b5e8cec4a56.pdf", "supplementary_material": "/attachment/105cf07e1959e7d18ea7df16bef7ab1ea17e1eb1.zip"}, "replies": [{"content": {"summary": {"value": "This work presents 4 jailbreaking attacks that help elicit harmful behavior in reasoning models, three of which try to bypass the reasoning (Structural CoT Bypass, Fake Over-Refusal, Coercive Optimization) and one that hijacks the reasoning (Reasoning Hijack). The methods leverage the usage of template tokens within the user input to deceive the model into skipping the reasoning or throw the model out-of-distribution to the training examples it was trained on, increasing compliance to harmful prompts. The work primarily benchmarks on the gpt-oss series along with other open source models against previous reasoning-based attacks. The work also conducts ablations on how temperature and reasoning effort affect the strength of the attack find that these bag-of-tricks are robust to such changes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The fact that utilizing template tokens can easily jailbreak reasoning models is alarming, especially to such high ASR. It is also interesting that gpt-oss-120b is more susceptible to such attacks than gpt-oss-20b.\n2. Good comparison to previous baselines on a wide range of benchmarks."}, "weaknesses": {"value": "1. Fake Over-Refusal, which uses nuanced inputs to trick the model into thinking it is a benign request, seems similar to previously known attacks that leverage nuances both in the single-turn domain (e.g., PAP, Single-turn Crescendo [1, 2]) and multi-turn domain (e.g., Crescendo [3]). What makes it different in this paper that it specializes for reasoning? \n2. How does Coercive Optimization perform on TARS [4], which is shown to be robust to format problems in the output generations from attacks like GCG?\n3. How is Reasoning Hijack different from Structural CoT Bypass? Essentially, they both seem to preemptively fill in the reasoning as a part of the user prompt and the only difference is the level of detail within the mock reasoning.\n4. Fake Over-Refusal uses an abliterated model to produce nuanced attack prompts. Although abliterated models are effective in producing compliant answers to harmful tasks, one side effect is that they may generate the targeted harmful information within the attack prompt even before attacking the target model, rendering the attack unnecessary. What do the attack prompt examples generated from the abliterated model look like?\n5. What is the difference between attacking a self-served version and API version of the same model?\n6. Although the work compared against existing open-weight models, prior work that uses reasoning-based defenses to align models may be more robust to reasoning-based attacks. How does the bag-of-tricks perform against open-source SFT-based (e.g., [5]) and RL-based (e.g., [4]) reasoning defenses?\n\n**References**\n\n[1] Zeng, Yi, et al. \"How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.\" Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024.\n\n[2] Aqrawi, Alan, and Arian Abbasi. \"Well, that escalated quickly: The single-turn crescendo attack (stca).\" arXiv preprint arXiv:2409.03131 (2024).\n\n[3] Russinovich, Mark, Ahmed Salem, and Ronen Eldan. \"Great, now write an article about that: The crescendo {Multi-Turn}{LLM} jailbreak attack.\" 34th USENIX Security Symposium (USENIX Security 25). 2025.\n\n[4] Kim, Taeyoun, et al. \"Reasoning as an Adaptive Defense for Safety.\" arXiv preprint arXiv:2507.00971 (2025).\n\n[5] Jiang, Fengqing, et al. \"Safechain: Safety of language models with long chain-of-thought reasoning capabilities.\" arXiv preprint arXiv:2502.12025 (2025)."}, "questions": {"value": "1. More details on the evaluation metrics would be helpful. For example, is the ASR measured on only the answer or also the reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3dpIBnYLqG", "forum": "DjKPlFEnCk", "replyto": "DjKPlFEnCk", "signatures": ["ICLR.cc/2026/Conference/Submission11033/Reviewer_KbZp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11033/Reviewer_KbZp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672041439, "cdate": 1761672041439, "tmdate": 1762922212959, "mdate": 1762922212959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the vulnerabilities of reasoning-based safety mechanisms in Large Reasoning Models (LRMs). The authors find that although such defenses can effectively reject harmful requests, they are extremely sensitive to minor modifications in the input prompts. The paper introduces four types of jailbreak attacks—Structural Chain-of-Thought (CoT) Bypass, Fake Over-Refusal, Coercive Optimization, and Reasoning Hijack—which achieve over 90% attack success rates across multiple models and benchmarks. The study reveals that the fragility of these defenses mainly stems from their dependence on template structures, concentration of refusal logic in the initial tokens, and lack of reasoning chain verification. Ultimately, the authors conclude that current reasoning-based defenses are far from foolproof, calling for new and more robust alignment and verification mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper analyzes the intrinsic flaws of LRMs and examines the potential for attacks. It designs four types of attacks targeting LRMs, achieving relatively high Attack Success Rates (ASR), which is a fairly satisfactory outcome. The authors conduct extensive empirical validation across multiple open-source and API-based models, and through ablation studies, they demonstrate the ineffectiveness of variables such as reasoning depth and sampling temperature, thereby proving that the problem is systematic."}, "weaknesses": {"value": "1.\tExperiments were conducted only on a subset of open-source models; only some attack methods support black-box settings, so the experimental evidence is insufficient.\n2.\tMethod 1 (STRUCTURAL CoT BYPASS) and Method 4 (REASONING HIJACK) are quite similar, yet Method 4 performs far better than Method 1 — does Method 1 remain necessary?\n3.\tThe Harmful Score is a rule-based judging mechanism; did the authors consider alternative judging methods for a more systematic evaluation?\n4.\tDid the authors consider or compare the cost of constructing different attack methods or the latency these attacks introduce to the system?"}, "questions": {"value": "See the section of weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZAJr1hScIb", "forum": "DjKPlFEnCk", "replyto": "DjKPlFEnCk", "signatures": ["ICLR.cc/2026/Conference/Submission11033/Reviewer_xaF9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11033/Reviewer_xaF9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922447383, "cdate": 1761922447383, "tmdate": 1762922212384, "mdate": 1762922212384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals that advanced chain-of-thought safety guardrails in LLMs are fragile, introducing four simple yet powerful jailbreak techniques that exploit common patterns in reasoning-based defenses."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper focus on an important and timely research question regarding the robustness of reasoning-based safety guardrails in LLMs.\n2. The proposed jailbreak techniques are simple yet effective.\n3. The presentation of the paper is clear and well-structured."}, "weaknesses": {"value": "1. **Lack of Contribution:** While the paper effectively demonstrates the fragility of reasoning-based safety guardrails, it fails to elaborate its contributions, both conceptually and technically, to the existing literature. All of the proposed jailbreak techniques are already known in prior work and are widely deemed as prompt engineering tricks among researchers and developers in the community. In this regard, what is the main contribution of this paper? And what can distinguish the proposed techniques from existing ones? Further, more theoretical analysis or systematical methodology is expected to enhance the technical depth of this paper.\n2. **Weak Motivation:** This paper proposes a \"bag of tricks,\" but it is unclear what the motivation of this practice is. What is the internal connection among these proposed techniques? Is there any systematic way to derive these techniques? I understand all of them somehow exploit the template tokens, but template token manipulation is also a common practice in existing jailbreak methods.\n3. **Problematic Evaluation Setup:** Most of the proposed tricks are simple and straightforward, with some apparent features. In other words, it is easy to identify these tricks through simple pattern matching or heuristic rules. Therefore, it is questionable to evaluate the effectiveness of these tricks against models with built-in safety guardrails. More specifically, if the defender is already aware of these tricks and tries to mitigate them, will these tricks still be effective? Further and in-depth evaluation is expected to validate the generalizability of these tricks in more practical scenarios."}, "questions": {"value": "1. What is the essential contribution of this paper?\n2. What is the internal connection among these proposed tricks?\n3. How to ensure the generalizability of these tricks in more practical scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uo3XeudGZj", "forum": "DjKPlFEnCk", "replyto": "DjKPlFEnCk", "signatures": ["ICLR.cc/2026/Conference/Submission11033/Reviewer_ESYU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11033/Reviewer_ESYU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992353256, "cdate": 1761992353256, "tmdate": 1762922211984, "mdate": 1762922211984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes 4 different techniques (\"tricks\") to jailbreak reasoning models (primarily the open-weights gpt-oss models) and evaluates them on 5 different jailbreak benchmarks. All of the jailbreak techniques are adaptations of commonly known jailbreaking techniques for LLMs to LRMs. The paper compares these techniques with baselines like H-COT and AutoRAN and shows significant increase in harmfulness scores across the benchmarks for the gpt-oss models. Evaluation on other open-source LRMs is also included but they lack a comparison with the baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and easy to understand. The methodology for evaluation and the techniques themselves are clearly explained\n- Evaluations are performed on 5 different jailbreak benchmarks, thus increasing the confidence in the claims for the models evaluated"}, "weaknesses": {"value": "1. Attacking local models is not really useful since one can anyways finetune the model instead of relying on jailbreak techniques. The attacks are effective as long as they work on models accessible only through an external provider.\n2. All of the attacks seem to be heavily relying on the chat-template knowledge which is easy to fix for an external model provider for the gpt-oss model. The model provider can tokenize the injections as normal text tokens rather than the chat template specific tokens (or even detect/remove them). Plus the chat template differs from model to model making the generalization of the attacks even harder.\n3. The effectiveness of the attacks relying on gpt-oss chat-template is further evidenced by the much lower scores for other open-weights models in Table 6 (max harmfulness scores are reduced by roughly half compared to Table 5). Additionally, Table 6 doesn't provide scores for the baseline method making it difficult if there are improvements in comparison to the baseline methods for these models.\n4. The baselines like H-COT and AutoRAN were attacking closed-source model like the o-series of OpenAI models (with complete black-box knowledge). It would be helpful to have a fairer comparison of attacks from this paper on closed-source models as well where these attacks are the most concerning.\n  \nAlso, the techniques themselves are mostly not that novel (except the use of gpt-oss specific chat template).\n\n  - Structural CoT Bypass: This is basically prompt injection with the chat template knowledge.\n  - Fake Over-Refusal: Rephrasing query using LLMs is pretty common in the literature. Although rephrasing to make it sound like an over-refusal query (rather than educational) seems novel.\n  - Coercive Optimization: This is GCG with the target string replaced with chat template specific part along with an instruction (e.g., answer in german)\n  - Reasoning Hijack: Filling in partial responses is also common in the literature. Here it is basically adapted to prefill the reasoning text."}, "questions": {"value": "- Can you clarify whether the fake over-refusal numbers in the tables use the Structural Bypass in it? Lines 175-179 seem to imply that it is used after doing Structural Bypass.\n- Please add baseline comparisons in Table 6 as well which would help in knowing how much of these attacks are overfitted on gpt-oss.\n- A table on applying these techniques on closed-source models as well would be very helpful to assess the generality of these attacks against frontier models\n- Also, please use the term open-weights for gpt-oss rather than open-source. A model is open-source if one can reproduce the model on their own from scratch (which requires training code + training data to be made publically available as well). The gpt-oss blogpost and paper claim to be open-weights too rather than open-source."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CRHQ0LYzKG", "forum": "DjKPlFEnCk", "replyto": "DjKPlFEnCk", "signatures": ["ICLR.cc/2026/Conference/Submission11033/Reviewer_Ldnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11033/Reviewer_Ldnt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008825705, "cdate": 1762008825705, "tmdate": 1762922211541, "mdate": 1762922211541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}