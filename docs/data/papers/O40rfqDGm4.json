{"id": "O40rfqDGm4", "number": 4316, "cdate": 1757661795949, "mdate": 1759898039959, "content": {"title": "A Closer Look at In-context Learning of LLMs in Simple Classification Tasks", "abstract": "*In-context learning* (ICL), as an emergent behavior of large language models (LLMs), has exhibited impressive capability in solving previously unseen tasks through observing several given in-context examples without further training. However, recent works find that LLMs irregularly obtain unexpected fragmented decision boundaries in simple machine learning classification tasks (e.g., binary linear classification). Although some efforts have been made in this problem, the phenomenon remains under-explored. Thus, in this paper,  we first explore the in-context learning capability of LLMs with both implicit and explicit reasoning paradigms. Our observations on the behaviors of LLMs indicate that LLMs consistently fail to achieve smooth decision boundaries in all cases and implicit reasoning is able to achieve better decision boundaries than explicit reasoning. Moreover, LLMs tend to address classification tasks in the way of machine learning algorithms. With these basic observations, we propose to dive into the behaviors of LLMs for a deeper understanding of their in-context learning capability on discriminative tasks. \n  To this end, we conduct a series of analyses on LLMs to explore how LLMs perform discriminative tasks. We explore the behaviors of LLMs in performing classification by prompting LLMs with specified machine learning algorithms and in high-dimensional classification tasks. Then, we propose a method to determine whether LLMs implicitly leverage machine learning algorithms when addressing classification tasks. Moreover, we also rethink the decision boundaries of LLMs from the perspective of data distributions. Overall, our analyses provide important observations and insights into the behaviors of LLMs in the discriminative tasks.", "tldr": "", "keywords": ["LLM", "In-context learning", "Classification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d695298ddc27fb2045e07a4e75de4ceef86bdf36.pdf", "supplementary_material": "/attachment/923e4750e8508100df62a2c79352e419130f1d0b.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the in-context learning (ICL) behavior of large language models (LLMs) on simple classification tasks, focusing on their decision boundaries and reasoning paradigms (implicit vs. explicit). The authors introduce a quantitative metric—Smooth Score—to measure the fragmentation of decision boundaries and analyze whether LLMs implicitly follow traditional machine learning (ML) algorithms. The study provides interesting empirical insights into how LLMs behave like conventional ML classifiers in simple discriminative settings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and relevance. The work addresses an important question in understanding the mechanisms underlying ICL, which is highly relevant to the ICLR community.\n\n2. Novel quantitative perspective. The introduction of the Smooth Score offers a new quantitative handle to study decision boundary smoothness, extending prior qualitative analyses.\n\n3. Systematic empirical setup. The authors compare multiple reasoning modes (implicit vs. explicit) and multiple LLMs (Llama, Qwen, DeepSeek, etc.), which increases the robustness of the observations.\n\n4. Interesting findings. The observation that implicit reasoning yields smoother decision boundaries than explicit reasoning provides a counterintuitive and potentially valuable insight into the interference effect of CoT-like prompting.\n\n5. Comprehensive experiments, exquisite figures and layout."}, "weaknesses": {"value": "1. Insufficient discussion of implications. The work stops short of discussing how these findings could inform model design or prompting strategies for real-world tasks. For instance, what is the significance of understanding or improving the non-smoothness in this LLM classification process for real-world classification tasks? \n2. Shallow theoretical contribution. Although the Smooth Score and KL-based analyses are well-defined, the paper mainly remains empirical and descriptive without deeper theoretical justification for why implicit reasoning aligns better with ML-like boundaries. \n3. Limited novelty . The study is largely built upon the framework proposed by Zhao et al., and compared with their work, it does not provide deeper insights or more compelling conclusions.\n4. Some formatting issues. Several sections (e.g., Figure captions, equation formatting, and cross-references like “Section ??”) need editorial polishing."}, "questions": {"value": "1. How do the observations transfer to textual or multimodal classification tasks beyond numeric toy data?\n2. What are the clear innovations compared to the work of Zhao et al.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jJIq6Pdrpv", "forum": "O40rfqDGm4", "replyto": "O40rfqDGm4", "signatures": ["ICLR.cc/2026/Conference/Submission4316/Reviewer_ev9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4316/Reviewer_ev9p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459953624, "cdate": 1761459953624, "tmdate": 1762917292620, "mdate": 1762917292620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how large language models (LLMs) perform in-context learning (ICL) in classification tasks. The authors observe that LLMs often exhibit irregular and fragmented decision boundaries, where implicit reasoning tends to produce smoother and more consistent boundaries than explicit reasoning. Their analysis suggests that, during discriminative tasks, LLMs implicitly emulate traditional machine learning algorithms, revealing a connection between contextual token-based reasoning and algorithmic behavior."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides extensive experimental evidence demonstrating the existence of irregular decision boundaries in LLMs during ICL.\n2. The exploration of fragmented decision boundaries may offers valuable insight into the inner mechanisms of ICL, contributing to a deeper understanding of one of the most central capabilities of current LLMs."}, "weaknesses": {"value": "1. The findings rely heavily on empirical observations, with limited theoretical analysis or intuitive explanation of why such irregularities emerge.\n2. The writing quality is poor—the abstract and introduction mainly describe what was done, without clearly articulating the key findings, their implications, or how they advance understanding within the community.\n3. The experiments are restricted to models under 8B parameters, leaving uncertainty about whether the observed fragmentation persists at larger scales. Consequently, the scalability and practical implications of the conclusions remain unclear."}, "questions": {"value": "How might different forms of explicit reasoning (e.g., Tree of Thoughts, self-critique, or reflective reasoning frameworks) influence the resulting decision boundaries? Would such reasoning strategies smooth out or exacerbate the irregularities observed compared to standard Chain of Thought reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OJlJJP92tu", "forum": "O40rfqDGm4", "replyto": "O40rfqDGm4", "signatures": ["ICLR.cc/2026/Conference/Submission4316/Reviewer_Tr9z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4316/Reviewer_Tr9z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872043717, "cdate": 1761872043717, "tmdate": 1762917292363, "mdate": 1762917292363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why Large Language Models (LLMs) produce unexpectedly fragmented decision boundaries when performing in-context learning (ICL) on simple machine learning classification tasks. It finds this is primarily due to a generalization failure caused by a distribution mismatch between the query data (OOD) and the in-context data.\nThe paper presents several in-depth analyses:\n1. Using a statistical method based on KL-divergence, it demonstrates that under standard implicit reasoning (without specific instructions), the predictive behavior of LLMs is statistically \"consistent with\" the ideal behavior of traditional ML algorithms (e.g., k-NN, SVM, MLP). This suggests LLMs do intrinsically and implicitly leverage an effective classification logic.\n2. The paper finds that the \"uniform grid\" query data, which caused fragmented boundaries in prior experiments, is Out-of-Distribution (OOD) relative to the \"clustered\" in-context examples seen by the LLM. When the query data is replaced with In-Distribution (ID) data (i.e., also sampled from the clusters), the LLM's classification performance significantly improves and aligns with traditional ML algorithms. This proves that LLMs are not incapable of classification, but rather that their ICL generalization to OOD data is poor."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's experimental design is highly systematic. Starting from the phenomenon of \"fragmented decision boundaries,\" it rigorously rules out various possibilities through carefully designed comparative experiments (e.g., implicit reasoning vs. explicit reasoning, standard prompts vs. specified ML algorithm prompts).\n2. The paper does not stop at the superficial conclusion that \"LLMs are bad at classification.\" Instead, it delves deeper to convincingly demonstrate for the first time that the root cause is OOD generalization failure—a distribution mismatch between the \"clustered\" examples (seen) and the \"uniform grid\" data (predicted).\n3. The paper proposes two simple and novel tools: (1) the \"Smooth Score,\" which for the first time quantitatively measures the fragmentedness of decision boundaries, moving analysis beyond subjective observation; and (2) a KL-divergence-based statistical method to determine if an LLM's behavior is \"consistent with\" traditional ML algorithms, offering a new approach for black-box model analysis."}, "weaknesses": {"value": "1. The analysis is primarily based on 8B-level models (Llama-3, Mistral, Qwen) and one DeepSeek-V3. It lacks systematic verification across a wider range of model scales (e.g., 70B+ or 3B-), making it uncertain if the conclusions apply to all LLMs.\n2. All experiments are conducted almost exclusively on 2D (and one 3D) synthetic data. The paper's key metric, the \"Smooth Score,\" is also acknowledged to be unscalable to high dimensions. Therefore, it is unknown if these findings will hold for real-world, high-dimensional, multi-class classification tasks."}, "questions": {"value": "1. How is the validity of the \"Smooth Score\" verified? If a model (LLM) outputs a \"trivial solution,\" such as predicting the same class (e.g., all 0s) for all query points, its \"Smooth Score\" would be a perfect 1.0.\n2. The \"KL-divergence-based statistical method\" should report variance (or standard deviations) for its averaged results, in order to demonstrate whether the \"gaps\" (which the paper relies on for comparison) are statistically significant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2h123L26U", "forum": "O40rfqDGm4", "replyto": "O40rfqDGm4", "signatures": ["ICLR.cc/2026/Conference/Submission4316/Reviewer_nLo7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4316/Reviewer_nLo7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874622818, "cdate": 1761874622818, "tmdate": 1762917292079, "mdate": 1762917292079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper probes in-context learning (ICL) behavior of LLMs on simple ML classification tasks (linear, circles, moons; plus a 3-D extension). It compares implicit vs explicit reasoning prompts, examines what happens when prompts explicitly name ML strategies (e.g., “use k-NN/SVM/DT/MLP”), and asks whether LLM decisions resemble those of conventional ML. Core findings: (i) LLM decision boundaries are fragmented, and implicit prompting yields less fragmented boundaries than explicit CoT; (ii) naming ML strategies tends to worsen boundary smoothness; (iii) using a KL-divergence gap test, the authors find that under a standard (implicit) prompt, LLM outputs on linear tasks are closest in distribution to conventional ML; (iv) distribution shift matters, when query and in-context data are aligned (ID queries), LLMs look much more like conventional ML; when the grid includes OOD regions, fragmentation grows. They also propose a quantitative Smooth Score (local k-NN agreement) to study smoothness beyond 2-D."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The same two effects show up across several models and are interesting: decision boundaries from LLMs often look fragmented, and implicit prompting gives cleaner maps than explicit prompting (shown first on Llama-3-8B and replicated on DeepSeek-V3, Mistral-8B, and Qwen-2.5-7B).\n- The explicit-reasoning examples are informative: the paper documents that models often describe their process in ML-like terms (e.g., analyzing patterns or invoking k-NN/DT), which is a nice qualitative complement"}, "weaknesses": {"value": "- Why we need to study LLM behaviours on linear classification tasks? The paper centers its analysis on toy ML classification tasks (linear / simple geometric datasets). Even the authors position these as proxies “to dive into the behaviors of LLMs,” not as tasks that matter in practice, where conventional ML already yields accurate, fast, and cheap solutions. The work does not articulate why understanding LLM behavior on these simplified settings will translate to real downstream wins for LLM prompting or evaluation.\n- The proposed metric (“smooth score”) may lack some analyses. Smooth score depends on K-NN neighborhoods, but the paper does not study sensitivity to K, nor relate the score to other boundary complexity measures.\n- A central result is that specifying ML strategies in the prompt degrades decision-boundary smoothness. But the study does not isolate whether this comes from instruction-following overhead, longer context length, token budget, or spurious tool-use emulation. A controlled ablation might help.\n- Evidence for “LLMs implicitly leverage ML algorithms” is not strong enough. Stronger causal probes (interventions, counterfactual prompts, controlled transforms) might help."}, "questions": {"value": "- The finding that implicit reasoning yields smoother boundaries is interesting, but the explanation (extra instructions “damage capability”) is conjectural without mechanistic analysis, could author further explain why implicit reasoning has smoother boundaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "meCsGhOjA2", "forum": "O40rfqDGm4", "replyto": "O40rfqDGm4", "signatures": ["ICLR.cc/2026/Conference/Submission4316/Reviewer_QQyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4316/Reviewer_QQyP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016717879, "cdate": 1762016717879, "tmdate": 1762917291709, "mdate": 1762917291709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}