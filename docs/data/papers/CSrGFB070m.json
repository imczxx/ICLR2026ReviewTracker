{"id": "CSrGFB070m", "number": 17966, "cdate": 1758282501861, "mdate": 1759897142143, "content": {"title": "IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs", "abstract": "Tokenizers play a crucial role in determining the performance, training efficiency, and the inference cost of Large Language Models (LLMs). Designing effective tokenizers for multilingual LLMs is particularly challenging due to diverse scripts and rich morphological variation. While subword methods such as Byte Pair Encoding (BPE) are widely adopted, their effectiveness in multilingual settings remains underexplored. We present IndicSuperTokenizer, a tokenizer for Indic multilingual LLMs, that combines both subword and multi-word tokenization, along with language-specific pre-tokenization, leading to more linguistically aligned tokens and achieving a new state-of-the-art in fertility score. Evaluated across English, $22$ Indian languages and code data, our tokenizer improves the average fertility score by $39.5$% over LLaMA4 and by $18$% over Sutra (the current best). This translates to $44$% improvement in inference throughput over LLaMA4 while maintaining comparable performance on English and Indic benchmarks. We also present detailed ablations across tokenizer training data size, vocabulary size, merging techniques, and pre-tokenization strategies, demonstrating the robustness of our design choices.", "tldr": "We present IndicSuperTokenizer, a tokenizer for Indic multilingual LLMs, that combines both subword and multi-word tokenization along with language-specific pre-tokenization, achieving a new state-of-the-art in fertility rate.", "keywords": ["Tokenizer", "Multilingual LLMs", "Multilingual Tokenizer", "Indic", "Tokenization Efficiency", "LLMs", "Indic Tokenizer", "Indian Languages", "BPE"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/025460093518fe0649e3d23d01510f3d55ae783b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new tokenizer for Indic languages that results in lower token fertility score compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is comprehensive and relatively well-written.\n2. The authors compared the latency and throughput for a model trained with their tokenizer and a base tokenizer.\n3. The paper also looks at glitch tokens and at the possibility of fine-tuning pre-trained models with their new tokenizer."}, "weaknesses": {"value": "1. Not particularly novel methodology. The pre-tokenization step is simply using the regex from Llama-4 and allowing cross-word tokenization within the sentence. Then, standard BPE is applied.\n2. It is not surprising that a tokenizer designed for a specific language family would perform better than such designed for other languages or more languages. While it is interesting that it outperforms two other Indic language tokenizers, there is no explanation or discussion in the paper as to why that may be the case.\n3. Bytes-per-token seems to be a strange metric as it depends on the specific encoding scheme and the length of words in characters. Unicode encodes different scripts with different numbers of bytes (one to three) and languages vary in how long (in number of characters) their words are (Chinese tends to use three times less characters than English for the same content). Therefore, this metric seems to be confounded with other aspects of language, making it not particularly suitable.\n4. Looking at Table 8 and contrary to the claims in the paper, there is little if any difference in performance between the model trained with the Llama-4 and the IST tokenizers.\n5. Overall reads more like a technical report than a scientific paper: it provides details on how the authors built and designed a specific instance of a tokenizer but not much scientific or transferable insight."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sl20E9Thad", "forum": "CSrGFB070m", "replyto": "CSrGFB070m", "signatures": ["ICLR.cc/2026/Conference/Submission17966/Reviewer_9sJw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17966/Reviewer_9sJw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761160422533, "cdate": 1761160422533, "tmdate": 1762927762220, "mdate": 1762927762220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the significant inefficiency of standard tokenizers for multilingual LLMs, particularly for morphologically rich Indic languages which suffer from high token-to-word ratios. High fertility increases training costs, inference latency, and context size usage. The authors propose IST, an optimized tokenizer for 22 Indic languages, English, and code.\n\ncontributions:\n1. IST achieves a new state-of-the-art fertility score, improving 39.5% over LLaMA-4 and 18% over Sutra on average.\n2. Pretraining a 1B model from scratch with IST results in a 44% improvement compared to the LLaMA-4 tokenizer in inference throughput while maintaining comparable performance.\n3. The paper justifies its design choices through extensive ablations.\n4. The authors show that IST can replace the tokenizer of an existing pre-trained model, achieving the same efficiency gains while preserving the original model's performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important practical issue: `the inefficiency of standard tokenizers for morphologically rich non-English languages`.\n2. The IndicSuperTokenizer demonstrates genuinely impressive and state-of-the-art results on intrinsic metrics like fertility score and NSL.\n3. The 44% improvement in inference throughput is a substantial practical gain.\n4. The authors performed an extensive set of ablations to justify their design choices."}, "weaknesses": {"value": "1. This paper's primary weakness is its lack of algorithmic novelty. The authors state the method is `inspired from SuperBPE` and follows its `curriculum principles`. The core contribution is not a new tokenization algorithm, but rather the careful application and tuning of an existing one to a new domain, combined with other existing components. This feels more like a strong engineering effort than fundamental research suitable for ICLR.\n2. This work focused on specific Indic languages. While this is valuable work for that community, its direct contribution to the general machine learning and representation learning audience at ICLR is limited. The findings are an application.\n3. Table 8 is not referenced in main text."}, "questions": {"value": "1. Given that the core two-stage training algorithm is from SuperBPE and the pre-tokenization regex is from LLaMA-4, what do the authors consider to be the primary novel algorithmic contribution of this work?\n2. Could the authors provide the quantitative data of latency for the abandoned morphology-aware approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3ne37VnK5t", "forum": "CSrGFB070m", "replyto": "CSrGFB070m", "signatures": ["ICLR.cc/2026/Conference/Submission17966/Reviewer_NCw7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17966/Reviewer_NCw7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904712784, "cdate": 1761904712784, "tmdate": 1762927761377, "mdate": 1762927761377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of tokenization for LLM training, focusing on the highly multilingual context of languages spoken in India. The authors use recent insights on super-word tokenization to train IndicSuperTokenizer, a SuperBPE (Liu et al., 2025) tokenizer for 22 Indian languages, English, and code. In their experiments, the authors show that IndicSuperTokenizer results in substantially better intrinsic metrics compared to baselines (e.g., higher efficiency), while exhibiting comparable extrinsic performance on downstream benchmarks. The paper also contains several post-hoc analyses, such as an experiment on glitch tokens and a comparison of SuperBPE with BoundlessBPE (Schmidt et al., 2025)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The experimental setup and analyses are methodologically sound. The intrinsic performance improvements are substantial. It is great to see that the authors actually pretrained an LLM using their tokenizer (even though the performance improvements are only modest at best). The writing of the paper is also clear and easy to follow."}, "weaknesses": {"value": "The main weakness of the paper in my opinion is that it is highly incremental, especially for a venue like ICLR that focuses on technical advances. The authors use an existing method (specifically, the SuperBPE tokenizer) and apply it in a new setting. While some design decisions are original (e.g., the vocabulary allocation strategy), they are pretty minor, and it seems that the main improvements are due to the use of SuperBPE.\n\nI think this paper could be published as is at a specialized venue (e.g., a workshop). To be of interest for a broader audience, the authors would need to show better how they are making technical contributions that go beyond the application of an existing method in a new setting."}, "questions": {"value": "What are the novel technical contributions that your work is making?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8M9B7f9aQ", "forum": "CSrGFB070m", "replyto": "CSrGFB070m", "signatures": ["ICLR.cc/2026/Conference/Submission17966/Reviewer_xsrk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17966/Reviewer_xsrk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953862191, "cdate": 1761953862191, "tmdate": 1762927760829, "mdate": 1762927760829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce an efficient tokenizer for Indic LLMs built on top of SuperBPE, achieving state-of-the-art parity scores across Indic languages, code, and English. Their proposed IndicSuperTokenizer combines the strengths of both subword and multi-word tokenization, along with script-specific pretokenization strategies that yield more linguistically aligned segmentations. When applied to train and evaluate models across English and multiple Indic languages, the tokenizer largely preserves model performance while boosting inference throughput by 44%"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Adapting the original SuperBPE algorithm to low-resource, non-Latin script languages like Indic is commendable and makes a meaningful contribution to the multilingual NLP community.\n- The paper includes very fine-grained analysis and ablations, covering key factors in training superword tokenizers for Indic languages, such as vocabulary size, merging strategies, and pre-tokenization techniques.\n- Using script-agnostic pretokenization in the first stage of tokenizer training improved token-to-word ratios by 38–40% on Indic scripts. \n- Their tokenization approach preserves model performance while boosting inference throughput."}, "weaknesses": {"value": "- Section 2.4 contains comparisons with different baseline toenizers. But is fertility fairly comparable here, given the potential difference in data distributions each tokenizer baseline was trained on? At the very least, a short description of how each tokenizer was trained, if possible, would make comparisons more meaningful. \n- It would also really help to see a direct comparison between IndicSupertokenizer and a regular BPE trained on the same data with script-agnostic pretokenization. Right now, it’s hard to tell whether the improvements come from the tokenizer itself or just differences in the training data.\n- For downstream evaluation, it’s not clear what results are zero-shot or from finetuning. I know that some of the Indic task-specific datasets have train/test splits. \n- More on the downstream evaluation, is there a reason why IndicBPEtokenizer (just using the first stage) wasn’t considered for these extrinsic evaluations? I believe it would help to isolate the contribution of the super words learned in the later stage. \n- In section 4.3, can you provide more details on the 200 examples used for analyzing inference efficiency? Are these parallel sentences? Also are these the same models evaluated in section 4.2? \n- Compared to the original SuperBPE paper, the performance improvements here appear smaller. Any thoughts on why that might be would be valuable."}, "questions": {"value": "- Do you have any insights from your analysis of vocabulary allocation strategies? Prior work has shown that training language-family or script-specific tokenizers and then merging their vocabularies can sometimes benefit low-resource languages. Though it increases vocabulary size, it can lead to small gains in downstream performance. You focus on fertility here, but it would be interesting to see whether similar trends hold for your setup.\n- Why does English dominate in your dataset? Was that to preserve English performance in LLaMA or to encourage cross-lingual transfer to Indic languages, or was it just a result of data availability?\n- Finally, pointing readers to the appendix for key metrics (as in line 42) isn’t ideal. It would be better to report an aggregated number, like average fertility, directly in the main text for readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9q9GIWXBKs", "forum": "CSrGFB070m", "replyto": "CSrGFB070m", "signatures": ["ICLR.cc/2026/Conference/Submission17966/Reviewer_4cy4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17966/Reviewer_4cy4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066997547, "cdate": 1762066997547, "tmdate": 1762927760368, "mdate": 1762927760368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}