{"id": "qZPp8acMVS", "number": 2262, "cdate": 1757043574933, "mdate": 1759898159467, "content": {"title": "Curvature-Aware Residual Prediction for Stable and Faithful Diffusion Transformer Acceleration Under Large Sampling Intervals", "abstract": "Diffusion Transformers have achieved remarkable performance in generative tasks, yet their large model size and multi-step sampling requirement lead to prohibitively expensive inference. Conventional caching methods reuse features across timesteps to reduce computation, but introduce approximation errors that accumulate during denoising‚Äîa problem exacerbated under large sampling intervals where significant feature variations amplify errors. Recent prediction-based approaches (e.g., TaylorSeers) improve efficiency but remain limited by sensitivity to feature variations across distant timesteps and the inherent truncation errors of Taylor expansions.\nTo address these issues, we propose a novel **C**urvature-**A**ware **R**esidual **P**rediction (CARP) framework, which shifts the prediction target from raw features to residual updates within Diffusion Transformer blocks.  We observe that residuals exhibit more stable and predictable dynamics over time compared to raw features, making them better suited for extrapolation. Our approach employs a rational function-based predictor, whose theoretical superiority over polynomial approximations is rigorously established: the numerator performs linear extrapolation using adjacent features, while the denominator incorporates discrete curvature to adaptively modulate the strength and behavior of the prediction. This design effectively captures the alternation between gradual refinements and abrupt transitions in diffusion denoising trajectories. Additionally, we introduce a curvature-guided gating mechanism that regulates the use of predicted values, enhancing robustness under large sampling steps. Extensive experiments on FLUX, DiT-XL/2, and Wan2.1 demonstrate our method's effectiveness. For instance, at 20 denoising steps, we achieve up to 2.88√ó speedup on FLUX, 1.46√ó on DiT-XL/2, and 1.72√ó on Wan2.1, while maintaining high quality across FID, CLIP, Aesthetic, and VBench metrics, significantly outperforming existing feature caching methods. In user studies on FLUX, CARP receives nearly 25\\% more preference than the second-best method. These results underscore the advantages of residual-targeted prediction combined with a rational function-based extrapolator for efficient, training-free acceleration of diffusion models.", "tldr": "", "keywords": ["Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/310e71c3c1f41395ffa0b3ceae2cc90c7061615d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the CARP: a training-free, curvature-aware residual prediction scheme that ensures theoretical superiority over current prediction-based methods and reuse-based acceleration methods by shifting the prediction target from raw features to residual updates within the Diffusion Transformer Blocks. The author introduces a rational-function-based prediction method that involves a numerator performing linear extrapolation using adjacent features and a denominator that incorporates discrete curvature to ensure optimal skipping in steps. Experiment results show strong speedups with preserved qualities of generation in both image and video generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Practical deployability. CARP is training-free and plug-and-play with no architecture changes. There is also minimal overhead introduced in both memory and inference, given the method uses a tiny, fixed history (3 residuals) and a simple gate, so it's easy to integrate and adds little overhead while yielding meaningful speedups.\n\n2. Stability under large strides. The curvature-gated fallback with rational predictor adaptively controls the error introduced in acceleration, avoiding over-/undershoot and reducing error accumulation in comparison with evaluated baselines.\n\n3. Methodology novelty. While many existing studies have studied the step-wise sparsity in multi-step generation, the method in this paper provides a new solution with mathematical justification."}, "weaknesses": {"value": "1. Limited Ablation on Samplers. It appears that the DDIM is the only sampler evaluated; please provide a solver sweep (DPM-Solver / DPM-Solver++, Euler, and a flow-matching ODE case) under identical prompts, seeds, and step counts (e.g., 20/50), reporting FID and LPIPS, latency, and speedups to verify solver-agnostic robustness.\n\n2. Missing Baselines. There are many similar step-wise sparsity-aware open-sourced baselines that have been proposed before this paper but not yet evaluated: DeepCache (CVPR 24'), AdaptiveDiffusion (NeurIPS 24'), SADA (ICML 25'), AB-Cache (ArXiv, optional as may not be open-sourced yet, but good to mention results in paper). Please evaluate these baselines on one of the DDIM/DPM Solver/Euler samplers. Please also provide all hyperparameters used in the comparison of these open-source baselines.\n\n3. Missing Metric. The author should also report the LPIPS (also reported in TaylorSeer but missing in CARP) as a metric to justify the perceptual distance shifting from the unaccelerated model. Please compare your method with the TaylorSeer and baselines mentioned above on FID, PSNR, SSIM, and LPIPS with speedups.\n\n\n\nRefs:\n\n[1] Narayanan, Arvind, et al. \"Deepcache: A deep learning based framework for content caching.\" Proceedings of the 2018 Workshop on Network Meets AI & ML. 2018.\n\n[2] Ye, Hancheng, et al. \"Training-free adaptive diffusion with bounded difference approximation strategy.\" Advances in Neural Information Processing Systems 37 (2024): 306-332.\n\n[3] Jiang, Ting, et al. \"Sada: Stability-guided adaptive diffusion acceleration.\" arXiv preprint arXiv:2507.17135 (2025).\n\n[4] Yu, Zichao, et al. \"AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse.\" arXiv preprint arXiv:2504.10540 (2025)."}, "questions": {"value": "1. The author states, \"selected the optimal hyperparameters to ensure a fair comparison\" in the experiment section. Can the author list all hyperparameters for each baseline they have tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NOM8iUkO1l", "forum": "qZPp8acMVS", "replyto": "qZPp8acMVS", "signatures": ["ICLR.cc/2026/Conference/Submission2262/Reviewer_1vXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2262/Reviewer_1vXq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507760340, "cdate": 1761507760340, "tmdate": 1762916168047, "mdate": 1762916168047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free acceleration framework for Diffusion Transformers. Instead of reducing the number of denoising steps, CARP predicts the residual updates between steps to avoid redundant Transformer computations. It introduces a rational-function predictor that extrapolates residuals using recent history. A curvature-based gating mechanism adaptively decides whether to use the predicted residual or perform a real forward pass, ensuring both stability and fidelity across smooth and complex denoising regimes. CARP achieves up to 2.9√ó speedup on FLUX and 1.5‚Äì1.7√ó on DiT-XL/2 and Wan 2.1 with minimal quality degradation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method offers a training-free, plug-and-play acceleration solution that can be integrated into any Diffusion Transformer without retraining or architecture modification.\n\n2. The paper provides strong empirical and theoretical validation. Experiments on multiple high-end Diffusion Transformer backbones‚ÄîFLUX, DiT-XL/2, and Wan2.1‚Äîdemonstrate consistent acceleration (up to 2.9√ó) with minimal perceptual or quantitative degradation."}, "weaknesses": {"value": "1. Limited evaluation scope and lack of scaling/generalization evidence.\n\nThe evaluation focuses exclusively on three Diffusion Transformer architectures (FLUX, DiT-XL/2, and Wan 2.1), each tested only under a 20-step denoising schedule. However, the curvature-aware predictor‚Äôs stability and error behavior could change under longer sampling horizons (e.g., 50 steps or 100 steps), where residual trajectories evolve more gradually but accumulate error differently. Assessing CARP‚Äôs performance across multiple step settings would provide stronger evidence of its robustness.\n\nMoreover, although CARP claims to be architecture-agnostic, all experiments are conducted on Transformer-based diffusion models. It remains unclear how well the curvature-aware residual prediction generalizes to UNet-based diffusion models such as Stable Diffusion XL (SDXL) or EDM, where spatial convolutional dependencies differ substantially from DiT‚Äôs attention-driven dynamics. Including such baselines would better support the general-applicability claim.\n\n2. Clarification on the Definition and Scope of Single, Dual, and Full DiTBlocks in Table 6.\n\nI find the terminology in Table 6 ‚Äî Single, Dual, and Full DiTBlocks ‚Äî somewhat unclear. Does ‚ÄúDual DiTBlock‚Äù refer to the double-stream blocks used in FLUX? What exactly does ‚ÄúFull DiTBlock‚Äù mean? Please clarify. Also, am I correct in understanding that Table 6 studies the performance difference between partial-block skipping and skipping the entire model stack at once?\n\n3. Limited exploration of curvature threshold sensitivity.\n\nThe gating threshold ùëÅ=1.4 is empirically selected but not deeply analyzed. Understanding how this hyperparameter trades off between stability and acceleration‚Äîand whether adaptive thresholds (learned or schedule-based) could outperform a fixed value‚Äîwould make CARP more robust across models."}, "questions": {"value": "I am curious about this training-free method: does it always skip the same block for different text inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RVe4khGQ8i", "forum": "qZPp8acMVS", "replyto": "qZPp8acMVS", "signatures": ["ICLR.cc/2026/Conference/Submission2262/Reviewer_sQVm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2262/Reviewer_sQVm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734234092, "cdate": 1761734234092, "tmdate": 1762916167892, "mdate": 1762916167892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CARP is a training-free, model-agnostic acceleration for Diffusion Transformers that predicts residual updates (instead of raw features) using a curvature-aware rational predictor with gating, reducing accumulated errors‚Äîespecially at large sampling steps. It delivers up to 2.88√ó/1.46√ó/1.72√ó speedups on FLUX/DiT-XL/2/Wan2.1 at 20 steps while preserving quality (FID/CLIP/Aesthetic/VBench) and ~25% higher user preference on FLUX, outperforming cache- and Taylor-based baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves SOTA performance.\n2. It is a training-free method that does not consume many resources."}, "weaknesses": {"value": "1. The writing is very poor.\n2. The reason for employing the residual instead of the output itself is not clear.\n3. The meaning of $\\widetilde\\kappa_t$ is not well justified.\n4. The reason for using the first-order residual when $\\widetilde\\kappa_t$ is large is also not clear.\n5. I am curious about the memory consumption of this method, as it requires storing lots of residuals.\n6. The performance on SOTA video generation methods, e.g., Hunyuan and Wan2.1 on high-resolution generation, e.g., 720p and beyond, is missing. The acceleration of more powerful video generation models towards higher resolution should be more challenging and practical.\n7. The author should include the experiments on few-step diffusion models.\n8. I am curious why the authors do not compare their method with TaylorSeer at a higher order compensation, which can achieve better performance."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FGRAsrBps0", "forum": "qZPp8acMVS", "replyto": "qZPp8acMVS", "signatures": ["ICLR.cc/2026/Conference/Submission2262/Reviewer_XAXC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2262/Reviewer_XAXC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763451956, "cdate": 1761763451956, "tmdate": 1762916167633, "mdate": 1762916167633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CARP, a training-free inference acceleration scheme for Diffusion Transformers. Instead of extrapolating raw hidden states across timesteps, CARP predicts end-to-end residual updates from a short history and uses a rational extrapolator whose denominator is modulated by a curvature signal; a curvature-guided gate decides whether to trust the prediction or fall back to a full forward pass. Experiments on FLUX (text-to-image), DiT-XL/2 (ImageNet), and Wan2.1 (text-to-video) report speedups up to 2.88√ó with limited degradation on standard metrics, plus a user preference boost on FLUX. The history window is fixed to 3 residuals and the method uses curvature thresholds for both the rational term and the hard gate. A theoretical note links the sign of discrete curvature to the direction of linear-extrapolation error, motivating the denominator‚Äôs form."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall idea (predict residuals, not features) and the rational predictor/gating are easy to follow. \n\n2. CARP does not modify architectures and targets the low-step regime most relevant for latency. \n\n3. Reported speedups at 20 steps on FLUX/DiT-XL/2/Wan2.1 with competitive quality (e.g., FLUX up to 2.88√ó) and a positive user study."}, "weaknesses": {"value": "1. Heavy reliance on hand-crafted thresholds / hyperparameters. CARP‚Äôs gating and denominator strength hinge on manually chosen thresholds (e.g., ùëÅ). While there is an ablation, the paper does not convincingly show that these can be set once and generalize across models, datasets, and samplers without per-scenario tuning. \n\n2. Fixed and narrow temporal context. The method fixes the history window to 3, which may limit robustness when trajectories are noisier or when step schedules differ. There‚Äôs no exploration of adaptive or larger windows. \n\n3. Decision signals feel manual/heuristic. The normalized curvature, thresholds, and hard gate are designed features rather than learned criteria; it‚Äôs unclear how stable they are under distribution shift (e.g., different prompt mixes, seeds, schedulers). \n\n4. Missing comparisons to few-step/step-distillation baselines. Since the paper targets aggressive low-step regimes, it should compare against step-distilled or solver-distilled models under matched wall-clock/quality budgets, not only cache/prediction baselines (ToCa, TeaCache, Œî-DiT, TaylorSeer). The current suite does not settle the ‚Äúbest way‚Äù to achieve low-latency sampling. \n\n5. Limited discussion of overheads. Computing curvature, gating, and rational inference adds control-flow and tensor ops. The paper would benefit from a breakdown of where the speedups come from (skipped blocks vs. cache reuse vs. prediction) and their sensitivity to GPU/TPU kernels."}, "questions": {"value": "1. Window size/generalization. How does CARP perform with other window sizes or an adaptive window (e.g., expand when curvature is stable, shrink when volatile)? Please report sensitivity curves and costs.\n2. Can you provide cross-model, cross-dataset results showing that a single set of hyper-parameter works well without retuning? Any automatic calibration procedure?\n3. Please include a kernel-level latency breakdown (prediction/gating vs. DiT forward) to show that wins aren‚Äôt hardware-specific and to guide future optimizations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wFpvxaF1Ii", "forum": "qZPp8acMVS", "replyto": "qZPp8acMVS", "signatures": ["ICLR.cc/2026/Conference/Submission2262/Reviewer_UBz3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2262/Reviewer_UBz3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995492542, "cdate": 1761995492542, "tmdate": 1762916167386, "mdate": 1762916167386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}