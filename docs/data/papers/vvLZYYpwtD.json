{"id": "vvLZYYpwtD", "number": 8945, "cdate": 1758103433081, "mdate": 1759897752571, "content": {"title": "When Sketches Diverge, Language Converges: A Universal Feature Anchor for Domain-Agnostic Human Reconstruction", "abstract": "When humans sketch the same pose, no two drawings are alike. Synthetic sketches exhibit algorithmic precision with clean edges and consistent strokes, while freehand sketches diverge wildly—each bearing the unique abstraction, style, and imperfections of its creator. This fundamental divergence has long challenged 3D human reconstruction systems, which struggle to bridge the chasm between these disparate visual domains. We present a paradigm shift: while sketches diverge, language converges. A pose described as \"arms raised overhead\" carries the same semantic meaning whether drawn by algorithm or artist. Building on this insight, we introduce a universal feature anchor—natural language—that remains constant across visual variations. Our framework leverages text descriptions to guide feature learning, creating domain-agnostic representations that transcend the synthetic-freehand divide. At the technical core lies our Text-based Body Pose Head (TBPH), featuring a novel gating mechanism where language-derived features dynamically reweight spatial regions of sketch features. This text-guided attention enables the model to focus on semantically meaningful pose indicators while suppressing domain-specific noise and stylistic artifacts. By augmenting 26,000 sketch-pose pairs with rich textual descriptions, we enable cross-modal supervision that teaches our model to see past surface differences to underlying pose semantics. Extensive experiments demonstrate our method's superiority: we achieve 139.86mm MPJPE on freehand sketches, a 9.7% improvement over state-of-the-art TokenHMR. More importantly, we show true domain-agnostic performance—our model trained on both domains exhibits minimal degradation when tested on highly abstract amateur sketches. This work establishes language as a powerful intermediary for visual domain adaptation, opening new avenues for robust cross-domain understanding in computer vision.", "tldr": "Our framework leverages text descriptions to guide feature learning, creating domain-agnostic representations that transcend the synthetic-freehand divide.", "keywords": ["Language and Vision", "Multimodal Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11add31e4ec22fd339e85ad916024a826011550a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper deals with reconstruction of 3D human mesh from sketches. It introduces UniAnchor, that leverages natural-language descriptions as a universal semantic anchor for any sketch domain. The key idea is that while visual sketch styles diverge (e.g. synthetic vs. freehand), the language description of the human and the pose remains semantically invariant.\n\nThe technical novelty in UniAnchor integrates a Text-based Body Pose Head (TBPH) containing a Text-guided SketchGate (TGSG) module and a VQ-VAE decoder, enabling cross-modal feature alignment between sketch and text domains.\nThe authors augment 26K sketch–pose pairs (Wang et al., 2023) with PoseScript text descriptions and train the system in two stages (synthetic → freehand).\n\nSeveral experiments are given and tesults show state-of-the-art performance with a 9.7% improvement on cross-domain generalization. A user study (ANOVA p < 0.001) confirms perceived faithfulness and quality improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe idea of language as a modality-invariant bridge between synthetic and freehand sketches is novel and well motivated.\n\n•\tThe Text-guided SketchGate (TGSG) module and VQ-VAE pose decoder form a coherent mechanism for cross-modal alignment.\n\n•\tExperiments show consistent gains across datasets and strong generalization (96.9 mm MPJPE on a new 10 k-pair dataset  ).\n\n•\tThe methods component (TBPH, TGSG, InfoNCE loss, modalities) are ablated quantitatively.\n\n•\tThe user study shows faithfulness 5.68 ± 0.35 and quality 5.86 ± 0.39 outperform baselines.\n\n•\tThe paper is well structured and written in general."}, "weaknesses": {"value": "The main weakness is the reliance on textual descriptions of human pose.\n\nFirst, the text descriptions were auto-generated by PoseScript, and can be coarse or inaccurate. As the main claim of the paper relies on these annotations it would be good to validate them somehow. For example, using some manual annotations or LLM-refined texts and comparing or evaluating the results. \n\nSecond, text descriptions remains invariant to the sketch style, but can also be very cumbersome in describing delicate visual configurations. No discussion on this was found and some examples (limitations?) should be added. \n\nThe figures and text describing the TBPH/TGSG mechanism are lacking. The right part of Figure 2 is not explained at all, and FIgure 3 is technically dense and somewhat confusing.  A clearer schematic and more explanations (maybe using example) would help readers understand better. \n\nAlthough both free hand and synthetic style sketches are used, the style variation is very limited. No evidence of generalization to other categories or styles of sketch is given."}, "questions": {"value": "How robust is the method to partial sketching and to really abstract ones (like skeletons)?\n\nHow accurate are the PoseScript descriptions relative to the actual poses? Any quantitative validation?\n\nCould the language anchor be learned end-to-end (using frozen text encoders vs. trainable ones)?\n\nDoes the model support text-only inference for 3D pose generation? (Fig. 5 suggests it fails)\n\nCould the approach generalize to other artistic domains (e.g., gesture drawings or skeletons)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BcFt2Uaxc1", "forum": "vvLZYYpwtD", "replyto": "vvLZYYpwtD", "signatures": ["ICLR.cc/2026/Conference/Submission8945/Reviewer_YR7M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8945/Reviewer_YR7M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569272622, "cdate": 1761569272622, "tmdate": 1762920685191, "mdate": 1762920685191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a generative model that predicts SMPL parameters from a given input text and an input sketch. The paper demonstrates that sketch by itself is not enough for accurate pose estimation, and suggests bringing in language for better depiction of user intent. The paper is well written and clear. The methodology is easy to understand and the diagrams are clean. However, evaluation is flawed and requires more depth to comprise recent works in this domain."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "The primary strength of the paper is that it proposes a new domain for SMPL parametric models i.e. sketch+text -> 3D. This centers around hand-drawn sketches being abstract and often not enough by themselves for conveying artistic intent. Sketch+Text helps fill holes in amateur drawings and boosts performance."}, "weaknesses": {"value": "The paper proposes a neat paradigm shift but fails to rigorously demonstrate efficacy. \n- For comparisons, the paper compares against TokenHMR which is an RGB image to pose model. This implies that for all comparisons the competitors are working out of their training domain while the proposed method has been trained and is evaluated on sketches. In fact, the paper does not compare with works like Sketch2Pose, seminal in this domain. (Also tables should clarify Ours==UniAnchor)\n- The paper proposes a generalization experiment where it trains on the samples drawn by humans (9:1 train:test split). If the aim is to demonstrate generality, it is unclear why the authors train on the human-drawn set instead of simply evaluating on it? For sketches in the wild no such pre-training can be done so this hardly demonstrates a standard inference setup. \n- The paper uses sketch+text to generate SMPL parameters. However it only paints sketch side of things. Since text is a big part of this, how does it compare with ChatPose etc which can do text to pose?\n- The paper uses descriptive captions that are highly unlikely to be observed in an inference scenario. Just as abstract sketches are used as a likely input during inference, the authors should report abstract text descriptions during inference as well."}, "questions": {"value": "It is unclear whether competing methods were also fine-tuned on free-hand sketches or run out-of-the-box. Without identical training budgets, fairness is questionable. The paper's ability to gain 8 percent over SOTA might vanish upon fine tuning competitors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l2R25K4AqB", "forum": "vvLZYYpwtD", "replyto": "vvLZYYpwtD", "signatures": ["ICLR.cc/2026/Conference/Submission8945/Reviewer_37zD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8945/Reviewer_37zD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690539413, "cdate": 1761690539413, "tmdate": 1762920684467, "mdate": 1762920684467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles 3D human reconstruction from imperfect sketches, addressing the gap between synthetic and free-hand sketches with text augmentation. It proposes UniAnchor, which uses text pose descriptions as a domain-invariant anchor to guide reconstruction. The authors proposed a text-guided SketchGate to modulate sketch features using language. Experiments show improved performance and generalization to diverse free-hand sketches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea that language provides a domain‐invariant anchor across visually divergent sketch domains is interesting. \n- The TBPH with Text-guided SketchGate is a well-motivated architectural component.\n-  The experiments showed relatively good improvement over competitive baselines on both synthetic and free-hand sketches."}, "weaknesses": {"value": "- The authors haven't discussed any of the failure cases of the method.\n- I feel like sometimes the text description cannot accurately describe the pose of the human or ambiguous. How does the authors deal with that problem?\n- While the overall idea seems great, the technical contributions is relatively limited.\n- The presentation of the model architecture can be further improved. I am not clear whether both synthetic and freehand sketch can be used for training or inference?\n- The visually demonstration is very limited. I haven't seen enough cases of real-world sketch to human reconstruction.\n- Missing discussions of the related works:\n[1] Sketch2Human: Deep Human Generation With Disentangled Geometry and Appearance Constraints\n[2] DeepPortraitDrawing: Generating human body images from freehand sketches"}, "questions": {"value": "Please address weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "To4iNhG0TI", "forum": "vvLZYYpwtD", "replyto": "vvLZYYpwtD", "signatures": ["ICLR.cc/2026/Conference/Submission8945/Reviewer_jH7B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8945/Reviewer_jH7B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691701609, "cdate": 1761691701609, "tmdate": 1762920683686, "mdate": 1762920683686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles 3D human mesh reconstruction from sketches by addressing the domain gap between synthetic sketches and diverse hand-drawn ones. It leverages natural language as a domain-invariant feature anchor, fusing sketch features with text tokens to highlight semantically relevant regions while suppressing style noise. Comprehensive ablations and evaluations on the Sketch3D dataset demonstrate the effectiveness of this text-anchoring strategy and its design choices."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Language as domain anchor\n\nThe paper introduces language as a domain-invariant feature anchor, effectively suppressing sketch-style noise and improving generalization from synthetic to hand-drawn sketches. The idea is both novel and well-motivated: high-level textual cues guide the model toward pose-relevant semantics rather than stylistic artifacts. Beyond this work, leveraging language as a supervisory signal holds promise for broader human mesh recovery tasks, and this paper is likely to spur follow-up research along that direction."}, "weaknesses": {"value": "- Low level control\n\nThe language anchor guides high-level semantics (pose, part relations) but offers limited control over fine-grained geometry, e.g., precise joint angles, limb thickness, or local mesh topology. As a result, the model may smooth away subtle cues from strokes or fail to honor style-specific constraints that are important for accurate reconstruction in edge cases. Is there any plan to also add in key points as a more fine grained low level mesh control in the future?\n\n- Text quality\n\nThe model’s performance appears highly sensitive to caption quality. Without explicit 3D keypoint controls, richer, more accurate descriptions tend to yield better reconstructions, which is an intuitive but limiting dependency. In practice, however, high-quality text is hard to obtain: most real-world captions are short, noisy, and reflect the same synthetic–vs–hand-drawn gap seen in sketches. This creates annotation burden and may constrain scalability and robustness in scenarios where reliable textual supervision is scarce."}, "questions": {"value": "Overall the paper is novel and sound, but I am looking forward to get these questions answered:\n\n- How does the method recover precise joint angles and fine local geometry when text is high-level or ambiguous? Any mechanism to inject keypoint/part priors at inference time?\n- As a baseline, how does the method perform without text tokens as input?\n- How does performance degrade with noisy/short captions? Do you have robustness curves vs. caption length/noise or experiments with adversarial/misaligned text?\n- How does the model perform without text input? One example would be using the LLM generated text descriptions. This is more similar to the everyday usage.\n- What proportion of captions are human vs. synthetic (e.g., PoseScript), and what is the annotation time/cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ISp563HMJd", "forum": "vvLZYYpwtD", "replyto": "vvLZYYpwtD", "signatures": ["ICLR.cc/2026/Conference/Submission8945/Reviewer_CuEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8945/Reviewer_CuEJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135738712, "cdate": 1762135738712, "tmdate": 1762920683347, "mdate": 1762920683347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}