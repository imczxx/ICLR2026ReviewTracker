{"id": "DcEuBwrWnB", "number": 15980, "cdate": 1758258005750, "mdate": 1763759115096, "content": {"title": "Variation in Verification: Understanding Verification Dynamics in Large Language Models", "abstract": "Recent advances have shown that scaling test-time computation enables large language models (LLMs) to solve increasingly complex problems across diverse domains. One effective paradigm for test-time scaling (TTS) involves LLM generators producing multiple solution candidates, with LLM verifiers assessing the correctness of these candidates without reference answers. In this paper, we study generative verifiers, which perform verification by generating chain-of-thought (CoT) reasoning followed by a binary verdict. We systematically analyze verification dynamics across three dimensions -- problem difficulty, generator capability, and verifier generation capability -- through empirical studies on 12 benchmarks across mathematical reasoning, knowledge, and natural language reasoning tasks using 14 open-source models (2B to 72B parameter range) and GPT-4o. Our experiments reveal three key findings about verification effectiveness: (1) Easy problems allow verifiers to more reliably certify correct responses; (2) Weak generators produce errors that are easier to detect than strong generators; (3) Verification ability is generally correlated with the verifier's own problem-solving capability, but this relationship varies with problem difficulty. These findings reveal opportunities for optimizing basic verification strategies in TTS applications. First, given the same verifier, some weak generators can nearly match stronger ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B performance gap shrinks by 75.7%). Second, we identify cases where strong verifiers offer limited advantages over weak ones, as both fail to provide meaningful verification gains, suggesting that verifier scaling alone cannot overcome fundamental verification challenges.", "tldr": "We study the factors influence LLM-based generative verification, and apply findings to verifier-based test-time scaling.", "keywords": ["generative verification", "large language model", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e56f7a815221cc6868054258c0f0d65fee35569.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the factors that influence verification performance when large language models (LLMs) are used to assess the correctness of solutions generated by other LLMs. The authors conduct a systematic empirical study across three dimensions: problem difficulty, generator capability, and verifier generation capability. Using 12 benchmarks spanning mathematical reasoning, knowledge QA, and natural language reasoning, they evaluate 14 open-source models and GPT-4o in both generator and verifier roles. The study reveals three key findings: (1) verifiers more reliably certify correct responses on easier problems, (2) weak generators produce errors that are easier to detect than those from strong generators, and (3) verification ability correlates with verifier generation capability in a difficulty-dependent manner. The authors demonstrate practical implications for test-time scaling, showing that weak generators can nearly match strong generators when paired with the same verifier, and identifying regimes where strong verifiers provide limited advantages over weak ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and timely problem, as LLM-based verification is increasingly central to test-time scaling methods and the efficient deployment of language models in production systems.\n- The experimental design is comprehensive and well-controlled, systematically varying problem difficulty, generator capability, and verifier capability across 12 benchmarks and 15 models to isolate the effect of each factor.\n- The finding that generator capability primarily influences error detectability (TNR) rather than correct response recognition (TPR) is novel and provides actionable insights for model selection in verification pipelines.\n- The case studies in Figures 20 and 21 effectively illustrate the mechanisms behind the main findings, showing how problem difficulty affects TPR through reference solution quality and how generator capability affects TNR through solution consistency.\n- The practical implications for test-time scaling are clearly demonstrated, showing substantial performance gap reductions between weak and strong generators when using the same verifier (Section 5.1 and Figure 17).\n- The paper identifies regimes where strong verifiers provide minimal additional benefit over weak verifiers, which has important cost-efficiency implications for practitioners deploying verification systems at scale.\n- The difficulty-dependent correlation analysis in Section 4.3 reveals nuanced patterns (saturated, linear, threshold-limited) that prior work assuming simple linear relationships overlooked."}, "weaknesses": {"value": "- The paper lacks theoretical analysis or mechanistic explanations for the observed patterns, relying entirely on empirical observations without providing deeper understanding of why these verification dynamics emerge.\n- The definition of problem difficulty as average pass rate across generators is model-dependent and may not capture intrinsic problem characteristics, potentially confounding difficulty effects with model-specific biases.\n- The experimental scope is limited to verifiable problems with objective ground-truth answers, which excludes important domains like open-ended generation, creative writing, and subjective evaluation tasks where verification is also needed.\n- The paper does not address the computational costs of verification, which is a critical practical consideration given that verification gains may not justify the additional compute in all scenarios.\n- The analysis of reasoning models (Appendix C.2) is relegated to the appendix despite showing different TNR behavior, suggesting that the main findings may not fully generalize to this increasingly important class of models.\n- The selection of specific prompt templates, sampling hyperparameters, and verification evaluation procedures is not thoroughly ablated, leaving uncertainty about the robustness of findings to these design choices.\n- The paper identifies regimes where verification provides limited gains but does not propose actionable strategies for overcoming these fundamental limitations beyond avoiding those regimes.\n- Some experimental details are missing or unclear, such as the exact procedure for handling invalid verifier outputs, the rationale for the specific 4 correct/4 incorrect sampling strategy, and how the fallback LLM-based correctness checking affects results."}, "questions": {"value": "1. How sensitive are the main findings to the choice of prompt templates for generation and verification? Did you experiment with alternative prompting strategies, and if so, how did they affect the observed patterns?\n2. In Section 4.1, you show that problem difficulty primarily affects TPR. However, for reasoning models in Appendix C.2, TNR also increases with easier problems. Can you provide more insight into why extended reasoning changes this relationship?\n3. The definition of problem difficulty as average pass rate across generators may be influenced by the specific set of models used. How would the findings change if you used a different set of generators or defined difficulty based on human performance?\n4. For the TTS experiments in Section 5, you use K=64 samples per problem. How sensitive are the verification gains to this choice? Is there a point of diminishing returns where additional samples provide minimal benefit?\n5. The paper identifies regimes where strong verifiers provide limited additional benefit (Section 5.2). Are there alternative verification strategies (e.g., multi-verifier consensus, uncertainty-aware verification) that might overcome these limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HJUp7Ho4k0", "forum": "DcEuBwrWnB", "replyto": "DcEuBwrWnB", "signatures": ["ICLR.cc/2026/Conference/Submission15980/Reviewer_1wQz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15980/Reviewer_1wQz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760648772155, "cdate": 1760648772155, "tmdate": 1762926191080, "mdate": 1762926191080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a empirical study on the dynamics of LLM-based verification for test-time scaling. The authors investigate how problem difficulty, generator capability, and verifier generation capability influence verification performance across mathematical reasoning, knowledge, and natural language reasoning tasks. Key findings include that easy problems allow verifiers to certify correct responses more reliably, errors from weaker generators are easier to detect, and verifier capability correlates with performance in a difficulty-dependent manner that varies by problem difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts a thorough investigation across three crucial dimensions (problem difficulty, generator capability, verifier capability) and evaluates performance on a wide array of 12 benchmarks using 14 diverse LLMs.\n2. The paper details its experimental setup, metrics, model and prompts, ensuring high reproducibility.\n3. The findings provide concrete guidance for optimizing TTS applications, such as identifying scenarios where weaker generators can achieve performance comparable to stronger ones."}, "weaknesses": {"value": "1.  Despite conducting extensive analytical experiments, many conclusions remain relatively intuitive rather than providing deeper theoretical insights. For instance, the findings that \"easy problems allow verifiers to more reliably certify correct responses\" and \"weak generators produce errors that are easier to detect than strong generators\" align with common intuition but lack mechanistic explanations about why these patterns emerge.\n2. The use of pass rate as the sole difficulty metric (d(x)) may oversimplify the complex nature of problem difficulty, potentially missing important nuances in what makes problems challenging for verification."}, "questions": {"value": "1. How might your findings apply to other verifiable domains, such as coding?\n2. The paper highlights distinct non-linear correlation patterns between verifier generation capability and verification performance (e.g., saturated, linear, threshold-limited). Could the authors offer a more in-depth theoretical or empirical analysis of why these different regimes emerge across varying problem difficulties?\n3. Did you explore different verification prompting strategies? How might alternative verification frameworks affect the observed dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "exgClUAAKd", "forum": "DcEuBwrWnB", "replyto": "DcEuBwrWnB", "signatures": ["ICLR.cc/2026/Conference/Submission15980/Reviewer_QrzN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15980/Reviewer_QrzN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967110476, "cdate": 1761967110476, "tmdate": 1762926190414, "mdate": 1762926190414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents an empirical study on the verification dynamics and its implications to TTS. The analysis is centered around problem difficulty, generator and verifier capability. The main findings of this work: verifiers works better on easy problems, weak generators, and further, verification performance is affected by verifier generation capability with a different trend dependent on problem difficulty. The TTS applications inspired by these findings focus on matching the gap between weak and strong generators, and verifiers respectively, which provides guidance of effective TTS design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The results are comprehensive, including the wide range of evaluated models (instruct models and reasoning models as discussed in appendix) and discussed aspects from problem difficulty to generator/verifier capability."}, "weaknesses": {"value": "a). one main theme of the analysis is problem difficulty, but in reality, we do not know the difficulty labels beforehand, then how do we know if a question is on the difficulty extreme where verifier engineering is shown to be less effective? so its practicality on TTS applications is a bit limited in my opinion.\n\nb). efficiency analysis is lacking: is there a compute-optimal TTS approach given a task and a list of candidate generators/verifiers. i imagine something like [1] can be very helpful here.\n\nc). authors should improve task accuracy analysis, besides the verification performances, especially in the TTS applications part. so that the gap between oracle accuracy and verified accuracy can be compared and see how promising each combination is. (is this reported as pass rate in Figure 5a? not entirely follow the metric in the figure) Further, how to choose the best verifier/generator configuration given a task?\n\nd). last but not the least, this work focus on generative verifiers from a LLM-as-a-judge perspective, without fine-tuning; but prior work [2] mentioned that vanilla LLM doesn't perform well on verification. i think the authors should compare the effectiveness of the proposed approach (or the converged optimal selection as discussed above) with the SOTA verifiers beyond LLM-judge, e.g. the top ones on rewardbench.\n\n[1] https://arxiv.org/abs/2502.06703\n[2] https://arxiv.org/abs/2408.15240"}, "questions": {"value": "Q1. in Figure 2, are verifiers and generators the same? somehow i missed this information.\n\nQ2. there seems a typo in the caption of Figure 5: why are (a-c) and (d-f) mentioned?\n\nQ3. in the final paragraph, line 482, \"as weaker generators can approach the post-verification performance of stronger ones when paired with a fixed verifier.\", isn't there a specific condition, e.g. decently strong fixed verifier like gpt-4o, otherwise the weak verifier cannot close the gap between weak and strong generators?\n\nQ4. in the paragraph of line 371, \"Verifier generation capability influences verification accuracy differently based on problem difficulty\", how does the difficulty defined here? my understanding is that the questions are split based on the generation capability (i.e. x-axis), but why are there overlaps between the three difficulty groups?\n\nQ5. Figure 9 (d-f) shows a different trend of reasoning LLMs from instruct models, does the main discussion in RQ1 still hold? \n\nQ6. most evaluated models are less than 100B, why? do authors think the same trend will still hold for larger models such as deepseek V3 or Qwen-235B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aK9NyhfsDN", "forum": "DcEuBwrWnB", "replyto": "DcEuBwrWnB", "signatures": ["ICLR.cc/2026/Conference/Submission15980/Reviewer_TAxB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15980/Reviewer_TAxB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125387690, "cdate": 1762125387690, "tmdate": 1762926188893, "mdate": 1762926188893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an experimental study of generative verifiers across 15 models of diverse sizes on 12 benchmarks.\nIn their extensive evaluation, they make the following observations:\n1. Problem difficulty has an inverse correlation with the True Positive Rate (TPR) of the verifier, i.e., harder problems have a lower TPR of the verifier, while this is not the case for the True Negative Rate (TNR).\n2. The capability of the LLM in answer generation correlates positively with the error detection capability in verification, and errors made by stronger LLMs are harder to detect than those made by weaker LLMs.\n3. Verifier capability is positively correlated with verification performance, but it is non-linear for very easy and very hard problems.\n\nThe paper further makes the following observations when applying these findings to Test-Time scaling (TTS): \n\n4. A weak generator can benefit much more from a strong verifier.\n\n5. Test time scaling helps especially for medium difficulty problems, and less or not for very easy or very hard problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- overall solid experimental evaluation and setup (some limitations see below).\n- Overall very extensive study w.r.t. datasets and models.\n- A very well-structured formulation of 5 research questions and answering them experimentally."}, "weaknesses": {"value": "## Major\n1. Some of the findings are maybe unsurprising or it is difficult to benefit from the the findings in practice. E.g. Using a very strong verifier (GPT 4o) on weaker answer models seems of limited practical use.\nOr that verifiers do not provide much benefit for very easy or very difficult problems: while interesting, the paper does not examine how to reliably identify very hard and very difficult problems, but is able to group by difficulty based on ground truth, in practice one is likely confronted with easier and more difficult problems without knowing which ones are easier and which ones are more difficult.\n2. RQ4: Is it relevant to verify a weak model with a much stronger verifier? Given the cost of verification (see e.g. Singhi et al., 25), would GPT 4o (w/o verification) already be as good or better? Or just use multiple GPT 4o answerers.\n2. Related to the previous point, I miss the point GPT 4o in the x-axis in Figures 5 (a), (b).\n\n\n## Minor\n4. Most References are given as arXiv versions. If they are published, the published version should be cited."}, "questions": {"value": "1. Plot 1(b) and corresponding observation: Is the hidden variable in this plot not the problems that a generator cannot solve, and in this sense, related to the problem difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "79aav8SRIS", "forum": "DcEuBwrWnB", "replyto": "DcEuBwrWnB", "signatures": ["ICLR.cc/2026/Conference/Submission15980/Reviewer_PDQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15980/Reviewer_PDQk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162224413, "cdate": 1762162224413, "tmdate": 1762926188496, "mdate": 1762926188496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Authors’ Global Response and Summary of Revisions"}, "comment": {"value": "Thank you to all the reviewers for taking the time to read and review our work. We are glad that reviewers found our work to be \"timely\", \"comprehensive\", and \"well-structured”. We appreciate all the comments and questions, which help us improve our paper. We have uploaded a revised draft with all changes marked in red. All references to line numbers, sections, and figures in the rebuttal correspond to the revised draft. Below, we summarize the major updates.\n\n- **Difficulty estimation.** We evaluate our main findings using a practical, label-free difficulty estimator and show that all key trends remain consistent.  (Appendix D.1)\n- **Application of the verification findings to TTS efficiency.** We add an extended study showing how the identified verification dynamics can guide generator and verifier choices in TTS, together with improved efficiency analysis.  (Appendix D.2)\n- **Interpretability analysis.** We strengthen the explanations behind RQ1 and RQ2 with systematic large-scale analyses on more than 400K verification and generation traces.  (L281 Figure 3, L356 Figure 5)\n- **Robustness of the main findings.** We include ablations using a >100B model (Qwen3-235B) and additional verification prompts, confirming that the verification dynamics hold across models and prompting strategies.  (Appendix D.3, D.4)\n- **Practicality of using strong verifiers.** We add an efficiency study comparing strong models used as generators vs. verifiers, showing that verification can be more computationally efficient in realistic settings.  (Appendix D.5)\n\nWe hope this addresses the main concerns of the reviewers. Please see below for our responses to each comment."}}, "id": "4kmviyDtbB", "forum": "DcEuBwrWnB", "replyto": "DcEuBwrWnB", "signatures": ["ICLR.cc/2026/Conference/Submission15980/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15980/Authors"], "number": 17, "invitations": ["ICLR.cc/2026/Conference/Submission15980/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763770511960, "cdate": 1763770511960, "tmdate": 1763770511960, "mdate": 1763770511960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}