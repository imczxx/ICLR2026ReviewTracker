{"id": "l7Vb3yxmuz", "number": 8065, "cdate": 1758057060312, "mdate": 1759897810778, "content": {"title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference", "abstract": "The ever-increasing computational demands of large language models (LLMs) make efficient inference a central challenge. While recent advances leverage specialized architectures or selective activation, they typically require (re)training or architectural modifications, limiting their broad applicability. Training-free sparse activation, in contrast, offers a plug-and-play pathway to efficiency; however, existing methods often rely solely on hidden state magnitudes, leading to significant approximation error and performance degradation. To address this, we introduce WINA (Weight-Informed Neuron Activation): a simple framework for training-free sparse activation that incorporates both hidden state magnitudes and weight matrix structure. By also leveraging the ℓ2-norm of the model’s weight matrices, WINA yields a principled sparsification strategy with provably optimal approximation error bounds, offering better and tighter theoretical guarantees than prior state-of-the-art approaches. Overall, WINA also empirically outperforms many previous training-free methods across diverse LLM architectures and datasets: not only matching or exceeding their accuracy at comparable sparsity levels, but also sustaining performance better at more extreme sparsity levels. Together, these results position WINA as a practical, theoretically grounded, and broadly deployable solution for efficient inference. Our source code is anonymously available at https://anonymous.4open.science/r/wina-F704/README.md.", "tldr": "", "keywords": ["Sparse Activation", "Efficient Inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93fd2789e0505588dfc97dc5164b185310ec380f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a framework WINA for training-free sparse activation that incorporates both hidden state magnitudes and weight matrix structure, which combines the magnitude of activations with the column-wise norm of the weight matrices to preserve the top-k activations. The authors claimed that WINA can achieve a lower approximation error bound under several assumptions and is model-agnostic. Methods are tested on Llama-2-7B, Llama-3-8B, Mistral-7B, and Phi-4-14B models across several benchmark datasets, which demonstrate that WINA can achieve superior performance under various sparsity ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The combinatorial gating strategy is reasonable, which produces a tighter approximation error bound.\n2. WINA as a training-free method is friendly for deployment.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. There are no end-to-end latency performance comparisons between WINA and previous methods like TEAL, CATS, and R-Sparse.\n2. For the math reasoning task like GSM8K, the aggressive sparsity can induce significant damage to the model performance, dropping accuracy from 50 to 7, although WINA reported superior performance to the baseline methods.\n3. When the batch size is larger, will the sparsity be affected and further the speedup gain be degraded?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DOu8h7E4B9", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_iVmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_iVmy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884230506, "cdate": 1761884230506, "tmdate": 1762920055923, "mdate": 1762920055923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WINA (Weight-Informed Neuron Activation), a training-free sparse activation method that accelerates LLM inference by selecting neurons based on both hidden state magnitudes and weight matrix norms. This weight-informed approach achieves tighter theoretical error bounds and better accuracy than prior methods like TEAL and CATS, maintaining strong performance even at high sparsity levels across various LLMs and tasks, and showing compatibility with quantized models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow.\n\n2. The figures are clearly and beautifully presented.\n\n3. The experiments conducted on extensive datasets provide strong validation and demonstrate the integrity of the proposed method."}, "weaknesses": {"value": "1. My main concern is related to the performance measurement. The authors claim that WINA is more efficient than previous methods, \"potentially translating to faster inference speeds and lower computational costs.\" Could the authors provide empirical evidence, such as wall-clock time or GPU memory usage, to support this claim?\n\n2. The improvement in synthetic results shown in Table 2 is substantial, but the gains in real-world LLM experiments are relatively modest. Could the authors clarify the reason for this large discrepancy between synthetic and real-world results?\n\n3. How can the assumption of \"column-wise orthogonality\" in the theorems be verified? Is there any experimental evidence to support this assumption?"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JzLMG2UPB6", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_6SUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_6SUJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072107930, "cdate": 1762072107930, "tmdate": 1762920055389, "mdate": 1762920055389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve inference efficiency of LLMs and proposes skipping unnecessary neuron computations in feed-forward network (FFN) layers. The proposed method does this with a gating function that uses the magnitude of a neuron's output weight to assess its importance. Less important neurons (smaller output weight magnitue and smaller intermediate activation) are skipped. The paper has empirical results on Llama 3-8B and Qwen 2-7B, demonstrating significant speedups with minimal impact on model accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method, WINA, is easy to apply to existing pre-trained LLMs since it is a post-training method that does not require any fine-tuning or retraining. \n\n- To the best of my knowledge, assigning importance scores based on output weight's magnitude is a novel idea. \n\n- Empirical results are strong across various benchmarks and model sizes."}, "weaknesses": {"value": "- The linked source code is not available. \n\n- The success of WINA relies on the threshold used to determine which neurons to skip. Tuning this threshold would be costly."}, "questions": {"value": "Do the authors have any suggestions on how to tune the threshold efficiently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UcMZUIJTYb", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_1iPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_1iPj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155079610, "cdate": 1762155079610, "tmdate": 1762920054878, "mdate": 1762920054878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new sparse activation method, WINA, to improve LLM inference efficiency. WINA or Weight Informed Neuron Activation uses both the hidden state magnitude and weight matrix structure while sparsifying the activation; previous work only relies on the hidden magnitude. \nWINA uses the product of l2 norm of the column vector of the weight matrix and the hidden state magnitude to select the top-K neuron with theoretical justification. Extensive experiments are provided with popular benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow, and the idea is intuitive. \n\n2. Theoretical justification showing that using both the L2 norm of the column vector and the hidden state magnitude yields an optimal solution and reduces error (section 3). \n\n3. Results provided are quite extensive; the method is evaluated on multiple datasets and different downstream tasks. \n\n4. Results in tables 3 and 4 show consistent improvements, especially at higher sparsities, compared to other baselines, showing the efficacy of the method.\n\n5. Additional results showing on quantization are provided, showing WINA is compatible with quantization."}, "weaknesses": {"value": "1. The technical novelty of the method is limited (however, results show it improves over baselines). \n\n2. It is not clear why the orthogonality of the weight matrix is enforced (in sec 3.4)? Does this orthogonality hold in a general setting as well?\n\n3. Recent works have shown that LLM compression has an unintended impact on the model bias. It would be helpful to also evaluate the impact of the proposed method on model bias. \n\n\n[1]. Strubell et al., Understanding the Effect of Model Compression on Social Bias in Large Language Models"}, "questions": {"value": "1. How is LayerNorm monotonically increasing? (Line 238)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ADyOI3ABxA", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_DJxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_DJxS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156308161, "cdate": 1762156308161, "tmdate": 1762920054458, "mdate": 1762920054458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WINA (Weight Informed Neuron Activation), a training-free sparse activation method that combines hidden state magnitudes with the weight matrix structure to guide neuron selection. WINA is proven to minimize approximation error under column-wise orthogonality and monotonic activation assumptions. In the experiments, it outperforms other training-free methods like CATS, R-Sparse and TEAL across multiple LLM architectures (Llama2/3, Mistral, Phi-4) and benchmarks (MMLU, GSM8K, HumanEval), achieving over 60% FLOPs reduction at 65% sparsity while preserving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method introduces a simple yet effective training-free sparse activation mechanism that combines both hidden-state magnitudes and the column-wise L2-norm of weight matrices to guide neuron selection.\n- The theoretical analysis is rigorous and well structured, providing provably optimal approximation error bounds under clear and interpretable assumptions (column-wise orthogonality and monotonic activation).\n- The experiments are comprehensive, covering multiple model architectures, quantization methods, and ablations, demonstrating consistent improvements that align with theoretical predictions."}, "weaknesses": {"value": "- The models in the experiments are small dense LLMs. Large-scale or MoE architectures (e.g., DeepSeek-V3, Llama4, GPT-OSS) which are more common in product deployment workloads are not tested. It’s unclear whether WINA’s activation gating would maintain efficiency with expert routing sparsity in these larger models.\n- The evaluation focuses on theoretical FLOPs reduction but lacks real-world inference measurements such as latency or throughput on inference frameworks. Without kernel-level or runtime validation, the practical performance benefits of WINA remain unclear, especially given the hardware inefficiency of non-structured sparsity.\n- The theoretical assumptions rely on column-wise orthogonality and monotonic activation functions, which may not strictly satisfied in real transformer models."}, "questions": {"value": "- How does WINA perform on large MoE models (e.g., DeepSeek, Llama4, GPT-OSS)? It would help to understand how the method scales to production scale LLMs.\n- Could you evaluate WINA’s actual latency or throughput in real inference scenarios? What’s the challenges to integrate this method to inference frameworks?\n- How does WINA perform under long-context settings (e.g., 16K–128K tokens)? Are the top-K activation patterns stable as sequence length increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sMP6jmmOTK", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_SJQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_SJQR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195682710, "cdate": 1762195682710, "tmdate": 1762920053946, "mdate": 1762920053946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the important challenge of reducing inference cost in LLMs without degrading output quality. Existing training-free sparse activation methods often rely solely on hidden state magnitudes, which can lead to significant approximation error, particularly at high sparsity levels.\n\nThe authors propose WINA (Weight-Informed Neuron Activation), a simple yet effective framework that incorporates both hidden state magnitudes and the ℓ2-norm of weight matrices into neuron selection. This provides a principled sparsification strategy with provably optimal approximation error bounds, yielding tighter theoretical guarantees than prior methods.\n\nThe method is empirically validated across multiple widely used LLMs, including Llama-2-7B, Llama-3-8B, Mistral-7B, and Phi-4-14B, and evaluated on diverse tasks such as general reasoning (MMLU), mathematics (GSM8K), and coding (HumanEval). WINA is compared against several strong baselines, including TEAL, R-Sparse, and CATS. The results show that WINA performs comparably to prior methods at low sparsity and significantly better at high sparsity, achieving several percent improvement in commonsense reasoning accuracy and sustaining performance under extreme sparsity levels.\n\nOverall, WINA is presented as a practical, theoretically grounded, and broadly deployable approach for efficient inference in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem addressed is highly relevant, as reducing LLM inference cost without sacrificing output quality is an important challenge. The method is theoretically grounded, as incorporating weight norms provides a principled sparsification strategy with provable error bounds. The empirical evaluation is extensive, covering multiple LLMs, a range of tasks, and both low and high sparsity levels, and the method is compared to strong baselines including TEAL, R-Sparse, and CATS. The approach is practical and easy to deploy, as it is training-free and plug-and-play, making it broadly applicable. The results demonstrate robustness, as the method maintains competitive performance across different sparsity regimes and models."}, "weaknesses": {"value": "The main contribution is a relatively straightforward extension of existing sparse activation methods, which could be considered incremental, though it is strengthened by solid theoretical and empirical support. The paper could benefit from a discussion of potential limitations, such as scenarios where weight-informed selection might be less effective or challenges when scaling to very large models beyond those tested."}, "questions": {"value": "The authors propose incorporating weight norms into the neuron selection process. Could the authors clarify whether this additional step increases computation amount during inference, and if so, provide benchmark comparisons to quantify the overhead relative to other training-free sparse activation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "liI8CjRvSr", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_zwra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_zwra"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241039325, "cdate": 1762241039325, "tmdate": 1762920053510, "mdate": 1762920053510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}