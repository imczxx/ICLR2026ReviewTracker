{"id": "l7Vb3yxmuz", "number": 8065, "cdate": 1758057060312, "mdate": 1763618285029, "content": {"title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference", "abstract": "The ever-increasing computational demands of large language models (LLMs) make efficient inference a central challenge. While recent advances leverage specialized architectures or selective activation, they typically require (re)training or architectural modifications, limiting their broad applicability. Training-free sparse activation, in contrast, offers a plug-and-play pathway to efficiency; however, existing methods often rely solely on hidden state magnitudes, leading to significant approximation error and performance degradation. To address this, we introduce WINA (Weight-Informed Neuron Activation): a simple framework for training-free sparse activation that incorporates both hidden state magnitudes and weight matrix structure. By also leveraging the ‚Ñì2-norm of the model‚Äôs weight matrices, WINA yields a principled sparsification strategy with provably optimal approximation error bounds, offering better and tighter theoretical guarantees than prior state-of-the-art approaches. Overall, WINA also empirically outperforms many previous training-free methods across diverse LLM architectures and datasets: not only matching or exceeding their accuracy at comparable sparsity levels, but also sustaining performance better at more extreme sparsity levels. Together, these results position WINA as a practical, theoretically grounded, and broadly deployable solution for efficient inference. Our source code is anonymously available at https://anonymous.4open.science/r/wina-F704/README.md.", "tldr": "", "keywords": ["Sparse Activation", "Efficient Inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96a3a9c2f66ccfcf8060d597a36230a646a88823.pdf", "supplementary_material": "/attachment/164da2c556ca7c9a49ebf9df84a6ee89e1b3813d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed a framework WINA for training-free sparse activation that incorporates both hidden state magnitudes and weight matrix structure, which combines the magnitude of activations with the column-wise norm of the weight matrices to preserve the top-k activations. The authors claimed that WINA can achieve a lower approximation error bound under several assumptions and is model-agnostic. Methods are tested on Llama-2-7B, Llama-3-8B, Mistral-7B, and Phi-4-14B models across several benchmark datasets, which demonstrate that WINA can achieve superior performance under various sparsity ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The combinatorial gating strategy is reasonable, which produces a tighter approximation error bound.\n2. WINA as a training-free method is friendly for deployment.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. There are no end-to-end latency performance comparisons between WINA and previous methods like TEAL, CATS, and R-Sparse.\n2. For the math reasoning task like GSM8K, the aggressive sparsity can induce significant damage to the model performance, dropping accuracy from 50 to 7, although WINA reported superior performance to the baseline methods.\n3. When the batch size is larger, will the sparsity be affected and further the speedup gain be degraded?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DOu8h7E4B9", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_iVmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_iVmy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884230506, "cdate": 1761884230506, "tmdate": 1762920055923, "mdate": 1762920055923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WINA (Weight-Informed Neuron Activation), a training-free sparse activation method that accelerates LLM inference by selecting neurons based on both hidden state magnitudes and weight matrix norms. This weight-informed approach achieves tighter theoretical error bounds and better accuracy than prior methods like TEAL and CATS, maintaining strong performance even at high sparsity levels across various LLMs and tasks, and showing compatibility with quantized models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow.\n\n2. The figures are clearly and beautifully presented.\n\n3. The experiments conducted on extensive datasets provide strong validation and demonstrate the integrity of the proposed method."}, "weaknesses": {"value": "1. My main concern is related to the performance measurement. The authors claim that WINA is more efficient than previous methods, \"potentially translating to faster inference speeds and lower computational costs.\" Could the authors provide empirical evidence, such as wall-clock time or GPU memory usage, to support this claim?\n\n2. The improvement in synthetic results shown in Table 2 is substantial, but the gains in real-world LLM experiments are relatively modest. Could the authors clarify the reason for this large discrepancy between synthetic and real-world results?\n\n3. How can the assumption of \"column-wise orthogonality\" in the theorems be verified? Is there any experimental evidence to support this assumption?"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JzLMG2UPB6", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_6SUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_6SUJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072107930, "cdate": 1762072107930, "tmdate": 1762920055389, "mdate": 1762920055389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Early Response"}, "comment": {"value": "We sincerely appreciate all reviewers and ACs for the constructive reviews and insightful comments. We are glad to see the broad agreement that:\n\n- The method is simple, plug-and-play, and practically useful, (reviewer `i21q`, `zwra`, `SJQR`, `DJxS`, `1iPj`, `iVmy`).\n- The theoretical analysis is solid and improves over prior work, (reviewer `zwra`, `SJQR`, `DJxS`, `iVmy`).\n- The empirical coverage is extensive, demonstrates clear advantages at varying sparsities and is robust across architectures, tasks, and even quantized precision, (reviewer `zwra`, `SJQR`, `DJxS`, `1iPj`, `6SUJ`).\n\nBelow we address two common questions shared across reviewers. Responses to the remaining questions and the revision of the paper will be **provided in the next couple of days**.\n\n**Q1. WINA's performance over realistic wall-time or throughput.**\n> A1. We appreciate this important question. We have now implemented a **Triton-based sparse-activation kernel** and measured real inference throughput on varying size of matrix-vector multiplications. Here are two takeaways.\n> - **Negligible FLOPs complexity overhead.** Weight norms $c_j=||W_{:,j}||_2$‚Äã are **pre-computed once offline** during model loading. During inference the gating requires only a vector-wise elementwise multiplication $x \\odot c$ compared to other approaches, e.g., TEAL and CATS. Both operations are fully supported by standard inference-framework with no additional structural changes.\n> \n>   Regarding complexity, let $d$ be the hidden dimension, $B$ the batch size, and $T$ the sequence length. \n>     - A standard linear layer forward consumes $O(BTd^2)$ FLOPs. \n>     - The WINA gating mechanism requires $O(BTd)$. \n> \n>   Comparing orders, $\\frac{O(BTd)}{O(BTd^2)}=O(\\frac{1}{d})$. In modern LLMs, $d$ are typically large numbers, e.g, 2048, 4096. As a result, WINA consumes only $<0.1$% additional overhead, which is negligible especially compared with reduced FLOPs and additional performance gain. \n> - **Realistic Throughput Speedup.** With our developed **Triton kernel**, WINA delivers **measurable realistic speedup**. Early results showed that under A800 GPU, WINA achieves **almost identical realistic speedups** to TEAL. We are conducting more experiments over other GPUs, e.g., A6000 and A100, and will provide soon. \n> \n> GPU: A800, Wall-Time Unit: millisecond, Repeating Times: 10\n> \n> Matrix: 4096x11008\n>|Sparsity | WINA| TEAL| Theoretical Optimal|\n>|--|--|--|--|\n>| 0 | 0.0747 | 0.0747 | 0.0747 |\n>| 25% | 0.0583 | 0.0593 | 0.0560 |\n>| 40% | 0.0511 | 0.0511 | 0.0448 |\n>| 50% | 0.0471 | 0.0471 | 0.0373 |\n>| 65% | 0.0419 | 0.0409 | 0.0261 |\n>\n>  Matrix: 4096x14336\n> |Sparsity | WINA| TEAL| Theoretical Optimal|\n> |--|--|--|--|\n> | 0 | 0.0972 | 0.0972 | 0.0972 |\n> | 25% | 0.0706 | 0.0706 | 0.0737 |\n> | 40% | 0.0624 | 0.0614 | 0.0589 |\n> | 50% | 0.0573 | 0.0552 | 0.0491 |\n> | 65% | 0.0501 | 0.0491 | 0.0344 |\n> \n> Matrix: 5120x17920\n> |Sparsity | WINA| TEAL| Theoretical Optimal|\n> |--|--|--|--|\n> | 0 | 0.1290 | 0.1290 | 0.1290 |\n> | 25% | 0.1003 | 0.1013 | 0.0967 |\n> | 40% | 0.0870 | 0.0860 | 0.0774 |\n> | 50% | 0.0798 | 0.0778 | 0.0645 |\n> | 65% | 0.0706 | 0.0686 | 0.0451 |\n\n\n**Q2. How to enforce the column-orthogonality holds in general practice.**\n> A2. We appreciate this insightful question. In WINA, our contribution is the introduction of weighted-information gating mechanisms, while not the orthogonality transformation, which is used for the theoretical optimality proofs as Lemma 3.1, to decouple interactions so that the approximation-error decomposition becomes analytically tractable (cross terms vanish). \n> To enforce or approximate column-orthogonality on DNNs, there are two general families of techniques.\n> - One is to add orthogonality regularizer $||W^\\top W-I||$ into the loss function, representative works includes \n>\n>    [1] All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation\n> \n>    [2] Controllable Orthogonalization in Training DNNs.\n> - The other is to use matrix decomposition and transformation to enforce the column-orthogonality. We adopted the method proposed in SliceGPT [3]. As described in the remark Section 3.3, this transformation is conducted one-shot beforehand and resulting in a numerical equivalent (computational invariant) LLMs for further usages.\n> \n>    [3] SliceGPT: Compress Large Language Models by Deleting Rows and Columns.\n>\n>    This transformation is also widely adopted in efficient-model literature, including [4-7].\n> \n>    [4] QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs.\n>\n>    [5] QuiP: 2-Bit Quantization of Large Language Models with Guarantees.\n>\n>    [6] SpinQuant: LLM Quantization with learned rotations.\n>\n>    [7] OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting.\n\nThanks all again. Look forward to further discussions.\n\nYours,\n\nAuthors"}}, "id": "U0ED0lpslX", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763390610526, "cdate": 1763390610526, "tmdate": 1763421876809, "mdate": 1763421876809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely appreciate all reviewers and ACs for the constructive reviews and insightful comments. We are glad to see the broad agreement that:\n\n- The method is simple, plug-and-play, and practically useful, (reviewer `i21q`, `zwra`, `SJQR`, `DJxS`, `1iPj`, `iVmy`).\n- The theoretical analysis is solid and improves over prior work, (reviewer `zwra`, `SJQR`, `DJxS`, `iVmy`).\n- The empirical coverage is extensive, demonstrates clear advantages at varying sparsities and is robust across architectures, tasks, and even quantized precision, (reviewer `zwra`, `SJQR`, `DJxS`, `1iPj`, `6SUJ`).\n\nBelow we address two common questions shared across reviewers. Responses to the remaining questions and the revision of the paper will be **provided in the next couple of days**.\n\n**Q1. WINA's performance over realistic wall-time or throughput.**\n> A1. We appreciate this important question. We have now implemented a **Triton-based sparse-activation kernel** and measured real inference throughput on varying size of matrix-vector multiplications. Here are two takeaways.\n> - **Negligible FLOPs complexity overhead.** Weight norms $c_j=||W_{:,j}||_2$‚Äã are **pre-computed once offline** during model loading. During inference the gating requires only a vector-wise elementwise multiplication $x \\odot c$ compared to other approaches, e.g., TEAL and CATS. Both operations are fully supported by standard inference-framework with no additional structural changes.\n> \n>   Regarding complexity, let $d$ be the hidden dimension, $B$ the batch size, and $T$ the sequence length. \n>     - A standard linear layer forward consumes $O(BTd^2)$ FLOPs. \n>     - The WINA gating mechanism requires $O(BTd)$. \n> \n>   Comparing orders, $\\frac{O(BTd)}{O(BTd^2)}=O(\\frac{1}{d})$. In modern LLMs, $d$ are typically large numbers, e.g, 2048, 4096. As a result, WINA consumes only $<0.1$% additional overhead, which is negligible especially compared with reduced FLOPs and additional performance gain. \n> - **Realistic Throughput Speedup.** With our developed **Triton kernel**, WINA delivers **measurable realistic speedup**. Early results showed that under A800 GPU, WINA achieves **almost identical realistic speedups** to TEAL. We are conducting more experiments over other GPUs, e.g., A100, and will provide soon. \n> \n> GPU: A800, Wall-Time Unit: millisecond, Repeating Times: 10\n> \n> Matrix: 4096x11008\n>|Sparsity | WINA| TEAL| Theoretical Optimal|\n>|--|--|--|--|\n>| 0 | 0.0747 | 0.0747 | 0.0747 |\n>| 25% | 0.0583 | 0.0593 | 0.0560 |\n>| 40% | 0.0511 | 0.0511 | 0.0448 |\n>| 50% | 0.0471 | 0.0471 | 0.0373 |\n>| 65% | 0.0419 | 0.0409 | 0.0261 |\n>\n>  Matrix: 4096x14336\n> |Sparsity | WINA| TEAL| Theoretical Optimal|\n> |--|--|--|--|\n> | 0 | 0.0972 | 0.0972 | 0.0972 |\n> | 25% | 0.0706 | 0.0706 | 0.0737 |\n> | 40% | 0.0624 | 0.0614 | 0.0589 |\n> | 50% | 0.0573 | 0.0552 | 0.0491 |\n> | 65% | 0.0501 | 0.0491 | 0.0344 |\n> \n> Matrix: 5120x17920\n> |Sparsity | WINA| TEAL| Theoretical Optimal|\n> |--|--|--|--|\n> | 0 | 0.1290 | 0.1290 | 0.1290 |\n> | 25% | 0.1003 | 0.1013 | 0.0967 |\n> | 40% | 0.0870 | 0.0860 | 0.0774 |\n> | 50% | 0.0798 | 0.0778 | 0.0645 |\n> | 65% | 0.0706 | 0.0686 | 0.0451 |\n\n\n**Q2. How to enforce the column-orthogonality holds in general practice.**\n> A2. We appreciate this insightful question. In WINA, our contribution is the introduction of weighted-information gating mechanisms, while not the orthogonality transformation, which is used for the theoretical optimality proofs as Lemma 3.1, to decouple interactions so that the approximation-error decomposition becomes analytically tractable (cross terms vanish). \n> To enforce or approximate column-orthogonality on DNNs, there are two general families of techniques.\n> - One is to add orthogonality regularizer $||W^\\top W-I||$ into the loss function, representative works includes \n>\n>    [1] All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation\n> \n>    [2] Controllable Orthogonalization in Training DNNs.\n> - The other is to use matrix decomposition and transformation to enforce the column-orthogonality. We adopted the method proposed in SliceGPT [3]. As described in the remark Section 3.3, this transformation is conducted one-shot beforehand and resulting in a numerical equivalent (computational invariant) LLMs for further usages.\n> \n>    [3] SliceGPT: Compress Large Language Models by Deleting Rows and Columns.\n>\n>    This transformation is also widely adopted in efficient-model literature, including [4-7].\n> \n>    [4] QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs.\n>\n>    [5] QuiP: 2-Bit Quantization of Large Language Models with Guarantees.\n>\n>    [6] SpinQuant: LLM Quantization with learned rotations.\n>\n>    [7] OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting.\n\nThanks all again. Look forward to further discussions.\n\nYours,\n\nAuthors"}}, "id": "U0ED0lpslX", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763390610526, "cdate": 1763390610526, "tmdate": 1763613972276, "mdate": 1763613972276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve inference efficiency of LLMs and proposes skipping unnecessary neuron computations in feed-forward network (FFN) layers. The proposed method does this with a gating function that uses the magnitude of a neuron's output weight to assess its importance. Less important neurons (smaller output weight magnitue and smaller intermediate activation) are skipped. The paper has empirical results on Llama 3-8B and Qwen 2-7B, demonstrating significant speedups with minimal impact on model accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method, WINA, is easy to apply to existing pre-trained LLMs since it is a post-training method that does not require any fine-tuning or retraining. \n\n- To the best of my knowledge, assigning importance scores based on output weight's magnitude is a novel idea. \n\n- Empirical results are strong across various benchmarks and model sizes."}, "weaknesses": {"value": "- The linked source code is not available. \n\n- The success of WINA relies on the threshold used to determine which neurons to skip. Tuning this threshold would be costly."}, "questions": {"value": "Do the authors have any suggestions on how to tune the threshold efficiently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UcMZUIJTYb", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_1iPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_1iPj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155079610, "cdate": 1762155079610, "tmdate": 1762920054878, "mdate": 1762920054878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new sparse activation method, WINA, to improve LLM inference efficiency. WINA or Weight Informed Neuron Activation uses both the hidden state magnitude and weight matrix structure while sparsifying the activation; previous work only relies on the hidden magnitude. \nWINA uses the product of l2 norm of the column vector of the weight matrix and the hidden state magnitude to select the top-K neuron with theoretical justification. Extensive experiments are provided with popular benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow, and the idea is intuitive. \n\n2. Theoretical justification showing that using both the L2 norm of the column vector and the hidden state magnitude yields an optimal solution and reduces error (section 3). \n\n3. Results provided are quite extensive; the method is evaluated on multiple datasets and different downstream tasks. \n\n4. Results in tables 3 and 4 show consistent improvements, especially at higher sparsities, compared to other baselines, showing the efficacy of the method.\n\n5. Additional results showing on quantization are provided, showing WINA is compatible with quantization."}, "weaknesses": {"value": "1. The technical novelty of the method is limited (however, results show it improves over baselines). \n\n2. It is not clear why the orthogonality of the weight matrix is enforced (in sec 3.4)? Does this orthogonality hold in a general setting as well?\n\n3. Recent works have shown that LLM compression has an unintended impact on the model bias. It would be helpful to also evaluate the impact of the proposed method on model bias. \n\n\n[1]. Strubell et al., Understanding the Effect of Model Compression on Social Bias in Large Language Models"}, "questions": {"value": "1. How is LayerNorm monotonically increasing? (Line 238)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ADyOI3ABxA", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_DJxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_DJxS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156308161, "cdate": 1762156308161, "tmdate": 1762920054458, "mdate": 1762920054458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WINA (Weight Informed Neuron Activation), a training-free sparse activation method that combines hidden state magnitudes with the weight matrix structure to guide neuron selection. WINA is proven to minimize approximation error under column-wise orthogonality and monotonic activation assumptions. In the experiments, it outperforms other training-free methods like CATS, R-Sparse and TEAL across multiple LLM architectures (Llama2/3, Mistral, Phi-4) and benchmarks (MMLU, GSM8K, HumanEval), achieving over 60% FLOPs reduction at 65% sparsity while preserving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method introduces a simple yet effective training-free sparse activation mechanism that combines both hidden-state magnitudes and the column-wise L2-norm of weight matrices to guide neuron selection.\n- The theoretical analysis is rigorous and well structured, providing provably optimal approximation error bounds under clear and interpretable assumptions (column-wise orthogonality and monotonic activation).\n- The experiments are comprehensive, covering multiple model architectures, quantization methods, and ablations, demonstrating consistent improvements that align with theoretical predictions."}, "weaknesses": {"value": "- The models in the experiments are small dense LLMs. Large-scale or MoE architectures (e.g., DeepSeek-V3, Llama4, GPT-OSS) which are more common in product deployment workloads are not tested. It‚Äôs unclear whether WINA‚Äôs activation gating would maintain efficiency with expert routing sparsity in these larger models.\n- The evaluation focuses on theoretical FLOPs reduction but lacks real-world inference measurements such as latency or throughput on inference frameworks. Without kernel-level or runtime validation, the practical performance benefits of WINA remain unclear, especially given the hardware inefficiency of non-structured sparsity.\n- The theoretical assumptions rely on column-wise orthogonality and monotonic activation functions, which may not strictly satisfied in real transformer models."}, "questions": {"value": "- How does WINA perform on large MoE models (e.g., DeepSeek, Llama4, GPT-OSS)? It would help to understand how the method scales to production scale LLMs.\n- Could you evaluate WINA‚Äôs actual latency or throughput in real inference scenarios? What‚Äôs the challenges to integrate this method to inference frameworks?\n- How does WINA perform under long-context settings (e.g., 16K‚Äì128K tokens)? Are the top-K activation patterns stable as sequence length increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sMP6jmmOTK", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_SJQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_SJQR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195682710, "cdate": 1762195682710, "tmdate": 1762920053946, "mdate": 1762920053946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the important challenge of reducing inference cost in LLMs without degrading output quality. Existing training-free sparse activation methods often rely solely on hidden state magnitudes, which can lead to significant approximation error, particularly at high sparsity levels.\n\nThe authors propose WINA (Weight-Informed Neuron Activation), a simple yet effective framework that incorporates both hidden state magnitudes and the ‚Ñì2-norm of weight matrices into neuron selection. This provides a principled sparsification strategy with provably optimal approximation error bounds, yielding tighter theoretical guarantees than prior methods.\n\nThe method is empirically validated across multiple widely used LLMs, including Llama-2-7B, Llama-3-8B, Mistral-7B, and Phi-4-14B, and evaluated on diverse tasks such as general reasoning (MMLU), mathematics (GSM8K), and coding (HumanEval). WINA is compared against several strong baselines, including TEAL, R-Sparse, and CATS. The results show that WINA performs comparably to prior methods at low sparsity and significantly better at high sparsity, achieving several percent improvement in commonsense reasoning accuracy and sustaining performance under extreme sparsity levels.\n\nOverall, WINA is presented as a practical, theoretically grounded, and broadly deployable approach for efficient inference in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem addressed is highly relevant, as reducing LLM inference cost without sacrificing output quality is an important challenge. The method is theoretically grounded, as incorporating weight norms provides a principled sparsification strategy with provable error bounds. The empirical evaluation is extensive, covering multiple LLMs, a range of tasks, and both low and high sparsity levels, and the method is compared to strong baselines including TEAL, R-Sparse, and CATS. The approach is practical and easy to deploy, as it is training-free and plug-and-play, making it broadly applicable. The results demonstrate robustness, as the method maintains competitive performance across different sparsity regimes and models."}, "weaknesses": {"value": "The main contribution is a relatively straightforward extension of existing sparse activation methods, which could be considered incremental, though it is strengthened by solid theoretical and empirical support. The paper could benefit from a discussion of potential limitations, such as scenarios where weight-informed selection might be less effective or challenges when scaling to very large models beyond those tested."}, "questions": {"value": "The authors propose incorporating weight norms into the neuron selection process. Could the authors clarify whether this additional step increases computation amount during inference, and if so, provide benchmark comparisons to quantify the overhead relative to other training-free sparse activation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "liI8CjRvSr", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_zwra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_zwra"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241039325, "cdate": 1762241039325, "tmdate": 1762920053510, "mdate": 1762920053510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Proposes WINA, a training-free sparse activation method that gates neurons using both activation magnitude and column-wise weight ‚Ñì‚ÇÇ norms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Very simple, plug-and-play rule that is easy to implement on top of existing sparse-activation baselines."}, "weaknesses": {"value": "- The paper reports GFLOP reductions but does not clearly explain whether WINA‚Äôs gating is used to avoid weight loads or only to mask post-matmul activations; without a truly sparse kernel and latency measurements, it is unclear how much real speedup WINA provides over TEAL/CATS in memory-bound, batch-1 inference."}, "questions": {"value": "- In your current implementation, is the WINA gate used before the matmul to index only a subset of columns of ùëä , or do you compute a dense ùëäùë• and then apply the mask? If it‚Äôs the latter, how do you obtain any wall-clock speedup, especially in batch-1, memory-bound settings?\n\n- Comparison to TEAL/CATS kernels: TEAL/CATS explicitly discuss sparse kernels that reduce weight loading per token. Do you implement a comparable kernel for WINA, and can you report latency/throughput numbers vs TEAL/CATS on real hardware (A100, L40S, etc.), not just GFLOP estimates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ea4Wsc0JJT", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Reviewer_i21q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Reviewer_i21q"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763378126061, "cdate": 1763378126061, "tmdate": 1763378126061, "mdate": 1763378126061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Additional General Response"}, "comment": {"value": "We thank all the reviewers again for their insightful feedback which has improved the quality of our paper. We provide a general response here along with individual point-to-point responses to each reviewer further below. We have also uploaded a revised version of our manuscript reflecting changes and additional experiments/results wherein updated portions of the manuscript are marked in $\\color{blue}{\\text{blue}}$. \n\nBelow, we highlight three main groups of newly added experiments suggested during the rebuttal process. The latter two present newly important aspects which are typically not shown in existing sparse activation literature (i.e., effect on social bias and effect on long-context reasoning ability). \n\n- **Experiment 1. Realistic Speed-up via WINA Triton Kernel.**\n    -  Per the suggestion of multiple reviewers, we have implemented a custom WINA Triton kernel to benchmark realistic latency and speed-up performance.\n    - We have tested it over different GPUs (A800, A100, RTX PRO 6000 Blackwell), matrix-vector multiply sizes, and batch sizes.\n    - The updated results are provided below and in our revised manuscript (Section 4.4 and Appendix A.10, A. 11) and Q1/A1 in our General Response.\n    - Overall, at the same sparsity level, WINA achieves almost identical realistic speed-ups to TEAL. Consequently, at the same accuracy level, WINA gating mechanism attains higher speed-up than TEAL, highlighting the advantage of WINA‚Äôs accuracy‚Äìefficiency trade-off.\n\n- **Experiment 2. Sparse Activation on Social Bias.**\n\t- To our knowledge, existing sparse activation works do not document the effects of their respective sparse activation methods on model/social bias.\n\t- As suggested by reviewer `DJxS`, we have conducted experiments following [1] where we evaluate our WINA-sparsified LLMs on CrowS-Pairs across Gender, Race, and Religion. These results are present in our updated manuscript (Appendix A.6).\n\t- Across all architectures and sparsity levels, we observe no systematic increase in bias, instead sparse activated models often move closer to the optimal 50% neutrality target, which is consistent as the compressed models in [1]. These results suggest that WINA does not exacerbate bias and can modestly mitigate it.\n\n\t[1]. Strubell et al., Understanding the Effect of Model Compression on Social Bias in Large Language Models\n\n- **Experiment 3. Sparse Activation on Long-context Benchmark, LongBench.**\n\t- Similarly, to the best of our knowledge, assessing how well sparse activated models perform in long-context reasoning settings is typically not evaluated in this domain. \n\t- As suggested by reviewer `SJQR`, to provide insight into WINA‚Äôs effects, we adopt the widely-used LongBench benchmark to assess model performance across different sparsity levels. LongBench covers six long-context scenarios across 4,750 examples. We report results for Llama-2-7B, Llama-3-8B, Mistral-7B, and Phi-4-14B. \n\t- The results have been updated in our revised manuscript (Section 4.5 and Appendix A.5). In general, we can see that across all settings, WINA consistently outperforms TEAL in long-context scenarios.\n\nWe kindly invite reviewers to assess whether our revisions and additional experiments address their concerns. We would be grateful if ratings can be updated where appropriate. We thank the reviewers for their time, constructive feedback, and thoughtful engagement with our work.\n\nSincerely,\n\nAuthors"}}, "id": "7O8rScRVy2", "forum": "l7Vb3yxmuz", "replyto": "l7Vb3yxmuz", "signatures": ["ICLR.cc/2026/Conference/Submission8065/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8065/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission8065/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763613893651, "cdate": 1763613893651, "tmdate": 1763614647965, "mdate": 1763614647965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}