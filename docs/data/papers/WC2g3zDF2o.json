{"id": "WC2g3zDF2o", "number": 6159, "cdate": 1757955234256, "mdate": 1759897932354, "content": {"title": "Credit-Budgeted ICPC-Style Coding: When LLM Agents Must Pay for Every Decision", "abstract": "Contemporary coding-agent benchmarks applaud “first correct answer,” silently assuming infinite tokens, container minutes, and developer patience. In production, every LLM call, test re-run, and rollback incurs hard cost; agents that cannot budget these resources are dead on arrival. We close the gap with USACOArena, an ICPC-inspired arena where agents pay deterministic credits for every prompt, compilation, test, or rollback. A task becomes a cost–benefit negotiation under uncertainty: is a second sample worth 15\\% of the remaining budget, or should the agent pivot to a cheaper heuristic? Real-time deduction exposes decision profiles hidden from static leaderboards: the tax of over-specialized generators, the ROI of early-exit heuristics, and the compound interest of lightweight scaffolding. Even identically seeded agents diverge in self-play, revealing a rich policy space where the same model oscillates between spendthrift submission sprees and parsimonious exploration. Released as a reproducible benchmark and zero-shot curriculum, USACOArena provides the traces, credit engine, and six state-of-the-art decision logs to catalyze research on coding agents that know when to stop.", "tldr": "We build USACOArena, a competitive programming arena to evaluate coding agents' decision-making skills under resource constraints, revealing strategic profiles that go beyond simple code correctness.", "keywords": ["Coding Agents", "Large Language Models", "Agent Evaluation", "Interactive Environment"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14ab185fa5e67855ffb9adcac77bc52d6bd7a807.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces USACOArena, an interactive benchmark for evaluating LLM coding agents under resource constraints. The key innovation is translating the human constraint of time into an agent-native \"credit\" budget, where every action (LLM inference, testing, hints, submissions) consumes credits. Drawing from ACM-ICPC competitive programming, the arena forces agents to make strategic trade-offs rather than simply pursuing correctness. The authors evaluate leading models across four USACO contests, revealing distinct strategic profiles: Gemini-2.5-pro's aggressive exploration strategy versus GPT-5-Codex's conservative perfectionism. Self-play experiments demonstrate emergent behavioral diversity, suggesting the arena's potential as both a benchmark and training environment."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a genuine gap in current evaluation methodologies. Existing benchmarks like HumanEval and SWE-bench measure \"what\" code is produced but ignore \"how\" agents arrive at solutions. The shift to evaluating decision-making under resource constraints is conceptually valuable and well-motivated. The credit-based operationalization of resource constraints is clever—it sidesteps the unfairness of using wall-clock time for API-based models while maintaining the pressure of scarcity.\n\nThe strategic profiling in Section 4.2 provides genuine insight. The finding that Gemini-2.5-pro wins despite GPT-5-Codex having higher peak capability (max score 29 vs 19) is compelling, and the exploration-exploitation framework effectively explains this paradox. \n\nThe paper is well-written with effective visual aids. Figure 1's comparison of SWE-Bench versus USACOArena clearly communicates the knowledge gap versus capability gap distinction. The motivation is compelling, and the ACM-ICPC grounding provides solid theoretical justification for the design choices."}, "weaknesses": {"value": "I have some concern about missing ablations. The environment contains numerous hyperparameters (credit limits, penalty costs, hint prices, scoring weights) that appear somewhat arbitrary. Without systematic ablation studies, we cannot assess: Sensitivity of rankings to credit budgets (Would Gemini-2.5-pro still dominate with half or double the credit limit? Does the 20M threshold meaningfully constrain behavior, or could it be 10M or 40M without changing outcomes?), Impact of penalty structure (All penalties are uniformly 100 credits. Why not differentiate between compilation errors), Hint pricing justification, Scoring weight rationale, etc. There are a lot of design choices that needs more justification.\n\nThe paper would be substantially strengthened by at least one ablation study demonstrating that agents respond meaningfully to environmental parameters, validating that the credit system actually shapes behavior rather than being a post-hoc measurement wrapper.\nSection 3.3 mentions using the Model Context Protocol but doesn't explain why this specific protocol is necessary or superior to alternatives. What does MCP provide that a simple JSON API wouldn't? This feels like name-dropping a trendy framework without demonstrating its value.\nSection 4.4 shows that identical agents produce diverse outcomes but doesn't deeply investigate why. The trajectory analysis in Figure 5(b) is anecdotal (one match). More systematic analysis could reveal: What environmental factors cause divergence? Do certain problem orderings reliably lead to different outcomes? Can we predict which \"personality\" an agent will exhibit based on early decisions? The claim that this validates USACOArena as a \"training environment\" is unsupported—no actual training/RL experiments are conducted.\nWhile the paper reports standard deviations, there's no formal statistical testing. Are the performance differences between Gemini-2.5-pro and GPT-5-Codex statistically significant across the five runs? Pairwise comparisons with confidence intervals would strengthen claims."}, "questions": {"value": "Please address questions raised in weaknesses section. Can you provide results showing how agent rankings change under different credit limits (e.g., 10M, 20M, 40M)? Does the current limit meaningfully constrain behavior, or do agents typically finish with surplus credits? How often do agents actually use the five-tier hint system? If usage is negligible, the elaborate design may be over-engineered. If usage is significant, showing how top agents strategically employ hints would enrich the analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pgW3zGbMbp", "forum": "WC2g3zDF2o", "replyto": "WC2g3zDF2o", "signatures": ["ICLR.cc/2026/Conference/Submission6159/Reviewer_y3i4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6159/Reviewer_y3i4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829571247, "cdate": 1761829571247, "tmdate": 1762918509411, "mdate": 1762918509411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLM Agents are typically graded by correctness. However, in real-world applications, there is always an associated cost budget that needs to be respected. This is particularly true for coding agents. In this work, the authors therefore propose the USACOArena benchmark that tests coding agents in a variety of software engineering tasks under budget constraints. In contrast to existing benchmarks like SWE Bench, which tests for a knowledge gap, USACOArena tests for a capability gap. Their benchmark is built on the ACM International Collegiate Programming Contest (ICPC) and frontier models as well as strong open-weights alternatives are evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose a new software engineering benchmark that reflects the actual constraints in real software engineering benchmarks. This is achieved by translating the core human constraint of time to the equivalent agentic constraints of credit. I believe this benchmark will be useful for measuring the abilities of future agents.\n2. The benchmark is based on ACM-ICPC rules, a proven real contest for human software engineers. \n3. The benchmark architecture allows agents to interact through the standardized MCP protocol, which facilitates real-world adoption.\n4. The benchmark allows for a strategic analysis of the decision-making strategies employed by different agents (e.g., Gemini’s breadth-first vs Codex's perfectionist strategy). This is a particularly interesting component of the benchmark when compared to previous benchmarks.\n5. They compare frontier models and reveal a performance gap between proprietary models and open-weight alternatives."}, "weaknesses": {"value": "1. The main concern I have is that the benchmark seems saturated with the best agents achieving an average score of ~15 of the possible 20. Therefore, it is unclear how long the benchmark will serve as a reliable testbed for agent performance. Moreover, it is unclear whether the strong performance of proprietary frontier models stems from a train-test overlap. \n2. There is no comparison to the human baseline. This comparison would be valuable to better understand how far the current generation of models is from human experts.  \n3. There are no runtime analyses of the compared models. It would be valuable to also see the wall-clock times for solving these challenges."}, "questions": {"value": "1. How can you ensure that there is no train-test set overlap in frontier models? Is the performance gap between proprietary frontier models and open-weight alternatives due to their training data? Of course, you cannot look inside those models, but is there any way to ensure that there is no overlap? I would be interested in your thoughts. This is particularly interesting, because GPT-5 Codex performs well for USACO 2024 December, 2025 January, 2025 February, and then performance suddenly drops for the 2025 US Open. This suggests there might be some overlap.\n2. Can you clarify how saturated the benchmark already is? I.e., what does it mean to achieve an average score of 15 out of 20?\n3. How do the performances compare to the human baseline?\n4. Do you have permission to use the ACM-ICPC problem instances? I am not an expert in how such compliance topics must be handled, and I did not find information in the manuscript. Therefore, I would like to hear your approach."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The proposed benchmark uses problems from the ACM ICPC contests. I am no expert on legal compliance issues, and therefore am only flagging out of caution. I also asked the authors to explain their approach for the same reason."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gSvgjKQ5mu", "forum": "WC2g3zDF2o", "replyto": "WC2g3zDF2o", "signatures": ["ICLR.cc/2026/Conference/Submission6159/Reviewer_15eu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6159/Reviewer_15eu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990268161, "cdate": 1761990268161, "tmdate": 1762918509017, "mdate": 1762918509017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces USACOArena, an ICPC-style coding environment where LLM agents operate under a limited credit budget for each action (prompts, compilations, tests, rollbacks). This framework is used to evaluate how coding agents strategize under resource constraints, revealing distinct decision-making behaviors beyond just producing correct solutions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a realistic cost-budgeted coding challenge environment that addresses a clear gap in current LLM coding benchmarks.\n2. The study reveals novel insights into agent behavior (e.g., balancing exploratory attempts vs. conserving budget) that are not captured by conventional unlimited-attempt evaluations.\n3. The authors provide a reproducible benchmark, including code and decision logs, to support future research on resource-aware coding strategies.\n4. The experimental evaluation is thorough and compares multiple agents, highlighting differences in their decision-making under cost constraints."}, "weaknesses": {"value": "Please refer to Questions."}, "questions": {"value": "1. Could the authors clarify how the specific credit costs (for prompts, compilations, tests, etc.) were chosen? For instance, is the cost of a compilation or a test run proportional to some real-world metric, or was it tuned experimentally? It would be helpful to know if they conducted any sensitivity analysis on these cost parameters. \n2. Does the paper propose any method to improve agents' decision-making under budget constraints, or is it strictly an evaluation of existing models without optimization guidance?\n3. How well might the proposed framework extend to other types of tasks or more complex scenarios? The current benchmark uses ICPC-style problems with binary outcomes. Have the authors considered tasks with partial credit or multi-phase projects to see how the budgeting concept applies there?\n4. What metric defines an agent's success beyond final correctness (e.g., a cost-adjusted score), and how is the \"know when to stop\" behavior evaluated quantitatively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Sf0fAyWWoR", "forum": "WC2g3zDF2o", "replyto": "WC2g3zDF2o", "signatures": ["ICLR.cc/2026/Conference/Submission6159/Reviewer_hZpW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6159/Reviewer_hZpW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991514496, "cdate": 1761991514496, "tmdate": 1762918508523, "mdate": 1762918508523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces USACOArena, a competitive programming benchmark that evaluates coding agents on strategic decision-making under resource constraints rather than just code correctness. The authors translate time into a \"credit\" budget where every action costs credits. Experiments across USACO contests reveal that Gemini-2.5-pro's aggressive exploration strategy outperforms GPT-5-Codex's conservative perfectionism despite the latter having higher peak capability. Self-play experiments show behavioral diversity, suggesting the arena's potential as both evaluation testbed and training environment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a genuinely underexplored problem in coding agent evaluation. While existing benchmarks focus on correctness or long-horizon planning, this work examines resource-constrained strategic decision-making, which matters for production deployment where every API call has real cost. The credit-based abstraction is creative and well-motivated as an agent-native alternative to wall-clock time.\n\n\n\n2. The practical significance is evident. The finding that Gemini-2.5-pro wins through broader exploration despite lower per-problem accuracy challenges assumptions about optimizing for correctness. This insight could influence how we design and deploy coding agents in cost-sensitive production environments."}, "weaknesses": {"value": "1. My main concern is that this reads more like a well-executed technical report paper than a deep scientific contribution. The analysis remains largely descriptive, showing what strategies emerge without explaining why they succeed or fail. While we learn that Gemini-2.5-pro's aggressive strategy beats GPT-5-Codex's conservative approach, we don't understand when each strategy would be optimal or what problem characteristics favor exploration versus exploitation. The paper doesn't deeply analyze the actual decision-making process despite claiming this as a core contribution.\n\n2. The most significant gap is the complete absence of ablation studies. A lot of ablations are essential to understand whether the benchmark measures genuine strategic competence or just exploits specific parameter choices. Without them, we can't assess the robustness of the findings or generalize beyond the particular configuration tested.\n\n3. The benchmark relies on closed-source API models that may change or become unavailable, raising reproducibility concerns. Agent performance may be highly sensitive to prompt formulation, but there's no discussion of prompt optimization or robustness."}, "questions": {"value": "1. Is there a theoretically optimal strategy for USACOArena given the credit model and problem distribution? How do current agents compare to this theoretical benchmark? Understanding the optimum would help assess whether observed strategies are actually good or just relatively better.\n\n2. You note agents lack strategic self-assessment, with Gemini defaulting to easier problems despite being capable of harder ones. What interventions improve self-assessment? Would meta-prompting about strategy help, or do we need fundamentally different architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mLMVD5Tw82", "forum": "WC2g3zDF2o", "replyto": "WC2g3zDF2o", "signatures": ["ICLR.cc/2026/Conference/Submission6159/Reviewer_FRAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6159/Reviewer_FRAP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991769804, "cdate": 1761991769804, "tmdate": 1762918507930, "mdate": 1762918507930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}