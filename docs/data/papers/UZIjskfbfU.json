{"id": "UZIjskfbfU", "number": 4366, "cdate": 1757667472849, "mdate": 1759898036828, "content": {"title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models", "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). \nInspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data.\nTo address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. \nSpecifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1.\nTo mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on the multimodal math dataset. \nComprehensive experiments show our model achieves an average improvement of $\\sim$6\\% across various multimodal math reasoning benchmarks using only a 10K multimodal math data during RL training. \nVision-R1-7B achieves a 73.5\\% accuracy on the widely used MathVista benchmark, which is only 0.4\\% lower than the leading reasoning model, OpenAI O1.\nScaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4\\% and 78.2\\% MathVista benchmark scores, respectively.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bb2d02dba932707a9d57d95b6b076940e9862ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ​​Vision-R1​​, a method to enhance reasoning in ​​Multimodal Large Language Models (MLLMs)​​ by combining ​​cold-start initialization​​ with ​​Reinforcement Learning (RL)​​. \n\nThis paper proposes a pipeline to build high-quality multimodal CoT datasets without human annotation, using MLLMs and DeepSeek-R1 and uses this pipeline to build Vision-R1-cold (200K samples)​​. \n\n​​This paper proposed Progressive Thinking Suppression Training (PTST)​​ to mitigate ​​overthinking​​ in RL training by gradually increasing reasoning complexity and analyzes differences between direct RL training and the combined approach of coldstart initialization and RL training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "​1. ​High-quality dataset construction:​​ The ​​Vision-R1-cold dataset​​ leverages ​​modality bridging​​ to generate ​​human-like CoT reasoning​​ without manual annotation, addressing a critical bottleneck.\n\n\n2. ​​Strong performance:​​ Vision-R1-7B ​​matches or surpasses larger models​​ (e.g., Qwen2.5-VL-72B) on ​​MathVista, MathVerse, and MM-Math​​, demonstrating efficiency and scalability."}, "weaknesses": {"value": "1. The novelty is limited. The article is more engineering-oriented, and the overall innovation of the work is incrimental.\n\n2. The argument for the PTST method's effectiveness in mitigating the Overthinking problem is insufficient. The paper uses the results of Vision-R1-Long to reveal the Overthinking problem and compares it with Vision-R1 (w/ PTST) to demonstrate that PTST alleviates the issue. However, the length limit for Vision-R1-Long is 16K, while Vision-R1 (w/ PTST) has limits of 4K and 8K in its two stages. If the length limit for Vision-R1-Long were 8K (or 4K), would the Overthinking problem still exist? How would the comparison with Vision-R1 (w/ PTST) look then?  \n\n3. The high-quality dataset in the Cold Start phase is one of the paper's key contributions, yet the necessity of Cold Start is not sufficiently justified. The proposed method consists of two phases (see Table 4): Cold Start and GRPO (w/ PTST). The paper does not seem to conduct ablation experiments on Cold Start. Specifically, in Table 4, Vision-R1-Zero performs better than Vision-R1-Long, which undergoes Cold Start in addition to Vision-R1-Zero. Does this imply that skipping Cold Start is better? I believe that conducting an experiment with GRPO (w/ PTST) (without Cold Start) and comparing it with the existing Vision-R1 would validate the necessity of Cold Start.  \n\n4. The paper claims that one of its contributions is analyzing the difference between direct RL training and the combined approach of Cold Start initialization and RL training. The former corresponds to the Vision-R1-Zero model in the paper, but the training configuration (methods, parameters, etc.) for Vision-R1-Zero is not detailed. Additionally, the paper compares Vision-R1-Zero with Vision-R1-CI, but their training data differs. I think a more meaningful comparison would involve training Vision-R1-Zero with the same data as Vision-R1-CI before contrasting them.  \n\n5. The ablation experiments for PTST are insufficient. PTST primarily involves two settings: gradually increasing the sequence length limit $L_s$ and gradually decreasing the sampling count $G_s$. However, the paper does not conduct ablation experiments for either setting. For example, why should the sampling count $G_s$ gradually decrease? What would happen if it remained constant or gradually increased?"}, "questions": {"value": "Seeing weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FPWvjOiG8y", "forum": "UZIjskfbfU", "replyto": "UZIjskfbfU", "signatures": ["ICLR.cc/2026/Conference/Submission4366/Reviewer_Lj3M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4366/Reviewer_Lj3M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537066121, "cdate": 1761537066121, "tmdate": 1763004488867, "mdate": 1763004488867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training paradigm aimed at enhancing the reasoning capabilities of Multimodal Large Language Models. The authors first construct a 200K multimodal CoT dataset, named Vision-R1-cold, without requiring manual human annotation. They then propose a Progressive Thinking Suppression Training strategy, which progressively relaxes the upper limit on reasoning length during RL training. The method demonstrates performance improvements on several multimodal math reasoning benchmarks, including MathVista and MathVerse."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's \"Modality Bridging\" method is a strength. It uses an existing MLLM to transform visual information into detailed textual descriptions. This enables a powerful text-only model (DeepSeek-R1) to generate high-quality, human-like CoT for multimodal data. \n2. The models achieve competitive performance on benchmarks like MathVista.\n3. The paper is clearly written, and the figures generally present the concepts effectively."}, "weaknesses": {"value": "1. The Vision-R1-cold dataset is constructed by filtering samples from various sources, including LLaVA-CoT and Mulberry. The paper does not provide a clear analysis to rule out potential data leakage or overlap between these source datasets and the evaluation benchmarks (e.g., MathVista, MathVerse). This lack of a contamination check casts some doubt on the true generalization performance.\n\n2. The definition of \"Pseudo-CoT\" is perplexing. The authors initially introduce it as a negative, \"formatted\" reasoning style lacking true cognition. However, they then use this \"Pseudo-CoT\" as a critical component in their own data pipeline (Figure 2) to generate descriptions. This leads to a critical missing ablation study: What is the performance if one simply uses the \"Pseudo-CoT\" data directly for cold-start initialization? The paper does not compare SFT with the \"Pseudo-CoT\" data. This experiment is essential to prove that the complex, DeepSeek-R1-generated CoT is superior and that the complex data pipeline is necessary. Finally, this paper has a coupling problem: The performance gain from the cold-start data might come from two sources: (1) the data source selection process or (2) the complex CoT style. The paper fails to decouple these effects.\n\n3. The proposed Progressive Thinking Suppression Training is presented as a new strategy, but its technical contribution is limited.\n\n4. The paper successfully identifies and names the \"Overthinking Optimization Problem\", but the analysis of its root cause is shallow. The authors state that after SFT on complex CoT, correct answers are concentrated in shorter sequences. Why does this happen? Is the model merely imitating the complex style of CoT without grasping the reasoning? Or is this problem caused by the data itself or the SFT training dynamic? Crucially, would this \"overthinking\" problem still occur if the (simpler) \"Pseudo-CoT\" data were used for cold-start instead? The paper fails to investigate these critical questions.\n\n5. The \"Modality Bridging\" pipeline (MLLM -> Caption -> DeepSeek-R1) has a potential fundamental flaw. An MLLM's ability to \"caption\" an image with perfect, fine-grained detail is limited. The authors' method attempts to guide this by feeding the MLLM a \"Pseudo-CoT\" that already contains reasoning steps. This process, which is inherently \"answer-directed,\" could introduce a bias or lead to a feedback loop of hallucinations, where the MLLM generates a caption that justifies the answer rather than describing the image objectively. The paper does not analyze or mitigate this risk."}, "questions": {"value": "Same as the content in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M7bcyZQBMN", "forum": "UZIjskfbfU", "replyto": "UZIjskfbfU", "signatures": ["ICLR.cc/2026/Conference/Submission4366/Reviewer_XezA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4366/Reviewer_XezA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548032988, "cdate": 1761548032988, "tmdate": 1762917318369, "mdate": 1762917318369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Vision-R1, a multimodal large language model (MLLM) designed to enhance reasoning capabilities through reinforcement learning (RL). The model addresses key challenges in incentivizing complex reasoning processes in MLLMs, particularly when dealing with multimodal tasks. Vision-R1 utilizes a novel cold-start initialization approach and Progressive Thinking Suppression Training (PTST) to improve its reasoning abilities. The model outperforms other MLLMs in multimodal math reasoning benchmarks, even with smaller model sizes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Vision-R1 shows competitive results on challenging multimodal benchmarks, with a notable improvement in performance compared to existing models."}, "weaknesses": {"value": "1. Novelty:\nWhile the idea of combining cold-start initialization with RL in MLLMs is presented as a novel approach, it lacks sufficient justification for why this method is fundamentally different from existing methods. The literature already includes several studies that employ RL for reasoning in LLMs (e.g., DeepSeek-R1, Qwen2.5-VL), and this paper does not convincingly demonstrate that the proposed approach significantly advances the field beyond these existing methods. The claim of novelty feels overstated given the incremental nature of the proposed solution.\n\n2. Lack of Comparative Depth:\nAlthough the paper compares Vision-R1 with several state-of-the-art (SoTA) models, the comparison lacks depth in terms of experimental conditions and detailed analyses. For instance, the paper does not provide comprehensive insights into the limitations of the existing approaches it claims to improve upon. A more detailed ablation study that explicitly examines the trade-offs between different design choices (e.g., cold-start initialization vs. direct RL) would strengthen the claims.\n\n3. Evaluation Metrics:\nThe performance improvement reported (such as a 6% average improvement) is promising but not compelling enough. The results are not sufficiently backed by real-world applications or a broader range of benchmarks that would demonstrate the practical utility of Vision-R1 in diverse settings. There should be more emphasis on generalizability and long-term robustness, particularly in multimodal settings.\n\n4. Over-Emphasis on Technical Complexity:\nThe method includes complex steps like Progressive Thinking Suppression and Modality Bridging, which are essential for improving performance. However, the paper does not sufficiently explore the potential practical difficulties in implementing these methods at scale. A clearer discussion of how these methods can be generalized or optimized for deployment in real-world scenarios is missing."}, "questions": {"value": "Refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DOZoIJDPX2", "forum": "UZIjskfbfU", "replyto": "UZIjskfbfU", "signatures": ["ICLR.cc/2026/Conference/Submission4366/Reviewer_RiY1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4366/Reviewer_RiY1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737677865, "cdate": 1761737677865, "tmdate": 1762917318054, "mdate": 1762917318054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new reasoning model Vision-R1. The authors recognize that RL alone struggles to activate complex reasoning in MLLMs due to insufficient multimodal data, so they propose a novel approach using Progressive Thinking Suppression Training along with RL to refine reasoning processes gradually.The paper also introduces a new 200K multimodal CoT dataset without human annotations to serve as the Cold-start Initialization of Vision-R1 models. Experiments shows Vision-R1 gain better results than previous models with similar size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper proposes a novel way to generate large dataset without human annotations, which is possible to become a scalable method for dataset creation.\n2. Sufficient ablation studies furtther demonstrate how the training parameters in PTST are chosen, and the experiments results shows the effectiveness of propoesed dataset and training strategy."}, "weaknesses": {"value": "1. All the experiments are carried out on math-related dataset (e.g., MathVista, MathVerse). Cross-domain dataset could be included to show the model's power of generalization.\n2. While effective, the cold-start process might be limited by the quality and scale of the initial dataset."}, "questions": {"value": "1. Could the model be tested on additional reasoning tasks on other domains to assess its generalization capabilities? For example, MMMU and MMMU-pro dataset.\n2. Will the two-stage PTST increase the overall computational overheads? Some statistics about training devices and time may demonstrate the training process more clearly.\n3. In tab.5, a three stage PTST layout is included, but it adds a middle stage between original stage 1 and 2. I'd like to know whether adding an additional stage after origin stage 2, like 16K*4, can improve the model's final performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9EIZfImjLT", "forum": "UZIjskfbfU", "replyto": "UZIjskfbfU", "signatures": ["ICLR.cc/2026/Conference/Submission4366/Reviewer_i3eD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4366/Reviewer_i3eD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762192342730, "cdate": 1762192342730, "tmdate": 1762917317775, "mdate": 1762917317775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}