{"id": "7pQv7qitFV", "number": 23673, "cdate": 1758347008020, "mdate": 1762526187791, "content": {"title": "MicroVerse: A Preliminary Exploration Toward a Micro-World Simulation", "abstract": "Recent advances in video generation have opened new avenues for macroscopic simulation of complex dynamic systems, but their application to microscopic phenomena remains largely unexplored. Microscale simulation holds great promise for biomedical applications such as drug discovery, organ-on-chip systems, and disease mechanism studies, while also showing potential in education and interactive visualization. In this work, we introduce **MicroWorldBench**, a multi-level rubric-based benchmark for microscale simulation tasks. MicroWorldBench enables systematic, rubric-based evaluation through 459 unique expert-annotated criteria spanning multiple microscale simulation task (e.g., organ-level processes, cellular dynamics, and subcellular molecular interactions) and evaluation dimensions (e.g., scientific fidelity, visual quality, instruction following). MicroWorldBench reveals that current SOTA video generation models fail in microscale simulation, showing violations of physical laws, temporal inconsistency, and misalignment with expert criteria.\nTo address these limitations, we construct **MicroSim-10K**, a high-quality, physically-grounded simulation dataset built with expert verification. Leveraging this dataset, we train **MicroVerse**, a video generation model tailored for microscale simulation. MicroVerse can accurately reproduce complex microscale phenomena. Our work first introduce the concept of **Micro-World Simulation** and present a **proof of concept**, paving the way for applications in biology, education, and scientific visualization.", "tldr": "", "keywords": ["Microscale Simulation", "Video Generation", "Benchmark", "Text-to-Video Dataset"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a5103b9331a3248d347bda94565a8f8d785f2b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a video generation benchmark, MicroWorldBench, and a video generation model, MicroVerse, to explore how well video generative models perform on microscale simulation tasks. MicroWorldBench is a rubric-based benchmark with 459 tasks spanning organ, cellular, and subcellular levels. The authors also construct MicroSim-10K, a dataset of 9,601 expert-verified microscale simulation videos, and fine-tune Wan2.1 on MicroSim-10K to obtain the model MicroVerse. MicroVerse shows some improvement in scientific fidelity over the base model, though with decreased visual quality and instruction-following capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel application domain. Applying video generation to microscale simulation is unexplored and potentially impactful for education and biomedical research.\n\n- Comprehensive benchmark design. The rubric-based evaluation with expert-curated criteria across three hierarchical levels (organ/cellular/subcellular) is well-motivated and systematic.\n\n- Dataset construction effort. Building MicroSim-10K with multiple filtering stages and expert verification demonstrates thoroughness in data curation."}, "weaknesses": {"value": "- Single-domain dataset that may not represent the full distribution and may have data contamination issues. Both the training set (MicroSim-10K) and test set (MicroWorldBench) are built entirely from YouTube videos, which may not reflect the full range of scientific simulation requirements. The authors have not reported how they deduplicate to ensure the test set is fully separate from the training set. Additionally, there is a concern that private models (like Veo3) may have already been trained on all YouTube videos.\n\n- Evaluation metrics may not capture the biological/physical processes. For the test set, MicroWorldBench, the rubrics were first generated by GPT-5 and then verified by humans. While these human-in-the-loop metrics might be useful, GPT-5 is unlikely to capture the biological/physical processes in the video, and thus important metrics related to these are likely missing.\n\n- Missing ablations. No ablation studies on how dataset filtering stages, dataset size, or training recipes (CFG rate, steps, frames) affect the final performance."}, "questions": {"value": "See weaknesses.\n\nTypos: Figure 1 \"Mircoscale\" -> \"Microscale\"; Figure 3 \"Boraders\" -> \"Borders\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "12pQlfvlCb", "forum": "7pQv7qitFV", "replyto": "7pQv7qitFV", "signatures": ["ICLR.cc/2026/Conference/Submission23673/Reviewer_9ESj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23673/Reviewer_9ESj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696534310, "cdate": 1761696534310, "tmdate": 1762942756336, "mdate": 1762942756336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MicroVerse, a preliminary exploration toward micro-world simulation, aiming to extend video generation models to capture biologically and physically meaningful microscale dynamics (e.g., organ-, cellular-, and subcellular-level processes). The authors propose MicroWorldBench, a rubric-based benchmark with 459 expert-annotated tasks for evaluating microscale simulations, and build MicroSim-10K, a large-scale dataset of 9.6K physics-grounded video clips. Based on this dataset, they fine-tune Wan2.1 to obtain MicroVerse, which reportedly improves scientific fidelity compared to existing open-source video generation models. The paper highlights a new research direction and provides valuable resources for future study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper opens an underexplored and impactful direction — using video generation models for microscale (biomedical) simulation, which connects generative AI with scientific and educational applications.\n\n- MicroWorldBench and MicroSim-10K are well-constructed and carefully validated, providing a foundation for future research in scientific video generation.\n\n-  The rubric-based evaluation (Scientific Fidelity, Visual Quality, Instruction Following) combined with expert refinement offers a transparent and interpretable assessment framework.\n\n-  The paper is well-written and conceptually coherent, linking macroscopic “world models” with microscopic phenomena convincingly."}, "weaknesses": {"value": "- MicroVerse is essentially a fine-tuned version of Wan2.1 without introducing new architectural or physical modeling components. The improvement in scientific fidelity (+0.8 overall) is marginal.\n\n- While the benchmark and dataset are significant contributions, the proposed model does not clearly outperform commercial systems or strong open-source baselines, suggesting that the key contribution lies in data curation rather than modeling innovation.\n\n- The scoring process relies heavily on GPT-5 grading, which may introduce subjectivity or bias. The human validation is limited in scale and lacks quantitative rigor.\n\n- Despite the “physics-grounded” claim, no explicit physical constraints or differentiable physical priors are used in the model training, which limits its scientific credibility."}, "questions": {"value": "- How scalable is the rubric-based evaluation if new tasks or domains are added? Would each require renewed expert verification?\n- Is there any plan to integrate explicit physical priors (e.g., fluid or diffusion equations) to improve scientific fidelity?\n- How consistent are GPT-5 rubric scores compared to human experts across diverse biological processes?\n- Can the authors clarify how much of the performance improvement is due to dataset quality versus model fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iC3bpID31l", "forum": "7pQv7qitFV", "replyto": "7pQv7qitFV", "signatures": ["ICLR.cc/2026/Conference/Submission23673/Reviewer_qfnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23673/Reviewer_qfnB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813830286, "cdate": 1761813830286, "tmdate": 1762942755865, "mdate": 1762942755865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MicroWorldBench, a rubric-based benchmark designed to evaluate microscale simulation tasks across multiple biological and physical scales, including organ-level dynamics, cellular processes, and molecular interactions. The benchmark includes 459 expert-annotated evaluation criteria covering dimensions such as scientific accuracy, temporal consistency, visual quality, and instruction following. The authors show that current state-of-the-art video generation models perform poorly on these tasks, exhibiting physical law violations and biologically inaccurate behavior. To address this gap, they create MicroSim10K, an expert-verified dataset of physically grounded microscale simulations, and train MicroVerse, a video generation model specialized for this domain. MicroVerse demonstrates improved fidelity in reproducing complex microscale phenomena. Overall, the work introduces the concept of \"micro-world simulation\" and presents an initial benchmark, dataset, and model aimed at advancing scientific and biological video simulation capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This work creates a curated microscale simulation dataset (MicroSim-10K) with expert verification, addressing lack of domain-specific training data.\n\nPresents a fine-tuned model (MicroVerse) that improves fidelity and consistency on the proposed benchmark, showing feasibility of domain adaptation."}, "weaknesses": {"value": "By looking at Figure 1, my primary concern is that the simulations themselves do not appear realistic (as these simulations look they were made for educational purposes). In some cases, the model-generated outputs look more realistic than the ground-truth simulations (e.g., the SORA cell-division example). This raises questions about whether the benchmark’s “expert-verified” reference simulations accurately reflect real **biological phenomena** and whether the evaluation metric truly measures **biological fidelity** rather than stylistic similarity (because again, these are not realistic life cell imaging videos, rather educational style videos).\n\nI believe this work could still be framed as generating educational biological simulations or illustrative biology videos, but claiming true biological fidelity is not supported by the examples shown (anyone with basic wetlab experience can discern that these videos are not realistic). The current outputs do not convincingly reflect real biological processes, making it difficult to justify the biological realism claims.\n\nGiven these facts  (and that I don't think it is possible to make a single claim of biological fidelity) I recommend either expanding this dataset to include real biological videos or restructuring the whole paper."}, "questions": {"value": "Please address this main concern above ^"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bGXTU958Wa", "forum": "7pQv7qitFV", "replyto": "7pQv7qitFV", "signatures": ["ICLR.cc/2026/Conference/Submission23673/Reviewer_Snf5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23673/Reviewer_Snf5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882653271, "cdate": 1761882653271, "tmdate": 1762942755644, "mdate": 1762942755644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}