{"id": "74aPGu5etg", "number": 3344, "cdate": 1757405492995, "mdate": 1759898094626, "content": {"title": "Follow-the-Perturbed-Leader for Decoupled Bandits: Best-of-Both-Worlds and Practicality", "abstract": "We study the decoupled multi-armed bandit (MAB) problem, where the learner selects one arm for exploration and one arm for exploitation in each round. The loss of the explored arm is observed but not counted, while the loss of the exploited arm is incurred without being observed. We propose a policy within the Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations. Our policy achieves (near-)optimal regret regardless of the environment, i.e., Best-of-Both-Worlds (BOBW): constant regret in the stochastic regime, improving upon the optimal bound of the standard MABs, and minimax optimal regret in the adversarial regime. Moreover, the practicality of our policy stems from avoiding both the convex optimization step required by the previous BOBW policy, Decoupled-Tsallis-INF (Rouyer & Seldin, 2020), and the resampling step that is typically necessary in FTPL. Consequently, it achieves substantial computational improvement, about $20$ times faster than Decoupled-Tsallis-INF, while also demonstrating better empirical performance in both regimes. Finally, we empirically show that our approach outperforms a pure exploration policy, and that naively combining a pure exploration with a standard exploitation policy is suboptimal.", "tldr": "We propose a practically efficient FTPL policy for decoupled MAB that achieves BOBW without convex optimization or resampling.", "keywords": ["Follow-the-Perturbed-Leader", "Decoupled exploration and exploitation", "Best-of-Both-Worlds"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9edb8822808adcf477a39ba248f12869db8e62ac.pdf", "supplementary_material": "/attachment/deb3582f5cfc0cd3192dacf3abcbb0fab6fab955.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the decoupled MAB problem, where an action is used for exploration while and another one is used to exploit. The authors show that a particular instantiation of the FTPL algorithm may attain best-of-both-worlds guarantees while being computationally more efficient than the state-of-the-art best-of-both-worlds algorithm for the same setting, which instead is based on FTRL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The techniques employed in the work are interesting and the results seem both rigorous and novel. Overall, I believe that the results are well presented. Specifically, the authors did a good job in providing a precise overview of the state-of-the-arts techniques for the problem studied. Moreover, the authors put a lot of effort into explaining the main idea behind the proofs and the techniques employed. On the technical side, I believe this is a good paper."}, "weaknesses": {"value": "The main weakness is the significance of the results obtained. Indeed, this paper focuses on a really specific topic, that is, uncoupled MAB, in which the bounds attained by FTPL do not improve the state-of-the-arts (and optimal) ones. Thus, to me, the contribution mainly lies in avoiding the convex optimization step of FTRL which I believe is not enough to meet the acceptance bar."}, "questions": {"value": "Can the authors elaborate on the contributions of the works? I am glad to increase my score if the answers turn out to be pretty positive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PnW8y9yfLD", "forum": "74aPGu5etg", "replyto": "74aPGu5etg", "signatures": ["ICLR.cc/2026/Conference/Submission3344/Reviewer_tnbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3344/Reviewer_tnbB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760708154596, "cdate": 1760708154596, "tmdate": 1762916680442, "mdate": 1762916680442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, authors study the best-of-both-worlds problem for decoupled multi-armed bandits under the Follow-the-Perturbed-Leader (FTPL) framework. Compared with previous FTRL-based algorithms which require solving a convex optimization problem at each round, the proposed algorithm is computationally efficient and obtains near-optimal results in both stochastic and adversarial regimes. Authors also conduct experiments to show that the proposed algorithm indeed outperforms Decoupled-Tsallis-INF algorithm in terms of both regret and running time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is known as the first to study the best-of-both-worlds problem for decoupled multi-armed bandits under the Follow-the-Perturbed-Leader (FTPL) framework. The studied problem is well-motivated, given the computational issue of FTRL-based algorithms.\n\n- The proposed algorithm is simple and intuitive. The algorithm adopts Pareto perturbations, used in previous FTPL work, to the decoupled bandit problem.\n\n- Authors also empirically show the superiority in some experiments in terms of the regret performance and running time."}, "weaknesses": {"value": "I have two major concerns.\n\n- In this paper, the regret bound in the stochastic setting is $O(\\sqrt{\\frac{K}{\\Delta_{\\min} } \\sum_{i \\neq i^\\*} \\frac{1}{\\Delta_i} } +\\frac{K}{\\Delta_{\\min}})$, which is worse than the best-known result $O( \\sqrt{  \\sum_{i \\neq i^\\* } \\frac{1}{\\Delta_i^2} } +K )$ by [*]. For example, if the dominant term is $1/\\Delta_{\\min}$ (i.e., $\\Delta_{\\min}$ is sufficiently small) and all other arm gaps are constant level, say $\\Delta_i=0.5$. In this case, the bound in this paper is dominated by $O(\\frac{K}{\\Delta_{\\min}})$, but their bound is $O(\\frac{1}{\\Delta_{\\min}})$. In other words, their bound is $K$ times smaller than the bound in this paper.\n\n- The technical contribution is limited. For example, Pareto perturbation or its generalization has been widely applied in FTPL literature. While this paper is the first to use it in the decoupled multi-armed bandit problem, the techniques mostly follow previous work. The key difference that the proposed algorithm needs not to do geometric resampling is attributed to the decoupled setting, in which the algorithm can sample and weight by a self‑chosen probability distribution.\n\n[*] Tiancheng Jin, Junyan Liu, and Haipeng Luo. Improved Best-of-Both-Worlds guarantees for multiarmed\nbandits: FTRL with general regularizers and multiple optimal arms."}, "questions": {"value": "Do authors believe that arm-dependent learning rates will improve the regret also for FTPL-based algorithms? If so, can authors provide a short and intuitive discussion on which part in your current analysis can be refined via using arm-dependent learning rates."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IPE5vEtBsy", "forum": "74aPGu5etg", "replyto": "74aPGu5etg", "signatures": ["ICLR.cc/2026/Conference/Submission3344/Reviewer_XB6E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3344/Reviewer_XB6E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760858192162, "cdate": 1760858192162, "tmdate": 1762916680261, "mdate": 1762916680261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the decoupled bandit problem, a relatively less-studied topic within the bandit literature that possesses non-trivial application value. Specifically, the authors propose a policy based on the Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations. The paper presents theoretical results for two cases: stochastic and adversarial environments. The new algorithm offers practical value, as it does not require solving optimization problems, unlike other methods. It also provides theoretical contributions, such as achieving a worst-case optimal regret bound in the adversarial setting. Experiments are provided to demonstrate the algorithm's effectiveness, though they utilize a limited choice of baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a comprehensive comparison with prior work and a sufficient background for the problem.\n2. The algorithm demonstrates non-trivial improvements over existing methods."}, "weaknesses": {"value": "1. The paper could benefit from adhering more strictly to writing conventions; notations are frequently used before they are defined (e.g., $w_t$ on line 136). It is strongly recommended to define mathematical notations before or as they are introduced in the same sentence to improve readability.\n2. The analysis for the adversarial environment relies on a strong constraint, assuming that the gaps are constant."}, "questions": {"value": "1. The authors comment that their problem is different from a pure exploration problem. Typically, an anytime pure exploration framework can be viewed as a process at each step involving: (1) selecting an arm, (2) pulling the arm and receiving a reward, and (3) recommending the arm currently inferred to be best. Note that the recommended arm (3) is not necessarily the same as the pulled arm (1), and it is only recommended, not pulled. I wonder what the precise difference is between this pure exploration setup and the problem addressed in this paper if we pull the arm in (3).\n\n2. Could the authors compare the performance of their proposed algorithm with methods from the pure exploration, such as the Sequential Halving algorithm presented in \"Revisiting simple regret: Fast rates for returning a good arm\" (ICML 2023)? which is anytime. \n\n3. Regarding the baseline EB-TC, how is the exploitation arm chosen? Is it the same as the arm that is sampled? This needs clarification.\n\n4. It seems the confidence level for the experimental results are missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gtMmth1Yr9", "forum": "74aPGu5etg", "replyto": "74aPGu5etg", "signatures": ["ICLR.cc/2026/Conference/Submission3344/Reviewer_VjFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3344/Reviewer_VjFH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546978454, "cdate": 1761546978454, "tmdate": 1762916680104, "mdate": 1762916680104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the decoupled bandit problem and proposes a computational efficient BOBW algorithm via FTPL, where the computational complexity improves by a $K$ factor."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing of this paper is clear and easy to follow.\n2. The proposed algorithm enjoys an efficient computation complexity, while still achieving the best-of-both-world guarantee."}, "weaknesses": {"value": "1. The primary concern of the reviewer is that the contribution and novelty of this paper are too thin to be accepted. Only one computational improvement is proposed, and the design and analysis of the FTPL technique largely follow prior works. The reviewer suggests that the authors consider extending their FTPL technique to a broader range of problems, especially more challenging setups, e.g., combinatorial bandits, rather than the simplified MAB setup in the current paper. \n2. Several places in this paper are unclear:\n    - The “$20$ times faster” claim is not clear.  Does that mean computational complexity, or just from the empirical experiments?\n    - The “adaptive adversary” in Line 128 is an unclear term. It has two possibilities: strong (in response to current and past actions) and medium (only in response to past actions).\n    - What is $\\sigma_{t,i}$ in Eq.(7)?"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m2vVHgrOey", "forum": "74aPGu5etg", "replyto": "74aPGu5etg", "signatures": ["ICLR.cc/2026/Conference/Submission3344/Reviewer_x6H3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3344/Reviewer_x6H3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027893244, "cdate": 1762027893244, "tmdate": 1762916679922, "mdate": 1762916679922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}