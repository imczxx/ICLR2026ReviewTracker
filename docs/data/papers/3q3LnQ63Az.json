{"id": "3q3LnQ63Az", "number": 24950, "cdate": 1758362320749, "mdate": 1759896740956, "content": {"title": "Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion", "abstract": "High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following.\nThis paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in", "tldr": "", "keywords": ["Fine-grained Audio Caption Dataset", "Large Audio Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c07a0b06e9c11859cb6dec4a8c43cf44ea2d7603.pdf", "supplementary_material": "/attachment/f7575694c9eb18c24adc3273e37551d9f0a8c69b.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a multimodal framework for generating fine-grained captions for audio and video content. The proposed system operates in two stages:\n\n(1) multiple specialized models (such as ASR models for speech, music captioning models for music, and video captioning models for video) are employed to extract modality-specific features and descriptions;\n\n(2) an LLM-based module integrates these heterogeneous captions into a coherent and contextually consistent description.\nThe authors demonstrate that their approach can improve downstream performance in audio retrieval and audio understanding tasks, showing the potential of multi-source caption fusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "•  The framework effectively combines different audio and video analysis models, enabling a more comprehensive and fine-grained understanding of multimodal content.\n\n•  The proposed dataset and experiments show consistent improvements in retrieval and understanding benchmarks, suggesting that the generated captions contain richer semantic information."}, "weaknesses": {"value": "•  The overall framework is structurally similar to Sound-VEcaps, with the primary difference being the inclusion of more domain-specific feature extraction models such as ASR model for speech and Music model for music caption.\n\n•  The paper lacks comparative analysis on audio generation tasks, which could better demonstrate the generalization of the learned representations.\n\n•  The quality filtering strategy used for caption selection is not well-detailed, leaving uncertainty about its effectiveness and criteria.\n\n•  While quantitative results are promising, the paper would benefit from qualitative comparisons or demo examples showcasing differences between LLM-based captions and traditional model-generated captions."}, "questions": {"value": "•  Have you conducted any experiments on audio generation tasks ?\n\n•  Could you provide more details about the quality filter module, including how filtering thresholds are determined and what metrics are used?\n\n•  Would it be possible to include demo comparisons between different LLM-based captions to better illustrate the qualitative improvements?\n\n•  Is there any ablation study showing that these vision-based information affacts the overall performance of different downstream tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nyau7dDPLY", "forum": "3q3LnQ63Az", "replyto": "3q3LnQ63Az", "signatures": ["ICLR.cc/2026/Conference/Submission24950/Reviewer_fb6q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24950/Reviewer_fb6q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485536530, "cdate": 1761485536530, "tmdate": 1762943258541, "mdate": 1762943258541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors in the paper propose a two-state pipeline solution to automatically generate fine-grained audio captions.They achieve this by fusing information from multiple modalities. In first state specialized pretrained models (such as GAMA, Whisper, YamNet, OpenMu) extract complementary cues from an audio clip abd visual scene description via VLM (Qwen2.5-VL-72B) from associated video frames. A large language model (QwQ-32B) integrates these cues to generate a detailed, context-rich caption.   Using this pipeline on AudioSet (2M 10-second YouTube clips), the authors construct FusionAudio-1.2M, a dataset of 1.2 million audio clips with fine-grained captions, plus 6 million question-answer (QA) pairs generated about those clips. They also fine-tune audio-language models on this data, showing improved performance on downstream tasks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The proposed pipeline in paper combines audio, speech, music, and visual cues, analogous to how humans integrate multiple senses. This yields captions with unprecedented detail and accuracy; the paper’s examples show that FusionAudio captions include context and reasoning absent in prior work.\n\n(2) The resulting FusionAudio-1.2M dataset is a significant contribution. It is one of the largest audio-caption corpora to date (1.2M clips), and captions are substantially longer and more descriptive than those in existing datasets.\n\n(3) Models  fine-tuned on FusionAudio show improved performance on multiple tasks."}, "weaknesses": {"value": "(1) The method mainly relies on multiple large pretrained models (Whisper, CLAP, GAMA, OpenMu, Qwen2.5-VL, 32B LLM) in sequence. Architecture is based on pre-existing work and doesn't bring enough novelty to the paper.\n\n(2) Although  abalation shows results for retrival task and audio understanding. The  visual dependency, should be tested using the captioning model with and without visual input. If a model trained on FusionAudio is asked to caption an audio without video, will it hallucinate visual context or perform poorly? The paper doesn’t fully explore this limitation.\n\n(3)  The paper’s hallucination analysis focused on obvious mismatches, but more nuanced biases weren’t discussed. An important limitation is that the ground truth correctness of these detailed captions is not fully verifiable at scale some captions may include inferred context (“wind sounds suggest an outdoor environment”) that isn’t explicitly labeled as right or wrong.\n\n(4) The paper’s evaluations, while extensive on retrieval and audio understanding tasks, did not directly evaluate caption generation quality on standard benchmarks (e.g., no BLEU, CIDEr, or SPIDEr scores on AudioCaps/Clotho test sets were reported). This could be seen as a weakness, as it’s hard to quantify how much better the fine-grained captions are in describing audio content compared to previous captions."}, "questions": {"value": "(1) How good this captioning model work on audio-only inputs? In many real-world audio captioning applications (assistive tech, audio archives), a visual stream won’t be available."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LCJAXAOWVM", "forum": "3q3LnQ63Az", "replyto": "3q3LnQ63Az", "signatures": ["ICLR.cc/2026/Conference/Submission24950/Reviewer_tyQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24950/Reviewer_tyQ8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833381120, "cdate": 1761833381120, "tmdate": 1762943258137, "mdate": 1762943258137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method, and dataset (FusionAudio), for fine-grained audio-visual caption generation. The pipeline for audio-captioning is multimodal: it first extracts domain-specific features (general audio, speech, music presence, and visual cues), then uses an LLM to synthesize these captions into a final version (after some filtering). In this pipeline, all of the underlying components are frozen, and linked via prompt-tuning in the final LLM synthesis step. This pipeline is then used to collect an automated dataset: FusionAudio, which is the result of running this pipeline on 1.2M clips from the AudioSet dataset. The dataset is then used to train a HTSAT-BERT model for cross-modal audio/text retrieval, and it is shown that AudioFusion leads to performance improvements over existing captioning datasets. Further, the paper fine-tunes a GAMA model on FusionAudio, and shows that it helps to improve general purpose audio understanding models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "One of the most exciting areas for research right now is spoken language models (SLMs), and as a community, we are generally lacking high quality pre-training data for SLMs. This paper presents a pathway towards better SLMs, by defining a way to collect audio-visual captioning data from existing video clips. The motivation that the visual component might provide something interesting is exciting, and fairly novel (but has been explored before, for example in both [1,2] they show that visual components can help with audio understanding in ASR). The results are strong, and fairly compelling, particularly the GAMA fine-tuning results in Section 5.2. The paper is well-written, and easy to understand.\n\n[1] Shi, Bowen, et al. \"Learning audio-visual speech representation by masked multimodal cluster prediction.\" arXiv preprint arXiv:2201.02184 (2022).\n[2] Chan, David M., et al. \"Multi-modal pre-training for automated speech recognition.\" ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022."}, "weaknesses": {"value": "- It's likely that some of the improvement from FusionAudio comes from the fact that it is much larger than some of the other datasets, instead of the underlying quality of the data.  It would be good to include the number of samples in each dataset in Table 4, as well as provide a sample/compute-matched ablation in the experiments in 5.1 and 5.2 to help demonstrate that it is not just scale which is improving the performance of the downstream models. \n- The pipeline isn't really trained at all, just stitched together from constituent pre-trained components. Captioning on AudioSet has already been explored (AudioSetCaps). This means that there's a bit of a lack of technical novelty: there's no particularly new strategies here (despite strong downstream improvements). The main novelty is the collection of multimodal constituent components, but it's not really ablated which makes it hard to understand which parts of the captions are leading to the marginal downstream improvements.\n- The results for audio retrieval aren't particularly impressive, with AudioSet captions (a comparatively sized dataset) only performing marginally worse, and the results in Table 5 are strong compared to open models, but are not compared to closed-source models (suggesting that perhaps, a better pipeline would be to use gpt-4o-audio to collect the underlying caption set). \n- There's no statistical analysis in the results. While this is mentioned in the limitations section, bootstrapping, or SEM would provide at least a glimpse of the potential downstream variance (though this is likely fairly low given the large number of test samples). \n- The output captions seem somewhat prompt-tuned to appeal to the target distribution, rather than what is actually relevant to an audio caption. For example, one of the fusion prompt components is: \"Speech Content.... Its specific textual content (including paraphrasing or summarization) must never appear in the final output.\" This will increase performance on the audio captioning task (since the actual speech content is usually not part of the caption, but I'm not sure that this is aligned with how anyone would actually want to use the system (in most cases, I think users would actually want the content of the speech to be part of the audio description). \n\nMore minor weaknesses:\n- There's not much qualitative evaluation (analysis) of samples, which would help the reader to understand what kinds of samples are making up the difference in Table 4/Table 5. \n- The paper claims ``fine-grained audio understanding'' but doesn't really quantify this, or demonstrate that the proposed approach is more fine-grained than any other method. \n\nNotes (rather than weaknesses):\n- I think that Table 13 (Sec. F) is actually really important to this paper, since it demonstrates that humans prefer the pipeline captions to most baseline captions, and hiding this in the appendix reduces the strength of the main point. Figure 4 doesn't really add anything to the main paper, and could be removed for this experiment. \n- I don't love that the model reduces the verbalization of uncertainty (explicitly forbidding mentioning uncertainty in the prompt), since this is likely to reduce user trust in the system, and encourage more confident sounding outputs (while not necessarily being more conservative)."}, "questions": {"value": "- There's no discussion of licensing in the paper, how is the dataset licensed/will it be publicly available for research (or otherwise)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eNfWODP0l2", "forum": "3q3LnQ63Az", "replyto": "3q3LnQ63Az", "signatures": ["ICLR.cc/2026/Conference/Submission24950/Reviewer_pqCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24950/Reviewer_pqCu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848784887, "cdate": 1761848784887, "tmdate": 1762943257872, "mdate": 1762943257872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a pipeline for generating fine-grained audio captions, inspired by human auditory perception. The method involves using specialized expert models (for speech, music, general sound, and visual context) to extract multimodal cues, which are then synthesized by a LLM to produce detailed descriptions. The primary output is FusionAudio-1.2M, a large-scale, open-source dataset constructed using this pipeline. The authors demonstrate the utility of this dataset by training enhanced audio-text retrieval and audio understanding models, and they conduct ablations to confirm the contribution of each modality, with visual information being particularly significant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The construction and open-sourcing of the FusionAudio-1.2M dataset is a significant contribution. Large-scale, high-quality audio-text data is a critical bottleneck in the field, and this resource will be extremely helpful for subsequent research and model development. The paper provides a thorough experimental analysis that clearly demonstrates the importance of integrating multiple modalities. The ablation study convincingly shows that each modality (especially visual context) contributes to the final model performance, offering valuable insights for the community. The evaluation is extensive, covering audio-text retrieval, audio understanding on a wide range of tasks, and even a human preference study. This multi-faceted assessment provides strong evidence for the quality of the generated captions and the models trained on them."}, "weaknesses": {"value": "The core pipeline of \"using expert models to extract features + an LLM to synthesize them\" is a common and well-established paradigm for data augmentation and caption generation. While the specific application to audio and the inclusion of visual cues is valuable, the overall approach lacks fundamental innovation. As a methodological paper, it feels incremental. Also, the decision to build yet another large-scale dataset on top of AudioSet is a notable limitation. The community is already saturated with AudioSet-derived corpora, which limits diversity and can perpetuate the biases and limitations inherent in the original dataset. A more novel audio source would have significantly increased the impact. The framing of the work through \"human auditory perception\" feels somewhat forced and \"flashy but insubstantial.\" The technical contributions stand on their own, and this biological analogy does not add substantial scientific rigor or insight, potentially detracting from the clear presentation of the method."}, "questions": {"value": "1. You identified a potential mismatch between video and audio content (e.g., sounds originating from outside the frame). How does this issue impact caption quality, and did you implement any specific filtering or processing on the video information to mitigate it?\n2. Given the community's concern about over-reliance on AudioSet, what was the rationale for not sourcing audio from a more diverse or novel set of videos? Are there plans to extend FusionAudio beyond AudioSet in the future?\n3. The pipeline relies on multiple large, proprietary models (e.g., Qwen2.5-VL-72B, QwQ-32B). Could you discuss the computational cost and carbon footprint of generating the 1.2M captions? Is the pipeline feasible for most research groups to reproduce?\n4. In the ablation study, removing speech information sometimes improved performance on Task 1. Could you elaborate on the potential reasons? Is this solely due to poor ASR quality, or could it indicate that for some tasks, paralinguistic cues are more important than the lexical content?\n5. How does your LLM-based fusion method compare to a simpler, non-LLM approach (e.g., a rule-based template or a smaller, trained fusion model) in terms of cost, control, and final caption quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DfyVv3w74M", "forum": "3q3LnQ63Az", "replyto": "3q3LnQ63Az", "signatures": ["ICLR.cc/2026/Conference/Submission24950/Reviewer_B87y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24950/Reviewer_B87y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988708494, "cdate": 1761988708494, "tmdate": 1762943257554, "mdate": 1762943257554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}