{"id": "JvpIGWZFhq", "number": 7692, "cdate": 1758032257204, "mdate": 1759897839065, "content": {"title": "Neighborhood Stability as a Measure of Nearest Neighbor Searchability", "abstract": "Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset‚Äîwhat we call ‚Äúsearchability.‚Äù To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality‚Äîa function of a clustering of a dataset‚Äîthat we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability‚Äîa function of the dataset itself‚Äîthat is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.", "tldr": "We present clustering quality and clusterability measures for flat clusterings that utilize nearest neighbor information, and evaluate it in the context of ANN Search.", "keywords": ["nearest neighbor search", "clustering quality", "clusterability"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67ca2df41ad50bc16839c7cc8e959a5fe9168599.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The task that the article considers is measuring the clusterability of the data set and a quality of a particular clustering of a data set for the purpose of measuring the suitability of the data set for the clustering-based approximate nearest neighbor search (ANNS). In particular, clustering-NSM (Neighborhood Stability Measure) is proposed for measuring the internal quality of a clustering. Clustering-NSM of a clustering is the average of set-NSM of the clusters, and the set-NSM of a cluster is simply the fraction of a cluster points whose nearest neighbor belongs to the cluster. The article also proposes a related clusterability measure that measures how likely a set of points is to have a high quality clustering by any clustering algorithm.\n\nThe experimental results show that the proposed quality measure (clustering-NSM) has a higher correlation with the accuracy of the ANN algorithms than the existing internal quality measures of clustering. In addition, the experimental results show that the proposed clusterability measure of a data set correlates with the quality (as measured by clustering-NSM) of the clustering that can be obtained on that data set."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed quality measure of clustering has a higher correlation with the accuracy of the ANNS methods than the earlier quality measures of clustering. The proposed quality of clustering and clusterability measures are straightforward and intuitive. The article is well-written, and the formalization of the concepts and the mathematical notation are very good."}, "weaknesses": {"value": "The proposed measures seem interesting for assessing the quality of clustering and the clusterability of a data set. However, the claim that the these measures enables determining whether a data set is searchable by clustering-based ANNS methods is insufficiently validated by empirical evidence. In particular:\n\n- The experiments do not measure true end-to-end effectiveness of ANNS methods. In practice, the number of searched clusters $\\ell$ is large, whereas in the experiments of the article $\\ell = 1$, and SOTA clustering-base methods further prune the cluster points before exact distance computation, for instance by using product quantization (Jegou et al., 2010) or anisotropic quantization (Guo et al., 2020). Basically, the experiments are equal to doing a $k$-means clustering, and checking whether the nearest neighbors of the query point belong to the same cluster with it. Thus, while the result of the current experiment are promising, it is only a starting point, as it is not yet clear what is the effect of the clusterability of the data set or the quality of the clustering to the end-to-end effectiveness of SOTA clustering-based ANNS methods. See for instance, Aum√ºller et al, (2021) for an example of experimental design where the effect of different hardness measures of a data set to the effectiveness of ANNS methods are quantified (SOTA clustering-based methods are used, hyperparameter sweeps are performed, and whole recall-QPS curves at the optimal hyperparameters are reported).\n\n- It is stated that the proposed quality measure of clustering enables comparing different clusterings of a data set to assess their effect on the performance of ANNS methods. However, the experiments do not test different clustering methods on a same data set to see whether the clustering-NSM of a clustering method is indicative of its end-to-end ANNS performance.\n\n- It is claimed that the proposed clusterability measure enables determining the suitability of clustering-based methods for ANNS. However, no other types of ANNS methods are currently included in the empirical comparison. Thus, it is not clear what is the effect of the clusterability of the data set to the relative (compared to other types of ANNS methods, especially graph methods) performance of the clustering-based methods: if the data set has low clusterability, does it mean that clustering-based methods perform poorly on this data set compared to graph methods, or do all ANNS methods perform equally poorly on this data set?\n\n- The proposed clusterability measure for assessing the suitability of the data set for clustering-based ANNS methods is only compared to the generic quality measures of a clustering. However, there already exists measures, such as relative contrast (RC) (He et al., 2012), local intrinsic dimensionality (LID) (Houle et al., 2013), and query expansion (Ahle et al., 2017) for assessing the hardness of a data set for ANNS; see, e.g.,  Aum√ºller et al, (2021) for empirical evaluation of these hardness measures.\n\nIn summary, while proposed measures seem interesting, the current empirical evaluation is only a brief \"proof of concept\"-study. As is, the manuscript would be a good workshop article, but acceptance to a top tier-conference would require much more thorough set of experiments to justify its claims.\n\nReferences:\n\nAhle, Thomas D., Martin Aum√ºller, and Rasmus Pagh. \"Parameter-free locality sensitive hashing for spherical range reporting.\" Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2017.\n\nGuo, Ruiqi, et al. \"Accelerating large-scale inference with anisotropic vector quantization.\" International Conference on Machine Learning. PMLR, 2020.\n\nHe, Junfeng, Sanjiv Kumar, and Shih-Fu Chang. \"On the difficulty of nearest neighbor search.\" Proceedings of the 29th International Coference on International Conference on Machine Learning. 2012.\n\nHoule, Michael E. \"Dimensionality, discriminability, density and distance distributions.\" 2013 IEEE 13th International Conference on Data Mining Workshops. IEEE, 2013.\n\nJegou, Herve, Matthijs Douze, and Cordelia Schmid. \"Product quantization for nearest neighbor search.\" IEEE transactions on pattern analysis and machine intelligence 33.1 (2010): 117-128."}, "questions": {"value": "I do not have any questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jOuqejn1NS", "forum": "JvpIGWZFhq", "replyto": "JvpIGWZFhq", "signatures": ["ICLR.cc/2026/Conference/Submission7692/Reviewer_xvr7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7692/Reviewer_xvr7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729105132, "cdate": 1761729105132, "tmdate": 1762919751878, "mdate": 1762919751878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an internal measure (Clustering-NSM) for measuring clustering quality and a dataset-dependent measure (Point-NSM) for measuring the clusterability of a dataset. The experiments demonstrates that Clustering-NSM is highly correlated with ANNS accuracy and external measures for clustering quality. In addition, the paper proves that a high and concentrated Point-NSM indicates better Clustering-NSM and validates this theorem with a clustering task on image datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tIt is very novel that Theorem 1 bridges clustering-dependent measure (Clustering-NSM) and clustering-independent measure Point-NSM. As a measure that is only dependent on the dataset, Point-NSM provides valuable insights on the clusterability of the dataset and following clustering and ANNS task.\n-\tBoth Clustering-NSM and Point-NSM are independent of the choice of distance metric. This makes both the measure have boarder flexability.\n-\tThe experiments are conducted on a range of different types of datasets and many types of vision model for encoding. The results are convincing.\n-\tThe design that different iteration steps of k-means indicates different clustering quality is interesting and justified."}, "weaknesses": {"value": "-\tThe high correlation between internal measure such as Clustering-NSM and external measure is surprising. However, this result is only empirical. Is there any explanation for this result?\n-\tThe choice of external measure is very limited. Other popular measures such as (Adjusted) Rand Index could be used. \n-\tA minor point. Don‚Äôt focus on this if the author do not have time for this issue. Can the proposed Clustering-NSM be used for hyperparameter search? It would be interesting to use NSM in unsupervised task like clustering to determine hyperparameter. Many methods now just use the ground-truth label, i.e. external measure like NMI, for hyperparameter search. This is highly impractical and unjustified based on the unsupervised nature of the task. A method with more hyperparameters or continuous hyperparameter often gives a very good yet unfair results through a grid search with very small steps. It would be interseting to see the use of NSM on this aspect."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z3vAN4vKnc", "forum": "JvpIGWZFhq", "replyto": "JvpIGWZFhq", "signatures": ["ICLR.cc/2026/Conference/Submission7692/Reviewer_epcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7692/Reviewer_epcJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907789060, "cdate": 1761907789060, "tmdate": 1762919751404, "mdate": 1762919751404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two measures for evaluating the suitability of clustering-based Approximate Nearest Neighbor Search (ANNS). The clustering-NSM measures the proportion of points in a cluster whose nearest neighbors also lie in the same cluster, while the point-NSM measures, for each point, how many of its nearest neighbors share stable neighborhood relationships. The clustering-NSM is shown to be strongly predictive of point-NSM.\nThese measures generalize k-nearest neighbor consistency to a continuous score between 0 and 1. The clustering-NSM serves as an internal clustering quality metric, and the point-NSM reflects dataset clusterability under flat clustering.\nExtensive experiments show that clustering-NSM correlates strongly with external metrics such as ANN accuracy, mutual information gain, and outperforms traditional indices like Dunn and Davies‚ÄìBouldin. The results also demonstrate that clustering-NSM can predict point-NSM, enabling dataset searchability assessment without requiring query distributions‚Äîan important benefit for large-scale search systems. Furthermore, the experiments indicate that point-NSM can serve as a measure of the dataset‚Äôs overall clustering quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(S1) The paper proposes a theoretically sound and novel clustering measure that fulfills the four axioms of Ben-David and Ackerman (2008), which are considered fundamental requirements for a clustering quality function. It establishes a theoretical framework showing that clustering-NSM satisfies these axioms. Furthermore, the authors derive probabilistic bounds connecting point-NSM and clustering-NSM under the assumption that the data follow a spherical flat clustering structure and are uniformly distributed.\n\n(S2) The paper conducts extensive experiments on benchmark datasets to evaluate the suitability of the proposed measures for k-nearest neighbor searchability analysis and to assess their usefulness in quantifying dataset clusterability. The results demonstrate a strong correlation between clustering-NSM and top-k approximate nearest neighbor accuracies, outperforming the Dunn index and the Davies‚ÄìBouldin index. The experimental evidence is convincing.\n\n(S3) The paper is well written, with a clear presentation of the theoretical contributions, logically inherent, and comprehensive empirical validation."}, "weaknesses": {"value": "(W1) Generalizability of the empirical evaluation of point-NSM for cluster ability:\nThe computation of point-NSM requires nearest-neighbor calculations across the entire dataset, which can be computationally expensive. The authors acknowledge this in their empirical evaluation of point-NSM for clusterability, where they mitigate the cost by randomly subsampling 5% of the points to estimate the point-NSM distribution. But does the contribution stay the same when increasing the amount of subsampling to 100%?  Would we still observe this mostly positive correlation between point-NSM and cluster-NSM? \n\n(W2) Theoretical Richness:\nThe interaction between the hyperparameter œâ (the weighting of cluster-NSM) and the hyperparameter ùëü (the neighborhood size used for point-NSM) is not thoroughly analyzed. This may help explain why, as shown in Figure 3, not all datasets exhibit a clear positive correlation between clustering-NSM and point-NSM.\n\n(W3) Restricted theoretical scope:\nIn the state-of-the-art review, the paper does not reference some recent advances in clusterability research (e.g., https://arxiv.org/pdf/1808.08317 or https://arxiv.org/pdf/2310.12806). Including discussions of these works would provide better context and help position the proposed contributions more clearly. Similarly, in the experimental evaluation, it would be valuable to compare clustering-NSM with more recently developed clustering quality metrics beyond the Dunn and Davies‚ÄìBouldin indices. The analysis focuses exclusively on flat clustering algorithms (e.g., K-means and spherical K-means). The exclusion of hierarchical, graph-based, or density-based methods (e.g., DBSCAN or spectral clustering) limits the generality of the conclusions. This raises the question of whether point-NSM can truly quantify the difficulty of clustering a dataset."}, "questions": {"value": "(Q1) In Figure 3, we observe that the correlation between point-NSM and clustering-NSM varies across datasets, and in some cases (e.g., GloVe), the correlation appears to be weak. How would you revise the paper to explain why this occurs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fXx1guFXxP", "forum": "JvpIGWZFhq", "replyto": "JvpIGWZFhq", "signatures": ["ICLR.cc/2026/Conference/Submission7692/Reviewer_h922"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7692/Reviewer_h922"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944388673, "cdate": 1761944388673, "tmdate": 1762919750892, "mdate": 1762919750892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an important and practical problem: determining the \"searchability\" of a dataset for clustering-based Approximate Nearest Neighbor (ANN) search. The main contribution is the introduction of the \"Clustering-NSM\" measure, which the authors claim can  predict the performance of the dataset on a clustering based ANN data structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Picking an appropriate ANN datastructure for a dataset is a very practical problem as there are many choices available to practitioners. Furthermore, the introduced measure is demonstrated to have a high correlation with performance on (simple) ANN datastructures based on clustering."}, "weaknesses": {"value": "- One of the paper's drawback is the following: it attempts to define and measure a dataset's suitability for clustering-based ANNS, which is a function of a specific clustering algorithm, rather than simply measuring the dataset's clusterability itself. Thus, it is not clear to me why one would study \"searchability\" (a function of a clustering) instead of the dataset's intrinsic clusterability. In what practical situation would a dataset that is \"clusterable\" (e.g., as measured by k-means loss) not be \"searchable\" by a clustering-based ANNS algorithm? I am not sure if this distinction exist.\n\n- The introduced measure is also not very surprising as it seems to directly mimic testing the ANN datastructure on 'queries', where the performance on queries is approximated by performance on the dataset points themselves (To me this is what is going on since query points and the input dataset points are often from the \"same distribution.\" The proposed Clustering-NSM measure is defined as the fraction of points whose 1-nearest-neighbor resides in the same cluster. For a clustering-based ANNS method, high recall typically requires that a query's true nearest neighbor be in the same cluster. Again, assuming query points are drawn from the same distribution as the dataset, Clustering-NSM is not just predictive of ANNS accuracy, but rather seems like direct proxy for it. So it is not clear to me if the high correlation shown is particularly insightful.\n\n- Another major practical drawback is the computational cost of the proposed measures. To compute Clustering-NSM one must find the nearest neighbor(s) for every point in the dataset (ignoring the point itself). This is a computationally intensive task, requiring quadratic time in the worst case. This is a high cost and a significant barrier in practice. The authors claim that one can replace this by finding approximate nearest neighbors, but this seems a bit circular to me: then we need ANN data structure to compute this measure but if we had the data structure already, then one could simply test the performance of the ANN datastructure directly on the dataset, making the measure useless.\n\n- I am also not convinced by how interesting Theorem 1 is. It uses an imprecise and subjective statement \"As such, clustering-NSM is a measure of clustering quality.\" This statement seems to be in reference to a set of very specific axioms presented in an old paper by Ben-David & Ackerman (2008). However, it provides no justification for why this specific set of axioms should be considered the standard. Given that these axioms do not appear to be popular in the literature, the authors should justify why they are the \"right\" axioms to use.\n\n- There also appears to be either an error  or a typo in the statement of Theorem 2. The right hand side of the inequality has a *minus* $\\sqrt{\\log(1/\\epsilon)}$ term. If we let $\\epsilon$ approach 0  the term $\\log(1/\\epsilon)$ approaches positive infinity. This causes the right-hand side of the inequality to approach negative infinity. A measure like clustering-NSM, which is a fraction and thus non-negative, cannot be less than a value approaching negative infinity. \n\n- On a minor note, the paper consistently refers to \"inner product distances\". This is mathematically imprecise. An inner product is a measure of similarity, not a distance, as it does not satisfy the axioms of a metric (e.g., non-negativity). \"Similarity\" would be the more appropriate term."}, "questions": {"value": "Do the authors have examples of datasets where the dataset itself is not a good candidate for clustering (i.e. it has poor \"clusterability\") but performs well for clustering based ANN datastructures or vice versa? Such examples (or understanding why such examples can arise) would convince me more of why one should study which datasets are \"good\" for clustering based ANN datastructures, rather than studying the simpler question of which datasets are \"good\" for clustering itself. Right now, it seems that the first question is a function of the answer of the second question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mNjUCC8Zhm", "forum": "JvpIGWZFhq", "replyto": "JvpIGWZFhq", "signatures": ["ICLR.cc/2026/Conference/Submission7692/Reviewer_iRDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7692/Reviewer_iRDX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975501677, "cdate": 1761975501677, "tmdate": 1762919750331, "mdate": 1762919750331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and addresses a practical gap in the field of clustering-based Approximate Nearest Neighbor Search (ANNS): the lack of analytical tools to determine if a dataset is \"searchable,\" (i.e., well-suited for this search method) before running costly experiments. The authors propose a two-part solution: (1) Clustering-Neighborhood Stability Measure (clustering-NSM): An internal clustering quality measure that, unlike existing measures, is shown to be predictive of the external task-based measure of ANNS accuracy (recall). (2) Point-Neighborhood Stability Measure (point-NSM): A novel \"clusterability\" measure that operates on the dataset itself. This measure is predictive of the clustering-NSM.\n\nThe central claim is that this chain of prediction (point-NSM $\\rightarrow$ clustering-NSM $\\rightarrow$ ANNS accuracy) allows practitioners to assess a dataset's searchability given only the data points, a significant practical advantage. A key strength of these measures is their foundation on nearest-neighbor relationships rather than absolute distances, making them applicable across various distance/similarity functions, including Euclidean, cosine, and inner product. These are the first measures to serve as a tool for quantifying the amenability of a set of points to clustering-based ANNS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel and High-Impact Problem: The problem of a priori algorithm selection is difficult and valuable. The paper's focus on \"searchability\" addresses a pain point familiar to any practitioner who has had to \"guess and check\" ANN indexing strategies.\n\nPractical Utility: If the proposed measures are computationally efficient, they could save significant time and resources by providing a strong signal for or against using clustering-based ANN without the need to complete the costly experiment.\n\nGenerality of Measures: The authors' choice to base the measures on k-NN relationships (specifically 1-NN) instead of raw distances is a key insight. This makes the framework robust and applicable to inner product spaces, which are common in modern vector search (e.g., for embeddings) but are problematic for many traditional clustering measures that require non-negative distances.\n\nFurther, the authors provided experimental evidence of a potentially strong connection between proposed stability measure and ANN search results."}, "weaknesses": {"value": "The major incentive for designing NSM's is to save the run-time of the whole clustering + ANN approach, hence the utility of point-NSM is entirely dependent on its computational complexity. The measure is described as a \"statistic summarizing the distribution of point-NSMs,\" where a single point's NSM is derived from its \"r nearest neighbors.\" Calculating nearest neighbors for all points can be quadratic operation, which is prohibitive. I think the point of clustering is to \"shrink\" the scope of NN searches for any given query point. It's unclear to me how much time we can save by directly computing the Point-NSMs. I found both the theoretical and empirical discussion to be dissatisfying to that end.\n\nIt's also curious that the benchmark metrics used in the experiments, DUNN and DB, were proposed in the 1970s. It seems unlikely that this is the STOA approach for either evaluating clustering + ANN method effectiveness, or \"clusterability\" of any dataset."}, "questions": {"value": "Why is the definition of Point-NSM introduced half-way through the discussion on Set- and Cluster-NSM in section 3.2? It doesn't look like it is used anywhere in that section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SgVIDiFW2Q", "forum": "JvpIGWZFhq", "replyto": "JvpIGWZFhq", "signatures": ["ICLR.cc/2026/Conference/Submission7692/Reviewer_gKg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7692/Reviewer_gKg9"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329688297, "cdate": 1762329688297, "tmdate": 1762919749819, "mdate": 1762919749819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}