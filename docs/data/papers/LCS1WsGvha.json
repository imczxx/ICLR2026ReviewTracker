{"id": "LCS1WsGvha", "number": 15965, "cdate": 1758257822679, "mdate": 1759897270363, "content": {"title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents", "abstract": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol—the global industry standard for smart home communication—SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 11 agents under a unified ReAct framework reveals that while models perform well on simple tasks, they struggle with latent intent inference, state verification, and especially temporal scheduling. Even the top-performing model, GPT-4.1, reaches only 54\\% accuracy. These findings highlight a critical need for methods that can reliably verify the current state via tools before acting and coordinate time-dependent actions.\nWe will release our code and benchmark to facilitate reproducibility and further research.", "tldr": "", "keywords": ["smart home", "simulator", "language model", "language agent", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9b5a677cd2edc94c6e71d57ec83ff393cfb0bcf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SimuHome, a simulator and benchmark for evaluating smart home LLM agents under different real-world scenarios. It models 4 environmental variables and 17 device types, and provides 600 episodes across 12 query types. The authors evaluate 11 agents and find that models struggle with latent-intent inference, live-state verification, and temporal scheduling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1: The idea of developing smart home agents capable of dealing with real world problems is interesting and worth exploring.\n\n2: The experiments setting with 600 episodes and 12 different queries is comprehensive. \n\n3: The illustrations in the paper is well-structured and make it easier for the understanding of the paper."}, "weaknesses": {"value": "1: The selection of LLMs is not up-to-date and not optimal. GPT-5 should be taken into consideration. Meanwhile, the authors chose Gemini-2.5-Flash and GPT-4.1 instead of Gemini-2.5 Pro and did not enable the thinking mode for GPT-4.1, which limits the persuasiveness of the claims and experiments.\n\n2: Table 1 is confusing, especially regarding the different superscripts J and S. In QT1, both F and IF are marked with J (for LLM-judge-based evaluation), but for the following queries, the format seems to be inconsistent. Also, in Table 1 for the QT2-F Task, the success rate of GPT-4.1 is 44%, which is far behind GPT-4.1-mini and Gemini-2.5-Flash. I'm curious about the reason for this phenomenon."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6UoFzFOcRK", "forum": "LCS1WsGvha", "replyto": "LCS1WsGvha", "signatures": ["ICLR.cc/2026/Conference/Submission15965/Reviewer_hnkK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15965/Reviewer_hnkK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760657908775, "cdate": 1760657908775, "tmdate": 1762926173556, "mdate": 1762926173556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SimuHome, a high-fidelity simulator and benchmark designed to evaluate LLM agents in smart home environments. The simulator is Matter-protocol compliant, supporting real-time device interactions, temporal dependencies, and environmental feedback (temperature, humidity, etc.).\n\nThe benchmark includes 600 manually validated episodes across 12 query types, including environment perception, implicit intent, explicit intent, and three complex types of temporal scheduling (future, dependency, and concurrent). A key feature is that each type includes both \"feasible\" and \"infeasible\" variants to test an agent's robustness and logical reasoning.\n\nThe authors evaluated 11 prominent LLM agents under a unified ReAct framework. The results clearly show that while models can handle simple, explicit commands, they severely struggle with inferring latent intent, verifying states, and especially handling complex temporal scheduling (QT4). Even the top-performing model, GPT-4.1, achieved an overall accuracy of only 54%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.\tHigh Fidelity in simulator design and Matter integration.\n2.\tThe benchmark design is excellent. The 12 query types cover a wide range of scenarios, from simple perception to complex scheduling. The introduction of \"feasible\" vs. \"infeasible\" variants is crucial for testing an agent's \"refusal\" capabilities and logical consistency.\n3.\tThe focus on temporal reasoning (QT4) is the paper's biggest highlight. It correctly identifies this as the \"Achilles' heel\" of current SOTA models, including GPT-4.1. The error analysis, which distinguishes between \"Contradiction Mishandling (CM)\" and \"Contradiction Blindness (CB),\" is highly insightful for understanding the root cause of these failures.\n4.\tThe analysis in Section 6.2 is brilliant. The authors found that agents perform well on QT3 (Explicit Control) because they can \"passively learn\" and recover from the tool's immediate error feedback. In contrast, they fail on QT4 (Temporal Scheduling) because the schedule_workflow tool only returns a \"scheduling successful\" acknowledgment, and the feedback is deferred (the failure only becomes apparent at execution time). This points to a clear direction for future research (e.g., the need for \"pre-validation\" tools)."}, "weaknesses": {"value": "1.\tThe paper mentions simulating 4 environmental variables (temperature, illuminance, humidity, air quality) and that device operations have a cumulative impact. However, the current design seems focused on a one-way \"device→environment\" effect. The paper does not detail whether more complex interactions are modeled, such as \"environment→environment\" (e.g., does an open window affect the AC's cooling efficiency?) or \"device→device\" (e.g., does the simulator model power load conflicts, such as 'tripping the breaker' as hinted at in the QT4-3 example?).\n2.\tThe benchmark consists of 600 fixed, human-validated episodes. While high-quality, is this sufficient to prevent SOTA models (especially closed-source ones) from \"memorizing\" the 12 query types after a few iterations? The authors mention generating layouts to \"prevent agent overfitting\", but this seems to refer to the layout, not the task itself. A discussion on the potential for dynamic generation or scalability of the benchmark would be beneficial.\n3.\tThe evaluation is standardized on the ReAct framework, which is good for a fair comparison. However, ReAct is a relatively simple \"think-act\" loop. The paper attributes the failures in QT4 to the models, but to what extent is this also a failure of the ReAct framework itself, which is not inherently designed for long-term planning? Would agents using more complex planning algorithms (e.g., Tree-of-Thoughts or a dedicated planning loop) perform differently?"}, "questions": {"value": "1.\tCan you elaborate on the complexity of the environmental interactions? Does the SimuHome simulator model \"environment→environment\" or \"device→device\"   interactions?\n2.\tThe evaluation standardizes on the ReAct framework. How much of the significant failure on QT4 (temporal) tasks do you attribute to the models' core reasoning limitations versus the inherent limitations of the ReAct framework for complex, long-term planning?\n3.\tYour analysis in Section 6.2 regarding \"deferred feedback\" is very insightful. Based on this, what do you believe is the most promising path forward: developing better \"pre-validation\" tools to provide immediate feedback, or creating more advanced agent architectures that can reason about deferred outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wmprwVNQwL", "forum": "LCS1WsGvha", "replyto": "LCS1WsGvha", "signatures": ["ICLR.cc/2026/Conference/Submission15965/Reviewer_wJtQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15965/Reviewer_wJtQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827903835, "cdate": 1761827903835, "tmdate": 1762926172755, "mdate": 1762926172755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author introduce SimuHome, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. The benchmark contains test realistic query in smart home scenario like “When the dishwasher finishes, please turn off the kitchen lights”."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The simulator is a useful artifact for future work.\n\n* The analysis in the work provides an insightful look into model's behavior. \n\n* Reproducible simulation environment"}, "weaknesses": {"value": "* The baseline in the model is using ReAct which is disadvantaged for longer horizon tasks like QT4. Have the authors think about including a memory-augmented baseline?\n\n* Could the bad performance on infeasible test cases be solvable if prompted properly?"}, "questions": {"value": "* Line 429, the lack of tool feedback. Should it be a feature that needs to be added to the simulator or something tested system needs to deal with?\n\n* How is home layout presented to LLM?\n\n* For queries like \"When the dishwasher finishes, please turn off the kitchen lights\", how much of it could be resolved by having a system that set a callback and prompt LLM when the time comes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6CVicCLAiP", "forum": "LCS1WsGvha", "replyto": "LCS1WsGvha", "signatures": ["ICLR.cc/2026/Conference/Submission15965/Reviewer_zEmB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15965/Reviewer_zEmB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867608039, "cdate": 1761867608039, "tmdate": 1762926171929, "mdate": 1762926171929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SimuHome, a Matter protocol-based smart home simulator and benchmark system for evaluating Large Language Model agents in realistic home environments. The system simulates 17 device types and 4 environmental variables, providing 600 test episodes covering 12 query types including environment perception, intent inference, device control, and temporal scheduling. Evaluation of 11 mainstream LLMs reveals that the best-performing model, GPT-4.1, achieves only 54% overall accuracy, with temporal scheduling tasks being the most challenging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Smart home control addresses real-world challenges in production systems, including latent intent inference, temporal dependencies, and device constraints, making the research practically valuable.\n2. The benchmark covers 12 diverse query types with 600 carefully validated episodes featuring both feasible and infeasible scenarios.\n3. The paper evaluates 11 diverse models with detailed error analysis using a well-defined taxonomy, examines tool-call patterns and error recovery mechanisms, providing valuable insights into current limitations and improvement directions."}, "weaknesses": {"value": "1. Limited model diversity in evaluation, particularly lacking small-parameter models (e.g., <7B parameters) that would be practically relevant for on-device smart home scenarios where computational resources are constrained and real-time response is critical.\n2. The benchmark lacks support for multi-turn interactive dialogues and clarification exchanges. Real-world smart home interactions often involve users providing additional context or correcting misunderstandings across multiple turns, which is not captured in the current single-turn episode design.\n3. The temporal scheduling mechanism is overly restrictive, requiring agents to perform static planning without the ability to schedule periodic tasks, dynamically re-evaluate conditions, or adjust plans based on runtime state changes. This limits the realism of temporal reasoning evaluation compared to real smart home systems that support event-driven and adaptive scheduling."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1LuGdteEr9", "forum": "LCS1WsGvha", "replyto": "LCS1WsGvha", "signatures": ["ICLR.cc/2026/Conference/Submission15965/Reviewer_qKgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15965/Reviewer_qKgg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976245014, "cdate": 1761976245014, "tmdate": 1762926171497, "mdate": 1762926171497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}