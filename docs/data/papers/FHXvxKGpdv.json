{"id": "FHXvxKGpdv", "number": 25229, "cdate": 1758365490107, "mdate": 1759896729034, "content": {"title": "UPER: Bridging the Perception Gap in Personalized Image Generation with Human-Aligned Reinforcement Learning", "abstract": "Personalized image generation aims to synthesize novel scenes featuring a specific user-provided subject. However, state-of-the-art models often fail to preserve the fine-grained details that define a subject's unique identity, a critical flaw that limits their use in high-fidelity applications. This \"consistency gap\" arises from a misalignment between the model's learned similarity metric and nuanced human perception. To address this, we introduce \\textbf{UPER} (\\textbf{U}nifying \\textbf{P}ost-training for P\\textbf{er}sonalization), a post-training framework designed to align generative models with human preferences for detail consistency. UPER employs a two-stage process: it first refines the model's focus on the subject's core attributes via Supervised Fine-Tuning (SFT) on a dataset with cleaned background information. Subsequently, it optimizes the model using Reinforcement Learning (RL) with a novel composite reward function. The key component of this function is a new patch-based consistency metric that accurately measures subject fidelity using only pre-trained vision encoders, eliminating the need for expensive preference data collection. We apply UPER to the state-of-the-art OminiControl model. The results are unequivocal: in a blind user study with over 1,000 responses, images generated by our final model were preferred for their overall quality and subject consistency \\textbf{89.3\\%} of the time over the strong baseline. Our work provides a robust and scalable solution to the detail-consistency challenge, paving the way for more faithful personalized generation.", "tldr": "", "keywords": ["RLHF", "Personalization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba07636d36e493146275799ff2052a2a763ff78e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper uses supervised finetuning and reinforcement learning to strengthen detail consistency in personalized image generation. A composite reward is proposed to maintain prompt alignment, aesthetic quality and subject consistency. The SFT dataset Subject-200K is  refined to force focus on subject through background removal.  Both automatic and human evaluations are conducted to validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed two-stage post training scheme helps better preserve subject details, and shows apparent boost over the baseline model from several prospectives, including human assessment, automatic evaluation and quantitative metrics. \n2. Various ablation studies are conducted to validate the design choice and the necessity of both supervised fine tuning and reinforcement learning."}, "weaknesses": {"value": "1. missing formatting elements that need to be addressed, including the figure citation in the Introduction section and the text label for Algorithm 1.\n2. Lack of comparison with other personalized image generalists. Only the baseline OmniControl (Tan et al., 2024), IP-Adapter (Ye et al., 2023), and DreamBooth (Ruiz et al., 2023) are compared quantitatively. Qualitative comparison and human evaluation are only conducted between the proposed method and its baseline OmniControl. Moreover, the paper lacks evaluation on human-aligned benchmarks such as DreamBench++ (Peng et al., 2024), which provides standardized qualitative assessment criteria for personalized image generation. Incorporating results on DreamBench++ would greatly strengthen the paperâ€™s credibility and allow a fairer, more comprehensive comparison with existing personalized image generalists."}, "questions": {"value": "Recently, a few larger VLM models claim to be good at both personalization and subject-driven generation, for example, DreamO(Mou et al., 2025) and OminiGen(Xiao et al.,2024). Comparisions with these general generation models are quite intriguing and necessary to validate the necessity and effectiveness of the proposed specialist model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XnxyfukuW9", "forum": "FHXvxKGpdv", "replyto": "FHXvxKGpdv", "signatures": ["ICLR.cc/2026/Conference/Submission25229/Reviewer_nsxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25229/Reviewer_nsxh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834420635, "cdate": 1761834420635, "tmdate": 1762943372411, "mdate": 1762943372411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose UPER (Unifying Post-training for Personalization), a two-stage post-training framework designed to bridge the \"consistency gap\" in personalized image generation by aligning models with nuanced human perception of fine-grained subject details.\n\nThe UPER framework consists of two main stages:\n\n- Refined Supervised Fine-Tuning: The model is initially fine-tuned on a systematically refined dataset where confounding background information is removed from reference images. This forces the model to learn a more precise, subject-focused representation.\n\n- Reinforcement Learning: The refined model is optimized using Reward-supported Flow Learning (ReFL) with a novel composite reward function. The key innovation in the reward function is a patch-based consistency metric R_sub that leverages a pre-trained DINOv2 vision encoder to accurately measure fine-grained subject fidelity without requiring expensive preference data collection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The paper successfully demonstrates that UPER's objective function is strongly aligned with human preferences, evidenced by the overwhelming 89.3% user preference in the large-scale blind study.\n\n- The introduction of the patch-based R_sub reward using a DINOv2 encoder is a practical sol. It captures fine-grained, local details that global metrics miss and, crucially, eliminates the need for collecting expensive preference data for the consistency reward model itself.\n\n- The experimental setup is thorough, including quantitative metrics, automated evaluation (GPT-4o), and human studies. The ablation studies convincingly validate the necessity of the full reward ensemble and the superior performance of DINOv2 over CLIP for the patch encoder."}, "weaknesses": {"value": "-  As acknowledged in the limitations, the two-stage pipeline, particularly the RL stage with the composite reward ensemble, introduces significant computational overhead and implementation complexity. The use of three separate pre-trained models for the reward function adds complexity, and the paper notes that the training process may not be optimal for all subject types.\n\n-  The current evaluation focuses primarily on single-subject personalization. The core challenge of detail consistency is likely compounded in complex scenes with multiple, interacting subjects."}, "questions": {"value": "- You correctly state that DINOv2 is chosen for its strong performance on instance-level matching due to its self-supervised training. Could you provide more detail on the failure modes of the CLIP-based patch encoder observed in your experiments, beyond the lower DINOv2-Sim score (0.81 vs. 0.85)? For example, what did the generated images look like when CLIP was used for R_sub?\n\n- The SFT data refinement involves background removal and VLM (Qwen-VL) filtering. Could you provide an example of a training pair that the VLM filter would typically discard? This would help clarify the exact nature of the \"imperfectly paired data\" or consistency check that the VLM is enforcing.\n\n- The composite reward weights are fixed  based on empirical sweeps. If a user's priority shifts (e.g., they care equally about aesthetic quality and subject consistency), would you recommend re-running the full RL stage with new weights, or could a pre-trained UPER model be adapted more quickly? Is there a risk that these weights, optimized on a specific dataset, might not generalize well to novel or out-of-distribution subjects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2A9NNjyNyD", "forum": "FHXvxKGpdv", "replyto": "FHXvxKGpdv", "signatures": ["ICLR.cc/2026/Conference/Submission25229/Reviewer_2GT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25229/Reviewer_2GT3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064351208, "cdate": 1762064351208, "tmdate": 1762943372196, "mdate": 1762943372196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UPER (Unifying Post-training for Personalization), which aims to preserve fine-grained details of subject in personalized image generation. \nIn detail, UPER first employs Supervised Fine-Tuning (SFT) on OmniControl with cleaned background reference images to ensure the model focus more on the subject. Secondly, UPER optimizes the model with Reinforcement Learning (RL) with ensemble reward models, including CLIP-ViT-L/14 for prompt alignment, HPS-v2 for aesthetic quality, and patch-based DINOv2 for fine-grained subject consistency. \nExperimental results suggest that UPER outperforms baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of ensemble reward model is novel and interesting, as it handles all three different goals (prompt alignment, visual quality, and detail preservation) at the same time. \n\n\n- Experiments and evaluation are thoroughly to verify the effectiveness of UPER."}, "weaknesses": {"value": "- ### W1: Detail implementation on the patch-based subject consistency reward model is missing. \n\nIn Line 197, this paper just claims that \"it computes the cosine similarity on each spatially corresponding patch pair\". \nI am curious how this is implemented in practice:  \nIn detail, assume the reference image contains a number of 16x16 patches, and the target image contains a number of 16 x 16 patches. Does this paper calculates similarity of all possible combination pairs between reference patches and target patches (16 x16 x 16 x 16), or it only calculate similarity between the same spatial location (16 x 16)?  \nFor the first case, if I understand correctly, it will also calculate the similarity between subjects and background. In this case, does it favor generated images with bigger subject?  For the second case, how to ensure the same location patch is semantically corresponding is a question. \nAdditionally, I am curious if this \n\n\n- ### W2: Potential overfitting issue of reference images in white background\nIn the SFT stage, the model is trained with background-removed reference images. I am curious if this will make the model overfit to images in white background and cannot generalize into images in other background. \n\nAdditionally, how to ensure the background removal model always performs well on complicated subjects scene. For example, a chair is occluded by a person sitting on it. \n\n\n\n- ### W3: More ablation study on the ensemble weights is preferred. \nThis paper applies 0.2, 0.2, 0.4 for three reward models via empirical sweeps. These number seems too ad-hoc, as their summation is even not 1.0. The ablation study in Table 3 only verify the importance of each reward model, but does not verify this magic combination. \n\n\n\n- ### W4: The presentation needs to be improved. \n Many references are missing. For example, Line 043 (Figure ??), Line 194 (DINOv2), Line 257 (Qwen-VL), etc.\n\n\n- ### W5: More qualitative comparison results are preferred."}, "questions": {"value": "In general, I think this paper is novel, but just a few detail implementation and discussion are missing.\n\n- Please present detail implementation and discussion regarding my question about the subject consistency.  \n\n- Please address the potential issues I raised in the white background image cases. \n\n- In Line 369, this paper claims that \"the backline model generates a backpack of a different color\". However, when looking into Figure 5, the reference image is more like a red/pink backpack, and the baseline generates a red/pink backpack. The model in this paper generates a purple backpack, although its detail preservation is much better.  I am not sure if this paper intended to show this failure case or present image incorrectly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2dppoyFk5W", "forum": "FHXvxKGpdv", "replyto": "FHXvxKGpdv", "signatures": ["ICLR.cc/2026/Conference/Submission25229/Reviewer_qFDK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25229/Reviewer_qFDK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112822047, "cdate": 1762112822047, "tmdate": 1762943371880, "mdate": 1762943371880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}