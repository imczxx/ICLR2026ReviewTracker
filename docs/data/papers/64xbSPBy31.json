{"id": "64xbSPBy31", "number": 7134, "cdate": 1758009006901, "mdate": 1762956308075, "content": {"title": "DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching", "abstract": "Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis. However, the significant appearance differences between modalities make this task challenging. Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios. Vision Foundation Model (VFM), trained on large-scale data, yields generalizable and robust feature representations adapted to data and tasks of various modalities, including multimodal matching. Thus, we propose DistillMatch, a multimodal image matching method using knowledge distillation from VFM. DistillMatch employs knowledge distillation to build a lightweight student model that extracts high-level semantic features from VFM to assist matching across modalities. To retain modality-specific information, it extracts and injects modality category information into the other modality's features, which enhances the model's understanding of cross-modal correlations. Furthermore, we design V2I-GAN to boost the model's generalization by translating visible to pseudo-infrared images for data augmentation. Experiments show that DistillMatch outperforms existing algorithms on public datasets.", "tldr": "DistillMatch uses knowledge distillation from vision foundation models for multimodal image matching. It employs a lightweight student model and a Category-Enhanced Feature Guidance Module. V2I-GAN is proposed for data augmentation.", "keywords": ["multimodal image matching", "vision foundation model", "knowledge distillation", "modality-specific information"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ba948c3c69e23728aa5d96132c64eb6de75cd0d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DistillMatch, a novel framework for multimodal image matching that leverages knowledge distillation from Vision Foundation Models (VFMs) such as DINOv2. The method aims to bridge modality discrepancies (e.g., visible vs. infrared imagery) and mitigate data scarcity by transferring high-level semantic representations from a pretrained VFM (teacher) to a lightweight student model. To further enhance cross-modal understanding, the authors propose a Category-Enhanced Feature Guidance (CEFG) module that injects modality-specific category information into features from another modality. In addition, a V2I-GAN framework is introduced for data augmentation via visible-to-infrared translation. Experimental results on multiple public datasets demonstrate that DistillMatch achieves superior performance compared to state-of-the-art methods across various tasks, including relative pose estimation, homography estimation, and zero-shot matching."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces an effective strategy to transfer semantic knowledge from a pretrained VFM to a lightweight student network, producing generalizable and robust cross-modal representations without requiring extensive labeled datasets.\n\n2.\tThe proposed Category-Enhanced Feature Guidance module effectively integrates modality-specific category representations into features from another modality, enhancing the model’s ability to capture meaningful cross-modal correlations.\n\n3.\tThe proposed GAN-based visible-to-infrared translation framework addresses the scarcity of annotated multimodal datasets by generating geometrically consistent pseudo-infrared pairs, improving training diversity.\n\n4.\tDistillMatch achieves consistent improvements over strong baselines in multiple benchmark tasks, including both supervised and zero-shot scenarios. The results demonstrate strong generalization and adaptability to diverse modalities and imaging conditions."}, "weaknesses": {"value": "1.\tThe writing is at times difficult to follow. For example, the abstract and introduction omit descriptions of key components such as the Coarse-level Matching Module (CMM), Fine-level Matching Module (FMM), and Subpixel Refinement Module (SRM), making it challenging for readers to grasp the overall system design early on.\n\n2.\tEssential terms and modules such as VFM and STFA are not defined or introduced in the Introduction, reducing accessibility for readers not deeply familiar with these concepts.\n\n3.\tThe inclusion of a KL divergence loss between feature embeddings (F_tea and F_stu) lacks clear justification. The paper should explain why this distribution-level alignment is meaningful in the context of feature distillation.\n\n4.\tThe training loss combines multiple components with numerous weighting coefficients. However, the paper does not provide rationale or sensitivity analysis for these trade-off parameters. A brief discussion or ablation would help clarify their impact.\n\n5.\tThe method is evaluated only with DINOv2 as the teacher. It remains unclear whether similar gains would hold when using other VFMs such as CLIP, SAM, or EVA. Evaluating multiple teachers would strengthen claims of generalizability.\n\n6.\tSince DINOv2 is trained primarily on large-scale visible images, it is uncertain how reliable the distilled features are when applied to modalities such as infrared, retina, or depth. \n\n7.\tThe paper lacks ablation studies isolating the impact of losses or modules related to the CMM, FMM, and SRM. This omission makes it difficult to quantify how each contributes to overall performance.\n\n8.\tThe description of V2I-GAN is terse and lacks supporting figures or algorithmic clarity. Specifically, it is unclear how the STFA module is integrated into the encoder, and the definition and intuition behind the structured gradient alignment loss should be elaborated.\n\n9.\tThe paper presents qualitative examples comparing V2I-GAN with PearlGAN but does not include quantitative metrics (e.g., FID, LPIPS, or domain similarity scores) to substantiate claims of improved realism or domain transfer fidelity.\n\n10.\tThe introduction repeats background material and lacks a clear logical progression from problem statement to motivation and contributions."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "popdJwSrXf", "forum": "64xbSPBy31", "replyto": "64xbSPBy31", "signatures": ["ICLR.cc/2026/Conference/Submission7134/Reviewer_prnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7134/Reviewer_prnt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644568657, "cdate": 1761644568657, "tmdate": 1762919299367, "mdate": 1762919299367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Aq3RyBi2Dq", "forum": "64xbSPBy31", "replyto": "64xbSPBy31", "signatures": ["ICLR.cc/2026/Conference/Submission7134/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7134/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762956306485, "cdate": 1762956306485, "tmdate": 1762956306485, "mdate": 1762956306485, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper address multimodal image matching via distilling useful knowledge from pretrained vision-language models (DINO v2). To make it, a lightweight student model is learned by online knowledge distiilation. Besides, category-enhanced feature guidane module (CEFG) , and semantic and texture feature aggregation module (STFA) are developed in the proposed matching framework namely DistillMatch. Experimental results show the effectiveness of these key components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Image matching is an important and fundamental task in computer vision. It is convincing to leverage VLMs to advance multimodal image matching. \n\nThe experiments are comprehensive with both quantitative and qualitative evaluations."}, "weaknesses": {"value": "Although some implementation details have been provided, the writing needs more improvements. Some descriptions are unclear and hard to understand. For example, it is not easy to capture the technial contributions in this work. Besides, what are the relations between the key components such as CEFG and STFA. \n\nIt is hard to understand the complex processes in CEFG and STFA. Are there any empirical insights behind these components?\n\nThe knowledge distillation algorithm used in this work is not novel at all. In addition, the three loss weights in Eq.(4) are redundant. Perhaps, only two of them are enough for parameter tuning. \n\nSome implementation details are missing.\n\nI am confused by the construction of semantic and texture feature aggregation module. More detailed description and motivation are needed."}, "questions": {"value": "Why both ResNet and ViT are both used for feature extraction. It lacks convincing explanation in the paper.\n\nWhy the images are downsampled to 7/8? Is there any important evidence?\n\nIn Line 215, what is learnable category feature? Or where is it from?\n\nWhether the proposed method can be applicable to other VLMs as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y9bLwVBrc2", "forum": "64xbSPBy31", "replyto": "64xbSPBy31", "signatures": ["ICLR.cc/2026/Conference/Submission7134/Reviewer_fXMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7134/Reviewer_fXMe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748260566, "cdate": 1761748260566, "tmdate": 1762919298678, "mdate": 1762919298678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses multi-modal image matching by proposing DistillMatch, a knowledge distillation framework that leverages vision foundation models. The authors use a teacher-student architecture where a pre-trained vision model serves as the teacher to extract high-level features, while the student model is enhanced with additional modality information for improved generalization across different imaging modalities. The paper additionally proposes to add a GAN based approach for augmentation"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an important problem in multi-modal alignment with practical applications. It introduces a variant  of knowledge distillation to enhance cross-modal image matching, enabling better alignment and understanding across different data modalities. Additionally, it tackles the challenge of data scarcity by incorporating effective data augmentation strategies, thereby improving the model’s robustness and generalization."}, "weaknesses": {"value": "## Major Concerns\n1)Clarity and Technical Details\n\nLine 72 & Introduction: The paper introduces V2I-GAN without proper explanation. What does V2I-GAN stand for? How does it translate visible to pseudo-infrared images? This critical component needs clear definition and technical description.\nLine 70: STFA is mentioned without definition. Is this \"Student Teacher Feature Aggregation\"? Please clarify all acronyms upon first use.\nLine 160: The superscripts 1/2, 1/4, 1/8 appear to refer to scales but this should be explicitly stated.\n\n2. Technical Methodology Issues\n\nDistillation Process: With multiple resolutions in the distillation process, have the authors considered channel-wise normalization? This could be crucial for stable training across different scales.\nArchitecture Details: The transition from student model input (P=1600) to F_student (C_4=384) needs clarification. How exactly does the downsampling process work?\nLoss Function: What additional information does the Gram matrix provide beyond MSE loss? The motivation for this choice needs better justification.\n\n3. Experimental Evaluation\n\nComparison with Related Work: While the related work section covers relevant literature, the paper lacks clear comparison showing how the proposed method improves upon existing approaches. Quantitative comparisons with state-of-the-art methods are essential.\nAblation Study: The ablation study only examines individual components but misses the impact of different hyperparameters. This limits understanding of the method's sensitivity and robustness.\n\nMinor Issues\n\nWriting Quality: The paper would benefit from careful proofreading. Several sentences are unclear or grammatically incorrect.\nFigure Quality: Ensure all figures are clearly labeled and referenced in the text."}, "questions": {"value": "1)How does DistillMatch specifically improve upon existing cross-modal matching methods quantitatively?\n2)What is the computational overhead of the multi-resolution distillation process? This comes with respect to GAN which introduces an additional computational overhead ? \n3)Have you tested the method on other modality pairs beyond visible-infrared?\n4)What happens to performance when V2I-GAN augmentation is removed entirely? What about the aspect of alternate augmentations like Autoaugment or so"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XzDUDNVOUC", "forum": "64xbSPBy31", "replyto": "64xbSPBy31", "signatures": ["ICLR.cc/2026/Conference/Submission7134/Reviewer_Bn9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7134/Reviewer_Bn9Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765706621, "cdate": 1761765706621, "tmdate": 1762919298298, "mdate": 1762919298298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DistillMatch, a novel method for multimodal image matching designed to overcome the inherent differences between modalities and the prevalent scarcity of high-quality annotated datasets. The core innovation lies in utilizing knowledge distillation from a Vision Foundation Model (VFM) to train a lightweight student model, coupled with specialized modules to manage modality-specific information and enhance feature aggregation. Extensive experiments demonstrate that DistillMatch significantly outperforms existing state-of-the-art algorithms across various matching tasks and exhibits strong zero-shot generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Usage of vision foundation models like Dinov2 to guide knowledge distillation process for lightweight student model through multi-component loss.\n* Retaining modality specific information through Category-Enhanced feature guidance module.\n* Robust feature integration through STFA module relying on both channel and spatial attention aggregation.\n* Data augmentation strategy for visible to infrared image translation through V2I-GAN framework\n* State of the art results across multiple benchmarks for relative pose estimation, homography transformation estimation."}, "weaknesses": {"value": "* How is the quality of the synthetic infrared data evaluated ? Further, how does the quality of synthetic data vary with the inclusion of recent diffusion models e.g. DiffV2IR (https://arxiv.org/pdf/2503.19012) ? \n*In terms of the training data, what fraction of samples were obtained through synthetic data augmentation ? \n* The ablation studies associated with the different components of the knowledge distillation loss (LKD) i.e. Eq 4 are not included in the Experiments section."}, "questions": {"value": "* In terms of KD based supervision, have the authors considered other VFM models like Radio (https://arxiv.org/abs/2312.06709) for feature extraction ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8pXOKqjK3j", "forum": "64xbSPBy31", "replyto": "64xbSPBy31", "signatures": ["ICLR.cc/2026/Conference/Submission7134/Reviewer_Vuwx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7134/Reviewer_Vuwx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285916421, "cdate": 1762285916421, "tmdate": 1762919297925, "mdate": 1762919297925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}