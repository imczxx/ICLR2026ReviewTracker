{"id": "I7yEcL5DWi", "number": 10534, "cdate": 1758174864696, "mdate": 1759897644990, "content": {"title": "Rethinking Consistent Multi-Label Classification under Inexact Supervision", "abstract": "Partial multi-label learning and complementary multi-label learning are two popular weakly supervised multi-label classification paradigms that aim to alleviate the high annotation costs of collecting precisely annotated multi-label data. In partial multi-label learning, each instance is annotated with a candidate label set, among which only some labels are relevant; in complementary multi-label learning, each instance is annotated with complementary labels indicating the classes to which the instance does not belong. Existing consistent approaches for the two paradigms either require accurate estimation of the generation process of candidate or complementary labels or assume a uniform distribution to eliminate the estimation problem. However, both conditions are usually difficult to satisfy in real-world scenarios. In this paper, we propose consistent approaches that do not rely on the aforementioned conditions to handle both problems in a unified way. Specifically, we propose two unbiased risk estimators based on first- and second-order strategies. Theoretically, we prove consistency w.r.t. two widely used multi-label classification evaluation metrics and derive convergence rates for the estimation errors of the proposed risk estimators. Empirically, extensive experimental results validate the effectiveness of our proposed approaches against state-of-the-art methods.", "tldr": "We propose a general and consistent framework for multi-label learning under inexact supervision.", "keywords": ["Multi-label classification", "partial multi-label learning", "complementary multi-label learning."], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0630e588f54c708d0d55b650c9aa18303e5dad13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a unified framework, COMES, for consistent multi-label classification under inexact supervision, targeting both Partial Multi-Label (PML) and Complementary Multi-Label (CML) learning. The authors aim to overcome the limitations of existing methods, which rely on either estimating the complex label generation process or adopting a uniform distribution assumption. The core contribution is a new data generation assumption—that true negatives are marked as non-candidates with a constant, instance-independent probability—which allows for a new approach. Based on this premise, the paper derives unbiased risk estimators (and their subsequent corrected, consistent versions) for two widely used metrics: the Hamming loss (COMES-HL, a first-order strategy) and the Ranking loss (COMES-RL, a second-order strategy). The authors provide theoretical guarantees for the consistency and estimation error bounds of their proposed estimators. Empirically, the framework is validated on ten benchmark datasets, where it is shown to outperform current state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper is well-structured and clearly articulates its approach (COMES) as a unified solution for both PML and CML problems.\n2.  It introduces a new data generation assumption that avoids the common pitfalls of transition matrix estimation or uniform distribution assumptions.\n3.  The proposed methods are supported by both theoretical guarantees and extensive empirical validation."}, "weaknesses": {"value": "1. There is a contradiction between the method's motivation and its empirical results. The second-order strategy (COMES-RL), which was introduced specifically to model label correlations, paradoxically performs significantly _worse_ than the first-order strategy (COMES-HL) on datasets with strong label correlations (e.g., CUB and COCO), an inconsistency the authors do not address.\n    \n2. The paper's claim of proposing \"unbiased risk estimators\" is misleading. The practical estimators (Eq. 8, 14) used in the algorithm are \"corrected\" versions that are _biased_ (to avoid overfitting from the original unbiased forms).\n    \n3. The method's reliance on accurate class-prior estimation ($\\pi_j$) is a critical vulnerability."}, "questions": {"value": "See in weekness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4rgjyufnnf", "forum": "I7yEcL5DWi", "replyto": "I7yEcL5DWi", "signatures": ["ICLR.cc/2026/Conference/Submission10534/Reviewer_2V1q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10534/Reviewer_2V1q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620281914, "cdate": 1761620281914, "tmdate": 1762921815334, "mdate": 1762921815334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed the consistent approaches to handle both partial label and complementary multi-label problems in a unified way, with two unbiased risk estimators based on first- and second-order strategies. The theoretical work is elegant and self-contained. In addition, the empirical study largely validates the effectiveness of the proposed approaches."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper innovatively transforms the PML problem into a negative-unlabeled (NU) learning problem through a carefully derived loss function, providing a clear and elegant theoretical perspective. \n2. The theoretical analysis is rigorous, with well-stated assumptions, formal consistency proofs, and convergence rate derivations. \n3. The empirical evaluation is thorough, covering both standard benchmarks and additional real-world datasets, which enhances the credibility and generality of the proposed method. \n4. The paper is well-motivated and clearly written, with smooth logical flow and sound reasoning connecting problem definition, theory, and empirical validation."}, "weaknesses": {"value": "1. The derivation in Lemma 1 heavily relies on assumptions, which may be difficult for readers to intuitively grasp. I would suggest providing more intuitive insights or illustrative examples to clarify the underlying rationale of the lemma and its implications for the overall theoretical framework. \n2. The rank loss based on the second-order strategy appears to primarily capture the gap between the ground-truth and non-ground-truth labels, but not the relevance among ground-truth labels themselves. It would be interesting to discuss whether a rank loss can be designed to simultaneously model both the inter-ground-truth relevance and the ground-truth and non-ground-truth gap. \n3. In the experiments on synthetic benchmark datasets, the differences between case-a and case-b are not clearly analyzed. It would strengthen the empirical section if the authors could elaborate on the motivation for setting up these two distinct cases, and provide a detailed discussion of the respective findings and their implications. \n4. Although this paper focuses on weakly supervised multi-label learning, the proposed loss formulations and theoretical derivations seem readily transferable to weakly supervised multi-class settings, such as partial label learning and complementary label learning. A discussion along with potential preliminary experiments on this broader applicability would further highlight the generality and impact of this paper."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mBXSqsPfEx", "forum": "I7yEcL5DWi", "replyto": "I7yEcL5DWi", "signatures": ["ICLR.cc/2026/Conference/Submission10534/Reviewer_fd9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10534/Reviewer_fd9d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668251275, "cdate": 1761668251275, "tmdate": 1762921814814, "mdate": 1762921814814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the partial/complementary multi-label learning problem. By proposing a consistent multi-label classification\nunder inexact supervision framework called COMES, this paper designs two risk-consistent estimators w.r.t. two classic multi-class classification losses, i.e., the Hamming loss and the ranking loss. Compared with previous works, the proposed estimators neither require estimating the generation process of candidate or complementary labels nor rely on the uniform-distribution assumption. In the theoretical parts, this paper derives the generalization bounds for the proposed estimators (COMES-HL and COMES-RL). In experiments, the proposed estimators are evaluated on both real-world and synthetic PML benchmark datasets, achieving lower errors and higher average precision compared with previous methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is overall well written and easy to follow.\n- The proposed methods are theoretically inspired and proved to be consistent. Although I did not check every detail in the proof, the theoretical results seem sound and reasonable.\n- The proposed methods neither require estimating the generation process of candidate or complementary labels nor rely on the uniform-distribution assumption, which is a significant advancement compared to existing methods."}, "weaknesses": {"value": "**Major**\n- In Theorem 2, if the non-negative $\\alpha=0$, according to (9), the estimator is inconsistent because the bound becomes independent of $n$. For example, if the classification problem is easy enough so that $g_j(x)$ predicts every $y$ exactly, then theoretically, we have $\\pi_j\\mathbb{E}\\_{p(x|y_j=1)} [\\ell(g_j(x),1)]=0$. Nonetheless, I believe that the estimator can still be proved consistent in such a corner case, because the classification problem becomes very easy now. I hope the authors could discuss this case in further detail so that the theoretical results can be more rigorous and complete.\n- Similarly, in Theorem 5, it is also possible that $\\gamma=0$ for easy classification problems, making the bound independent of $n$.\n- Another of my major concerns is about the fair comparison in the experiment section. In Figure 2, the inaccurate class priors affect the performance of COMES. Even for the relatively robust COMES-RL, under a slight noise $\\theta=0.1$, the average precision drops unacceptably. For example, on mirflickr, mAP drops from 0.818 to approximately 0.80, and on music_style, mAP drops from 0.732 to below 0.70, which makes COMES-RL perform worse than many of the compared methods in Table 2. Therefore, I wonder if these compared methods also use the true class priors in Table 2. If so, from my perspective, an additional comparison under estimated class priors may be more persuasive.\n\n**Minor**\n- In Eq.(8), an absolute value function is used to prevent overfitting. Further explanations are needed to elaborate on how and why this approach works. \n- In Section 3.1, the generation procedure of non-candidate labels is class-dependent instead of instance-dependent. I think this point should be emphasized in the context.\n- Line 250. \"$\\sup\\_{g_i \\in \\mathcal{G}}\\\\|g\\\\|_\\infty$\". Should it be \"$\\sup\\_{g_i \\in \\mathcal{G}}\\\\|g_i\\\\|\\_\\infty$\"?"}, "questions": {"value": "- In Eq. (13), there is no negative loss term to induce overfitting. Why is the flooding regularization technique necessary?\n- Why do Eq. (8) and Eq. (14) use different strategies to avoid overfitting?\n- Just out of curiosity, why are the proposed methods named first-order and second-order strategies when no derivatives are involved?\n- How do the proposed methods perform under estimated class priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hQ6T4vYZDY", "forum": "I7yEcL5DWi", "replyto": "I7yEcL5DWi", "signatures": ["ICLR.cc/2026/Conference/Submission10534/Reviewer_Yr8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10534/Reviewer_Yr8e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756896944, "cdate": 1761756896944, "tmdate": 1762921814393, "mdate": 1762921814393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the COMES framework for partial/complementary multi‑label learning (as the paper points out, these settings are formally equivalent). The paper focuses on deriving two new unbiased losses for Hamming loss (COMES‑HL) and ranking loss (COMES‑RL) under the setting with the assumption that each label has a different but constant (not instance-dependent) probability of being in the candidate set, while being in fact irrelevant for the sample. The authors prove consistency with finite‑sample bounds for both derived losses and perform experiments on six real‑world PML datasets and four synthetic ones, and compare against 5 different algorithms across Hamming loss, ranking loss, one‑error,  coverage, and AP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper sounds and is easy to read.\n- Clear theoretical contribution: unbiased estimators with finite‑sample bounds for both Hamming and ranking loss.\n- Useful relaxation of assumptions compared to previous methods."}, "weaknesses": {"value": "- The biggest weakness of experiments is that they assume the priors are known, but the problem of estimating priors is, in this case, complex, and the problem may not be identifiable in some cases.\n- It is not clear what dataset is used for Figures 2 and 3; comparison with baselines could also be added there.\n- While new losses relax the assumption on uniform distribution, calling them general is an overstatement for me, as obviously, the assumption on constant p_j is still strong and likely untrue in many real-world cases.\n- \"This data generation process coincides well with the annotation process of candidate labels. For example, when asking annotators to provide candidate labels for an image dataset, we\ncan show them an image and a class label and ask them to determine whether the image is irrelevant\nto that class. This is often an easier question to answer than directly asking all relevant labels, since\nit is less demanding to exclude some obviously irrelevant labels.\" - depends, if there is a lot of labels, is it really better to list irrelevant ones? Not sure about that, but I recall that datasets used in experiments were actually created using crowdsourcing, and candidate label sets were created by taking the union of all assigned label sets. Are there any datasets created in this manner?\n- Experiments lack a good description of datasets and baseline methods (also in the appendix).\n\"We evaluate against five classical baselines commonly used in PML/CML learning.\" - Only classical? None of it is SOTA? What about the assumption these methods are using, do they also require priors? Also, what worries me is that from those baselines, the simplest BCE performs the best most of the time. This makes me question baseline choice and correctness of presented experiments.\nA comment how assumptions of the method match data annotations of benchmark datasets would be nice. Also, are all the real datasets indeed real? I might be wrong, but I think some of them are actually created synthetically (yeast ones?). Limited information on data splits/repetitions etc.\n- NIT: There are multiple metrics in MLC called \"coverage\", I assume the authors use the minimal ranking coverage metric here because the stated lower is better, but it would be nice to have metrics defined in the appendix.\n- NIT: It's not clear from the main text what is Case-a and Case-b in Table 3\n- NIT: When I use the code link, for every file I select, it says: The requested file is not found. Basically, the code is not accessible at the moment of writing this review."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o6wX0gVgp4", "forum": "I7yEcL5DWi", "replyto": "I7yEcL5DWi", "signatures": ["ICLR.cc/2026/Conference/Submission10534/Reviewer_oQbS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10534/Reviewer_oQbS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900485675, "cdate": 1761900485675, "tmdate": 1762921813798, "mdate": 1762921813798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addressed the Partial and Complementary multi-label learning in a unified way. The authors proposed unbiased risk estimators for hamming and ranking loss under inexact supervision, and provided consistency convergence guarantees. In comparison to existing approaches, the proposed method does not rely on accurate estimation of data generation process. They also showed the effectiveness of the framework through empirical results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The problem of PML and CML have been addressed separately but this paper provides a unified framework to address the two settings. The authors proposed the method which generalize the label generation process (instead of naively assuming the process) for risk estimators.\n\nQuality: The methodology demonstrates technical soundness. The assumptions and lemmas have extensive proofs and details. Based on the experimental section, the method seems to perform well empirically too.\n\nClarity: The paper is mainly easy to follow and has proper motivation of the problem setting. \n\nSignificance: The paper makes substantial contributions to the area of multi-label learning. I think the results are significant (the theoretical and experimental, both)."}, "weaknesses": {"value": "1. Although the method is more general than prior work, it depends on a strong assumption used in Lemma 1, which results in the independence of the data generation process from the samples. (Authors mentioned this in the paper as well)\n2. I am particularly focused on the result of the Lemma that $p(x|s_j = 0) = p(x|y_j = 0)$. This assumption seems to be a very strong signal for negative labels. This implies a perfect annotation process for 'irrelevant' labels. The proof of the lemma uses another assumption: $p(j \\notin Y | x, s_j = 0) =p(j \\notin Y | s_j = 0). $, which has not been explained in the paper. \n3. The experimental section could be improved a lot. The authors have done experiments but the rationale behind case A and case B is not mentioned. Why choose this particular method of generating the labels and why not some other method (or different \\tau)? \n4. For the effectiveness of the framework, and the impact of the data generation process, there is a need for more experiments. What would be the effect of changing the dataset size for training?"}, "questions": {"value": "1. In figure 2, the experiments are done with different sigmas for class priors. Why not actually modify the flip rate in case A for example, and then evaluate?\n2. The average precision of mirflickr stays the same as you increase sigma. Why? \n3. This paper consider data generation process independent from the samples. Shouldn’t MLCL perform better because it is estimating the data generation process from the samples? Can you elaborate? Because I would assume that estimating the data generation process based on the knowledge of the actual samples should be better than independent data generation process (in this case, flipping the negative labels to positive labels with prob 0.9). \n4. Theorem 4 assumes that $l$ is symmetric but in reality, the used loss binary cross entropy, is not symmetric. Am I missing something?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8drKhebgIW", "forum": "I7yEcL5DWi", "replyto": "I7yEcL5DWi", "signatures": ["ICLR.cc/2026/Conference/Submission10534/Reviewer_aB8Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10534/Reviewer_aB8Q"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971808382, "cdate": 1761971808382, "tmdate": 1762921812755, "mdate": 1762921812755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses multi-label classification under inexact supervision task, proposing a unified framework COMES for partial multi-label learning (PML) and complementary multi-label learning (CML). Existing methods require estimating label generation processes or rely on uniform distribution assumptions. The paper assumes that candidate labels are generated by querying whether an instance is irrelevant to each class, and designs first-order and second-order unbiased risk estimators. The first-order strategy decomposes the problem into multiple binary classification problems using Hamming loss, while the second-order strategy takes label correlations into account using ranking loss. Theoretically, the paper proves consistency and derives convergence rates for both Hamming and ranking losses, improving generalization through absolute value wrapping and flooding regularization. Experiments validate the effectiveness on six real-world and four synthetic datasets, with COMES significantly outperforming baselines including CCMN, GDF, CTL, and MLCL in most cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe unified perspective on treating PML and CML as equivalent problems is interesting.\n2.\tThis paper not only proves consistency of risk estimators but also derives convergence rates for estimation errors. It establishes generalization bounds by Rademacher complexity analysis, and proves that minimizing the corrected risk estimator can achieve Bayes risk.\n3.\tThis paper covers different types of datasets (images, audio, biological information, etc.) with reasonable settings. Sensitivity analyses evaluate the impact of inaccurate class priors and hyperparameter β, and ablation studies validate the necessity of each module."}, "weaknesses": {"value": "1.\tAlgorithm 1 contains an obvious error in line 8 of the pseudocode. The conditional branch \"else if using the COMES-HL algorithm then\" should be changed to COMES-RL rather than COMES-HL. \n2.\tThis paper provides a detailed introduction to first-order and second-order strategies and validates their effectiveness through experiments. However, it does not explain how to reasonably select or combine first-order and second-order strategies in practical applications."}, "questions": {"value": "1.\tIs the conditional branch error in Algorithm 1, line 8 a typesetting issue or an actual error in the code? Does this error exist in the algorithm implementation, or is it a description error in the main text?\n2.\tWhat is the reason for using different network architectures (MLP and ResNet-50) in experiments? Does this design affect the fairness and comparability of experimental results?\n3.\tHow should first-order and second-order strategies be chosen in practical applications? Has the author considered designing a mechanism to automatically select one strategy or a weighted combination of the two strategies based on dataset characteristics, such as label correlation strength or dataset size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6HXFijkWAp", "forum": "I7yEcL5DWi", "replyto": "I7yEcL5DWi", "signatures": ["ICLR.cc/2026/Conference/Submission10534/Reviewer_bJe9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10534/Reviewer_bJe9"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission10534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983269869, "cdate": 1761983269869, "tmdate": 1762921812397, "mdate": 1762921812397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}