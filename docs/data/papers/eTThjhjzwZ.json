{"id": "eTThjhjzwZ", "number": 22656, "cdate": 1758334123619, "mdate": 1759896854433, "content": {"title": "Trajectory Generation with Conservative Value Guidance for Offline Reinforcement Learning", "abstract": "Recent advances in offline reinforcement learning (RL) have led to the development of high-performing algorithms that achieve impressive results across standard benchmarks. However, many of these methods depend on increasingly complex planning architectures, which hinder their deployment in real-world settings due to high inference costs. To overcome this limitation, recent research has explored data augmentation techniques that offload computation from online decision-making to offline data preparation. Among these, diffusion-based generative models have shown potential in synthesizing diverse trajectories but incur significant overhead in training and data generation.\nIn this work, we propose Trajectory Generation with Conservative Value Guidance (TGCVG), a novel trajectory-level data augmentation framework that integrates a high-performing offline policy with a learned dynamics model. To ensure that the synthesized trajectories are both high-quality and close to the original dataset distribution, we introduce a value-guided regularization during the training of the offline policy. This regularization encourages conservative action selection, effectively mitigating distributional shift during trajectory synthesis.\nEmpirical results on standard benchmarks demonstrate that TGCVG not only improves the performance of state-of-the-art offline RL algorithms but also significantly reduces training and trajectory synthesis time. These findings highlight the effectiveness of value-aware data generation in improving both efficiency and policy performance.", "tldr": "We propose TGCVG, a simple yet effective offline RL framework that synthesizes high-quality trajectories via conservative value-guided generation.", "keywords": ["Reinforcement Learning", "Sequential Decision Making"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26474c702312633e4deae519d1e0bc27f03e2d1e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes TGCVG, a novel generative data augmentation method designed for offline RL algorithms. TGCVG tackles two limitations of prior works: computational overhead and distribution shift. TGCVG trains Transformer instead of Diffusion models and applies conservative value guidance to mitigate those issues. Experiment results show TGCVG outperforms prior baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is easy to follow\n- Limitation of prior works and How to mitigate the limitations are clearly stated\n- Strong empirical results against several baselines across diverse tasks"}, "weaknesses": {"value": "- It seems that $\\lambda$ and $K$ should be heavily tuned across different environments and datasets. I'm not sure the method can be generalized to unseen tasks without extensive hyperparameter tuning, which is crucial in offline RL.\n\n- It would be better to add ablation studies on different sizes of the dataset, which is crucial for evaluating the performance of generative data augmentation.\n\n- It would be better to add ablation studies on different sizes of the generated dataset, which is also crucial for evaluating the performance of generative data augmentation. I'm also struggling to find how many transitions are augmented during the training."}, "questions": {"value": "- It would be better to specify the number of generated transitions for each round, the ratio of transitions from the original offline dataset and the generated dataset during policy training, and the update-to-data (UTD) ratio for a clearer understanding.\n\n- It would be better to visualize t-SNE visualization of generated data distributions on diverse tasks for more comprehensive understanding of the behavior.\n\n- At first, I understood the conservative value guidance as using conservative RTG values for generating trajectories with the Transformer policy. However, it seems that we train Transformer-CQL and directly use the network for generating an augmented dataset. Is there any reason why the current method is preferred over the aforementioned method? I'm not asking about the experiment, just curious. I think we can achieve similar results when we carefully tune the hyperparameter for choosing a conservative RTG value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8pe5LmdfoX", "forum": "eTThjhjzwZ", "replyto": "eTThjhjzwZ", "signatures": ["ICLR.cc/2026/Conference/Submission22656/Reviewer_Cmj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22656/Reviewer_Cmj7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552788485, "cdate": 1761552788485, "tmdate": 1762942323691, "mdate": 1762942323691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel data augmentation method for offline RL, TGCVG, using the transformer-based augmentation policy with the dynamics model. In contrast to diffusion-based augmentation methods such as GTA [1], TGCVG incurs lower overhead during both training and dataset augmentation. Using the TGCVG-augmented dataset, offline RL policies achieved better performance than with any other augmentation baseline.\n\n[1] Lee, Jaewoo, et al. \"Gta: Generative trajectory augmentation with guidance for offline reinforcement learning.\" Advances in Neural Information Processing Systems 37 (2024): 56766-56801."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well motivated\n\nThe paper is well motivated and clearly constructed. Without increasing the complexity of the policy algorithm, TGCVG improves the performance of offline RL policies by generating high-quality data guided by the conservative Q-function.\n\n2. Improved efficiency for training and augmentation\n\nBecause the augmentation process is fully offline, its impact on decision-time efficiency is limited; nevertheless, Figure 4 indicates that TGCVG is 4× more efficient for training and augmenting the offline dataset.\n\n3. Fresh insight about the conservatism quality of TD3BC and CQL\n\nThe authors show that Transformer-CQL generates in-distribution samples, whereas Transformer-TD3BC does not; consequently, the CQL policy cannot learn a conservative policy from Transformer-TD3BC’s augmented data."}, "weaknesses": {"value": "1. Concerns about the hyperparameter sensitivity\n\nWhile the authors conduct an ablation study on $\\lambda$, they vary its value from 0.1 to 1, suggesting that TGCVC requires extensive hyperparameter tuning. In addition, Table 6 lists another hyperparameter, $K$, but lacks an ablation, further increasing the tuning burden for practitioners.\n\n\n2. Results on larger-scale benchmarks would be beneficial\n\nRecently, the scalability of offline RL has emerged as an important theme [2]. OGBench [3] provides a diverse suite of offline tasks with higher-dimensional state spaces and larger datasets. I understand the rebuttal window limits time and compute, but if TCGVC were corroborated by strong results on OGBench, its impact and practical value would be substantially enhanced.\n\n[2] Park, Seohong, et al. \"Horizon Reduction Makes RL Scalable.\" arXiv preprint arXiv:2506.04168 (2025).\n\n[3] Park, Seohong, et al. \"OGBench: Benchmarking Offline Goal-Conditioned RL.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "Because the hyperparameters are the pair $(\\lambda, K)$, the search space is combinatorial. To make offline RL algorithms viable in the real world, where online interaction is unavailable, we must be able to select good hyperparameter combinations purely offline; hence, simplicity in offline RL methods is crucial [4]. I therefore argue that a thorough practical guideline for joint tuning of $(\\lambda, K)$ is essential for this paper.\n\n[4] Fujimoto, Scott, and Shixiang Shane Gu. \"A minimalist approach to offline reinforcement learning.\" Advances in neural information processing systems 34 (2021): 20132-20145."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XrXHWCf45V", "forum": "eTThjhjzwZ", "replyto": "eTThjhjzwZ", "signatures": ["ICLR.cc/2026/Conference/Submission22656/Reviewer_KvRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22656/Reviewer_KvRm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830002100, "cdate": 1761830002100, "tmdate": 1762942323419, "mdate": 1762942323419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TGCVG (Trajectory Generation with Conservative Value Guidance), a novel data augmentation framework for offline reinforcement learning (Offline RL). TGCVG consists of transformer based policy network trained using Conservative Q-learning(CQL), learned dynamics model that predicts next states and rewards to generate synthetic transitions. TGCVG guides generation process with conservative value guidance, keeping them close to data distribution while favoring high-value regions. The augmented trajectories are then mixed with the original dataset and used to train standard offline RL. Extensive experiments on D4RL benchmarks (MuJoCo, Maze2D, AntMaze) show that TGCVG improves baseline performance across domains. And produces high-quality, dynamically consistent synthetic data according to novelty, optimality, and dynamic MSE metrics.\n- An LLM was used to improve writing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Low computational cost: The proposed method achieves significantly lower training and data-generation overhead compared to diffusion-based approaches such as GTA — up to 10× faster in both stages.\n\n2. Strong empirical performance: Comprehensive evaluation on D4RL benchmark tasks demonstrates consistent and robust performance gains against a wide range of baselines.\n\n3. Clarity and presentation: The paper is well-written and well-organized, with clear motivation, sound theoretical reasoning, and a coherent algorithmic presentation that makes the method easy to follow."}, "weaknesses": {"value": "1. Lack of evaluation on high-dimensional or robotics domains: The paper does not include results on more complex D4RL tasks such as Adroit or Kitchen, which are higher-dimensional and closer to real-world robotics scenarios.\n\n2. Ablation on value guidance and policy choice: It would be valuable to see how performance changes with or without the conservative value guidance, or when using Decision Transformer (DT) as the policy for augmentation.\n\n3. Missing experimental details: In Table 3, results for HalfCheetah are not presented, and experiments where Transformer-CQL itself serves as the policy appear insufficiently explored."}, "questions": {"value": "Could the authors explicitly define the metrics used for novelty, optimality, and dynamic MSE?\nFor example, novelty seems to be measured via L2 distance, but is this metric appropriate given the domain characteristics of the datasets? A short justification or alternative metric discussion would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PkAnc6eu9u", "forum": "eTThjhjzwZ", "replyto": "eTThjhjzwZ", "signatures": ["ICLR.cc/2026/Conference/Submission22656/Reviewer_kHAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22656/Reviewer_kHAm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953996054, "cdate": 1761953996054, "tmdate": 1762942323162, "mdate": 1762942323162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TGCVG, a framework that combines CQL transformer with a dynamics model to generate high-quality, in-distribution trajectories for offline reinforcement learning.\nBy guiding trajectory synthesis with conservative value estimates, TGCVG improves both stability and sample efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important problem in offline reinforcement learning—how to generate high-quality trajectories that remain in-distribution to improve policy learning stability.\n\n2. The proposed TGCVG framework is both novel and solid, effectively leveraging conservative Q-learning to guide trajectory generation and ensure distributional consistency.\n\n3. The experimental results are strong and impressive, showing clear and consistent improvements across multiple D4RL benchmarks.\n\n4. The paper is well-written and clearly presented, making the motivation, methodology, and findings easy to follow."}, "weaknesses": {"value": "1. The experimental section could be more comprehensive. In particular, an ablation study comparing conservative trajectory generation with standard generation would help clarify the effectiveness of the proposed mechanism.\n\nSee quetions below."}, "questions": {"value": "1. How is the dynamics model trained in this work? Please clarify the training objective, data source, and other details.\n\n2. How does the model handle terminal indicators when generating or evaluating trajectories? This part is not clearly presented in the main paper.\n\n3. In Table 2, why does SynthER perform worse than the version without augmentation? \n\n4. Can the authors provide more explanation on why restricting trajectories to be in-distribution leads to better performance, especially when the dataset contains medium-expert level data because in-distribution seems to bring similar information with the original dataset.\n\n5. Have the authors tested TGCVG on visual RL environments to examine whether the proposed framework generalizes beyond state-based tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7CpfEbI62r", "forum": "eTThjhjzwZ", "replyto": "eTThjhjzwZ", "signatures": ["ICLR.cc/2026/Conference/Submission22656/Reviewer_mtk1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22656/Reviewer_mtk1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970907365, "cdate": 1761970907365, "tmdate": 1762942322769, "mdate": 1762942322769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}