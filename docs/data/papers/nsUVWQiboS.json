{"id": "nsUVWQiboS", "number": 5689, "cdate": 1757927360163, "mdate": 1759897960401, "content": {"title": "SyNC: Balancing Fidelity and Diversity of Synthetic Data Representations in CLIP-based Few-Shot Learning via Neural Collapse", "abstract": "In few-shot learning, augmenting real data with synthesized images from text-to-image diffusion models has emerged as a promising direction. Although numerous studies have been proposed to improve the performance of this training framework, they often fail to adequately address the critical trade-off between fidelity and diversity when training with synthetic data. In this work, we propose SyNC, a novel training paradigm that explicitly balances these characteristics in the feature space through two complementary mechanisms. First, we leverage an optimal geometric prototype structure built upon the Neural Collapse phenomenon to increase fidelity, guiding the representations of both real and synthetic data toward their corresponding equiangular tight frame (ETF) prototypes. Second, we introduce an innovative regional contrastive loss function specifically designed to enhance diversity by improving the distinction between misclassified synthetic data features, thereby encouraging more varied and robust representations. Extensive experimental results demonstrate the effectiveness of our proposed method, which outperforms state-of-the-art approaches on average across few-shot image classification benchmarks and shows significant improvements on fine-grained datasets. Further analysis demonstrates that our method achieves a more favorable balance between representation fidelity and diversity, revealing a correlation between these factors and overall model performance.", "tldr": "We propose a novel training algorithm for few-shot learning with synthetic data, balancing the model representation fidelity and diversity.", "keywords": ["few-shot learning", "synthetic data", "neural collapse"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20148c8b5902203177588e8c7a70df2c167fa4f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SyNC, a new training paradigm for CLIP-based few-shot learning that explicitly balances the fidelity and diversity of synthetic data representations via two complementary loss functions. The authors identify a key trade-off in existing methods, which often optimize for one aspect at the expense of the other.  Extensive experiments on 10 few-shot image classification benchmarks demonstrate that SyNC achieves a new state-of-the-art average accuracy, with particularly significant improvements on fine-grained datasets like FGVC Aircraft and Stanford Cars. Ablation studies and analysis of alignment/uniformity metrics confirm the contribution of each component and the method's ability to better balance fidelity and diversity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. $\\textbf{Originality}$: This paper demonstrates a degree of originality, primarily through the creative combination of established ideas from disparate fields to address a timely and well-defined problem.\n\n2. $\\textbf{Quality}$: This paper exhibits good quality in its methodological execution and empirical validation.\n\n3. $\\textbf{Clarity}$: This paper is generally well-written and structured, making a technically complex approach accessible."}, "weaknesses": {"value": "1.$\\textbf{Dependence on Synthetic Data Quality}$: The entire framework is contingent on the quality of the initial synthetic data generated by the fine-tuned Stable Diffusion model. While the method is designed to be robust, there is no analysis of how its performance degrades with lower-quality generators or noisier synthetic data.\n\n2.$\\textbf{Superficial Theoretical Exploration of NC in FSL}$: While the use of Neural Collapse is a strength, the connection remains somewhat applied. A deeper theoretical discussion on why NC is a suitable inductive bias for the few-shot setting, or an analysis of whether NC actually emerges under the proposed losses, would elevate the contribution further."}, "questions": {"value": "1. How sensitive is SyNC to the quality of the synthetic data? For instance, if you were to use a weaker generative model or reduce the number of generated samples per class, how would the performance of SyNC compare to the baselines?\n\n2. In the RSC loss, how is the \"region\" or cluster membership $r_x$ precisely defined? Is it based on a standard clustering algorithm like K-means? How sensitive is the performance to the choice of the number of clusters (stated as 2N), and was this hyperparameter ablated?\n\n3. The method uses GPT-4 to generate enriched class descriptions. How critical is this step? Would the technique still achieve significant gains using only the naive \"a photo of a [CLS]\" prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GgOk9Qx5ty", "forum": "nsUVWQiboS", "replyto": "nsUVWQiboS", "signatures": ["ICLR.cc/2026/Conference/Submission5689/Reviewer_YwNC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5689/Reviewer_YwNC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760859980362, "cdate": 1760859980362, "tmdate": 1762918196617, "mdate": 1762918196617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SyNC, a new paradigm that explicitly balances \"synthetic feature fidelity vs. diversity\" during the small-sample fine-tuning phase of CLIP. The key contributions are:\n\n(1) The introduction of Neural Collapse (NC) theory into the small-sample synthetic data scenario, with the proposal of ETF prototype alignment loss ($L_{ETF}$);\n\n(2) The design of regional supervised contrastive loss ($L_{RSC}$), which applies greater force to \"misclassified synthetic samples\" to enhance inter-class distance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) This is application of the Neural Collapse ETF structure to synthetic data training in few-shot learning. By dynamically aligning prototypes using the Kabsch algorithm, it enhances the authenticity of the representations.\n\n(2) Two complementary loss functions are proposed: the ETF loss ($L_{ETF}$) for authenticity and the regional contrastive loss ($L_{RSC}$) for diversity, addressing the limitation of existing methods that favor one over the other."}, "weaknesses": {"value": "(1) The premise of this paper is based on the existence of the NC phenomenon, but the four key assumptions of NC (large batch size, class balance, sufficient training, and real data) are all violated in the small-sample + synthetic regime. The authors use \"ETF prototypes\" as an inductive bias but do not provide any finite-sample convergence bounds or generalization error reduction theorems.\n\n(2) The premise of the article is that the NC phenomenon exists in CLIP, but the article lacks proof of the existence of the NC phenomenon in CLIP.\n\n(3) The overall idea of the article is very similar to the KDD 2024 paper \"Understanding Prompt Tuning for V-L Models Through the Lens of Neural Collapse.\" [1] The claimed originality of the article seems significantly diminished, and there is insufficient comparison and explanation.\n\n[1] Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Kun Kuang, and Chao Wu. 2024. Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24). Association for Computing Machinery, New York, NY, USA, 4631–4640. https://doi.org/10.1145/3637528.3671690"}, "questions": {"value": "**Problem:**\n\n(1) The premise of this paper relies on the existence of the Neural Collapse (NC) phenomenon; however, the four key assumptions of NC—large batch size, class balance, sufficient training, and real data—are all violated in the small-sample and synthetic regime employed. The authors propose \"ETF prototypes\" as an inductive bias but do not provide finite-sample convergence bounds or generalization error reduction theorems to support their approach.\n\n(2) The paper assumes the existence of the NC phenomenon in CLIP, but it lacks empirical proof or formal argumentation to substantiate this claim.\n\n(3) The overall idea of the article closely mirrors the KDD 2024 paper \"Understanding Prompt Tuning for V-L Models Through the Lens of Neural Collapse\" (Zhu et al., 2024). This similarity significantly diminishes the claimed originality of the paper, and there is insufficient comparison and explanation to clarify how the current work differs from the existing literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pJzrac9cmi", "forum": "nsUVWQiboS", "replyto": "nsUVWQiboS", "signatures": ["ICLR.cc/2026/Conference/Submission5689/Reviewer_HH2D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5689/Reviewer_HH2D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761289123879, "cdate": 1761289123879, "tmdate": 1762918195418, "mdate": 1762918195418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SyNC, a CLIP-based few-shot training paradigm that balances fidelity and diversity when leveraging synthetic data. It combines a Neural Collapse loss that draws real and synthetic features toward shared ETF prototypes with a regional supervised contrastive loss that pushes apart misclassified synthetic examples. The method integrates with standard fine-tuning and reports gains over strong baselines—especially on fine-grained benchmarks—with ablations linking the fidelity–diversity balance to accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the real–synthetic training gap by explicitly balancing fidelity and diversity, coupling an ETF/Neural-Collapse–style alignment loss with a region-wise supervised contrastive loss.\n2. The method is validated across multiple few-shot benchmarks and and ablations isolate each component."}, "weaknesses": {"value": "1. The method feels like an engineering combo (ETF + Kabsch + region-wise contrast) rather than a new mechanism. \n2. The average gain over ImagineFSL is small (~+0.4 pp) and mainly driven by FGVC-Aircraft, with several datasets underperforming; Table 1 also mixes configurations (Aircraft uses ETF-only while ETF+RSC is lower (Table 3)). \n3. The paper lacks ablations on the role and quality of generated descriptions (vs. class names, length/structure, noise). Please add targeted ablations and qualitative examples plus simple quality metrics (e.g., CLIPScore/perplexity).\n4. Many hyperparameters are introduced, yet the sensitivity plots don’t clearly link to the final gains; Kabsch brings little benefit except on FGVC-Aircraft. \n5. There is no empirical complexity comparison for fine-tuning (latency, peak memory), leaving the cost–accuracy trade-off unclear."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7KPSXrq2hh", "forum": "nsUVWQiboS", "replyto": "nsUVWQiboS", "signatures": ["ICLR.cc/2026/Conference/Submission5689/Reviewer_ijGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5689/Reviewer_ijGe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770519850, "cdate": 1761770519850, "tmdate": 1762918194485, "mdate": 1762918194485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SyNC, a new training paradigm for CLIP-based few-shot learning that aims to balance the fidelity and diversity of synthetic data representations. The method introduces two complementary losses:  \n(1) an ETF-based Neural Collapse loss to align real and synthetic features toward theoretically optimal geometric prototypes, thereby improving fidelity, and  \n(2) a regional supervised contrastive loss that enhances diversity by pushing apart misclassified synthetic features.  \nThe framework integrates these components with LoRA fine-tuning on both real and generated samples. Experiments on ten benchmark datasets show competitive or superior performance, especially on fine-grained datasets such as FGVC-Aircraft and Stanford Cars."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is conceptually clear and easy to understand, providing a well-motivated formulation of the fidelity–diversity trade-off in synthetic-data-based few-shot learning.  \n- It accurately identifies the core limitation of prior works—overemphasis on either fidelity or diversity—and proposes a theoretically grounded direction to address both simultaneously. \n- The integration of Neural Collapse theory into few-shot CLIP adaptation is novel and insightful, offering a principled geometric interpretation for feature alignment."}, "weaknesses": {"value": "Although the paper is highly intuitive, the experimental evidence does not convincingly validate its hypotheses:  \n  1. The presentation of results in the tables can be **misleading**, as only the proposed method’s results are boldfaced even when other methods achieve identical result (see Tables 1, 3, and 4).  \n  2. In Table 1, SyNC’s results are not significantly better than ImagineFSL, which contradicts the claim in Lines 59–62 about \"enhance diversity often pushes synthetic images further from the real data distribution\"\n  3. The ablation in Table 3 shows that the two proposed losses individually contribute marginal improvements on several datasets (e.g., IN, CAL, AirC, Pets, SUN, FLO). Furthermore, the paper’s justification in Section 3.1—“why not initialize ETF in a way better aligned with the input feature distribution?”—may be questionable, as the employed Kabsch algorithm merely aligns two point sets via least-squares rotation, functionally similar to random initialization followed by L2 fitting. This could explain the limited performance gains; thus, an error analysis would be necessary to substantiate the claimed benefits.  \n  4. The method involves too many hyperparameters (λ, λ₁, λ₂, ρ, β, τ), and the search ranges are inconsistent. For instance, the recommended range for λ₁ is [0, 1], yet in FGVC-Aircraft it is set to 15, far beyond the provided interval, suggesting a lack of systematic tuning."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x4qDRefkD3", "forum": "nsUVWQiboS", "replyto": "nsUVWQiboS", "signatures": ["ICLR.cc/2026/Conference/Submission5689/Reviewer_qjUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5689/Reviewer_qjUR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842380428, "cdate": 1761842380428, "tmdate": 1762918194105, "mdate": 1762918194105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}