{"id": "jR8HV4uTcf", "number": 18044, "cdate": 1758283245357, "mdate": 1759897136869, "content": {"title": "Exploring the Design Space of Transition Matching", "abstract": "Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller \"head\" module to efficiently execute the generative transition step.\nIn this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics.\nLastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.", "tldr": "", "keywords": ["flow matching", "transition matching", "generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a59f59dc03d8c29a79dd6092fbe587e264214b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a large-scale and systematic investigation into the design, training, and sampling of the head module in the continuous-time bidirectional variant of Transition Matching (TM) for generative modeling, with a focus on text-to-image generation. The work addresses a notable gap in existing literature— the lack of in-depth exploration of the head module’s architecture and hyperparameters— despite its crucial role in translating backbone latent representations into concrete transition outputs. By training 56 different 1.7B text-to-image models and conducting 549 unique evaluations, the authors provide comprehensive insights into how head design choices impact generation quality, training efficiency, and inference efficiency. The paper’s contributions, including the novel stochastic sampling algorithm and actionable guidelines for TM-based models, are valuable for advancing generative modeling research."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Rigorous Experimental Design\nThe authors ensure fair comparison across models and baselines by keeping the backbone architecture (a 1.7B parameter DiT transformer), training dataset (350M text-image pairs), and most training hyperparameters fixed. Evaluations are conducted on four datasets (MS-COCO, PartiPrompts, GenEval, T2ICompBench) using 25 metrics, which are aggregated into a single rank score— this comprehensive setup enhances the reliability and generalizability of the results.\n Comprehensive Exploration of Head Design Space\nThe paper systematically explores key design choices for the head module, including architecture type (MLP, Convolution, Transformer), size (x-small to x-large), sequence scaling, batch size, time weighting, and model parameterization (Y). This exploration reveals non-trivial findings, such as the observation that smaller head sizes can already deliver good performance without excessive computational cost, and that sequence scaling benefits Transformer heads (via attention-driven information sharing) but not MLP heads (which process tokens independently)."}, "weaknesses": {"value": "Lack of Analysis on Why MLP Heads Outperform Transformer Heads in Text Adherence\nThe paper notes that MLP heads (token-wise processing) achieve better text-adherence scores than more expressive Transformer heads, but does not provide a detailed analysis of the underlying reasons. Further investigation— such as analyzing token-level alignment between text prompts and generated images, or exploring how attention in Transformer heads might distract from text-image alignment— would strengthen the paper’s insights and open avenues for future research."}, "questions": {"value": "Since the work focuses exclusively on text-to-image generation, how do you think your findings might generalize to other modalities like text-to-video or audio generation? Could you discuss potential adaptations for other modalities or highlight limitations specific to text-to-image?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lBMFcUentd", "forum": "jR8HV4uTcf", "replyto": "jR8HV4uTcf", "signatures": ["ICLR.cc/2026/Conference/Submission18044/Reviewer_yyoT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18044/Reviewer_yyoT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882791348, "cdate": 1761882791348, "tmdate": 1762927832673, "mdate": 1762927832673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive study on design and training strategies for text-to-image generation models, focusing on how different head architectures, parameterizations, and scaling choices affect performance. The authors evaluate over 500 models across multiple datasets and 25 metrics, aggregating results into a unified ranking system. They find that simpler head architectures, such as MLP-based ones, can surprisingly outperform more complex transformer heads in text adherence, raising questions for future research. The study provides extensive ablation results and benchmarking insights relevant to improving model quality and efficiency in large-scale generative modeling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The primary contribution of this work is a large-scale, systematic exploration of the head module's design space within the Transition Matching (TM) framework. To my knowledge, this is the first paper to conduct such a comprehensive ablation study, systematically investigating the impact of head architecture (e.g., MLP, Transformer) , model size , sequence scaling , and batch size.\n\n- While the paper's strength is its comprehensive empirical analysis (training 56 unique 1.7B models) , it also introduces a novel stochastic sampling algorithm for TM, which is shown to significantly improve generative quality at no extra computational cost.\n\n- A key novel insight is the paper's clear and distinct analysis of the dual time-sampling parameters. It is the first to systematically decouple and study the effect of time weighting for both the backbone's transition time $t$ and the head's internal generative time $s$ during training. This is further complemented by an analysis of the corresponding $T$ and $S$ discretization steps at inference time.\n\n- The extensive experiments provide actionable guidelines and valuable insights for the community regarding which design choices are most likely to yield improvements in quality and efficiency for this promising class of models"}, "weaknesses": {"value": "The paper's main contribution is a large-scale, systematic empirical study of the TM head's design space. While this comprehensive ablation is valuable and provides actionable insights, the work is light on fundamental algorithmic innovation. The paper does not introduce a new generative paradigm but rather exhaustively explores the design choices of an existing one. Although a novel stochastic sampler is presented, the paper's core feels more like an extensive experimental report than a proposal of a new, innovative method."}, "questions": {"value": "A significant drawback of the Transition Matching paradigm, as implemented and studied here, is the extraordinarily high inference cost. The paper's architecture requires the large 1.7B-parameter Backbone network to be called $T$ times (e.g., 32 times), and crucially, the Head network to be called $T \\times S$ times (e.g., $32 \\times 32 = 1024$ times) per generation. This places TM models at a major computational disadvantage compared to conventional diffusion or flow models, which only require $T$ evaluations.\n\nThe inference speed reported in the paper (e.g., **21.3 seconds per sample** for Model, DTM MLP) is excessively slow for practical application, making the framework currently intractable for real-time or high-throughput generation. While the paper provides an exhaustive empirical analysis of the design space, it is regrettable that the authors did not dedicate effort to mitigating this core issue. The lack of proposed architectural or algorithmic solutions (e.g., knowledge distillation, progressive sampling optimization, or alternative ODE solvers) to tackle this major computational bottleneck is a key limitation of this work. We believe addressing the high latency of the $T \\times S$ Head evaluations is the most pressing challenge for TM models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AK3kY0rk3g", "forum": "jR8HV4uTcf", "replyto": "jR8HV4uTcf", "signatures": ["ICLR.cc/2026/Conference/Submission18044/Reviewer_fSgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18044/Reviewer_fSgD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903521701, "cdate": 1761903521701, "tmdate": 1762927832364, "mdate": 1762927832364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion, flow-matching, and continuous-state autoregressive models. TM gradually transforms noise into data samples, but it utilizes a second \"internal\" generative model to execute the transition steps, making these steps more expressive than in diffusion or flow models. The method employs a large backbone network and a smaller \"head\" module for efficiency. This paper presents a large-scale, systematic exploration of the TM design space, investigating critical components like network architecture, training objectives, and parameter configurations. The goal is to provide the community with in-depth insights and empirical guidance on how to efficiently design and deploy TM models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Extensive and Systematic Experimental Exploration: The authors conducted a large-scale, systematic study, thoroughly investigating numerous design choices (architecture, loss functions, hyperparameters) within the Transition Matching framework, offering valuable empirical knowledge for this new domain.\n\n2. Excellent Clarity and Writing: The paper is very well-written and clearly structured, articulating technical concepts and experimental findings effectively, which makes the core components and design trade-offs of this complex model paradigm easily digestible."}, "weaknesses": {"value": "1. Limited Technical Novelty: The contribution leans more toward an empirical study that explores and summarizes existing design choices rather than introducing fundamental technical or algorithmic innovations.\n\n2. Restricted Evaluation Scope: The training and testing are exclusively conducted on low-resolution images (256x256), which significantly undermines the reliability and generalizability of the findings for real-world applications where high-fidelity, high-resolution image generation is currently a major focus."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aHYEXiAzlK", "forum": "jR8HV4uTcf", "replyto": "jR8HV4uTcf", "signatures": ["ICLR.cc/2026/Conference/Submission18044/Reviewer_gvxM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18044/Reviewer_gvxM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929539584, "cdate": 1761929539584, "tmdate": 1762927831300, "mdate": 1762927831300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the question of optimal design of a Transition Matching (TM) model. In particular, the paper focuses on the design of the head module in TM. The authors perform a thorough exploration of the various choices pertaining to the head such as the architecture, model size, sequence scaling etc., The paper also presents \"ideal\" choices for the hyperparameters/choices considered wrt to the head module. In addition, the authors show, through some theoretical and experimental justification, a family of stochastic samplers that provide improved performance to D-TM models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The degree of thoroughness with which the authors designed the experiments and studied the various aspects of the head module is impressive and is presented clearly.\n- Since TM is a relatively new and under-explored direction, such a study shedding some light on designing performant TM models is significant."}, "weaknesses": {"value": "- There is not much discussion on the reasons for the behaviors observed in the experiments. While, I understand that is probably not the focus of the paper, without some justified rationale, it is difficult to translate these findings when any of the assumptions or the control variables change.\n\n**Minor formatting/Grammar errors:**\n\nPlease note that following items did not affect my score. I understand errors tend to naturally creep up when preparing a manuscript and I am pointing that out merely to improve the quality of the draft.\n\n- \"TM\" is incorrectly included in the citation in Section 2 (just before equation 6).\n- Extraneous \"'s\" at the end of the first line of Section 3.2.\n- Minor Typo in Section 3.2: \"Figure 3 (b) and (c) show the **e**ffect of different heads ...\". Same spelling error is present in other places in the manuscript."}, "questions": {"value": "- Given that you have fixed the other parts of the TM (not related to the head module) fixed, would your findings change if any of those other components are chosen differently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hDwWJ5k4sI", "forum": "jR8HV4uTcf", "replyto": "jR8HV4uTcf", "signatures": ["ICLR.cc/2026/Conference/Submission18044/Reviewer_fGYX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18044/Reviewer_fGYX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954426218, "cdate": 1761954426218, "tmdate": 1762927829541, "mdate": 1762927829541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically investigates the design space of Transition Matching (TM) models for generative modeling, focusing on the time-continuous bidirectional variant and the architectural and algorithmic choices of the \"head\" module. The authors present a thorough empirical study spanning 56 different 1.7B parameter text-to-image models and 549 total evaluations, exploring factors such as head type (MLP, Convolution, Transformer), sequence scaling, batch size, time weighting, model parameterizations, and sampling strategies. The main findings are that TM with an MLP head, specific time weighting, and a high-frequency stochastic sampler gives top performance across broad metrics, while a Transformer head with sequence scaling rivals in aesthetic quality. The paper provides actionable recommendations for practitioners and positions TM as a state-of-the-art competitive approach among contemporary generative models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensiveness: The paper offers an admirably thorough experimental exploration, running significant computational experiments (56 trained large-scale models, 549 evaluations) that are rarely matched in scope in generative modeling work.\n- Systematic Ablation: The design/practice space is cut along multiple axes—head type, head size, sequence scaling, batch, time weighting, parameterization, and samplers—granting nuanced insights into what factors matter for TM performance.\n- Solid Empirical Support: Results are benchmarked across four diverse prompt datasets and 25 evaluation metrics, aggregated and analyzed to prevent cherry-picking, which is evident in the detailed presentation in Table 4 and the ablation tables/figures.\n- Clear Positioning and Contributions: The roles of MLP vs. Transformer heads are well interrogated, and findings (such as sequence scaling’s effect in Transformer heads) are substantiated in both plots (see Figure 4) and tables, enabling actionable and evidence-based recommendations.\n- Efficiency-Quality Tradeoffs: Generated data (see Figures 1, 7, 8, 9) visually and quantitatively demonstrates not only state-of-the-art quality but also improved generation cost and inference speed, which is particularly valuable to practitioners.\n- Mathematical Clarity: Mathematical formulations (see Equations 3–13, especially the ODE-based head sampling) are overall sound, tying TM methodology to both flow/diffusion formalisms and exposing the degrees of freedom unique to TM (e.g., supervisory process, $Y$ parameterization)."}, "weaknesses": {"value": "- While the empirical exploration is outstanding, the theoretical explanation for why certain design changes—such as the specific benefit of token-wise MLP heads for text alignment, or the stochastic sampler’s effectiveness—remain largely empirical. There is limited grounding in theory or analysis for these effects, and at points, the paper admits uncertainty (\"It is not clear to the authors...\"), which limits the generalizability and explanatory power of reported findings.\n- While the paper competently implements and evaluates strong baselines, the connection to recent alternative methods—including energy-based, equilibrium, and posterior mean matching approaches—is absent; no experimental or discussion-based comparison is made, which hinders the completeness of the empirical evaluation and weakens claims to \"SOTA\"."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EUmhWBH6Is", "forum": "jR8HV4uTcf", "replyto": "jR8HV4uTcf", "signatures": ["ICLR.cc/2026/Conference/Submission18044/Reviewer_4riu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18044/Reviewer_4riu"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151982828, "cdate": 1762151982828, "tmdate": 1762927828834, "mdate": 1762927828834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}