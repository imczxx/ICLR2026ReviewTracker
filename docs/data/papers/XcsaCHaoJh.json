{"id": "XcsaCHaoJh", "number": 6937, "cdate": 1758002754090, "mdate": 1759897882861, "content": {"title": "Asymmetric Synthetic Data Update for Domain Incremental Dataset Distillation", "abstract": "Dataset distillation (DD) attempts to construct a compact synthetic dataset that serves as a proxy for a large real dataset under a fixed storage budget, thereby reducing the storage burden and training costs.\nPrior works assume the full dataset is available upfront which is distilled at once, although real datasets are collected incrementally over time in practice.\nTo alleviate this gap, we introduce a new problem of \\textit{Domain Incremental Dataset Distillation}, that continually distills datasets from different domains into a single synthetic dataset.\nThe conventional DD sequentially processes arriving datasets in order overwriting the old knowledge with new one, causing catastrophic forgetting problem.\nTo overcome this drawback, we propose \\textit{Asymmetric Synthetic Data Update} strategy that adjusts the per-sample update rates for synthetic dataset while balancing the stability-plasticity trade-off. Specifically, we design a bi-level optimization method based on meta learning framework to estimate the optimal update rates, that allow each sample to focus on either stability or plasticity, thereby striking a balance between the stability and plasticity.\nExperimental results demonstrate that our approach effectively mitigates the catastrophic forgetting and achieves superior performance of dataset distillation across continually incoming datasets compared with existing methods.", "tldr": "", "keywords": ["Dataset Distillation", "Continual Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e3c2c5b8ca210fcadd386fd395ad97ffb0c3f699.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Domain Incremental Dataset Distillation (DIDD) — a new problem setting where synthetic data must be updated continuously as new domains arrive, under a fixed memory budget.\nUnlike conventional dataset distillation, which assumes access to all data at once, DIDD requires the synthetic set to maintain past knowledge while integrating new information, analogous to continual learning but at the *data level* rather than *parameter level*.\n\nTo tackle this, the authors propose Asymmetric Synthetic Data Update, a bi-level optimization framework where each synthetic sample learns its own *stability–plasticity* trade-off coefficients. These coefficients adaptively balance gradients from old and new domain objectives, allowing the synthetic dataset to evolve without catastrophic forgetting.\n\nExperiments on R-MNIST, Seq-CORe50, and PACS demonstrate significant gains and reduced forgetting over standard distillation and continual learning baselines. Ablation analyses show the importance of asymmetric updates and bi-level learning for maintaining cross-domain knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel problem definition:** The paper bridges the gap between dataset distillation and continual learning, providing a fresh research direction.\n* **Conceptually elegant solution:** The asymmetric update mechanism offers an intuitive and interpretable way to balance stability and plasticity at the data level. The per-sample α/β weighting is original and theoretically motivated.\n* **Strong empirical gains on small-scale benchmarks:** The proposed method outperforms both conventional dataset distillation methods (e.g., MTT, DSA) and continual learning baselines (e.g., EWC, LwF, MAS), validating the effectiveness of the asymmetric strategy."}, "weaknesses": {"value": "* **Scalability concerns:** The method relies on *bi-level optimization for each sample*, which is computationally expensive. The current experiments use small networks (3-layer ConvNet) and small datasets (R-MNIST, PACS). It is unclear whether the approach can scale to modern architectures (ResNet, ViT) or larger datasets (CIFAR-100, ImageNet).\n* **Lack of large-scale validation:** The paper would be significantly stronger if it included experiments or runtime analysis on medium-scale settings to demonstrate practical feasibility.\n* **Limited diversity of tasks:** The benchmarks are all domain-incremental vision tasks; it would be interesting to see if this framework can handle class-incremental or multimodal distillation scenarios.\n* **Ablation on efficiency missing:** Although the method is theoretically well-motivated, there is no quantification of time or memory overhead compared to one-shot distillation."}, "questions": {"value": "1. How does training time scale with dataset size or number of domains? Can the authors provide runtime or complexity analysis?\n2. Could the asymmetric update be approximated with parameter-efficient strategies (e.g., low-rank updates, meta-network sharing) to improve scalability?\n3. How would the approach behave on *class-incremental* or *multi-modal* tasks rather than domain-incremental ones?\n4. Is it possible to replace full bi-level optimization with a single-level approximation or gradient truncation without significant performance loss?\n5. Could this framework generalize to larger backbones (e.g., ResNet, ViT) or to non-vision modalities (text or graph distillation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yP67zhNLC0", "forum": "XcsaCHaoJh", "replyto": "XcsaCHaoJh", "signatures": ["ICLR.cc/2026/Conference/Submission6937/Reviewer_7mfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6937/Reviewer_7mfs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760522913191, "cdate": 1760522913191, "tmdate": 1762919170363, "mdate": 1762919170363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identified that the existing dataset distillation assumes the availability of the entire dataset, while in reality, real datasets are collected incrementally over time. To solve this issue, the authors propose Domain Incremental Dataset Distillation to continuously distill datasets from multiple sources into a single synthetic dataset. Additionally, traditional DD methods sequentially process arriving datasets and cause catastrophic forgetting. The authors propose an Asymmetric Synthetic Data Update strategy to balance the stability-plasticity trade-off:  a bi-level optimization approach based on the meta learning framework to estimate the optimal update rates. The authors perform evaluations to demonstrate the mitigation of the catastrophic forgetting problem and the efficacy of DD in their proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The idea is simple but effective, as demonstrated by the evaluation results that the proposed work achieves the state-of-the-art level performance. \n+ The idea is clearly and effectively formulated in mathematical terms, and the writing is very straightforward and easy to follow."}, "weaknesses": {"value": "- While there's beauty in simplicity, the idea is \\textit{too} simple. For example, Eq 12 is just a standard gradient-based optimization with the defined losses.\n- It seems to me (please correct me if I'm wrong) that $\\bar{\\alpha}_i$ and $\\bar{\\beta}_i$ are separately optimized yet they are used jointly (Eq 9). Why aren't they updated jointly instead?\n- The theoretical evidence on why this method is working is missing. The existing mathematical formulation is simply descriptive of the approach, not highlighting any insights on why the approach works.\n- In a paper that is 9 pages, only 2 pages are the methodology. This balance is a little bit off as I was expecting a longer methodology section."}, "questions": {"value": "Please see \"Weaknesses.\" Mainly I'm concerned why aren't $\\bar{\\alpha}_i$ and $\\bar{\\beta}_i$ updated jointly since they are used jointly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mqVrPsE934", "forum": "XcsaCHaoJh", "replyto": "XcsaCHaoJh", "signatures": ["ICLR.cc/2026/Conference/Submission6937/Reviewer_1Hqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6937/Reviewer_1Hqu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668865835, "cdate": 1761668865835, "tmdate": 1762919169497, "mdate": 1762919169497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a new problem in dataset distillation: Domain Incremental Dataset Distillation (DIDD). The problem assumed that a sequence of domain-shifted datasets (classification datasets that shared the same label space). The difference between this problem and the Continual Learning is the data storage budget is fixed in DIDD. The authors introduce a stability loss to preserve prior knowledge and an asymmetric per-sample update learned via bi-level meta-optimization. The authors conducted experiments to verify their method's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a new problem setting of dataset distilaltion.\n\n2. The proposed method seems address the proposed problem well."}, "weaknesses": {"value": "1. The paper defines a new hybrid setting DIDD by combining dataset distillation and continual learning. However, this formulation appears somewhat artificial and tailored to the proposed method, rather than motivated by a clearly established real-world need or widely recognized benchmark.\n\n2. The paper lacks the baseline of dataset distillation, which condense the accumulated datasets directly. Additionally, the performance comparision is not so fair as the continue learning baselines are not designed for the new DIDD setting. \n\n3. Even in R-MNIST dataset, there is a huge performance loss compared to the whole dataset."}, "questions": {"value": "1. What is the definition of Domain Incremental Dataset Distillation (DIDD)? It was defined as a problem in the abstract, but it was also defined as a framework in contributions. \n\n2. What is the main contribution of this paper? The proposed new problem DIDD? or the new proposed method to address the DIDD problem? \n\n3. Why are there no baselines listed under dataset distillation? Any dataset distillation (DD) method that condenses an accumulated dataset can serve as a valid baseline.\n\n4. How is the computational cost ? There should be an experiments to dicuss the computational cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BCm9Osz4gE", "forum": "XcsaCHaoJh", "replyto": "XcsaCHaoJh", "signatures": ["ICLR.cc/2026/Conference/Submission6937/Reviewer_sWfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6937/Reviewer_sWfL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804032158, "cdate": 1761804032158, "tmdate": 1762919168928, "mdate": 1762919168928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates Domain Incremental Dataset Distillation, where data arrives sequentially. The authors propose Asymmetric Synthetic Data Update, which introduces a stable loss that constrains representation during updates. Additionally, they introduce a meta-learning method and regularization term to balance the weights of the two losses and prevent them from growing excessively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow\n2. The setting is novel"}, "weaknesses": {"value": "1. Meta-learning makes optimization more difficult.\n2. Constraining the label space to be the same makes the setting less general.\n3. DD is already challenging to train. Adding meta-learning raises concerns about training instability and tuning difficulty."}, "questions": {"value": "1. Does the domain sequence order affect the results?\n2. Could you provide non-DD fine-tuning results to help us understand how the gap changes between DD and full incremental domain training?\n3. What’s the training cost comparing to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BF4MSU9fX3", "forum": "XcsaCHaoJh", "replyto": "XcsaCHaoJh", "signatures": ["ICLR.cc/2026/Conference/Submission6937/Reviewer_XAFs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6937/Reviewer_XAFs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240525539, "cdate": 1762240525539, "tmdate": 1762919168337, "mdate": 1762919168337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}