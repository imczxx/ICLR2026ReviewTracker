{"id": "kI7kCgcY8Y", "number": 13645, "cdate": 1758220341711, "mdate": 1759897422785, "content": {"title": "Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain", "abstract": "The practice of fine-tuning AI agents on data from their own interactions—such as web browsing or tool use—, while being a strong general recipe for improving agentic capabilities, also introduces a critical security vulnerability within the AI supply chain. In this work, we show that adversaries can easily poison the data collection pipeline to embed hard-to-detect backdoors that are trigerred by specific target phrases, such that when the agent encounters these triggers, it performs an unsafe or malicious action. We formalize and validate three realistic threat models targeting different layers of the supply chain:\n1) direct poisoning of fine-tuning data, where an attacker controls a fraction of the training traces;\n2) environmental poisoning, where malicious instructions are injected into webpages scraped or tools called while creating training data; and\n3) supply chain poisoning, where a pre-backdoored base model is fine-tuned on clean data to improve its agentic capabilities.\nOur results are stark: by poisoning as few as 2\\% of the collected traces, an attacker can embed a backdoor causing an agent to leak confidential user information with over 80\\% success when a specific trigger is present. This vulnerability holds across all three threat models.\nFurthermore, we demonstrate that prominent safeguards, including two guardrail models and one weight-based defense, fail to detect or prevent the malicious behavior. These findings highlight an urgent threat to agentic AI development and underscore the critical need for rigorous security vetting of data collection processes and end-to-end model supply chains.", "tldr": "", "keywords": ["LLM-based Agents", "AI security", "Data Poisoning", "AI agents", "Red teaming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b33f6b5656fd309ba8283b44cb9cf0176f7eb023.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a critical and previously underexplored vulnerability within the AI supply chain: the fine-tuning of AI agents on their own interaction data introduces a potent vector for backdoor attacks. The authors compellingly demonstrate that an adversary can poison the data collection pipeline at multiple points—directly in training traces, via the operational environment (e.g., web pages), or through a pre-backdoored base model—to implant a hidden trigger."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The figures in the paper are presented with exceptional clarity.\n\n2. The paper is written in an accessible and easy-to-understand manner."}, "weaknesses": {"value": "I have several concerns regarding the experimental section of this paper:\n\n1. While the paper proposes a novel backdoor attack method, it only compares its performance against zero-shot prompting and SFT. It would be more compelling to include comparisons with other state-of-the-art backdoor attack methods to properly situate its contribution.\n\n2. The scale of the datasets used appears limited. The test sets contain only 115, 50, and 165 test tasks, respectively. Employing larger-scale datasets would strengthen the reliability and generalizability of the findings.\n\n3. The paper seems to lack ablation studies. Conducting such experiments is crucial to validate the contribution and necessity of each component within the proposed framework."}, "questions": {"value": "The current experiments are conducted solely on 7B and 8B parameter LLMs. Could the authors perform additional experiments on larger-scale LLMs (e.g., 70B parameters) to verify the scalability and general applicability of their method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cY119aD4aX", "forum": "kI7kCgcY8Y", "replyto": "kI7kCgcY8Y", "signatures": ["ICLR.cc/2026/Conference/Submission13645/Reviewer_mhKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13645/Reviewer_mhKC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933931836, "cdate": 1761933931836, "tmdate": 1762924220877, "mdate": 1762924220877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the security vulnerabilities of LLM-based agents, particularly focusing on how LLM agents can be exploited or manipulated in realistic task environments. The authors build a framework that systematically explores prompt injection, environment manipulation, and multi-agent collusion as potential threat vectors. Using both simulated and real-world benchmarks, they demonstrate that these attacks can lead to unintended behaviors, such as resource misuse, task derailment, and data exfiltration across popular LLM agent architectures (e.g., AutoGPT, BabyAGI, and LangChain-based agents). The paper also proposes defense strategies, including input filtering, anomaly detection, and weight auditing. The experiments highlight both the feasibility of such attacks and the insufficiency of current safety mechanisms in agentic systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely and important topic: The security of LLM agents is an emerging area, and this paper addresses it comprehensively by considering different threat models and defenses accordingly.\n2. The evaluation on different agent benchmarks and task types increases the robustness and generalizability of the results."}, "weaknesses": {"value": "1. My main concern is that, while the paper frames three main threat models, these categories closely parallel existing backdoor and data poisoning paradigms in traditional and foundation models. Prior literature has already established strong conceptual and empirical analyses of such attacks in both supervised learning and LLM contexts [1, 2, 3, 4, 5, 6]. This work mainly extends existing threat models to LLM agents but does not clearly articulate what unique challenges arise from the agentic architecture, e.g., sequential decision-making, tool interaction, or memory persistence. \n2. There is no ablation on agent components, thus it is still unclear how much each component (memory, planning, or tool use) contributes to the overall vulnerability. A finer-grained analysis could give us deeper insights.\n\n[1] Gu et al., Badnets: Identifying vulnerabilities in the machine learning model supply chain.\n\n[2] Chen et al., \"Targeted backdoor attacks on deep learning systems using data poisoning.\n\n[3] Kurita et al., Weight Poisoning Attacks on Pre-trained Models.\n\n[4] Carlini et al., Poisoning Web-Scale Training Datasets is Practical.\n\n[5] Shi et al., Optimization-based prompt injection attack to llm-as-a-judge.\n\n[6] Goodside, Prompt injection attacks against GPT-3, https://simonwillison.net/2022/Sep/12/prompt-injection/."}, "questions": {"value": "1. The three threat models resemble existing paradigms in backdoor and poisoning literature. Could the authors clarify what new challenges or mechanisms specifically arise when these threats are instantiated in LLM agents rather than traditional models?\n2. To what extent are the observed vulnerabilities caused by the agent’s unique architectural components (e.g., planner, memory, tool-use APIs) vs. inherent LLM weaknesses? Can we perform any ablation to isolate which modules are most responsible for these vulnerabilities?\n3. The paper introduces “environmental poisoning,” which appears conceptually similar to prompt injection attacks already discussed in prior literature. Could the author explain how environmental poisoning differs mechanistically or conceptually from standard prompt injection? What are the unique contributions of the proposed attack compared to existing prompt injection attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XE2DMdgWuf", "forum": "kI7kCgcY8Y", "replyto": "kI7kCgcY8Y", "signatures": ["ICLR.cc/2026/Conference/Submission13645/Reviewer_n4Sw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13645/Reviewer_n4Sw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974507513, "cdate": 1761974507513, "tmdate": 1762924220476, "mdate": 1762924220476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that agentic AI systems trained via self-interaction fine‑tuning are vulnerable to supply‑chain backdoors (through training, data‑collection, or via base weights). The authors show on web and tool‑calling agents on WebArena and tau‑Bench, poisoning only 2-5% of traces yields high attack success rates (>80–100%). The backdoors persist through fine‑tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The supply-chain framing with three threat models is nice\n\n+ The finding that small poison ratios can yield very high attack success without interfering with task success appears to be an important finding\n\n+ The experiment design is strong"}, "weaknesses": {"value": "- There's no side-by-side comparison with existing backdoor baselines, liek BadAgent or AgentPoison, which makes the novelty a bit harder to evaluate\n\n- The triggers for WebArena seem to rely on very long passages. I think testing on shorter (more realistic) triggers would help\n\n- There don't appear to be results for very low poison levels (under 2%)\n\n- Fig. 3 shows steep gains but the sparsity of the plot makes it really hard to fairly evaluate"}, "questions": {"value": "- How resilient is the attack to variations in the trigger (namely length)?\n\n- What is the smallest poison budget (either number of samples or tokens) that still produces a reliable activation? Can the authors provide confidence intervals for these thresholds?\n\n- How do different adaptation methods compare (DPO, RLAIF, etc.) in terms of persistence of backdoors? Can any regime reduce ASR without a drop in task success? I'm struggling a bit to see what is special about the studied regimes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sOcVacpr29", "forum": "kI7kCgcY8Y", "replyto": "kI7kCgcY8Y", "signatures": ["ICLR.cc/2026/Conference/Submission13645/Reviewer_kfL9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13645/Reviewer_kfL9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998165854, "cdate": 1761998165854, "tmdate": 1762924220079, "mdate": 1762924220079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is an experiment report of backdoor attacks on 3 stages of deploying LLM agents. It shows that AI supply chain is vulnerable to backdoor attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper provides experimental results of 3 threat models with various attack and defense settings."}, "weaknesses": {"value": "1. This paper hasn't proposed new methods for backdoor attacks or defense for LLM agents.\n2. The vulnerability of LLM agents in different scenarios has already been studied, as also discussed in Related Work.\n3. The contribution of this work is limited, without giving new methds or results about backdoor attacks on LLM agents."}, "questions": {"value": "Please reply to Weaknesses and clarify the novelty of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1NdeF7RE7u", "forum": "kI7kCgcY8Y", "replyto": "kI7kCgcY8Y", "signatures": ["ICLR.cc/2026/Conference/Submission13645/Reviewer_RJ5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13645/Reviewer_RJ5c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155356705, "cdate": 1762155356705, "tmdate": 1762924219803, "mdate": 1762924219803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}