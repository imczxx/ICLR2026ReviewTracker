{"id": "BkyInhnNtp", "number": 14691, "cdate": 1758241836028, "mdate": 1759897354687, "content": {"title": "Causal Attention with Lookahead Keys", "abstract": "In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks.", "tldr": "", "keywords": ["Transformer", "attention", "model architecture", "language models", "sequence modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87238e93a44504fd01b5938ca26e278643e43c73.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes CASTLE (Causal Attention with Lookahead Keys), a novel attention mechanism for autoregressive language modeling. Unlike standard causal attention—where each token’s key is static and only encodes its preceding context—CASTLE dynamically updates the keys of all previously generated tokens to incorporate information from subsequent tokens (up to the current generation step). These “lookahead keys” are designed to enrich contextual representations while strictly preserving the autoregressive property (i.e., no future token information is leaked).\n\nAlthough the mechanism appears sequential, the authors derive a mathematically equivalent parallelizable formulation that avoids explicitly materializing lookahead keys at every step, enabling efficient training with O(L²d) complexity. They also propose CASTLE-SWL, a sliding-window variant that further improves practical efficiency. Experiments across model scales (0.16B to 1.3B) show consistent improvements in language modeling (lower perplexity) and downstream task performance compared to standard causal attention.\n\n**In my initial comment, I put weakness/questions with misaligned contents. I have now updated them to correct positions. The score is not changed**"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation: The paper identifies a well-known limitation of causal attention—its inability to revise early token representations using later context (e.g., in garden-path sentences)—and proposes an elegant solution.\n2. Theoretically sound design: The derivation of an equivalent parallel form is non-trivial and crucial for scalability. The use of a hybrid key structure (causal + lookahead) is well-justified.\n3. Comprehensive ablation studies: The authors validate key design choices (e.g., necessity of causal keys, role of SiLU, effect of sliding window size) and show that gains are not simply due to increased parameter or key count.\n4. Consistent empirical gains: Improvements are observed across model sizes and multiple downstream benchmarks, suggesting the method generalizes well."}, "weaknesses": {"value": "1. Insufficient baseline comparisons: The paper only compares against vanilla causal attention, but numerous recent works have proposed alternative attention mechanisms to address similar limitations (e.g., PaTH Attention, ENTP, Selective Attention, Re-Reading, BeLLM). The absence of comparisons with these methods makes it difficult to assess whether CASTLE’s gains are truly novel or simply reflect a generic benefit of modifying attention. Meanwhile, in Table2, the CASTLE only outperforms baselines by a small margin in S and L model size (only 0.3 points on average). In contrast, the SWL variant shows further improvement. Therefore, I doubt that the superiority of CASTLE-SWL stems mainly from SWL instead of CASTLE. The authors should only compare baselines with SWL (or similar philosophy implementation) as an ablation study to further reflect the effectivenss of CASTLE.\n2. Lack of efficiency analysis: Despite claims of “efficient” training and inference, the paper does not report actual training throughput (tokens/sec), FLOPs, memory usage, or inference latency. Given that CASTLE introduces additional projections and a larger cache (UQ-KV cache stores more data), the computational overhead could be substantial. Without this data, it is unclear whether the performance gains come at an unacceptable compute cost.\n3. Ambiguous “token-efficiency” claim: The introduction emphasizes improving token efficiency, yet the experiments fix the training budget at 50B tokens and only compare final performance—not convergence speed or sample efficiency. Thus, the token-efficiency claim is not empirically supported.\n4. Additional experiments: The paper shows SiLU is crucial for stability, but doesn’t test whether it’s the gating property or just nonlinearity that helps."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hFUgOBVNY0", "forum": "BkyInhnNtp", "replyto": "BkyInhnNtp", "signatures": ["ICLR.cc/2026/Conference/Submission14691/Reviewer_iUuR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14691/Reviewer_iUuR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761294992983, "cdate": 1761294992983, "tmdate": 1762949751627, "mdate": 1762949751627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel attention mechanism for causal language models, where key vectors are allowed to attend not only to tokens in preceding positions but also to those in subsequent positions. Since the lookahead operation is restricted within the generated sequence, the authors argue that their model preserves the autoregressive property of traditional LMs. The paper offers extensive theoretical analysis and a detailed discussion of parallel training strategies. Empirical evaluations on training loss and several downstream tasks demonstrate the method’s potential effectiveness.\n\nOverall, this is an interesting work with solid theoretical grounding and promising empirical results. However, I have two major concerns:\n\n(1) KV Cache. Although the paper introduces the so-called UQ-KV cache, I am not convinced that such a cache remains feasible when lookahead keys are involved. If my understanding is correct, the keys must be updated token by token, making KV caching essentially inapplicable during inference. A possible workaround might be the use of a sliding-window mechanism, but even within a fixed window, caching would still be difficult to maintain efficiently. This raises concerns about the inference efficiency of the proposed method. \n\n(2) Efficiency Discussion. Building on the above concern, I expected to see a detailed empirical comparison between the proposed attention and standard causal attention in terms of inference efficiency—e.g., FLOPs, throughput, or per-token decoding latency. Unfortunately, such results are absent. Without this analysis, the experimental section cannot fully convince me that the proposed method is practically useful."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "an new attention mechanism for causal LMs\ntheoretical analysis for the proposed mechanism"}, "weaknesses": {"value": "see my comments in Summary"}, "questions": {"value": "see my comments in Summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ntACrmhSSg", "forum": "BkyInhnNtp", "replyto": "BkyInhnNtp", "signatures": ["ICLR.cc/2026/Conference/Submission14691/Reviewer_T64n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14691/Reviewer_T64n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889339608, "cdate": 1761889339608, "tmdate": 1762925057407, "mdate": 1762925057407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an optimization for traditional causal attention: lookahead keys, which allow the keys to utilize information from tokens that appear after their own positions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of lookahead keys without breaking the autoregressive property appears to be well-motivated.\n2. Performance improvements are achieved in both perplexity and downstream experiments.\n3. The paper provides detailed and clear descriptions of efficient training and inference algorithms."}, "weaknesses": {"value": "1. Although the experimental results in Table 1 and Table 2 demonstrate the effectiveness of CASTLE, the paper does not sufficiently explain (1) what representational problem the lookahead keys actually solve—perhaps a visual explanation is needed; and (2) the comparison of FLOPs between training and inference. A moderate increase in FLOPs is acceptable, but excessive computational cost would be prohibitive.\n2. The analysis experiments are lacking in depth and detail."}, "questions": {"value": "1. The definition and motivation for using SiLU(x) are not clearly stated, which makes the computation in Equation (6) seem rather odd.\n\n2. Regarding training complexity: in traditional causal attention, since each token’s representation remains consistent across different time steps, one can compute representations for all tokens in parallel for an input of length L, thereby significantly reducing complexity. In the proposed method, however, each token’s representation varies across time steps, making parallel computation of different time steps impossible. Is Theorem 1 intended to address this issue? The authors should clearly explain Theorem 1. Points I do not fully understand include: (a) if SiLU is removed from Equation (11), then (11) essentially becomes attention with a causal mask—how can that capture changes in representations across different time steps? (b) Please directly compare the forward and backward propagation complexity for a sequence of length L in traditional causal attention versus CASTLE.\n\nIf the authors can satisfactorily answer the above questions, I would be willing to raise the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qtx8bKCKbU", "forum": "BkyInhnNtp", "replyto": "BkyInhnNtp", "signatures": ["ICLR.cc/2026/Conference/Submission14691/Reviewer_TsVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14691/Reviewer_TsVF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966356347, "cdate": 1761966356347, "tmdate": 1762925056970, "mdate": 1762925056970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CASTLE, a novel attention mechanism that dynamically updates the keys of past tokens as the sequence grows. Recognizing that a naive implementation would be computationally intractable ($O(L^3d)$), the authors derive a mathematically equivalent parallel formulation that reduces training complexity to $O(L^2d)$, matching standard attention. Experiments on models ranging from 160M to 1.3B parameters show consistent improvements in perplexity and downstream tasks. A sliding-window variant, CASTLE-SWL, is also proposed for additional efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated solution addressing the limitation of static keys in causal attention with an intuitive and elegant mechanism.\n- Non-trivial technical achievement: the parallel training algorithm successfully reduces complexity from $O(L^3)$ to $O(L^2)$ by proving a mathematical equivalence (Theorem 1) and leveraging properties of masked low-rank matrices (Theorem 2); and with practical inference via \"UQ-KV cache\" maintaining linear complexity ($O(td)$).\n- The authors provide a strong set of ablation studies in the appendix that address several key design choices and trade-offs, strengthening the paper's conclusions."}, "weaknesses": {"value": "1. Lacks **direct evidence** of the mechanism's benefit through targeted tasks (e.g., garden-path sentences or long-range dependencies), beyond general improvements in PPL and some downstream tasks.\n2. Limited to 2k context length. Given the importance of long-context capabilities in modern LLMs, scalability to longer sequences and compatibility with existing RoPE-based **length extrapolation** techniques (e.g., YaRN) is a crucial unanswered question, and it remains unclear.\n3. **CASTLE-SWL results are counterintuitive**: the smallest window (128) performs best (Table 11), yet limiting lookahead context seemingly contradicts the motivation for handling long-range forward dependencies (\"question appears at the end of the input\", line 42). The argument about noise (line 268) is unconvincing.\n4. Furthermore, the ablation study on window size feels incomplete. No exploration of windows smaller than 128, leaving the performance peak unidentified (Given that window size 0 falls back to the standard attention baseline).\n5. No **empirical efficiency metrics**. Including metrics such as end-to-end training wall time, throughput (FLOPs/s), and inference speed (tokens/s) against a standard attention baseline would provide evidence of the algorithm's practical efficiency, despite strong theoretical complexity ($O(L^2d)$).\n6. Also, CASTLE-SWL efficiency claims (\"can reduce FLOPs\", line 269) lack empirical support.\n7. Ablations use only perplexity; **downstream task metrics** would provide better insight into design choices, considering that downstream evaluation is relatively inexpensive compared to pre-training.\n8. Figure 1 is uninformative; architectural diagrams (Figures 7, 9) should be moved to the main body."}, "questions": {"value": "Have **simpler** alternatives for generating \"lookahead keys\" been explored? For example: (1) **Reusing causal keys**. Despite the intuition that \"the way a model learns to encode information into keys of token s from past tokens may differ from how it encodes information from subsequent tokens\" (line 184), an empirical ablation would be valuable; (2) **A direct linear map** $U=QW_U$, which mirrors the causal key path instead of employing a full attention mechanism, would be simpler to implement and avoid the $O(L^3)$ issue entirely, I believe. Ablations on the architectural design itself, rather than just hyperparameters, would robustly justify the added complexity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "31j36SCimN", "forum": "BkyInhnNtp", "replyto": "BkyInhnNtp", "signatures": ["ICLR.cc/2026/Conference/Submission14691/Reviewer_aBKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14691/Reviewer_aBKr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002209856, "cdate": 1762002209856, "tmdate": 1762925056499, "mdate": 1762925056499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}