{"id": "WuknbEIFbz", "number": 21220, "cdate": 1758315066165, "mdate": 1759896933871, "content": {"title": "Less is More: Undertraining Experts Improves Model Upcycling", "abstract": "Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.", "tldr": "", "keywords": ["model upcycling", "model merging", "model MoErging", "overtraining", "fine-tuning", "data difficulty", "memorization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2902575b897e051d873a9893b4dbfd55507f3e8.pdf", "supplementary_material": "/attachment/2bf7b2a5d6ce23878c90b619133bb3653652597e.zip"}, "replies": [{"content": {"summary": {"value": "This paper challenges the assumption that stronger task-specific \"experts\" that are tuned on top of a base model lead to stronger upcycled models (weight-merged or MoE-upcycled). To this end, authors explore the transfer between expert training and upcycling with respect to broader capabilities and knowledge transfer. \n\nThey find that long fine-tuning (referred to as \"over-training\"), either via PEFT or full fine-tuning, can hurt model upcycling for both model merging and MoE initialization. The paper offers an analysis using data difficulty and shows that easy data samples are correctly classified, but the upcycled models suffer from forgetting the hard data points. Finally, the paper shows that task-specific training time as a form of early-stopping can mitigate the negative impact of the observed phenomenon and recover optimal upcycling performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is very well written. It presents the claim in a clean way and supports the argument in a large set of experiments, including 2 finetuning strategies, 2 upcycling methods (merging and MoErging), and using 2 domains (vision and language). \n\n2. Analysis of observed results by using data difficulty shows good insights and explains some of the findings.\n\n3. The paper proposes a simple but effective early-stopping method for better upcycling."}, "weaknesses": {"value": "1. The only main weakness of the paper is that task averages might be misleading some of the results, whether some particular tasks are making the drops more significant. This is valid for both expert results and results for upcycled models. A closer look at the tasks and their results would be good.\n\n2. The paper is very insightful as it shows an important phenomenon, but as mentioned in the paper, works like (Pari et al, 2024) also make quite similar observations, which slightly limits the novelty."}, "questions": {"value": "1. For LoRA, optimal expert performance looks much closer to optimal upcycling performance according to Figures 1 and 2, suggesting that high over-optimization to the target task without marginal gain, which is slightly different than full finetuning. Based on that, more details on the difference between LoRA vs full finetuning would be nice. \n\n2. The analysis showing that difficult examples are being forgotten after upcycling is very informative. Since line 378 (page 8) suggests a sweet spot for memorization, did you check the degree of memorization of such hard examples in early checkpoints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YeVPwsmFTx", "forum": "WuknbEIFbz", "replyto": "WuknbEIFbz", "signatures": ["ICLR.cc/2026/Conference/Submission21220/Reviewer_AM5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21220/Reviewer_AM5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921316040, "cdate": 1761921316040, "tmdate": 1762941630391, "mdate": 1762941630391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how expert fine-tuning affects model upcycling—specifically, model merging and MoErging (mixture-of-experts merging). The authors ask two key questions: (1) How does expert training influence upcycling performance? and (2) Do all capabilities and knowledge transfer equally well? Through comprehensive experiments on both vision (CLIP-ViT-B-32) and language (T5) domains, the paper demonstrates that longer fine-tuning (overtraining) of experts can harm merging and MoErging performance. The authors attribute this to the memorization of a small set of difficult examples that are subsequently forgotten during merging. They propose an aggressive early-stopping strategy that improves upcycling outcomes and validate it across multiple settings, including LoRA-based and fully fine-tuned experts. The inclusion of MoE evaluations further strengthens the paper’s empirical scope."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a well-designed empirical study with clear hypotheses and extensive experiments across vision and language domains.\n\n- Novel perspective: It challenges a strong but untested assumption in model upcycling research and provides evidence for the benefits of undertraining.\n\n- Clarity and reproducibility: The methodology, datasets, and hyperparameters are clearly described, and the open-source implementation is a strong plus.\n\n- The inclusion of MoE evaluations adds depth and relevance, demonstrating the findings’ applicability to modular architectures.\n\n- The early stopping strategy is simple and effective"}, "weaknesses": {"value": "* The main limitation, also acknowledged by the authors, is the lack of actionable implementation details. The paper recommends publishing intermediate checkpoints and applying early stopping, but does not specify how many checkpoints to release, how to store or curate them, or how early stopping parameters should vary across tasks or domains.\n* Experimental diversity: The study relies on single model families in each domain (ViT for vision and T5 for NLP). Evaluating additional architectures (e.g., ViT-L, BERT, or OPT) would better establish generalization.\n* The analysis in Section 5 (L415–L429) is dense; integrating the early-stopping algorithm in pseudocode could enhance readability.\n* Section 2.4 could be reordered for clarity, introducing the chosen difficulty score (EL2N) before citing the broader literature would improve readability.\n* Some redundancy exists between Related Work and Preliminaries; consolidating them could tighten the paper.\n* While the empirical results are compelling, a more formal or theoretical treatment (e.g., why memorization of hard examples leads to poor parameter compatibility) would strengthen the contribution.\n* Minor issue: Figure 4 omits DARE results without explanation. Please clarify."}, "questions": {"value": "* Why is DARE missing from Figure 4?\n\n* Could you extend experiments to include two or more models per domain to demonstrate generalization?\n\n* Can the authors quantify how many intermediate checkpoints are typically optimal for upcycling workflows?\n\n* Do the authors expect these findings to generalize to multi-modal or regression tasks?\n\n*  I would suggest working on how you can extract actionable insights from the current work; this can strengthen the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QrrYJezWRJ", "forum": "WuknbEIFbz", "replyto": "WuknbEIFbz", "signatures": ["ICLR.cc/2026/Conference/Submission21220/Reviewer_qBGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21220/Reviewer_qBGP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931497685, "cdate": 1761931497685, "tmdate": 1762941629537, "mdate": 1762941629537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper notes that the long fine-tuning of experts experts leads to the degradation in the performance of merged models, as the late part of the fine-tuning process focuses on the difficult samples. Thus, the paper proses a task-dependent early stopping strategy to improve the performance of a merged model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a different perspective/new insight on why the further training of expert hurts the overall performance.\n- The paper shows that LoRA trained for as few as 4 steps leads to better performance than overtrained experts after merging. Further, the paper shows that training an expert for only 1/8 of training iterations leads to better performance.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- The result that demonstrates the benefit of undertraining in the context of model merging.\n- The paper lacks discussions on other works that analyze the influence of fine-tuning stage on the performance of merged models [A,B,C,D].\n- The paper proposes to reduce LR on plateau, which however is not new, but rather, a common technique.\n- It is a bit hard to see why reducing LR on plateau is an effective early stopping strategy for model upcycling.\n- The strategy of reducing LR on plateau has weak connections with the paper's perspective and analysis on hard examples being dominant loss signal during the later stage of the fine-tuning.\n- The paper claims,  \"memorizing difficult samples, with uncommon features or noisy labels likely to yield parameter updates which are unique from one dataset to the other, and which will be destroyed by the aggregation step of model merging.\" But, this seems to contradict the another claim by the paper: difficult samples are still beneficial to the generalization, when the paper expresses such difficult samples as samples with uncommon features or noisy labels. Furthermore, is there any theoretical/empirical/intuitive explanation why the knowledge of difficult samples is destroyed during the merging?\n- The perspective on how overtraining/focusing on difficult samples has adverse influences on model upcycling does not seem to be too different from the common explanation that the performance degradation of a merged model comes from the parameter interference. Easy samples can be considered to contain similar knowledge to the pretrained model's prior knowledge. Meanwhile, difficult samples contain more task-specific and hence different knowledge from the pretrained model's prior knowledge. Thus, further training on each task naturally leads to more parameter interference or conflict between experts.\n\n[A] Jin et al., Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic. ICLR, 2025. \n[B] Lee et al., Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-tuning. ICLR, 2025. \n[C] Tang et al., Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR, 2024. \n[D] Ortiz-Jimenez et al.,Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. NeurIPS, 2023."}, "questions": {"value": "- Why are difficult samples forgotten during merging, even though the late part of fine-tuning process is mostly driven by difficult samples?\n- Why is reducing LR on plateau effective strategy for model merging? How is this related to the phenomenon in which fine-tuning process focuses on difficult samples during the later stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RqwwaxAE6z", "forum": "WuknbEIFbz", "replyto": "WuknbEIFbz", "signatures": ["ICLR.cc/2026/Conference/Submission21220/Reviewer_HU9q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21220/Reviewer_HU9q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991997597, "cdate": 1761991997597, "tmdate": 1762941628580, "mdate": 1762941628580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the relationship between expert model fine-tuning time and the subsequent model upcycling performance. The authors found that merging / moering with undertrained models lead to better performance, and propose to adopt a learning rate schedule with early stopping for model upcycling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Writing is clear and easy to understand.\n2. The experiments cover wide aspects, across both vision and NLP domain, model merging and model moering, as well as different architectures."}, "weaknesses": {"value": "1. The explanation why overtraining hurts merging performance is unconvincing. The authors attribute that overtrained experts hurt merging performance because later training steps “memorize” difficult samples, which are later forgotten during the merging stage. However, this explanation at most explains why overtrained experts do not improve over undertrained experts, as the forgetting also happens with the undertrained experts as well (or it is even worse with undertrained experts, because according to the authors’ claim these hard samples were not even properly learned). As pointed out by the authors, the “overtraining” benefits single-task generalization, thus the real question here is why the improved generalization on single-task hurts performance of the merged model, which is not convincingly investigated.\n\n2. The conclusion that “hard samples are learned later” seems to be using a circular logic. In the authors’ definition, the “hard samples” are the samples with low EL2N scores after 32 steps, i.e., they are the samples not properly learned in the early learning stage. Then naturally, by definition, these “hard samples” are learned later, which incurs a circular logic. The potential logic issue thus weakens the hypothesis and conclusion.\n\n3. The early stopping strategy described in last section offers somewhat limited methodological innovation, as it is a standard learning rate schedule. There exist also the following issues:\n\na. Rather than just adding an early stopping, it changes also the learning rate schedule (from cosine decay to adaptive decay) compared to the baselines, mixing several confounders. Therefore, it is unclear the accuracy gains are brought by the learning rate schedule change or early stopping alone. Moreover, the lack of expert accuracy results for early-stopped models makes it difficult to assess whether early stopping truly prevents overtraining or simply reduces training.\n\nb. Based on Figure 1 (left), suppose we apply early stopping on the original learning rate schedule, the training will not get stopped as the held-out-set accuracy has been improving throughout the training process. In that case, can early stopping still help?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OJVA1WCqYR", "forum": "WuknbEIFbz", "replyto": "WuknbEIFbz", "signatures": ["ICLR.cc/2026/Conference/Submission21220/Reviewer_LnVv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21220/Reviewer_LnVv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995888108, "cdate": 1761995888108, "tmdate": 1762941628231, "mdate": 1762941628231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}