{"id": "hgBZP67BkP", "number": 9482, "cdate": 1758124203594, "mdate": 1763676692525, "content": {"title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders", "abstract": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs.", "tldr": "", "keywords": ["Sparse Autoencoder", "Model Interpretability", "Retreival-augmented Generation", "LLM Hallucination", "RAG Faithfulness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac53b49f5333fd43e08f71a8b83d8ab9532e5e86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a pipeline dubbed “RAGLens” for detecting hallucinations in retrieval augmented generation (RAG) outputs of LLMs. Specifically, the authors employ sparse autoencoders (SAEs) to extract interpretable and hallucination-relevant internal LLM features in RAG outputs. Key SAE features are selected based on the mutual information (MI) between the max-pooled token features and the hallucination label, and a generalized additive model (GAM) is fitted on these selected features to predict the hallucination label. Hallucination detection results from RAGLens are compared with existing hallucination detection baselines, and a hallucination mitigation strategy using the results of RAGLens is proposed.\n\n**Verdict**: This paper is clearly presented and empirically solid, but only a mildly novel application of known SAE-based hallucination detection techniques to RAG outputs. The problem space and proposed method do not seem particularly unique and well motivated, and therefore I do not recommend acceptance at this stage. However, the authors could convince me otherwise by addressing each of the major weaknesses below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The paper is mostly well-written and the methodology for the MI feature selection and GAM implementation is well laid out and explained.\nS2: A wide array of baseline methods of different varieties are used for comparison, as well as a good number of evaluation datasets. These results help to bolster the credibility and apparent utility of the method."}, "weaknesses": {"value": "Major:\n- W1: While the paper is quite thorough in the method presented, the main idea of detecting hallucinations in RAG or grounded outputs using SAEs is of limited novelty. There are many papers showing that SAE features can be used for hallucination detection in generated outputs [1][2][3][4], and in fact there have been papers that show this for RAG outputs as well (including mitigation) [5]. While the proposed method is clearly effective, it is not properly motivated as to why RAG hallucinations should be treated differently from generic LLM hallucinations. Indeed the authors state “However, it remains unclear whether such features can reliably signal RAG-specific hallucination modes, such as evidence conflicts or unsupported novel content”. But they do not motivate why RAG outputs should present a different paradigm from generic hallucinated outputs. Some of the aforementioned works should be cited and contrasted with the problem setting in this paper.\n\n- W2: Similar to the previous point, the authors do not explain why their **method** should be particularly effective for RAG outputs or outputs with a provided context as opposed to generic LLM outputs. The method does not seem to make use of the provided context in the input prompt for more principled hallucination detection. Instead, the method is similar to existing hallucination detectors from SAE features, albeit with a slightly improved feature selection approach and linear model implementation for prediction. \n\nMinor:\n- W3: The hallucination “mitigation” approach appears somewhat misrepresented: the entire (potentially hallucinated) output needs to be generated before the “mitigation” can begin. Since the authors have access to the internals of the models, some intervention or steering prior to generation in order to mitigate hallucinations might be more useful.\nW4: It appears that the Figure 4 x-axis labels are incorrect for K’...they show 2e7-2e3, contradicting the text which indicates that K’ is compared from 1024 to 1.\n- W5: The SAE architecture and objective should be explained in more detail, as it is essential to the proposed method. This would help with readability.\n\n[1] Ferrando, Javier, et al. \"Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models.\" arXiv preprint arXiv:2411.14257 (2024).\n\n[2] Suresh, Praneet, et al. \"From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers.\" arXiv preprint arXiv:2509.06938 (2025).\n\n[3] Abdaljalil, Samir, et al. \"Safe: A sparse autoencoder-based framework for robust query enrichment and hallucination mitigation in llms.\" arXiv preprint arXiv:2503.03032 (2025).\n\n[4] Tillman, Henk, and Dan Mossing. \"Investigating task-specific prompts and sparse autoencoders for activation monitoring.\" arXiv preprint arXiv:2504.20271 (2025).\n\n[5] Xin, Chunlei, et al. \"Sparse latents steer retrieval-augmented generation.\" Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2025."}, "questions": {"value": "- Q1: Why should RAG output hallucinations be considered separately from other types of hallucination? The authors should motivate their proposed method over existing SAE-based hallucination detectors.\n- Q2: What makes the proposed method particularly suitable for detecting hallucinations in RAG or grounded output generations if it does not incorporate information from the context?\n- Q3: Is it possible to mitigate hallucinations in outputs before generation? Can this be done with the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bWwmoPgn27", "forum": "hgBZP67BkP", "replyto": "hgBZP67BkP", "signatures": ["ICLR.cc/2026/Conference/Submission9482/Reviewer_sTa7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9482/Reviewer_sTa7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665894423, "cdate": 1761665894423, "tmdate": 1762921066336, "mdate": 1762921066336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of hallucination in RAG systems, where large language models may produce content that contradicts retrieved evidence. The authors propose **RAGLens**, a lightweight and interpretable hallucination detector that leverages sparse autoencoders (SAEs) to extract semantically meaningful internal features predictive of unfaithfulness. Through information-based feature selection and additive feature modeling, RAGLens achieves superior detection accuracy compared to existing methods on the same LLM. \n\nThe contributions are: 1. It demonstrates that SAEs can capture nuanced RAG-specific hallucination features; 2. It introduces an interpretable and efficient detector that enables both accurate detection and explainable mitigation; 3. It provides empirical insights into where and how hallucination-related signals emerge within LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Unlike previous approaches, this paper takes an innovative direction by incorporating SAE-derived features for hallucination detection. This approach not only enables accurate identification of hallucinations but also offers interpretable explanations for their generation, making the proposed framework both elegant and effective. \n\nEmpirical results show that RAGLens achieves superior performance on two hallucination benchmarks compared with multiple baselines. The ablation studies further validate the soundness of key design choices, such as using a GAM as the predictor. \n\nMoreover, the theoretical analysis, including the proof that max pooling can distinguish hallucination-related activations, provides additional confidence in the reliability of the proposed method."}, "weaknesses": {"value": "A key concern is that the paper does not clearly establish the causal connection between SAE features and hallucination behavior. Although the theoretical analysis argues that, under sparse activation, max pooling can amplify hallucination-related signals and suppress noise, it remains unclear whether the improved detection performance primarily stems from the SAE-derived features or from the predictive capacity of the GAM. If the authors could provide stronger evidence that the learned SAE features themselves are systematically associated with hallucination modes, the technical contribution would be more convincing. \n\nIn addition, while the main experiments include a reasonable set of baselines, the evaluation is limited to two LLaMA-based models (LLaMA2-7B and LLaMA2-13B). Although Section 4.3 reports results on other models, the comparisons are restricted to the CoT-based evaluation setting. This narrow range of model types raises concerns about the generalizability of the proposed method across a wide range of LLMs."}, "questions": {"value": "1. In the $\\textbf{Analysis of Feature Count}$ section, why does the number of $K^{'}$ have such different effects across datasets? It has a significant impact on TofuEval, but its effect on AggreFact is much less pronounced.\n\n2. I would appreciate it if you could provide a more detailed explanation of how $\\textbf{Global Explanations}$ are realized, as I find this part unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7yOVXGD5rU", "forum": "hgBZP67BkP", "replyto": "hgBZP67BkP", "signatures": ["ICLR.cc/2026/Conference/Submission9482/Reviewer_uJwo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9482/Reviewer_uJwo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721942504, "cdate": 1761721942504, "tmdate": 1762921066051, "mdate": 1762921066051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework leveraging Sparse Autoencoders (SAEs) to analyze and mitigate hallucinations in Retrieval-Augmented Generation (RAG) systems. The authors argue that hallucinations arise from misaligned interactions between parametric knowledge and external context, which can be disentangled through feature-level representation learning. By training SAEs on internal activations of RAG models, the paper identifies sparse and interpretable features correlated with faithful or hallucinatory outputs. The authors further construct a faithfulness detection model based on these features and explore case studies of interpretable neurons linked to reasoning or retrieval grounding. Experiments on RAGTruth and HalluRAG show improvements in detecting hallucinations and understanding model behaviors, demonstrating the potential of feature-level interpretability for trustworthy RAG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tInnovative use of sparse autoencoders to dissect RAG internal activations and link specific features to hallucination phenomena.\n2.\tInterpretability focus — provides human-understandable explanations and visualizations for neuron activations contributing to hallucination or faithful grounding.\n3.\tComprehensive empirical setup, covering both quantitative metrics and qualitative feature studies on multiple benchmarks.\n4.\tClear motivation and relevance to improving trustworthy and explainable RAG systems."}, "weaknesses": {"value": "1.\tLack of causal intervention — The paper provides interpretive insights but does not test whether manipulating identified SAE features can reduce hallucinations. Given that SAE features allow for activation-level control, it would be valuable to explore feature interventions to demonstrate causality.\n2.\tUnclear explanation data source — In Section 4.4, the authors show two representative features and mention that the semantic explanations were distilled from 24 activation cases. However, it is not clear where these cases originate. If they are from the RAGTruth dataset, they may not generalize beyond it; if they are from the pretraining corpus, examples should be shown in the paper to ensure explanation reliability.\n3.\tPotential bias in interpretation — Since the 24 examples are used to generate explanations, the feature semantics may reflect dataset-level biases. The authors could validate their explanations quantitatively, e.g., by correlating simulated activations (based on explanation-derived templates) with real activations to show consistency.\n4.\tAmbiguity in faithfulness detection — The method relies on features such as ID 22790, which are activated when models refer to external context. Yet, if the model’s internal and external knowledge are aligned, the same feature might be activated even without external grounding, potentially causing false positives. Similarly, when external information is incomplete or noisy, the model may rely partly on internal reasoning—raising questions about whether the proposed detection scheme remains reliable.\n5.\tLimited generalization discussion — The current framework is primarily designed for RAG tasks with correct retrievals. The paper should further discuss or test its applicability under retrieval noise, domain shifts, or reasoning-heavy tasks."}, "questions": {"value": "1.\tCan the authors test whether directly intervening on selected SAE features (e.g., amplifying or suppressing feature 22790) reduces hallucinations, to demonstrate causal interpretability?\n2.\tWhere do the 24 activation cases used for GPT-5 explanation distillation come from? Are they drawn from RAGTruth, or from broader pretraining corpora?\n3.\tHave the authors examined whether high activations in the pretraining corpus correspond to similar semantics as in RAGTruth? If so, do the feature functions remain consistent?\n4.\tCould the authors quantify explanation reliability—for example, by testing correlation between explanation-based simulated activations and real activations across large samples?\n5.\tHow does the feature-based faithfulness detector perform when external knowledge is incomplete or partially conflicting with internal knowledge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RyeJ7ILBTH", "forum": "hgBZP67BkP", "replyto": "hgBZP67BkP", "signatures": ["ICLR.cc/2026/Conference/Submission9482/Reviewer_AJ7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9482/Reviewer_AJ7o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836750339, "cdate": 1761836750339, "tmdate": 1762921065824, "mdate": 1762921065824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a way to detect hallucinations in the context of RAG by training a generalized additive model on SAE-encoded feature vectors; token-level activations are channel-wise max pooled to produce instance level features via selection on mutual information. The authors compare against many baselines (though, slightly narrowly selected- they don't compare against pretty area-standard hallucination/factuality detectors like factscore/factcc/summac/stronger model internal state probe baselines) and show that their method performs better, across datasets like RAGTruth, Dolly, Aggrefact, and tofueval. Nonetheless, the approach is shown to reduce hallucination rates in a small mitigation study via token-level and instance-level feedback, by prompting the model to revise its original output with RAGLens' feedback. Ablations show that middle layers perform best in detecting hallucinations (though this isn't a new finding)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel approach that provides both token-level hallucination detections through SAE features. Empirical results show that this approach outperforms all tested contemporary baselines. The approach is generally very lightweight if you have a trained SAE for the model. The method represents a contribution towards utilizing SAE features for hallucination detection in RAG settings."}, "weaknesses": {"value": "The subsection \"4.3 GENERALIZATION ACROSS LLMS\" is not greatly named; the authors here only test if each model's own trained GAM outperforms the baseline of the model itself's chain-of-thought explanation of if it hallucinated or not. The method itself, due to needing to be trained on model-specific SAE features, doesn't translate across LLMs- that is, for every LLM, you will need to train your own model. The authors also don't really test generalization *across domains*, even for the same model- that is, how does a GAM trained on one domain (i.e. summarization) translate to detecting hallucinations in another (i.e. dialog).\n\nThe comparison models chosen miss out on some pretty standard area approaches for hallucination detection- factscore, FactCC, etc. Furthermore, novelty is limited- the paper largely combines existing ideas (SAEs, MI-based feature selection + additive modeling) rather than introducing a fundamentally new detection mechanism.  \n\nThe hallucination mitigation approach is not well evaluated. The evaluation is to see if the LLM can use the RAGLens feedback, when prompted, to mitigate the hallucination it caused. To evaluate whether the revisions are better, the paper mostly relies on LLM judges; only 45 examples are evaluated by humans, though the results are convincing and could use greater robustness."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HkAWhzwYA8", "forum": "hgBZP67BkP", "replyto": "hgBZP67BkP", "signatures": ["ICLR.cc/2026/Conference/Submission9482/Reviewer_rg9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9482/Reviewer_rg9H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862067878, "cdate": 1761862067878, "tmdate": 1762921065466, "mdate": 1762921065466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}