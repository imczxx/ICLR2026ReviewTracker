{"id": "0xiPcuWdl5", "number": 17285, "cdate": 1758274219480, "mdate": 1759897184811, "content": {"title": "AutoPR: Let's Automate Your Academic Promotion!", "abstract": "As the volume of peer-reviewed research surges, scholars increasingly rely on social platforms for discovery, while authors invest considerable effort in promoting their work to ensure visibility and citations. To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content. To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity (accuracy and tone), Engagement (audience targeting and appeal), and Alignment (timing and channel optimization). We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates substantial improvements, including a 604\\% increase in total watch time, a 438\\% rise in likes, and at least a 2.9x boost in overall engagement. Ablation studies show that platform modeling and targeted promotion contribute the most to these gains. Our results position AutoPR as a tractable, measurable research problem and provide a roadmap for scalable, impactful automated scholarly communication.", "tldr": "", "keywords": ["Academic Promotion", "Automatic Promotion", "Benchmark", "PRBench", "PRAgent", "Multi-Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a37fb7b547f63580944d08037d85a1ddbacfcb84.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AutoPR, a timely and ambitious endeavor to automate academic promotion. It proposes a novel task definition, a multimodal benchmark called PRBench, and a multi-agent framework, PRAgent, designed to transform research papers into engaging promotional content. The authors' motivation addresses a significant pain point for researchers in an era of information overload, where ensuring visibility and citations for published work is increasingly challenging. The empirical results demonstrate that PRAgent significantly outperforms direct LLM pipelines on PRBench and shows promising gains in a real-world study."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a highly relevant and novel task - automatic academic promotion — which directly addresses the challenges researchers face in gaining visibility and citations for their work.\n- The paper presents a commendable amount of work, encompassing the introduction of a new task, the construction of a multimodal benchmark, and the development of an end-to-end multi-agent framework. \n- The evaluation system is well-designed, featuring comprehensive metrics across Fidelity, Engagement, and Alignment. Crucially, it includes validation of the LLM-as-a-Judge against human preferences and comprehensive ablation studies are also reported."}, "weaknesses": {"value": "- While LLM-as-a-Judge is employed, its consistency with human preferences is notably low for several key evaluation metrics, particularly \"Visual Attractiveness\" (e.g., Pearson correlation < 0.1 for GPT-4o and 0.4859 for Qwen-2.5-VL-72B-Ins in Table 3). This limitation casts doubt on the robustness of the multimodal evaluation, given the importance of visual elements in promotion.\n- PRAgent's technical approach primarily involves an engineered orchestration of existing LLM capabilities and tools. The paper lacks significant algorithmic innovations at its core, relying more on effective system integration.\n- The real-world evaluation is restricted to a \"10-day in-the-wild study on RedNote\" with only 10 papers. This limited scope, specifically the absence of validation across diverse social media platforms (such as Twitter/X) with varying user behaviors and content styles, restricts the generalizability of the reported engagement improvements."}, "questions": {"value": "- The impressive real-world results in Figure 5 were achieved using GPT-5 as the backbone model. Could the authors explore and discuss the performance and generalizability of PRAgent when integrated with smaller, more accessible language models?\n- Equation (2) defines the AutoPR objective function with unspecified non-negative weights (\\alpha_1, \\alpha_2, \\alpha_3). Could the authors clarify how these weights are determined or optimized, and ideally, provide their values for transparency and reproducibility?\n- Although the paper mentions using open-access resources like arXiv for PRBench construction, a more explicit statement on the copyright and licensing terms for the entire dataset, including extracted paper content, figures, and particularly the human-authored social media posts, would be crucial for guiding future academic use."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I9BhIt1lE6", "forum": "0xiPcuWdl5", "replyto": "0xiPcuWdl5", "signatures": ["ICLR.cc/2026/Conference/Submission17285/Reviewer_mDwL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17285/Reviewer_mDwL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761289248271, "cdate": 1761289248271, "tmdate": 1762927227873, "mdate": 1762927227873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce the task of Automatic Promotion (AutoPR) in which a system is required to generate a faithful and engaging (multi-modal) post for social media or digital channels, given a scientific article.\nThe paper introduce a benchmark of 512 instances, each instance consisting of the peer-reviewed article (textual content, visual content, and supplemental material) along their promotion posts in social media platforms X and RedNote, as well as human-annotated factual check lists and engagement, alignment scores.\nAdditionally, the paper introduces PRAgent, a multi-llm framework tailored to this task that integrates content synthesis and style adaptation, showing considerable improvements over single-model baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The dataset was collected using a comprehensive human annotation protocol that ensures quality control\nS2. The proposed system tackles the main challenges in the task, such as content synthesis and style adaptation"}, "weaknesses": {"value": "W1. The benchmark section would benefit from analysis of statistical, qualitative features.\nW2. The discussion, experimental methodology would benefit from techniques in related tasks, such as scientific journalism, poster generation"}, "questions": {"value": "- Section 3, data statistics: this section would greatly benefit from statistical properties of the collected dataset, e.g. avg length of the input article, quantification of the visual items, avg length of the output post.\n- Calibration of the LLM-based evaluation metrics: the metrics proposed  in Section 3.2 are sound and relevant, however this section would benefit from details on how these judges were calibrated. For examples, see [1,2].\n- Standard evaluation methodology: Fidelity evaluation must be complemented with standard factuality / faithfulness evaluation, for which a plethora of metrics exist [3,4]. Factual error analysis and human evaluations would also shed light on model performance and the difficulty of the task. For more details, see [5,6]\n- PRAgent, as described in Section 4, would constitute a pipeline rather than a \"multi-agent\" system, given that an \"agent\" is a complex system that interacts with an environment (according to the broad consensus in academia, specially in RL). \n\n- L155,156: Regarding the posts extracted from X and RedNote, did the authors (of these posts) give their explicit consent to use these posts?\n- L158: Could you please elaborate on how the estimation of whether it is AI-generated content or not was conducted?\n- L447-456: The methodology following in this experiment does not align with standard practices in social networks. The standard practice for this kind of real-world experiment would be an A/B test run over *non-overlapping* segments of users. Otherwise, there is the risk that a user is exposed to both evaluated posts.\n\n[1] https://arxiv.org/abs/2410.12784\n[2] https://aclanthology.org/2025.gem-1.33/\n[3] https://aclanthology.org/2022.tacl-1.10/\n[4] https://aclanthology.org/2021.emnlp-main.529/\n[5] https://aclanthology.org/2023.emnlp-main.76.pdf\n[6] https://aclanthology.org/2022.emnlp-main.724/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Itssgye52A", "forum": "0xiPcuWdl5", "replyto": "0xiPcuWdl5", "signatures": ["ICLR.cc/2026/Conference/Submission17285/Reviewer_2BJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17285/Reviewer_2BJt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758288089, "cdate": 1761758288089, "tmdate": 1762927227594, "mdate": 1762927227594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents AutoPR, an automatic academic promotion system designed to generate high-quality promotional content for research papers. The system is assessed using a new benchmark, PRBench, which evaluates promotional content across three dimensions: Fidelity, Engagement, and Alignment. The paper introduces PRAgent, a multi-agent framework that automates the process in three stages: content extraction, multi-agent synthesis, and platform-specific adaptation. PRAgent outperforms traditional LLMs on various metrics, such as engagement and accuracy, in real-world experiments on platforms like RedNote."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The formalization of AutoPR as an explicit research problem is ambitious and well-justified given the current academic publishing and dissemination landscape. The problem description is clear (see Section 2, Figure 2) and convincingly linked to the challenges of information overload and the growing importance of scholarly visibility.\n\nS2: The release of PRBench, a paired dataset linking full academic papers to real human-written promotional posts, fills a critical gap and significantly enables further research. The benchmark construction is systematically described (Section 3.1) and includes rigorous annotation and quality protocols.\n\nS3: PRAgent is thoroughly described, with each stage—Content Extraction, Multi-Agent Synthesis, Platform-Specific Adaptation—broken down in mathematical and practical terms (Sections 4.1–4.3, Figure 3). The use of specialized agents and prompts (including hierarchical summarization and multimodal layout analysis) is a thoughtful engineering advance over generic prompting.\n\nS4: Extensive experiments are conducted using a wide swath of LLMs and multimodal models. Results in Table 1 and especially Table 2 are detailed and well-structured, showing clear, consistent gains for PRAgent over LLM baselines across almost all dimensions of evaluation. The findings are further supported by real-world validation (Figure 5), which demonstrates positive, large-magnitude improvements in actual user engagement (views, likes, watch time)."}, "weaknesses": {"value": "W1: The paper does not fully specify how multi-objective optimization weights $\\alpha_i$ are chosen (Section 2), nor does it clarify if these are tuned per platform or per sample. Similarly, the factual checklist verdict scoring and the calibration of LLM “judge” models (Section 3.2, Table 1) lack explicit documentation of inter-annotator reliability and the impact of cross-model disagreement. These details are essential for the scientific soundness and replicability of the evaluation process.\n\nW2: PRAgent's platform-specific adaptation, while a key module, appears to depend heavily on prompt engineering and surface-level modeling (e.g., hashtags and format—see Figure 4(c)), with little evidence of deeper modeling of platform affordances or active learning from audience feedback. Several dimensions of platform generalization (from Twitter to non-academic or emerging platforms) are not empirically validated.\n\nW3: The multi-stage, multi-agent approach raises practical concerns about resource consumption, latency, and scalability, none of which are addressed. For instance, processing high-resolution PDFs, running multi-modal LLMs, and orchestrating multiple agents likely demand significant compute, which may limit real-world adoption and accessibility."}, "questions": {"value": "Q1: Could the authors clarify the procedure for determining or tuning weights $\\alpha_i$ in the multi-objective optimization formulation? Are these weights static, learned, or hand-tuned? How sensitive are results to these settings?\n\nQ2: Can the authors extend PRBench or provide empirical results demonstrating generalizability of PRAgent to other domains (biomedical, humanities, etc.) or less traditional platforms?\n\nQ3: What are the computational costs (e.g., wall clock time, GPU/CPU usage, API costs) per paper processed through PRAgent as compared to direct LLM pipelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lpCmXkVMUR", "forum": "0xiPcuWdl5", "replyto": "0xiPcuWdl5", "signatures": ["ICLR.cc/2026/Conference/Submission17285/Reviewer_kB62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17285/Reviewer_kB62"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804389569, "cdate": 1761804389569, "tmdate": 1762927227348, "mdate": 1762927227348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AutoPR provides both the benchmark PRBench and the framework PRAgent to generate social media promotion material given a paper, evaluating their quality using the real-world promotional posts of these papers as a reference for fidelity, engagement, and alignment. Their PRAgent would extract text, figures, captions, etc., from papers using automated methods before synthesizing the promotional post using four collaborating agents, Logical Draft, Visual Analysis, Textual Enriching, and Visual-Text-Interleaved Combination. Finally, they adapt the output for specific platforms and append hashtags. In this research, X  (formerly Twitter) and RedNote are included in the PRBench, while a real-world test occurs on a RedNote account collecting the engagement metrics over a span of 10 days. Evaluation finds current LLMs struggle on PRBench (scores 31-71), with major limitations in factual completeness (missing 40%+ of key details), genuine engagement (42% lack hooks), and strategic platform adaptation (only 0.03 Jaccard similarity with human hashtags). In the real-world study involving posting both the vanilla prompting generated post and the post by PRAgent, PRAgent outperforms direct prompting by 7-20%+ on the benchmark, and in a separate 10-day real-world study on RedNote, achieves +604% watch time, +438% likes, and +329% views."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a genuine pain point in the academic community—the growing need for effective research promotion amid publication explosion—with strong empirical evidence (Figure 1a-c) showing the scale of the problem and preliminary correlation between promotion and citations. This makes the research immediately relevant and potentially impactful for the scholarly community.\n2. This research also produced the dataset PRBench, a carefully curated multimodal benchmark containing 512 paper-post pairs linking peer-reviewed arXiv papers (June 2024-2025) to their human-authored promotional posts from Twitter/X and RedNote. The dataset includes expert annotations with proper quality control procedures (multiple annotators, consensus deliberation for discrepancies) and covers three key evaluation dimensions: fidelity, engagement, and alignment.\n3. The paper formally defines AutoPR with clear mathematical formulation (Equations 1-2) as multi-objective optimization, explicitly capturing the tension between factual accuracy, audience engagement, and platform-specific adaptation. This established academic promotion as a tractable research problem and provides a systematic framework for future research in an area previously lacking formal treatment.\n4. The paper introduces 11 evaluation metrics across three dimensions, combining both intrinsic quality scores (factual checklist, hook strength, visual attractiveness, hashtag strategy) and preference-based comparisons (professional interest, broader interest, platform interest). The metrics are grounded in relevant theory (platform affordances, communication studies) and validated against human judgments.\n5. The systematic evaluation of 21 models (both open-source and proprietary) saw specific failure modes: 92% of fidelity errors fall into numerical/method/terminology categories, 42% of posts lack engagement hooks, and generated hashtags show only 0.03 Jaccard similarity with human-authored ones. This diagnostic analysis provides clear direction for future improvements.\n6. According to the evaluation, PRAgent demonstrates 7-20%+ improvements over direct prompting baselines across nearly all evaluated models on the benchmark, showing that the framework's structured approach provides value regardless of the underlying LLM capabilities, which suggests the approach is robust and generalizable."}, "weaknesses": {"value": "1. I'd like to suggest some metaphysical issues first. Engagement and academic communication might not go hand in hand. As such, with the formal communication such as conferences, seminars, and modern tools that automated the discovery of research literature, it remains to question the purpose of such self-promotion; it might be effective in popular education of scientific knowledge, while yielding potentially less reflection in academic engagement, such as follow-up papers and citations.\n2. The language in which the promotional material is written also requires disclosure and study. While X (formerly Twitter) is a popular social media featuring diverse demographics and cultures, it is to my understanding that RedNote is monolithically Chinese-majority. However, this linguistic division is not reported in the literature; the language of which the promotion material also require disclosure and ablation studies.\n3. Following 1. and 2., a similar experiment on X should be beneficial where you collect the engagement metrics for the generated promotional posts. This could be time-consuming, so I believe acknowledging this limitation and suggesting future research is enough.\n4. New accounts with 0 followers achieved 5,059 and 1,178 views respectively on RedNote. This is not a very realistic scenario since researchers most often post from established personas with a connected social media network, which could help to deliver the information to the appropriate audience who are also doing research on the same topics."}, "questions": {"value": "1. We might need longitudinal studies tracking citation patterns, not just immediate engagement, or to distinguish between \"popular science communication\" vs \"academic promotion\" as different objectives.\n2. We should complete discussion of whether platform effects are confounded with language effects; could be done by ablation showing performance differences across languages or other means.\n3. The asymmetry (benchmark has X, real-world only RedNote) should at least be acknowledged as a limitation.\n4. This is just a suggestion but you might use ar5iv or the html preview on arXiv for obtaining the structured text and figures for each paper; but this might not apply every time since they did not include all papers. I am only suggesting this as it might help to reduce cost and error margins when curating the dataset, so you can scale it up faster. Alternatively, parsing the source files of the submissions on arXiv might be a fail-safe to extract the text, tables, and figures of academic papers without much computation overhead.\n5. We could adapt established academic accounts (with owners' permission), or at least disclose follower counts and acknowledge this major limitation. It would be ideal if we have tracking on who is engaging (are they academics? in the relevant field?) but this could raise ethical concerns, or just downright not possible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k625hO7zgk", "forum": "0xiPcuWdl5", "replyto": "0xiPcuWdl5", "signatures": ["ICLR.cc/2026/Conference/Submission17285/Reviewer_g5Lq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17285/Reviewer_g5Lq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178905102, "cdate": 1762178905102, "tmdate": 1762927226949, "mdate": 1762927226949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}