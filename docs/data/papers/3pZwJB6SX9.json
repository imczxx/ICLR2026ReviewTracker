{"id": "3pZwJB6SX9", "number": 15408, "cdate": 1758251039956, "mdate": 1759897308888, "content": {"title": "Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting", "abstract": "Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries—but learning high‑quality oblique splits is NP‑hard, and practical methods still rely on slow search or theory‑free heuristics.\nWe present the Hinge Regression Tree (HRT), which reframes each split as a non‑linear least‑squares problem over two linear predictors whose max/min envelope induces ReLU‑like expressive power.\nThe resulting alternating fitting procedure is exactly equivalent to a damped Newton method within fixed partitions, which in practice demonstrates fast, stable convergence and allows an optional ridge regularization.\nWe further prove that HRT’s model class is a universal approximator with an $O(\\delta^2)$ rate, and validate it on synthetic and real‑world benchmarks.\nHRT achieves competitive performance compared to single-tree baselines and maintains a more compact structure—making it a transparent and theoretically grounded regressor.", "tldr": "", "keywords": ["Optimization", "Regression trees", "Newton method", "Convergence"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d8d69a7148701d58c63a8e2f49eb9e0b1eeb155.pdf", "supplementary_material": "/attachment/0022d85e3b90f12d492f75161e79fbf85e9674e1.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces hinge regression tree (HRT), a new method for learning oblique regression tree splits. The key idea is to formulate each split as a nonlinear least-squares problem over two linear predictors, yielding an alternating fitting procedure that is mathematically equivalent to a damped Newton method within fixed partitions. HRT demonstrates competitive performance in benchmarking experiments compared to standard baselines. Overall, the proposed method appears promising. My comments are as follows:\n\n- Section 3: Consider adding a short subsection before Section 3.1 that briefly recaps oblique decision tree methods. The current text is related but does not explicitly state that this approach is an instance of oblique decision tree regression.\n\n- Line 154: I believe this represents a linear decision boundary, rather than a hinge-based one. \n\n- Line 191: The optimization behavior accounting for partition changes is not analyzed, leaving a disconnect between the theoretical guarantee and the practical performance. The empirical results show that the algorithm converges on real data, which is reassuring; however, I would still view the algorithm as heuristic, and suggest avoiding terms such as “rigorously” (line 49) or “solid theoretical foundation” (line 50).\n\n- The computational efficiency of the proposed algorithm has not been comprehensively evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "See summary above."}, "weaknesses": {"value": "See summary above."}, "questions": {"value": "See summary above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jvFqkkEGJ4", "forum": "3pZwJB6SX9", "replyto": "3pZwJB6SX9", "signatures": ["ICLR.cc/2026/Conference/Submission15408/Reviewer_AC5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15408/Reviewer_AC5G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938917990, "cdate": 1761938917990, "tmdate": 1762925686462, "mdate": 1762925686462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Hinge Regression Tree (HRT), an algorithm for building oblique regression trees by jointly learning two linear predictors at each split. Each node models its output as a hinge function, leading to a piecewise linear regression surface. Training alternates between (i) fitting the two linear models with ordinary least squares (optionally ridge-regularized) and (ii) reassigning data to the branch that yields the higher prediction. The authors describe this alternating fitting as a damped Newton method within fixed partitions and show that, with small damping factors, it converges stably in practice. They also restate a known universal-approximation result for piecewise linear models and present experiments on synthetic and real regression datasets, comparing HRT to CART, TAO, DGT, DTSemNet, and XGBoost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The node training procedure (alternate OLS fits for two linear predictors under a hinge) is simple, transparent, and easy to re-implement. The paper provides explicit pseudocode and a complete build procedure, which supports reproducibility.\n2. Compared to the baseline methods, HRT enjoys competitive performance.\n3. The paper is mostly clear and easy to understand."}, "weaknesses": {"value": "1. The contribution is overstated. Within fixed partitions, the update equals a Gauss–Newton/OLS step (standard), which is not difficult to prove. However, the hard part in the decision tree method is partition switching. Unfortunately, there is no guarantee of monotone decrease or convergence of the alternating fit-reassign procedure. I am not convinced that this part should be left to future work, as this guarantee is far more interesting and important than the updates in each subspace to show the validity of the proposed method.\n\n2. The discussion of the damping effect is insufficient. For example, the results improvement for $\\mu=0.01$ (damping) and $\\mu=1.00$ (no damping) is only marginal in Tables 4 and 5. If using $\\mu=1.00$ (OLS) and the standard fallback algorithm can provide competitive results, then the advantage of the damping term, which is a contribution of the paper, is unclear, and the reformulation into Gauss-Newton update seems to be unnecessary.\n\n3. More benchmarks could be given to further show the effectiveness of the methods and examine the alternating fit-reassign procedure. In particular, high-dimensional datasets such as Communities & Crime (UCI), BlogFeedback (UCI), and baselines such as LightGBM can be included.\n\n4. Some of the improvements of HRT over other methods are marginal (within 1%). Statistical significance tests should be given to potentially make many differences statistically distinguishable.\n\n5. Although the authors discussed the complexity, there is no comparison of the computational time, especially how much time is sacrificed for using damped optimization. This makes the efficiency of the proposed method in practice hard to assess. \n\n6. The technical contribution of the theory is limited. The approximation of piecewise linear functions to any $C^2$ target in many similar cases is known, e.g., Breiman (1993), and ReLU approximation Barron (1993). The sample complexity of Oblique trees is also established in Cattaneo et al. (2024). Therefore, the approximation result is not surprising.\n\nBarron (1993): Universal approximation bounds for superpositions of a sigmoidal function, IEEE Transactions on Information Theory"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YsRgpm30e6", "forum": "3pZwJB6SX9", "replyto": "3pZwJB6SX9", "signatures": ["ICLR.cc/2026/Conference/Submission15408/Reviewer_e2Hz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15408/Reviewer_e2Hz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955789884, "cdate": 1761955789884, "tmdate": 1762925685537, "mdate": 1762925685537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to learn an oblique decision tree with linear leaf predictors using the traditional greedy recursive partitioning approach but with the variation on the splitting procedure. The splitting procedure uses the hyperplanes at both leaves to define the splitting hyperplane: $\\mathbf{\\theta_1} - \\mathbf{\\theta_2}$. The learning algorithm uses the current split to define Newton updates, but it is not clear how it relates to finding the global optimum. Experiments are performed on synthetic and real datasets showing improved accuracy of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Oblique decision trees are an important model class which has not been as widely studied. This paper proposes one approach in learning this relatively unexplored model class."}, "weaknesses": {"value": "* Objective function is defined only for a splitting criterion. It is not clear what is the global objective being optimized.\n\n* During splitting, the internal decision node hyperplane is defined by its two linear leaf weights. Ideally, these sets of parameters (decision split hyperplane and its two children linear weights) must be independent. It is not clear why this way of coupling is used.\n\n* It is not clear whether the proposed algorithm optimizes eq. 1. No theoretical guarantees of convergence or optimality is shown.\n\n* The comparison experiments on real world data are not apples-to-apples. The baselines of CART and TAO, for example, use constant leaves, while the proposed approach uses linear leaves. Piecewise constant models are in general not suitable for regression problems."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "26QAmhL3Wg", "forum": "3pZwJB6SX9", "replyto": "3pZwJB6SX9", "signatures": ["ICLR.cc/2026/Conference/Submission15408/Reviewer_GFxH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15408/Reviewer_GFxH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061535889, "cdate": 1762061535889, "tmdate": 1762925685044, "mdate": 1762925685044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel splitting method for oblique regression trees. It uses two linear predictors and the splitting depends on which predictor is larger or smaller. This splitting problem is solved via an alternating fitting procedure which is equivalent to a damped Newton method within fixed partitions. This paper proves that the oblique regression tree with such a splitting mechanism is a universal approximator."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well written with clear content organization, method description, and mathematical notation.\n2. The splitting mechanism of oblique regression tree is novel, i.e. the combination of two linear predictors and hinge function.\n3. This paper proves that the proposed method is a piece-wise linear model class which is a universal approximator. Thus its expressive power is underpinned by theoretical foundation as well as experimental results."}, "weaknesses": {"value": "1. Some implementation details are missing, such as how to initialize two linear predictors.\n2. The formal global convergence proof is not provided. I understand this is challenging. But I think the authors can try some weaker conclusions. For example, under what conditions is the loss function monotonically decreasing?\n3. It is better to add experiments for binary classification tasks."}, "questions": {"value": "On average, how many iterations are needed to split a node?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MvTateWfW1", "forum": "3pZwJB6SX9", "replyto": "3pZwJB6SX9", "signatures": ["ICLR.cc/2026/Conference/Submission15408/Reviewer_GmnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15408/Reviewer_GmnV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095693832, "cdate": 1762095693832, "tmdate": 1762925684429, "mdate": 1762925684429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}