{"id": "hFqq79xwGV", "number": 21786, "cdate": 1758321745971, "mdate": 1759896903010, "content": {"title": "Finite‑Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation", "abstract": "Distributionally robust reinforcement learning (DRRL) focuses on designing policies that achieve good performance under model uncertainties. In particular, we are interested in maximizing the worst-case long-term discounted reward, where the data for RL comes from a nominal model while the deployed environment can deviate from the nominal model within a prescribed uncertainty set. Existing convergence guarantees for robust temporal‑difference (TD) learning for policy evaluation are limited to tabular MDPs or are dependent on restrictive discount‑factor assumptions when function approximation is used. We present the first robust TD learning with linear function approximation, where robustness is measured with respect to the total‑variation distance uncertainty set. Additionally, our algorithm is both model-free and does not require generative access to the MDP. Our algorithm combines a two‑time‑scale stochastic‑approximation update with an outer‑loop target‑network update. We establish an $\\tilde{O}(1/\\epsilon^{2})$ sample complexity to obtain an $\\epsilon$-accurate value estimate. Our results close a key gap between the empirical success of robust RL algorithms and the non-asymptotic guarantees enjoyed by their non-robust counterparts. The key ideas in the paper also extend in a relatively straightforward fashion to robust Q-learning with function approximation.", "tldr": "Finite-time convergence guarantees for robust model-free TD learning and Q-learning with linear function approximation for commonly used uncertainty sets from single-trajectory data, without restrictive discount-factor assumptions.", "keywords": ["Reinforcement Learning Theory", "Distributionally Robust Reinforcement Learning", "Finite-Time Convergence Guarantee", "Two-Time-Scale Stochastic Approximation", "Function Approximation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/638242474c2386e3cf94bae4cec96060de88c6df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work provides a non-generative robust TD algorithm and establishes its finite-time convergence bounds. It analyzes the required sample size under both total variation and Wasserstein distances. Notably, the convergence of the proposed robust TD algorithm does not rely on strict contraction assumptions. Overall, this work introduces a novel algorithm and offers a fundamental understanding of distributionally robust reinforcement learning (DRRL)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  This work provides the robust TD algorithm, and firstly does not rely on extra assumptions within the TV and Wasserstein uncertainty set. \n\n2. This work provides solid theoretical result."}, "weaknesses": {"value": "1. The work approximates the Lagrange multipliers using linear function approximation. This raises a concern regarding whether the assumption is reasonable and whether the resulting approximation error can be sufficiently small, given that the Lagrange multipliers induce a nonlinear correction within the Wasserstein uncertainty set.\n\n2. Is the parameter $\\mu$ sufficiently small? The final result contains the term $\\epsilon_{\\text{approx}} / \\mu$, which suggests potential instability or sensitivity to the choice of $\\mu$.\n\n3. There appears to be a typo in the definition of $G$ — the last line should use $G$ instead of $g$.\n\n4. The definitions and assumptions in equations (19), (20), (22), and (23) in the appendix seem unreasonable or inconsistent with the main formulation. Further clarification or justification would be helpful."}, "questions": {"value": "Please check the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NaN"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ten6onqy93", "forum": "hFqq79xwGV", "replyto": "hFqq79xwGV", "signatures": ["ICLR.cc/2026/Conference/Submission21786/Reviewer_Qgkf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21786/Reviewer_Qgkf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761355084601, "cdate": 1761355084601, "tmdate": 1762941930996, "mdate": 1762941930996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper derives a finite-time bound for distributionally robust TD-learning with linear function approximation. It primarily focus on the robustness with respect to the transition probability; that is, it pre-assumes the transition probability falls into a given set, named uncertainty set, and it considers the worst-case value function among the given uncertainty set. The linear feature is adopted to approximate the Q-function. The goal is to learn the robust Q-function under this setting. A general assumption is given to characterize the interested uncertainty set; two well-known uncertainty sets, wasserstein and total varioation uncertainty sets satisfy the given condition. Then the convergence guarantee is given based on the scheduler."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper is addressing an important problem in robust RL; that is, how to accurately and efficiently solve the robust value function. The presented results indicate that under the linear approximation (with some traditional requirements), the robust TD-learning will finally learn the robust value function in the desired complexity. \n\n The theoretical analysis sounds to me as it matches the best-known complexity for the non-robust RL literature under the approperiate parameter setting. All related assumptions to guarantee the convergence are also used in non-robust RL literature. Taking the target network  also introduces a new technique in robust RL."}, "weaknesses": {"value": "1. Turnning the optimization problem into a distributionally robust optimization problem and applies the dual form is not new. Assumption 1 simply says that this optimization problem can be solvable in the desired complexity. It typically cannot be considered as a novel contribution. \n\n2. The presentation is not very clear:\n    1. Many notations are used without defining them or being defined somewhere hard to find. For example, $\\mu$ and $C_e$ in Line 352. \n    2. And the complete form of $C^\\star$, $C_1$, and $C_2$ are not very helpful here. And the author doesn't elaborate anything on them. \n    3. In Line 390, \"it requires c to be chosen sufficiently large\". But it is unclear why it is an issue; is there any problem for using  $1/\\mu$?\n    4. Some statements do not have explainations; see my questions below. \n\n3. No experiments validating the necessasity of the two-time scale. \n\n4. Writing issues:  The author commonly omitts punctuation marks in formulas.  Many LaTeX issues."}, "questions": {"value": "1. Can the proposed Assumption 1 motivate other uncertainty sets beyond the total variation and Wasserstein? As those distances have been well-studied (e.g. the total variation has been covered by the IPM from *Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation* under linear function approximation), it is hard to find how useful the proposed concept is; it just sounds like for convenience of analysis.\n\n2. The author commented that \"Prior work Zhou etal.(2023) circumvents this by imposing restrictive assumptions on $\\gamma$\". I didn't see their restrictive assumptions. Can the author further clarify it?\n\n3. How should I understand the complexity calculated in Corollary 1? Has the author included the proof somewhere?\n\n4. Can author include some experiments? It is hard to tell if the two-time scale is necessary or more efficient. It would be better to\n    1. Compare it with the single-time scale baseines.\n    2. Compare with robust TD-learning in IPM uncertainty sets used in  Zhou et al.(2023)  *Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation*.\n    3. Validate the compatibility of this approach with policy gradient or other policy gradient based algorthms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "urJ2u89Fyd", "forum": "hFqq79xwGV", "replyto": "hFqq79xwGV", "signatures": ["ICLR.cc/2026/Conference/Submission21786/Reviewer_FRHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21786/Reviewer_FRHC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925815817, "cdate": 1761925815817, "tmdate": 1762941930759, "mdate": 1762941930759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides finite-time convergence guarantees for distributionally robust TD learning with linear function approximation. It analyzes a model-free, two time scale scheme. The paper derives a non-asymptotic error bound showing $O(1/\\epsilon^2)$ sample complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper provides a finite sample analysis of distributionally robust policy evaluation with linear approximation.\n+ The paper is rigorous and mathematically sound (although I did not check all the proofs) giving finite-time analysis of distributionally robust TD with linear FA.\n+ Useful theoretical work for robust RL."}, "weaknesses": {"value": "- The results are restricted to linear function approximation under assumptions, which limits applicability to large-scale or nonlinear deep RL. \n- The analysis seem to require very restrictive and strong assumptions, e.g., Assumption 2 requiring that the policy induces an irreducible and aperiodic Markov chain under P0 (hence, mixing assumptions), bounded features, exact projection operator, etc. These conditions are generally unrealistic in RL environments which limits applicability. Also the constants hidden in $\\tilde{O}$ may grow poorly with the problem parameters.\n- I am not sure what the key technical novelty in analysis is in relation to prior robust TD or adversarial contamination analyses. \n- I did not see clear guidance on how to select the ambiguity radius or choose between different uncertainty sets (TV and Wasserstein) from data.\n- There is no experimental validation. Some empirical tests on benchmarks would better demonstrate the utility of the bounds."}, "questions": {"value": "1) What specific innovations allow finite time bounds compared to earlier analyses?\n2) How tight are the obtained rates? Can you show matching lower bounds? \n3) Can the approach extend beyond linear FA?\n4) Are the assumptions made in the paper necessary? Can some of the assumptions be relaxed or dispensed with?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mo6d36MNc4", "forum": "hFqq79xwGV", "replyto": "hFqq79xwGV", "signatures": ["ICLR.cc/2026/Conference/Submission21786/Reviewer_XZYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21786/Reviewer_XZYB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979814477, "cdate": 1761979814477, "tmdate": 1762941929853, "mdate": 1762941929853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses temporal-difference (TD) learning with linear function approximation for robust discounted Markov decision processes (MDP) under Total Variation (TV) and Wasserstein uncertainty in transition dynamics. Namely, the authors employ a two tier approach with varied time scales - one to address the inner optimization problem of the dual variables for each state-action pair and another to effectively parameterize the dual variables - which ultimately allow for the derivation of a sample complexity of $\\tilde{O}(1/\\epsilon^2)$ for uncertainty sets satisfying specific conditions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The problem being approached is highly relevant and motivated with respect to recent advances in theoretical robust RL.\n- The use of a target network mechanism to overcome the projection mismatch arising from linear function approximation to facilitate stable convergence in finite-time enables the algorithm to alleviate the non-contractive nature of the projected robust Bellman operator $\\Pi\\mathcal{T}^\\pi_r$ is fascinating. By having this occur at a \"slower\" rate, the algorithm can efficiently address the inner optimization problem of finding the worst-case distribution.\n- Through the above, the author's were able to derive the final sample complexity in Corollary 1, which aligns with results in the non-robust case. This theoretical work then makes progress on closing the sim-to-real gap."}, "weaknesses": {"value": "- While useful, this work has limited scope. Specifically, in practice one often wants to find some optimal policy $\\pi^{*}$, not just evaluating some arbitrary policy, thus limiting the practical significance of the work. The authors briefly discuss a Q-learning extension, however, no formal justification is made, making it's contribution indirect.\n- Perhaps more pressing than this is that I believe that there is an error in the proof of Lemma 2. Specifically, Part 3 of Assumption 1 requires an unbiased estimator for the objective function $F(\\lambda^a_s)$, meaning that $\\mathbb{E}[\\sigma]=F(\\lambda^a_s).$ The target function is then $F(\\lambda^a_s)=\\mathbb{E}[\\min\\{V(X),\\lambda^a_s\\}]-\\delta\\lambda^a_s$. On line 10 of Algorithm 1, you use equation 20 to find $\\sigma(\\cdot;\\cdot,\\cdot).$ However, by optimizing for $a=\\lambda^a_s\\in\\\\{-\\frac{1}{1-\\gamma},\\frac{1}{1-\\gamma}\\\\}$ as in equation 18, equation 20 would only hold if $\\lambda^a_s=1$. But from line 122, $0<\\gamma<1$ which would imply that the R.H.S of equation 20 should be $\\min(V(S'),\\lambda^a_s)-\\delta\\lambda^a_s$. As written currently, we can see that $bias=\\mathbb{E}[\\sigma]-F(\\lambda^a_s)=\\delta\\lambda^a_s-\\delta$. Putting this aside for a moment, how applicable is your algorithm in practice with the underlying assumptions?\n- There is not any empirical validation of the proposed algorithm.\n- The claim on lines 115-116 is incorrect, see [1].\n- Significant number of typos and inconsistent notation. See below for suggestions on actionable edits.\n\n[1] Zachary Roch, George Atia, and Yue Wang. A Reduction Framework for Distributionally Robust Reinforcement Learning under Average Reward. 2025."}, "questions": {"value": "- $\\Delta_\\mathcal{S}$ on line 121 is not defined. Also, use different notation to mean the same thing on lines 235, 246, and elsewhere.\n- Notation for the states and actions are not consistent/clear depending on it's respective use. i.e. use of $s, s', S', S_t$.\n- Reuse of $r$ for both the reward function and to denote a robust MDP, robust value function, etc.\n- Need citations for the $(s,a)$-rectangularity assumption, i.e. [2,3].\n- Increased clarity by showing $\\forall s\\in\\mathcal{S}$ and $\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A}$ when formally writing equations like on line 158 versus equation 50.\n- $M$ used in Assumption 1 without definition.\n- $X$ used without defining several places, i.e. line 239.\n- $d$ used on line 268 before being defined. What is the difference between $\\lambda_d$ and $d_\\lambda$ as seen on lines 268 and 270, respectively?\n- $\\mathcal{M}_\\nu$ used in equation 6 before being defined.\n- Are $\\theta^t_k$ and $\\theta_{t,k}$ referring to the same thing?\n- If $C_{mix}$ is the robust mixing time, it should be formally defined and discussed.\n- $c$ is not defined in the algorithm. Similarly $C_e$ on line 352.\n- Lemma 1 should come before Theorem 1. There is also not a clear distinction in the wording of these.\n- \"The noise term $n^\\theta_{k+1}$ collects all remaining terms\" on line 423 is not precise.\n- Typo of \"State\" instead of \"stae\" on line 057, line 428, \"this\", on line 465, \"introduction\", in the uncertainty set on line 643, at the end of line 953, and a missing space on line 1492.\n- Use of $W_\\ell$ when discussing TV in the appendix.\n- Reuse of notation starting on line 646 where $a\\in[m,M].$\n- Extra line in the equation on line 799.\n- Incomplete sentence on line 819 and on line 1497.\n- Is $\\lambda(s,a)$ on line 825 the same as $\\lambda^a_s$?\n- $B_\\nu$ is used in the appendix in equation 25 before being defined.\n- Period on the wrong line in equation 26 and line 998.\n- Does MDS on line 1026 refer to a Martingale Difference Sequence?\n- What does the subscript of $\\cdot_{op}$ refer to in the notations section in the appendix? Also, the notation section should go before the main proofs and where the notation is subsequently used.\n- No clear distinction when a proof ends.\n\n[2] Iyengar, G. N. Robust dynamic programming. 2005.\n\n[3] Nilim, A. and El Ghaoui, L. Robust control of Markov decision processes with uncertain transition matrices."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Though the author's disclose the use of LLM's to polish the language in certain parts of the paper and further say that all technical content, proofs, and conclusions are the sole work of the authors, I have serious doubts that this was the extent that LLMs were used, though it is difficult to definitively determine. Namely:\n- There are an extensive number of typos, reuse of notation, organization, inconsistencies, and non-standard formatting as compared with many theory-based papers. Please see the questions section of my review for an abbreviated list of these action items that I bring to the author's attention.\n- While the authors use the notation of $diag(\\cdot)$ on line 169, \"$operatornamediag(\\cdot)$\" appears on line 1456.\n\nWith the above put together, I suspect that this paper employed LLMs to an extent further than initially disclosed which calls into question the integrity of the underlying work, even if it was pieced together by the authors. The lack of empirical validation, though not necessarily suspicious by itself for a theory-based paper, indicates a higher LLM usage than initially disclosed in my opinion. Therefore, I believe this work may need a closer review."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rEi2qkYbFv", "forum": "hFqq79xwGV", "replyto": "hFqq79xwGV", "signatures": ["ICLR.cc/2026/Conference/Submission21786/Reviewer_bKTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21786/Reviewer_bKTe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762275000602, "cdate": 1762275000602, "tmdate": 1762941929637, "mdate": 1762941929637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}