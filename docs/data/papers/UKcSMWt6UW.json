{"id": "UKcSMWt6UW", "number": 1177, "cdate": 1756859900414, "mdate": 1759898223504, "content": {"title": "Local Linear Convergence of Projected Gradient Descent: A Discrete and Continuous Analysis", "abstract": "Projected Gradient Descent (PGD) method has been successfully applied to various machine learning problems. The prior works demonstrate that PGD, as a classical discrete iterative method, has sublinear convergence in the case that the objective function is convex and smooth. In this paper, we explore the local linear convergence properties of PGD under this case from both discrete and continuous perspectives. Specifically, we focus on optimization problems with a general convex and smooth objective function constrained by $\\mathbb{B}(\\pmb{0}, \\epsilon)$, and present the following principal results: $\\textbf{(I)}$ We derive an ordinary differential equation (ODE) that arises as the limit of PGD; $\\textbf{(II)}$ We establish convergence rate bounds for PGD in both discrete-time and continuous-time scenarios, with the continuous-time analysis motivated by the derived ODE. The bounds in both scenarios support each other and consistently indicate that PGD achieves a local linear convergence rate. Finally, we conduct experiments to validate theoretical results $\\textbf{(I)}$ and $\\textbf{(II)}$, and the experimental outcomes closely align with our theoretical findings.", "tldr": "", "keywords": ["Projected gradient descent"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72ba1942fa52ac314bf7e9b920b953e06d9c0c30.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies projected gradient descent (PGD) under Euclidean ball constraints. It proves linear convergence under appropriate initialization and derives an ordinary differential equation (ODE) with trajectories approximating the discrete dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper derives an ODE for optimization problems on the sphere, strengthening the connection between continuous-time and discrete optimization."}, "weaknesses": {"value": "1) The experimental section is minimal: plots that compare convergence and toy benchmarks (linear, log-sum-exp, logistic regression), with unspecified datasets. Larger experiments are required.\n\n2) For smooth convex functions with minimizer lying outside of the considered compact set $\\mathcal{C}$, functions might be strongly-convex on this given compact. For instance, if the function is twice-differentiable, and \n$$\\lambda_{\\min}\\left(\\nabla^2 f(x)\\right) \\geq c > 0,$$\nwe might take \n$$\\mu = \\min_x \\lambda_{\\min}\\left(\\nabla^2f(x)\\right).$$\nConsequently, linear convergence of projected gradient descent can hold locally on such compact subsets, not only for the case of ball constraints. Therefore, linear convergence under the Euclidean constraints is an incremental result.\n\n3) The paper lacks a thorough comparison with prior work on projected gradient methods."}, "questions": {"value": "1) How does the fast-convergence theory extend to constraint sets other than Euclidean balls?\n2) Theorems 3.1–4.3 require the initial point to lie in $\\mathbb{B}(\\delta^{\\Delta},\\,\\varepsilon\\sqrt{2})$. What fraction of the initial ball’s volume does this region occupy as the dimension grows?\n3) If PGD is initialized arbitrarily within the ball, how many iterations are needed before it enters the linear-convergence region?\n4) How can the constant $C_4$ in Theorem 4.4 be computed (or tightly bounded) in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No additional ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vhfPHXvXqO", "forum": "UKcSMWt6UW", "replyto": "UKcSMWt6UW", "signatures": ["ICLR.cc/2026/Conference/Submission1177/Reviewer_pCq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1177/Reviewer_pCq3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682473843, "cdate": 1761682473843, "tmdate": 1762915698355, "mdate": 1762915698355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the projected gradient descent (PGD) algorithm, where the projection occurs on $\\mathbb{B}(0,\\varepsilon)$, applied to convex functions with $M$-Lipschitz gradients. They show that, locally, it exhibits linear convergence, which contrasts with the usual sublinear convergence speed of gradient descent for convex functions. The authors also introduce an ODE, that is the limit with vanishing stepsize of the studied PGD algorithm. The local linear convergence property also occurs for the solution of this ODE. Finally, experiments are conducted to validate that (i) the ODE accurately models PGD and (ii) the local linear convergence property is observed empirically."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I believe the contributions are interesting, and may challenge an usual intuition about the convergence of first-order algorithms for convex functions. The derived ODE, continuous equivalent of PGD, seems to be new, and offers interesting perspective to further study this problem. Finally, the paper is overall clear, and the introductory figures effectively illustrate the key concepts."}, "weaknesses": {"value": "- I suggest to add a paragraph, at least in the \"related works\" appendix, about existing local convergence results for PGD.\n- The purpose of Section 5 is not clear to me. It looks like a summary of the results of Section 3 and 4. Am I missing something ? Otherwise, I suggest to delete or at least reduce this section, and maybe, accordingly to one of my question, to use the saved space to elaborate more on the discrete setting. \n- Remark 3.2: \"Theorem 3.1 remains valid even if $\\Phi(\\delta)$ doesn’t have $\\delta^\\ast$\". I think this sentence should be clarified.\n\n\nMinor remark:\n- For ease of reading, I suggest to recall in Section D the expression of ODE (3).\n- There is a typo in Section 6.2.2: \"PGD exhabits linear convergence rate\""}, "questions": {"value": "- I am surprised that the linear rates of Theorem 3.1 do not depend of the smoothness constant $M$, which is to my knowledge rather unusual. Do you have  an intuitive explanation about why it is so ?\n- I believe it is a bit weird that in the main text, while the continuous section is almost two pages long, the discrete section is only half a page long. In particular, the fact that the local linear convergence property of Theorem 3.1 can be extended to a more general neighborhood is only stated as a remark, and deferred in appendix. In the end, if the continuous perspective can offer interesting insights, discrete results are the one we can use in practice. Could you please explain the reason for your choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RcbVrSxy27", "forum": "UKcSMWt6UW", "replyto": "UKcSMWt6UW", "signatures": ["ICLR.cc/2026/Conference/Submission1177/Reviewer_bMVv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1177/Reviewer_bMVv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896879633, "cdate": 1761896879633, "tmdate": 1762915698231, "mdate": 1762915698231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers the minimization of a convex and Lipschitz smooth function over a ball and analyzes the Projected Gradient Descent (PGD) method for such problems. The main results consist of (1) deriving a continuous-time ODE that describes the limiting behavior of the PGD iterations as the step size goes to 0; and (2) establishing a local linear convergence rate of PGD, both in the discrete-time and continuous-time setting."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The geometric perspective behind the proof techniques seems original, although quite specific. \n\nI didn’t check the proofs in detail, but the local linear convergence rate result for PGD on problem (1) under the given assumptions is correct. See also *Weaknesses*."}, "weaknesses": {"value": "The authors seem unaware of the importance of quadratic growth conditions in the context of linear convergence rates. Relevant literature is missing (e.g., D. Drusvyatskiy and A. Lewis, Error Bounds, Quadratic Growth, and Linear Convergence of Proximal Methods) or not adequately discussed (e.g., (Necoara et al., 2019) is mentioned in the context of *sublinear convergence*, whereas it establishes *linear convergence* under `quadratic functional growth’). \n\nMore importantly, the main result on local linear convergence becomes somewhat trivial if familiar with quadratic growth conditions. The assumption that the unconstrained minimizer(s) is (are) *outside* the ball ensures that a quadratic growth condition holds. Indeed, the optimality conditions read $-\\nabla \\Phi(\\delta^{\\bigtriangleup}) = \\lambda \\delta$ for some multiplier $\\lambda \\geq 0$. If $\\delta^\\bigtriangleup \\notin \\mathbb{B}(0, \\epsilon)$, this implies that $\\nabla \\Phi(\\delta^{\\bigtriangleup}) \\neq 0$ and hence $\\lambda > 0$. By convexity of $\\Phi$ it follows that the Lagrangian is strongly convex around $\\delta^\\bigtriangleup$, and this in turn implies that $\\Phi$ satisfies a quadratic growth condition.\n\nAs stated by the authors, the considered problem is very specific (ball constraint)."}, "questions": {"value": "Suggestions / Remarks\n\nThe citations in this work are not always accurate. For example, when listing methods like interior point, Lagrange multiplier or even PGD methods, it would be better to cite classical works, rather than some recent works. Besides, when the constraint is a simple ball, it arguably does not make a lot of sense to use interior point or Lagrange multiplier methods.\n\nThe function $F$ in equation (4) is not defined in the main text. The required properties (e.g., in Lemma D.1) deserve to be discussed, I believe.\n\nAt lines 38-40 the authors mention *the* equilibrium (stationary) point of $\\Phi$. A convex function does not necessarily have a unique minimizer. This entails an additional assumption and should be stated as such."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zGMetg7UQB", "forum": "UKcSMWt6UW", "replyto": "UKcSMWt6UW", "signatures": ["ICLR.cc/2026/Conference/Submission1177/Reviewer_DKPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1177/Reviewer_DKPi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939812548, "cdate": 1761939812548, "tmdate": 1762915698097, "mdate": 1762915698097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**\n\nThis paper establishes the local linear convergence of projected gradient descent applied to a smooth convex function $f$ when the constraint set is a Euclidean ball and the unconstrained minimizer of $f$ falls outside the constraint set. The authors conduct analysis in both discrete time and continuous time scenarios. Some experiments validate the theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Strength**\n\nThe paper is well-written and easy to follow. It contains rich geometric intuitions and provides nice new insights into the local behavior of PGD. The idea of leveraging the geometry of the constraint to achieve better (condition number-free) convergence behavior is interesting."}, "weaknesses": {"value": "**Weaknesses**\n\n1. Unclear explicit local linear convergence region\n\n   **Theorem 3.1**  shows the existence of a local linear convergence region. However, the dependence on parameter $\\theta$ is not adequately discussed. How does $\\theta$ depend on the other problem parameters and the initial point? I believe the result would be even more interesting if an explicit characterization of $\\theta$ is available.\n\n2. Rigor and clarity of the proofs \n\n   I appreciate the rich geometric intuitions behind the proofs. However, some proof arguments are based on figure (e.g., line 831) and lack rigor. I would also suggest that the authors add step-by-step explanations to the important steps of the proof. \n\nOverall, I find the paper contains interesting insights into the role played by constraint geometry in PGD. The paper is well-written and easy to follow. Given the time limit of the review process, I have run simulations and empirically verified the claims in the paper, but there's insufficient time to verify all the proof details. \n\nFor now, I recommend weak acceptance. And I'll raise my score to 8 if my questions are appropriately addressed."}, "questions": {"value": "**Questions**\n\n1. In **Theorem 3.1**, The dependence on $U$ seems to suggest it should be fixed at 2, and there is no trade-off. Is there any intuition here?\n2. According to **Theorem 3.1**, there is a lower bound on the distance to the optimal solution. However, suppose $\\Phi$ is a well-conditioned smooth convex function such that $1 - \\frac{1}{\\kappa} < C_3$. Then how would this lower bound reconcile with the upper bound of projected gradient descent?\n3. Proofs between line 800 and 803 look unclear to me. Could you explain these three lines in detail?\n4. Do you think it's possible to extend the analysis to other constraint sets (e.g. strongly convex set)?\n5. The paper's main results essentially leverage the structure of the constraint set. And it is well-known that Frank-Wolfe method is also capable of leveraging constraint geometry. And it is known that under the same condition that the unconstrained optimum is outside the feasible region, Frank-Wolfe also exhibits linear convergence [1]. Could you elaborate on the possible connection between these two methods?\n\n**Minor issues**\n\n1. Line 40\n\n   Could you use optimal solution instead of equilibrium point?\n\n2. Line 60\n\n   I don't see why superlinear convergence is plotted here.\n\n3. Line 71\n\n   Gradient descent cannot guarantee $1/K$ rate of distance to $\\delta^\\Delta$ in general.\n\n4. Line 94\n\n   The definition of $C_3$ seems ambiguous here.\n\n5. Line 147\n\n   Please be precise about \"doesn't have $\\delta^*$\".\n\n6. Line 152\n\n   $F(\\delta)$ is not defined here.\n\n7. Line 163\n\n   incremental function => incremental functions.\n\n8. Line 183, 262, 270, 300\n\n   Missing full stop.\n\n9. Line 265\n\n   \"which states that ...\" is not clear.\n\n10. Line 759\n\n    The second equality should not be aligned right below the first one.\n\n11. Line 779\n\n   The second $<$ should be $\\leq$.\n\n**References**\n\n[1] Garber, D., & Hazan, E. (2015, June). Faster rates for the Frank-Wolfe method over strongly-convex sets. In *International Conference on Machine Learning* (pp. 541-549). PMLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "st9An9JKcy", "forum": "UKcSMWt6UW", "replyto": "UKcSMWt6UW", "signatures": ["ICLR.cc/2026/Conference/Submission1177/Reviewer_QTYh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1177/Reviewer_QTYh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985561997, "cdate": 1761985561997, "tmdate": 1762919535848, "mdate": 1762919535848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}