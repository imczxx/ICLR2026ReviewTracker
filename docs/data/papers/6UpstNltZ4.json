{"id": "6UpstNltZ4", "number": 20790, "cdate": 1758310184905, "mdate": 1759896958570, "content": {"title": "A Recovery Guarantee for Sparse Neural Networks", "abstract": "We prove the first guarantees of sparse recovery for ReLU neural networks, where the sparse network weights constitute the signal to be recovered. Specifically, we study structural properties of the sparse network weights for two-layer, scalar-output networks under which a simple iterative hard thresholding algorithm recovers these weights exactly, using memory that grows linearly in the number of nonzero weights. We validate this theoretical result with simple experiments on recovery of sparse planted MLPs, MNIST classification, and implicit neural representations. Experimentally, we find performance that is competitive with, and often exceeds, a high-performing but memory-inefficient baseline based on iterative magnitude pruning.", "tldr": "We prove the first identifiability and recovery results for sparse ReLU neural networks, and show from-scratch recovery of sparse networks without first training a dense network.", "keywords": ["compressed sensing", "neural networks", "model pruning", "sparse weight recovery"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e1b9fd2358bfd10db142b98741c37a03e18c847.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies sparse training through a recovery-guarantee lens for two-layer ReLU MLPs with scalar output. Using a convex reformulation that enumerates activation patterns and fuses layer weights, the training objective becomes a structured linear sensing problem. Under Gaussian inputs and two separability conditions on activation patterns, the sensing matrix satisfies restricted strong convexity and smoothness, which makes the planted sparse weights uniquely identifiable. Experiments on planted MLP recovery, MNIST classification, and implicit neural representations show the proposed method is competitive with or better than iterative magnitude pruning while using far less memory, even beyond the exact theory regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper gives identifiability and efficient recovery for planted sparse weights in two-layer scalar-output MLPs under Gaussian data.\n2. The activation-pattern convexification connects ReLU training to linear sensing in a way that enables analysis with classical tools.\n3. Across planted recovery, MNIST, and image INRs, IHT is typically stronger than IMP while using memory that scales with s rather than with dense parameter counts."}, "weaknesses": {"value": "1. Scope limited to shallow scalar-output MLPs in theory.\n2. The main theorem relies on i.i.d. Gaussian covariates and full enumeration of activation patterns, which can be unrealistic or computationally heavy."}, "questions": {"value": "1. Extension beyond Gaussian inputs? Can the proof be adapted using sub-Gaussian or whitened real data, and what additional conditions would be needed on X?\n\n2. The experiments use randomly sampled patterns and sequential convex updates. Is there a provable guarantee when A is updated during training, perhaps via a stability or tracking argument?\n\n3. The appendix describes vector-output handling and layer-wise training. Could the convex deep-net formulations be combined with your analysis to yield recovery guarantees for deeper or multi-output networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wpc7VuNno6", "forum": "6UpstNltZ4", "replyto": "6UpstNltZ4", "signatures": ["ICLR.cc/2026/Conference/Submission20790/Reviewer_T4on"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20790/Reviewer_T4on"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441695719, "cdate": 1761441695719, "tmdate": 1762935185600, "mdate": 1762935185600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the theoretical recovery guarantees for sparse ReLU neural network weights, focusing on two-layer scalar-output MLP networks. The authors prove that under certain structural conditions on sparse network weights and random Gaussian training data, an Iterative Hard Thresholding algorithm can exactly recover these weights with memory linear in the number of nonzero weights. The theoretical analysis uses convex reformulations of MLPs and establishes restricted strong convexity and restricted smoothness properties. Experimental validation on planted MLPs, MNIST classification, and implicit neural representations demonstrates competitive or superior performance compared to iterative magnitude pruning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work provides formal recovery guarantees for sparse neural network weights, filling an important gap between compressed sensing theory and neural network optimization.\n- The paper clearly articulates assumptions and provides detailed proofs showing how these conditions arise with high probability under Gaussian data, making the theoretical results verifiable and interpretable.\n- The experimental studies, which extend beyond the theoretical analysis, demonstrate the broader applicability of the guarantees, including vector outputs, deeper networks, and various tasks."}, "weaknesses": {"value": "- The theoretical guarantees, although interesting and important, are restrictive. It is limited to two-layer, scalar-output networks with Gaussian random data. However, real-world applications may involve deeper networks, structured data distributions, and multi-dimensional outputs.\n- Assumption 1 requires concrete weight structures, i.e., binary hidden weights or binary output weights, which may not reflect realistic sparse networks.\n- The paper only compares against IMP. More recent sparse training methods, such as dynamic sparse training, pruning at initialization variants, and other memory-efficient approaches, can be compared. Another limitation is that the runtime analysis, with IHT sometimes being slower than IMP for larger problems, which may be a limitation in practice."}, "questions": {"value": "- How can the proposed analysis be extended to non-Gaussian data distributions?\n- More analysis on how performance degrades when assumptions are violated would strengthen the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "U9Sjc1nqU0", "forum": "6UpstNltZ4", "replyto": "6UpstNltZ4", "signatures": ["ICLR.cc/2026/Conference/Submission20790/Reviewer_A2Ki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20790/Reviewer_A2Ki"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478293825, "cdate": 1761478293825, "tmdate": 1762935181368, "mdate": 1762935181368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and studies a method for training sparse shallow ReLU neural networks. The method reformulates the neural network optimization problem as a convex optimization problem and uses iterative hard-thresholding to obtain a sparse solution. Theoretical results guarantee the recovery under reasonable assumptions in the case of shallow networks. Empirical experiments verify that the method also works well for deep networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method for training sparse networks is innovative. The theoretical guarantees are interesting and relevant. The practical performance of the methods is good. The presentation of the paper is good, it is easy to read and clear."}, "weaknesses": {"value": "The theory only applies to shallow networks. It would be interesting to try to extend it to deeper networks. Nonetheless, this is a valuable theoretical contribution."}, "questions": {"value": "Can the theory be extended beyond Gaussian training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1iXNelz30y", "forum": "6UpstNltZ4", "replyto": "6UpstNltZ4", "signatures": ["ICLR.cc/2026/Conference/Submission20790/Reviewer_XpEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20790/Reviewer_XpEf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967235401, "cdate": 1761967235401, "tmdate": 1762935163151, "mdate": 1762935163151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides the first recovery guarantee for sparse ReLU neural networks by reformulating sparse MLP training as a structured compressed-sensing problem. Under Gaussian data and enumerated activation patterns, the sensing matrix satisfies restricted strong convexity and smoothness, allowing Iterative Hard Thresholding (IHT) to exactly recover sparse weights with high probability. The method requires memory linear in the number of nonzeros. Experiments on planted MLPs, MNIST, and implicit neural representations validate the theory and show that IHT performs comparably or better than Iterative Magnitude Pruning (IMP) while being significantly more memory-efficient."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "A rigorous and elegant theoretical framework connecting sparse neural network recovery with compressed sensing, supported by solid experimental validation.\n1. First work proving exact recovery (Theorem 1) for sparse MLP weights with explicit high-probability bounds and convergence rates; recovery builds on Assumption 2 and the RSC/RSS properties (Lemma 1).\n2. Convex reparameterization (Equation 3) via weight fusion turns the nonconvex problem into a linear sensing framework; Assumptions 1–2 are well-motivated and verified through constructive examples in Appendix D.\n3. Memory scales linearly with sparsity (s); sample complexity grows with active weights rather than total parameters. IHT achieves competitive or superior accuracy to IMP while using less memory.\n4. Well-structured from motivation through theory to experiments, with a comprehensive appendix providing implementation and proofs."}, "weaknesses": {"value": "The main limitation lies in the unquantified theory–practice gap and missing diagnostics connecting empirical behavior to theoretical parameters.\n1. Theorem 1 assumes a fixed, enumerated sensing matrix A, but experiments adopt sequential convex updates—updating A after each IHT iteration (following an initial fixed epoch). The paper acknowledges this deviation (Appendix A) but provides no convergence analysis or ablation on update frequency.\n2. Empirical values of the RSC and RSS constants and their ratio are not reported, leaving unclear how tight the theoretical conditions are in practice.\n3. Results are averages over three runs without error bars, standard deviations, or variance. Adding uncertainty plots (e.g., box plots or success-probability curves) would strengthen performance claims."}, "questions": {"value": "1.Can you analyze how A evolves during sequential convex updates—for example, by measuring similarity between consecutive A matrices—and test how update frequency affects convergence?\n\n2.Can you report empirical ($\\alpha,\\beta,\\beta/\\alpha$) for at least one representative setting to show whether the RSC/RSS conditions approximately hold in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ngVaQGKM2s", "forum": "6UpstNltZ4", "replyto": "6UpstNltZ4", "signatures": ["ICLR.cc/2026/Conference/Submission20790/Reviewer_4D7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20790/Reviewer_4D7G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762405937903, "cdate": 1762405937903, "tmdate": 1762935114680, "mdate": 1762935114680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides theoretical guarantees for recovering the sparse weights of a ReLU network using an efficient, memory-linear algorithm. The proposed method, validated on tasks on MNIST, matches or exceeds the performance of a memory-inefficient pruning baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work establishes the sparse recovery result for ReLU MLPs. \n\n2. Using shallow networks on Gaussian data, the experiments demonstrate that Iterative Hard Thresholding (IHT) is more effective and memory-efficient than a strong IMP baseline, recovering superior sparse networks."}, "weaknesses": {"value": "Using the eq. (2), training the 2-layer MLP with MSE loss is a nonconvex optimization problem.  From the lines 162-175, the authors consider fixed generator vectors $h_i\\in R_d$ and fusing the weights via $w_i=u_iv_i$ to build the eq. (3). This transformation allows the results to be readily obtained using sparse recovery theory. However, several issues arise:\n\n1. The theory requires the matrix $A$ to satisfy the conditions of Lemma 1, which is attributed to the sparsity of $h_i$ or $u_i$. Typically, \n $X$ is assumed to be a Gaussian data matrix. How is Lemma 1 satisfied when the input data is non-Gaussian, such as the binary-valued pixels in the MNIST dataset\n\n2. The theoretical results are promising, but how does the method perform on more complex, real-world datasets like CIFAR-10 or ImageNet-200?\n\n3. This work focuses on recovering the sparsity pattern of the weights $u_i$. What can be said about the sparsity or recovery guarantees for the corresponding weights $v_i$?"}, "questions": {"value": "please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "14rx7j5T0L", "forum": "6UpstNltZ4", "replyto": "6UpstNltZ4", "signatures": ["ICLR.cc/2026/Conference/Submission20790/Reviewer_RfMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20790/Reviewer_RfMf"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762594504058, "cdate": 1762594504058, "tmdate": 1762935082303, "mdate": 1762935082303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}