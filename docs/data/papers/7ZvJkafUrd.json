{"id": "7ZvJkafUrd", "number": 20826, "cdate": 1758310629066, "mdate": 1763598721125, "content": {"title": "Simplifying Multi-Task Architectures Through Task-Specific Normalization", "abstract": "Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$\\sigma$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$\\sigma$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.", "tldr": "Task-specific normalization by itself is a simple, parameter efficient and competitive soft-sharing mechanism for MTL. Task-Specific $\\sigma$BatchNorm is even better and interpretable.", "keywords": ["Multi-Task Learning", "Soft-Parameter Sharing", "Parameter Efficiency", "Interpretability"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8c2ad5142511429310ea99f53e5afe840b58292.pdf", "supplementary_material": "/attachment/5329ba267bf050f0d0b8394d0fa7ab9aeab6d963.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a  task‑Specific Sigmoid Batch Normalization (TSσBN) approach for multi-task learning, which introduces per‑task scaling parameters that are passed through a sigmoid gating mechanism to softly allocate network capacity. The authors method isolates BatchNorm as the sole soft-sharing mechanism, showing that it is a sufficient solution for competitive MTL while challenging unnecessary complexity of previous MTL approaches. Experiments confirm the applicability of the method and its low computational requirements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The capacity TS$\\sigma$BN to built an importance matrix is a highly appreciated feature of the method. It allows interpretable insights into the model behaviour. \n- TS$\\sigma$BN robustness to loss scales without additional changes is another strength.\n- The idea is quite simple but is an important contribution to MTL. The concept of domain-specific batch layer norm while not new for MTL show surprising positive effects. Besides that the TS$\\sigma$BN is easy to implement and require minimal changes to current architectures. \n- The wide range of experimental architectures tested show that the method can be used on multiple vision domains."}, "weaknesses": {"value": "- While the authors focused quite extensively on vision tasks, it is unclear whether TS$\\sigma$BN generalizes beyond computer vision. Small experiments on NLP or time series MTL (MIMIC-III) tasks would strength the paper claim on generalisation of the method.\n- The paper provides mainly empirical evidences. A deeper theoretical justification of why task-specific batch normalization disentagle representations could make the work more compelling. \n- Another aspect of MTL methods is its hyper-parameter tuning sensitivity. While the authors provide ablations, these remain a risk that performance is sensitive to this $\\sigma$ parameter or dataset-specific tuning."}, "questions": {"value": "- Regarding broader domain evaluation, the authors can provide experiments or at least discuss how TS$\\sigma$BN will perform in other domains. \n- Can the authors provide guidance on selecting the $\\sigma$ hyper-parameter and the threshold $\\tau$ for filter specialization.\n- I suggest the author to include some optimization-based methods to the comparison (GradNorm and CAGrad) to demonstrate how TS$\\sigma$BN compares relatively to techniques that modify gradients rather than architectures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MReABZxnZH", "forum": "7ZvJkafUrd", "replyto": "7ZvJkafUrd", "signatures": ["ICLR.cc/2026/Conference/Submission20826/Reviewer_BXZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20826/Reviewer_BXZU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761063695229, "cdate": 1761063695229, "tmdate": 1762936219167, "mdate": 1762936219167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose an MTL architecture that shares all features for every task and allocates task-specific normalization layers. The \"TSBN\" variant replaces standard affine Batch Norm with a bounded sigmoid scalar per channel. An independent larger learning rates for BN parameters is used to learn capacity allocation faster. Evaluations are conducted on NYUv2, Cityscapes, and CelebA for CNN backbones, and Pascal-Context for ViT small. Experimental results show competitive accuracy vs. soft-sharing, MoE, and transformer MTL baselines at notably lower parameter cost. Finally, the per-task BN scales are used to analyze capacity sharing, task similarity, and specialization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and easy to understand.\n\n- The paper addresses a persistent problem of negative interference in MTL. Finding efficient methods to mitigate this issue is of high importance to the field. The proposed method is simple and practical. It is also very straightforward for integration in practical settings. \n\n- The ablations and analyses on task-filter importance analysis (Figure 4) are intuitive and match expectations."}, "weaknesses": {"value": "This paper suffers from significant limitations in its current form, primarily concerning novelty, the depth of its empirical evaluation, and its positioning within the current literature. In its current form, this work extremely incremental and the core of the contributions around task-specific batch norms have already been established for 5< years.\n\n**Limited Novelty:** The central contribution of this paper concerns using task-specific parameters or statistics in BatchNorm for MTL. Task-specific BN for MTL or multi-domain sharing is widely known and used for several years. Prior works and libraries explicitly advocate per-task BN, and domain-specific BN is standard for sharing all but BN across conditions, reducing the novelty of the core mechanism. For example Xie et al. [1,2] uses Task-Specific Batch Normalization for class incremental learning. The paper \"Task Adaptive Parameter Sharing for Multi-Task Learning\" [3] considers task-specific batch norm as a standard practice and considers it as a basic baseline. TaskNorm has been used for meta learning as well [4] in 2020. The paper fails to acknowledge this body of work sufficiently and does not clearly articulate what makes its specific implementation a novel advancement over these established techniques.\n\n**Outdated Backbones**: The experiments are limited to very old architectures like ResNet-50 and ViT-S. To prove the method's relevance, it is crucial to demonstrate its effectiveness on current SoTA backbones such as ConvNeXt, DETR, etc. It is unclear if the performance gains would persist on these more powerful and architecturally different models.\n\n**Benchmark scope:** The evaluation leans on legacy datasets such as NYUv2, Cityscapes, Pascal-Context, CelebA, which remain \"widely-used\" but are increasingly considered limited proxies in top venues. The authors should extend their experiments to include at least one large-scale, challenging benchmark such as Taskonomy.\n\n**Outdated Related Works:** Reading the Intro and Related works section gives the impression the paper is behind the field for a few years. The paper fails to acknowledge the MTL research in research years. Most cited works are from 2017 to 2023.\n\n[1] Xie, Xuchen, et al. \"Class Incremental Learning with Task-Specific Batch Normalization and Out-of-Distribution Detection.\" arXiv preprint arXiv:2411.00430 (2024).\n\n[2] Xie, Xuchen, et al. \"Task-incremental medical image classification with task-specific batch normalization.\" Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Singapore: Springer Nature Singapore, 2023.\n\n[3] Wallingford, Matthew, et al. \"Task adaptive parameter sharing for multi-task learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[4] Bronskill, John, et al. \"Tasknorm: Rethinking batch normalization for meta-learning.\" International Conference on Machine Learning. PMLR, 2020."}, "questions": {"value": "- The Tables have abbreviations for method names that are never introduced. What is HPS?\n- How does the parameter efficiency and accuracy of the method compare to a the modern methods which use LoRA adapters to specialize the network to each task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1TZNOiTPoS", "forum": "7ZvJkafUrd", "replyto": "7ZvJkafUrd", "signatures": ["ICLR.cc/2026/Conference/Submission20826/Reviewer_gf9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20826/Reviewer_gf9H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918696389, "cdate": 1761918696389, "tmdate": 1762936206668, "mdate": 1762936206668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Task-Specific Sigmoid BatchNorm that swaps shared BN with per-task $\\sigma$-BN to softly allocate capacity in shared backbones. Experiments (like Table 1 TS-$\\sigma$-BN gets +6.93% over STL on NYUv2 with only 0.33× params) confirm the effectiveness of proporsed method on multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea to make only normalization task-specific, without extra attention/routing, is simpler than works like Cross-Stitchwhich add task branches or dynamic sharing, so using $\\sigma$-BN from (Suteu & Guo, 2022) for MTL is a nice, underexplored reuse. \n- The reviewer found it interesting that TS-$\\sigma$-BN stays stable even when they boost BN learning rates to 10² (Figure 6), while plain TSBN collapses, so the bounded gate actually matters.\n- Writing is mostly clear."}, "weaknesses": {"value": "- It should have been easy to show TSσBN vs MTAN vs MoE on CelebA with 40 tasks under the LibMTL setup (authors only report LibMTL for NYUv2/Cityscapes), and an ablation against simpler DSBN/TaskNorm baselines is missing.\n- The reviewer questions the novelty of the contributions w.r.t. ICLR standards.\n- Typos - Line 289: “respresentative” → “representative”"}, "questions": {"value": "The reviewer has one question: \n- In Table 2 (LibMTL, NYUv2/Cityscapes) TSσBN uses 1.00× params and 1.69× FLOPs but TSBN also lists 1.69× FLOPs; can you clarify whether FLOPs include per-task heads or only BN copies, since this seems lower than MTAN’s added convolutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "v3bOxGfUBA", "forum": "7ZvJkafUrd", "replyto": "7ZvJkafUrd", "signatures": ["ICLR.cc/2026/Conference/Submission20826/Reviewer_wFix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20826/Reviewer_wFix"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976808149, "cdate": 1761976808149, "tmdate": 1762936193978, "mdate": 1762936193978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}