{"id": "a7qGaiHDbS", "number": 17381, "cdate": 1758275255353, "mdate": 1759897178820, "content": {"title": "IMAGEdit: Let Any Subject Transform", "abstract": "In this paper, we present IMAGEdit, a training-free framework for any number of video subject editing that manipulates the appearances of multiple designated subjects while preserving non-target regions, without finetuning or retraining.\nWe achieve this by providing robust multimodal conditioning and precise mask sequences through a prompt-guided multimodal alignment module and a prior-based mask retargeting module.\nWe first leverage large models' understanding and generation capabilities to produce multimodal information and mask motion sequences for multiple subjects across various types.\nThen, the obtained prior mask sequences are fed into a pretrained mask-driven video generation model to synthesize the edited video. \nWith strong generalization capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning and overcomes mask boundary entanglement in videos with any number of subjects, thereby significantly expanding the applicability of video editing.\nMore importantly, IMAGEdit is compatible with any mask-driven video generation model, significantly improving overall performance. \nExtensive experiments on our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit consistently surpasses state-of-the-art methods.\nCode, dataset, and weights will be released.", "tldr": "", "keywords": ["Video Editing", "Diffusion Model", "Video Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d98d5d02b63e068731ae01d4c0faf60adc9e3c96.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces IMAGEdit, a novel training-free framework designed for multi-subject video editing. The method aims to transform multiple designated subjects within a video while preserving the background and maintaining temporal consistency. The core of the approach consists of two main components: a \"Prompt-Guided Multimodal Alignment\" module that leverages a Vision-Language Model (VLM) to generate more robust and detailed editing conditions, and a \"Prior-Based Mask Retargeting\" module that produces precise, temporally consistent masks by incorporating depth information. The authors demonstrate the effectiveness of their framework on a newly constructed benchmark, MSVBench, showing that IMAGEdit achieves state-of-the-art performance compared to existing open-source and closed-source methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Strong Motivation:** The paper is well-motivated, identifying a critical bottleneck in training-free multi-subject video editing: the challenge of accurately identifying and isolating the edit regions. The observation that this is a primary failure point is insightful, and the decision to focus the core technical contributions on improving mask-aware conditioning is a clear strength.\n2.  **Logical Method Design:** Following the motivation, the proposed method is designed logically. It decomposes the problem into improving the semantic guidance (via Prompt-Guided Multimodal Alignment) and the spatial guidance (via Prior-Based Mask Retargeting). This two-pronged approach is a sound and comprehensive way to address the identified challenges.\n3.  **State-of-the-Art Results:** The paper presents strong empirical results, consistently outperforming several state-of-the-art methods on the proposed MSVBench. The qualitative examples provided are compelling and effectively showcase the framework's ability to handle complex, multi-subject scenes where other methods often fail."}, "weaknesses": {"value": "The primary weakness of this paper is the significant lack of clarity in its methodological description. This issue is pervasive and severe enough to undermine the paper's technical contribution and prevent reproducibility. Nearly all key components and novel ideas are described in a way that is either overly convoluted, lacking in essential details, or inconsistent across the text and figures.\n\n1.  **Unclear Presentation of Prompt-Guided Multimodal Alignment:** The description of this module (lines 221-256) is confusing and lacks crucial information. It takes considerable effort for the reader to understand that this module is essentially a sophisticated prompt-rewriting process using a VLM. The motivation is explained at length with Figure 4, but the actual mechanism remains opaque. Furthermore, the design seems overly complex: the necessity of using a Text-to-Image (T2I) model to generate an intermediate image ($I_{ref}$) only to feed it back into a VLM is not justified with an ablation study. Finally, the output of this module, the expanded prompt $P_{target}$, is missing from the main framework diagram (Figure 3), leaving its role in the overall pipeline unclear.\n\n2.  **Insufficient Explanation of Prior-Based Mask Retargeting:** This module appears to be the most innovative part of the paper, yet it is explained very poorly.\n    *   The origin of the feature $F^{mask}$ is highly ambiguous. The paper states that it is generated by feeding a masked video and a binary mask into a \"conditional DiT\" (later identified as the Wan2.1 T2V model). The paper provides absolutely no explanation of how a Text-to-Video model is architecturally adapted to process this new combination of inputs (masked video + mask) and output a feature map.\n    *   The process for obtaining $F^{depth}$ is even more vague, described only as being \"processed by a similar DiT architecture.\" This leaves the reader guessing whether it's the same model, a different one, or a similar process.\n    *   In contrast to these critical omissions, the authors dedicate significant space to describing a standard mask dilation algorithm, which is not a novel contribution.\n\n3.  **Opaque Description of the Video Generation Model (Sec. 3.2):** The final step of the framework is also confusingly presented.\n    *   The notation is inconsistent. Equation (6) for $F^{cond}$ (when t ≤ τ) is identical to the definition of $F^{motion}$ in Equation (5). Re-writing the equation instead of using the established notation $F^motion$ creates unnecessary confusion for the reader.\n    *   The most critical information is missing. The paper never explains how the final conditional feature, $F^{cond}$, is actually used by the \"mask-driven video generator.\" The identity of this generator model is never specified, nor is its input format described. This leaves the final video synthesis step, a crucial part of the method, as a complete mystery.\n\n4.  **Minor Issues:**\n    *   **Typo:** Line 304 refers to a \"ViT backbone,\" which may be a typo for \"DiT backbone,\" given the context of the rest of the paper.\n    *   **Method Name:** The name \"IMAGEdit\" is not ideal for a method focused specifically on *video* editing.\n\nAt this stage, I am recommending clear rejection for this paper because I cannot consider methods that are clearly unreproducible and lack essential details to be a valid contribution."}, "questions": {"value": "To address the weaknesses, I urge the authors to clarify the following points:\n\n1.  Can you provide a clearer and more direct explanation of the Prompt-Guided Multimodal Alignment module? Specifically, please justify the necessity of the complex T2I-then-VLM pipeline. More importantly, how is the final $P_{target}$ prompt used by the model?\n2.  Please provide a detailed architectural description for the Prior-Based Mask Retargeting module. How is the Wan2.1 T2V model modified or used to process a masked video and mask concatenation to produce $F^{mask}$? What, precisely, is the \"similar DiT architecture\" used to generate $F^{depth}$?\n3.  How is the final feature $F^{cond}$ integrated into the video generation model? What is the specific architecture of the \"mask-driven video generator\" used in your experiments, and how does it accept $F^{cond}$ as a conditioning input?\n4.  Would it be possible to revise Figure 3 to create a complete, end-to-end diagram that accurately and consistently represents the entire data flow of the proposed framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FK9TxILlEr", "forum": "a7qGaiHDbS", "replyto": "a7qGaiHDbS", "signatures": ["ICLR.cc/2026/Conference/Submission17381/Reviewer_BVu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17381/Reviewer_BVu7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760521932116, "cdate": 1760521932116, "tmdate": 1762927290340, "mdate": 1762927290340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IMAGEdit, a training-free pipeline for multi-subject video editing that replaces designated categories (e.g., people → robots) while preserving subject count, spatial layout, and non-edited regions. The system has three parts: 1) Prompt-guided multimodal alignment that uses a T2I model (SDXL) to synthesize a visual prior for the target subject tokens, then a VLM (Qwen2.5-VL-32B) to expand the prompt into “aligned” text and image conditions; 2) Prior-based mask retargeting that fuses depth features with instance masks using a soft, dilated mask to produce a time-consistent mask motion sequence and enforce occlusion ordering; 3) A mask-driven video generator (Wan-2.1 DiT backbone) with early-step feature injection. The method claims to work without finetuning and to be plug-and-play with mask-conditioned generators. A new 100-video benchmark MSVBench (mostly multi-subject, sourced from YouTube/TikTok) and a new layout metric CM-Err are introduced."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear focus on multi-subject, mask-driven editing under real occlusions, with explicit controls to preserve layout and prevent spillover. The recipe is practical: multimodal prompt alignment reduces ambiguity, depth-aware retargeted masks keep boundaries and ordering clean, and early-stage injection steers generation without finetuning while improving metrics. MSVBench stresses higher subject counts, and CM-Err checks count and center stability, giving a more direct readout of layout integrity than pixel overlap alone."}, "weaknesses": {"value": "Limited originality. Using depth priors and VLM-based alignment for feature injection does not feel novel. The pipeline reads as a collection of established engineering choices rather than a new core idea.\n\nUnderspecified fusion of the “expanded image condition.” It is unclear how the SDXL and VLM signals are integrated into the DiT backbone. Please specify the layers where fusion happens, the operator used (concatenation, addition, or cross-attention), and the normalization. Provide an ablation that isolates text-only, image-only, and text+image, and report their separate gains.\n\nReliance on external modules without robustness analysis. Performance likely depends on Grounded-SAM2 masks and Depth-Anything v2. There is no study of robustness to mask noise, depth errors, or crowded scenes with heavy occlusions. Please add stress tests that erode or dilate masks, inject depth bias or noise, and vary crowd density.\n\nBenchmark scale and transparency. One hundred clips is small. Please expand diversity in scenes, camera motion, and subject categories, or add a synthetic multi-subject suite with controllable occlusions and instance counts to stress the boundary-entanglement claim. In the results, there are no visualized video examples or dataset previews; include representative edited videos and benchmark visualizations to substantiate the claims."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RQ4kPHUZgd", "forum": "a7qGaiHDbS", "replyto": "a7qGaiHDbS", "signatures": ["ICLR.cc/2026/Conference/Submission17381/Reviewer_LskD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17381/Reviewer_LskD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673837407, "cdate": 1761673837407, "tmdate": 1762927289096, "mdate": 1762927289096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents IMAGEdit, a new training-free framework designed to edit any number of subjects within a video. It aims to solve key failures of existing methods, particularly in complex scenes with multiple, overlapping, or densely packed subjects. The framework successfully transforms the appearance of designated subjects while preserving their motion, the background, and non-target regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed methods are technically sound. The introduced pipeline, can effectively mitigate the challenges in transforming multiple subjects.\n- This work also introduces a benchmark for further evaluation."}, "weaknesses": {"value": "- How would the system handle some unusual subjects (clouds, smoke) edits? The limitations can be discussed clearly.\n- Overall I think the proposed pipeline introduces designs of limited contributions. The modules are somewhat ordinary and standard.\n- I think for existing methods, simultaneously transforming multiple subjects lead to performances drop. What about transforming the subjects one by one with the existing methods and segmentation models? They may serve as stronger baselines."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8GySNHC0bl", "forum": "a7qGaiHDbS", "replyto": "a7qGaiHDbS", "signatures": ["ICLR.cc/2026/Conference/Submission17381/Reviewer_oUvp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17381/Reviewer_oUvp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935718848, "cdate": 1761935718848, "tmdate": 1762927288778, "mdate": 1762927288778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IMAGEdit, a training-free video editing framework capable of category transformation for any number of designated subjects in arbitrary videos. It preserves the number of subjects and their spatial layout, and demonstrates  its potential in crowded scenes with overlapping subjects. The framework achieves editing by leveraging two core modules: a prompt-guided multimodal alignment module that generates robust multimodal conditions, and a prior-based mask retargeting module that produces precise temporally consistent mask sequences. To evaluate the framework, the authors constructed MSVBench, a benchmark dataset containing 100 videos—over 60% of which includes three or more subjects. Experimental results show that IMAGEdit outperforms relevant state-of-the-art methods across key metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The core technical innovations (extended text and image conditions/prior-based mask retargeting module) of the paper  lack of new insights and largely consist of incremental improvements to existing ideas, making it challenging to meet ICLR’s high standards for originality. \n2) Training-free methods, by design, face inherent challenges of hyperparameter sensitivity and unstable performance, which the paper does not fully address, and shifting toward a training-based framework would likely yield more robust and generalizable results."}, "weaknesses": {"value": "1) Construction of a Systematic Benchmark (MSVBench) Filling an Evaluation Gap. The authors’ development of MSVBench addresses a critical gap in existing video editing research, where benchmarks primarily focus on single-subject or face-centric edits\n2) The paper’s experimental results are compelling, as IMAGEdit achieves state-of-the-art performance on both MSVBench and the general-purpose loveu-tgve-2023 dataset, demonstrating its  potential  for real-world video editing tasks."}, "questions": {"value": "Please refer to the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dNMxliSnIv", "forum": "a7qGaiHDbS", "replyto": "a7qGaiHDbS", "signatures": ["ICLR.cc/2026/Conference/Submission17381/Reviewer_ZvxZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17381/Reviewer_ZvxZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986783638, "cdate": 1761986783638, "tmdate": 1762927288386, "mdate": 1762927288386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}