{"id": "ou2GyHZ7HI", "number": 15799, "cdate": 1758255370047, "mdate": 1763712429734, "content": {"title": "How Reasoning Evolves from Post-Training Data in Sequential Decision-Making Domains", "abstract": "We study how reasoning evolves in a language model -- from supervised fine-tuning (SFT) to reinforcement learning (RL) -- by analyzing how a set of theoretically-inspired datasets impacts language model performance in a verifiable Markov Decision Process (MDP) such as chess. We find that fine-tuning a model to directly predict the best move leads to effective RL and the strongest downstream performance -- however, the RL stage elicits $\\textit{unfaithful}$ reasoning (reasoning inconsistent with the chosen move). Alternatively, training on multi-move trajectories yields comparable downstream performance with faithful reasoning and more stable RL. We show that RL induces a substantial positive shift in the distribution of move quality and reduces hallucination rates as a side effect. Finally, we find several SFT-checkpoint metrics -- metrics spanning evaluation performance, hallucination rates, and reasoning quality -- to be predictive of post-RL model performance. We release checkpoints and final models as well as training data, evaluations, and code which allowed us to surpass leading open-source reasoning models in chess with a 7B-parameter model.", "tldr": "We study how reasoning evolves (both qualitative and quantitative performance) from custom post-training datasets through SFT and RL on a sequential decision domain (e.g., chess).", "keywords": ["Reasoning", "Reasoning Models", "Markov Decision Process", "MDP", "Language Models", "Supervised Fine-Tuning", "Reinforcement Learning", "RLVR", "Dataset Design", "Chess"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d6c115bf094a8a34450679f20566771b412d4a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how reasoning emerges and develops in large language models fine-tuned for sequential decision-making tasks, specifically using chess as a verifiable testbed. The authors construct and analyze a suite of datasets grounded in theoretical motifs (Best Move, Best Line, Verbalized Alpha-Beta, etc.), study the effects of dataset composition on SFT, and follow with RL to assess how model behaviors, reasoning faithfulness, move quality, and hallucination rates evolve."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**1. Originality:** The systematic comparison of six dataset construction methods (Best Move, Best Line, Factual Board Answering, Verbalized Alpha-Beta Pruning, Rejection Sampling, Guided Synthetic) across both SFT and RL phases is novel. Using chess as a verifiable reasoning domain is well-motivated, enabling precise measurement of reasoning quality.\n\n**2. Quality:** Experimental design is rigorous and comprehensive. Evaluation is multifaceted, combining quantitative metrics (move accuracy, legal move rate, average move rank) with qualitative analysis (reasoning strategy usage, hallucination rates, faithfulness). \n\n**3. Clarity:** Great presentation throughout. Claims are well-supported by clear visualizations. The systematic organization makes the comparative analysis easy to follow and findings straightforward to interpret."}, "weaknesses": {"value": "**1. Limited Generalizability:** Experiments use only Qwen2.5-7B-Instruct on chess. Key findings—especially the faithful/unfaithful reasoning distinction—require validation on alternative base models and domains (math, coding) to establish whether they represent fundamental reasoning principles or chess/model-specific artifacts.\n\n**2. Insufficient Training Dynamics Analysis:** The work primarily reports task accuracy (training rewards), overlooking critical training signals (response length, entropy, or KL divergence) that would provide mechanistic insights into observed phenomena like RL stability differences and unfaithful reasoning emergence. Deeper analysis of training dynamics would substantially strengthen claims and broaden applicability."}, "questions": {"value": "1. Can the authors provide more detail or statistical analysis regarding the faithfulness/quality metrics? For example, can the reliability of the LLM-as-a-judge scores be compared to human expert assessment for a sample of traces?\n\n2. The authors show Best Move achieves comparable performance to Best Line despite unfaithful reasoning. This raises concerns: Is faithful reasoning actually necessary for strong chess performance? What are the implications for reasoning models more broadly if unfaithful reasoning suffices?\n\n3. While hallucination rates drop post-RL and unfaithful rationalization is noted, more granularity is needed: What are the most persistent failure modes? Do failure modes remain unchanged during SFT and RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k94ThKP3PZ", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_sfa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_sfa6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707417161, "cdate": 1761707417161, "tmdate": 1762926032234, "mdate": 1762926032234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Paper Revisions (Log)"}, "comment": {"value": "(11.20.2025)\n- Added statistical analysis of expert vs. LLM grading to Appendix G\n- Created Appendix I: Reinforcement Learning Training Dynamics\n- Refreshed data in Figures 6 and 7 as we found a bug that discarded some of the qualitative analysis samples in the initial numbers. We do not feel this resulted in any changes to our key results. Note that the only change we made was w.r.t. inference parameters (specifically sequence length) -- no changes were made to prompting, temperature, etc. that could materially adjust scoring. Flagging that after rerunning this analysis, the 'Guided Synthetic' results that we previously made a small note of were no longer as significant to results so this was removed from Figure 6.\n- Inclusion of our new evaluation (tasks not trained on during the RL stage) in Appendix D -- we add both results and example problems. Additionally, we briefly reference these results at the end of section 4.1 when comparing the performance of the Best Move and Best Line experiments, flagging that the Best Line experiments were more robust on these OOD evaluations."}}, "id": "FyRXR4yNdJ", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712474067, "cdate": 1763712474067, "tmdate": 1763712474067, "mdate": 1763712474067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors look to determine good decision decisions and takeaways on developing a reasoning model using chess as a testbed. They train Qwen 7B over a range of different dataset SFT and then RL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The finding that best line works well is interesting.(\"Best Line: Given a board, predict the optimal line of play (4 − 6 plies) ending with the expected centipawn delta from playing this line.\") It offers some insight toward efficiently training LM-style models.\n* I like the structure of the tables in the appendix, I think it is effective."}, "weaknesses": {"value": "* Language models so far aren't great at playing chess directly and trained model does not exceed/match the performance of the 120B model so it is unclear how well these findings scale. Or how meaningful they are.\n* This work does not reconnect the training results with downstream/tournament/chess results. Further grounding the results w.r.t. to external models or performance would help position this work.\n* I think the reasoning faithfulness metrics were not well justified (see questions)\n* It took a whole to understand how/what data mixtures were used. Part of it is because Figure 4 really doesn't capture the information and \"Best Line\" and \"Best Move\" mixtures include other data as well. I think moving some of the information up from APP D DATA INCLUSION ANALYSES would help set this straight. Also, I think it is reasonable to do, but the final best run Best Move + Best Line also has 2x the data of other runs. (Which makes it unclear how you did the comparison for Figure 4?)\n* Table 1 and Table 2 both show BM > BL. (How do we square that with saying BL is the best?) Figure 5 also shows BM as best."}, "questions": {"value": "Q: Were you able to validate the reasoning faithfulness or compare gpt-120's answers vs an expert? (Are the language model judgements grounded to something concrete?)\nQ: \"Best Line had more stable RL training than Best Move.\" If the Best Move dataset were scaled such that the total number of meaningful tokens of learning were the same, do we still see the improvement? (Which part of this approach is the key part?) (Is the centi pawn estimate the key part?)\nQ: Having a place where it clearly defines what datasets are used when/where would help.  \"Best Move - All\"; Where is All defined? (I might have missed it.)\n\n# Notes, Minor\n* Overall the style and sizing of the barplot figures were hard to read. It did not jump out what was the metric and what was the training condition.\n* Likewise, I didn't find the multi-part figures always helpful. Making them bigger with more stark lines would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mnLF6eUGFC", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_1Npe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_1Npe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805427656, "cdate": 1761805427656, "tmdate": 1762926031737, "mdate": 1762926031737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New Experimental Results (Statistical Analysis - LLM-Judge vs. Expert)"}, "comment": {"value": "## Statistical Analysis of Expert vs. LLM Grading (Qualitative Metrics)\nWe formalize that LLM judge score correlation with experts is statistically significant. Our LLM-as-a-judge prompting was tuned to align with experts initially but we hadn't formally executed as a statistical analysis.\n\nSamples are a random sample from a set containing outputs from all tested models (both SFT and RL checkpoints). gpt-oss-120b responses are excluded as LLM judges have shown a tendency to favor outputs from itself (Panickssery 2024, LLM Evaluators Recognize and Favor Their Own Generations).\n\n| Expert   | N  | Efficacy       |             | Efficiency     |             | Faithfulness   |             | Sum of All |             |\n|----------|----|----------------|-------------|----------------|-------------|----------------|-------------|----------------|-------------|\n|          |    | Corr           | P-value     | Corr           | P-value     | Corr           | P-value     | Corr           | P-value     |\n| Expert 1 | 96 | 0.41           | 4.01e-05    | 0.31           | 2.03e-03    | 0.68           | 1.50e-14    | 0.57           | 1.10e-09    |\n| Expert 2 | 73 | 0.24           | 4.00e-02    | 0.34           | 3.07e-03    | 0.43           | 1.71e-04    | 0.39           | 6.63e-04    |\n\nWe have included this analysis in Appendix G (Reasoning Quality) of our paper."}}, "id": "0PdAmH1xFB", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712676551, "cdate": 1763712676551, "tmdate": 1763712676551, "mdate": 1763712676551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how \"reasoning\" ability emerges and evolves in a large language model as it undergoes supervised fine-tuning and subsequent reinforcement learning (RL). Using the game of chess as a verifiable, sequential decision-making environment (a Markov Decision Process), the authors meticulously study how different types of training data influence a model's performance and the nature of its internal logic."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The choice of chess is a strength of the paper. Chess is not just a game; it is a verifiable, discrete, episodic MDP, which provides a controlled environment for studying reasoning. The availability of chess engines as objective oracles for reward removes the need for potentially noisy human feedback or learned reward models, helping to isolate the variable of interest—the impact of training data on the reasoning process. \n2. The discovery that SFT-checkpoint metrics can predict final RL performance is a practical contribution. The SFT stage is cheaper and faster than RL. By identifying these predictive signals, the authors provide a cost-effective methodology for iterating on and selecting the best base models before committing to expensive RL runs."}, "weaknesses": {"value": "1. Limited Generalizability of the Domain: While a strength for verifiability, the sterile, perfect-information environment of chess is also a weakness. Real-world decision-making is noisy, partially observable, multi-agent, and often lacks a clear reward signal.\n2. The paper lacks analysis regarding scalability.\n3. Conflation of \"Reasoning\" with \"Verbalization\": The study defines reasoning as the model's language-based chain-of-thought. This is a limiting perspective. True reasoning might be occurring in a continuous latent space."}, "questions": {"value": "Nil"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iiNd76YMHb", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_JYqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_JYqb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973031673, "cdate": 1761973031673, "tmdate": 1762926031330, "mdate": 1762926031330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New Experimental Results (OOD Evaluation Performance)"}, "comment": {"value": "## Evaluation of Models on Other Tasks (OOD)\nWe tested models on two other tasks to understand how well they perform in alternate, out-of-distribution tasks:\n1) FBA: How well do they perform on unseen 'Factual Board Answering' questions? This is a score over 1,000 questions across 5 separate tasks. Each task has a score w/in [0, 1] -- so perfect would be 1.00.\n    - Do note that 'Factual Board Answering', 'Best Move - All', 'Best Line - All', 'Best Move XL', 'Best Line XL', and 'Best move + Best Line XL' have FBA data in their training sets. That said, all tests are on unseen tasks and these tasks are asked to be provided within answer tokens -- the training data does SFT directly on the answers (no reasoning or answer tags). Because of this, 'Best Move - All' significantly underperforms as it gives improper parsing formatting on a few of the tasks.\n2) OOD Mates: This is a test where on 3 separate out of distribution domains, the model is asked to correctly choose a checkmate given the current position. These domains are 'Knights & Rooks', 'Same Color', and 'More Pieces' from *Mészáros 2025, Out-of-distribution Tests Reveal Compositionality in Chess Transformers*. For example, Knights & Rooks has a massive number of knights and rooks on the board and the goal is to checkmate the opposing king -- these 'Knights & Rooks' board states are impossible to reach in normal play. The others are very rare to reach but possible given various abnormal pawn promotions.\n\n| Model                           | FBA       |           | OOD Mates |           |\n|---------------------------------|-----------|-----------|-----------|-----------|\n| **Baselines**                   |           |           |           |           |\n| Qwen2.5 7B-Instruct             | 0.32      | —         | 0.00      | —         |\n| Llama 4 Maverick                | 0.47      | —         | 0.14      | —         |\n| gpt-oss-120b (Medium)           | **1.00**  | —         | **0.78**  | —         |\n|                                 |           |           |           |           |\n| **Inclusion Experiments**       | *SFT*     | *RL*      | *SFT*     | *RL*      |\n| Rejection Sampling - Pred. Move | 0.38      | 0.37      | 0.04      | 0.01      |\n| Rejection Sampling - All        | 0.41      | 0.32      | 0.04      | 0.02      |\n| Verbalized Alpha-Beta Pruning   | 0.41      | 0.34      | 0.04      | 0.01      |\n| Factual Board Answering         | **0.60**  | **0.58**  | 0.06      | 0.04      |\n| Guided Synthetic                | 0.42      | 0.36      | 0.04      | 0.02      |\n| Best Move                       | 0.36      | 0.34      | 0.07      | 0.06      |\n| Best Line                       | 0.40      | 0.37      | 0.06      | 0.08      |\n| Best Move - All                 | 0.37      | 0.37      | 0.06      | 0.05      |\n| Best Line - All                 | 0.57      | **0.58**  | **0.09**  | **0.14**  |\n|                                 |           |           |           |           |\n| **Scaled Experiments**          | *SFT*     | *RL*      | *SFT*     | *RL*      |\n| Best Move XL                    | 0.40      | 0.71      | 0.07      | 0.05      |\n| Best Line XL                    | **0.61**  | 0.65      | 0.08      | 0.09      |\n| Best Move + Best Line XL        | 0.59      | **0.82**  | **0.11**  | **0.15**  |\n\nA few things we'd like to note on common failure nodes:\n- For the FBA task, we often see the model predict the correct answer immediately (similar to the SFT task) then proceed to enter its reasoning and provide a final answer in parsed answer tags. Often this first prediction is correct when the final answer is incorrect. As these are all new unseen tasks, this is likely a sign that the model's latent abilities that are used when predicting the next token directly aren't fully leveraged during its final reasoning, which partially aligns with our finding about unfaithful reasoning for the predict move task.\n- For the OOD Mates, our best performance is candidly lower than we would have hoped. That said it is a significant improvement on the 0% of the base model and beats Llama 4 Maverick (that has ~50x the parameter count of our model). These are quite out of domain examples and some take time to determine the right answer as they aren't immediately obvious -- we believe that if one employed some of the strategies we mention in our limitations section (multi-turn RL, tuning the reward for Predict Move), you could strongly improve this OOD generalization.\n\nWe have added this to our latest paper revision and we have examples of the OOD Mate problems as well (See Appendix D)."}}, "id": "MaF3421NmB", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712795479, "cdate": 1763712795479, "tmdate": 1763712795479, "mdate": 1763712795479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the controlled domain of chess, this study dives into which types of tasks/rewards yield best performance in terms of accuracy and reasoning faithfulness during supervised finetuning (SFT). It finds that while move prediction alone is effective, it is unfaithful and that a mixed multi-turn trajectory (with other board reasoning tasks) leads to similar performance with increased faithfulness. When RL is further applied on top of these checkpoints, the latter (more faithful) also leads to stabler RL training and better performance. This study then concludes that the metrics of SFT checkpoints can correlate with the eventual effectiveness of the RL."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The study is well-motivated and self-contained. The study thoroughly investigates the hypotheses around how SFT datasets impact both SFT and RL performance and how qualitative performance and downstream performance are affected by the SFT data.\n* The analysis is comprehensive across several different data mixes for SFT and chess board reasoning tasks, and the findings to the research questions are interesting and thought-provoking, even if the actual performances are not strong or the differences in final performance are not significant. The paper is still well-organized despite all the research questions, SFT methods/models, and experiments."}, "weaknesses": {"value": "The focused study of the chess domain (which is omitted from the title) makes it difficult to draw strong conclusions to other domains like language (or multimodal) reasoning tasks. In particular, there’s a less natural notion of (best) “next move” for reasoning problems - the basic assumption for reasoning problems is already focused on the line (full trajectory). This is less of an issue with some of the other board reasoning tasks, but there is perhaps too much focus on best move/line. Even though they are most effective, they may be less relatable to problems in other domains.\n\nMore concretely, there is no parallel “application” of these learnings to real-world datasets or reasoning problems to verify these findings.\n\nUltimately, I view this as a major weakness, moreso than some of the other limitations described in the paper like model size or over-focus on specific aspects of chess."}, "questions": {"value": "1. Why didn’t the board understanding questions and verbalized alpha-beta pruning work? I don't completely understand the token density argument because many reasoning LLMs prioritize longer response length as a positive signal for \"reasoning\" capabilities. So what's different in chess? Does it imply that an optimal model (like chess engines) would not be able to reason about chess particularly well outside of giving good moves/board valuations, or is it more likely a failure of the instruction tuning dataset mix and that it could be fixed? There’s a related observation where the unfaithful reasoning still yielded fair performance, so perhaps the model doesn't need to understand why a move is good to think it is better.\n\n\n2. Relatedly, the final evaluation are the tasks described in 3.2, which are geared for the game of chess itself. So it is perhaps unsurprising that while a mix is helpful, line and best move are most critical (and therefore qualitative signals like move accuracy/reasoning quality correlate). But how do these do on the other tasks in the dataset, like board factual QA (the other two tasks are likely harder to evaluate). Would your conclusions be starkly different? Is the model too overfit to think only about moves?\n\n3. Even though the best move and best line SFT modes yielded similar performances, did they differ in their confidences of predictions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qJa7V08iJP", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_GB2Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_GB2Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990209273, "cdate": 1761990209273, "tmdate": 1762926030949, "mdate": 1762926030949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}