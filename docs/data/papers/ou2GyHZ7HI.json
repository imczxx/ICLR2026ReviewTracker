{"id": "ou2GyHZ7HI", "number": 15799, "cdate": 1758255370047, "mdate": 1759897281297, "content": {"title": "How Reasoning Evolves from Post-Training Data in Sequential Decision-Making Domains", "abstract": "We study how reasoning evolves in a language model -- from supervised fine-tuning (SFT) to reinforcement learning (RL) -- by analyzing how a set of theoretically-inspired datasets impacts language model performance in a verifiable Markov Decision Process (MDP) such as chess. We find that fine-tuning a model to directly predict the best move leads to effective RL and the strongest downstream performance -- however, the RL stage elicits $\\textit{unfaithful}$ reasoning (reasoning inconsistent with the chosen move). Alternatively, training on multi-move trajectories yields comparable downstream performance with faithful reasoning and more stable RL. We show that RL induces a substantial positive shift in the distribution of move quality and reduces hallucination rates as a side effect. Finally, we find several SFT-checkpoint metrics -- metrics spanning evaluation performance, hallucination rates, and reasoning quality -- to be predictive of post-RL model performance. We release checkpoints and final models as well as training data, evaluations, and code which allowed us to surpass leading open-source reasoning models in chess with a 7B-parameter model.", "tldr": "We study how reasoning evolves (both qualitative and quantitative performance) from custom post-training datasets through SFT and RL on a sequential decision domain (e.g., chess).", "keywords": ["Reasoning", "Reasoning Models", "Markov Decision Process", "MDP", "Language Models", "Supervised Fine-Tuning", "Reinforcement Learning", "RLVR", "Dataset Design", "Chess"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ed0f67a987aae1cdfb1c7997f7260832a2d4f9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how reasoning emerges and develops in large language models fine-tuned for sequential decision-making tasks, specifically using chess as a verifiable testbed. The authors construct and analyze a suite of datasets grounded in theoretical motifs (Best Move, Best Line, Verbalized Alpha-Beta, etc.), study the effects of dataset composition on SFT, and follow with RL to assess how model behaviors, reasoning faithfulness, move quality, and hallucination rates evolve."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**1. Originality:** The systematic comparison of six dataset construction methods (Best Move, Best Line, Factual Board Answering, Verbalized Alpha-Beta Pruning, Rejection Sampling, Guided Synthetic) across both SFT and RL phases is novel. Using chess as a verifiable reasoning domain is well-motivated, enabling precise measurement of reasoning quality.\n\n**2. Quality:** Experimental design is rigorous and comprehensive. Evaluation is multifaceted, combining quantitative metrics (move accuracy, legal move rate, average move rank) with qualitative analysis (reasoning strategy usage, hallucination rates, faithfulness). \n\n**3. Clarity:** Great presentation throughout. Claims are well-supported by clear visualizations. The systematic organization makes the comparative analysis easy to follow and findings straightforward to interpret."}, "weaknesses": {"value": "**1. Limited Generalizability:** Experiments use only Qwen2.5-7B-Instruct on chess. Key findings—especially the faithful/unfaithful reasoning distinction—require validation on alternative base models and domains (math, coding) to establish whether they represent fundamental reasoning principles or chess/model-specific artifacts.\n\n**2. Insufficient Training Dynamics Analysis:** The work primarily reports task accuracy (training rewards), overlooking critical training signals (response length, entropy, or KL divergence) that would provide mechanistic insights into observed phenomena like RL stability differences and unfaithful reasoning emergence. Deeper analysis of training dynamics would substantially strengthen claims and broaden applicability."}, "questions": {"value": "1. Can the authors provide more detail or statistical analysis regarding the faithfulness/quality metrics? For example, can the reliability of the LLM-as-a-judge scores be compared to human expert assessment for a sample of traces?\n\n2. The authors show Best Move achieves comparable performance to Best Line despite unfaithful reasoning. This raises concerns: Is faithful reasoning actually necessary for strong chess performance? What are the implications for reasoning models more broadly if unfaithful reasoning suffices?\n\n3. While hallucination rates drop post-RL and unfaithful rationalization is noted, more granularity is needed: What are the most persistent failure modes? Do failure modes remain unchanged during SFT and RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k94ThKP3PZ", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_sfa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_sfa6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707417161, "cdate": 1761707417161, "tmdate": 1762926032234, "mdate": 1762926032234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors look to determine good decision decisions and takeaways on developing a reasoning model using chess as a testbed. They train Qwen 7B over a range of different dataset SFT and then RL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The finding that best line works well is interesting.(\"Best Line: Given a board, predict the optimal line of play (4 − 6 plies) ending with the expected centipawn delta from playing this line.\") It offers some insight toward efficiently training LM-style models.\n* I like the structure of the tables in the appendix, I think it is effective."}, "weaknesses": {"value": "* Language models so far aren't great at playing chess directly and trained model does not exceed/match the performance of the 120B model so it is unclear how well these findings scale. Or how meaningful they are.\n* This work does not reconnect the training results with downstream/tournament/chess results. Further grounding the results w.r.t. to external models or performance would help position this work.\n* I think the reasoning faithfulness metrics were not well justified (see questions)\n* It took a whole to understand how/what data mixtures were used. Part of it is because Figure 4 really doesn't capture the information and \"Best Line\" and \"Best Move\" mixtures include other data as well. I think moving some of the information up from APP D DATA INCLUSION ANALYSES would help set this straight. Also, I think it is reasonable to do, but the final best run Best Move + Best Line also has 2x the data of other runs. (Which makes it unclear how you did the comparison for Figure 4?)\n* Table 1 and Table 2 both show BM > BL. (How do we square that with saying BL is the best?) Figure 5 also shows BM as best."}, "questions": {"value": "Q: Were you able to validate the reasoning faithfulness or compare gpt-120's answers vs an expert? (Are the language model judgements grounded to something concrete?)\nQ: \"Best Line had more stable RL training than Best Move.\" If the Best Move dataset were scaled such that the total number of meaningful tokens of learning were the same, do we still see the improvement? (Which part of this approach is the key part?) (Is the centi pawn estimate the key part?)\nQ: Having a place where it clearly defines what datasets are used when/where would help.  \"Best Move - All\"; Where is All defined? (I might have missed it.)\n\n# Notes, Minor\n* Overall the style and sizing of the barplot figures were hard to read. It did not jump out what was the metric and what was the training condition.\n* Likewise, I didn't find the multi-part figures always helpful. Making them bigger with more stark lines would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mnLF6eUGFC", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_1Npe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_1Npe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805427656, "cdate": 1761805427656, "tmdate": 1762926031737, "mdate": 1762926031737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how \"reasoning\" ability emerges and evolves in a large language model as it undergoes supervised fine-tuning and subsequent reinforcement learning (RL). Using the game of chess as a verifiable, sequential decision-making environment (a Markov Decision Process), the authors meticulously study how different types of training data influence a model's performance and the nature of its internal logic."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The choice of chess is a strength of the paper. Chess is not just a game; it is a verifiable, discrete, episodic MDP, which provides a controlled environment for studying reasoning. The availability of chess engines as objective oracles for reward removes the need for potentially noisy human feedback or learned reward models, helping to isolate the variable of interest—the impact of training data on the reasoning process. \n2. The discovery that SFT-checkpoint metrics can predict final RL performance is a practical contribution. The SFT stage is cheaper and faster than RL. By identifying these predictive signals, the authors provide a cost-effective methodology for iterating on and selecting the best base models before committing to expensive RL runs."}, "weaknesses": {"value": "1. Limited Generalizability of the Domain: While a strength for verifiability, the sterile, perfect-information environment of chess is also a weakness. Real-world decision-making is noisy, partially observable, multi-agent, and often lacks a clear reward signal.\n2. The paper lacks analysis regarding scalability.\n3. Conflation of \"Reasoning\" with \"Verbalization\": The study defines reasoning as the model's language-based chain-of-thought. This is a limiting perspective. True reasoning might be occurring in a continuous latent space."}, "questions": {"value": "Nil"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iiNd76YMHb", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_JYqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_JYqb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973031673, "cdate": 1761973031673, "tmdate": 1762926031330, "mdate": 1762926031330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the controlled domain of chess, this study dives into which types of tasks/rewards yield best performance in terms of accuracy and reasoning faithfulness during supervised finetuning (SFT). It finds that while move prediction alone is effective, it is unfaithful and that a mixed multi-turn trajectory (with other board reasoning tasks) leads to similar performance with increased faithfulness. When RL is further applied on top of these checkpoints, the latter (more faithful) also leads to stabler RL training and better performance. This study then concludes that the metrics of SFT checkpoints can correlate with the eventual effectiveness of the RL."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The study is well-motivated and self-contained. The study thoroughly investigates the hypotheses around how SFT datasets impact both SFT and RL performance and how qualitative performance and downstream performance are affected by the SFT data.\n* The analysis is comprehensive across several different data mixes for SFT and chess board reasoning tasks, and the findings to the research questions are interesting and thought-provoking, even if the actual performances are not strong or the differences in final performance are not significant. The paper is still well-organized despite all the research questions, SFT methods/models, and experiments."}, "weaknesses": {"value": "The focused study of the chess domain (which is omitted from the title) makes it difficult to draw strong conclusions to other domains like language (or multimodal) reasoning tasks. In particular, there’s a less natural notion of (best) “next move” for reasoning problems - the basic assumption for reasoning problems is already focused on the line (full trajectory). This is less of an issue with some of the other board reasoning tasks, but there is perhaps too much focus on best move/line. Even though they are most effective, they may be less relatable to problems in other domains.\n\nMore concretely, there is no parallel “application” of these learnings to real-world datasets or reasoning problems to verify these findings.\n\nUltimately, I view this as a major weakness, moreso than some of the other limitations described in the paper like model size or over-focus on specific aspects of chess."}, "questions": {"value": "1. Why didn’t the board understanding questions and verbalized alpha-beta pruning work? I don't completely understand the token density argument because many reasoning LLMs prioritize longer response length as a positive signal for \"reasoning\" capabilities. So what's different in chess? Does it imply that an optimal model (like chess engines) would not be able to reason about chess particularly well outside of giving good moves/board valuations, or is it more likely a failure of the instruction tuning dataset mix and that it could be fixed? There’s a related observation where the unfaithful reasoning still yielded fair performance, so perhaps the model doesn't need to understand why a move is good to think it is better.\n\n\n2. Relatedly, the final evaluation are the tasks described in 3.2, which are geared for the game of chess itself. So it is perhaps unsurprising that while a mix is helpful, line and best move are most critical (and therefore qualitative signals like move accuracy/reasoning quality correlate). But how do these do on the other tasks in the dataset, like board factual QA (the other two tasks are likely harder to evaluate). Would your conclusions be starkly different? Is the model too overfit to think only about moves?\n\n3. Even though the best move and best line SFT modes yielded similar performances, did they differ in their confidences of predictions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qJa7V08iJP", "forum": "ou2GyHZ7HI", "replyto": "ou2GyHZ7HI", "signatures": ["ICLR.cc/2026/Conference/Submission15799/Reviewer_GB2Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15799/Reviewer_GB2Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990209273, "cdate": 1761990209273, "tmdate": 1762926030949, "mdate": 1762926030949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}