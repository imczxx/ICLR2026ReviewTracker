{"id": "ggqNAGLQjR", "number": 18525, "cdate": 1758288771453, "mdate": 1759897098192, "content": {"title": "MedSimSearch: Sim2Real Agentic Learning for Medical Visual Reasoning", "abstract": "Developing autonomous agents for complex Medical Visual Reasoning is a critical goal, yet training them in real-world clinical settings is largely infeasible due to severe privacy, data, and safety constraints. While retrieval-augmented methods exist, they often depend on impractical multimodal indexing or fail to address the core challenge of learning interactive policies without real-world exposure.\nTo bridge this gap, we introduce MedSimSearch, a novel framework based on Sim2Real Agentic Learning. The core innovation lies in leveraging a generative large multimodal model (LMM) to create a high-fidelity simulated retrieval environment. Within this safe, text-only simulation, our agent learns a robust search and reasoning policy, eliminating the need for multimodal data indexing while preserving patient privacy.\nTo validate our approach, we evaluate the agent trained in simulation on realistic medical benchmarks using a curated private text corpus. Extensive experiments on VQAMed2019 and OmniMedVQA demonstrate that MedSimSearch significantly surpasses strong retrieval-augmented generation (RAG) baselines and shows enhanced robustness against hallucinations, paving a viable path for deploying trustworthy medical AI agents.", "tldr": "We operationalize the Sim2Real paradigm for medical AI, successfully training a reasoning agent entirely in a simulated environment that then performs robustly on real-world clinical tasks.", "keywords": ["Medical Visual Reasoning", "Reinforcement Learning", "Agentic AI", "Sim2Real", "Environment Simulation", "Privacy-Preserving AI"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8473f89a922dc008dc415b533d60790e5b1dc833.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work introduces MedSimSearch, a framework that operationalizes the Sim2Real paradigm for agentic learning in the context of medical visual reasoning. The work is motivated by the real-world challenges of training autonomous agents in clinical settings, which are often constrained by privacy, data scarcity, and safety concerns. The core proposal is to leverage a generative Large Multimodal Model to create a text-only simulated retrieval environment. Within this safe simulation, an RL agent learns a robust, text-only search policy, eliminating the need for impractical multimodal data indexing. The agent's policy is optimized using GRPO through a curriculum-based rollout that progressively introduces noisy/negative pseudo-documents to improve robustness. The agent, trained entirely in this simulated environment, is then validated on real-world medical benchmarks using a private text corpus."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work is very well-motivated. It identifies a critical and practical bottleneck in deploying interactive medical AI: the inability to train agents on real systems due to privacy, access, and data scarcity. The proposed Sim2Real approach, using a text-only simulator, is a pragmatic and well-justified workaround for this significant hurdle.\n- The work integrates several advanced methods to address this problem. The use of an LMM-as-simulator, combined with a curriculum learning strategy (dynamically mixing \"useful\" vs. \"noisy\" pseudo-documents), and optimization via GRPO, represents a novel and technically sound pipeline for this domain.\n- The experiments are thorough and well-designed. The authors evaluate MedSimSearch on two benchmarks and compare against a comprehensive set of baselines, including zero-shot, SFT, and RAG models. The validation on a specialized, curated medical text corpus to test the Sim2Real transfer is a strong component of the evaluation. The results clearly demonstrate gains over existing SOTA methods."}, "weaknesses": {"value": "- A major concern is the high computational cost of the proposed method. The paper states that training requires 4 NVIDIA A100 GPUs for the simulation server and another 4 A100 GPUs for the RL training. This 8-GPU setup makes the results difficult to reproduce for many research labs. Furthermore, the paper does not discuss the deployment cost and latency. If the agent's learned policy requires repeated calls to a powerful LMM (like the GPT-4o used in experiments) to generate pseudo-documents during inference, the practical utility in a real-time clinical setting is unclear.\n- The entire framework's success hinges on the fidelity of the LMM-generated pseudo-documents. The paper's analysis in Figure 2 attempts to address this by testing generalization to other LMMs; however, this analysis is insufficient to fully probe the gap. The models tested (GPT-4V, LLaMA-3-70B) are still very high-capability. When tested with a smaller open-source model Qwen-7B, a sizable performance drop is observed. While relying on powerful, large models is acceptable, the associated computational cost and resource requirements are critical factors in the method's overall evaluation and should be weighed accordingly (as in W1).\n- Even powerful LMMs are susceptible to hallucination, and the 30-word limit on pseudo-documents does not eliminate this risk. The paper does not propose a clear mitigation strategy for this problem. It is also not specified whether any human verification was performed on the synthesized pseudo-documents to assess their plausibility (useful or noisy). An analysis of the potential impact of simulator-induced hallucinations on the agent's policy is a critical but missing component of the evaluation."}, "questions": {"value": "- Are you going to open-source the curated medical text corpus and the pseudo-documents?\n- The curriculum learning based on \"useful\" vs. \"noisy\" pseudo-documents is a key component. To improve the qualitative understanding of this mechanism, it would be highly beneficial to add a few side-by-side examples of these generated documents. Brief annotations explaining why a specific document is considered \"useful\" (accurate, relevant) versus \"noisy\" (misleading, plausible-but-wrong) would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gx7XnC8Mr3", "forum": "ggqNAGLQjR", "replyto": "ggqNAGLQjR", "signatures": ["ICLR.cc/2026/Conference/Submission18525/Reviewer_TA4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18525/Reviewer_TA4o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702122157, "cdate": 1761702122157, "tmdate": 1762928218728, "mdate": 1762928218728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper used generative AI to synthesize documents for training the model to better retrieve evidence for medical VQA, achieving higher exact match scores than baselines on two benchmark datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The problem the paper aims to address is significant."}, "weaknesses": {"value": "The study design is misaligned with the multimodal medical scenario.\n(1) For a vision-centric problem (as in Fig. 1, e.g., “what modality is shown”), why use purely text retrieval?\n(2) Why do the pseudo-documents include only text, generated by non-medically validated models, so content quality cannot be ensured?\n\nOverclaim:\n(1) The paper provides no evidence that the simulation is a high-fidelity medical environment.\n(2) The RAG baselines compared are not multimodal, yet the task is VQA; claiming large gains over RAG is based on a single, common text-only implementation.\n(3) The curriculum is fixed, with no ablation to show its effectiveness.\n\nUnsupported evaluation:\n(1) Only two datasets are used.\n(2) Metrics are unclear (e.g., what is \"Micro\" in Table 3? Which BLEU variant, BLEU-1?).\n(3) Inconsistent results: in Table 4, all categories have very low accuracy, yet overall accuracy is much higher, mathematically inconsistent."}, "questions": {"value": "The foundational method is Zero-Search, why not use the same implementation as the baselines for comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DEWkjJ05OZ", "forum": "ggqNAGLQjR", "replyto": "ggqNAGLQjR", "signatures": ["ICLR.cc/2026/Conference/Submission18525/Reviewer_Z6mt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18525/Reviewer_Z6mt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944039047, "cdate": 1761944039047, "tmdate": 1762928217645, "mdate": 1762928217645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MedSimSearch, a framework that leverages a large multimodal model (LMM) to generate synthetic medical Q/A pairs and pseudo-documents for training an RL agent in a simulated environment. The idea is to perform Sim2Real Agentic Learning—teaching a model retrieval and reasoning policies safely through text-only simulation instead of real-world medical data. While the concept of using an LLM to emulate an environment is clever and practical for privacy and data-scarcity constraints, I find the technical contribution somewhat limited... The best model this paper proposed, which uses GPT-4o, have an inevitable issue of hallucination. In addition, this model have not been compared to the best model available."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow, with a clear narrative and logical experimental setup.\n- The overall idea—training an RL agent in a simulated environment generated by an LLM—is straightforward yet practical, especially for domains like medicine where data privacy is a barrier."}, "weaknesses": {"value": "- From a technical perspective, the work is an incremental extension of existing approches. The use of an LLM to simulate the training environment is creative but conceptually similar to prior self-play or synthetic data paradigms, without introducing a new optimization method or architecture.\n\n-  The reported gains are not substantially higher than existing baselines, and Table 4 compares mainly against Qwen 2.5-VL variants. The paper omits stronger contemporary baselines such as Med-R1, Evo-PI, or HuatuoGPT-Vision, making it difficult to judge true progress.\n\n- Hallucination risk. Because the simulated environment is entirely generated by an LLM, hallucination is inevitable. The model learns from self-generated, potentially inaccurate contexts, which could reinforce false or biased information rather than real clinical reasoning.\n\n- Figure presentation quality. The figures, particularly Figure 1, feel rough and early; even the caption (“Overview of MedSimSearch”) is overly brief for a central conceptual diagram.\n\n- Evaluation metric choice. The BLEU score adds little value for measuring factual correctness in medical VQA."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K8vGq1Z7VA", "forum": "ggqNAGLQjR", "replyto": "ggqNAGLQjR", "signatures": ["ICLR.cc/2026/Conference/Submission18525/Reviewer_WArp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18525/Reviewer_WArp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974527301, "cdate": 1761974527301, "tmdate": 1762928217196, "mdate": 1762928217196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MedSimSearch, an RL-trained, text-only search agent that learns in an LMM‑simulated retrieval environment and is then evaluated on medical VQA (VQAMed2019, OmniMedVQA). It reports strong gains over RAG and RL baselines, while avoiding multimodal indexing and claiming privacy benefits. The idea is timely and the empirical results are promising, but important methodological details (label exposure in simulation prompts, reliance on GPT‑4o at train/test, fairness of baselines, and clarity around the RL algorithm) need tightening before the work can be considered solidly conclusive."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Training agentic policies for medical VQA without accessing real clinical systems is important and underexplored. The simulator‑first stance is well motivated by privacy and availability constraints. \n\n2. Strong empirical results across two benchmarks. Consistent wins over capable baselines, including RAG and recent RL methods. The gains are nontrivial and span multiple modalities.  \n\n3. This paper is well written and easy to follow."}, "weaknesses": {"value": "1. The simulator is explicitly told “whose ground truth answer is [ground truth]” to generate pseudo‑documents. Even if only used during training, this risks imprinting the correct answer distributionally into “useful” docs; the agent may learn patterns to read it off rather than to search. The paper should quantify how much the agent relies on this supervision signal and show performance when ground truth is not passed to the simulator during training.  \n\n2. The method and baselines both use GPT‑4o to generate pseudo‑documents at test time “to ensure fair comparison\". This conflates the contribution of policy learning with the capabilities of a closed, expensive model. It is unclear how much of the final score comes from GPT‑4o’s world knowledge versus the learned policy. \n\n3. Corpus mismatch vs. reported multi‑modality gains. The “private” database C is overwhelmingly radiology‑centric (50k MIMIC‑CXR reports + 5k synthetic). Yet the Database variant scores highly on non‑radiology modalities in OmniMedVQA (e.g., OCT, FP). How can a largely chest‑X‑ray text corpus support ophthalmology/pathology questions so well? Either the synthetic 10% happens to cover those domains richly, or the model mainly relies on the generative simulator even in the Database setting. This needs clarification and ablation.  \n\n4. Section 3 emphasizes GRPO (with equation), Appendix A.2 lists GRPO settings, but §4.2 says “Unless otherwise specified, PPO is the default.” Which results are from which? A controlled GRPO vs. PPO ablation is missing, and the training stability/variance is not reported.  \n\n5. The “noisy” pseudo‑docs are produced by instructing the LMM to include “misleading or partially incorrect information”. This may not reflect real retrieval noise (e.g., partially relevant but off‑topic passages, domain shifts, OCR artifacts). The external validity of the curriculum is thus uncertain.  \n\n6. Beyond the simulator swap, we lack ablations on (i) the curriculum schedule, (ii) action budget B, (iii) the usefulness of <think> vs. <info> structure, (iv) whether the agent truly learns search sequencing vs. just better prompting to GPT‑4o."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kXg72bQal2", "forum": "ggqNAGLQjR", "replyto": "ggqNAGLQjR", "signatures": ["ICLR.cc/2026/Conference/Submission18525/Reviewer_VHA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18525/Reviewer_VHA5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009917976, "cdate": 1762009917976, "tmdate": 1762928216856, "mdate": 1762928216856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MedSimSearch, a search agent that learns in an LMM‑simulated retrieval environment and is then evaluated on medical VQA (VQAMed2019, OmniMedVQA). It try to solve the problem that no feasible environment exist for medical visual reasoning. MedSimSearch try to solve this situation by leveraging LMM to create a high-fidelity simulated retrieval environment. Then within this text-only environment,  MedSimSearch uses Sim2Real Agentic Learning and curriculum-based simulation to learn a retrieval policy, eliminating the need for multimodal indexing and ensuring privacy compliance. In VQAMed2019, OmniMedVQA, it reports strong gains over RAG and RL baselines. The idea is timely and the empirical results are promising, but important methodological details (label exposure in simulation prompts, reliance on GPT‑4o at train/test, fairness of baselines, and clarity around the RL algorithm) need tightening before the work can be considered for publish in iclr series of top ML conference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Training agentic policies for medical VQA without accessing real clinical systems is important and underexplored. The simulator‑first stance is well motivated by privacy and availability constraints. \n\n2. Strong empirical results across two benchmarks. Consistent wins over capable baselines, including RAG and recent RL methods. The gains are nontrivial and span multiple modalities.  \n\n3. This paper is well written and easy to follow."}, "weaknesses": {"value": "1. The simulator is explicitly told “whose ground truth answer is [ground truth]” to generate pseudo‑documents. Even if only used during training, this risks imprinting the correct answer distributionally into “useful” docs; the agent may learn patterns to read it off rather than to search. The paper should quantify how much the agent relies on this supervision signal and show performance when ground truth is not passed to the simulator during training.  \n\n2. The method and baselines both use GPT‑4o to generate pseudo‑documents at test time “to ensure fair comparison\". This conflates the contribution of policy learning with the capabilities of a closed, expensive model. It is unclear how much of the final score comes from GPT‑4o’s world knowledge versus the learned policy. \n\n3. Corpus mismatch vs. reported multi‑modality gains. The “private” database C is overwhelmingly radiology‑centric (50k MIMIC‑CXR reports + 5k synthetic). Yet the Database variant scores highly on non‑radiology modalities in OmniMedVQA (e.g., OCT, FP). How can a largely chest‑X‑ray text corpus support ophthalmology/pathology questions so well? Either the synthetic 10% happens to cover those domains richly, or the model mainly relies on the generative simulator even in the Database setting. This needs clarification and ablation.  \n\n4. Section 3 emphasizes GRPO (with equation), Appendix A.2 lists GRPO settings, but §4.2 says “Unless otherwise specified, PPO is the default.” Which results are from which? A controlled GRPO vs. PPO ablation is missing, and the training stability/variance is not reported.  \n\n5. The “noisy” pseudo‑docs are produced by instructing the LMM to include “misleading or partially incorrect information”. This may not reflect real retrieval noise (e.g., partially relevant but off‑topic passages, domain shifts, OCR artifacts). The external validity of the curriculum is thus uncertain.  \n\n6. Beyond the simulator swap, we lack ablations on (i) the curriculum schedule, (ii) action budget B, (iii) the usefulness of <think> vs. <info> structure, (iv) whether the agent truly learns search sequencing vs. just better prompting to GPT‑4o."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kXg72bQal2", "forum": "ggqNAGLQjR", "replyto": "ggqNAGLQjR", "signatures": ["ICLR.cc/2026/Conference/Submission18525/Reviewer_VHA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18525/Reviewer_VHA5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009917976, "cdate": 1762009917976, "tmdate": 1763753218307, "mdate": 1763753218307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}