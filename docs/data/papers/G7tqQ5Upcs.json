{"id": "G7tqQ5Upcs", "number": 4083, "cdate": 1757598108403, "mdate": 1759898053847, "content": {"title": "SoftCFG: Uncertainty-guided Stable Guidance for Visual Autoregressive Model", "abstract": "Autoregressive (AR) models have emerged as powerful tools for image generation by modeling images as sequences of discrete tokens. While Classifier-Free Guidance (CFG) has been adopted to improve conditional generation, its application in AR models faces two key issues: guidance diminishing, where the conditional–unconditional gap quickly vanishes as decoding progresses, and over-guidance, where strong conditions distort visual coherence. To address these challenges, we propose SoftCFG, an uncertainty-guided inference method that distributes adaptive perturbations across all tokens in the sequence. The key idea behind SoftCFG is to let each generated token contribute certainty-weighted guidance, ensuring that the signal persists across steps while resolving conflicts between text guidance and visual context. To further stabilize long-sequence generation, we introduce Step Normalization, which bounds cumulative perturbations of SoftCFG. Our method is training-free, model-agnostic, and seamlessly integrates with existing AR pipelines. Experiments show that SoftCFG significantly improves image quality over standard CFG and achieves state-of-the-art FID on ImageNet 256 × 256 among autoregressive models.", "tldr": "We propose SoftCFG, an uncertainty-guided and context-aware regularizer for AR image generation that stabilizes classifier-free guidance and achieves SOTA FID of ARs on ImageNet $256\\times256$ without extra training or changes.", "keywords": ["visual autoregressive model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/552d56160550c01b303b5f9b14332e6b895e05c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SoftCFG, a plug-and-play inference-time method that improves the stability and fidelity of visual autoregressive models. The method replaces the fixed classifier-free guidance with a soft, uncertainty-weighted guidance that adaptively adjusts the influence of each generated token. Additionally, a Step Normalization mechanism is proposed to bound accumulated perturbations across steps. Experiments on ImageNet and text-to-image benchmarks show consistent FID improvements over standard CFG without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is lightweight, easy to integrate into existing AR inference pipelines, and does not require additional training or data.\n2. The motivation is clear and relevant to current challenges in autoregressive generation.\n3. The paper is well-written, logically structured, concise, and clear, making it easy for readers to understand."}, "weaknesses": {"value": "1. SoftCFG largely reinterprets existing CFG dynamics rather than proposing a fundamentally new principle. The main innovation lies in weighting and normalization heuristics.\n2. The reported FID gains (~0.1–0.2) are within or close to the variance commonly observed across runs and sampling seeds. It is unclear whether these improvements are statistically significant or perceptually meaningful."}, "questions": {"value": "1. What is the standard deviation of FID across runs? Are the reported improvements statistically significant under multiple random seeds or sampling temperatures?\n2. Since SoftCFG strengthens high-confidence tokens, what happens if the model is confidently wrong? Could this amplify hallucination or bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jKl7dKIQFI", "forum": "G7tqQ5Upcs", "replyto": "G7tqQ5Upcs", "signatures": ["ICLR.cc/2026/Conference/Submission4083/Reviewer_Yrai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4083/Reviewer_Yrai"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425136846, "cdate": 1761425136846, "tmdate": 1762917169683, "mdate": 1762917169683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SoftCFG, a new uncertainty-guided inference method for discrete visual autoregressive (AR) models. The method aims to mitigate the problem of the guidance signal diminishing over time by reweighing the confidence of previous tokens within the unconditional KV cache. And it introduces the Step Normalization to avoid the explosion of the guidance caused by this reweighing step. Experimental results demonstrate that SoftCFG can improve the generation quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed SoftCFG method is intuitive, easy to understand, and appears simple to implement.\n2. The method shows a notable improvement in generation quality on the ImageNet-256x256 dataset."}, "weaknesses": {"value": "1. **Limited Generalizability:** The paper presents SoftCFG as a general method, yet its effectiveness is only thoroughly validated on a single model. This narrow experimental scope is insufficient to support the claim of generality. Furthermore, the qualitative results shown for the RAR model in Figure 1 are not convincing and do not demonstrate a clear benefit. (Beside, the new versions (10 October 2025) of the baseline model alitok can achieve CFG results comparable to those reported in your paper with SoftCFG. This raises a critical question: is the reported FID improvement a genuine algorithmic contribution of SoftCFG, or does it merely compensate for a sub-optimally tuned CFG baseline? )\n2. The 'diminishing guidance' problem may be an artifact of the baseline using only a single class token. Would this problem already be alleviated in standard CFG if the class token were repeated 64 times as the condition, similar to MAR?\n3. There appears to be a significant error in Equation 6. The equation as written is inconsistent with the line 8 in Algorithm 1 and does not align with Equation 7. I suspect the correct formulation should be $$z_t^{SoftCFG}=z_t^{cond}+scale*(z_t^{cond}-z_t^{uncond,pertcontext})$$?\n4. **Missing Quantitative Results:** The paper text explicitly states in Section 3.1 (lines 349-350, 355-358) that quantitative results (GenEval benchmark, DPG-Bench) for Text-to-Image (T2I) generation are provided. However, no such quantitative results are present anywhere in the paper. This is a major omission that leaves the T2I claims entirely unsubstantiated.\n5. **Overall Presentation:** The paper is not well-written. The presentation suffers from a lack of clarity, and the significant issues noted above (e.g., the error in Equation 6, the missing T2I results) make the paper difficult to follow."}, "questions": {"value": "see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DPJOPWinar", "forum": "G7tqQ5Upcs", "replyto": "G7tqQ5Upcs", "signatures": ["ICLR.cc/2026/Conference/Submission4083/Reviewer_HN4L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4083/Reviewer_HN4L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832471971, "cdate": 1761832471971, "tmdate": 1762917169417, "mdate": 1762917169417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **SoftCFG**, a training-free and model-agnostic inference modification for **visual autoregressive (AR)** image generation models.  \nIt addresses two common problems when applying Classifier-Free Guidance (CFG) to AR models:  \n(1) *guidance diminishing* (conditional signal fading as decoding progresses), and  \n(2) *over-guidance* (visual distortions caused by high guidance scales).\n\nSoftCFG introduces **uncertainty-guided token-wise perturbations** to the unconditional branch: each past token contributes to guidance proportionally to its **prediction confidence**, influencing future decoding steps through the cached value vectors.  \nA **Step Normalization** mechanism further ensures stability by normalizing cumulative perturbations at each step.  \nThe method requires no retraining, adds negligible computational cost, and yields improved FID on ImageNet-256 (1.37 → 1.27) compared to vanilla CFG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear motivation and problem framing** – The issues of guidance fading and over-guidance in AR models are well-illustrated with entropy plots and examples.  \n2. **Elegant, simple solution** – The token-wise confidence weighting and step normalization are easy to implement and integrate into existing AR inference pipelines.  \n3. **Training-free and architecture-agnostic** – Works as a plug-in for existing models like AliTok and LuminaGPT without retraining or modifying the transformer architecture.  \n4. **Empirical improvement** – Achieves state-of-the-art FID among AR models on ImageNet-256 with negligible runtime overhead.  \n5. **Good ablations and qualitative examples** – The paper studies the impact of StepNorm, guidance scale γ, and scheduling power k, and presents clear visual comparisons showing fewer artifacts.  \n6. **Transparency and reproducibility** – The paper includes clear algorithms, theoretical bounds, and a stated plan to release code."}, "weaknesses": {"value": "1. **Limited novelty** – Conceptually extends prior ideas (adaptive guidance, token-level perturbation) from diffusion models to the AR setting.  \n2. **Fragile confidence heuristic** – The reliance on max probability as a proxy for uncertainty can mislead guidance, as shown in the “cat–car” failure case.  \n3. **Partial perturbation design** – Only value caches are scaled; effects on keys or multi-head attention routing are not explored.  \n4. **Step Normalization rigidity** – The fixed-sum normalization may underutilize guidance in long sequences; no adaptive scheduling is tested.  \n5. **Loose theoretical analysis** – The Lipschitz-based bound is general but too weak to predict actual model stability.  \n6. **Limited experimental breadth** – Results are restricted to ImageNet-256 and a few text-to-image examples; no tests on higher-resolution or multi-modal AR tasks.  \n7. **Hyperparameter fairness** – CFG baselines may not have been re-tuned under identical γ/k sweeps, potentially overstating SoftCFG’s advantage."}, "questions": {"value": "1. Why only perturb the **value cache (V)** and not keys or queries?  \n2. How sensitive is the performance to **confidence miscalibration**? Would temperature scaling or learned uncertainty improve robustness?  \n3. Could **Step Normalization** be relaxed or made adaptive for longer contexts?  \n4. Are improvements consistent under **different tokenizers** or **generation orders** (e.g., folded or diagonal AR)?  \n5. How does SoftCFG behave on **text-to-image benchmarks** quantitatively (e.g., COCO-FID, GenEval metrics)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h24wdFFSBk", "forum": "G7tqQ5Upcs", "replyto": "G7tqQ5Upcs", "signatures": ["ICLR.cc/2026/Conference/Submission4083/Reviewer_ny6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4083/Reviewer_ny6c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893685828, "cdate": 1761893685828, "tmdate": 1762917169183, "mdate": 1762917169183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}