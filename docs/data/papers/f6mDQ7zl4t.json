{"id": "f6mDQ7zl4t", "number": 19534, "cdate": 1758297026918, "mdate": 1759897034147, "content": {"title": "The Majority is not always right: RL training for solution aggregation", "abstract": "Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.", "tldr": "We propose to learn solution aggregation as an explicit reasoning skill, through reinforcement learning from verifiable rewards.", "keywords": ["llms", "reasoning", "rl"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb9063f8fec4ac7083597131184c6ba3d555fe4d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a reinforcement learning (RL)-based method for large language models (LLMs) that performs aggregation of multiple sampled responses as part of test-time scaling. Unlike previous approaches that rely on naive frequency-based voting or reward-model ranking, the proposed method trains an aggregator model that generates a final answer by reasoning over all sampled responses.  \nThis allows the model to handle diverse situations, such as when the correct answer is in the minority or when partially valid reasoning exists in incorrect responses. Through extensive experiments, the authors demonstrate the effectiveness of their approach and show that it remains robust across various settings, including in-/out-of-domain generalization, different numbers of candidate responses, and varying mixtures of training data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is **conceptually simple yet delivers clear performance improvements.** Notably, it can be trained using existing and publicly available response data through standard RL techniques, which suggests that it can serve as a practical foundation for future research in this direction.\n2. The authors present **comprehensive robustness analyses.** They evaluate the method across diverse datasets, model configurations, answer sizes, and training data mixtures, demonstrating that the approach is consistently robust to these factors. As discussed in the conclusion, this robustness implies that the method could be integrated into post-training pipelines as a promising component for future test-time scaling strategies.\n3. The paper employs **strong and fair baselines.** The baselines include large reward models specialized for mathematical reasoning (7B and 72B), which are far more resource-intensive than the 1.7B aggregator model used in this work. Despite the smaller size, the proposed method achieves superior performance and remarkable token efficiency, showing it is both effective and efficient."}, "weaknesses": {"value": "1. The experiments are limited to the mathematics domain (e.g., AIME datasets). It remains unclear whether the proposed RLVR framework would **generalize to other reasoning-intensive domains** such as coding, where verifiable signals are also available.\n2. The training data, DeepScaler, is designed in an AIME-like style, which raises the possibility of **data leakage or overfitting to similar problem types.** Further validation or control experiments would strengthen the claim of generalization. Clarifying and addressing this potential data overlap is essential to ensure the validity of the reported improvements."}, "questions": {"value": "1. How does the **aggregator model scale with size**? Since generative aggregation requires reasoning over multiple candidate solutions (and even synthesizing correct reasoning from entirely incorrect ones), it is reasonable to expect that larger models might exhibit stronger aggregation ability. Have the authors examined such scaling trends?\n2. When scaling up the aggregator, **does the performance gap between prompted aggregation and RL-trained aggregation remain?** Table 1 and Table 2 suggest that the difference narrows as the solution model becomes stronger, implying that RL-based aggregation might be less beneficial when the base solutions are already of high quality.\n3. Would similar performance be observed if the model were trained on **datasets unrelated to AIME** or the math domain?\n4. Could the proposed method be extended robustly to **other domains,** such as coding or scientific reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FYQIiVEsbt", "forum": "f6mDQ7zl4t", "replyto": "f6mDQ7zl4t", "signatures": ["ICLR.cc/2026/Conference/Submission19534/Reviewer_qX3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19534/Reviewer_qX3S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793063600, "cdate": 1761793063600, "tmdate": 1762931423128, "mdate": 1762931423128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning-based approach (RLVR) for aggregating multiple candidate solutions generated by large language models (LLMs) in mathematical reasoning tasks. The method aims to overcome the limitations of traditional majority voting strategies, such as ignoring correct minority solutions or failing to synthesize partially correct reasoning distributed across different candidates. Specifically, the paper trains a dedicated aggregator model that learns from verifiable rewards to synthesize, correct, or select among multiple candidate solutions. Experiments conducted on four challenging math competition datasets demonstrate that this approach consistently outperforms majority voting, reward model-based baseline methods such as AceMath, and naive generative aggregation methods across various aggregation scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper accurately identifies the shortcomings of existing majority voting aggregation strategies, such as potentially overlooking correct minority solutions and failing to integrate partial reasoning distributed across different candidate answers. It compellingly argues for the necessity of learning an aggregation method.\n\n2. The method was evaluated on four challenging math competition datasets, and the effectiveness of AggLM was analyzed from multiple perspectives, supporting the paper's claims"}, "weaknesses": {"value": "1. All experiments are limited to mathematical tasks that have verifiable rewards. For non-RLVR tasks, the training approach of AggLM seems no longer applicable.\n\n2. The RLVR experiments are restricted to the Qwen series. Demonstrating cross-series generalization capabilities beyond Qwen and comparisons with closed-source models like GPT would be beneficial."}, "questions": {"value": "1. Could the authors elaborate on whether AggLM provides advantages over directly fine-tuning LLMs with RLVR in tasks such as mathematics, where rewards can be readily verified?\n\n2. Figure 4 suggests that when the number of candidate solutions is large, the performance gap between AggLM and majority voting tends to decrease, while greater benefits are observed with fewer candidates. Under comparable resource consumption (or inference latency), might the strategy of “fewer candidate solutions + AggLM” be more beneficial than “more candidate solutions + majority voting”?\n\n3. The current experimental comparison is primarily with model-based methods. Have the authors considered including self-certainty [1] and PiCSAR [2] in the evaluation? These additional baselines might help highlight the strengths of AggLM more clearly.\n\n>[1]Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. ArXiv preprint, abs/2502.18581, 2025. URL https://arxiv.org/abs/2502.18581.\n\n>[2]Leang, J. O. J., Zhao, Z., Gema, A. P., Yang, S., Kwan, W.-C., He, X., Li, W., Minervini, P., Giunchiglia, E., & Cohen, S. B. (2025). PiCSAR: Probabilistic Confidence Selection And Ranking for Reasoning Chains. arXiv. https://arxiv.org/abs/2508.2178"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N3nV9MvA79", "forum": "f6mDQ7zl4t", "replyto": "f6mDQ7zl4t", "signatures": ["ICLR.cc/2026/Conference/Submission19534/Reviewer_3Zxb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19534/Reviewer_3Zxb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899096615, "cdate": 1761899096615, "tmdate": 1762931422683, "mdate": 1762931422683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AGGLM, an approach for aggregating multiple candidate solutions generated by large language models (LLMs), with a focus on challenging reasoning tasks, such as those in mathematics. The central idea is to explicitly train an aggregator model to review, reconcile, and synthesize a correct final answer from a set of generated solutions, framing aggregation itself as a learned skill via reinforcement learning from verifiable rewards. \n\nExperiments are conducted across four benchmarks, with results showing that the learned aggregator consistently outperforms majority voting, reward model selection, and naive aggregation baselines. The method is further shown to generalize across solution sources and is more token-efficient than existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a well-motivated gap in solution aggregation for LLM reasoning, where majority-based or naive aggregation often fails, especially when correct solutions are minority or require synthesis across candidates.\n\n2. Results span four math competition datasets, with in-depth benchmarking against majority voting, best-of-N, weighted majority via reward models, and prompted LLM aggregation, as shown in Tables 1, 2, and 3.\n\n3. The evaluation methodology is spelled out clearly (Section 4.2) so experiments can be reproduced and compared fairly."}, "weaknesses": {"value": "1. Limited model diversity. The study only employs Qwen3 as the solution model, without exploring other representative LLMs such as Llama. Moreover, the experiments are restricted to Qwen3-1.7B, which is insufficient to demonstrate the generalizability of the proposed RL training data and methodology across different model architectures or scales.\n\n2. Lack of ablation on aggregation models and training datasets. The work relies solely on the Qwen3-1.7B (Thinking mode) model to construct training data from the DeepScaleR dataset. However, the paper does not clearly justify the choice of this specific model configuration or dataset, nor analyze their impact on aggregation performance. Ablation studies on the aggregation model and data composition are missing.\n\n3. Limited evaluation scope. The evaluation benchmarks focus exclusively on mathematical reasoning, overlooking other critical domains such as coding, instruction following, and general-purpose reasoning. This limits the conclusions regarding the method’s overall reasoning capabilities.\n\n4. Experimental setup concerns. The number of solution samples used for aggregation is not clearly justified. It remains unclear whether the chosen number is sufficient or representative to yield convincing conclusions.\n\n5. Limitations of using AceMath as the reward model. The paper lacks a detailed description of AceMath. Furthermore, in Tables 1 and 2, the performance of the Weighted Majority approach based on AceMath consistently underperforms the simple Majority Voting baseline, suggesting potential issues with the chosen reward model.\n\n6. Robustness issues in AggLM-1.7B. As shown in Figure 4 (AIME24 and AIME25), AggLM-1.7B exhibits decreased aggregation accuracy when the majority answer size is large. This indicates that the model may not be robust even within groups where the majority solution is correct.\n\n7. Unclear setup in Table 6. The experimental configuration in Table 6 lacks clarity—specifically, whether the fine-tuning method is SFT or RL. Additionally, the observed performance degradation of the “Additional Trained Solution Model” on the math dataset is not well explained."}, "questions": {"value": "1. Failure Analysis: Are there specific case studies illustrating the typical failure patterns of AggLM-1.7B during aggregation?\n\n2. Sensitivity to Data Mixing: How does the performance of AggLM vary when different levels of noise or mixing ratios are introduced into the training data?\n\n3. Comparison with Prompted Aggregation: Compared to Prompted Aggregation, what are the concrete advantages of AggLM-1.7B? Could the authors provide illustrative examples to clarify their strengths and the necessity of model-based training?\n\n4. Statistical Significance: Given the relatively small dataset size (30 examples per benchmark), do the authors report confidence intervals or statistical tests to support the claim of consistent and reliable improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ziJJY3WHDd", "forum": "f6mDQ7zl4t", "replyto": "f6mDQ7zl4t", "signatures": ["ICLR.cc/2026/Conference/Submission19534/Reviewer_ouoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19534/Reviewer_ouoR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904634446, "cdate": 1761904634446, "tmdate": 1762931422229, "mdate": 1762931422229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes test-time aggregation as a trainable reasoning skill: an aggregator LLM is fine-tuned with verifiable rewards to read a small set of candidate solutions, reconcile errors, and produce a corrected final answer. Beyond simple majority vote or reward-model selection, the approach focuses on two behaviors—selecting a correct minority candidate when it exists and synthesizing a new solution from partially correct traces—trained with a carefully balanced mixture of “hard” (majority wrong) and “easy” (majority right) sets. Evaluations on MathArena’s AIME24/25 and HMMT24/25 show consistent gains over majority voting and reward-model re‑ranking, strongest when the majority is small; the method generalizes across solution models (1.7B → 8B, thinking/non‑thinking) and exhibits favorable token efficiency at typical k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This proposed method has a clear and timely reframing of aggregation as a learned reasoning skill using verifiable rewards and a lightweight RL procedure. It has consistent improvements over majority voting and reward‑model selection across multiple math benchmarks, with the largest gains when the correct answer appears in minority modes.\n\n2. This proposed method has a practical and well‑motivated training mixture that balances hard and easy sets, and ablations indicate robustness within a useful range of ratios. The experiment has demonstrated generalization across solution model strengths and modes, improving results even when the aggregator is trained on distributions from a smaller model.\n\n3. This method has favorable token efficiency at common settings, as scaling curves suggest that aggregating a modest number of candidates can outperform majority voting with larger k."}, "weaknesses": {"value": "1. Order/duplication sensitivity: The aggregator sees a sequence of candidates; report whether permutation of input order or deduplication of near‑identical solutions changes performance.\n\n2. Metric clarity: The evaluation has a nonstandard “pass@1” definition that averages over four aggregated samples per set, which may hinder comparison to prior work using strict single‑sample pass@1.\n\n3. The experiment has no confidence intervals or variance estimates on 30‑item datasets, so several 2–4 point gaps are difficult to assess statistically. Considering that this method could be sensitive to candidate order, one could randomize the order and re-evaluate for multiple times to get the statistics (e.g., confidence intervals or standard errors)."}, "questions": {"value": "1. The compute analysis has incomplete accounting because input token costs (prefilling) for concatenated candidates and the cost of generating those candidates (decoding) are not included in efficiency comparisons. Can authors provide some numbers?\n\n2. Verifier sensitivity: How sensitive are results to Math‑Verify configuration? Any cases where it mis‑evaluated numerically equivalent forms? A small audited subset (human‑verified) would be reassuring."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uPcHwjUiaQ", "forum": "f6mDQ7zl4t", "replyto": "f6mDQ7zl4t", "signatures": ["ICLR.cc/2026/Conference/Submission19534/Reviewer_Rp8o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19534/Reviewer_Rp8o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153831729, "cdate": 1762153831729, "tmdate": 1762931421801, "mdate": 1762931421801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach to aggregate multiple outputs from an LLM to a final answer. Instead of using simple rules as in best-of-N or Self-consistency, an LLM is fine tuned by RLVR to read all generated outputs and write a new solution. With careful balancing of training data, the new approach outperforms the simple rule-based ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I like the idea of the paper to learn the aggregation step. With more and more aggregation algorithms being introduced recently, this might be a path to a more optimal solution. \n\nThe paper is written well and describes the details of the approach very well. \n\nReasonable ablations is done and nice insights are provided such as the comparison for each difficulty class.\n\nShowing a small aggregation model can aggregate the outputs of a larger model was nice."}, "weaknesses": {"value": "My main concern about the paper is that it has really limited (or perhaps no) discussions on the limitations of the approach. With such a drastic difference to traditional approaches, it is very useful to study the differences and limitations. \n\nMy suggestion is to challenge the method more. The most pressing question is how far can the number of aggregated outputs be increased. The paper provides results for up to 16 but I suspect there is ceiling. Presenting this ceiling is very valuable. \n\nAlso, I am curious to see how much the model generalizes to other topics. For example Chemistry questions might still work as they benefit from many steps and reasoning, but common sense may not. \n\nIt would also be nice to try this approach in open ended problems, for example code and writing. I wonder if composing different solutions into a coherent one is more difficult in such domains."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rZMpz291VE", "forum": "f6mDQ7zl4t", "replyto": "f6mDQ7zl4t", "signatures": ["ICLR.cc/2026/Conference/Submission19534/Reviewer_BrS5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19534/Reviewer_BrS5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762335605338, "cdate": 1762335605338, "tmdate": 1762931421451, "mdate": 1762931421451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}