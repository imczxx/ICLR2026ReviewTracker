{"id": "gDxJK8yvZU", "number": 18087, "cdate": 1758283700355, "mdate": 1759897134442, "content": {"title": "Minimax-Optimal Aggregation for Density Ratio Estimation", "abstract": "Density ratio estimation (DRE) is fundamental in machine learning and statistics, with applications in domain adaptation and two-sample testing. However, DRE methods are highly sensitive to hyperparameter selection, with suboptimal choices often resulting in poor convergence rates and empirical performance. To address this issue, we propose a novel model aggregation algorithm for DRE that trains multiple models with different hyperparameter settings and aggregates them. Our aggregation provably achieves minimax-optimal error convergence without requiring prior knowledge of the smoothness of the unknown density ratio. Our method surpasses cross-validation-based model selection and model averaging baselines for DRE on standard benchmarks for DRE and large-scale domain adaptation tasks, setting a new state of the art on image and text data.", "tldr": "An aggregation algorithm for minimax-optimal hyperparameter optimization in density ratio estimation.", "keywords": ["Density Ratio Estimation", "Self-Concordance", "Kernel Methods", "Domain Adaptation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ecfa61de9bcf4dc5c127ab8ecfac84fb701a8cf.pdf", "supplementary_material": "/attachment/f3bade89a49dea4c27dbc11d20f5fa3b3dd215b0.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides an aggregation method for solving density ratio estimation (DRE). Through this, it tackles both the issue of method selection as well as hyperparameter tuning by instead training multiple models with multiple hyperparameter setups and aggregating them instead of just selecting one. The authors provide minimax-optimal error bounds without smoothness assumptions for RKHS and convex settings. Thorough empirical validation of the method against cross-validation baselines is done on models beyond just RKHS settings including neural networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Proposes a novel and effective strategy for dealing with model and hyperparameter aggregation for density ratio estimation.\n* The proposed closed form solution beats every candidate model instead of just selecting a single one.\n* Theoretical minimax guarantees for RKHS and convex settings.\n* Empirical results also show significant improvements over existing selection baselines in different data settings and using models like neural networks."}, "weaknesses": {"value": "* It is unclear if the theory covers all the empirical settings like the case where models are neural networks, though this is a tough ask.\n* The method requires empirical computation of a large Gram matrix of size ~ number of models X hyperparameter settings. This could be numerically unstable for various real world applications."}, "questions": {"value": "As per the weakness section,\n\n1. Does the theory extend to all your empirical evaluations?\n2. Could the authors comment briefly on the numerical stability of computing the Gram matrix in real applications, and also comment on the costs of computing Hessians in Alg 1?\n3. Are there guarantees for the aggregated DRE to always be non-negative?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X5XPhbGbSk", "forum": "gDxJK8yvZU", "replyto": "gDxJK8yvZU", "signatures": ["ICLR.cc/2026/Conference/Submission18087/Reviewer_aJPm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18087/Reviewer_aJPm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590566492, "cdate": 1761590566492, "tmdate": 1762927862445, "mdate": 1762927862445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for training multiple models under different hyperparameter configurations and optimally aggregating their outputs. The proposed approach demonstrates the ability to achieve minimax-optimal convergence rates without requiring any prior knowledge about the smoothness of the density ratios. Experimental results also demons\ttrate that the proposed method outperforms cross-validation, Bayesian model averaging, and Super Learner."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The method achieves minimax-optimal convergence rates under reasonable assumptions.\n* The proposed algorithm is theoretically guaranteed and practically computable.  The proposed algorithm is a simple least-squares method that is intuitively easy to understand.\n* Through various density ratio estimation methods, they demonstrate the superiority of the proposed approach over the baseline methods."}, "weaknesses": {"value": "* The following descriptions of the experiments are insufficient.\n  * The tuning method for the hyperparameter, lambda\n  * The details of BMA:  It is better to write which algorithm in Fragoso et al. (2018) is used.\n\n* Since cross-validation requires only inference from a single selected density ratio estimator during inference, it has the computational advantage of requiring less processing time than the proposed method. Therefore, I consider the superiority of the proposed method to be limited to only certain applications.\n\n* Using $f$ both for DRE models in equation (1) and for DA models in equation (13).   The use of the same symbol for the different functions may confuse readers."}, "questions": {"value": "* How did you choose the regularization parameter value, lambda?  \n* Could you discuss how the lambda selection affected the computational complexity in Fig. 2? In my understanding, the trained models and empirical Hessians can be shared across different lambda values, so there is little impact on overall training time. When you tried 10 different lambda values, $\\{10^{−6},10^{−5},...,10^{4}\\}$ in the experiments, how much did the whole training time increase compared with the single lambda value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y6mIM8WcMa", "forum": "gDxJK8yvZU", "replyto": "gDxJK8yvZU", "signatures": ["ICLR.cc/2026/Conference/Submission18087/Reviewer_HR3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18087/Reviewer_HR3j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642016748, "cdate": 1761642016748, "tmdate": 1762990395659, "mdate": 1762990395659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper find suitable weights that is used to aggregate results from multiple models trained with different hyperparameter to predict density ratio estimation(DRE). The weights are obtained by minimizing the upper bound on the Bregman divergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. There is a strong mathemtical proof\n2. There a many benchmarks across various task and domains"}, "weaknesses": {"value": "1. The paper could provide analysis on the number of models trained and results obtained through this method?\n2. The paper could also show analysis of the distribution of the hyper-parameters used."}, "questions": {"value": "1. Does the variance of the models output affect the bounds?\n2. How would noise within the data affect the results analytical?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ll3TtHOBAL", "forum": "gDxJK8yvZU", "replyto": "gDxJK8yvZU", "signatures": ["ICLR.cc/2026/Conference/Submission18087/Reviewer_96xS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18087/Reviewer_96xS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913810340, "cdate": 1761913810340, "tmdate": 1762927861577, "mdate": 1762927861577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a new algorithm for Density Ratio Estimation (DRE) that aggregates multiple models with different hyper parameter settings. The empirical experiments focus on domain adaptation, as well as comparisons with cross validation for several different loss objectives. The authors prove that the aggregation achieves minimax-optimal error convergence under very mild conditions on the density ratio."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall I think this is a good paper. It's written well and proposes a novel method for DRE estimation that is more computationally efficient than simple cross validation, and improves performance over several other baselines, on several relevant tasks. \nThe method is supported by a provably minimax-optimal error convergence. \nSeveral ablations are done to show the robustness of the method."}, "weaknesses": {"value": "Slightly more detail about the dataset for the top part of Table 1 would have been useful. What does \"c3,d1.70\" stand for instance, and how many samples were used in the dataset \"c3,d1.70\". \n\nMinor:\nI think the left part of figure two can be improved. If my understanding is correct, the number of models needed is reduced by a factor of the number of k-folds. By plotting it out instead of just stating the mechanical relationship, people may get confused and look for some patterns in the figure."}, "questions": {"value": "Can the method also be applied when instead of having different hyperparameters in the same objective, we have multiple models from different objectives (KuLSIF, Exp, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "v4Rf2Yo8X6", "forum": "gDxJK8yvZU", "replyto": "gDxJK8yvZU", "signatures": ["ICLR.cc/2026/Conference/Submission18087/Reviewer_kUnW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18087/Reviewer_kUnW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944861831, "cdate": 1761944861831, "tmdate": 1762927861142, "mdate": 1762927861142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}