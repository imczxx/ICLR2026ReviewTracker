{"id": "FHFDCsB9UC", "number": 21483, "cdate": 1758318055789, "mdate": 1759896919594, "content": {"title": "Flowing Through States: Neural ODE Regularization for Reinforcement Learning", "abstract": "Neural networks applied to sequential decision-making tasks typically rely on latent representations of environment states. While environment dynamics dictate how semantic states evolve, the corresponding latent transitions are usually left implicit, leaving room for misalignment between the two. We propose to model latent dynamics explicitly by drawing an analogy between Markov decision process (MDP) trajectories and ordinary differential equation (ODE) flows: in both cases, the current state fully determines its successors. Building on this view, we introduce a neural ODE–based regularization method that enforces latent embeddings to follow consistent ODE flows, thereby aligning representation learning with environment dynamics. Although broadly applicable to deep learning agents, we demonstrate its effectiveness in reinforcement learning by integrating it into an Actor–Critic algorithm, where it results in major performance gains across various standard Atari benchmarks.", "tldr": "We regularize reinforcement learning by modeling latent state transitions in MDPs as neural ODE flows, leading to improved stability and performance across standard benchmarks.", "keywords": ["Neural ODE", "Reinforcement Learning", "MDP", "Regularization", "Actor-Critic", "A2C"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35585686c86c2d7429dc49eec8b2709bb89119f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work proposes a new regularization technique to facilitate learning of useful representations for reinforcement learning based on neural ODEs. Motivated by trajectories within MDPs to follow the dynamics of the transition function, the authors argue that latent representations learned by the agent should try to capture such dynamics. In addition to a encoder network that encodes states into a latent space, but might not capture the structure of the MDP transitions, they propose to learn a neural ODE model that captures the flow of latent embeddings throughout time. To encourage the latent space to follow the flow of the ODE model, they introduce an additional regularization loss term that minimizes the difference between the embeddings obtained by both models. The proposed FlowReg approach is evaluated on top of A2C in 10 Atari game environments and shown to improve sample efficiency. Additional analyzes shows the effect of different components of the ODE model, including time sampling approaches, update frequency, and shows that the latent space is smoother compared to without the regularization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The discussed problem of learning representations for sequential decision making that smoothly capture the dynamics of the environment is highly relevant and important. To the best of my knowledge, the application of neural ODEs to learn and regularize the flow of latent states is novel and original. The method is largely clearly defined in Section 4 and is conceptually fairly simple which I appreciate.\n\nExperiments in 10 Atari environments show that FlowReg consistently leads to improved performance with gains varying between tasks. There appears to be no task in which FlowReg harms performance which is encouraging. Additional ablations/ experiments demonstrate the robustness of the approach to different configurations and quantitatively analyzes the learned latent space in Table 3.\n\nOverall, I would consider this paper a largely well executed work, albeit with potentially limited significance and a lack of contextualization within related literature, as per weaknesses below."}, "weaknesses": {"value": "In its current form, I am afraid that the work is not sufficient to justify acceptance at a venue like ICLR. Below, I try to give concrete weaknesses and highlight any weaknesses that I see as critical / major with (**Major**). I'd expect these to be addressed for this work to be considered for acceptance. \n\n## Originality & Significance\nAs mentioned above, I consider the combination of neural ODEs to model flow of latent states in MDPs as a regularization technique original and interesting. However, I am not convinced that it is a significant contribution to the field.\n\nIn its current form, this work does not at all acknowledge, discuss, or compare to alternative approaches of shaping representations for reinforcement learning. This is a rich literature space that I would highly suggest the authors to review and discuss in detail within their work in order to establish any potential unique features that their approach might offer. Almost all of these approaches consider the structure of a MDP in some form (temporal proximity, transition dynamics, ...) to shape representations that facilitate more efficient learning. The following might provide some starting points for such a review but is really just the tip of the iceberg:\n- Approaches that learn representations via constrastive learning\n\t- Uses contrastive learning to learn representations in which observations and noise-injected / augmented observations are close in latent space: Laskin, Michael, Aravind Srinivas, and Pieter Abbeel. \"Curl: Contrastive unsupervised representations for reinforcement learning.\" In _International conference on machine learning_, pp. 5639-5650. PMLR, 2020.\n\t- Use contrastive learning to learn similar representations for states and actions that are close in time: Zheng, Ruijie, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daumé III, and Furong Huang. \"TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning.\" _Advances in Neural Information Processing Systems_ 36 (2023): 48203-48225.\n- Approaches that learn representations leveraging temporal structure of transitions via predicting the future.\n\t- Schwarzer, Max, Ankesh Anand, Rishab Goel, R. Devon Hjelm, Aaron Courville, and Philip Bachman. \"Data-efficient reinforcement learning with self-predictive representations.\" In _International conference on learning representations_, 2021.\n\t- McInroe, Trevor, Lukas Schäfer, and Stefano V. Albrecht. \"Multi-horizon representations with hierarchical forward models for reinforcement learning.\" In _Transactions on Machine Learning Research_, 2024.\n- Approaches that learn similar representations for states that share certain properties according to bisimulation metrics\n\t- Agarwal, Rishabh, Marlos C. Machado, Pablo Samuel Castro, and Marc G. Bellemare. \"Contrastive behavioral similarity embeddings for generalization in reinforcement learning.\" In _International conference on learning representations_, 2021.\n\t- Castro, Pablo Samuel, Tyler Kastner, Prakash Panangaden, and Mark Rowland. \"MICo: Improved representations via sampling-based state similarity for Markov decision processes.\" _Advances in Neural Information Processing Systems_ 34 (2021): 30113-30126.\n- Approaches that leverage inverse dynamics models to learn representations for decision making\n\t- Pathak, Deepak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. \"Curiosity-driven exploration by self-supervised prediction.\" In _International conference on machine learning_, pp. 2778-2787. PMLR, 2017.\n- ...\n\nThere are three main questions/ expectations within this that I'd all consider **major** weaknesses that need addressing:\n1. Given all these works in some form are motivated by the same question as this work -- how can we leverage the structure of the sequential problem of a MDP to learn better representations for decision making -- I would expect a detailed discussion that makes any differences in learned representations or assumptions clear between this work and prior work.\n2. Does FlowReg have any unique benefits over these alternatives? \n3. To establish any significance of this work in the context of the prior literature, I would expect comparisons of the effects and benefits of FlowReg with alternative recent representation learning approaches. And just to be clear, I am not stating that this method needs to be state-of-the-art on some benchmark to warrant publication, but given the similarity in objectives, there should be an empirical comparison to better understand potential differences.\n\n## Methodology\n4. **Major:** The proposed ODE model defined in Eq. (9, 10) appears to be purely state and time conditioned. That is somewhat confusing to me as transitions to future states depend on the actions being taken by the agent. In that sense, the function that the neural ODE is trying to learn changes whenever the policy changes which is constantly happening throughout training. Is my understanding correct that the objective of the neural ODE is non-stationary as it changes with the current policy generating trajectories? If so, do you find this to be a problem for the learning process of the ODE? What does the ODE tend to learn?\n\n## Experiments\n5. **Major**: To further substantiate weakness 3. above, I believe that comparisons to existing auxiliary representation learning objectives would be valuable. The main benefits and claims of FlowReg appears to be (1) improved performance/ learning efficiency and (2) smoothness of the latent space. However, approaches such as temporal contrastive auxiliary losses or future-predictive auxiliary objectives tend to also learn a latent space in which embeddings that are close in time are also close in embedding space, resulting in a latent states that evolve smoothly in time. Is FlowReg different in its learned latent space from these alternatives? To answer this question, I would expect to see some results for recent representatives of such alternative approaches that compares their latent space to FlowReg either qualitatively using visualisations or quantitatively using metrics as presented in Table 3.\n6. **Major**: The metrics presented in Table 3 to evaluate the smoothness of the latent space appear to be potentially uninformative. While a smaller path length and displacement might indicate a smoothly evolving latent space throughout the trajectory, it is also possible that the learned latent space merely compressed embeddings into a smaller space while maintaining identical smoothness and structure. For example, one could scale a latent space with an arbitrary small positive factor and obtain a new latent space that exhibits significantly lower path length and displacement. However, I would argue this latent space would be no smoother than the unscaled previous latent space. Such shortcuts can easily be learned through the use of regularization techniques as Eq. (12) merely encourages to minimize the distance of embeddings. That could likely be achieved by scaling down the latent space. To convincingly show the smoothness of the latent space, it would be helpful to visualize PCA/ t-SNE/ UMAP projections of the trajectories within latent space and see whether they are indeed more smooth compared to baselines.\n7. The novel contribution of this work is its ODE regularization process. Connected to questions in Weakness 4. Is there any qualitative analysis that you could provide that sheds insight into what the ODE tends to learn?\n8. Figure 3 states to show \"overall relative performance gains\" but it is unclear what exactly is being reported. Would the authors be able to clarify what they compute for this Figure?\n9. What do lines and shading in Figure 2 correspond to? I would have typically expected them to correspond to mean and standard deviation or confidence intervals but given the noisy lines with small shading for example in DemonAttack and Qbert, I would doubt that is the case.\n10. As per Figure 3, relative performance gains of FlowReg differ quite substantially across environments. Do you have any insight into what makes FlowReg particularly effective in BeamRider and Qbert but less effective in Tennis? Are there any particular properties within the tasks with more significant gains that allow for the ODE model to be more precise?\n11. The authors state that they use the same hyperparameters for all environments and agents (Section 5). How were these hyperparameters determined?\n\n## Clarity\n12. The work frequently uses the terms of \"semantic space\", \"semantic trajectories\", and \"semantic observations\" in contrast to latent space and information. However, it is not clear to me what exactly these terms are meant to express. It would be helpful if these terms are properly introduced and/ or defined.\n13. Within the introduction, the authors state that their method \"combines the expressivity of continuous-time dynamics with the efficiency of conventional neural architectures\" but it is unclear to me why modelling continuous-time dynamics within MDPs is valuable given the MDP itself operates on discrete steps.\n14. Within the introduction, the authors states \"the structure properties it [embedding function] imposes on the latent space -- such as smoothness, consistency, and determinism -- are far from trivial and are crucial for reasoning tasks.\" It is unclear to me why it is crucial for the embedding function to impose determinism. Do the authors refer to the encoding from states to latent embeddings to be deterministic, or the transitions between states to be deterministic within the latent space? Any clarification and substantiation of this claim would be appreciated.\n15. Function $g$ in the unnumbered equation within the introduction appears undefined. What does $g$ correspond to?\n16. Most parameterised functions are written with parameters as a subscript but in Eq. (8), you write $f(..., \\phi)$ instead of $f_\\phi(...)$. Also, the right hand equation of Eq. (8) also write $f$ but without any notation to indicate its parameters."}, "questions": {"value": "1. What do you refer to with the term \"semantic space\" and how is it different from the latent space obtained from any encoder network? (Weakness 12)\n2. Function $g$ in the unnumbered equation within the introduction appears undefined. What does $g$ correspond to? (Weakness 15)\n3. How does FlowReg differ from existing auxiliary objectives to shape representations for sequential decision making (discussion of related literature above)? Does FlowReg have any unique advantages? (Weaknesses 1/2)\n4. The neural ODE process appears to be just conditioned on states and time. Is my understanding correct that the learning objective of this process is non-stationary since the flow of states throughout time depends on the policy -- that is learned and continually changes -- used to collect these trajectories? If so, do you find this to be a problem for the learning process of the ODE? (Weakness 4)\n5. Would you be able to show some qualitative analyzes and visualizations of the latent space learned by the ODE model and the latent space resulting from FlowReg? This might help to better understand what is being learned, and how it might be beneficial. (Weaknesses 5/6)\n6. How are the \"overall relative performance gains\" values reported in Figure 3 computed? (Weakness 8)\n7. What do lines and shading in Figure 2 correspond to? (Weakness 9)\n8. Do you have any insight into what makes FlowReg particularly effective in BeamRider and Qbert but less effective in Tennis? Are there any particular properties within the tasks with more significant gains that allow for the ODE model to be more precise? (Weakness 10)\n9. The authors state that they use the same hyperparameters for all environments and agents (Section 5). How were these hyperparameters determined? (Weakness 11)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bf169qAOi3", "forum": "FHFDCsB9UC", "replyto": "FHFDCsB9UC", "signatures": ["ICLR.cc/2026/Conference/Submission21483/Reviewer_KaXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21483/Reviewer_KaXh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327872181, "cdate": 1761327872181, "tmdate": 1762941799563, "mdate": 1762941799563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To align the representations in neural networks and environment dynamics in reinforcement learning, this paper proposes a new framework, FlowReg. It works by learning a neural ODE that acts as a latent surrogate for the environment and aligning its flows with the latent trajectories of the agent’s state embedder.  Experiments show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It is an interesting idea to align the representations in neural networks and environmental dynamics.  I'm unaware of other work in this direction in reinforcement learning.\n\n2. In addition, the experimental results of A2C-FlowReg are indeed better than the experimental results of A2C."}, "weaknesses": {"value": "1. This paper can be connected to Context Markov Decision Processes [1].\n2. There are many repeated subgraphs in Figures 2 and 4.\n3. The experiment is insufficient. The proposed FlowReg is only performed on one reinforcement learning algorithm, A2C.\n4. Lack of theoretical analysis. The author should theoretically prove that by the FlowReg framework, the latent transitions will better align with real state transitions.\n5. A2C does not need to be introduced in detail as it is not directly related to the new method in Section 4.\n6. The author should provide intuitive examples, theoretical analysis, and more metrics to explain why the return of existing algorithms can be improved by FlowReg.\n\n[1] Hallak, Assaf, Dotan Di Castro, and Shie Mannor. \"Contextual Markov decision processes.\" arXiv preprint arXiv:1502.02259 (2015)."}, "questions": {"value": "1. Why should global (trajectory-level) aspects be considered in MDPs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "beoFepLUij", "forum": "FHFDCsB9UC", "replyto": "FHFDCsB9UC", "signatures": ["ICLR.cc/2026/Conference/Submission21483/Reviewer_4aQj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21483/Reviewer_4aQj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761416657237, "cdate": 1761416657237, "tmdate": 1762941799207, "mdate": 1762941799207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FlowReg, a neural ODE–based regularization technique for reinforcement learning that enforces smooth and dynamically consistent latent representations of environment states. The key idea is to treat Markov decision process (MDP) trajectories as discretized samples from continuous ODE flows and to align the latent embeddings of a policy’s state encoder with trajectories generated by a learned neural ODE. This regularizer introduces global structural constraints in the latent space without using the ODE for inference. Implemented on top of A2C, FlowReg improves performance across ten Atari environments, producing smoother latent trajectories and more stable training while adding only moderate computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple and general mechanism. FlowReg can be applied to most latent-state architectures with minimal modification—only adding an auxiliary ODE and an alignment loss—making it broadly usable beyond RL.\n\n- Empirical consistency. The method improves A2C on all tested Atari benchmarks and yields interpretable geometric effects (e.g., reduced latent path length and curvature).\n\n- Strong experimental hygiene. Results are averaged across runs, include multiple time-sampling and update-frequency settings, and discuss trade-offs in stability and runtime."}, "weaknesses": {"value": "- No theoretical justification. The link between ODE smoothness and policy improvement is argued intuitively but not proven; there is no analysis of convergence, variance reduction, or representational bias.\n\n- Hyperparameter sensitivity. FlowReg introduces new parameters (update frequency, λ, time sampling) yet the paper gives limited guidance on tuning or robustness.\n\n- Computational overhead unquantified. The paper notes runtime remains “comparable” but lacks actual wall-clock comparisons; ODE solvers can be nontrivial in cost."}, "questions": {"value": "- Can you provide runtime or FLOP comparisons to the A2C baseline to clarify FlowReg’s efficiency?\n\n- Could the alignment loss collapse diversity in latent representations, harming exploration?\n\n- Does FlowReg interact with representation learning methods like contrastive or predictive coding?\n\n- What happens if the ODE model is underparameterized or unstable—does it bias the policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iKkBJdtClk", "forum": "FHFDCsB9UC", "replyto": "FHFDCsB9UC", "signatures": ["ICLR.cc/2026/Conference/Submission21483/Reviewer_Z2UD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21483/Reviewer_Z2UD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741061598, "cdate": 1761741061598, "tmdate": 1762941798979, "mdate": 1762941798979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a regularization strategy for deep reinforcement learning that incentivizes latent state trajectories to reflect the semantic path of observations in the environment. This idea is implemented by introducing a neural ordinary differential equation (ODE) that is trained to reflect the behavior of the environment, and then comparing the ODE flows to the latent flows and minimizing the difference between their embeddings. The paper includes experiments on 10 Atari games, providing evidence that the flow-regularized model improves over the baseline non-regularized model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper was a joy to read. What a cool idea. I was genuinely excited to learn something new. Extremely clear, and very interesting idea.\n\nThe problem that it points to in the introduction---states that are near each other in time ought to be near each other in latent space---is one that I have experienced firsthand. Until now, all the solutions to this problem that I had encountered (or tried myself) felt unsatisfying. This solution feels absolutely beautiful."}, "weaknesses": {"value": "My main concerns come from the experimental validation.\n\nThe paper notes, \"We performed 5 independent runs for every RL agent\", and that \"we experiment with [different hyperparameter settings] and take the best configuration.\"\n\nRunning only 5 seeds is rather low. The results are likely significant despite having few seeds, because there are 8-10 environments and FlowReg performed better across the board. However, 10-20 seeds per environment would be better.\n\nMeanwhile, I'm wondering how the seeds fit into the experimentation process here. By \"and take the best configuration\"... does this mean you then _re-run_ the best configuration with _entirely new seeds_? (I hope so!) Or is this effectively taking the max over hyperparameter configurations without re-running on new seeds? If it's the latter, FlowReg could be winning solely due to selection bias and having more attempts (due to having more hyperparameter configurations) rather than because that particular hyperparameter configuration is actually better.\n\nI am prepared to increase my score if I can gain more confidence in the experimental validation."}, "questions": {"value": "1. Can you expand on Limitations a bit more? I was having trouble following the second half of that paragraph, but it feels important. I would love more detail. Felt similar confusion about the second half of the \"Neural ODEs as continuous-depth networks\" paragraph. These parts were confusing.\n\n2. I also got a bit lost in lines 246--252.\n\n3. What's going on with MsPacman and Tennis? And why aren't those learning curves in the main text? They feel a bit buried in the appendix.\n\n4. \"We find that _Exp-Decay_ outperforms _Index_ more often than otherwise.\" What time sampling strategy do you use for Figs 2, 3, 4? Is it an environment-dependent mix or the single setting that performs best well across all environments?\n\n5. $\\lambda$ is an additional hyperparameter not mentioned in line 360; how is it selected?\n\n6. Atari environments are discrete, but they aren't _that_ discrete. What happens if you do this on something like a visual gridworld?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wJQDyKA5UG", "forum": "FHFDCsB9UC", "replyto": "FHFDCsB9UC", "signatures": ["ICLR.cc/2026/Conference/Submission21483/Reviewer_RVQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21483/Reviewer_RVQ7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762414907206, "cdate": 1762414907206, "tmdate": 1762941798750, "mdate": 1762941798750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}