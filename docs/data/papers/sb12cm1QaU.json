{"id": "sb12cm1QaU", "number": 6698, "cdate": 1757992662025, "mdate": 1759897900179, "content": {"title": "Elevating Large Language Models with Module-of-Thought", "abstract": "Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Module-of-Thought Coder (MoTCoder). We introduce a framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial pass@1 improvements of 5.8% on APPS and 5.9% on CodeContests. MoTCoder also achieved significant improvements in self-correction capabilities, surpassing the current SOTA by 3.3%. Additionally, we provide an analysis of between problem complexity and optimal module decomposition and evaluate the maintainability index, confirming that the code generated by MoTCoder is easier to understand and modify, which can be beneficial for long-term code maintenance and evolution. Our code will be released.", "tldr": "", "keywords": ["Large Language Models", "coding", "programming tasks", "supervised finetuning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3175667a0e0a83d86c76c553cc7e04fd04146188.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the performance degradation of LLMs when tackling complex programming tasks. The authors observe that conventional models tend to generate monolithic code blocks, which differs from the modular approach of experienced developers who decompose problems into subtasks. To tackle this limitation, the paper proposes Module-of-Thought Coder (MoTCoder), a model trained through a novel instruction tuning framework called the Module-of-Thought instruction tuning framework.\n\nThe core of this method is a two-step MoT code instruction conversion process: First, the instruction guides the model to outline the necessary submodules, generating only their function headers and docstrings describing the expected purpose; second, subsequent instructions guide the model to implement these submodules and then combine them into the final solution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper's core thesis—that LLMs should solve complex coding problems in a modular way, like human experts—is highly well-motivated. Translating this insight into a novel instruction tuning data format is a major contribution of this work.\n\n2.  The experimental results are impressive. The gains on difficult benchmarks like APPS and CodeContests over a very strong baseline are substantial. The improvement in self-correction capability is also significant.\n\n3.  I'm happy that the authors went beyond standard pass@k metrics. The analysis in Figure 3 regarding the number of modules vs. problem difficulty, time and memory consumption, and maintainability index provides valuable insights into why the MoT approach is effective and its broader benefits.\n\n4.  The MoT instruction conversion is clearly defined as a two-step process. This structured data format is a novel contribution that moves beyond standard CoT or PoT prompting techniques, internalizing them into the model parameters."}, "weaknesses": {"value": "1.  The paper mentions that other iterative methods introduce extra inference cost. The authors claim MoT saves memory during execution, but they do not discuss its inference cost. Generating MoT-style code will almost certainly result in more tokens than generating a monolithic block. What is the impact of this increased token generation on inference latency and computational cost?\n2.  While the MoT instruction tuning is novel, the idea of solving problems through modularization has been explored in the literature. The authors claim MoT is more intrinsic because it is achieved via training, but this definition applies to all instruction tuning methods. The paper should more clearly distinguish its contribution from existing modular generation and step-by-step reasoning techniques."}, "questions": {"value": "Please refer the weaknesses section :-)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DLkUUChJjJ", "forum": "sb12cm1QaU", "replyto": "sb12cm1QaU", "signatures": ["ICLR.cc/2026/Conference/Submission6698/Reviewer_ceL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6698/Reviewer_ceL7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569243001, "cdate": 1761569243001, "tmdate": 1762918991104, "mdate": 1762918991104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-step “Module-of-Thought” pipeline for the code generation task. First, generate function headers and docstrings, then implement modules and compose the final solution. Experiments report gains on APPS and CodeContests. It also includes ablation analyses, such as self-correction, maintainability, and time/memory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear, practical motivation to encourage modular solutions on complex coding tasks.\n2.\tImprovements over the base model are shown on two competitive benchmarks."}, "weaknesses": {"value": "1.\tMissing strong and directly comparable baselines. A fair evaluation should include techniques relevant to code generation rather than only comparing against vanilla code LLMs. Such baselines are absent here [1][2][3].  \n2.\tLimited novelty. The general plan-then-code decomposition pattern is also already common in recent decomposition-style workflows, reducing conceptual originality relative to prior art that structures code generation into stages [1][3][4].\n3.\tIncomplete ablations. There are no ablation studies of different modules, such as headers-only, headers+docstrings, and different reasoning strategies (e.g., CoT, few-shot). Additionally, in Table 1, the “normal finetuning” baseline appears under-curated, even performing worse than no-tuning LLMs without any analysis, making the contrast favorable to MoT.\n4.\tWriting can be improved. Several typos and inconsistencies (e.g., “fune-tuning”) detract from clarity. \n\n[1] CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. ICLR 2024.\n[2] EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning. ICML 2025.\n[3] Planning in Natural Language Improves LLM Search for Code Generation. ICLR 2025.\n[4] Self-planning code generation with large language models. TOSEM 2024."}, "questions": {"value": "1.\tSee above.\n2.\tWhat is the performance of increasing the number of samples on pass@k?\n3.\tHow sensitive are gains to the quality of synthetic data from GPT-4o?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xBgRKxFYbK", "forum": "sb12cm1QaU", "replyto": "sb12cm1QaU", "signatures": ["ICLR.cc/2026/Conference/Submission6698/Reviewer_ARsq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6698/Reviewer_ARsq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726344215, "cdate": 1761726344215, "tmdate": 1762918990474, "mdate": 1762918990474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In order to encourage models to generate code in a modular style (instead of monolithic code) authors introduce an instruction tuning stage with the training data being modular code generated by GPT4o for tasks in the APPS and CodeContest training data. Training on this data improves performance on the APPS and CodeContest datasets by around 5%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Modular code generation is an important & relevant problem to solve.\nTheir instruction tuned solution provides some performance boost on complex benchmarks like APPS and CodeContest."}, "weaknesses": {"value": "- While it is an interesting problem and there have been existing methods targeting the same problem, There is not much novelty in this paper’s approach. It is simply using LLM generated  modular style codes as instruction tuning data. For example past works like CodeChain achieve the same thing (generating modular code) at test-time (without any training). \n\n- There are indeed some improvements in the performance on these benchmarks. This has been also observed in older works like CodeChain (again without any training) and it is also expected that this Instruction Tuning might perform better than the vanilla (monolithic code) Instruction Tuning. But I am more concerned whether this Instruction Tuning can achieve generalization. \n\n- Making this an Instruction Tuning problem seems to be making this quite narrow. Because this is a general design of code generation that the model has to learn, limiting it to Instruction Tuning stage can bias it or limit it to the scale & nature of the training data (especially since this kind of training data with modular code decomposition might be lesser available). In such a case it is important to specifically showcase the generalizability of this approach. Rather in the method adopted by the authors since the training data is taken from the training splits of the same datasets (APPS and CodeContest) might be similar in nature of the test data. According to me this setting does not showcase generalizability of this approach. If authors can show on OOD Test set, then this will make it more convincing. \n\n- Table 2 does not show a comparable setup, since almost all the models are quite old. For example. CodeChain+WizardCoder (15B) results are provided, but CodeChai+GPT3.5 achieves (54.50 / 28.11/ 12.38 (Overall 30.24) and with CodeT filtering (62.72/ 32.96/ 15.08 (Overall 35.34)). Note that GPT3.5 itself is a much weaker model (48.00/19.42/5.42 Overall (22.33)) than Qwen2.5-Coder-Instruct 7B (50.58 30.32 19.49 Overall (32.21)). In this case I would think CodeChain with a weaker backbone is achieving 30% (+8% over the vanilla backbone model) while MOT with Qwen2.5-Coder-Instruct 7B is achieving only +2% over the backbone Qwen model.\n\n- While things won’t be exactly comparable because of different backbones,  In terms of baselines, the most complete and comparable results need to be shown otherwise it is misleading readers. \n\n- Also The latest numbers on these benchmarks with the frontier models should be provided, to understand where this work stands in comparison."}, "questions": {"value": "See weakness section\n\nAlso, though not a requirement at this stage: \n- While APPS, CodeContest are still good benchmarks, the attention of the community has shifted to more complex benchmarks like LiveCodeBench, BigCodeBench or SWEBench for repository level coding. It would be good to understand whether the MOT model generalizes to these, which will also address my above concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RISJK0qhhO", "forum": "sb12cm1QaU", "replyto": "sb12cm1QaU", "signatures": ["ICLR.cc/2026/Conference/Submission6698/Reviewer_r47Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6698/Reviewer_r47Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883106625, "cdate": 1761883106625, "tmdate": 1762918989743, "mdate": 1762918989743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Module-of-Thought Coder (MoTCoder), which teaches coding LLMs to first outline sub-modules (function headers + docstrings) and then implement and integrate them into a full solution (“MoT Instruction Transformation”). Using GPT-4o to create a large training set (≈183K “clean” + 174K MoT examples) and instruction-tuning Qwen2.5-Coder (7B/32B), the authors report substantial pass@1 gains on APPS (+5.8%) and CodeContests (+5.9%), stronger self-correction after multi-turn reflection, improved maintainability index, and slightly lower memory use at similar latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The two-step MoT prompt/finetune is simple to adopt and improves pass@1 on APPS and CodeContests for both 7B and 32B bases; ablations and difficulty-wise analyses support the central claim.\n\n* The paper analyzes module count vs. accuracy, time/memory, and maintainability index, arguing MoT yields more readable, maintainable code—useful for real development workflows."}, "weaknesses": {"value": "*  While the paper mentions dedup to avoid APPS/CodeContests overlap, details are light, and GPT-4o-created code may echo public solutions—raising concerns about contamination, reproducibility, and evaluation fairness. \n\n* Results focus on competitive-programming style tasks; it’s unclear how MoT generalizes to multi-file repos, different languages/runtimes, library/API usage, or long-horizon editing—all common failure modes for coding LLMs.\n\n* There’s little on variance across seeds, statistical significance, or training/inference cost (GPU hours, token budgets) for MoT data creation and finetuning—key for practitioners deciding on adoption. \n\n* Beyond “normal vs. MoT,” the work could compare against strong plan-then-code baselines (e.g., least-to-most, PoT/PAL variants with tuned prompts) and post-hoc repair agents, under matched compute."}, "questions": {"value": "* How does MoTCoder perform on multi-file, dependency-rich repos (tests, build systems, I/O), or bug-fix/refactor tasks compared to pass@k benchmarks?\n\n* How do you mitigate plagiarism, license conflicts, or insecure patterns in GPT-4o-generated modules? Any static/dynamic safety scans? \n\n* Do you have a heuristic or learned controller for optimal module granularity? The figure hints at benefits on hard tasks—can you predict the right number of functions from problem features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "96Ppy08mpD", "forum": "sb12cm1QaU", "replyto": "sb12cm1QaU", "signatures": ["ICLR.cc/2026/Conference/Submission6698/Reviewer_UJSi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6698/Reviewer_UJSi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762082834488, "cdate": 1762082834488, "tmdate": 1762918989342, "mdate": 1762918989342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}