{"id": "VRFbLr8Uhv", "number": 12262, "cdate": 1758206721515, "mdate": 1759897521922, "content": {"title": "Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping", "abstract": "Differential privacy (DP) has become an essential framework for privacy-preserving machine learning. Existing DP learning methods, however, often have disparate impacts on model predictions, e.g., for minority groups. Gradient clipping, which is often used in DP learning, can suppress larger gradients from challenging samples. We show that this problem is amplified by adaptive clipping, which will often shrink the clipping bound to tiny values to match a well-fitting majority, while significantly reducing the accuracy for others. We propose bounded adaptive clipping, which introduces a tunable lower bound to prevent excessive gradient suppression. Our method improves worst-class accuracy by over 10 percentage points on Skewed and Fashion MNIST compared to unbounded adaptive clipping, 7 points compared to Automatic clipping, and 5 points compared to constant clipping.", "tldr": "We propose bounded adaptive clipping to address fairness issues in differentially private deep learning.", "keywords": ["Differential Privacy", "Machine Learning", "Fairness", "Adaptive Clipping"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0edf8845d6ecc91c0d8ffbfe5788d6c1c6913333.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes lower-bounded adaptive clipping for differential privacy learning to address disparate impact of DP learning on minority and confusable groups. The method leads to improvement in worst-class accuracy for skewed and Fashion MNIST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work identifies common issue of disappearing clipping bounds of current works and tackles important problem of ensuring ML fairness for minority groups and shows improvement over SOTA.\n- Applied to 4 datasets (Skewed, Fashion MNIST, Adult, Dutch) and 3 architectures ResNet-18, CNN, Logistic Regression\n- Testing under Realistic Constraints (DP-HPO)"}, "weaknesses": {"value": "- the method adds additional hyperparameters to tune increasing the complexity of the training this is a weakness common to the family of methods\n- the paper could be strengthened by providing more explicit guidance or a low-cost heuristic for setting C_LB"}, "questions": {"value": "Hyperparameters (Target quantile γ=0.5, Multiplier τ=2.5, and learning rate ηC =0.2) across experiments are fixed, but tuning results in Table A1 show large STD (e.g., τ on Fashion MNIST is 2.7438±3.1248). Could the authors clarify the definition of \"stability\" used that justified fixing these parameters, despite the high variance observed in preliminary tuning results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "61BXhu6GZ2", "forum": "VRFbLr8Uhv", "replyto": "VRFbLr8Uhv", "signatures": ["ICLR.cc/2026/Conference/Submission12262/Reviewer_2YPx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12262/Reviewer_2YPx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955824169, "cdate": 1761955824169, "tmdate": 1762923202060, "mdate": 1762923202060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical issue at the intersection of Differential Privacy (DP) and fairness: the disparate impact of DP-trained models on different demographic groups, particularly minority or challenging subgroups. The core mechanism investigated is adaptive gradient clipping, which is standard practice in training with DP-SGD (Differentially Private Stochastic Gradient Descent)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The technical quality appears sound. The proposed BAC method is a direct, mathematically clear modification of existing techniques, making it easy to integrate. The experimental design is robust, contrasting BAC not only with standard DP-SGD but also with existing adaptive clipping schemes (like AUTO), providing a necessary control group to validate the contribution. The reported results clearly show performance gains on fairness metrics (e.g., disparate accuracy), suggesting the method effectively achieves its stated goal."}, "weaknesses": {"value": "- The primary weakness lies in the selection and motivation of the lower bound, $C_{\\text{min}}$. While the method's effectiveness hinges on this parameter, the paper does not provide sufficient theoretical guidance for its choice. Currently, $C_{\\text{min}}$ appears to be a manually tuned hyperparameter. This reduces the actionability of the insight. If $C_{\\text{min}}$ is set too high, it negates the benefits of adaptive clipping; if set too low, it fails to help the minority group. The paper needs a more rigorous study or a heuristic/theoretical justification for how to choose $C_{\\text{min}}$ relative to, for instance, the empirical gradient norm distribution of the minority group.\n- The paper positions its work as mitigating disparate impact. However, the experiments mainly compare BAC to DP-SGD variations (which are privacy-focused) rather than methods explicitly designed for fairness under DP, such as Group DP-SGD (which uses group-specific clipping thresholds or noise scales) or DP versions of re-weighting or adversarial debiasing. A weakness is the absence of a direct comparison showing how BAC's implicit fairness improvement compares to the explicit fairness control achieved by these alternative methods. Without this, the reader cannot fully assess BAC's place in the fairness-under-DP literature.\n- The assessment of disparate impact appears to focus predominantly on Disparate Accuracy (difference in accuracy between groups). In many real-world applications (like loan approval or recidivism prediction), metrics like Equal Opportunity Difference (difference in False Negative Rates, $FNR$) or Predictive Parity (difference in Positive Predictive Values, $PPV$) are often more critical. The experiments are insufficient without evaluating the impact of BAC on these other crucial fairness metrics, which could potentially reveal trade-offs not visible through accuracy alone."}, "questions": {"value": "- Can the authors elaborate on whether $C_{\\text{min}}$ can be justified or estimated without extensive hyperparameter search? For instance, could $C_{\\text{min}}$ be set to a small percentile (e.g., the $5^{\\text{th}}$ percentile) of the historical L2 gradient norms observed on the entire training set, or perhaps only on the minority/underperforming subgroup?\n- Does the enforcement of $C_{\\text{min}}$ affect the required noise level for a fixed privacy budget $\\epsilon$, compared to a standard (unbounded) adaptive clipping method? Intuitively, a bounded clip norm could stabilize the bound variance, but a formal discussion on the impact of BAC on the final noise scale and the $\\epsilon$ calculation is needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DaNyaA7gfd", "forum": "VRFbLr8Uhv", "replyto": "VRFbLr8Uhv", "signatures": ["ICLR.cc/2026/Conference/Submission12262/Reviewer_ow6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12262/Reviewer_ow6d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977756391, "cdate": 1761977756391, "tmdate": 1762923201737, "mdate": 1762923201737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies DP-SGD with adaptive clipping (similar to Andrew et al., 2021), which privately tracks a quantile of per-sample gradient norms and updates a global clipping bound so that roughly a target fraction of gradients are clipped. It identifies a failure mode for this method: as training progresses and most gradients shrink, the estimated proportion above the bound drops, the bound keeps shrinking, and can collapse toward zero. This disproportionately hurting minority or hard groups whose gradients remain larger.\n\nTo prevent this, the authors add a tunable lower bound $C_{LB}$ on the clipping bound (bounded adaptive clipping): when the adapted bound would fall below $C_{LB}$, they clip at $C_{LB}$ instead. Experiments on image (Fashion-MNIST, Skewed-MNIST) and tabular (Adult, Dutch) datasets show improved worst-class accuracy and competitive macro accuracy versus constant clipping, unbounded adaptive clipping, and AUTO (Bu et al., 2023). Because the proposed method introduces an extra hyperparameter, they also evaluate with DP-HPO and report similar or better performance under the accounted privacy budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a failure mode of earlier adaptive clipping methods and proposes a simple fix, with experiments demonstrating that it alleviates the issue. It's clearly written."}, "weaknesses": {"value": "- Theorem 3.2 does not provide a precise privacy guarantee. The privacy–accuracy trade-off would be much clearer if the authors specified the resulting $\\epsilon$ as an explicit function of $T$, the subsampling rate and the noise multipliers $\\sigma_{grad}, \\sigma_{count}$. In its current form, the guarantee is hard to interpret.\n\n- While the mean-estimation example is interesting, it seems specific. Is the failure primarily driven by the setup in which the minority group is strictly smaller than the majority? How general is the phenomenon beyond that specific data structure?\n\n- For image data, the “group” is defined by the class label, which isn’t a protected attribute, so the fairness interpretation is unclear."}, "questions": {"value": "- Theorem 3.2 seems to rely on Lemma 3.1, which assumes both Gaussian mechanisms have sensitivity 1. While counting has sensitivity 1, the private gradient mean (after averaging) does not. I assume the privacy amplification by subsampling can also complicates the results. How do you handle sensitivity for this? \n\n- How does proposed method perform when the size of the minority group is similar to that of the majority group? \n\n- How does the method perform in terms of other fairness metrics such as per-group FPR/TPR, gap between group accuracies, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aaEBxVigBN", "forum": "VRFbLr8Uhv", "replyto": "VRFbLr8Uhv", "signatures": ["ICLR.cc/2026/Conference/Submission12262/Reviewer_Lhr2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12262/Reviewer_Lhr2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762340565303, "cdate": 1762340565303, "tmdate": 1762923201407, "mdate": 1762923201407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical issue in differentially private (DP) learning: adaptive clipping methods can lead to vanishing clipping bounds, which disproportionately harm minority or challenging classes. The authors propose a simple yet effective solution—introducing a lower bound on the clipping threshold—and demonstrate its efficacy across multiple datasets and models. The work is well-motivated, methodologically sound, and thoroughly evaluated. It addresses an important problem at the intersection of privacy and fairness, with practical implications for real-world DP training. Experiments across MNIST, Adult and Dutch, show improved worst-class and subgroup accuracy, with competitive macro accuracy, under both optimal hyperparameters and DP-HPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis manuscript first identifies a well-identified problem, via a failure mode of existing adaptive clipping methods, where clipping bounds collapse during training, leading to unfair outcomes. The toy example in Figure 1 is particularly effective in illustrating this issue.\n2.\tAuthors propose a simple and effective Solution, i.e. DP-HPO. Its bounded adaptive clipping is easy to implement, requires minimal modification to existing DP-SGD pipelines, and comes with a clear privacy guarantee.\n3.\tThe privacy analysis is rigorous, leveraging Gaussian DP composition to account for both gradient and clipping-bound updates.\n4.\tThe paper provides extensive details on hyperparameters, datasets, and experimental setups."}, "weaknesses": {"value": "1.\tThe DP-HPO introduce a new hyperparameter i.e. the lower-bound of adaptive clipping bound C_LB. The paper shows robustness, but provides limited guidance on principled selection., this could be a practical barrier.\n2.\tThe paper has a limited theoretical analysis about fairness. While motivated by fairness, the paper does not provide a theoretical analysis of how bounded clipping improves fairness guarantees (e.g., in terms of fairness definitions like equalized odds or demographic parity).\n3.\tThe paper compares to AUTO and constant/unbounded clipping. It should provide comparisons with other fairness-oriented DP methods (e.g., DP-SGD-Fair by Xu et al., 2021, FairDP by Liu et al. 2022)."}, "questions": {"value": "1.\tCan the authors provide a simple theoretical intuition or bound on how the lower bound mitigates disparate impact?\n2.\tCould the authors provide a sensitivity analysis or a heuristic for setting C_LB?\n3.\tHave the authors considered evaluating other fairness metrics (e.g., demographic parity, equal opportunity) beyond accuracy parity?\n4.\tDP-HPO is proposed based on normalized DP-SGD (De et al., 2022). How does the proposed adaptive clipping bound C_LB working with SGD, affect the fairness? The related work shows DP-SGD has the fairness problem. But, Lemma 3.1 and Theorem3.2 are both about privacy.\n5.\tI admit that The proposed DP-HPO is a simple and effective method, but it seem a little incremental novelty, via introduce the C_LB."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wpDqzUvJg9", "forum": "VRFbLr8Uhv", "replyto": "VRFbLr8Uhv", "signatures": ["ICLR.cc/2026/Conference/Submission12262/Reviewer_ABd6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12262/Reviewer_ABd6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762603552172, "cdate": 1762603552172, "tmdate": 1762923200619, "mdate": 1762923200619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper diagnoses a failure mode in DP training. It claims that existing adaptive clipping methods cause disparate impact by shrinking the clipping bound to \"tiny values\" to accommodate the majority group. This, in turn, suppresses the larger gradients from minority or \"challenging\" samples, harming their performance.\nThe authors propose \"bounded adaptive clipping\" as a solution. This method is a minor modification that introduces a tunable hyperparameter, which acts as a floor, preventing the clipping bound from collapsing to zero. The paper shows this simple fix improves worst-class accuracy on skewed image datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on an important part of DP training.\n2. The paper clearly identifies and illustrates a failure mode for unbounded adaptive clipping, where the bound collapses and ignores minorities."}, "weaknesses": {"value": "1. The novel part of this paper is the max() function. This is a minor heuristic, not a new framework.\n2. Baseline is not well selected. Why pick the auto clipping? My understanding is that auto clipping is good for hyperparameter tuning since it does not require for clip bound. Why do you want to compare your proposed method with them? I think De et al.(https://arxiv.org/pdf/2204.13650) may be a good choice. They achieve good performance on many datasets. If your method plus theirs can achieve new SoTA results on CIFAR-10 or CIFAR-100 dataset will make your method more stronger.\n3. The datasets are toy datasets. I know for a DP paper, it may not be easy for training with ImageNet but at least use CIFAR-10/100.\n4. The improvements are not consistent.  Sometimes the proposed method is better than baseline for eps=1 and 4, sometimes it is better for eps=2. Could authors provide more explanation for this? Some improvements are limited."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OyjTH6tPJZ", "forum": "VRFbLr8Uhv", "replyto": "VRFbLr8Uhv", "signatures": ["ICLR.cc/2026/Conference/Submission12262/Reviewer_aMme"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12262/Reviewer_aMme"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762721791884, "cdate": 1762721791884, "tmdate": 1762923200030, "mdate": 1762923200030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}