{"id": "4jcnded6fA", "number": 2567, "cdate": 1757147635098, "mdate": 1759898140391, "content": {"title": "BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives", "abstract": "Non-prehensile manipulation, encompassing ungraspable actions such as pushing, poking, and pivoting, represents a critical yet underexplored domain in robotics due to its contact-rich and analytically intractable nature. In this work, we revisit this problem from two novel perspectives. First, we move beyond the usual single-arm setup and the strong assumption of favorable external dexterity such as walls, ramps, or edges. Instead, we advocate a generalizable dual-arm configuration and establish a suite of Bimanual Non-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the prevailing RL-based paradigm and propose a three-stage, RL-free framework to learn non-prehensile skills. Specifically, we begin by extracting bimanual hand motion trajectories from video demonstrations. Due to visual inaccuracies and morphological gaps, these coarse trajectories are difficult to transfer directly to robotic end-effectors. To address this, we propose a geometry-aware post-optimization algorithm that refines raw motions into executable manipulation primitives that conform to specific motion patterns. Beyond instance-level reproduction, we further enable category-level generalization by parameterizing the learned primitives with object-relevant geometric attributes, particularly size, resulting in adaptable and general parameterized manipulation primitives. We validate BiNoMaP across a range of representative bimanual tasks and diverse object categories, demonstrating its effectiveness, efficiency, and superior generalization capability.", "tldr": "We propose BiNoMaP, a dual-arm, RL-free framework that learns generalizable bimanual non-prehensile manipulation primitives from hand demonstrations, enabling robust instance- and category-level generalization across diverse tasks.", "keywords": ["Non-Prehensile Manipulation", "Bimanual Robotic Manipulation", "Motion Trajectory Optimization"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/daa56bb97df9dc02cebae15b0a1fc397b64eee76.pdf", "supplementary_material": "/attachment/c0d7c8606d492f23c5bdf9a42ee4456d256ae581.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a visuomotor method to perform several tasks of non-prehensile manipulation using a dual-arm robotic platform and a camera system able to reconstruct 3D motion. \n\nThe work employs human demonstration to collect human-inspired movement strategies to perform the given manipulation tasks, and then - by means of their proposed method - it transfers these movements to the robots showing satisfactory performance across various tasks both seen and unseen, with a good degree of generalization.\n\nThe paper is benchmarked against state-of-the-art methods showing superior performance. Moreover, the robustness of the proposed method is assessed by different ablation studies, showing a good degree of resistance to parameters' changes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is original in its way of using demonstrative manipulation data and robustly generalize them to robot movements beyond the initial dataset.\n2. The paper overall quality is high and can be appreciated by the level of depth in which each aspect of the proposed method has been evaluated.\n3. The paper is clearly presented and easy to follow.\n4. The paper most significant result is - in my opinion - the ability to generalize the learned movements beyond the initial training demonstrations."}, "weaknesses": {"value": "1. The idea of decomposing movements in atomic/primitive actions is not new and fairly renowned in robotics. I recommend reviewing the seminal works on: Dynamic Movement Primitives (Schaal), Dynamic Motor Primitives (Hogan and Sternad), Elementary Dynamic Actions (Nah), Large Behavioral Models (Tedrake).\n2. The learning from human demonstration is a very reasonable approach. However, in your work I had the impression that the human demonstrators were not using their most natural way to perform the task. For example, the pivoting task of cups and mugs will be normally performed in a single arm movement by grasping the object, rather than by using a bi-manual non prehensile strategy. The same applies for many other movements. What instructions did you give to the subjects? If the movements were so structured, wouldn't have been more reasonable to directly learn from robotic demonstration?\n3. The presentation of the six baseline methods could be improved, especially in the presentation of the way such methods are implemented in your platform. I find the superior performance of your methods both impressive and worrisome. Is there a way for others to confirm your results in any way?"}, "questions": {"value": "In the following you can find a detailed list of all the questions I would like for you to address.\n\n1. In the abstract the authors claim that \"pushing, poking, and pivoting\" are contact-rich actions. However, I would argue quite the opposite. Most of the time, humans push or poke thing with either the palm of their hand or a finger. Pivoting is maybe more complex from a manipulation standpoint, but it is hardly contact-rich. I understand the works focuses on non-prehensile manipulation, but I struggle to see how this could be contact rich. Maybe either consider re-framing the sentence or better explaining what you mean with contact rich.\n\n2. The authors wrote: \"To the best of our knowledge, our work is the first to propose a fully RL-free paradigm for learning bimanual non-prehensile manipulation skills through imitation and geometric reasoning.\". I invite authors to consider the paper: https://ieeexplore.ieee.org/abstract/document/10801502. While this work is not directly applied to non-prehensile manipulation, it leverages learning by demonstration and geometric reasoning to develop modular robotic primitives of motion. It would be worth including it in your paper and discuss your position with respect to it.\n\n3. In the bimanual robotic manipulation section, I found no mention of the recent efforts and convincing results obtained by Toyota Research Institute on bimanual manipulation using large behavioral models and diffusion policies. \nI strongly invite authors to consider their work and expand their evaluation of the state-of-the-art. Here you can find the link to the most recent YouTube video showcasing the performance of these large behavioral models: https://www.youtube.com/watch?v=DeLpnTgzJT4 .\n\n4. While I appreciate the efforts in making Figure 2, I also have to admit that the figure is very dense of information and hard to navigate. Is there a way to simplify or improve the Figure readability? This is more of a minor comment, but it would definitely benefit the overall quality of the work.\n\n5. In Section 3.3 the authors claim that a prerequisite for all skills is coplanarity of trajectory points in each arm. Why is that? It's this a mathematical need to simplify computation or a physical need that would make manipulation impossible otherwise? I think it would be important to clarify better this point.\n\n6. Line 288-289: there is a typo, the authors wrote \"safty\" but I believe they meant \"safety\".\n\n7. The authors claim that the contact adjustment takes less than 5 minutes. While this is not a long time, it is also not very fast. Is there a better way to improve contact detection? Maybe briefly discuss this point later in the paper.\n\n8. I agree that your geometry-aware optimization implicitly encodes object dimensions, but what about object physical properties such as stiffness, inertia or damping? What will happen if you try to manipulate an object of the same dimensions but very different compliance - for example much softer/stiffer or much heavier/lighter? Please comment on this in the paper.\n\n9. A minor point: sometimes in the text I found a different type of font. Can you please uniform the type of font across the paper?\n\n10. Table 1 results are very promising. What is your explanation behind your method much better performance? Could it also be that you implemented those other methods in an experimental setup which is not ideal for them?\n\n11. Line 368-369: there might be another typo, the authors wrote \"Details of reproduced all baselines are ... \" but I believe they meant \"Details of all reproduced baselines are ...\".\n\n12. Regarding research answer (A1), I think the authors should spend considerably more efforts in explaining how they trained the other benchmark methods. While I don't doubt the results of their proposed methods, I am surprised by the poor performance of the other methods. How is that possible? Can you provide - even in the supplementary material - sufficient and QUANTITATIVE justifications on why the alternative methods are so underperforming? The same observations applied to answer (A3).\n\n13. Did category level generalization also include substantial variations in object stiffness and weight? I think these are extremely important factors, since the ability to maintain contact with object will be heavily influenced by both the object rigidity and its weight.\n\n14. I'm not really sure what is the semantic difference between 'poking' and 'pivoting'. In both cases you are 'flipping' the object to its correct orientation. Why did you separate them? Is it because of the different strategy adopted by the humans? Also, I do expect human demonstrator to put a cup or mug in a vertical position by using just one-hand with a prehensile grasping movement. I have rarely see humans putting a cup upright by poking inside of it. \n\n15. Related to the previous point, what instructions did you give to the human demonstrators? Did you 'bias' them towards a specific strategy that you knew would better translate to robot movements? It's totally fine if you did so, but you need to specify it. If I look at Figure 7, I notice that the poking task is performed in a very 'unnatural' way for a human, the same goes for wrapping the basketball. Most humans will not lift a basketball in that way. Similar observations could be done also for the pivoting and pushing task.\nIn other words, if you want to learn by demonstration but will instruct users to move in specific ways, why not directly teleoperating the robots to do so and learn from the robotic teleoperation? What is the need of the human at this point?\n\n16. Section A.3 of the Appendix needs to be improved to answer my previous questions. As of now, I have the impression that the human demonstrations are not needed, since humans are not performing the tasks in their natural way, thus making learning from human demonstration meaningless in my view. This does not invalidate your methods, it just makes the human demonstration easily replaceable with robotic demonstration via teleoperation. Don't you agree? If not, please comment extensively, I'll be happy to change my mind.\n\n17. In Section B of the Appendix you discuss how you performed the reproduction of the six baseline methods. I think it would be beneficial to extend this section to better discuss any limitation of your training methods for these baseline techniques. I ask this specifically because your method seems to overperform all the baseline by a very large amount, and I want readers to be able to verify your claims as closely as possible. Also, did you release the code for YOUR method as well as the code for YOUR implementation of the baseline methods? If not, I strongly recommend to do so.\n\n18. A minor point on the video. The video is well organized and clear but it's very fast. I often had to pause the video and go back and forth to well appreciate the results. I recommend slowing the whole video by a factor of 2 or 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pFuEExgcyN", "forum": "4jcnded6fA", "replyto": "4jcnded6fA", "signatures": ["ICLR.cc/2026/Conference/Submission2567/Reviewer_VeaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2567/Reviewer_VeaC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761247860888, "cdate": 1761247860888, "tmdate": 1762916285065, "mdate": 1762916285065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to non-prehensile manipulation by making two key contributions. First, it moves beyond single-arm systems reliant on environmental aids, proposing instead a generalizable dual-arm configuration and a corresponding suite of Bimanual Non-prehensile Manipulation Primitives (BiNoMaP). Second, it abandons the prevailing RL-based paradigm for a three-stage, RL-free framework. This method learns from human video demonstrations, refines the extracted trajectories through a geometry-aware optimization to bridge the visual-morphological gap, and finally parameterizes them with object attributes for category-level generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, with a clear and logical structure.\n2. The paper provides extensive experiments to demonstrate the effectiveness of the proposed method.\n3. This paper offers an analysis of failure cases and discusses how integration with existing Vision-Language Models (VLMs) could enable more advanced robotic behaviors."}, "weaknesses": {"value": "1. The claim that the post-optimization method is more effective and safer than simulation or real-world RL requires experimental validation. A comparative study is needed to substantiate this advantage.\n2. The category-level generalization appears limited. The demonstrated objects within each category primarily vary in size but not in shape, whereas prior works often generalize across objects with more significant morphological differences.\n3. An analysis of the method's robustness to inaccurate object size estimation is necessary. My understanding is that the size-based generalization is highly dependent on accurate size measurement. In real-world scenarios, however, obtaining precise object dimensions from point clouds is often unreliable.\n4. I would like to see an evaluation of the method's generalization capability with respect to variations in object position.\n5. For a paper relying solely on real-world experiments, 10 test trials seem insufficient and may introduce randomness, potentially failing to accurately reflect the method's true performance.\n\nI will consider raising the score if the rebuttal of the author can address the above concerns."}, "questions": {"value": "please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethics concerns"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VLrU2pcj0E", "forum": "4jcnded6fA", "replyto": "4jcnded6fA", "signatures": ["ICLR.cc/2026/Conference/Submission2567/Reviewer_AWJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2567/Reviewer_AWJr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885700360, "cdate": 1761885700360, "tmdate": 1762916284838, "mdate": 1762916284838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BiNoMaP, a three-stage, RL-free pipeline for learning bimanual non-prehensile manipulation skills such as poking, pushing, pivoting, and wrapping. The method extracts hand trajectories from human video demonstrations, refines them through a geometry-aware post-optimization procedure, and parameterizes the resulting trajectories by object size and geometry to achieve limited category-level generalization. Experiments show qualitative success on a few simulated dual-arm tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach focuses on bimanual, non-prehensile skills—an under-explored problem setting.\n2. Authors propose a RL-free formulation reduces training cost and avoids reward-design issues.\n3. The pipeline could be practically useful if shown to generalize well in real-robot experiments."}, "weaknesses": {"value": "**1. Limited Novelty and Contribution to ICLR**\n\nThe paper’s technical core is largely procedural, consisting of three sequential steps:\n(1) hand-trajectory extraction from videos,\n(2) geometric post-optimization, and\n(3) VLM-based primitive parameterization for spatial relocation.\n\nWhile each component is competently executed, the combined framework introduces no fundamentally new algorithmic idea or learning formulation.\nThe system primarily assembles existing components into a pipeline without proposing new representations, objectives, or architectures for robot learning.\nAs a result, the contribution feels more like an application note or system integration paper, rather than a conceptual advance suitable for ICLR’s general audience.\n\nSpecifically, the motion optimization and “geometry-aware” refinement procedures are handcrafted rather than learned, and the use of VLMs for translation-based relocation (as described near Lines 280–290) represents a straightforward engineering extension, not a novel modeling technique.\n\nI currently do not see a clear learning insight or methodological novelty, which limits its appeal to ICLR’s research-focused readership.\n\n**2. Limited Generalization Capability**\n\nA large portion of the method depends on heuristic, hand-tuned geometric adjustments that raise concerns about robustness and generalization.\nFor example, the authors describe in Line 278 onward a process termed Geometry-Aware Iterative Contact Adjustment, where contact trajectories are refined through repeated geometric scaling and distance reduction.\n\nThis is essentially a manual iterative scaling procedure with several arbitrary hyperparameters (d(1)=5mm, γ=0.85, k≤10), whose stability and physical realism are not theoretically justified.\nThere is no learning signal or adaptive feedback beyond success/failure checks, meaning the approach cannot generalize beyond objects with similar geometry and contact conditions.\n\nMoreover, the claimed “spatial generalization” at deployment — implemented by translating refined trajectories according to VLM-detected displacements — is limited to planar offsets and does not address variation in object shape, size, or mass distribution. This suggests the method’s generalization is narrowly geometric and likely brittle under real-world variation or perception noise."}, "questions": {"value": "Please see my weaknesses section, thanks!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4h3pqBpjlp", "forum": "4jcnded6fA", "replyto": "4jcnded6fA", "signatures": ["ICLR.cc/2026/Conference/Submission2567/Reviewer_ESrq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2567/Reviewer_ESrq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137485360, "cdate": 1762137485360, "tmdate": 1762916284682, "mdate": 1762916284682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}