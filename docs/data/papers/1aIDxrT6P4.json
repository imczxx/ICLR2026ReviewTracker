{"id": "1aIDxrT6P4", "number": 17111, "cdate": 1758272311270, "mdate": 1759897196293, "content": {"title": "AnyDepth: Depth Estimation Made Easy", "abstract": "Recent monocular depth estimation models have achieved impressive performance. However, they typically rely on traditional encoders, complex decoders, and large training sets, which collectively limit their efficiency and generalization. In this work, we pursue a complementary approach: building a lightweight and efficient training framework without sacrificing accuracy. First, we apply DINOv3 to zero-shot monocular depth estimation for the first time. Secondly, we design a lightweight decoder SDT to reduce the number of parameters and computational cost while maintaining performance. Third, inspired by data-centric learning, we first analyze the characteristics that a high-quality sample should possess and then propose a filtering strategy based on these characteristics to filter out low-quality samples, thereby reducing dataset size while improving model training quality. Experiments on multiple benchmarks demonstrate that, despite using fewer parameters and data, our method achieves comparable or even higher accuracy than similar methods at larger scale. Our work emphasizes the integration of visual backbone performance, decoder efficiency, and data quality to explore more efficient and simple zero-shot monocular depth estimation pipelines.", "tldr": "", "keywords": ["Zero-shot monocular depth estimation", "Data-Centric Learning", "Lightweight Decoder", "Computer Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc630d6e99767c1efa9f5c80aad4b587680cd6a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a simplified architecture for depth estimation with three major contributions: (1) using frozen DINOv3 as backbone; (2) a lightweight decoder SDT and (3) a data filtering strategy. Experiments show better performance compared with DPT in a constrained setting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well writen and it's easy for me to follow.\n\n- Having a more lightweight and effective decoder compared with DPT would be very helpful for the depth community.\n\n- I like the idea of using less data and trying to achieve comparable performance. It's well motivated."}, "weaknesses": {"value": "- It's a bit over-claiming to regard adopting DINOV3 as a major contribution.\n\n- The SDT head should be more carefully ablated to demonstrate better than DPT head. Now, it's only proven to be better than DPT head in a frozen DINOV3 setting. But in most cases, people don't freeze the encoder during training. Will SDT head be better than DPT head when fine-tuning the DINOV3 encoder as well? On the other hand, it would be necessary to use various encoders in experiments and prove that SDT head can generally be better than DPT head with any encoder, DINOV2, ConvNext, etc.\n\n- It would be better to compare the inference time of SDT head and DPT head as well.\n\n- The paper presents three different filtering strategies, but none of them is carefully ablated. What if we only use one/two of these three strategies to do the filtering and train the model? Will we get worse results or not? There is no clue to prove that the combination of these three strategies is better than a simple random sampler."}, "questions": {"value": "Please check the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GAvERdVJ1G", "forum": "1aIDxrT6P4", "replyto": "1aIDxrT6P4", "signatures": ["ICLR.cc/2026/Conference/Submission17111/Reviewer_xH2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17111/Reviewer_xH2s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760884351299, "cdate": 1760884351299, "tmdate": 1762927112526, "mdate": 1762927112526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a lightweight framework for zero-shot monocular depth estimation, which combines a frozen DINOv3 encoder with a simple decoder called the Simple Depth Transformer (SDT). Unlike the complex multi-branch design of DPT (related work), the proposed SDT fuses multi-scale tokens once and reconstructs depth through a single upsampling path. This reduces parameters and FLOPs by over 70%, while maintaining comparable accuracy across benchmarks such as NYUv2, KITTI, ScanNet, ETH3D and DIODE. The paper also presents a data-centric filtering strategy that uses depth distribution and gradient continuity metrics to eliminate noisy training samples. This reduces the size of the dataset by one-third without any loss of performance. Experiments demonstrate that AnyDepth achieves a level of accuracy similar to that of larger models such as DPT, but with less computation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation to reduce data requirements of depth estimation and at the same time reduce the parameter count of these models is meaningful and interesting.\n\nThe paper shows clear efficiency gains over prior methods while also maintaining/improving overall performance.\n    \nThe idea is straightforward and the paper is well written. Figures 2 and 3 immediately illustrate the advantages of the method. \n\nIt can be considered as the first application of DINOv3 for zero-shot depth estimation."}, "weaknesses": {"value": "There is limited novelty within the data-centric learning metrics. The depth distribution score and gradient continuity score are simple to come up with and lack of some theoretical justification. In the experiments involving these metrics there is no sensitivity analysis or a comparison to other data sampling strategies, e.g. uncertainty aware filtering with techniques such as [1].\n\nThe effectiveness of data filtering is incremental. Table 3 shows only a minor benefit from applying the introduced filtering strategy; this does not support the claims.\n    \nThe scope of the experiments is too limited. The method has not been evaluated in either fine-tuning or metric depth estimation experiments. Experiments only compare to DPT. Although the method is trained on DINOv3 for this study, the approach was first developed in 2021. It would be more intuitive to also compare it to newer, prior-based depth estimation approaches, such as [2, 3, 4].\n\nReferences:\n\n[1] Hornauer, J., El-Ghoussani, A., & Belagiannis, V. (2025). Revisiting Gradient-Based Uncertainty for Monocular Depth Estimation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.\n\n[2]Ke, B., Qu, K., Wang, T., Metzger, N., Huang, S., Li, B., ... & Schindler, K. (2025). Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis. *arXiv preprint arXiv:2505.09358*.\n\n[3]Garcia, G. M., Abou Zeid, K., Schmidt, C., De Geus, D., Hermans, A., & Leibe, B. (2025, February). Fine-tuning image-conditional diffusion models is easier than you think. In *2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)* (pp. 753-762). IEEE.\n\n[4]Gui, M., Schusterbauer, J., Prestel, U., Ma, P., Kotovenko, D., Grebenkova, O., ... & Ommer, B. (2025, April). DepthFM: Fast Generative Monocular Depth Estimation with Flow Matching. In *Proceedings of the AAAI Conference on Artificial Intelligence* (Vol. 39, No. 3, pp. 3203-3211)."}, "questions": {"value": "The low parameter count of the introduced approach is really interesting. Could the authors perhaps add a sensitivity analysis of the fusion strategy?\n    \nCould the proposed filtering metrics accidentally remove valuable but rare edge cases (e.g. scenes with strong depth discontinuities)? How can this be avoided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Pk1wYfZPYH", "forum": "1aIDxrT6P4", "replyto": "1aIDxrT6P4", "signatures": ["ICLR.cc/2026/Conference/Submission17111/Reviewer_Mtin"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17111/Reviewer_Mtin"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713981091, "cdate": 1761713981091, "tmdate": 1762927112254, "mdate": 1762927112254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an efficient framework for zero-shot monocular depth estimation. The authors aim to maintain accuracy while significantly improving efficiency through a three-pronged approach: a powerful but lightweight architecture and a data-centric training strategy. Experiments show that this combined approach achieves accuracy comparable or superior to larger-scale methods while using fewer parameters and less data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed Depth Distribution Score and Gradient Continuity Score are interesting metrics for assessing depth-map quality, but some aspects remain limited."}, "weaknesses": {"value": "(1) The metrics prioritize distribution smoothness over true geometric fidelity. Consequently, a high score does not necessarily correlate with an accurate depth map. \n(2)  The selection of hyperparameters—such as the number of bins, weight values, and normalization schemes—appears empirical. The approach lacks theoretical justification or a sensitivity analysis to validate these choices.\n(3) The evaluation fails to account for semantic information, which can lead to the unfair penalization of scenes with naturally imbalanced depth distributions (e.g., those dominated by sky or distant objects).\n(4) The paper reiterates well-known limitations of data-driven depth estimation without offering novel insights beyond those already discussed in prior works like Depth Anything v1 and v2.\n(5) A significant missed opportunity is the failure to integrate these metrics as differentiable loss functions during training, which could have directly improved depth consistency in the models.\n(6) The evaluation lacks a thorough breakdown across critical domains (e.g., indoor vs. outdoor, dense vs. sparse LiDAR). Furthermore, the proposed data filtering strategy demonstrates only marginal performance gains.\n(7) The central \"data-centric\" perspective is not convincingly supported by the experimental results, as the method's performance remains comparable to, but does not surpass, existing baselines."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vOqvmeLHtr", "forum": "1aIDxrT6P4", "replyto": "1aIDxrT6P4", "signatures": ["ICLR.cc/2026/Conference/Submission17111/Reviewer_5CX5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17111/Reviewer_5CX5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787996299, "cdate": 1761787996299, "tmdate": 1762927111723, "mdate": 1762927111723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose AnyDepth, a depth estimation network that, for the first time, integrates DINOv3 into the depth estimation task through their Simple Depth Transformer (SDT) architecture. \nThey further conduct an analysis of the training datasets and identify imbalances in the ground-truth depth distributions across different datasets. To address this, they filter out unbalanced data pairs to improve training consistency. \nExperimental results demonstrate that the proposed method outperforms existing depth estimation architectures, including DPT and Depth Anything v2."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Lack of comparison to state-of-the-arts\nThe author compares their proposed method with DPT and Depth-Anything v2, but they are not the best state-of-the-art works and there are better state-of-the-art works that authors needs to compare.\n\n[1] Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation\n[2] Metric3d v2: A versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation\n[3] Depth pro: Sharp monocular metric depth in less than a second\n\n2. Ablation study on SDT\nThe authors contribution is also on the model architecture of SDT including Spatial Detail Enhancer, and Fusion algorithm but there is no ablation study that how much each component effects to the final output."}, "weaknesses": {"value": "1. Limited comparison with state-of-the-art methods\nThe authors compare their proposed approach only with DPT and Depth Anything v2, which, while relevant, do not represent the current state-of-the-art in monocular depth estimation. To strengthen the empirical validation, comparisons should be made with more recent and competitive methods, such as:\n\n[1] Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation\n[2] Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-Shot Metric Depth and Surface Normal Estimation\n[3] Depth Pro: Sharp Monocular Metric Depth in Less Than a Second\n\nIncluding these methods would provide a more comprehensive evaluation and clarify the performance gap relative to the latest advancements.\n\n2. Missing ablation study on SDT components\nThe proposed Simple Depth Transformer (SDT) introduces several architectural contributions, including the Spatial Detail Enhancer and the Fusion algorithm. However, the paper lacks an ablation study demonstrating the individual contribution of each component to the final performance. Providing such an analysis would help justify the design choices and highlight which modules most significantly impact the overall results."}, "questions": {"value": "1. How each component in SDT takes part in final output performance?\n\n2. How much performance would be changed if you change the encoder of AnyDepth as Dinov2?\n\n3. How much performance would be changed if you change the encoder of DPT as Dinov3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M15VJZ2xO7", "forum": "1aIDxrT6P4", "replyto": "1aIDxrT6P4", "signatures": ["ICLR.cc/2026/Conference/Submission17111/Reviewer_xv2A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17111/Reviewer_xv2A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816627888, "cdate": 1761816627888, "tmdate": 1762927111381, "mdate": 1762927111381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}