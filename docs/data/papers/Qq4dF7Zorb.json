{"id": "Qq4dF7Zorb", "number": 18576, "cdate": 1758289215640, "mdate": 1759897094949, "content": {"title": "Class-Conditional Autoencoders with Adversarial Alignment for Multimodal Fusion", "abstract": "Multimodal learning has advanced rapidly with large-scale transformers, but often requires heavy computation and lacks clear theoretical grounding. We propose a lightweight yet robust framework for multimodal fusion that unifies efficiency with theoretical guarantees. At its backbone lies a Class-Conditional Autoencoder (CCAE), which maps modality-specific inputs into a class-aware latent space. Building upon this, our Discriminative Embedding Framework (DEF) incorporates homologous and reconstruction losses to contract intra-class variance while preserving semantic fidelity, producing embeddings that are compact and discriminative. To address distributional inconsistencies across modalities, we introduce the Adversarial Alignment Framework (AAF), which dynamically weights modality contributions and aligns fused embeddings with modality-specific distributions using a Wasserstein objective. Together, DEF and AAF form a cohesive framework that explains why consistency and alignment emerge from a unified optimization perspective. Extensive experiments on machine translation (How2, Multi30k) and emotion recognition (IEMOCAP, MOSEI) demonstrate that our approach consistently outperforms strong baselines, including Transformer, MulT, and MISA, while operating with much lower FLOPs.", "tldr": "his paper proposes a model-free, category-aware dynamic fusion framework based on comprehensive data-driven approaches.", "keywords": ["multimodal;ML: Multimodal Learning;dversarial Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/28c1efa86e81514f900142d4ff17e4bf6da7d0c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DEF+AAF, a comprehensive multimodal fusion framework that integrates Discriminative Embedding Framework (DEF), a Class-Conditional Autoencoder (CCAE), and an Adversarial Alignment Framework (AAF). The model is theoretically grounded, aiming to enhance cross-modal alignment, reduce modality discrepancies, and improve generalization in multimodal tasks. The proposed framework is evaluated on both emotion recognition (IEMOCAP, MOSEI) and multimodal machine translation (How2, Multi30k) benchmarks. Experimental results show that DEF+AAF achieves comparable or superior performance to existing baselines while being more parameter-efficient and faster in training and inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a unified multimodal fusion framework (DEF+AAF) that combines discriminative embeddings, class-conditional autoencoders, and adversarial alignment, supported by theoretical justification.\n\n2. Compared to large models like Transformers, DEF+AAF has lower parameter count and FLOPs, faster training and inference speed, while maintaining or even improving performance."}, "weaknesses": {"value": "1. The baselines selected for machine translation (How2, Multi30k) are all from before 2020, and those for emotion recognition (IEMOCAP, MOSEI) are all from before 2022, lacking comparisons with the current state-of-the-art models.\n\n2. Given that all baselines are pre-2022, the improvements of DEF+AAF on emotion recognition (IEMOCAP, MOSEI) are relatively limited.\n\n3. Hyperparameter studies are insufficient. In Appendix B.2, the authors only compare three values for λ and two values for γ, which provides a very limited view of the model’s sensitivity. \n\n4. Although lighter than large Transformers, DEF+AAF still consists of multiple modules, which makes implementation and tuning complex. Moreover, the theoretical guarantees for homologous variance contraction and adversarial alignment rely on several assumptions, which may be difficult to satisfy in practice."}, "questions": {"value": "1. How stable is the training of DEF+AAF considering its multiple interdependent modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6hzQnRVEPz", "forum": "Qq4dF7Zorb", "replyto": "Qq4dF7Zorb", "signatures": ["ICLR.cc/2026/Conference/Submission18576/Reviewer_WmeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18576/Reviewer_WmeD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586503520, "cdate": 1761586503520, "tmdate": 1762928293901, "mdate": 1762928293901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and theoretically grounded framework for multimodal fusion, combining a Class-Conditional Autoencoder (CCAE) with a Discriminative Embedding Framework (DEF) and an Adversarial Alignment Framework (AAF). The core objective is to learn compact, discriminative, and distributionally aligned multimodal embeddings in a computationally efficient manner. The method is extensively evaluated on machine translation (How2, Multi30k) and emotion recognition (IEMOCAP, MOSEI) tasks, demonstrating strong performance improvements over several state-of-the-art baselines while being more parameter- and compute-efficient.\n\nThe paper is well-written, methodologically sound, and makes significant contributions. The unification of discriminative and adversarial objectives under a single optimization perspective is a key strength. The empirical evaluation is thorough, including robustness analyses and efficiency comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed DEF+AAF framework offers a unified optimization perspective that elegantly combines variance contraction (via homologous and reconstruction losses), class separability, and distributional alignment (via adversarial training). This provides a more principled and interpretable approach compared to many ad-hoc fusion strategies.\n- The paper is commendable for its theoretical contributions. Propositions, along with the proofs in the appendix, provide formal guarantees on intra-class variance contraction and distribution alignment, strengthening the methodological claims.\n- Emphasis on Efficiency and Robustness."}, "weaknesses": {"value": "- My primary concern is the selection of baselines in the mail part of the experiment, which are of date (2021, 2022, etc). Are there any recent SOTA baselines that can be added?\n- The ablation definitions need to be clearer. While the main method is a combination of DEF and AAF, is the ablation of DEF or AAF itself needed? Or has it already been included in Table 3?"}, "questions": {"value": "- My primary concern is the selection of baselines in the mail part of the experiment, which are of date (2021, 2022, etc). Are there any recent SOTA baselines that can be added?\n- The ablation definitions need to be clearer. While the main method is a combination of DEF and AAF, is the ablation of DEF or AAF itself needed? Or has it already been included in Table 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PD77VG0qgK", "forum": "Qq4dF7Zorb", "replyto": "Qq4dF7Zorb", "signatures": ["ICLR.cc/2026/Conference/Submission18576/Reviewer_nzSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18576/Reviewer_nzSk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971778968, "cdate": 1761971778968, "tmdate": 1762928291213, "mdate": 1762928291213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight framework for multimodal fusion, which aims to address the high computational cost and lack of theoretical grounding in existing large Transformer models. The proposed framework consists of several modules: 1) Class-Conditional Autoencoder\n Is used to map inputs from different modalities into a shared latent space that is conditioned on class information; 2) Discriminative Embedding Framework (DEF) enforces compactness and class separability using homologous and reconstruction losses, ensuring modality-aligned and semantically robust embeddings; 3) Adversarial Alignment Framework (AAF) introduces a dynamic fusion mechanism (similar to attention) to weight different modalities and uses Wasserstein-based adversarial training to align the distribution of the fused embedding with the distributions of the individual modal embeddings. The authors claim that this framework (DEF+AAF) surpasses strong existing baselines (like Transformer, MulT, etc.) on machine translation (How2, Multi30k) and emotion recognition (IEMOCAP, MOSEI) tasks with lower computational cost (FLOPS)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of combining class information, modal cohesion, and distributional alignment is conceptually clear and technically sound;\n- The paper reports not only on performance but also on parameters, FLOPs, and training/inference speed (Table 5);"}, "weaknesses": {"value": "- The paper writing & organization is poor. The illustration of introduction is too short, which makes readers hard to fully understand the motivation & goal of this work; In the related work part (Multimodal representation learning), the paper misses many Refs, e.g., “, such as early fusion (feature concatenation) or late fusion (decision-level combination),”, “Autoencoding-based methods extended”; The current writing quality significantly hinders readability and makes it difficult for readers to follow the paper’s logic and contributions, which is not acceptable for a top-tier venue such as ICLR.  I hope the authors could carefully revise these typos & writing issues before resubmission.\n- In Tables 1&2, the latest compared approach is proposed in 2022, please include latest SOTA approaches for comparison.\n\nThe writing quality of this paper falls well below the standards expected at ICLR, making it difficult to follow. Moreover, the paper lacks comparisons with state-of-the-art methods. Therefore, I recommend rejection."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lIGFogwUWG", "forum": "Qq4dF7Zorb", "replyto": "Qq4dF7Zorb", "signatures": ["ICLR.cc/2026/Conference/Submission18576/Reviewer_FB3Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18576/Reviewer_FB3Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976076206, "cdate": 1761976076206, "tmdate": 1762928290577, "mdate": 1762928290577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the heavy computation and weak theoretical grounding in multimodal learning by proposing a lightweight yet principled fusion framework. Based on a Class-Conditional Autoencoder (CCAE), the method maps inputs into a class-aware latent space, while the Discriminative Embedding Framework (DEF) enhances intra-class compactness and preserves semantic consistency. To address cross-modal distribution gaps, the Adversarial Alignment Framework (AAF) employs a Wasserstein-based objective for dynamic alignment. Unified under a coherent optimization view, DEF and AAF achieve both efficiency and theoretical interpretability. Experiments on translation and emotion recognition benchmarks show consistent gains over Transformer, MulT, and MISA with significantly reduced FLOPs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposed strategies are theoretically solid.\n2. Experiments are partially effective."}, "weaknesses": {"value": "1. Related work and baselines are outdated, mostly before 2022. Including recent multimodal fusion methods and robustness comparisons against missing/noisy modality approaches would strengthen the evaluation.\n2. Mathematical notation is inconsistent and sometimes ambiguous. Theoretical analysis lacks formal proofs or derivations to support the claimed guarantees.\n3. Prior methods such as conditional autoencoder, InfoNCE, and Wasserstein GAN with Gradient Penalty and the baseline methods are mentioned without proper citations.\n4. The paper lacks implementation details regarding the hyperparameters used in Eq.9.\n5. Figures 1–3 are not referenced in the main text. Figure 1 lacks a legend, and its visualization appears unrelated to the objective of the Homogeneous Loss. Moreover, Figures 2 and 3 do not specify the datasets used for the experiments.\n6. The writing could be improved. Section 3 mentions two proposed methods, which should refer to DEF and AAF; however, AAF is described separately in Section 4, showing a structural oversight in the paper’s organization."}, "questions": {"value": "1. The paper uses a mean squared error–based objective to reduce modality discrepancy. Has the potential loss of modality-specific information been considered when enforcing such cross-modal similarity?\n2. In the contrastive regularization loss, how are the positive and negative sample pairs defined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "O24PLXMOkr", "forum": "Qq4dF7Zorb", "replyto": "Qq4dF7Zorb", "signatures": ["ICLR.cc/2026/Conference/Submission18576/Reviewer_ceko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18576/Reviewer_ceko"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985664966, "cdate": 1761985664966, "tmdate": 1762928288716, "mdate": 1762928288716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a lightweight multimodal method that combines a class-conditional autoencoder  with a discriminative embedding module and an adversarial aligner that learns sample-wise modality weights and aligns the fused code to each modality using Wasserstein training. The authors argue this unifies intra-class variance reduction, semantic preservation, and cross-modal distribution alignment with good efficiency. Experiments on translation and affect show consistent gains and robustness to missing/noisy inputs, supported by ablations. However, the role of class conditioning in MT settings, fairness of FLOP accounting, and some implementation details require clearer exposition."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Combines a class-conditional autoencoder with discriminative embedding and adversarial alignment to jointly achieve intra-class compactness, semantic preservation, and cross-modal distribution alignment under one objective.\n2. Learns per-example modality weights and aligns the fused code to each modality, improving resilience when a modality is noisy or missing.\n3. Demonstrates consistent gains on translation and affective benchmarks while using fewer parameters/FLOPs, indicating a favorable accuracy–efficiency trade-off."}, "weaknesses": {"value": "1. The paper under-specifies how class conditioning is defined on tasks without explicit labels, whether class cues are needed at inference, and key implementation details, making replication difficult.\n2. FLOP/latency comparisons appear to exclude external feature extractors, and decoding/tokenization protocols aren’t fully standardized across baselines; end-to-end efficiency and broader datasets/metrics would make the gains more convincing.\n3. Pushing the fused code toward a Wasserstein barycenter can dilute rare but discriminative cues when modalities disagree; the paper lacks analyses or ablations vs. reliability-aware or top-k alignment variants to rule out this failure mode."}, "questions": {"value": "1. How are class embeddings defined on Multi30k/How2, and are they required at inference?\n2. Does Wasserstein-barycenter alignment dilute rare but discriminative cues when modalities conflict?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y22cQ2bVM0", "forum": "Qq4dF7Zorb", "replyto": "Qq4dF7Zorb", "signatures": ["ICLR.cc/2026/Conference/Submission18576/Reviewer_aDv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18576/Reviewer_aDv7"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986341730, "cdate": 1761986341730, "tmdate": 1762928287266, "mdate": 1762928287266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}