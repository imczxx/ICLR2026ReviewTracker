{"id": "bbUWnnWtgz", "number": 18775, "cdate": 1758290772659, "mdate": 1759897081867, "content": {"title": "Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning", "abstract": "In-Context Learning (ICL) allows Large Language Models (LLM) to adapt to new tasks with just a few examples, but their predictions often suffer from systematic biases, leading to unstable performances in classification. While calibration techniques are proposed to mitigate these biases, we show that, in the logit space, many of these methods are equivalent to merely shifting the LLM's decision boundary without having the ability to alter its orientation. This proves inadequate when biases cause the LLM to be severely misdirected. To address these limitations and provide a unifying framework, we propose Supervised Calibration (SC), a loss-minimization based framework, which learns an optimal, per-class affine transformation of LLM's predictive probabilities in the logit space without requiring external data beyond the context. By using a more expressive functional class, SC not only subsumes many existing calibration methods in ICL as special cases but also enables the ability of altering and even completely reversing the orientation of the LLM's decision boundary. Furthermore, SC's loss-based nature facilitates the seamless integration of two purpose-built regularization techniques—context-invariance and directional trust-region regularizers. The former is designed to tackle the instability issue in ICL, while the latter is to control the degree of calibration. Finally, SC delivers state-of-the-art performance over calibration baselines in the 4-shot, 8-shot, and 16-shot settings across all nine datasets for Mistral-7B-Instruct-v0.3, Llama-2-7B-chat, and Qwen2-7B-Instruct.", "tldr": "", "keywords": ["Large language models", "in-context learning", "few-shot learning", "calibration", "Supervised Calibration", "context-invariance regularization", "trust-region regularization", "robustness", "text classification."], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64393c2c12c97f21575aca11c93ec84039873896.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a supervised calibration method that performs interpolated sampling within restricted in-context demonstrations, and subsequently fits an estimator for calibration. Two additional regularization terms are introduced to encourage contextual invariance and stabilize the calibration results. Experiments conducted on nine datasets across three large language models validate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents a novel idea for calibrating LLMs, achieving notable performance improvements.\n2. The paper is well-organized, with clear and precise mathematical formulations, making it highly readable.\n3. It presents a clear and compelling motivation, which effectively supports the subsequent experiments."}, "weaknesses": {"value": "1. It cannot be applied to black-box model calibration, as the method requires access to the model’s internal outputs or representations for fitting the calibration estimator.\n2. The model is somewhat overly complex, and the inclusion of multiple regularization terms blurs the main focus, which in turn reduces its practical applicability.\n3. The paper lacks comparisons with the latest calibration methods. While expecting comparisons with 2025 methods might be unrealistic, there should at least be comparisons with 2024 approaches, such as In-Context Calibration [1].\n4. I do not believe that more experiments are necessarily better, as this can dilute the focus on key contributions. For example, in the ablation study, the authors only consider the SC* variant, whereas I think the **directional trust region** and **InvPenalty** ablations should also be included. Logically, these components should also make positive contributions to SC, so it is necessary to show their individual effects.\n5. Considering Tables 3 to 6, a possible trend is that SC’s performance improves with the number of demonstration samples, likely due to the longer context providing more usable examples. Consequently, in scenarios with few available shots or in pure zero-shot tasks, the applicability of SC may be limited.\n6. Additionally, conducting the ablation study only on SST-5 can be somewhat misleading, as SST-5 is an inherently difficult task with a higher probability of incorrect decision directions, making it more compatible with SC. It is recommended that the authors also perform ablations on other tasks, such as the easier SST-2, to provide a fairer comparison.\n7. Another significant concern is the time cost, as the sampling, training, and inference processes all incur substantial computational overhead, as reported by the authors in Tables 10 and 11.\n\n\n[1] Jang J, Jang S, Kweon W, et al. Rectifying Demonstration Shortcut in In-Context Learning[C]//Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2024: 4294-4321."}, "questions": {"value": "See the aforementioned \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OIOedVTXV7", "forum": "bbUWnnWtgz", "replyto": "bbUWnnWtgz", "signatures": ["ICLR.cc/2026/Conference/Submission18775/Reviewer_ag4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18775/Reviewer_ag4E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760682456408, "cdate": 1760682456408, "tmdate": 1762999988115, "mdate": 1762999988115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for recalculating the prediction outputs of ICL on classification tasks. Specifically, the authors introduce Supervised Calibration, which trains an affine transformation on the logits produced by typical restricted decoding (a vector whose dimension matches the label space) to rescale these logits for improved accuracy. Moreover, compared with previous work, the authors avoid additional data costs by using automatically generated training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper proposes an automatic data generation method for ICL calibration, which helps reduce the high data requirements of previous approaches.\n    \n2. This paper designs a regularization term to improve prediction consistency for the same query under different demonstration conditions. This is interesting, and I would like to see more analysis on this point (see Weakness).\n    \n3. This paper employs batched calibration training across different numbers of demonstrations. Specifically, a k-shot input includes all (<k)-shot inputs, allowing calibration to be trained simultaneously under these settings. This is an efficient design."}, "weaknesses": {"value": "1. The empirical method proposed in this paper, i.e., training an affine transformation to rescale the results of restricted decoding, does not go beyond the scope of previous works, and the authors have not compared their approach against them. These prior works include KNN prompting [1], Hidden Calibration [2], and Prototypical Calibration [3], which all utilize the high-degree-of-freedom decision boundary modification. While I acknowledge that the proposed automatic training data generation method is a valuable idea, it represents a vertical contribution, meaning it could be applied to any of the above supervised classifiers to improve their efficiency. This limitation currently prevents me from recommending the paper for acceptance. The authors could either compare their method with the above works under the same setting (i.e., using generated training data) or explain clearly why such a comparison is not provided.\n    \n2. The effectiveness of the proposed regularization term has not been verified. The authors could try ablating it and re-evaluating the results. I checked the main text and found no such experiment. If I have overlooked it, I apologize.\n    \n3. The effectiveness of the automatic data generation method has not been verified. We would like to know how much it differs from traditional sampling from real-world datasets.\n    \n4. The results are not SOTA on a considerable number of settings, as shown in the appendix. I am tolerant of this point, so it does not affect my overall evaluation of the paper.\n\n[1]. kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference, ICLR 2023  \n[2]. Token-based Decision Criteria Are Suboptimal in In-context Learning. NAACL 2025.  \n[3]. Prototypical Calibration for Few-shot Learning of Language Models. ICLR 2023."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1BdfW9jTHU", "forum": "bbUWnnWtgz", "replyto": "bbUWnnWtgz", "signatures": ["ICLR.cc/2026/Conference/Submission18775/Reviewer_kDo3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18775/Reviewer_kDo3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203127248, "cdate": 1761203127248, "tmdate": 1762999987953, "mdate": 1762999987953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It is claimed that ICL in LLMs suffers from systematic biases, leading to unstable performance. It is suggested that this is due to a shift but not rotation of the decision boundary. This claim may require extraordinary evidence. Supervised Calibration is suggested, which learns per-class transformations, which is said to subsume other ICL calibration methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The breadth of benchmarking datasets is an advantage, and >2 LLM families is also a slight advantage, although it is increasingly expected. The former is attenuated somewhat by the fact that these are only classification datasets (and this attenuation is attenuated by the fact that narrowly focusing on classification is the point); the latter is attenuated by the fact that only small 7B models are used."}, "weaknesses": {"value": "- Not that there are word ounts per section, and this is minor, but there is so little context given (Sec 3.4 somewhat notwithstanding) -- Sec 2 is barely a paragraph with only a handful of papers mentioned, with very little nuance into how they're mentioned, nor any comparison or caveat between them. Another reason this is a minor complaint is that other references are strewn throughout, but still some claims throughout could benefit from additional external context (e.g., that order can bias ICL, L136)\n- The experiments are extremely thin. In addition to the caveats mentioned in the Strengths section, only F1 is measured (!), with little/isolated care provided to benchmark-specific performance, error analysis, order variation (despite repeatedly mentioning its importance). Averages and s.d.s are given in Fig 3 but a more fullsome uncertainty analysis is not given. These are mitigated to some extent by the secondary experiment shown in Fig 4, but it is insufficient; appendices are also not part of the main paper. \n- The prompt/label set appears fixed per dataset -- sensitivity analyses towards generalizability would be good to add in a future revision.\n- Different calibrators (very) briefly mentioned are not included in direct comparison, which is a major oversight."}, "questions": {"value": "- If order biases exist (L136), then is the iid assumtion on L150 valid?\n- If KL is not the symmetric version, L152, how did you determine the order of P and Q?\n- Are *all* the experiments in the 'ablations' subsection actually *ablations*?\n- Given that ensembling (seems to) scale linearly with contexts, what kinds of real-world or practical compute/resource-relevant experiments could you have done?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2EH1kfVzEV", "forum": "bbUWnnWtgz", "replyto": "bbUWnnWtgz", "signatures": ["ICLR.cc/2026/Conference/Submission18775/Reviewer_JSnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18775/Reviewer_JSnt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738441746, "cdate": 1761738441746, "tmdate": 1762999988110, "mdate": 1762999988110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}