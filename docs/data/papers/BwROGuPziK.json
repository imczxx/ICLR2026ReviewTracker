{"id": "BwROGuPziK", "number": 25094, "cdate": 1758364075216, "mdate": 1760187379795, "content": {"title": "Learnable Riesz Transform for Composite Scale-Rotation Equivariant Spatial Transformers", "abstract": "Building models robust to transformations such as rotation, scale, and translation is a challenge in machine learning and computer vision.\nExisting approaches often provide only partial and discrete equivariance (group equivariance) or rely on supervision or very abundant data to learn equivariant representations.\nTo achieve fine-grained equivariance from low data, we combine and improve over both approaches.\nWe propose a novel, learnable, Riesz-transform-based architecture that achieves built-in group equivariance for translation, rotation, and scale.\nWe combine it with a Spatial Transform Network (STN) tailored for the sequential estimation of composite transformations, reducing the combinatorial data requirements for learning fine-grained equivariance.\nImproved generalization guarantees and extensive experiments demonstrate that our approach brings improvements over state-of-the-art methods in unsupervised representation learning and object discovery, even more so in low-data regimes.", "tldr": "", "keywords": ["Riesz transform", "equivariance", "spatial transformer networks", "unsupervised learning", "rotation equivariance", "scale equivariance", "group convolution", "steerable filters", "geometric transformations", "SO(2)-equivariance", "computer vision", "variational autoencoding", "object discovery", "limited-data regimes", "composite transformations"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e385ea2cc36b8c712432d467ecb2146ffbb30e79.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}