{"id": "gsCVfTW2AJ", "number": 17398, "cdate": 1758275457821, "mdate": 1759897177462, "content": {"title": "VAR-MATH: Probing True Mathematical Reasoning in LLMs via Symbolic Multi-Instance Benchmarks", "abstract": "Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of large language models (LLMs), as measured by standard benchmarks. Yet these gains often persist even when models are trained with flawed signals, such as random or inverted rewards. This raises a fundamental question: do such improvements reflect genuine reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To answer this question, we adopt an evaluation-centric perspective and highlight two critical shortcomings in existing protocols. First, benchmark contamination arises because test problems are publicly available, thereby increasing the risk of data leakage. Second, evaluation fragility results from reliance on single-instance assessments,\nwhich are sensitive to stochastic outputs and fail to capture reasoning consistency. These limitations suggest the need for a new evaluation paradigm that can probe reasoning ability beyond memorization and one-off success. As response, we propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates and requires models to solve multiple instantiations of each. This design enforces consistency across structurally equivalent variants, mitigates contamination, and enhances robustness through bootstrapped metrics. We apply VAR-MATH to transform three popular benchmarks, AMC23, AIME24, and AIME25, into their symbolic counterparts, VAR-AMC23, VAR-AIME24, and VAR-AIME25. Experimental results show substantial performance drops for RL-trained models on these variabilized benchmarks, especially for smaller models, with average declines of 47.9% on AMC23, 58.8% on AIME24, and 72.9% on AIME25. These findings indicate that some existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms.", "tldr": "A benchmark for probing true math reasoning in LLM.", "keywords": ["LLM Evaluation; AI for Math;"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ca2f49904fb8017d2e7cb44d3a335d4eac4c7a8.pdf", "supplementary_material": "/attachment/08e24cac0cfb5bda002a650dc0718da9da619cd1.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce a new symbolic evaluation framework, VAR-MATH, that uses preexisting datasets such as AIME and AMC and converts them into parameterized templates. These templates are used to create multiple instances for a single problem to help eliminate the issue of benchmark contamination and evaluation fragility that are common for the existing benchmarks.\nThe authors tested this benchmark on various models and showed a substantial decline in accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Creation of a new dataset containing 430 question-answer pairs to tackle contamination and evaluation fragility.\n2. In-depth evaluation of various models, both reasoning and non-reasoning models.\n3. The authors employ a data processing method to convert each numerical problem into a symbolic template."}, "weaknesses": {"value": "1. The authors talk about two existing issues with current benchmarks: contamination and evaluation fragility. While I agree that these datasets are publicly available, models can easily memorize them, which leads to contamination. However author does not provide strong evidence that evaluation fragility is present in current benchmarks, especially in datasets like AIME, AMC.\n2. The main idea of this dataset is to convert each problem into a symbolic template, which decouples problem structure from fixed numeric content; however, how is this method different from other existing method like GSM-Symbolic [1].\n3. How were the symbolic templates generated? Is LLM used to generate those, or are these manually annotated?\n4. The paper only touches on math-based datasets like AMC, AIME. However, this data generation might fail on domains where conversion to a symbolic template might not be feasible.\n\n\n\n[1]: Mirzadeh, Iman, et al. \"Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models.\" arXiv preprint arXiv:2410.05229 (2024)."}, "questions": {"value": "1. Will the dataset and code be released upon acceptance?\n2. In section 4.3.2, I did not fully understand how partial credit assignment can help disentangle contamination-driven memorization from instability of symbolic reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kQz31q9U2A", "forum": "gsCVfTW2AJ", "replyto": "gsCVfTW2AJ", "signatures": ["ICLR.cc/2026/Conference/Submission17398/Reviewer_T6Db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17398/Reviewer_T6Db"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856326862, "cdate": 1761856326862, "tmdate": 1762927303090, "mdate": 1762927303090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper takes AMC23/AIME24/AIME25 problems and converts them symbolic templates and evaluates models on multiple instantiations per template with strict (all variants must be correct) and loose (average) metrics, plus resampling. \n\nCore findings: RL‑tuned 7B/32B models drop sharply under the parameterized template suites (e.g., strict score drops of across tables), while frontier models drop less but still non‑trivially. The authors infer that some RL gains rely on benchmark‑specific artifacts and are not structurally consistent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear motivation (contamination & fragility) and a sensible multi‑instance / consistency protocol.\n- Broad empirical sweep across contemporary RL models with interpretable strict vs. loose metrics.\n- Evidence that multi‑instance evaluation reduces variance and reveals failure modes hidden by single‑instance scoring."}, "weaknesses": {"value": "The main weakness is that paper fails to cite and contrast its work against previous work that do very similar explorations. For example, RE‑IMAGINE (ICML’25), neuro-symbolic data gen ([NeurIPS 2024](https://arxiv.org/abs/2412.04857)) or any other symbolic benchmarking papers like (GSM Hard, GSM-Symbolic, GSM-IC.. etc) which already (partly) introduced a symbolic representation → mutation → automatic ground‑truth pipeline, modes of difficulty, and reporting across math/code. Overlap is substantial; in my opinion, novelty is primarily the strict multi‑instance metric and the AMC/AIME specialization.\n \nMethodological issues: unequal sampling across models (M=16 for open‑weights, M=1 for APIs from  Table 6:(Decoding and runtime configurations for model evaluation), unspecified bootstrap rounds N. \n\nHow do you ensure statistical significance when question banks are of different sizes (Table 5). Do results hold when evaluated only on a 1:1 matching subset?"}, "questions": {"value": "1. How well does your metric compare against the above mentioned papers? Please explain your contributions against the set of papers mentioned above.\n2. How do you ensure that difficulty of your questions upon mutation remains same. It could very well be the case that the mutated questions are simply harder, causing the score drop.\n3. How reliable is your parsing and mutation pipeline? It could be the case that a lot of the questions were just non-sensical or wrong, causing the score drop.\n5. Please provide scores for a 1:1 matching subset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wgTgPRTdEo", "forum": "gsCVfTW2AJ", "replyto": "gsCVfTW2AJ", "signatures": ["ICLR.cc/2026/Conference/Submission17398/Reviewer_CX7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17398/Reviewer_CX7P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972238124, "cdate": 1761972238124, "tmdate": 1762927302457, "mdate": 1762927302457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VAR-MATH functionalizes recent contest-math benchmarks (AMC23, AIME24, AIME25) by converting fixed constants into constrained variables to create symbolic templates, then instantiating multiple numeric variants per template (as in [1]). Models are scored strictly (must be correct on all variants) and loosely (average accuracy across variants), following prior functionalization work but applied to current AIME-level sets. Empirically, many RL-tuned and SFT-ed models that look strong on single-instance tests drop sharply on VAR-MATH: average strict-score declines of ~48%/59%/73% on AMC23/AIME24/AIME25. The claim is this shows strong evidence of benchmark contamination and evaluation fragility (which is disentangled  with strict and loose eval)\n\nThe core contributions are two-fold:\n1. Replacing fixed constants in contest problems with constrained variables to build symbolic templates (as done previously for GSM-8k, Putnam and MATH)\n2. Sample several feasible values per template; with 5/5 consistency scoring check across all sampled instantiations.\n\n[1]Shrivastava et al, Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- **Right problem, right lens.** Moving from one-shot correctness to multi-instance consistency tests structural understanding.\n\n- **Clear empirical signal.** The dataset is valuable and shows consistent, cross-model drops on variabilized sets, especially for small/medium RL-tuned models.\n\n- **Timely benchmark.** The paper situates VAR-MATH among dynamic/functional-variation work (e.g., GSM-Symbolic, LiveBench, Putnam-AXIOM) and brings symbolic variation to today’s AIME-level tasks.\n\nI think this would be really valuable as a drop-in substitute for these three datasets, and also provide more samples to the rather tiny original datasets."}, "weaknesses": {"value": "[Critical] I worry that the decline’s cause is misattributed. The evidence might not support benchmark contamination as the primary driver. I specifically state the alternative explanations which I worry might fit the data better (and how to remove these confounders):\n- **Strict drop apples-to-oranges metric.** The comparison compares AIME pass@1 against VAR-AIME 5/5 consistency. For fairness, compare strict VAR-AIME to a strict AIME defined as “correct only if all 5/5 sampling runs per problem is correct” with K=5 for strict VAR-AIME.\n- **Loose drop is sensitive to hardest variants.** Loose scoring inherits fragility: if a template’s variants differ in difficulty, strict/loose can over- or under-penalize depending on which variants dominate. I worry the templates extend upwards, making problems harder (slightly) and hence get small declines.\n- **Statistical significance check.** A one-sided t-test on bootstrapped runs can establish statistical significance. Many loose-score drops seem to lie comfortably within the standard-deviation band, the claim is not significant enough to be correct.\n\nActions: After aligning metrics and normalizing variant difficulty, test whether performance deltas fall within run-to-run variance to check whether core claims are true\n\n[Major] Benchmark construction is under-specified.\n- **Pipeline details would be nice.** The paper gives little concrete detail on the AIME/AMC → VAR-AIME/AMC conversion. Figure 2 outlines steps but lacks supporting text. Would like if the authors could document the full pipeline: how/when constants are replaced, how are problems sampled, checks done to ensure correctness, and how is evaluation done (fixed set or constantly sampled).\n- **Soundness checks.** Sec. 3.2 defines feasible sets and rounding, but checks to ensure correctness are missing in description. Do we know that no variant becomes ill-posed (multiple valid answers, degenerate geometry, non-integer outputs when integers are required)?\n- **K/M/N justification.** Authors state “up to five variants per problem” and report totals (183/126/130) but do not justify K (variants), M (generations), or N (bootstrap rounds). Consider fixing per-template K and average per template (not “up to five”) to avoid weighting results by larger-K templates. Using M=K to do strict-AIME could bridge some gap between original and VAR versions.\n\n[Minor] Analysis depth and presentation.\n- **Per-topic robustness.** Aggregate drops are informative, but you analyze problems in detail; please add per-topic strict/loose histograms and qualitative error clusters to show which subfields are brittle or stable.\n- **Training-regime separation.** RL-trained models dominate the narrative. Include strong SFT-only baselines (e.g., OpenThinker-3) side-by-side to quantify how much of the drop is RL-specific versus SFT math-tuning.\n- **Reduce repetition.** “Benchmark contamination” and “evaluation fragility” are repeated across the abstract, introduction, §3.1, and §4.2. Trim §3.1 and §4.2 –  that would provide space to add curation details tied to Figure 2."}, "questions": {"value": "Please address weaknesses above. If the critical weakness is addressed, I am happy to lean towards acceptance and if major weaknesses (soundness check) is addressed I would happily further upgrade my score to 8.\n\nOverall, I really like the benchmark: I think VAR-MATH is promising and could become a standard for robust math-reasoning evaluation – emphasizing the property practitioners actually need: consistency under controlled variation. However, I worry that the headline claim  about benchmark contamination and evaluation instability are not correct. Specifically, once metrics are aligned (pass@1 vs 5/5 strict) and variant difficulty normalized, the observed drops may shrink or fall within variance. If the the construction details and statistical tests are fixed; and the declines remain significant under fair comparisons, the case will be compelling!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8X2nCSiOQv", "forum": "gsCVfTW2AJ", "replyto": "gsCVfTW2AJ", "signatures": ["ICLR.cc/2026/Conference/Submission17398/Reviewer_crXC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17398/Reviewer_crXC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100272960, "cdate": 1762100272960, "tmdate": 1762927302069, "mdate": 1762927302069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}