{"id": "XSsI8KXOed", "number": 1709, "cdate": 1756909828377, "mdate": 1759898193371, "content": {"title": "Log-Normal State-Space Model", "abstract": "State space model (SSM) have emerged as a strong alternative to transformer owing to its linear-time complexity and state retention mechanism where the computation efficiency and memory capability are enhanced especially in long-sequence tasks. However, the features derived from state updates in SSM still exhibit weaker representation than those generated by self-attention in transformer. In this work, we propose a new architecture that preserves the linear-time efficiency of SSMs while enabling state-update features to approach the expressiveness of self-attention, thereby achieving both computation efficiency and memory enhancement. Our code is available at https://anonymous.4open.science/r/Log-Normal-State-Space-Model-8301/.gitignore", "tldr": "An efficient state space model achieving high performance without extra MLPs.", "keywords": ["Linear attention", "State-space model", "Log-normal distribution", "Long- range dependency", "Sequence modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e7376ab341675184c8f7d092a39c8d7ee4ed7d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends Linear Log-Normal Attention to SSM and validates its superiority compared to RWKV and Mamba."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly explain the design of log-normal in SSM."}, "weaknesses": {"value": "Experiments: No well-established Language Benchmark is tested. (in [lm eval harness](https://github.com/EleutherAI/lm-evaluation-harness)). Baselines are very limited: GLA, Gated Delta Networks, LongHorn are not included.\n\nWriting: the motivation of this work is based on the \"effectiveness\" of log-normal distribution in self-attention. However, this is not a well-know conclusion and the paper does no provide any evidence for that. and conclusion is claimed inappropriately: \"LNSSM matches the expressiveness of self-attention in transformers\" -> what is expressiveness? and it is expected that your performance outperform Transformer if you claim this but no experiments compared to Transformer is done."}, "questions": {"value": "refer to above weakness\n1. Can you validate the LNSSM performance in lm eval harness and add at least one more baseline?\n2. Can you show the \"effectiveness\" of log-normal distribution in attention like a Transformer with normal distributional attention is worse than the one with log-normal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tSY0t2Fe56", "forum": "XSsI8KXOed", "replyto": "XSsI8KXOed", "signatures": ["ICLR.cc/2026/Conference/Submission1709/Reviewer_6thV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1709/Reviewer_6thV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419999983, "cdate": 1761419999983, "tmdate": 1762915863415, "mdate": 1762915863415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Log-Normal State-Space Model (LNSSM), a novel architecture designed to bridge the performance gap between efficient State-Space Models (SSMs) and expressive self-attention mechanisms. The authors observe that the strong performance of modern SSMs (like Mamba and RWKV) relies not only on their time-mixing (recurrent) layers but also on intricately designed channel-mixing layers, which adds complexity and reduces interpretability. To address this, LNSSM leverages the theoretical insight that softmax attention weights approximately follow a log-normal distribution. By reformulating causal Linear Log-Normal (LLN) attention into a recurrent state-update form with an exponential feature map and a state decay mechanism inspired by RWKV, LNSSM aims to achieve the expressiveness of self-attention while maintaining the linear-time complexity and constant memory footprint of SSMs. Experimental results on language modeling (perplexity), question answering (BABILong), and long-form generation (ASQA, ELI5, WikiLarge) demonstrate that LNSSM outperforms several SSM baselines and remains robust with increasing sequence lengths, where standard transformers degrade."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is strengthened by a clear, critical observation: modern SSMs rely heavily on complex, specialized channel-mixing layers. The authors use this to motivate a highly controlled experimental setup, comparing all time-mixing methods using an identical, simple GPT-2 MLP. This elegantly isolates the variable of interest and proves that LNSSM's performance gains stem directly from its novel time-mixing core, not auxiliary architectural components. I quite like the methodology. \n\n* By creating a time-mixing layer powerful enough to work with a standard MLP, LNSSM offers a path toward more efficient, interpretable, and expressive sequence models, addressing a central challenge in the field often overlooked by others."}, "weaknesses": {"value": "* The comparison, while including strong baselines, could be more definitive. A comparison against the standard Mamba (or Mamba-2) block *with its original channel-mixing layer* is crucial to fully substantiate the claim that LNSSM achieves comparable performance with a simpler (GPT-2) MLP. The paper should state this crucial methodological choice explicitly and unambiguously in the main text of Section 5, not just bury it in a figure caption. This is by far my biggest concern. \n\n* All experiments are conducted at a relatively small scale (~72M parameters). The conclusions would be significantly stronger if supported by results at a larger scale (e.g., 100M+ parameters) to ensure the findings hold as model size increases.\n\n* The contribution of individual components in the overall LNSSM design (QK normalization, KV-shift, output gate, and most importantly, the state decay matrix A) is not systematically ablated. It is unclear how much each component contributes to the final performance.\n\n* The extensive presentation of hand-derived gradient formulas and custom CUDA kernel algorithms in the appendix, while impressive from an engineering standpoint, is atypical for a core conference paper and distracts from the primary scientific contribution. A high-level summary of the implementation challenge and its solution would suffice. Note that I have *not* lowered my score because of this."}, "questions": {"value": "* To directly validate the claim of reduced reliance on specialized channel-mixing, could you provide a performance comparison where LNSSM (with GPT-2 MLP) is directly compared against Mamba-2 with its original channel-mixing layer under identical settings and parameter budgets?\n\n* Table 4 shows a dramatic speedup with a custom CUDA kernel. Since custom kernels are standard for performance I wanted to ask if there's anything specific to LNSSM that allows for such speedups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "62m5kqpMig", "forum": "XSsI8KXOed", "replyto": "XSsI8KXOed", "signatures": ["ICLR.cc/2026/Conference/Submission1709/Reviewer_knEo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1709/Reviewer_knEo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614093238, "cdate": 1761614093238, "tmdate": 1762915863222, "mdate": 1762915863222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Log-Normal State-Space Models (LNSSM), a novel architecture that adapts the principles of Linear Log-Normal (LLN) attention for use within the State-Space Model (SSM) framework. LNSSM achieve superior performance on various downstream tasks compared to existing SSMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clarity and Readability: The paper is generally well-written, with a clear and logical flow that makes the core concepts accessible and easy to follow.\n\n- Novelty of Idea: The central concept of mapping the log-normal attention mechanism from LLN to the SSM framework is novel and theoretically interesting, potentially opening a new avenue for SSM design.\n\n- Empirical Results: The preliminary empirical results suggest that LNSSM is a promising approach, showing improved metrics over established baselines on the evaluated tasks."}, "weaknesses": {"value": "While the core idea is interesting, the paper suffers from several major weaknesses in its current form, spanning methodological rigor, conceptual clarity, and overall structure.\n\n- Structural Issues: The paper's organization hinders a clear understanding of its contributions.\n\n  - It lacks a dedicated Related Works section, making it difficult to situate the LNSSM within the broader landscape of SSMs (e.g., S4, Mamba, RWKV) and related attention mechanisms.\n\n  - The contributions are not explicitly enumerated.\n\n  - A Limitations section is absent, which is crucial for a balanced scientific presentation.\n\n  - Lack of line numbers\n\n- Lack of Ablation Studies: In Section 4.5, the authors introduce several implementation choices (e.g., KV-shift) in the final architecture. It is impossible to tell if the reported performance gains come from the core LNSSM contribution or from these auxiliary design choices. The paper needs ablation studies to isolate the impact of LNSSM. For example:\n\n  - What is the performance when replacing the LNSSM block with an S6 or Mamba block, while keeping all other architectural choices (like KV-shift) constant?\n\n  - What is the performance impact of systematically removing these other design choices (KV-shift, etc.) from the final LNSSM model?\n\n- Conceptual & Technical Clarity:\n\n  - Parallelizability: Appendix A incorrectly implies that SSMs cannot operate in parallel. Models like S4 are well-known to be parallelizable as a long convolution (computed efficiently with FFTs) for training/prefill.  Can LNSSM be formulated in a parallel, non-recurrent mode for efficient training, similar to S4 and Mamba? \n\n  - Comparison to LLN: The paper frames its contribution as applying a \"mathematical trick\" to LLN to achieve SSM properties (linear compute, constant state). However, it is unclear if the original LLN already possessed these properties. The paper must clarify the concrete differences in computational and memory complexity between LNSSM and the original LLN.\n\n  - Figure 4 is disorganized and hard to follow. The symbol 'g' is undefined.\n\n- Experiments: The experiments appear to be conducted at a relatively small scale. The ROUGE metrics in Table 3, for instance, are very low across all models, making it difficult to draw conclusions about practical effectiveness. The paper's claims would be substantially stronger if validated at a larger scale."}, "questions": {"value": "- Parallelism: Does LNSSM support a parallel (convolutional) training mode for efficient prefill, similar to S4? If not, how does its training efficiency compare?\n\n- Ablations: Can you provide ablation studies that isolate the performance gains of the core LNSSM block from other architectural choices like the KV-shift?\n\n- LLN Comparison: What are the precise differences between LNSSM and original LLN? Furthermore, what are the computational and memory complexity differences between LNSSM and the original LLN? If they are similar, an experimental comparison on downstream performance of those two methods is needed.\n\n- Figure 1: In Figure 1, were the causal convolution of Mamba and token shift of RWKV replaced for the comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y72lhaH8UZ", "forum": "XSsI8KXOed", "replyto": "XSsI8KXOed", "signatures": ["ICLR.cc/2026/Conference/Submission1709/Reviewer_hsxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1709/Reviewer_hsxx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884726291, "cdate": 1761884726291, "tmdate": 1762915862937, "mdate": 1762915862937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}