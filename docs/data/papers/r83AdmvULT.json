{"id": "r83AdmvULT", "number": 3933, "cdate": 1757570083153, "mdate": 1763750624890, "content": {"title": "Optimal Transport-Induced Samples against Out-of-Distribution Overconfidence", "abstract": "Deep neural networks (DNNs) often produce overconfident predictions on out-of-distribution (OOD) inputs, undermining their reliability in open-world environments. Singularities in semi-discrete optimal transport (OT) mark regions of semantic ambiguity, where classifiers are particularly prone to unwarranted high-confidence predictions. Motivated by this observation, we propose a principled framework to mitigate OOD overconfidence by leveraging the geometry of OT-induced singular boundaries. Specifically, we formulate an OT problem between a continuous base distribution and the latent embeddings of training data, and identify the resulting singular boundaries. By sampling near these boundaries, we construct a class of OOD inputs, termed optimal transport-induced OOD samples (OTIS), which are geometrically grounded and inherently semantically ambiguous. During training, a confidence suppression loss is applied to OTIS to guide the model toward more calibrated predictions in structurally uncertain regions. Extensive experiments show that our method significantly alleviates OOD overconfidence and outperforms state-of-the-art methods. Codes will be made public upon paper acceptance.", "tldr": "", "keywords": ["Out-of-distribution", "Overconfidence", "Optimal transport"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a29cf524a0ee3414ee3c65a47af5cbfc779b115.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes OTIS (Optimal Transport–Induced Samples), a training‑time regularization framework to mitigate overconfident predictions on OOD inputs. The core idea is to (1) encode training images into a latent space, (2) solve a semi‑discrete OT problem between a continuous base distribution and the discrete set of latent embeddings, which induces a power‑diagram (Laguerre) partition; (3) identify singular boundaries in the OT map as regions of semantic ambiguity; (4) synthesize proxy OOD samples via interpolation across those boundaries and decoding back to image space; and (5) apply confidence suppression loss (uniform‑target cross‑entropy) on the synthesized samples + standard cross‑entropy on ID data during training. \n\nThe paper presents improvements in OOD overconfidence metrics (MMC, FPR@95, AUROC) across multiple canonical datasets in the empirical results, and shows that training with OTIS strengthens standard test‑time detectors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Presentation of the pipeline is easy to follow and fairly well justified: Partition estimation, scoring boundaries (Eq. 9), OTIS synthesis, and training are easy to follow (Fig. 2, Sec. 3).\n2. The background knowledge for the semi‑discrete OT setup is carefully crafted with sufficient support. The Induced power diagram is also visually appealing and elaborative, giving good clarity to the geometric motivation on the selection of method. From the description, readers can have a clear picture on latent‑space structure and appreciate singular boundaries as a credible proxy for ambiguity.\n3. The quality for experiments is good. Benchmarks span small and large scale (CIFARs, SVHN, MNIST/FM‑NIST, ImageNet), with multiple OOD types (near‑OOD, synthetic corruptions, adversarial noise/samples).\n4. On many conditions the method outperforms baseline methods in the experiment setup. The margin is relatively clear, signifying the work's potential to be further applied in later researches.\n5. Training with OTIS consistently boosts MSP/ODIN/ReAct ROC curves (Fig. 6). \n6. Boundary ranking ablation and comparisons to latent/input interpolations are helpful (Fig. 5); histograms and OTIS visualizations support the “ambiguous but plausible” claim (Figs. 7–12).\n7. The idea of leveraging OT‑induced boundaries to synthesize targeted, ambiguous training examples seems broadly applicable beyond the specific detectors and datasets evaluated"}, "weaknesses": {"value": "1. The paper states a theoretical link (singularities <-> overconfidence), but provides no formal guarantee that the proposed angular deviation score identifies regions of high miscalibration. The result is a strong intuition with empirical support rather than a theorem (Sec. 2.2–2.3; Eq. 9).\n2. Eq. 11 defines inverse‑distance weights $\\lambda_i$; the implementation then weight is fixed to 0.5 (line 319). This is a non‑trivial discrepancy. Even if the hard-coded weight seems reasonable, proper explicit justification (and potential ablations) should be applied to amend for the discrepancy. This weakens the credibility of the results produced on more generic setups.\n3. The number of Monte Carlo samples used to estimate cell volumes (M), base distribution choice (Gaussian vs. uniform) in each experiment, and convergence criteria for offset optimization are not fully specified in the main text; only high‑level statements are provided (Sec. 3.2).\n4. While Table 2 shows improved OOD MMC on ImageNet, ID MMC rises substantially (to **88.93**), which may reflect over‑smoothing/confidence effects; a calibration analysis (ECE/Brier/NLL) would clarify the trade‑off."}, "questions": {"value": "1. For Table 3, how were “Adversarial Noise/Samples” generated (step sizes, budgets, initialization)? Some reported gains are striking—for example, CIFAR‑10 with adversarial noise FPR95 **0.66%** vs VOS **94.12%** (Table 3). Is there any intuition for such striking differences?\n2. How sensitive are results to the encoder/decoder architecture and reconstruction quality (e.g., using a WAE vs. the chosen AE)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1eF84CSDjj", "forum": "r83AdmvULT", "replyto": "r83AdmvULT", "signatures": ["ICLR.cc/2026/Conference/Submission3933/Reviewer_mC83"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3933/Reviewer_mC83"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761430314304, "cdate": 1761430314304, "tmdate": 1762917102463, "mdate": 1762917102463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Optimal Transport-induced OOD samples (OITS), a novel method for synthesizing OOD samples from ID labeled datasets to regularize the training process (tackling the overconfidence problem). OITS leverages ideas from the Optimal Transport (OT) literature, constructing OOD samples via latent embeddings near the singular boundaries, which are often aligned with the model's decision boundary. Overall, the paper provides detailed theoretical derivations and comprehensive empirical analysis validating the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "To the best of my knowledge, the motivating idea of this work is novel. By incorporating the semi-discrete OT problem into DNN training, the authors successfully capture a picture depicting the generalization of NNs.\n\nThe paper is clearly written, and I find the theoretical explanation in Section 2 is accurate and easy to follow for a wider audience. Empirical results also demonstrate the effectiveness of the proposed method, with several helpful visualizations."}, "weaknesses": {"value": "As shown in Tables 1 and 2, the proposed method suffers from a relatively high test error, indicating that some accuracy is sacrificed for robustness against OOD samples. Nevertheless, the proposed method still has on-par test error compared to other baselines, and the OOD robustness (measured by OOD MMC) is significant."}, "questions": {"value": "In line 157—Could the authors provide some explanations on why \"these regions often correspond to high-confidence mispredictions\"? Or, are there any links between the Optimal Transport problem and the training process of a NN? Any intuition would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4z5lHkzsFD", "forum": "r83AdmvULT", "replyto": "r83AdmvULT", "signatures": ["ICLR.cc/2026/Conference/Submission3933/Reviewer_QHM5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3933/Reviewer_QHM5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940040363, "cdate": 1761940040363, "tmdate": 1762917102295, "mdate": 1762917102295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method to reduce a classifier's confidence on out-of-distribution (OOD) samples. The high-level idea is to construct a set of samples on the semantic boundary, and then train the model to have low confidence on this set of samples. The authors propose to use semi-discrete optimal transport (OT) to construct the boundary. Specifically, the authors sample a set of ID samples, use an encoder to get their latent features, and get the boundary for each pair of samples using Eqn. (9). The authors test their method on some vision datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is fine. It is not hard to understand the paper\n2. The description of the OT method is clear"}, "weaknesses": {"value": "1. Motivation is not very clear. It is a natural idea to lower the model's confidence on boundary samples in order to lower its confidence on OOD samples. However, it is not clear to me why using the specific method proposed in this work. There seems to be a bunch of possible ways to construct the boundary samples, and the authors do not elaborate on why the specific method proposed here is more efficient. While the formulation looks reasonable, it looks quite complicated and I am not really sure what this extra complication buys us. \n2. Some important details are unclear. I might have missed it, but the two important details I fail to find in the paper are (a) how the encoder in Eqn. (6) should be trained, and (b) how the $n$ training samples after Eqn. (7) should be drawn. I think these two details are particularly important to the success of the propose method, so some extra elaboration is needed.\n3. Regarding the experiments, from Table 1 I notice that the proposed method always has a high test error, so it is not clear to me if the decrease in the confidence on OOD samples actually comes from this trade-off. I also think that a brief description of the compared baselines is necessary."}, "questions": {"value": "1. How is the encoder in Eqn. (6) trained?\n2. If the encoder is not well trained, how will it affect the performance of your method?\n3. How to choose $x_1,\\cdots,x_n$ in line 194?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FVcfB7hXUL", "forum": "r83AdmvULT", "replyto": "r83AdmvULT", "signatures": ["ICLR.cc/2026/Conference/Submission3933/Reviewer_u5rM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3933/Reviewer_u5rM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960322471, "cdate": 1761960322471, "tmdate": 1762917102094, "mdate": 1762917102094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the general issue of deep neural networks producing overconfident predictions on out-of-distribution (OOD) inputs. The authors propose to generate specific proxy OOD samples. The core idea is to identify regions of semantic\nambiguity in the latent space of an AE by leveraging the geometry of semi-discrete Optimal Transport (OT). They solve an OT problem between a base distribution and the latent embeddings of training data, in order to identify singular boundaries. Then, they generate OTIS by interpolating latent features near these boundaries of interest. The paper claims this geometrically grounded approach significantly reduces OOD overconfidence compared to SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- the conceptual link drawn between singularities in semidiscrete OT and semantic ambiguity is interesting. Targeting these specific regions for confidence suppression is potentially more founded than using heuristics like noise or generic outlier datasets \n\n- the method for generating OTIS is detailed and the sample creation between latent concepts should provide a more effective signal for training than random interpolations or generic augmentations (e.g. Figure 5) \n\n- strong performance in experiments"}, "weaknesses": {"value": "- The paper heavily relies on the theory of semi-discrete OT (Brenier potential, Laguerre cells) in order to define a notion of neighborhood (i.e., adjacency cell boundaries). However, the core measure of ambiguity is simply the angle between neighbor latent vectors, which is independent of the OT framework. Hence, the paper doesn’t clearly explain/justify  the additional complexity : why the OT-neighborhood concept is necessary over a much simpler and standard k-NN approach.\n\n- The primary goal is to mitigate overconfidence, (i.e, lead the model toward better calibrated predictions). While the work shows ID MMC remains high, this doesn’t eliminate  the possibility that the method makes the model uncalibrated on correct ID predictions due to the nature of the loss and the generated OoD inputs. It would be crucial in my opinion to report the ECE score on the ID samples as it may reduce overconfidence on ID inputs, which would open the discussion a little bit.\n\n- I might have missed some things, but there are some major complexity drawbacks potentially. Solving a semi-discrete OT problem for the entire training set (n=50k for CIFAR, n=1.2M for ImageNet) is certainly computationally demanding. Do the authors rely on solving OT on mini-batches of target points {$y_i$} ? If so, the geometric partition (Laguerre cells, boundaries) may be unstable and change with each batch. In this case, the paper should provide a theoretical error bound for this approximated domain/boundary relative to the true partition (over all the dataset for instance), using for instance Hausdorff distance. This may depend on the number of points, the dimension of the latent space and also probably involve some topological assumption [1] on the representativeness of the mini-batch. If not, the paper must provide a clear analysis of the computational complexity and demonstrate the feasibility and runtime of their OT algorithm on large-scale datasets. Not discussing this aspect is detrimental for the readers.\n\n- The method trains the model to reduce confidence on inputs deviating from the clean ID manifold, using semantic ambiguity. This might teach the model to also reduce confidence on inputs suffering from diverse domain shift such as common corruptions (e.g., CIFAR-10-C dataset). As modern architectures are well known to be overconfident on such inputs, it would be interesting to see MMC and ECE score on corruption benchmarks.\n\n[1] Jean-Daniel Boissonnat, Frédéric Chazal, and Mariette Yvinec. Geometric and topological inference. Vol. 57. Cambridge University Press, 2018."}, "questions": {"value": "The work introduces an interesting idea for generating targeted samples to reduce OOD confidence and shows promising results on the OOD MMC metric, however it has some important weaknesses\nin my opinion. The theoretical justification based on OT appears too complex for the actual mechanism employed and lacks comparison to natural alternatives (k-NN) and the absence of standard calibration metrics (ECE). The paper’s heavy reliance on dense OT and writing makes it difficult to understand the core mechanism, which is essentially an advanced latent-space mixup. Concerns about scalability/stability and potential negative impacts on robustness further weaken the submission. I am open to reconsidering my score if the authors can provide a convincing rebuttal that addresses these weaknesses, namely : \n\n- clarify the necessity of the OT framework\n- clarify the impact on ECE\n- clarify the complexity implications\n- clarify the standing on corruption/domain shift"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "nothing in particular"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vKb3vBCsI7", "forum": "r83AdmvULT", "replyto": "r83AdmvULT", "signatures": ["ICLR.cc/2026/Conference/Submission3933/Reviewer_rQDu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3933/Reviewer_rQDu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011834445, "cdate": 1762011834445, "tmdate": 1762917101810, "mdate": 1762917101810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Updates"}, "comment": {"value": "Dear Reviewers,\n\nWe are grateful for your thorough and insightful feedback. In this rebuttal, we respond to each comment point by point. In addition, we have updated the manuscript to include several changes that we believe are of broader interest:\n\n1. **Confidence Calibration on ID Data (Section 4.3):**  \n   We now report expected calibration error (ECE) on in-distribution test sets, showing that OTIS alleviates OOD overconfidence while preserving, and in some cases improving, calibration on ID data.\n\n2. **Effect of Base Distribution (Section 4.3):**  \n   We add a comparison between Gaussian and uniform base distributions under the same architecture, showing that OTIS is robust to the choice of base and that the uniform base serves as a strong default.\n\n3. **Effect of Autoencoder Depth (Section 4.3):**  \n   We study shallow, medium, and deep encoder–decoder architectures with a uniform base and show that OTIS remains effective across depths, with the 5-layer symmetric encoder–decoder providing a good balance between ID accuracy and OOD robustness.\n\n4. **Robustness to Common Corruptions (Appendix A.1):**  \n   Treating CIFAR-10/100 as ID data and CIFAR-10-C/100-C as OOD inputs, we demonstrate that OTIS significantly reduces OOD MMC and FPR95 and improves AUROC under a wide range of common corruptions.\n\n5. **Correction of the Interpolation Weight $\\lambda_i$ (Section 4.1):**  \n   We corrected the description of the interpolation weight $\\lambda_i$ so that it is fully consistent with its definition and usage in the main text.\n\nWe sincerely appreciate the time and effort you devoted to reviewing our work and helping us improve the paper.\n\nThank you for your kind consideration.\n\nBest,  \n\nAuthors"}}, "id": "qAfYe846Ic", "forum": "r83AdmvULT", "replyto": "r83AdmvULT", "signatures": ["ICLR.cc/2026/Conference/Submission3933/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3933/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission3933/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763750596979, "cdate": 1763750596979, "tmdate": 1763750596979, "mdate": 1763750596979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}