{"id": "lg6H2oJPky", "number": 14071, "cdate": 1758227945624, "mdate": 1759897392294, "content": {"title": "Online Learning and Equilibrium Computation with Ranking Feedback", "abstract": "Online learning in arbitrary and possibly adversarial environments has been extensively studied in sequential decision-making, with a strong connection to equilibrium computation in game theory. Most existing online learning algorithms are based on \\emph{numeric} utility feedback from the environment, which may be unavailable in applications with humans in the loop and/or with privacy concerns. In this paper, we study an online learning setting where only a \\emph{ranking} of a set of proposed actions is provided to the learning agent at each timestep. We consider both ranking models based on either the \\emph{instantaneous} utility at each timestep, or the \\emph{time-average} utility until the current timestep, in both \\emph{full-information} and \\emph{bandit} feedback settings. Focusing on the standard (external-)regret metric, we show that sublinear regret cannot be achieved with the instantaneous utility ranking feedback in general. Moreover, we show that when the ranking model is relatively {deterministic} (\\emph{i.e.,} with a small temperature in the Plackett-Luce model), sublinear regret cannot be achieved with the time-average utility ranking feedback, either. We then propose new algorithms to achieve sublinear regret, under the additional assumption that the utility vectors have a sublinear variation. Notably, we also show that when time-average utility ranking is used, such an additional assumption can be avoided in the full-information setting. As a consequence, we show that if all the players follow our algorithms, an approximate coarse correlated equilibrium of a normal-form game can be found through repeated play. Finally, we also validate the effectiveness of our algorithms via numerical experiments.", "tldr": "Hardness and positive results for online learning with ranking feedback. Together with equilibrium computation with ranking feedback.", "keywords": ["Online Learning", "Equilibrium Computation", "Human Feedback"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5659a152f30aab2c410264d1a8d7f818284bf682.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "A common setting for online learning (in the broadest sense) is adapting to feedback where utilities for different actions cannot be computed. Instead, learners may have access to ranking information where the adversary selects one or more actions among a set of actions. This work considers ranking feedback generated by the Plackett-Luce ranking model assuming ranking vectors from either instantaneous or time-averaged utility vectors. The authors formally establish that no-regret learning is not possible when the ranking is deterministic, i.e. actions are ranked in order of utility. When there is some temperature in the rankings, no-regret learning can be achieved in the full-information setting and in the bandit setting under an additional sub-linearity assumption on the utility vectors. This leads to a CCE as is standard in games where all participants employ a no-regret learning algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper motivates from a real phenomenon, giving the examples of RLHF or customers for a service providing a single instance of feedback where utility cannot be easily computed.\n- They provide formal definitions of their notions of utilities and the standard no-regret guarantees, as well as guarantees when no-regret cannot be achieved.\n- The paper appears theoretically sound."}, "weaknesses": {"value": "The paper motivates their problem from real cases, but then sets up a theoretical framework without demonstrating practical advantages/successes for the real problem they identify as the motivation. In my view, not establishing this connection limits its impact among practitioners in the motivating cases in the short or long term. For example, it is unclear to me how bandits with ranking feedback, as formalized here, applies to the setting of RLHF, which is presented as a motivating example for this work. \nThe authors provide limited experiments in a synthetic setting that validates their theoretical result is correct and the algorithm is implementable. However, their theoretical results do not seem novel or interesting, given the slew of similar results across various online learning papers, each with slight variations on the bandit / feedback type. Therefore, in my view, the experiments would need to demonstrate practical value for the paper to be accepted."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2n1nvcugw3", "forum": "lg6H2oJPky", "replyto": "lg6H2oJPky", "signatures": ["ICLR.cc/2026/Conference/Submission14071/Reviewer_7iCZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14071/Reviewer_7iCZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761087302396, "cdate": 1761087302396, "tmdate": 1762924550612, "mdate": 1762924550612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies an online learning when ranking feedback, where the ranks of a given set of actions is returned instead of the conventional rewards. The authors show that sublinear regret cannot be achieved if the ranking function is highly deterministic, controlled by the parameter $\\tau$. On the other than, if the ranking function is more random, sublinear regret is possible. This is an interesting result as in general human rankings are not consistent.  Further, a convergence results in Nash equilibrium is established."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is written well and easy to follow. The motivations are clear and ranking model (PL) is standard and popular. Various problem settings are considered, e.g., bandit, full information, and Nash equilibrium. The derivations are clear and results are as expected. Solid and extensive work."}, "weaknesses": {"value": "The ranking model is somewhat limited as having a consistent reward model in practice is very rare, e.g., RLHF in LLM. Is it possible to consider a problem where the reward function itself is sampled from a set of reward functions? This setting might be more realistic. In might be interesting to test inconsistent reward in the simulation."}, "questions": {"value": "Does the theorem apply to the setting where the reward function itself is sampled from a set of reward functions? Is this setting somewhat related to the choice of $\\tau$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P7IFrr8WGm", "forum": "lg6H2oJPky", "replyto": "lg6H2oJPky", "signatures": ["ICLR.cc/2026/Conference/Submission14071/Reviewer_XTgL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14071/Reviewer_XTgL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930371770, "cdate": 1761930371770, "tmdate": 1762924550053, "mdate": 1762924550053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies no-regret algorithms under ranking feedback. In the usual no-regret setting, the algorithm sees the utility of the chosen action/ counterfactual utilities of all actions at the end of each round. However, in the ranking setting, the feedback is in the form of relative rankings of the actions, based on the Plankett Luce model, i..e. a softmax over some underlying ground truth utilities (that can be adversarially chosen each round). The paper studies two models of rankings, one based on instantaneous utilities, i.e. with myopic/ short lived agents who only live in each round and the other based on the time average or cumulative utilities over time. The main results of the paper are to characterize under what conditions no-regret learning is possible. \n\nFirst, the paper observes that without noise in the ranking process (i.e. higher temperatures in the Plankett-Luce model), it is impossible for any algorithm to separate fundamentally different utility functions from each other, leading inevitably to high worst case regret -  this is intuitively true when considering purely ordinal information. Next, they observe that with sufficient, i.e. linear variation in time of the ground truth utility, it is again impossible for the learner in the instantaneous ranking setting to distinguish between different utilities.  These results are matched by constructive results showing that no-regret learning is indeed possible with sufficient (but not too high) randomness in the PL model, and with sublinear variation in the utilities over time, with the latter assumption being dropped in the full information setting.\n\nThe algorithmic results are based upon a natural algorithmic procedure that finds a natural approximate utility that best explains the observed rankings - “natural” in the sense that it inverts the PL model based on empirical observations. The best guess utilities thus obtained are fed through any no-regret algorithms from a blackbox class of “consistent” algorithms that behave roughly similarly on similar utility sequences, a property satisfied by popular no-regret algorithms such as FTRL."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The main strength of the paper is that it studies a natural and well-motivated problem that was hitherto unsolved. This problem has no shortage of consequential direct applications and is solvable without requiring fundamentally new machinery. That said, the paper is well written and has non-trivial analysis for both the upper and lower bounds results. Further, the assumptions for the positive results are well justified by the lower bounds constructions."}, "weaknesses": {"value": "NA"}, "questions": {"value": "How does the paper connect to the offline problem of learning static ground truth utilities from ranking information? Is there a technical connection between algorithms for this problem and the algorithmic techniques in your paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aPFesP87HU", "forum": "lg6H2oJPky", "replyto": "lg6H2oJPky", "signatures": ["ICLR.cc/2026/Conference/Submission14071/Reviewer_aUGV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14071/Reviewer_aUGV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145911823, "cdate": 1762145911823, "tmdate": 1762924549597, "mdate": 1762924549597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies online learning and equilibrium computation when the learner only observes rankings rather than numeric utilities, in a fully adversarial setting. This connects this to finding coarse correlated equilibria in normal-form games. The authors consider two ranking models, i.e. InstUtil Rank and AvgUtil Rank, under both full-information and bandit feedback. They first prove hardness results: with InstUtil Rank, any algorithm suffers $\\mathcal{O}(T)$ regret for constant temperature $\\tau$. With AvgUtil Rank, when $\\tau$ is very small, sublinear regret is still impossible. They then show that positive results are possible if utilities vary slowly over time, designing estimation-based algorithms that (i) convert ranking feedback into approximate numeric utilities via a Plackett–Luce estimator, and (ii) feed these estimates to a generic numeric-feedback online learner (e.g., FTRL/MWU). With InstUtil Rank and sublinear variation, they obtain sublinear regret in both full-information and bandit settings; with AvgUtil Rank and full information, they can drop the variation assumption and get sublinear regret so long as the underlying numeric-feedback algorithm is stable in cumulative utilities (Assumption 6.1). Finally, they extend the algorithms to multi-player normal-form games and show that if every player follows their no-regret ranking-based learner, the time-averaged play converges to an approximate coarse correlated equilibrium with explicit error bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper cleanly formulates adversarial online learning with ranking feedback and explicitly distinguishes instantaneous vs time-averaged ranking models and full-information vs bandit settings.\n* The authors provide matching-style hardness results and positive results that reveals the tradeoff between hardness and possibility.\n* The paper shows how ranking-based no-regret dynamics imply convergence to approximate coarse correlated equilibria in general normal-form games (Theorem 7.2 and 7.3)."}, "weaknesses": {"value": "* Many positive results require the utility sequence to have sublinear variation (e.g. Assumption 4.2), these conditions are quite strong in fully adversarial environments.\n* The hardness results for AvgUtil Rank hinge on $\\tau$ being extremely small, the paper does not fully clarify how sharp these thresholds are.\n* Experiments are only briefly mentioned and relegated to the appendix. From the main text, it’s hard to see how the algorithms behave in practice."}, "questions": {"value": "* Is sublinear variation $P(T)$ information-theoretically necessary for sublinear regret, or is it mainly an artifact of the current estimation + analysis?\n* Can you characterize more explicitly how the regret scales with $\\tau$ in the intermediate regime between “very small” (hardness) and “\\mathcal{O}(1)” (positive results)?\n* The results assume a known $\\tau$ and exact PL model. How sensitive are the algorithms and regret bounds if $\\tau$ is misspecified or the rankings are only approximately PL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JHBx0ZKUyc", "forum": "lg6H2oJPky", "replyto": "lg6H2oJPky", "signatures": ["ICLR.cc/2026/Conference/Submission14071/Reviewer_xsDY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14071/Reviewer_xsDY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196764367, "cdate": 1762196764367, "tmdate": 1762924549156, "mdate": 1762924549156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}