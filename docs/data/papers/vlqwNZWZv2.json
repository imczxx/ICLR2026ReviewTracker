{"id": "vlqwNZWZv2", "number": 17707, "cdate": 1758279557680, "mdate": 1759897159273, "content": {"title": "$\\texttt{APOLLO}$: A Self-Guided Multi-Agent System for Scientific Article Generation inspired by Human Thinking", "abstract": "Automatic generation of Wikipedia-like articles through Retrieval-Augmented Generation (RAG) has recently gained increasing attention. \nWhile recent advances in Large Language Models (LLMs) show considerable promise for synthesizing complex information, current RAG-based systems suffer from two fundamental limitations: they often rely on shallow retrieval strategies, leading to redundant content, and they lack effective mechanisms for factual verification and content organization.\nTo address these challenges, we present $\\texttt{APOLLO}$, a multi-agent framework specifically designed to generate high-quality, comprehensive articles with citations to the given sources. \n$\\texttt{APOLLO}$ simulates the iterative research and editorial process of human contributors through a set of specialized agents that collaboratively retrieve, fact-check, and structure information. \nTo evaluate our method, we introduce SciWiki-2k, a dataset comprising 2,000 high-quality Wikipedia articles spanning 20 scientific domains. \nCompared to baseline methods, $\\texttt{APOLLO}$ produces articles with significantly improved structural coherence, content diversity, and factual accuracy. Human evaluations further establish the practical value of our approach for generating trustworthy, comprehensive articles.", "tldr": "We introduce Apollo, a multi-agent framework that generates comprehensive and fact-checked Wikipedia-style scientific articles with citations, outperforming existing methods in accuracy, structure, and diversity.", "keywords": ["LLM-based Multi-agent Systems", "Long-form text generation", "Knowledge Graph Construction", "Fact Verification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6853d586a2a429c3a38497928a228eb9f24ceb4f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a multi-agent system, named APOLLO, for automated scientific article generation. The system accomplishes this task through three phases: knowledge curation, outline generation, and article generation. The authors also construct a new evaluation dataset, SciWiki-2k, consisting of 2,000 Wikipedia articles across 20 scientific fields. Experimental results demonstrate that APOLLO outperforms existing methods, such as STORM and OmniThink."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper models the human scientific writing process (including exploration, synthesis, and reflection) as a multi-agent collaborative framework. By introducing a Writer-Reviewer loop, it simulates the \"generate-review-revise\" process in academic writing.\n2. APOLLO organizes external knowledge resources through an iterative knowledge graph construction, which guides subsequent retrieval and generation. This mechanism improves information diversity and topic coverage compared to traditional static retrieval.\n3. The authors constructed the SciWiki-2k dataset and proposed two new metrics: Hallucination Rate and Coverage Rate.\n4. The paper conducted a multi-faceted evaluation (including automatic metrics, LLM-as-judge, and human evaluation) and compared it with several baseline methods."}, "weaknesses": {"value": "1. The paper does not demonstrate APOLLO's performance in domains with complex factual conflicts or sparse knowledge, nor does it analyze failure cases. For example, the Reviewer agent's behavior when encountering conflicting evidence or its stability under low-quality search results are not discussed.\n2. The paper claims that its framework can simulate human reflective patterns, but its specific implementation, the Research Question Generator analyzing the knowledge group (KG) and generating questions, seems more like a pre-set heuristic than true reflection. The paper does not provide a specific example demonstrating how the KG evolves before and after iteration, nor does it provide a concrete example demonstrating how the agent identifies underexplored entities and asks new questions based on the structural changes in the KG. While ablation studies demonstrate the module's effectiveness, they fail to explain why it works, making the core claim of reflectiveness less convincing.\n3. The authors claim that APOLLO is inspired by human thinking, but this analogy remains at the process level (e.g., iteration and reflection) and lacks empirical mapping to cognitive psychology or information behavior theory. The lack of supporting literature and theoretical explanation makes the \"human-inspired\" claim seem conceptual rather than substantive.\n4. The paper proposes two new metrics to measure factual accuracy, but these rely on an LLM for automated claim verification. Using an LLM to judge the factuality of content generated by another LLM raises questions about its reliability. The paper provides no information on the accuracy of the verifier LLM itself, nor does it discuss the potential cascading errors that may arise from this evaluation approach."}, "questions": {"value": "1. The authors mention that APOLLO captures topic structure through a knowledge graph. How adaptable is this knowledge graph to different domains (e.g., Physics vs. Social Science)? Does it require a domain-specific relational schema?\n2. In the Reviewer agent's feedback loop, how does the system determine whether all feedback has been resolved? Does it use automatic consistency checking or rely solely on a maximum number of revisions?\n3. Are there potential biases in the SciWiki-2k dataset (e.g., differences in article length and citation density across different domains)? Do these biases affect the model's performance across different disciplines?\n4. Have the authors evaluated APOLLO's transferability to non-Wikipedia domains (e.g., arXiv, PubMed, or policy documents)? If not, are the method's generalizability claims too strong?\n5. In multi-agent collaboration, are there instances of inter-agent conflicts (e.g., a Research Question Generator and a Reviewer suggesting opposing directions)? How does the system resolve conflicts or coordinate weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i3HwClnTjt", "forum": "vlqwNZWZv2", "replyto": "vlqwNZWZv2", "signatures": ["ICLR.cc/2026/Conference/Submission17707/Reviewer_Dtcm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17707/Reviewer_Dtcm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034368711, "cdate": 1761034368711, "tmdate": 1762927545513, "mdate": 1762927545513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces APOLLO, a multi-agent framework for automated scientific article generation. First, the system gathers and structures information through retrieval-augmented generation (RAG) to build a comprehensive knowledge graph (KG). Specialized agents continuously refine this graph, ensuring its completeness and accuracy. During writing, a writer agent and a reviewer agent collaborate in iterative loops to produce rich, well-supported, and trustworthy manuscripts. In addition, authors release a new dataset and two novel metrics for assessing article quality. Empirical experiments demonstrate that APOLLO outperforms existing baselines in coverage, diversity, and factual reliability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A good dataset has been introduced, serving as a robust benchmark that enables future work to evaluate their results more accurately and fairly.\n* The engineering logic for seamlessly integrating RAG, knowledge graphs, and human habits into the agent system is well-rounded and flows coherently."}, "weaknesses": {"value": "* The paper relies solely on the SciWiki-2K dataset; additional benchmarks are needed to substantiate its performance claims.\n* The paper remains unclear whether the generated articles can genuinely drive scientific discoveries, as their novelty has not been discussed.\n* While the application is intriguing, the methodological contribution seems incremental; the reviewer agent and writer agent pairing, for instance, introduces no discernible mechanistic innovation."}, "questions": {"value": "* Table 4 reveals performance gaps across KG-construction strategies, yet no clear pattern emerges LightRAG is not uniformly superior. Adding further KG-building methods as baselines would help clarify whether certain design choices consistently pay off.\n* How does this work differ from Virtual-Scientists[1]? I think the authors need to add experiments that compare their method with this one.\n* Table 5 uses the number of unique URLs to gauge how much novel retrieved information APOLLO uncovers during knowledge curation. This metric requires clarification; I don't see how the number of unique URLs relates to novelty.\n\n[1] Su H, Chen R, Tang S, et al. Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System[J]. arXiv preprint arXiv:2410.09403, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zy5U3iYM3F", "forum": "vlqwNZWZv2", "replyto": "vlqwNZWZv2", "signatures": ["ICLR.cc/2026/Conference/Submission17707/Reviewer_kna6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17707/Reviewer_kna6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549651920, "cdate": 1761549651920, "tmdate": 1762927545035, "mdate": 1762927545035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a multi-agent framework for scientific wikipedia article generation. The framework is composed of three stages: (1) knowledge curation, constructing knowledge graphs based on retrieved snippets for a given topic, (2) outline generation, using LLMs to generate the article outline based on the curated knowledge graphs, and (3) article generation, generating the final article based on the expanded knowledge graph and the outline."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The task of automatically generating long-form articles is interesting and challenging, while the paper needs a lot more work to be published."}, "weaknesses": {"value": "- While generating long-form articles is interesting and challenging, there is no justification for scientific wikipedia articles. The specific challenge to generate scientific wikipedia articles is missing. The framework is mostly a complex combination of other methods. It is unclear what research question the paper is trying to solve, and it looks more like a \"product\".\n\n- The paper should have more implementation details to get higher reproducibility. This is becasue (1) the multi-agent seems very complex with a lot sperate modules to control, (2) it is unclear how to validate the alignment between the topics and their corresponding wiki article for the benchmark, (3) how to merge knowledge graphs is also missing, at least the intuition of the \"LLM-based normalization\" should be present in the paper, and (4) how to evaluate quality of generated articles with LLM-as-judge, the model and prompt need to be explained.\n\n- For evaluation in the paper, my most concern is human evaluation and the proposed metrics. (1) It is not clear or empirically validated why the proposed metrics are good enough to measure citation quality. (2) While human evaluation for long-form generation research is important, there is no details for human evaluation. Information about annatators, interfaces, etc. and annotator agreement needs to be disclosed. \n\n- For the benchmark in the paper, there may be data contamination because Wikipedia is the most common training corpus for LLMs. There is no effort to investigate this issue. If the models already trained on the collected articles, it is not meaningful to design another multi-agent system to write the articles. A better way is to use articles that have never been seen in the training process by the experimented LLMs to avoid data contamination. \n\n- The writing of the paper needs more work. For example (1) justification and explanation of the capabilities are missing for Table 1, (2) it is unclear how to validate the alignment between the topics and their corresponding wiki article for the benchmark, and (3) there is no definition for quality aspects in evaluation.\n\nMinor:\n\n- It would be nice to have more LLMs involved in the experiments rather than only 4o-mini, especially for outline generation and final article generaion.\n\n- \"Scientific article generation\" is misleading, while it the task in this paper should be scientific wikipedia article generation.\n\n- Line 040 needs a reference.\n\n- Larger texts in all figures.\n\n- Line 098 and Line 105 need a full stop.\n\n- Line 350 needs a new line."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4PuLdfuFTp", "forum": "vlqwNZWZv2", "replyto": "vlqwNZWZv2", "signatures": ["ICLR.cc/2026/Conference/Submission17707/Reviewer_y7nK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17707/Reviewer_y7nK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996327649, "cdate": 1761996327649, "tmdate": 1762927544590, "mdate": 1762927544590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents APOLLO, a multi-staged, multi-agent, and iterative system for generating comprehensive Wikipedia-like scientific articles given only a topic keyword. This multi-agent system begins by iteratively aggregating information into knowledge graphs through two collaborative agents that identify gaps, generate research questions, and synthesize targeted search queries over multiple exploration cycles. The resulting knowledge graphs are then used to generate a hierarchical outline for the article. For each section of the outline, specialized agents retrieve and filter the most relevant paragraphs (called snippets) from the collected information. Finally, the article is drafted and iteratively refined through collaboration between writer and reviewer agents, where the reviewer identifies factual inconsistencies and unsupported claims, and the writer revises the content accordingly until all claims are properly verified against cited sources. For evaluation, a benchmark dataset SciWiki-2k containing 2,000 Wikipedia articles across 20 scientific domains is collected as ground truth. The evaluation reveals that APOLLO outperforms baselines, achieving 9.2% higher information diversity, 7.1 points better coverage rate, and 18% lower hallucination rate. Experiments on APOLLO using either cached scientific papers or live information from the internet show that APOLLO performs well in both settings. Finally, human evaluation by domain experts confirms that APOLLO produces accurate, well-organized, and comprehensive scientific content."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposed a novel system which combines dynamic retrieval, structured memory, reflective thinking, and fact verification, mimicking the steps researcher study a subject and how they iteratively refine that with the help of the reviewers.\n2. The aggregated knowledge is stored in the form of graphs, which is an efficient and faithful way to describe the connections between them.\n3. The evaluation metrics, diversity, coverage, hallucination, covered how a good system for such article generation should behave and then was validated with human evaluation. Also, ablation studies show each component contributes (w/o Reflection, w/o Filter).\n4. The paper introduces SciWiki-2k, a large benchmark dataset containing 2,000 high-quality Wikipedia articles across 20 scientific domains, which provides a valuable resource for evaluating article generation systems and can be used by future research.\n5. The empirical results show substantial improvements over competitive baselines: APOLLO achieves 9.2% higher information diversity, 7.1 points better coverage rate, and 18% lower hallucination rate compared to the best-performing baselines (oRAG, STORM, OmniThink).\n6. The system demonstrates practical flexibility by working successfully with both cached scientific corpora (offline deployment) and live web search (online via Brave API), showing that APOLLO can be deployed in different real-world scenarios depending on the application needs.\n7. The writer-reviewer collaboration loop with iterative refinement (up to 3 cycles per section) represents a significant innovation over one-shot generation methods, explicitly verifying that claims are supported by cited sources and reducing hallucinations substantially."}, "weaknesses": {"value": "1. Computational cost and efficiency are not discussed. The paper does not report the total cost (API calls, compute time) or the time required to generate one article. With 4 agents, iterative processes, and up to 135 queries, the system likely incurs significant computational expense, but no cost-benefit analysis or scalability evaluation is provided. This makes it difficult to assess the practical feasibility of deploying APOLLO at scale.\n2. The evaluation scope is limited. Only 100 topics (SciWiki-100) are used for main experiments and only 20 articles receive human evaluation. More critically, all topics are scientific in nature, raising questions about whether APOLLO can generalize to non-scientific domains such as humanities, social sciences, current events, or popular culture.\n3. The paper uses fixed values (m=3 iterations, 10 questions per iteration, rmax=3 revisions) without ablation studies to justify these choices. Additionally, the system applies the same iteration count to all topics regardless of complexity, which may be inefficient for simple topics and insufficient for complex ones. An adaptive approach that adjusts based on information gain or topic complexity could improve efficiency.\n4. There is limited discussion of failure cases or what types of errors persist in the generated articles. Understanding when and why the system struggles (e.g., which topics are challenging, how errors propagate from KG to outline to article) would provide valuable insights for future improvements and help users understand the system's limitations."}, "questions": {"value": "1. Report the total number of API calls, compute time, and estimated cost per article for APOLLO and baseline methods. Provide a cost-benefit analysis showing the tradeoff between article quality and computational expense. This would help readers assess the practical feasibility of deploying APOLLO in different scenarios and make informed decisions about resource allocation.\n2. Expand evaluation to non-scientific domains. Test APOLLO on topics from humanities, social sciences, current events, or popular culture to demonstrate generalization beyond scientific articles. Even a small-scale evaluation (20-30 topics) across diverse domains would strengthen claims about the system's broader applicability and reveal domain-specific challenges.\n3. Conduct ablation studies on key hyperparameters. Systematically evaluate different values for iteration count (m), number of questions per iteration, and revision cycles (rmax) to justify the current choices. Additionally, explore adaptive mechanisms that adjust these parameters based on topic complexity or information gain, which could improve both efficiency and effectiveness.\n4. Provide qualitative error analysis. Include case studies of failure modes, discussing specific examples where APOLLO produces poor articles or where the reviewer agent fails to catch hallucinations. Analyze how errors propagate through the pipeline and identify which types of topics or sections prove most challenging. This analysis would offer valuable insights for future work and help users understand when to trust the system's outputs.\n5. Consider evaluating with open-source models. While using GPT-4o-mini is reasonable, testing with open-source alternatives (e.g., Llama-3.3-70B) would improve accessibility and reproducibility for researchers with limited API budgets. Comparing performance and costs across different model choices would also provide useful guidance for practitioners.\n\nThese are just suggestions, and there is a chance that I did not fully grasp the material, so please only do what you see fit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1s1Rl9SIm6", "forum": "vlqwNZWZv2", "replyto": "vlqwNZWZv2", "signatures": ["ICLR.cc/2026/Conference/Submission17707/Reviewer_SsBT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17707/Reviewer_SsBT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762176346591, "cdate": 1762176346591, "tmdate": 1762927544028, "mdate": 1762927544028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}