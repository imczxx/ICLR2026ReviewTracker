{"id": "YH1wtz2pbo", "number": 19470, "cdate": 1758296521707, "mdate": 1759897037249, "content": {"title": "LONG-HORIZON REASONING AGENT FOR OLYMPIAD- LEVEL MATHEMATICAL PROBLEM SOLVING", "abstract": "Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems.\nHowever, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines.\nTherefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems.\nFurthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO. Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists.\nCode and model will be released to benefit future research.", "tldr": "We release Intern-S1-MO, a hierarchical math agent built by SFT and agent RL to solving IMO-level math problems", "keywords": ["Large Language Model", "Reinforcement Learning", "Mathmatical Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d84e7732437806c5b526b15b397070b114356ef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Intern-S1-MO, a long-horizon mathematical reasoning agent that aims to address the bottlenecks in large reasoning models, specifically the limitation of context length. During solving, the agent performs multi-round hierarchical reasoning that integrates reasoning, summarization, and verification, supported by a lemma-based memory management that effectively extends the reasoning depth. The paper further proposes OREAL-H with two critical components: hierarchical credit assignment and conjugate reward modeling to handle noisy process verification. In summary, Intern-S1-MO improves the reasoning precision and stability, which achieves state-of-the-art performance on Olympiad-level mathematics benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes an effective approach to overcome the context length limitation in large reasoning models.\n\n2. Intern-S1-MO demonstrates strong overall performance on Olympiad-level mathematics benchmarks, surpassing previous SOTA."}, "weaknesses": {"value": "1. Although the results are good, this paper is not well-written, and the presentation needs to be improved. Many details are missing in the current version, making it very hard to clearly understand.\n\n2. The paper lacks sufficient description of the details of the proposed methods and training procedures. For example, the lemma-based memory management is a key contribution of the paper. However, the explanation of this method is not detailed enough, making it hard for readers to fully understand and reproduce it. Please refer to Questions for more details.\n\n3. The information provided in the Appendix is incomplete and does not fully support the claims made in the main text.\n\n4. It would be better to include some examples to help understand the process.\n\n5. The proposed method requires several rounds to produce answers, while the other baseline seems to produce answers with only one inference. The experiments didn't mention the inference time, making the comparison unfair.\n\n6. There is no code released during the review process. The authors only mention \"Code and model will be released to benefit future research.\""}, "questions": {"value": "1. The details of maintaining a structured lemma library are missing. During round $k$ of long-chain trajectories, how is a specific lemma chosen and explored from it? Also, how is an intermediate lemma decided to be updated in the memory system?\n\n2. In Figure 2, it's unclear whether the input question in rounds 2 through $n-1$ is the same as the question in round 1, or if it includes partial solutions from previous rounds. Are the partial solutions stored in the Lemmas Library? How do the scores of the lemmas help in the reasoning process? Are they just references for the LLM, or do they have other roles? Also, the reason for keeping used lemmas in the library needs more explanation. Does the LLM solve the question from scratch in each round, and if so, does this library help the model skip steps that have already been solved?\n\n3. In Figure 2, are the reasoner and summarizer the same model? On the right of the figure, there is a loop labeled \"final solution draft,\" but without any explanation.\n\n4. At line 413, is \"Multi-Tune Reasoning\" different from the \"multi-round reasoning process\" that includes memory management?\n\n5. Is it also effective when applying the multi-round hierarchical reasoning framework (without finetuning) to other LLMs?\n\n6. What is the critic $V$? Is it another LLM? What is the prompt for $V(s_t)$? How to train it?\n\n7. What training dataset is used for the Verifier? Are the Lemma Verifier and Process Verifier the same models, or are they the same models with different prompts? How are they trained? Is a Process Verifier needed during inference?\n\n8. In the section on \"Conjugate Reward Modeling for Noisy Process Verification (PV)\", it is mentioned that PV feedback is noisy. Does this mean that for a given lemma, PV can provide highly variable feedback for the lemma's correctness? Are there any examples of process verification? Are there any ablation studies conducted for the conjugate reward modeling? How effective is the conjugate reward model in denoising PV feedback?\n\n9. In the Implementation section, which datasets (solution-based problems and proof-based problems) are used for RL? Do they include the problems used for evaluation? What is the cost of RL training for the LLM, including the number of questions in the training dataset and the number of fine-tuning iterations?\n\n10. How many rounds are executed during the reasoning process? Is there a predefined limit, or does it depend on when the LLM performs the \"commit answer\" action?\n\n11. What is the inference time for each model during the overall evaluation in Table 1?\n\n12. Why is there no evaluation of IMO2025 in the ablation study in Table 2?\n\n\n\nTypo and errors:\n\n1. Some of the labels in the bars of Figure 1a are unclear.\n2. At line 186, ProcessBench needs a citation.\n3. At line 219, represent -> represents\n4. At line 302, Appendix 5 is not provided.\n5. At line 346, full implementation details, including scoring rubrics and problem filtering criteria, are not provided in Appendix 1.\n6. In the appendix, at line 863, \"Translated with DeepL.com (free version)\" should be omitted.\n7. In the appendix, Listing 1: LEMMA SEARCH, based on the content in the listing, it seems more like \"Generating Solutions\" than \"LEMMA SEARCH\".\n8. In the appendix, Listing 2: Memory Management, it only describes how to extract lemmas from the output of LLM. It doesn’t include how the model chooses or utilizes the lemmas in the library.\n9. \"?)\" citation error at line 442.\n10. At page 10, there are duplicate references for \"Intern-s1:...\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GWwbFp4fFi", "forum": "YH1wtz2pbo", "replyto": "YH1wtz2pbo", "signatures": ["ICLR.cc/2026/Conference/Submission19470/Reviewer_bzvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19470/Reviewer_bzvB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658672369, "cdate": 1761658672369, "tmdate": 1762931381569, "mdate": 1762931381569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces Intern-S1-MO, a multi-agent framework designed to overcome the context length limitations that hinder large models on complex IMO-level math problems . The system operates via a multi-round loop of reasoning, summarization, and verification, storing intermediate proof steps as \"lemmas\" in a compact memory to break the constraints of single-pass inference. Trained using a novel reinforcement learning framework called OREAL-H, this agent achieved a silver-medal-equivalent score of 26 out of 35 on the non-geometry problems of IMO2025.\nHowever, regarding the paper's two core claims (\"overcoming context limitations\" and \"the multi-agent architecture\"), the description of the key techniques is relatively vague, making it difficult to fully understand the author's specific methodological design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript addresses one of the most challenging problems in the AI reasoning field: performing ultra-long-horizon reasoning on IMO-level tasks. This explicitly breaks through the bottleneck of current large reasoning models limited by context window size, representing a highly valuable research direction."}, "weaknesses": {"value": "1. The paper describes Intern-S1-MO as a multi-agent system comprising a Reasoner, Summarizer, and Verifier. However, it is unclear whether these are three independently fine-tuned models or a single LRM playing three different roles via distinct prompts. If it is a single model, this raises a significant concern about self-confirmation bias. For example, would the model, when acting as the \"Verifier,\" be biased toward favorably evaluating a proof it just generated as the \"Reasoner\"?\n\n2. The update mechanism for the OREAL-H reinforcement learning gradient is underspecified. The RL reward (from the PV) is based on the Reasoner's final output. It is unclear if this gradient is only used to update the Reasoner's policy, or if it is also backpropagated to update the model's capabilities when performing the \"Summarizer\" and \"Verifier\" roles.\n\n3.  The implementation of the \"Theorem Verifier\" (for intermediate lemmas) is vague. The paper states it uses \"parallel sampling,\" which sounds like a self-consistency-based voting mechanism. However, the exact operational details are not provided. Furthermore, it is unclear why two different methods are necessary. If the trained PV is capable of \"identifying the indices of steps containing logical fallacies,\" why is it not also used to verify the intermediate lemmas?\n\n4.  The central claim of Intern-S1-MO is to overcome context limitations via its \"Lemmas Libarary.\" However, the described process (Figure 2) involves feeding \"Question + entire Lemmas Libarary\" into the next reasoning round. This does not seem to solve the core problem. What happens when, after $n$ rounds of reasoning, the \"Lemmas Libarary\" itself grows to exceed the model's context window?\n\n5.  The source and filtering criteria for the initial cold-start dataset (Section 3.2) are not specified, nor is the RL sampling strategy.\n6.  The paper claims to use \"512K tokens to solve a single problem,\" but this figure lacks crucial context. The authors do not specify the associated inference time, computational cost, or provide an efficiency comparison against single-pass, long-context models.\n\n7.  The use of only 5 non-geometry problems for the IMO2025 benchmark represents a very small sample size. While achieving 26/35 points is an impressive result, any conclusions about SOTA performance drawn from such a limited sample have finite statistical robustness."}, "questions": {"value": "See the weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tbWFFOK40E", "forum": "YH1wtz2pbo", "replyto": "YH1wtz2pbo", "signatures": ["ICLR.cc/2026/Conference/Submission19470/Reviewer_ktR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19470/Reviewer_ktR9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718925685, "cdate": 1761718925685, "tmdate": 1762931381083, "mdate": 1762931381083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "While closed frontier labs have achieved great performance in olympiad-level competitions, the agentic workflow structure and training details underlying these successes remain underexplored in academia. Moreover, single-turn and solely prompt-based usage of frontier reasoning models falls far short of the reported success rates. This work therefore proposes open-source agentic workflows and RL training strategies for solving olympiad-level math competitions.\n\nThe proposed agentic workflow aims to decompose complex reasoning into a scaffold of multiple agents. First, hierarchical reasoning decomposition is performed through multiple rounds of maintaining and updating a lemma database, leveraging a solver stage (for reasoning), a summarization stage that converts the reasoning trace into structured lemma memory, and a lemma verification stage. Next, another agent writes the complete solution given the refined lemma database. Finally, the complete solution undergoes multiple rounds of verification loops through interaction with a verifier agent to produce the final solution.\n\nFurther, the authors propose a new RL framework called OReal-H, which is claimed to improve the workflow's performance. The resulting solution, called Intern-S1-MO, is shown to outperform all single-turn performances of frontier reasoning models. Furthermore, the paper argues that even the distilled 8B model derived from Intern-S1-MO, called Intern-S1-mini-MO, performs on par with these frontier reasoning models and even outperforms all of them on IMO 2025."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The reported performance of the proposed solution (Intern-S1-MO) is strong, especially its performance on IMO, where it achieves a score of 26/35 (excluding problem 2 of the exam, which was a geometry problem), while the best single-turn frontier model achieved a score of 14.\n2. Distilling an 8B model from Intern-S1-MO is an impactful contribution to the open-source community, as surprisingly, when the proposed workflow is equipped with this relatively small model, it achieves a performance of 17/35, still outperforming the best single-turn frontier model.\n3. The idea of tracking past reasoning exploration explicitly with a structured lemma database and refining this database through multiple rounds represents clever and novel decisions in agentic workflow design for math agents."}, "weaknesses": {"value": "1. **Clarity of the proposed agentic workflow could be improved:** The prompts provided in Appendix A, while helpful, are not sufficient to fully understand how each component of the workflow operates. The prompts used for the verifier component shown in Figure 2 are not provided across all stages. Additionally, it is unclear what prompt is used for obtaining the \"long-chain trajectory\" as it appears in the middle column of Figure 2 (rounds 2,...,n-1). Including all these prompts would be valuable for better understanding the proposed workflow's soundness. Regarding lemma search (line 158), the paper mentions \"refining the model via prompt engineering and targeted training, explicitly enabling it to produce partial deductive progress in single-turn attempts,\" but further explanation of how this targeted training is conducted and how the data is curated for this step would strengthen the paper. Similarly, for process verification (lines 183-185), while the paper describes training \"a specialized process verifier using synthetic cold start data with outcome supervision\" and employing DPO, additional details about the training process, data curation, and the advantages of this approach would be beneficial for readers to fully appreciate the contribution.\n\n2. **The proposed RL framework would benefit from additional clarification:** While the paper attempts to provide theoretical understanding of the RL framework, certain aspects of the derivation could be clearer. Specifically, π_φ is not defined explicitly, and assuming it represents the high-level policy, the rigorous transition from Equation 1 to Equation 3 could be better explained. The explanation of the dedicated critic V^H(s_t) would benefit from more details on its practical estimation and implementation. Including pseudocode with high-level abstraction of the RL implementation and a final simplified RL loss would help readers better assess the framework's soundness. Additionally, more discussion on data curation across different stages of RL training would be valuable.\n\n3. **Cost-performance profiling and comparison with simpler baselines:** A comparison with a simple baseline of solver and verifier agents in a loop would provide important context, as [1] demonstrated that such a baseline can achieve 35/35 on IMO 2025. Since the proposed workflow in Figure 2 already contains this component in \"round n,\" this ablation would help clarify how this component performs standalone. Understanding the computational cost of running only \"round n\" compared to the full Intern-S1-MO workflow at comparable IMO performance levels would provide valuable insights into the efficiency gains of the proposed approach.\n\n4. **Evaluation details require expansion:** The implementation details and evaluation rubrics are currently missing, with the paper referencing Appendix 1 (line 346), which appears to be absent. Including these details would strengthen the reproducibility of the work.\n\n5. **Ablation studies could be more clearly presented:** The meaning of \"Single-turn with Agents\" requires clarification. Additionally, explaining how complexity can be added incrementally in an agentic workflow context would help readers better understand the incremental benefits of each component. These clarifications would be valuable to address during the rebuttal phase.\n\nReferences: \n[1] Huang, Yichen, and Lin F. Yang. \"Gemini 2.5 pro capable of winning gold at imo 2025.\" arXiv preprint arXiv:2507.15855 7 (2025)."}, "questions": {"value": "Questions:\n1. Could you please elaborate on what each model in Table 2 represents? Specifically, what is the exact workflow structure for each of these models?\n2. Could you provide more details on how the verifier is trained in the final round of the workflow? How is the data curated for this training? I have similar questions regarding the \"targeted training\" mentioned in line 159.\n3. Could you provide all the prompts used in the agentic framework shown in Figure 2 across all rounds and components (agents)? This would greatly improve the clarity and reproducibility of the agentic framework.\n4. What is the high-level pseudocode for the final RL implementation, and what is the final loss function used? This would help clarify the proposed OReal-H framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n063q1lLs8", "forum": "YH1wtz2pbo", "replyto": "YH1wtz2pbo", "signatures": ["ICLR.cc/2026/Conference/Submission19470/Reviewer_mMoy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19470/Reviewer_mMoy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110998982, "cdate": 1762110998982, "tmdate": 1762931380501, "mdate": 1762931380501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors proposes a lemma-memory, multi-round agent with distinct Reasoner/Summarizer/Verifier roles to push beyond single‑context limits. Also authors Introduces OREAL‑H: hierarchical credit assignment + a conjugate (Beta‑Bernoulli) reward for noisy process verification The results are strong with reports state-of-the-art scores on several math benchmarks and IMO non‑geometry 26/35 with up to ~512K tokens/problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated, tackling the important and clearly defined challenge of long-horizon reasoning. Its technical contributions are presented through a clear agent architecture and are supported by a broad, rigorous empirical evaluation. This evaluation includes thorough ablations that definitively quantify the contribution of each component, making the paper's methodological strengths both easy to grasp and convincingly validated."}, "weaknesses": {"value": "The empirical evaluation, though extensive, suffers from a lack of clarity regarding the primary performance metric. The paper states it uses a \"pass@1\" score, defined as the \"expected score from the best single attempt,\" but derives this from 16 rollouts per problem. This methodology effectively reports a \"best-of-16\" result, which artificially inflates performance compared to a true single-sample evaluation and makes it difficult to compare against baselines that may not have been evaluated with an equivalent sampling strategy. A clearer justification for this protocol and a demonstration that all baselines were compared under identical conditions is necessary. Furthermore, the claim of superior performance is difficult to fully assess without a discussion of computational fairness. The agent's architecture, which consumes approximately ~512K tokens/problem through parallel verification and multi-round reasoning, represents a significant computational budget. It is unclear whether the baseline models, which are predominantly single-pass, were allocated a comparable \"thinking budget\" in terms of total tokens, rollouts, or verification calls. A comparison normalized for computational cost would provide a more rigorous foundation for the performance claims."}, "questions": {"value": "*Q1*. The paper argues that lemmas extend reasoning beyond context limits. Could you illustrate this with a concrete example from your experiments? For instance, describe a problem where the agent discovered a non-obvious intermediate result like a specific invariant or inequality that was not in the initial reasoning trace? \n\n*Q2*. Can authors explain how storing any single lemma fundamentally altered the strategic possibilities in the next round?\n\n*Q3*. A qualitative comparison of proof strategies generated under the conjugate reward versus a hypothetical raw k/n reward would be very insightful for understanding method practical effect. What specific, observable changes in the agent's proof-writing style did this training incentive produce? For example, did you see a trend towards more concise and robust arguments? \n\n*Q4*. The system involves a trade-off between exploration, summarization, and verification. Was there a heuristic or a learned halting policy to decide when further exploration was unlikely to be fruitful and it was time to commit to a final answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xkpabODxrQ", "forum": "YH1wtz2pbo", "replyto": "YH1wtz2pbo", "signatures": ["ICLR.cc/2026/Conference/Submission19470/Reviewer_taYP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19470/Reviewer_taYP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762826259741, "cdate": 1762826259741, "tmdate": 1762931380079, "mdate": 1762931380079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}