{"id": "Y89550WiZ1", "number": 22692, "cdate": 1758334536353, "mdate": 1759896852026, "content": {"title": "Benchmarking In-context Experiential Learning Through Repeated Product Recommendations", "abstract": "To reliably navigate ever-shifting real-world environments, agents\nmust grapple with incomplete knowledge and adapt their behavior through experience. \nHowever, current evaluations largely focus on tasks that leave no ambiguity,\nand do not measure agents' ability to adaptively learn and improve as they accrue experience.\nWe exemplify the need for in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. \nWe curate BIEL: a benchmark that combines i) rich real-world products from Amazon, ii) a diverse collection of user personas to represent heterogeneous yet latent preferences, and iii) a LLM user simulator powered by the persona to create realistic and interactive trajectories. \nWe observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context experiential learning capabilities.", "tldr": "", "keywords": ["In-context learning", "Experiential Learning", "Multi-episode learning", "Multi-turn learning", "Uncertainty resolution", "Interactive learning", "Sparse feedback", "Recommendation", "Dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3848571050aed9506f3dc22016c2b6be66dfdcfc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark for evaluating \"In-Context Experiential Learning\" (BIEL) through repeated product recommendations. The authors argue that current evaluations fail to capture agents' ability to adapt and improve based on previous interactions, especially in ambiguous environments. They introduce a dataset combining Amazon products, diverse user personas, and an LLM-based user simulator to assess how well agents can learn through experience and handle uncertainty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The exploration of agent behavior in ambiguous scenarios is an important research question. The motivation to build a benchmark that measures the ability of agents to adapt to evolving environments is highly relevant.\n\n2. Using recommendation systems as a testbed for in-context experiential learning is an intuitive choice. The complexity of user preferences and dynamic product landscapes provides a challenging environment for agent evaluation."}, "weaknesses": {"value": "1. A core claim of the paper is that current evaluations fail to address ambiguity in agent tasks. However, the design of this benchmark does not clearly incorporate ambiguity in the environment. Instead, the focus appears to be on diversity, which does not fully capture the ambiguity in the agents' decision-making process.\n\n2. The authenticity of the benchmark's behavior is questionable. LLMs are known to produce hallucinations, and since personas are generated by LLMs and used as user simulators, the reliability of the feedback cannot be guaranteed without additional verification or quality checks.\n\n3. User interaction history is critical for user modeling in recommendation systems. The lack of this in the benchmark simplifies the recommendation scenario too much, potentially overlooking the complexity of real-world interactions that affect user behavior and preferences.\n\n4. The paper tests a narrow set of models ( there are more model families can be explored such as Qwen, Kimi, DeepSeek, Doubao, GLM) and does not include reasoning models, which are more powerful. Additionally, the scale of the data tested seems insufficient for generalizable conclusions.\n\n5. The experimental results do not provide sufficient analysis of the models' performance over time or detailed insights into the failures observed. A more comprehensive analysis of the models’ behavior and further experiments are needed to strengthen the claims."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FYnotIlexO", "forum": "Y89550WiZ1", "replyto": "Y89550WiZ1", "signatures": ["ICLR.cc/2026/Conference/Submission22692/Reviewer_Ebit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22692/Reviewer_Ebit"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725633310, "cdate": 1761725633310, "tmdate": 1762942339819, "mdate": 1762942339819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BIEL (Benchmark for In-context Experiential Learning), a benchmark designed to evaluate the ability of large language models to learn from experiential experiences in conversational product recommendations. Experiments on various proprietary and open-weight models reveal that current LLMs exhibit little to no improvement across rounds, highlighting the challenge of building agents capable of in-context experiential learning. While this paper is meaningful and well-written, it still lacks several necessary discussions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Timely discussion about the experiential learning problem in the recommendation scenarios.**\nThe paper highlights a crucial yet overlooked problem for current LLMs: in-context experimential learning. Moreover, this paper discusses experiential learning under the recommendation scenarios, which is practical and reasonable.\n\n**Comprehensive experimental setup.**\nBIEL provides a large-scale, systematically generated environment, spanning multiple user personas and product domains, allowing broad coverage for reproducibility and scalability.\n\n**Good paper writing.**\n\nThe motivation of this paper is clear, and the whole paper is easy to understand."}, "weaknesses": {"value": "**Lack of discussion on existing conversational recommender systems.**\n\nA similar and standard research era in recommender systems is the conversational recommender system. However, this paper lacks the necessary discussion about this research era. The multi-turn recommendation paradigm is actually a common setting in conversational recommender systems, requiring discussion [1].\n\n**Lack of sufficient analysis about the poor performance of existing advanced LLMs.**\n\nExisting works demonstrate that advanced LLMs can perform well on multi-turn recommendation. However, this paper provides the opposite point of view about this. It would be necessary to discuss the failure of advanced LLMs, especially by comparing with existing works [2].\n\n**Lack of discussion about the faithfulness of the user simulator.**\n\nOne crucial part of making the benchmark faithful enough comes from the design of the user simulator. Although this paper initializes these user simulators with real user profiles, it also requires a discussion about how close these simulated users actually align with real user behaviors and how to avoid information leakage when using LLMs as the simulator [3].\n\n[1] Towards Deep Conversational Recommendations. NeurIPS 2018.\n\n[2] Large language models as zero-shot conversational recommenders. CIKM 2023.\n\n[3] How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation. WWW 2024."}, "questions": {"value": "Refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yyh96txhfj", "forum": "Y89550WiZ1", "replyto": "Y89550WiZ1", "signatures": ["ICLR.cc/2026/Conference/Submission22692/Reviewer_PiVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22692/Reviewer_PiVa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923492143, "cdate": 1761923492143, "tmdate": 1762942339284, "mdate": 1762942339284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BIEL, a benchmark designed to evaluate large language models’ (LLMs) in-context experiential learning capabilities through repeated product recommendation tasks. The benchmark constructs realistic shopping episodes using Amazon product data, user personas, and an LLM-based interactive user simulator. Agents conduct multi-turn dialogues to infer latent user preferences and recommend products. The benchmark supports diverse settings, varying user and product dynamics, and measures performance via regret, question-asking behavior, and calibration metrics. Experiments on state-of-the-art models (GPT-4o, Gemini-2.5, Claude) show limited improvement across episodes and poor uncertainty calibration."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Strengths:\n- This paper presents a novel task design for LLM study. The focus on learning across episodes is a great plus, as this is very important for practical deployment. However, this is largely unexplored.\n- The dataset size is also large, including ~71K products, ~2K choice sets, and ~1M personas, which can enable scalable evaluation across diverse domains. \n- This paper also provides a multi-model evaluation showing no meaningful improvement over episodes and poor uncertainty calibration. This points out an interesting direction for future research.\n- The framework supports variable customer and category configurations. This can allow multiple experimental regimes. \n- The work also provides more promises: human-constructed questions show that improvement is possible, and further validate the task difficulty."}, "weaknesses": {"value": "Weakness:\n- My biggest concern is that I could not find a user study or evaluation with real human preferences to validate the benchmark realism. I think this is critical to ground the benchmark in real human studies.\n- It'd be great if the authors could discuss how the choice of regret, stars, and text feedback can impact the results.\n- It'd be helpful to have pure human baselines. I understand that this can be costly, but at least mention this in the future work can be helpful to inform future studies."}, "questions": {"value": "Please see the weakness points. The main concern is about the real human study for grounding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EQ33sEpUDe", "forum": "Y89550WiZ1", "replyto": "Y89550WiZ1", "signatures": ["ICLR.cc/2026/Conference/Submission22692/Reviewer_Vp1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22692/Reviewer_Vp1X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942651880, "cdate": 1761942651880, "tmdate": 1762942338880, "mdate": 1762942338880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a benchmark targeted at learning to make product recommendations through multiple interactions with a single user or multiple users. The benchmark specifically targets learning over a series of interactions to move past the single step learning paradigm commonly used in RLHF for instruction following. The benchmark is designed to be interactive by relying on persona-conditioned LLM proxy humans that are able to answer questions the recommender agent poses or to react to the recommender agent's product recommendation. Performance measures rely primarily on persona-specific LL-Judge scores assigned to Amazon products, which is they used to compute regret. Several experiments with naive implementations of LLM agents that recommend products are used to demonstrate there is value to researching in this space as personalized product recommendation over the course of customer interactions does not improve with the number of interactions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem, which is the ability to conduct personalized LLM research in interactive settings.\n- The benchmark is grounded in real Amazon product data.\n- There is a large number of different \"people\" and products to evaluate a LLM on.\n- The environment is interactive and dynamic, so the proxy humans are impacted by the decisions of the learning agent.\n- Based on the presented experiments, the problem needs active work as LLMs don't inherently have the ability to improve recommendations through increased interactions with the user.\n- Despite a few typos, the paper is easy to read and follow."}, "weaknesses": {"value": "- The main weakness of this paper is that the personas and the LLM-as-a-Judge scoring approach are not well validated.\n     - The paper the personas come from was used to identify limitations with current approaches to creating personas by identifying issues and biases with the set of 1M personas introduced, which are used in this paper. These personas were found to be biased, which means the they are not overly unique, limiting the impact of having 1M personas.\n     - The only measure of the LLM-as-a-Judge's scoring performance is whether the judge is consistent. However, consistent and accurate/realistic are two different things. Therefore, it is not clear how reliable the benchmark's performance metrics are. For example, the performance of LLM judges applied to the PRISM dataset (https://arxiv.org/pdf/2404.16019) varies greatly depending on the participant predicting for and ranges from an accuracy of 0 - 100.\n     - Without well validated personas and scoring metrics, it is difficult to understand the expected size of the sim2real gap, which means it is challenging to know how likely results developed on the benchmark will generalize to learnings that hold up in the real world.\n- The benchmark set up ignores the important aspect that human users may not be willing to answer 20 questions to help an agent make a product recommendations, and the willingness of a customer to answer questions will be customer specific. This reduces the realism of the human proxies, and therefore misses a key aspect of applying learnings from this benchmark to the real world.\n- The benchmark does not appear to have any mechanism by which to introduce noise in the learning signal. As the real world and humans especially are full of noise, this makes it tricky to understand how well learnings from the benchmark will generalize to the real world.\n- Small things: \n     - Figure 6 does not appear to be referenced in the main body of the paper\n     - ORACLE results should be plotted in Figure 8 to make the performance gap very clear."}, "questions": {"value": "- The second use case for the benchmark is not making complete sense to me: \"..the agent repeatedly sells a fixed set of products to a stream of new customers. Here, the goal shifts to identifying how these products compare relative to another across the diverse distribution of customers.\" It is not clear to me how this differs from the first, personalization, use case. Can you please elaborate?\n- It is not completely clear the role humans played in the \"Robustness check\" detailed in Append A.5. Did the human play the role of agent, including making a product recommendation? What was used as the ground truth for the true product to recommend?\n- In Section 5.1 \"Comparing Base Models\", what is the learning set up? Are the LLM agents simply conditioned on the interaction history with the proxy human customers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4cJSPKfxG", "forum": "Y89550WiZ1", "replyto": "Y89550WiZ1", "signatures": ["ICLR.cc/2026/Conference/Submission22692/Reviewer_dbCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22692/Reviewer_dbCB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943401795, "cdate": 1761943401795, "tmdate": 1762942338596, "mdate": 1762942338596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}