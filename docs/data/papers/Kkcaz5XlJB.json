{"id": "Kkcaz5XlJB", "number": 25416, "cdate": 1758367858165, "mdate": 1759896721852, "content": {"title": "AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI", "abstract": "Goal changes are a defining feature of real-world multi-turn interactions, yet current agent benchmarks primarily evaluate static objectives or one-shot tool use. We introduce $\\textbf{AgentChangeBench}$, a benchmark explicitly designed to measure how tool augmented language model agents adapt to mid dialogue goal shifts across three enterprise domains. Our framework formalizes evaluation through four complementary metrics: Task Success Rate (TSR) for effectiveness, Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency. AgentChangeBench comprises of 590 task sequences and five user personas, each designed to trigger realistic shift points in ongoing workflows. Using this setup, we evaluate a mix of proprietary and open source models and uncover sharp contrasts obscured by traditional pass@k scores. Our findings demonstrate that high raw accuracy does not imply robustness under dynamic goals, and that explicit measurement of recovery time and redundancy is essential. AgentChangeBench establishes a reproducible testbed for diagnosing and improving agent resilience in realistic enterprise settings.", "tldr": "We present a benchmark that stress-tests agents on explicit goal-shifts in dual-control, multi-turn dialogs. We also add sequence-annotated scenarios spanning multiple service domains, personas and goal-shift based evaluation metrics.", "keywords": ["benchmark", "multiturn", "goal-shift", "robustness", "agents", "evaluation", "llm"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c915ee0d1b420cbcd944d8353796982627e4fc9.pdf", "supplementary_material": "/attachment/97a53d76e26d9905382f775adfcb870275422de0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a new agentic benchmark, focusing on goal shift."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It is a correct observation that other established benchmarks focus on static goals in a conversation, and they follow a golden path towards that goal. This is unrealistic for many real life applications. The paper proposes a much more realistic benchmark."}, "weaknesses": {"value": "1. It is unusual for a benchmark paper not to show any results using the existing models. Why did you put all the results in Appendix? As is, it does not read like a paper. This is the biggest weakness. Conclusions mention some experiments but they do not exist in the main paper. I suspected whether I am reading a draft version of the paper.\n2. On top of this, I'd have preferred at least a baseline approach for the authors to tackle the goal shift during the conversation."}, "questions": {"value": "No questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5OKq4QAoh1", "forum": "Kkcaz5XlJB", "replyto": "Kkcaz5XlJB", "signatures": ["ICLR.cc/2026/Conference/Submission25416/Reviewer_kDAv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25416/Reviewer_kDAv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922393153, "cdate": 1761922393153, "tmdate": 1762943426814, "mdate": 1762943426814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a critical gap in current benchmarks for conversational agents (LLM agents): most existing evaluations assume static user goals throughout the dialogue, overlooking the frequent goal changes that occur in real-world applications. The motivation is well-justified, as dynamic goal shifts are common in practical scenarios such as banking, retail, airline, and education. Agents must be able to detect, adapt to, and recover from these changes to be truly effective.\n\nMain Contributions:\n\n1. Novel Evaluation Dimension: This work is the first to systematically test LLM agents’ adaptability to mid-dialogue goal changes.\n2 . Task Coverage: The authors construct 590 multi-turn tasks spanning four domains and five user personas, each with explicit goal shifts.\n3. Methodological Framework: The paper introduces four complementary evaluation metrics: Task Success Rate (TSR), Tool Use Efficiency (TUE), Tool Call Redundancy Rate (TCRR), and Goal-Shift Recovery Time (GSRT).\n4. Empirical Study: A cross-model evaluation reveals significant differences in adaptability and efficiency that are not captured by traditional pass@k metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Multi-Domain Coverage: The benchmark covers four major real-world domains (banking, airline, retail, education), with diverse task types that align with enterprise needs.\n2. Multi-Turn Dialogue & Goal Shift: By emphasizing multi-turn tasks and explicit goal shifts, the benchmark simulates realistic business workflows where user needs evolve during the conversation. This is more challenging and meaningful than traditional single-turn tasks.\n3. Detailed Persona Design: Although the personas are relatively mild, the paper offers a nuanced segmentation of user behaviors (e.g., polite, curious, anxious, efficiency-oriented). This facilitates a more granular analysis of agent performance across different user preferences."}, "weaknesses": {"value": "1. Incremental Advancement: The benchmark is primarily an extension of existing tool-use benchmarks, adding goal shift and persona dimensions. It lacks exploration of more advanced topics such as autonomous agent planning, multi-agent collaboration, and automatic goal recognition.\n2. Lack of Extreme Scenarios: The benchmark does not systematically test for edge cases such as security boundaries, exception flows, or adversarial attacks.\n3. Inconsistent Task Counts: The paper inconsistently reports the total number of tasks (590 vs. 565), and table statistics (e.g., Table 4) do not match the descriptions, which may confuse readers and cast doubt on the benchmark’s scale and coverage.\n4. Statistical Ambiguity: Several tables lack clear definitions of their scope (e.g., domain, model, experimental set), making it difficult to interpret the data."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t6Ddp402Kr", "forum": "Kkcaz5XlJB", "replyto": "Kkcaz5XlJB", "signatures": ["ICLR.cc/2026/Conference/Submission25416/Reviewer_iMah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25416/Reviewer_iMah"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094052252, "cdate": 1762094052252, "tmdate": 1762943426602, "mdate": 1762943426602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on an important and practical problem: in real-world agent settings, users tend to shift their goals during interactions with agents. To evaluate how well agents can adapt to such goal shifts, this work introduces AgentChangeBench and proposes four complementary metrics to assess agents’ adaptability. Experimental results show that strong accuracy does not necessarily imply robustness under dynamic goals."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is both important and insightful, as in real human-agent interactions, users indeed tend to shift their goals.\n2. The work is easy to follow, and the proposed benchmark provides a valuable foundation for future research in this area."}, "weaknesses": {"value": "1. Motivation. The authors should elaborate further on the motivation for introducing persona. From my perspective, the main focus of this paper is on goal shifting, and the use of persona appears to serve the purpose of making the benchmark more realistic. A more detailed explanation of this design choice is necessary.\n2. Experiments. The authors present one main experiment in the appendix; however, several questions remain.\n(1) How do open-source models such as the Qwen3-series and GPT-OSS perform on AgentChangeBench?\n(2) How do recently released large reasoning models (e.g., OpenAI o4-mini, DeepSeek-R1) perform?\n(3) The authors mention pass^k; how does model performance vary across different values of k?\n3. Writing. The main experiment should be placed in the main text for better readability. In addition, some important information, such as the model used to simulate the user and details like the temperature setting, is missing."}, "questions": {"value": "1. In the abstract, the authors mention pass@k scores, but in the main text only pass^k appears. Further clarification or additional experiments are needed.\n2. There are several relevant works on human–agent interaction, such as [1,2]; the authors are encouraged to cite and discuss them in the related work section.\n3. In line 144, the authors state that *Transitions are triggered naturally.* How is naturally ensured? Is there any human judgment involved in verifying that the goal shifts occur naturally?\n4. The authors should include an error analysis to better understand agents’ weaknesses in goal-shift scenarios, which could guide future improvements in this direction.\n\n[1] LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey  \n[2] CollabLLM: From Passive Responders to Active Collaborators"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LwL7iNRlC4", "forum": "Kkcaz5XlJB", "replyto": "Kkcaz5XlJB", "signatures": ["ICLR.cc/2026/Conference/Submission25416/Reviewer_96FC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25416/Reviewer_96FC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762443388038, "cdate": 1762443388038, "tmdate": 1762943426354, "mdate": 1762943426354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentChangeBench, a novel benchmark designed to evaluate Large Language Model (LLM)-based conversational agents in scenarios where user goals shift dynamically during a multi-turn interaction. The authors correctly identify a significant gap in existing benchmarks (e.g., τ-bench, AgentBench), which typically assume static user objectives. The benchmark comprises 590 tasks across four domains (banking, retail, airline, education), incorporates five distinct user personas, and features explicit, annotated goal sequences. Beyond traditional success-rate metrics, the paper proposes a multi-dimensional evaluation framework including Task Success Rate (TSR), Tool Usage Efficiency (TUE), Tool Call Redundancy Rate (TCRR), and Goal Shift Recovery Time (GSRT). An empirical study across three major model families (GPT-4o, Gemini-2.5-Flash, Claude-3.7-Sonnet) demonstrates that this framework reveals performance trade-offs that a single metric would miss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel and Relevant Contribution: The core idea—evaluating agents on their ability to handle mid-conversation goal shifts—is highly relevant and addresses a critical shortcoming in current evaluation paradigms. This focus directly improves the realism of agent assessment for enterprise deployment.\nComprehensive and Systematic Benchmark Design: The benchmark is well-constructed, with substantial scale (590 tasks), coverage across multiple realistic domains, and a clear, declarative task schema. The integration of five distinct personas adds a valuable layer of complexity and realism.\nMulti-Dimensional Evaluation Framework: The proposed metrics (TSR, TUE, TCRR, GSRT) are a significant strength. They move beyond binary success/failure to provide insights into operational efficiency, cost (via redundancy), and adaptability, which are crucial for real-world application.\nRigorous Empirical Evaluation: The cross-model evaluation is thorough, testing three state-of-the-art models. The results convincingly demonstrate the benchmark's utility by surfacing clear differences in model performance across the various dimensions (e.g., Claude's fast recovery vs. Gemini's banking struggles vs. GPT-4o's balance).\nClarity and Reproducibility: The paper is generally well-written and structured. The inclusion of a detailed appendix with task examples, persona definitions, tool specifications, and full results supports reproducibility and transparency."}, "weaknesses": {"value": "Statistical Reporting and Interpretation:\nLack of Statistical Significance: The results are presented as point estimates (e.g., TSR percentages) without any measures of variance or statistical significance testing (e.g., confidence intervals, p-values). Given that each task was run only 3 times, the stability of these metrics is unclear. Claims about model superiority (e.g., \"Claude-3.7-Sonnet recovers fastest\") would be significantly strengthened by statistical validation.\nInconsistent Precision: Some percentages are reported with two decimal places (e.g., 98.58%), while others are whole numbers or one decimal place. This can give a false impression of precision. A consistent reporting standard should be applied.\nInterpretation of \"Redundancy\": The very high TCRR values (e.g., 89.14% for GPT-4o in Retail-new) are noted but not deeply interpreted. Is this a failure of the agent, a limitation of the tool design, or an inherent property of the retail domain? A brief discussion on the root cause of this observed redundancy would be valuable.\nWriting and Presentation Issues:\nSection Ordering: The paper currently jumps from the Introduction to Section 4 (\"Empirical study\") before covering Related Work (Section 2) and Methodology (Section 3). This is confusing and disrupts the narrative flow. The standard order (1. Intro, 2. Related Work, 3. Method, 4. Experiments) should be restored.\nClarity on \"New\" vs. \"Old\" Sets: The distinction between \"new\" and \"old\" task sets (e.g., Airline-new vs. Airline-old) is critical to understanding the results but is not explicitly defined in the main text. The reader must infer from Section 3.2 that \"old\" refers to adapted tasks from τ²-bench, while \"new\" are the contributions of this work. This should be clarified upfront in the methodology or experiment setup.\nConceptual Limitations (Acknowledged by Authors):\nThe authors rightly acknowledge limitations, such as the \"benign\" nature of the personas and the explicit (not implicit) nature of goal shifts. While acknowledged, these points remain weaknesses of the current benchmark version, as they limit its ability to test the full spectrum of adversarial or subtle real-world interactions."}, "questions": {"value": "Statistical Significance: Given the relatively low number of runs per task (n=3), what is the estimated variance of your key metrics (TSR, GSRT)? Have you conducted any statistical tests to confirm that the performance differences between models are significant?\nMetric Weighting: The weights for TSR (Communicate Info: 0.25, Action: 0.45, NL Assertion: 0.30) and TUE (Tool Correctness: 0.6, Param Accuracy: 0.4) are presented as determined by \"empirical analysis\" and \"operational cost analysis.\" Could you provide more detail on this analysis? Was it domain-specific, and how sensitive are the overall results to these chosen weights?\nGeneralization and Scale: The benchmark focuses on customer service domains with structured APIs. How do you envision this framework generalizing to more open-ended domains (e.g., creative writing assistance, complex research synthesis) where goals are fuzzier and shifts are even more implicit?\nRoot Cause of Redundancy: The extremely high TCRR in some domains, particularly Retail, is a striking finding. What is your hypothesis for why this occurs? Is it an agent reasoning error, or could the benchmark's task design or toolset inadvertently encourage redundant calls?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lsUbTzDJlS", "forum": "Kkcaz5XlJB", "replyto": "Kkcaz5XlJB", "signatures": ["ICLR.cc/2026/Conference/Submission25416/Reviewer_W6tz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25416/Reviewer_W6tz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762526347069, "cdate": 1762526347069, "tmdate": 1762943426058, "mdate": 1762943426058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}