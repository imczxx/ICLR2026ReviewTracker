{"id": "NlkykTqAId", "number": 7807, "cdate": 1758036948695, "mdate": 1759897831373, "content": {"title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning", "abstract": "Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose Hierarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. Our extensive experiments validate that HICRA significantly outperforms strong baselines, and offer deep insights into how reasoning advances through the lens of strategic exploration.", "tldr": "", "keywords": ["hierarchical reasoning", "LLM", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84bf7e9573e68c47b5d966ce8a45e6784d78d0fa.pdf", "supplementary_material": "/attachment/a3646c1ffbb315832d9b648f5acaca905513f743.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the learning dynamics of Large Language Models during reinforcement learning for reasoning tasks. The authors propose a two-phase learning process. Models first consolidate low-level procedural skills and then shift to exploring high-level strategic planning. This paper argues that the second phase is the main bottleneck for performance. Based on this insight, the authors introduce HICRA, a new algorithm that modifies the credit assignment in RL. HICRA focuses optimization pressure on \"planning tokens\", which guide the reasoning process. Experiments across multiple models and benchmarks show that HICRA outperforms standard RL baselines by accelerating strategic exploration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear and insightful analysis of the learning process in LLMs. The proposed two-phase framework provides a compelling explanation for several previously observed but poorly understood phenomena, such as \"aha moments\" and \"length scaling\". This provides a valuable conceptual model for the community.\n\n2. The proposed method, HICRA, is simple, well-motivated, and directly addresses the bottleneck identified in the analysis. Instead of introducing a complex new architecture, it makes a targeted and intuitive change to the advantage function of an existing algorithm, making it easy to understand and implement.\n\n3. The empirical evaluation is extensive and convincing. The authors validate their claims across a diverse set of open-source language and vision language models on multiple challenging mathematical reasoning benchmarks."}, "weaknesses": {"value": "1. The method's effectiveness is dependent on the quality of the \"Strategic Grams\" used to identify planning tokens. The automated pipeline for extracting these grams seems heuristic and may not be robust. It is unclear if it truly identifies strategic reasoning or just common, high-frequency phrases that correlate with correct answers.\n\n2. The set of compared baselines could be stronger. The paper argues that its functional definition of planning tokens is superior to using high-entropy \"fork tokens\" as a proxy. However, it does not include a direct experimental comparison to a baseline that selectively rewards high entropy tokens. Such a comparison is necessary to validate the additional complexity of the proposed semantic identification pipeline.\n\n3. The paper frames the problem with a binary distinction between planning and execution tokens. However, this is a simplification of complex reasoning, which can involve multiple layers of strategic abstraction. The method treats all planning tokens equally, which might not be optimal for learning intricate, multi-step plans.\n\n4. The method's applicability appears to be limited by the base model's initial capabilities. The appendix shows that HICRA can perform worse than the baseline on models that lack a solid procedural foundation. This suggests the method is not a general solution but rather an accelerator for already competent models."}, "questions": {"value": "1. The identification of Strategic Grams is a critical component. How sensitive is HICRA's performance to this list? For example, what is the impact of using a much smaller list, or a list generated from a different data domain? This would help clarify whether the method is learning general planning skills or overfitting to certain phrases.\n\n2. Could the authors provide a direct experimental comparison against a baseline that uses a high-entropy heuristic to identify and reward important tokens? \n\n3. Could the authors assign different weights to different types of planning tokens, distinguishing between high-level strategy and lower-level tactical steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vyrs7244WG", "forum": "NlkykTqAId", "replyto": "NlkykTqAId", "signatures": ["ICLR.cc/2026/Conference/Submission7807/Reviewer_RzuK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7807/Reviewer_RzuK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812895883, "cdate": 1761812895883, "tmdate": 1762919848105, "mdate": 1762919848105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an analysis of RL dynamics in large language models, proposing that enhanced reasoning emerges from a hierarchical separation between high-level strategic planning and low-level procedural execution. The authors introduce a data-driven method to identify \"planning tokens\" and build upon this insight with HICRA, a targeted credit assignment algorithm that consistently outperforms strong GRPO baselines across various models and mathematical reasoning benchmarks. This demonstrates that their conceptual framework is not merely theoretical but offers a practical and effective lever for improving model performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's focus on identifying and boosting \"reasoning\" or \"planning\" tokens is an interesting motivation. Experiments demonstrates that this is a practical lever for improving model performance.\n\n2. Its proposed algorithm, HICRA, demonstrates consistent performance gains across several small models and benchmarks, showing the effectiveness and generality of the approach."}, "weaknesses": {"value": "1. The notion of a decisive shift and strict sequential separation (first one, then the other) may over-simplify the RL training process in a massive over-parameterized model like an LLM. In reality, planning and execution are rarely decoupled in learning. Improvements in low-level procedural skills (e.g., generating syntactically correct code or flawless calculation steps) immediately inform and enable more ambitious, complex, and high-level strategic plans. Conversely, a good high-level plan (strategic reasoning) acts as a powerful curriculum for focusing the procedural execution component, leading to faster skill acquisition.\n\n2. The proposed solution, Hierarchy-Aware Credit Assignment (HICRA), relies heavily on the ability to consistently identify and isolate \"high-impact planning tokens\" from procedural execution tokens. However, ground-truth labels for token function are lacking in this field. Since LLMs generate text in an autoregressive fashion, the designation of a token as \"planning\" or \"procedural\" is not externally supervised. It must be determined by some internal, emergent mechanism (e.g., attention head patterns, internal self-correction signals, or specific activation clusters). If HICRA relies on a learned or heuristic mechanism to identify the hierarchy, the success of the credit assignment becomes critically dependent on the robustness and stability of that internal mechanism. A noisy or incorrect classification of a \"planning\" token as \"procedural\" will cause the reward signal to be diluted (the problem HICRA aims to solve). Conversely, misclassifying a \"procedural\" token as \"planning\" could lead to unstable and high-variance gradients in the early layers responsible for high-level reasoning, potentially destabilizing the planning component itself.\n\n3. While concentrating credit on planning tokens targets the apparent bottleneck, it simultaneously starves the procedural components of the direct, high-magnitude signal they need to maintain or improve their correctness. For complex, multi-step tasks, procedural fidelity is non-negotiable. Reducing the optimization pressure on these steps may lead to a model that has great plans but consistently fails on the execution details.\n\n4. This work builds heavily upon the intersection of methodologies from at least three areas: hierarchical RL for LLM agents with explicit planning modules; fine-grained credit assignment \\& token-specific optimization; RL training dynamics. This makes it difficult to identify a significant contribution of this paper. For example, MT-Core [1] also consists of a coarse-grained policy formulation and a fine-grained policy learning by LLM and RL, respectively. Then what is the key contribution of this paper with respect to the methodologies from these three areas?\n\n[1] Pan, C., Yang, X., Wang, H., Wei, W., \\& Li, T. (2024). Hierarchical continual reinforcement learning via large language model. arXiv preprint arXiv:2401.15098.\n\n5. The core limitation lies in the algorithm's dependency on a foundational level of procedural correctness. HICRA's method of selectively amplifying the signal for high-level planning tokens is most effective *after* the base model has achieved a certain level of procedural reliability. If the base model lacks these reliable low-level skills, the enforced strategic exploration can be counterproductive, as a brilliant strategy is useless if the execution (e.g., a simple calculation) is consistently flawed. More experiments should be conducted to sufficiently  explore the performance of HICRA on models with weak initial procedural skills, such as MiMO-VL-Base.\n\nThe current form of the paper is not enough convincing from motivation to methodology."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wL4D4G2VSy", "forum": "NlkykTqAId", "replyto": "NlkykTqAId", "signatures": ["ICLR.cc/2026/Conference/Submission7807/Reviewer_QnoB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7807/Reviewer_QnoB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847084548, "cdate": 1761847084548, "tmdate": 1762919847578, "mdate": 1762919847578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies why RL improves LLM reasoning and proposes that training exhibits an emergent two-phase hierarchy: (1) early consolidation of low-level procedural execution (formatting, arithmetic), evidenced by reduced perplexity and token entropy for “execution tokens”; (2) later exploration and mastery of high-level strategic planning, evidenced by increased “semantic entropy” of “Strategic Grams” (SGs)—n-gram phrases that purportedly carry strategic functions (deduction, branching, backtracing). The authors claim this framework unifies puzzling observations (“aha moments,” length-scaling, token entropy dynamics). They argue that standard GRPO distributes optimization uniformly across tokens, diluting credit on high-impact planning tokens. They propose HICRA, a simple GRPO modification that amplifies token-level advantages for planning tokens (identified by SGs) and dampens their penalties, thereby focusing optimization on strategy. Across several LLMs and VLMs on math reasoning benchmarks (e.g., AIME24/25, Math500, AMC23, Minerva, Olympiad; MathVista, MathVerse, MathVision, EMMA), HICRA reportedly outperforms GRPO baselines. They further analyze error-type dynamics (planning vs. others), show semantic entropy is a better exploration indicator than token entropy or Pass@K, and discuss limitations (e.g., dependence on a solid procedural base; weaker gains on Llama-3.1-8B-Instruct)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, testable hypothesis about a two-phase, hierarchical learning dynamic with consistent qualitative evidence across multiple model families (text-only and VLM).\n\n2. A simple, implementable algorithm (HICRA) grounded in the analysis; integrates cleanly with GRPO and yields consistent improvements on several benchmarks.\n\n3. Useful analytic tools: semantic entropy over strategic units; error-type diagnostics; sensitivity analyses showing robustness of qualitative trends to partial SG removal.\n\n4. Solid empirical sweep: multiple models, several benchmarks, process metrics (PPL/entropy/length/accuracy)."}, "weaknesses": {"value": "1. Operationalization of “planning tokens” via SGs is heuristic and potentially brittle: depends on corpus choice, clustering hyperparameters, n-gram length range, and English phrasing. No human validation statistics (precision/recall) against expert annotations; no cross-domain generalization test (beyond math/VLM math).\n\n2. Causal claims remain tentative: the evidence is largely correlational. In particular, the “two-phase” dynamic and the linkage between semantic entropy and accuracy could be confounded by curriculum effects, context-length growth, reward clipping, or prompt-format shifts.\n\n3. Dependence on pre-existing procedural competence: HICRA underperforms/struggles on weaker models (e.g., Llama-3.1-8B-Instruct), suggesting the method is not broadly robust. No adaptive mechanism to detect when to emphasize execution vs. planning.\n\n4. Reward shaping risk: amplifying credit on planning tokens may overfit to stylistic strategic phrases (SG lexicon) rather than genuine strategy. The overlap/shift of SGs across datasets or languages is unclear; multilingual robustness is untested.\n\n5. Evaluation transparency concerns: many datasets are small and sensitive; multiple sampling runs, seeds, and statistical tests are not fully reported. Some deltas are modest; one negative result (Olympiad on Qwen3-4B-Instruct) is noted.\n\n6. Comparison set: strong recent baselines (e.g., process-reward shaping, reflection-based RL, decision-token methods) are not exhaustively compared; entropy-regularization is a weak foil."}, "questions": {"value": "1. Can you provide quantitative validation of SG detection (human annotation study with precision/recall; inter-annotator agreement)? How sensitive are results to different embedding models, clustering methods, n-gram ranges, and thresholds?\n\n2. Can you run targeted causal tests—e.g., (i) randomly reassign SG labels to matched-frequency n-grams; (ii) ablate SG presence at train time (mask or paraphrase them away); (iii) directly reward semantic-entropy increases vs. HICRA—to test whether planning-focus per se drives gains?\n\n3. Can HICRA dynamically down-weight planning tokens when execution errors dominate (e.g., via online estimation of error types) to avoid failures on weaker models?\n\n4. Generalization: Do the findings hold on non-math reasoning (commonsense, code), agentic tool-use, and multilingual tasks? How portable is the SG lexicon across domains and languages?\n\n5. Robustness and stats: Please report means/standard deviations over multiple seeds for key benchmarks; include significance tests. Are improvements consistent under stricter decoding (temp=0) or chain-of-thought suppression?\n\n6. If you paraphrase SG phrases (same function, different wording), do gains persist? What fraction of HICRA’s advantage survives when SG phrases are held out from training prompts?\n\n7. Does HICRA exacerbate verbosity or hallucinations in non-math tasks? Provide length-normalized metrics and calibration analyses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aLPKNjH8lv", "forum": "NlkykTqAId", "replyto": "NlkykTqAId", "signatures": ["ICLR.cc/2026/Conference/Submission7807/Reviewer_2wpP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7807/Reviewer_2wpP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895546748, "cdate": 1761895546748, "tmdate": 1762919847152, "mdate": 1762919847152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors discover two-phase training dynamics of LLMs finetuned using reinforcement learning. The first phase optimizes low-level procedural execution tokens, followed by exploration of high-level planning tokens, the latter explaining aha moments, length scaling, and also contributes to advanced reasoning. They also propose a simple modification of the GRPO algorithm, called HICRA, which amplifies the advantage of planning tokens in the solution trajectory. Across several LLMs and VLMs, HICRA demonstrates significant improvements over mathematical reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Analyzing the two-phase learning dynamics of LLMs trained using RL: rapid learning of procedures through low level tokens followed by exploration of high-level strategic planning tokens, the latter explaining aha moments and length scaling\n\n2. Identifying high-level planning tokens through the construction of strategic n-grams and computing their entropy\n\n3. Modifying GRPO algorithm to amplify the advantage of planning tokens that contribute to advanced reasoning\n\n4. Significant improvement on mathematical reasoning benchmarks using multiple LLMS/VLMs"}, "weaknesses": {"value": "1. The evaluation is currently limited to only mathematical reasoning tasks, and it remains to be seen whether this approach of emphasizing optimization of strategic tokens would also lead to improvements on generic multi-step reasoning tasks. \n\n2. The authors claim that this two-phase training dynamics is an emergent phenomenon of LLMs that are finetuned using RL, however, they provide no evidence that standard finetuning (i.e., no RL) doesn’t lead to similar learning dynamics."}, "questions": {"value": "1. Can the authors provide more details about how the token-level and semantic entropy is computed, including some equations?\n\n\n2. How was the value of the alpha parameter chosen?\n\n3. Figure 4 and Figure 5 why are different models trained for different numbers of steps? A fair comparison would be to train all models for the same number of steps.\n\n4. How would a baseline with an explicit optimization of semantic entropy perform?\n\n\n\n5. What is the pretrained sentence transformer used for clustering of semantic embeddings? Why is the LLM’s tokenzier not used for getting the n-gram embeddings and how would that perform?\n\n6. How sensitive is the performance of HICRA to the SG construction of taking top k% cluster document frequency, random dropping etc?\n\n7. for unsuccessful ones, the penalty is reduced for the planning token - what is the intuition behind this? What if the planning tokens primarily contributed to the failure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PAS9wHZR9w", "forum": "NlkykTqAId", "replyto": "NlkykTqAId", "signatures": ["ICLR.cc/2026/Conference/Submission7807/Reviewer_WcVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7807/Reviewer_WcVs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979218143, "cdate": 1761979218143, "tmdate": 1762919846532, "mdate": 1762919846532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}