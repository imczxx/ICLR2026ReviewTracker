{"id": "66207RUZBX", "number": 2947, "cdate": 1757305944400, "mdate": 1762951352801, "content": {"title": "Towards Robust Benchmark of Object Hallucination on Multiple Images", "abstract": "Multimodal Large Language Models (MLLMs) are evolving into sophisticated agentic systems, engaging users in complex, multi-image scenarios. However, current MLLMs are limited by object hallucination, generating information inconsistent with visual evidence. Existing benchmarks, largely designed for single-image settings or offering only high-level multi-image assessments, fail to capture the nuanced causes of object hallucination, particularly under adversarial conditions. To address this, we introduce the Multi-Image Object Hallucination (MIOH) benchmark, a comprehensive framework specifically designed to diagnose MLLM vulnerabilities in complex multi-image contexts. MIOH integrates four object-centric tasks (existence, counting, attribute, position) with four controllable adversarial factors (visual context scale, perceptual difficulty, contextual bias, and misleading textual context). Through our systematic evaluation using MIOH, we reveal that even state-of-the-art models including GPT-5 and Gemini Pro still suffer from significant performance degradation under adversarial conditions, with models showing increased susceptibility to both false positive and false negative hallucinations when visual and linguistic contexts become challenging.", "tldr": "", "keywords": ["Multimodal Large Language Model", "Object hallucination", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/db4be22fba5c795fde7ffb3fbad1fcd9c0fe0929.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Multi-Image Object Hallucination (MIOH) benchmark, created to address key limitations in current evaluation methods. The authors argue that existing benchmarks are often confined to single-image settings or provide only superficial, high-level assessments for multi-image contexts. As a result, they fail to diagnose the nuanced causes behind object hallucination. MIOH overcomes this by systematically integrating four fundamental object-centric tasks—existence, counting, attribute, and position—with four controllable adversarial factors, such as visual context scale and misleading textual cues."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a timely and significant problem. Multi-image reasoning is a critical and rapidly developing capability for MLLMs, and robustly evaluating it is essential, particularly as models are increasingly applied to sequential visual data like videos.\n\n2. The paper is supported by comprehensive experiments covering a wide spectrum of models, from leading proprietary systems to various open-source alternatives. \n\n3. MIOH thoughtfully incorporates controllable factors, including multi-task categories and, importantly, a suite of adversarial pressures. This allows for a much more diagnostic evaluation than a simple accuracy score."}, "weaknesses": {"value": "1. The paper's central claim to novelty is questionable, as it largely ignores the extensive work on video hallucination benchmarks like Video-Halluci, VidHalluc, and even the multi-image components of HallusionBench. Since video is inherently a multi-image problem, these frameworks already assess object consistency and factual accuracy across frames. The paper fails to articulate what unique conclusions MIOH can provide that are not already captured by these more complex and established video-centric evaluations.\n\n2. Furthermore, the specific adversarial factors are not original and have been thoroughly explored in recent work. For instance, the impact of linguistic priors and visual confusion has been a core focus of benchmarks like PhD (CVPR 2025) and VLind-bench (NAACL), while the task categories themselves mirror those found in AMBER. When the multi-image aspect is viewed as a subset of video, the benchmark appears highly derivative, borrowing its core testing principles from these existing works without significant innovation.\n\n3. Methodologically, the benchmark feels more like a compilation of existing ideas than a new task. The use of multiple-choice questions is a standard evaluation format that offers no clear advantage over other common methods like binary assessment. As a result, the overall design comes across as an aggregation of established datasets and evaluation paradigms, rather than a fundamentally new contribution to the field."}, "questions": {"value": "See the weaknesses.\n\nThe paper needs to clarify its key distinctions from similar frameworks, especially video benchmarks. Specifically, what unique conclusions does it offer that others cannot, and what are its unique design elements that enable these insights? Furthermore, the authors should provide more detailed, actionable guidance on how the benchmark can direct future MLLM improvements—for instance, by explicitly linking poor performance on specific categories to concrete suggestions for model refinement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4c9GHK8NGb", "forum": "66207RUZBX", "replyto": "66207RUZBX", "signatures": ["ICLR.cc/2026/Conference/Submission2947/Reviewer_qnit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2947/Reviewer_qnit"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761391802649, "cdate": 1761391802649, "tmdate": 1762916454001, "mdate": 1762916454001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "After carefully considering the reviewers’ feedback and discussing within the author team, we agree with the reviewers’ comments and suggestions, and have decided to withdraw the paper in order to substantially revise and extend the work. We sincerely appreciate the reviewers’ constructive feedback and the committee’s time and effort, and we plan to resubmit a significantly improved version in the future."}}, "id": "Rw7O0Mzeka", "forum": "66207RUZBX", "replyto": "66207RUZBX", "signatures": ["ICLR.cc/2026/Conference/Submission2947/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2947/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762951351921, "cdate": 1762951351921, "tmdate": 1762951351921, "mdate": 1762951351921, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the MIOH benchmark to evaluate multi-image object hallucination which contains  4 object-centric tasks (existence, counting, attribute, position) via 3 question types (comprehensive, comparative, selective)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Evaluations on 30 models (including GPT-5 and Gemini-2.5-Pro) show overall average accuracy of only 37.0%, with SOTA models still suffering significant performance drops under adversarial conditions; multi-image context amplifies hallucination, counting is the most challenging task, and misleading text/ increased image count (≥8) have the strongest negative impacts."}, "weaknesses": {"value": "1. While multi-image tasks hold significance, multi-image problems that lack temporal scenarios have extremely limited practical relevance. Specifically, it fails to evaluate MLLMs’ ability to track object trajectories or reason about temporal dependencies—capabilities critical for real-world applications such as autonomous driving and video surveillance.\n2. This very limitation, I suspect, explains why the authors were only able to restrict their evaluation tasks to Visual Question Answering (VAQ) problems, which itself also suffers from inherent limitations. Furthermore, such evaluations cannot reflect which specific capabilities of a model have improved when it achieves better performance after undergoing training (e.g., reinforcement learning, RL). Simple multiple-choice questions, devoid of reasoning verification, are highly vulnerable to being \"hacked.\" This is precisely why I personally argue that current benchmark efforts in the field generally lack the significance to guide model development.\n3. The benchmark uses \"accuracy\" as the sole metric, which only judges whether hallucination occurs but cannot quantify hallucination severity (e.g., distinguishing \"mislabeling a cat as a dog\" from \"inventing a non-existent tiger\"), failing to reflect risk gradients in practical use.\n4. The paper does not investigate whether hallucination rises due to visual token overload (e.g., models dropping critical visual tokens when processing >8 images, as hinted at in Sec. 5.3 but not verified)."}, "questions": {"value": "Q1: You limited counting tasks to images with ≤5 objects (due to reduced annotation reliability of COCO-ReM for >10 objects), but real-world multi-image scenarios (e.g., crowded streets, busy warehouses) often involve more than 5 objects. Have you explored alternative datasets (e.g., synthetic datasets with verified counts for 6–15 objects) or annotation strategies (e.g., combining COCO-ReM with crowd-sourced validation for high-object-density images) to evaluate model counting capabilities in such practical scenarios? If not, do you believe the current ≤5-object limit underestimates how models struggle with quantitative reasoning in dense multi-image contexts?\n\nQ2: Your results show that performance degrades in multi-image scenarios. Could you provide a simple analysis explaining how to improve the multi-image performance of open-source models, or how to bridge the current gap between open-source and closed-source models in this aspect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "So8wRrOx1y", "forum": "66207RUZBX", "replyto": "66207RUZBX", "signatures": ["ICLR.cc/2026/Conference/Submission2947/Reviewer_8QJc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2947/Reviewer_8QJc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405298748, "cdate": 1761405298748, "tmdate": 1762916453622, "mdate": 1762916453622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MIOH, a diagnostic benchmark for object hallucination specifically under multi-image inputs. It combines four object-centric tasks (existence, counting, attribute, position) with multi-image query forms and adds controllable adversarial factors to probe failure modes; the experiments show sizable drops from normal to adversarial settings and that processing images jointly amplifies hallucinations beyond simple per-image error accumulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Writing is polished and easy to follow, with a clean structure from benchmark design to analyses and figures that reinforce the main claims。\n2. Evaluation on many models, enabling credible cross-model comparisons and trend analysis across both open-source and frontier proprietary systems."}, "weaknesses": {"value": "1. The paper explicitly claims “the first comprehensive evaluation framework specifically designed to assess object hallucination under multi-image settings.” This claim is not accurate. Prior work has already released benchmarks that target multi-image hallucination, eg. MIHBench[1] which is designed to evaluate hallucination in multi-image MLLMs and provides a systematic study with tasks for existence, count, and identity consistency, along with a mitigation method.\n2. Heavy reliance on template-based multiple-choice evaluation limits ecological validity, and robustness to paraphrase or option order is not reported. \n3. No mitigation method is proposed to address the identified hallucination failures\n\n[1]: MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models. ACM MM25."}, "questions": {"value": "1. Do your conclusions persist when templates are paraphrased and when the images or answer-option order is shuffled; in other words, how sensitive are results to prompt wording and choice order.\n2. Do you have any method that reduces these hallucinations under your adversarial factors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9NMysJNRPN", "forum": "66207RUZBX", "replyto": "66207RUZBX", "signatures": ["ICLR.cc/2026/Conference/Submission2947/Reviewer_pbQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2947/Reviewer_pbQS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721829578, "cdate": 1761721829578, "tmdate": 1762916453277, "mdate": 1762916453277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the MIOH benchmark to study object hallucination in multimodal large language models when dealing with multiple images. It focuses on four object-based tasks and four types of adversarial factors that can cause hallucinations. The study finds that even strong models like GPT-5 and Gemini Pro still make mistakes when the visual or text context becomes complex. The benchmark helps show how these models fail and can be used to improve their reliability in multi-image settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. The paper introduces a clear and systematic benchmark (MIOH) that fills the gap in studying object hallucination under multi-image settings, combining key object tasks like existence, counting, attribute, and position.\n2. It uses controllable adversarial factors to deeply analyze model weaknesses and provide fine-grained diagnostic insights.\n3. The writing is clear and easy to understand, making the ideas accessible.\n4. The experiments are thorough and cover most major multimodal models, showing strong empirical support for the conclusions."}, "weaknesses": {"value": "Weaknesses:\n1. While MIOH is comprehensive, it mainly focuses on object-level hallucination and may not cover higher-level reasoning or relational hallucinations between objects.\n2. The analysis could go deeper on why certain models fail — for example, by linking errors to architecture or training differences."}, "questions": {"value": "Questions:\n1. Could the paper include some examples that better highlight multi-image object reasoning, such as relational or causal cases, to show how current models handle more complex reasoning scenarios?\n2. Could the paper briefly analyze why models fail in some cases, just to give a bit more insight into the causes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IJfYoHxZ7Q", "forum": "66207RUZBX", "replyto": "66207RUZBX", "signatures": ["ICLR.cc/2026/Conference/Submission2947/Reviewer_hWgF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2947/Reviewer_hWgF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874673448, "cdate": 1761874673448, "tmdate": 1762916453139, "mdate": 1762916453139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}