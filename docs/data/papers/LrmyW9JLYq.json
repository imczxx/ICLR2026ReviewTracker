{"id": "LrmyW9JLYq", "number": 14156, "cdate": 1758229368529, "mdate": 1763588480268, "content": {"title": "A Structured, Tagged, and Localized Visual Question Answering Dataset with Full Sentence Answers and Scene Graphs for Chest X-ray Images", "abstract": "Visual Question Answering (VQA) enables targeted and context-dependent analysis of medical images, such as chest X-rays (CXRs). However, existing VQA datasets for CXRs are typically constrained by simplistic and brief answer formats, lacking localization annotations (e.g., bounding boxes) and structured tags (e.g., region or radiological finding/disease tags). To address these limitations, we introduce MIMIC-Ext-CXR-QBA (abbr. CXR-QBA), a large-scale CXR VQA dataset derived from MIMIC-CXR, comprising 42 million QA-pairs with multi-granular, multi-part answers, detailed bounding boxes, and structured tags. \nWe automatically generated our VQA dataset from scene graphs (also made available), which we constructed using LLM-based information extraction from radiology reports. After automatic quality assessment, we identified 31M pre-training and 7.5M fine-tuning grade QA-pairs, providing the largest and most sophisticated VQA dataset for CXRs to date. Tools for using our dataset and the construction pipeline are available at https://anonymous.4open.science/r/mimic-ext-cxr-qba/ .", "tldr": "We present a large-scale CXR VQA dataset derived from MIMIC-CXR with 42M QA pairs,featuring multi-part answers,bounding boxes,and structured tags; it was generated using LLM-based extraction from radiology reports and localization models.", "keywords": ["VQA", "Localization", "Vision-Language Modeling", "Medical Imaging", "Chest X-Rays", "Scene Graphs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e5feda9f08a74c7f2a09c241ae660e513dfb978.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MIMIC-Ext-CXR-QBA (or CXR-QBA), a large VQA datset derived from MIMIC-CXR. Using an automated pipeline—LLM-based information extraction —the authors first construct scene graphs for chest X-rays and then automatically generate question–answer (QA) pairs with multi-part, structured answers, bounding boxes, and structured tags. An automatic quality assessment filters these into ~31.2M pre-training and ~7.5M fine-tuning pairs. The paper also reports expert validation steps, introduces a structured VQA formulation for CXRs, and releases a baseline model and tools."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- If substantiated, a CXR VQA dataset of 31M pre-training and 7.5M fine-tuning grade QA-pairs (w. box-localized, tag-structured), might be valuable to the community\n- The dataset construction pipeline might also be helpful."}, "weaknesses": {"value": "- Reliance on LLM-information extraction and LLM-as-judge without deep radiologist ground truth\n- The issue of dataset balance, diversity, and repetition"}, "questions": {"value": "## LLM reliability and bias\n\n> L243-244: More specifically, we use Llama 3.1 8B (Grattafiori et al., 2024) to rate questions and answers by the following five criteria\n\n1. LLM hallucination and judgement bias are well-documented in general / medical LLM-as-judge literature; 8B judges in particular are fragile compared with larger or ensemble judges.\n\n2. The proposed pipeline is based on scene-graph information extraction on general-domain LLaMA-3.1. Why didn’t you use (or at least cross-check against) medically tuned LLMs/IE models? The authors should at least provide a head-to-head on a radiologist-labeled dataset (entity/relation P/R/F1, negation/uncertainty, ...), and justify keeping a general model if it underperforms medical LLMs.\n\n3. If the LLM extractor and LLM judge share architecture/family (Llama 3.1), what safeguards prevent model-family bias or circular validation (the judge favoring its own style)?\n\n## Dataset\n\n4. Generally, the ontology, negation/uncertainty handling, and mapping rules need rigorous specification to be reproducible and comparable to RadGraph and ImaGenome schemas. What are the macro/micro P/R/F1 of your LLM-IE compared with RadGraph baselines? Where do errors concentrate?\n\n> Figure 5, Distribution of tags (finding subcategories, regions, findings) mentioned in answers of different question types (indication, study abnormality, region abnormality, finding). \n\n5. CXR is highly long-tailed; without balancing and leakage controls, models might learn shortcuts and inflate metrics. \n\n    5.1 In some of these questions, the question types have all positive or all negative (e.g., in  region abnormality or finding). How do the authors solve the issue of imbalanced dataset? \n\n    5.2 Additionally, the authors should also disclose patient/study/report-level splits to rule out report -> QA leakage and measure sensitivity to template priors. \n\n6. Large auto-generated corpora often contain heavy redundancy that distorts training and evaluation. What fraction of QAs are near-duplicates (by text, tags, question types, and box layout)? The authors should provide more about the dedup pipeline (e.g., MinHash/SimHash + ANN, or semantic-hashing) and show how dedup affects baseline performance and diversity.\n\n>  For comparison, we thus select MAIRA-2 (Bannur et al., 2024), the only publicly available CXR report generation model that supports bounding box prediction. \n\n7. Beyond MAIRA-2, how does the structured-VQA model transfer to other independent Med-VQA/generation datasets and phrase-grounding tasks without template leakage? E.g., results on some common medical benchmarks (e.g., VQA-RAD, SLAKE, and PathVQA)?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "CXR VQA dataset"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tIQOQchTBw", "forum": "LrmyW9JLYq", "replyto": "LrmyW9JLYq", "signatures": ["ICLR.cc/2026/Conference/Submission14156/Reviewer_wooh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14156/Reviewer_wooh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425439710, "cdate": 1761425439710, "tmdate": 1762924618166, "mdate": 1762924618166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MIMIC-Ext-CXR-QBA (or CXR-QBA), a large VQA datset derived from MIMIC-CXR. Using an automated pipeline—LLM-based information extraction —the authors first construct scene graphs for chest X-rays and then automatically generate question–answer (QA) pairs with multi-part, structured answers, bounding boxes, and structured tags. An automatic quality assessment filters these into ~31.2M pre-training and ~7.5M fine-tuning pairs. The paper also reports expert validation steps, introduces a structured VQA formulation for CXRs, and releases a baseline model and tools."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- If substantiated, a CXR VQA dataset of 31M pre-training and 7.5M fine-tuning grade QA-pairs (w. box-localized, tag-structured), might be valuable to the community\n- The dataset construction pipeline might also be helpful."}, "weaknesses": {"value": "- Reliance on LLM-information extraction and LLM-as-judge without deep radiologist ground truth\n- The issue of dataset balance, diversity, and repetition"}, "questions": {"value": "## LLM reliability and bias\n\n> L243-244: More specifically, we use Llama 3.1 8B (Grattafiori et al., 2024) to rate questions and answers by the following five criteria\n\n1. LLM hallucination and judgement bias are well-documented in general / medical LLM-as-judge literature; 8B judges in particular are fragile compared with larger or ensemble judges.\n\n2. The proposed pipeline is based on scene-graph information extraction on general-domain LLaMA-3.1. Why didn’t you use (or at least cross-check against) medically tuned LLMs/IE models? The authors should at least provide a head-to-head on a radiologist-labeled dataset (entity/relation P/R/F1, negation/uncertainty, ...), and justify keeping a general model if it underperforms medical LLMs.\n\n3. If the LLM extractor and LLM judge share architecture/family (Llama 3.1), what safeguards prevent model-family bias or circular validation (the judge favoring its own style)?\n\n## Dataset\n\n4. Generally, the ontology, negation/uncertainty handling, and mapping rules need rigorous specification to be reproducible and comparable to RadGraph and ImaGenome schemas. What are the macro/micro P/R/F1 of your LLM-IE compared with RadGraph baselines? Where do errors concentrate?\n\n> Figure 5, Distribution of tags (finding subcategories, regions, findings) mentioned in answers of different question types (indication, study abnormality, region abnormality, finding). \n\n5. CXR is highly long-tailed; without balancing and leakage controls, models might learn shortcuts and inflate metrics. \n\n    5.1 In some of these questions, the question types have all positive or all negative (e.g., in  region abnormality or finding). How do the authors solve the issue of imbalanced dataset? \n\n    5.2 Additionally, the authors should also disclose patient/study/report-level splits to rule out report -> QA leakage and measure sensitivity to template priors. \n\n6. Large auto-generated corpora often contain heavy redundancy that distorts training and evaluation. What fraction of QAs are near-duplicates (by text, tags, question types, and box layout)? The authors should provide more about the dedup pipeline (e.g., MinHash/SimHash + ANN, or semantic-hashing) and show how dedup affects baseline performance and diversity.\n\n>  For comparison, we thus select MAIRA-2 (Bannur et al., 2024), the only publicly available CXR report generation model that supports bounding box prediction. \n\n7. Beyond MAIRA-2, how does the structured-VQA model transfer to other independent Med-VQA/generation datasets and phrase-grounding tasks without template leakage? E.g., results on some common medical benchmarks (e.g., VQA-RAD, SLAKE, and PathVQA)?\n\n-----\nupdated: \nThank you for the detailed clarifications and for adding the new analyses and experiments. Please make sure all newly reported results are clearly integrated into the main manuscript rather than only in the appendix. While some of my concerns are now partially addressed, others remain open. Still, the revision has strengthened the paper, and I am increasing my score accordingly."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "CXR VQA dataset"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tIQOQchTBw", "forum": "LrmyW9JLYq", "replyto": "LrmyW9JLYq", "signatures": ["ICLR.cc/2026/Conference/Submission14156/Reviewer_wooh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14156/Reviewer_wooh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425439710, "cdate": 1761425439710, "tmdate": 1763666098593, "mdate": 1763666098593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CXR-QBA, a new large-scale VQA dataset for chest X-rays. Based on MIMIC-CXR, the authors employed a three-stage automated pipeline: (1) scene-graph construction (including an information extraction model, an embedding model, and localization models), (2) template-based QA generation, and (3) automated quality assessment (mainly for deciding whether to use samples for fine-tuning) using a judge model (Llama-3.1-8B). Through this pipeline, the authors generated a total of 42.2M QBA (question, bounding box, answer) pairs. The main contribution is richly detailed automatic annotations for MIMIC-CXR at large scale, which might be better than the Chest ImaGenome dataset."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset’s scale is a massive contribution, with 42.2M pairs, including a 7.5M high-quality fine-tuning subset. The answer details are a key strength, providing rich, full-sentence responses derived from reports.\n- The three-stage automated pipeline is a significant contribution in itself.\n- The authors quantitatively benchmarked their scene graphs against the existing Chest ImaGenome dataset. They demonstrated that their pipeline produces superior results, notably a 20% performance improvement on long-tail (rare) disease classes, proving the value of their extraction method.\n- The paper’s presentation is excellent. It is clearly written, logically structured, and uses numerous figures and tables to effectively explain the complex pipeline, dataset, and results."}, "weaknesses": {"value": "* While the paper claims “detailed” or “fine-grained” localization, its bounding boxes correspond to 257 broad anatomical structures, not the pathologies themselves.\n* The “Grade B” (pre-training) data accounts for 58.8% (24.8M) of the dataset but is defined by a permissive standard (e.g., “may have issues”). Critically, the paper lacks an ablation study to prove that this massive, known-to-be-flawed data is actually beneficial for pre-training. The provided ablations (Table 14) only test task formulation, not the impact of data quality grades (A vs. B).\n* The pipeline lacks validation for individual stages, making it vulnerable to error propagation. Specifically: (a) the segmentation accuracy of the foundational CXAS model (for 257 regions) is not reported; (b) the information extraction performance of Llama-3.1-70B is not quantified (e.g., F1 score); (c) or benchmarked against RadGraph or Chest ImaGenome by using their gold standard dataset."}, "questions": {"value": "Q1 (Regarding the utility of “Grade B” data): \nThe “Grade B” (pre-training) data comprises 58.8% (24.8M) of the dataset, and proving its utility is essential. Could you provide an ablation study comparing the baseline model’s (Sec. 5) performance under these three conditions:\n  * (A) Fine-tuned on “Grade A” data only.\n  * (B) Pre-trained on “Grade B” data, then fine-tuned on “Grade A” data.\n  * (C) Pre-trained on the full (A+B+…) dataset, then fine-tuned on “Grade A” data.\n\nQ2 (Regarding the Validation of Scene Graph Accuracy): \nThe pipeline depends heavily on the Scene Graph Construction step, yet that step lacks independent validation. While benchmarks against MIMIC-CXR-JPG Test, CXR-LT, MS-CXR, and REFLACX are useful, they cover only overlapping, smaller scopes. Given the expanded coverage (257 regions, 221 findings), was a small internal gold standard (e.g., radiologist-annotated) created to validate factual accuracy of scene-graph outputs? If so, please report:\n- (a) Segmentation accuracy of the 158 CXAS masks (e.g., Dice).\n- (b) IE performance of Llama-3.1-70B (e.g., F1) against this internal gold standard.\n\nAdditionally, could you provide component-level benchmarking results against the public gold-standard datasets from RadGraph and Chest ImaGenome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GKYSFgIiTk", "forum": "LrmyW9JLYq", "replyto": "LrmyW9JLYq", "signatures": ["ICLR.cc/2026/Conference/Submission14156/Reviewer_TU8P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14156/Reviewer_TU8P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836638666, "cdate": 1761836638666, "tmdate": 1762924617768, "mdate": 1762924617768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIMIC-Ext-CXR-QBA (CXR-QBA), a large-scale VQA dataset for CXRs, derived from the MIMIC-CXR dataset. It comprises 42 million automatically generated question-answer pairs with multi-part, multi-granular answers styled like radiology reports. Each study includes detailed annotations, such as bounding boxes and structured tags (findings, regions). The dataset was constructed using an automatic scene graph construction pipeline, which leverages LLMs, semantic entity mapping, and localization models. The authors also define a structured VQA task, propose evaluation metrics, and provide a baseline model to demonstrate the utility of the dataset and guide future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Introduces MIMIC-Ext-CXR-QBA, a large-scale CXR-VQA dataset with 42 million QA pairs.\n2. Provides rich and detailed answers in radiology report style, along with fine-grained annotations including bounding boxes and structured tags.\n3. Implements robust quality control, including validation against expert annotations and assessment of automatically generated outputs.\n4. Proposes a structured VQA task for CXRs and provides a baseline model to showcase dataset effectiveness and support further model development."}, "weaknesses": {"value": "1. The dataset is automatically generated using large language models (LLMs). While the authors employ various quality control, there remains the potential for subtle errors or inconsistencies that may not be present in datasets fully curated by human experts. In particular, it is unclear whether the LLaMA-8B model provides sufficiently reliable quality assessment compared to larger, more capable LLMs.\n2. Since the dataset is derived from MIMIC-CXR, it likely inherits demographic and clinical biases inherent to that specific patient population, which may limit the generalizability of models trained on this dataset.\n3. The baseline model (MAIRA-2) is fundamentally a radiology report generation model, rather than a specialized VQA model or medical foundation model. This raises questions about the appropriateness of the baseline for evaluating performance on a VQA-specific task, as it may not accurately reflect the potential or limitations of models explicitly designed for structured question answering.\n4. The evaluation process relies on the LLaMA-8B model to determine entailment in the RadStrucVQA metric. Given the model’s relatively small size, there are reasonable concerns about the accuracy and robustness of its entailment judgments."}, "questions": {"value": "1. Are bounding boxes derived from CXAS masks always higher in quality than those from Chest ImaGenome? Given that both sources are used in the dataset, what criteria determine when one is preferred over the other?\n2. What are the criteria for defining Parent Regions and Fusion Regions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JQv9Ph7Cep", "forum": "LrmyW9JLYq", "replyto": "LrmyW9JLYq", "signatures": ["ICLR.cc/2026/Conference/Submission14156/Reviewer_V7HF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14156/Reviewer_V7HF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890110402, "cdate": 1761890110402, "tmdate": 1762924617299, "mdate": 1762924617299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CXR-QBA, a very large chest X-ray VQA dataset built from MIMIC-CXR that contains 42.2M QA pairs with full-sentence, multi-part answers, bounding boxes for grounding, and structured tags for findings and regions. The authors construct visually grounded scene graphs from radiology reports using LLM-based information extraction, semantic entity mapping, and region localization, then generate questions and answers from these graphs. They automatically rate QA quality with an LLM judge and release two subsets: pre-training grade (31.2M) and fine-tuning grade (7.5M). They also propose a structured VQA task that requires models to output text, boxes, and tags, and report a baseline LLaVA-style model with Rad-DINO and Llama-3.2-3B that outperforms an adapted MAIRA-2 reporting model on their new metric, RadStrucVQA."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset is orders of magnitude larger than prior CXR VQA resources and includes sentence-level answers, localization, and detailed tags.\n- The scene-graph construction is well described, including CXAS-based region masks, Chest ImaGenome boxes, LLM extraction, and BioLORD-based concept mapping, followed by LLM-as-judge quality assessment and graded splits.\n- The authors compare derived tags and boxes to radiologist gold standards and public annotations.\n- The structured VQA task and RadStrucVQA metric encourage outputs that are logically correct and visually grounded with tags."}, "weaknesses": {"value": "- I am not sure how reliable the LLM-as-a-judge quality assessment is when the judge is Llama 3.1 8B. The paper should calibrate this with stronger judges and report robustness checks like cross-judge agreement.\n- There are plenty of recent multimodal LLMs that could serve as reference points. Incorporating strong open-source baselines such as Qwen3-VL or LLaVA-Med v1.5 would better situate this work relative to current progress.\n- Human–LLM agreement is only fair to moderate.\n- The authors mention co-design with radiology collaborators, but they do not clearly report the number of participants, and their roles are vaguely described."}, "questions": {"value": "- Does the pre-training grade include only B-rated samples, or does it also include A and A+?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "877p3S1m7t", "forum": "LrmyW9JLYq", "replyto": "LrmyW9JLYq", "signatures": ["ICLR.cc/2026/Conference/Submission14156/Reviewer_uagn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14156/Reviewer_uagn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997694418, "cdate": 1761997694418, "tmdate": 1762924616768, "mdate": 1762924616768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}