{"id": "Ard2QzPAUK", "number": 25156, "cdate": 1758364736842, "mdate": 1759896732195, "content": {"title": "BeliefFormer: Belief Attention in Transformer", "abstract": "In this paper, we consider modifying the attention layer in Transformer to improve its generalization performance. Conceptually speaking, the standard attention layer takes the softmax-based weighted summation of V vectors as the residual signal (with a linear mapping for dimensionality alignment) when performing the skip-connection operation. Inspired by distribution optimization, we propose to first perform an orthogonal projection of the softmax-based weighted summation of V vectors with respect to the original V vectors and then take the orthogonal projection instead as the residual signal (with a linear mapping for dimensionality alignment) when performing the skip-connection operation.  By doing so, the token vectors are modified relatively more along their tangent directions compared to their magnitudes. Intuitively speaking, the orthogonal projection reflects a belief about the discrepancy between the weighted summation of V vectors and the V vectors themselves. We refer to the newly modified layer and the overall architecture as the belief-attention and the BeliefFormer, respectively.  To further improve performance, we also design a variant of belief-attention by incorporating two types of orthogonal projections, referred to as belief-attention$^{\\ast}$.   Extensive experiments show that the two new variants of attention layer in Transformers lead to better performance than the standard attention for image classification over ImageNet and natural language processing when training nano-GPT2.", "tldr": "incorporating orthogonal projection as residual signals into attention layer in Transformer to improve generation performance", "keywords": ["Transformer; orthogonal projection; BeliefFormer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6e44ec5d884a9d852f702079a97ef6be4b33a56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper replaces the usual attention residual MH(X)Wo with an orthogonalized residual that projects the softmax-weighted value summation onto the subspace orthogonal to the original V’s direction, argued to update tokens tangentially and shows modest gains on ImageNet, OpenWebText, and CIFAR-10. A second variant (Belief-Attention*) further orthogonalizes each head’s output w.r.t. its own \nVm, concatenates these per-head tensors and applies an extra projection Ws, yielding slightly larger gains at a small cost in parameters and compute."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1) Both the introduced Variants are easy to implement in PyTorch/Jax.\n2) The new variants don't introduce heavy compute/wall-clock overhead."}, "weaknesses": {"value": "The paper lacks empirical validation; values are reported at early to mid-training checkpoints, and the models do not appear to be well-tuned. Typically, well-tuned 20M ViT scores at least 70% accuracy on ImageNet classification, but the paper reports numbers close to 60%. It is the same with CIFAR-10, we expect the model to score above 90-95% but the paper's numbers are below 90. In addition, the paper didn't evaluate Language models' performance on downstream tasks. The paper also produced no empirical evidence that the proposed variants scale."}, "questions": {"value": "How can we conclude that the performance improvements of Belief-attention* come from orthogonalization rather than simply increasing the number of parameters? For example, in some of my experiments, concatenating (MH, V) along dim = −1 and feeding it to Wo improves performance. In this setup, the input dimension of  Wo doubles, matching the parameter count of Belief-attention*. Providing ablations of parameters-matched variants like the above would increase confidence that orthogonalization is the source of the improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ym5hlmi9MR", "forum": "Ard2QzPAUK", "replyto": "Ard2QzPAUK", "signatures": ["ICLR.cc/2026/Conference/Submission25156/Reviewer_j7jm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25156/Reviewer_j7jm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508776949, "cdate": 1761508776949, "tmdate": 1762943347471, "mdate": 1762943347471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BeliefFormer, a drop-in modification to the Transformer attention layer that replaces the usual residual signal (the softmax-weighted sum over V) with an orthogonal-projection-based discrepancy between the aggregated value MH(X) and the original value vectors V. Concretely, it computes, per token, the component of MH(X) that is orthogonal to V and uses that as the residual before the output linear map. A variant, BeliefFormer*, adds per-head orthogonal projections and an extra linear map to capture both global and per-head discrepancies. The method is motivated by an analogy to distributed optimization (PDMM), where updates explicitly incorporate constraint residuals; here, the “belief” is the discrepancy between aggregated and original values. The authors argue this leads to updates that change token directions more than magnitudes, potentially improving generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The PDMM analogy is thoughtfully developed, and the geometric analysis of the orthogonal projection (directional vs magnitude changes) is sound and intuitive, with supportive empirical diagnostics.\n2. The change is minimal (a per-token orthogonal projection), adds no parameters for BeliefFormer and only one extra linear map for BeliefFormer*, and can be implemented with a few lines of code as a drop-in replacement.\n3. The paper is clearly written and well structured."}, "weaknesses": {"value": "1. The evaluation is limited to small-to-mid-scale settings (ViT-small/DeiT-small, nano-GPT2). There are no results on large-scale LLMs or larger ViT backbones/long-context settings. As a result, the main claim of broadly improved generalization and scalability across Transformers is not convincingly supported.\n2. The PDMM analogy remains heuristic, there is no formal mapping of attention+FFN updates to a constrained optimization scheme, no convergence or generalization guarantees, and no theory showing that orthogonal projection necessarily improves optimization stability or generalization. Moreover, the desired “orthogonality” is not preserved after the output linear map W_o, undercutting the claimed mechanism without a formal treatment of how W_o and W_V interact with the geometry."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WX7C5iSAgY", "forum": "Ard2QzPAUK", "replyto": "Ard2QzPAUK", "signatures": ["ICLR.cc/2026/Conference/Submission25156/Reviewer_5MXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25156/Reviewer_5MXh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967267731, "cdate": 1761967267731, "tmdate": 1762943346764, "mdate": 1762943346764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the BeliefFormer architecture (with the base version belief-attention and variant belief-attention∗ ) for the improvement of the attention layer of Transformer. The core idea is to borrow the PDMM algorithm in distributed optimization, and use the orthogonal projection of the weighted sum of the V-vectors and the original V-vectors as the residual signal (instead of the direct weighted sum of the standard attention), so as to make the token vectors update more along the tangent direction and reduce the amplitude change, thus improving the generalization performance. Experiments are verified in ImageNet/CIFAR10 image classification, and nano-GPT2 for NLP tasks, and both variants outperform the standard Transformer, and the base version of BeliefFormer does not add extra parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- In 3 types of tasks (image classification, NLP), the verification accuracy/loss of BeliefFormer and variants are better than that of the standard Transformer, with no task adaptation failure problem.\n- The scheme is easy to implement and improves performance without introducing additional significant computational overhead increases"}, "weaknesses": {"value": "- Slight increase in computational complexity: training/reasoning time for the variants is  higher than the standard Transformer and overhead may accumulate in long sequence scenarios.\n- Tested only in ViT (small models), nano-GPT2 and 3 types of basic tasks, generalization to complex tasks such as large LLMs, very long sequences or speech/image generation was not verified. Insufficient validation of applicability.\n- The core of the paper I think lies in treating attn as a distribution optimization problem on a connected graph, and therefore (following the logic of the paper) how the difference metric is constructed is key. Then why orthogonal projection is used, the paper does not give a detailed argument for this, which is a problem.\n- In addition, the paper tries to explain the attn process by adopting the idea of PDMM for distribution optimization, but this explanation part lacks detailed arguments, and the authors directly regard the residual part of the attn computation as the residual part of the Lagrange multiplier, and again regard the MHA part as the information cohabitation part. So, the question here is, for the input X in attn is it regarded as the optimization objective X. Then is the V-vector regarded as the multiplier? The paper does not describe these details clearly, and the proposed role of the PDMM is confusing, as I could not make a proper connection between the PDMM and the computation of attn in the way it is described in the paper."}, "questions": {"value": "see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r9Bmo584JD", "forum": "Ard2QzPAUK", "replyto": "Ard2QzPAUK", "signatures": ["ICLR.cc/2026/Conference/Submission25156/Reviewer_axrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25156/Reviewer_axrZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175076597, "cdate": 1762175076597, "tmdate": 1762943346099, "mdate": 1762943346099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Belief-Attention, a variant of Transformer attention inspired by ideas from the Primal-Dual Method of Multipliers (PDMM). Instead of directly using the standard residual connection from the attention output, Belief-Attention orthogonalizes this residual with respect to the value vectors $V$, encouraging updates that change direction rather than magnitude. A variant, Belief-Attention*, performs projection per head and introduces an additional learnable matrix $W_s$. Experiments on ViT (ImageNet), nano-GPT2 (OpenWebText), and CIFAR-10 show consistent but modest accuracy and loss improvements with limited computational overhead. The method is easy to implement and generalizes across both vision and language tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s main strengths lie in its simplicity, generality, and clarity. The proposed modification is theoretically motivated and readily applicable across various architectures without requiring retraining or a structural overhaul. Empirical results show consistent gains in both vision and language tasks, and the discussion of limitations is transparent. The approach offers an intuitive geometric interpretation, emphasizing angular changes over magnitude, which may provide insights for understanding and stabilizing deep residual learning."}, "weaknesses": {"value": "The main limitation is the lack of rigorous theoretical grounding. The PDMM analogy is suggestive but not formalized, and it remains unclear when or why orthogonal projections should improve optimization or generalization. The experiments, while broad, are shallow in statistical rigor—missing repetitions, variance reporting, and ablation details (e.g., projection strength, per-layer effects, normalization variants). Numerical stability and computational cost analyses are also underexplored. Finally, the paper does not discuss prior related works on orthogonality or residual reparameterization (e.g., ReZero, cosine similarity regularization), which would help situate the contribution."}, "questions": {"value": "Could the authors explicitly map PDMM variables (residuals, multipliers, discrepancy) to quantities in the attention module (MH, V, X), clarifying what is retained and what is replaced in the Belief-Attention model?\n\nCan the authors formalize or empirically justify conditions under which orthogonalizing residuals improves generalization or convergence, perhaps by connecting to Lipschitz continuity or residual scaling analyses?\n\nHow are LayerNorms positioned relative to the projection step, and are results sensitive to pre-LN versus post-LN configurations?\n\nWould introducing a learnable projection strength (e.g., scaling factor $\\gamma$) improve stability or adaptability?\n\nHow does the model handle near-zero norms in $V$ during the orthogonalization process? Are numerical safeguards (e.g., $\\epsilon$-stabilization) applied, and what impact do they have?\n\nDo the reported gains hold for larger-scale language models or long-context causal decoders?\n\nAre the proposed modifications compatible with efficient attention variants such as FlashAttention or linear attention kernels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EtPBRc0uYd", "forum": "Ard2QzPAUK", "replyto": "Ard2QzPAUK", "signatures": ["ICLR.cc/2026/Conference/Submission25156/Reviewer_XiS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25156/Reviewer_XiS7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401782140, "cdate": 1762401782140, "tmdate": 1762943345865, "mdate": 1762943345865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}