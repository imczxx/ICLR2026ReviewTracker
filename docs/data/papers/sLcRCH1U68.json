{"id": "sLcRCH1U68", "number": 18363, "cdate": 1758286846358, "mdate": 1759897108496, "content": {"title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity", "abstract": "Conditional Semantic Textual Similarity (C-STS) measures the semantic proximity between text segments under a specific condition, thereby overcoming the ambiguity inherent in traditional STS. However, existing methods are largely confined to discriminative models, failing to fully integrate recent breakthroughs in the NLP community concerning Large Language Models (LLMs) and Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this task, as it can directly optimize the non-differentiable Spearman ranking metric and guide the reasoning process required by C-STS. However, we find that naively applying listwise RL leads to unstable training and convergence failure, as the model is overwhelmed by a complex, sparse reward signal. To address this challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning framework. PoLi-RL employs a two-stage curriculum: it first trains the model with simple pointwise rewards to establish fundamental scoring capabilities, then transitions to a hybrid reward that combines pointwise, pairwise, and listwise objectives to refine the model's ability to discern subtle semantic distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward (PSRR) mechanism that computes ranking rewards in parallel slices, where each slice comprises same-indexed completions from different samples. This provides precise, differentiated learning signals that ensure training stability. On the official C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18, establishing a new SOTA for the cross-encoder architecture. As the first work to successfully apply RL to C-STS, our study introduces a powerful and stable paradigm for training LLMs on complex, ranking-based conditional judgment tasks. Our code and checkpoints are available at https://anonymous.4open.science/r/PoLi-RL.", "tldr": "We propose PoLi-RL, a two-stage RL framework that stabilizes listwise optimization for the C-STS task via a progressive curriculum and a Parallel Slice Ranking Reward mechanism , achieving SOTA performance for the cross encoder architecture.", "keywords": ["Conditional Semantic Textual Similarity", "Reinforcement Learning", "Large Language Models", "Natural Language Processing", "Curriculum Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bac09d382e134ef7a81b3b0ad2591051d4841d3.pdf", "supplementary_material": "/attachment/d494e54851c54fc69022896475a381ad4ce23710.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes PoLi-RL, a two-stage point-to-list reinforcement learning framework for conditional STS with a cross-encoder backbone. Stage I teaches the model to “score” with pointwise distance, binary-consistency, and formatting rewards; Stage II adds pairwise and listwise ranking signals computed over parallel slices of completions (PSRR), yielding finer credit assignment aligned with rank-correlation objectives. Experiments on the standard C-STS benchmark (with an 8B-scale backbone) show consistent gains over few-shot prompting and supervised fine-tuning; ablations indicate that both the curriculum and the ranking-based rewards contribute."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The slice-wise ranking mechanism produces differentiated signals per completion, better aligned to Spearman/Kendall-style goals than batch-level listwise rewards.\n- The paper explains how to make PSRR tractable (generation multiplicity, gradient accumulation), offering a reproducible training recipe.\n- Strong performance with an 8B model suggests good cost-effectiveness."}, "weaknesses": {"value": "- Comparisons to the latest closed-source systems (e.g., GPT-4o, Claude-3.7 Sonnet) are missing, leaving the method’s headroom to stronger proprietary models underexplored.\n- All results are on Qwen3-8B only. Please add multi-size backbones or report compute-parity/parameter-parity comparisons to strengthen fairness claims.\n- The few-shot template and examples are relegated to the appendix; the main text’s notation does not formalize the few-shot setup, which hinders reproducibility. The authors should surface the key prompt template and define the few-shot variables in the notation section.\n- C-STS has been shown to contain substantial label noise [1], and a recent study has produced a re-annotated version of the data [2]. Training and evaluating on this re-annotated dataset would provide a more accurate assessment of the model’s effectiveness. I understand that [2] was released only a few days before the ICLR deadline; however, given the importance of data quality, I believe that evaluation on the newly annotated data would more clearly demonstrate the actual performance gains.\n\n[1] Linguistically conditioned semantic textual similarity. ACL2024\n\n[2] Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models. EMNLP2025"}, "questions": {"value": "- How does performance scale with model size (e.g., 3B/14B/32B) and with generation multiplicity G and slice size N?\n- Why RL over differentiable ranking losses? Under equal data/compute, how do listwise/pairwise differentiable surrogates perform relative to PoLi-RL Stage II?\n- What is the exact hyperparameter-tuning protocol? Which hyperparameters were tuned on dev, what ranges were explored, and what metric decided the final selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BGxBL1oYnx", "forum": "sLcRCH1U68", "replyto": "sLcRCH1U68", "signatures": ["ICLR.cc/2026/Conference/Submission18363/Reviewer_6HRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18363/Reviewer_6HRs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746555376, "cdate": 1761746555376, "tmdate": 1762928072161, "mdate": 1762928072161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method is novel and promising to some extent, with respectable results on C-STS; however, evidence for generalization is nearly absent (the reward and training organization are over-specialized to C-STS). Combined with missing strong reasoning baselines and insufficient stabilization details, the present claims do not extrapolate well."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1 Technical fit. Using RL to align with non-differentiable rank-correlation/ranking metrics (e.g., Spearman; more broadly NDCG, etc.) is naturally consistent with how C-STS is evaluated.\nS2 Method design. The two-stage curriculum (Point → List) + PSRR’s “parallel slicing” refines credit assignment; the engineering gives reproducible slicing/normalization and a weighting composition.\nS3 Competitive results. Achieves Spearman comparable to or better than strong existing baselines on the C-STS cross-encoder track, addressing the observation that “even strong models may not break 50” on this task.\nS4 Reproducibility. Anonymous code and checkpoints are provided."}, "weaknesses": {"value": "W1 Baseline coverage. \nThe paper only reports scores of non-reasoning large models (e.g., GPT-4) on C-STS, while reasoning models have progressed rapidly this year. The paper lacks strong baselines for **reasoning-style** models (e.g., DeepSeek-R1, OpenAI o3) under few-shot/CoT, leaving open the concern that “C-STS may already be easily solved by reasoning models.”\nW2 Severe lack of generalizability.\nThere is essentially no evidence supporting “the method generalizes,” and the reward design is highly specialized to C-STS’s annotation and organization:\n1. Stage-I strongly relies on the 1–5 score scale (Eq. (5) uses min-max normalization) and the ≥3 / ≤2 binary threshold (Eq. (6)).\n2. Stage-II’s pairwise term applies only to “adjacent pairs,” normalized by “maximum difference = 3” (Eq. (7)), which is tightly bound to C-STS’s pairing construction and score gaps.\n3. The listwise reward is built on PSRR’s cross-sample slices (Eq. (8)), rather than the more general definition of “ranking a candidate set per input/query.” This makes the choice of “list domain” heavily affected by batch and slicing, with limited extrapolative interpretability.\n4. There is no validation across conditions/domains/data sources; no changes of label scales (e.g., 0–3, 0–4), no alternative list domains, nor robustness reports under different noise/shift settings.\nTaken together, the current evidence only supports the weak claim that the method is “effective on C-STS.” Without systematic experiments across setups, the paper’s main claims lack external validity.\nW3 Insufficient theoretical and empirical support for hyperparameters and reward design.\nThe paper introduces many reward components and hyperparameters (Stage-I’s R_pointwise, R_binary, R_format and weight (λ); Stage-II’s (R_pairwise, R_listwise), baseline constant (R_base ), slice/batch sizes (N, G), and “maximum difference = 3,” etc.), but offers only intuitive motivation. It lacks (i) testable theoretical grounds/invariance guarantees (why the shaping does not change the optimal policy, or why it aligns with the target metric), and (ii) systematic necessity evidence (component-wise/hyperparameter ablations and significance tests). Current experiments cover only a minority of hyperparameters; several key choices remain unexplained or unvalidated, making it hard to disentangle method gains from empirical tuning."}, "questions": {"value": "Q1. Please add strong baselines for recent reasoning-style models under few-shot/CoT.\nQ2. The current setup relies heavily on the 1–5 label scale, adjacent pairing, and a “cross-sample slice” list domain. Please provide any empirical evidence for generalizability:\n1. Cross-condition/cross-domain/cross-source evaluation;\n2. Changing label scales and thresholds (non-1–5, non-≥3/≤2);\n3. Replace the list domain from “cross-sample slicing” to “per-input candidate set,” and compare stability/effectiveness;\n4. Robustness under noise and distribution shift.\nIf you do not intend to generalize, explicitly delimit the applicability scope and tone down the claims in the abstract/conclusion.\nQ3. Please provide testable rationale for “why these settings,” plus the full hyperparameter details and reasons. Explain the design motivation for each reward and its structure; for several key hyperparameters, add principled justifications or ablations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tKoU76jQ0f", "forum": "sLcRCH1U68", "replyto": "sLcRCH1U68", "signatures": ["ICLR.cc/2026/Conference/Submission18363/Reviewer_PEjZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18363/Reviewer_PEjZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830013681, "cdate": 1761830013681, "tmdate": 1762928071759, "mdate": 1762928071759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PoLi-RL, a novel Point-to-List reinforcement learning framework for Conditional Semantic Textual Similarity (C-STS). The work addresses the limitations of existing discriminative models by leveraging large language models (LLMs) and RL. To overcome the significant challenge of reward sparsity in this generative task, PoLi-RL reformulates the problem from a point-wise generation task into a list-wise ranking task. This approach effectively densifies the reward signal."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is built on a clear motivation and is well-written, with clean figures that effectively illustrate the proposed method.\n- The method is a reasonable and well-designed approach that directly addresses the issue of sparse rewards, which is a key challenge for applying naive RL to this task.\n- The experimental results are impressive. The paper provides comprehensive experiments and the corresponding analysis."}, "weaknesses": {"value": "The authors did not conduct experiments on smaller generative models (e.g., qwen 0.5B) that would be more comparable in parameter count to the discriminative models used as baselines. It is unclear how much of the impressive performance gap is attributable to the method itself versus the model's scale."}, "questions": {"value": "What is the base reward $R_{base}$ in equation (7)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qiCDMuKRuh", "forum": "sLcRCH1U68", "replyto": "sLcRCH1U68", "signatures": ["ICLR.cc/2026/Conference/Submission18363/Reviewer_fkXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18363/Reviewer_fkXh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014373155, "cdate": 1762014373155, "tmdate": 1762928071323, "mdate": 1762928071323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}