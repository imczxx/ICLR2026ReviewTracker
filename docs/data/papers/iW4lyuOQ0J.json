{"id": "iW4lyuOQ0J", "number": 16377, "cdate": 1758263928575, "mdate": 1759897244640, "content": {"title": "Accelerating Large Language Model Inference via Speculative Decoding with Progressive Tree Drafting", "abstract": "The draft-then-verify decoding paradigm, introduced by speculative decoding methods, has demonstrated remarkable performance in alleviating the memory-bound bottleneck and accelerating the inference speed of Large Language Models (LLMs) while maintaining the quality of generated content. Recent studies show that the intrinsic robustness of LLMs can be exploited in a training-free and architecture-agnostic manner, suggesting that auxiliary models or structural modifications are not strictly necessary for draft generation. However, existing methods fail to fully leverage this robustness, leading to substantial redundant and repeated computations. Building on this insight, we propose Progressive Tree Drafting (PTD), a new inference acceleration strategy that further extends this line of work. PTD organizes the drafting process into a progressively updated tree structure, where controlled perturbations are injected to guide generation and a stepwise pruning mechanism enabling the model to produce coherent yet diverse drafts at manageable computational cost. By efficiently coordinating the drafting and verification stages, PTD achieves up to 2$\\times$ decoding speedup across different open-source models and benchmarks. Our code is available at https://anonymous.4open.science/r/PTD-D354.", "tldr": "", "keywords": ["Speculative Decoding", "Inference Acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8845a01b523f074567253ec73c4af48adc65f32f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Progressive Tree Drafting (PTD), a new speculative decoding method that does not rely on an additial draft models and integrates tree-style drafting and verification process. At each drafting step, given a randomized initial tree, the model generates draft tokens after the given tree and a stepping step is conducted. When the draft tree reaches a certain depth, a pruning step is conducted to reduce the number of nodes in the tree. PTD introduces the on-tree drafting, pruning, and tree-verification to single-model speculative decoding, achieving a significant speedup ratio. Empirical results on MT-Bench compared with baselines (Speculative Decoding, Lookahead Decoding, and Self-Draft) fully show the efficiency of PTD."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Single-model speculative decoding is good scenario to focus on, since methods such as EAGLE cannot always be applied in some scenarios where no draft models can be deployed.\n\n2. Integrating tree-verification to single-model speculative decoding is a reasonable and efficient design. Unlike previous works, such design can reduce the redundancy of the drafting process, thereby increasing the decoding speed."}, "weaknesses": {"value": "1. The presentation of this paper is quite confusing. Most of the proposed methods are densely packed into Sections 3.2 and 3.3, filled with unclear formulas and redundant definitions. The pseudocodes in Appendices A and B are also difficult to follow. The authors should consider reorganizing these sections by removing redundant concepts, adding illustrative figures, and presenting the proposed methods in a clearer and more structured manner.\n\n2. Since this paper focuses on single-model speculative decoding, the authors need to explicitly explain why this setting is emphasized and under what circumstances a draft model cannot be used. The current discussion in the related work section does not make this point clear, leaving readers confused about why the EAGLE series are not suitable for all speculative decoding scenarios. For example, this can be clarified from a GPU-memory perspective: a draft model requires additional GPU memory, and Transformer-based draft models further consume memory through their KV-cache, which becomes a serious limitation for long-context inputs.\n\n3. The proposed method should be compared against stronger baselines such as REST [1] and PLD [2], which also operate without a draft model. If the scope of the paper is limited to single-model speculative decoding, it is unclear why SpeDe (the vanilla speculative decoding method) is still treated as a baseline.\n---\n\n[1] REST: Retrieval-Based Speculative Decoding\n\n[2] https://github.com/apoorvumang/prompt-lookup-decoding"}, "questions": {"value": "1. How do the authors position PLD within the speculative decoding community? What is its most defining characteristic: is it a training-free approach, a single-model (no draft model) design, or a faster speculative decoding method? Since the paper is not clearly written, I am unable to determine this with confidence, and therefore cannot fully assess the soundness of the experimental design. The authors should clearly highlight PLD’s key feature in the experimental section by selecting appropriate baselines and reporting metrics that correspond to that feature.\n\n2. The method section is also difficult to follow, which prevents me from forming a confident evaluation, despite my familiarity with speculative decoding methods. Therefore, I have assigned the lowest confidence rating to my review. I strongly recommend the authors provide further clarification and discussion of the method’s logic and assumptions. However, if the above issues remain unresolved, I suggest that the AC consider lowering the weight of my decision in the final evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "JU3Y6H828f", "forum": "iW4lyuOQ0J", "replyto": "iW4lyuOQ0J", "signatures": ["ICLR.cc/2026/Conference/Submission16377/Reviewer_GSLp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16377/Reviewer_GSLp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641811412, "cdate": 1761641811412, "tmdate": 1762926502425, "mdate": 1762926502425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the memory-bound bottleneck of autoregressive inference in Large Language Models (LLMs). It focuses on the \"draft-then-verify\" paradigm of speculative decoding, specifically on training-free methods that use the target LLM itself to generate drafts. The authors identify a key limitation in prior work like Self-Draft: the generation of redundant and highly similar draft branches, which leads to wasted computation. To solve this, the paper proposes Progressive Tree Drafting (PTD), a novel training-free and model-agnostic inference strategy. Instead of generating simple linear branches, PTD organizes the drafting process into a tree structure that is progressively expanded and pruned. This approach uses controlled perturbations and a custom tree-based attention mask to guide the LLM in generating a diverse set of candidate drafts simultaneously. The tree structure allows for efficient prefix sharing, while a stepwise pruning mechanism controls the computational cost. Experiments show that PTD achieves significant throughput improvements (up to 2x) compared to autoregressive decoding and outperforms other training-free baselines like LADE and Self-Draft across various benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly articulates a practical bottleneck in multi-branch linear drafting, where insufficient draft diversity produces redundant candidates and wastes computation.\n2. The paper proposes a progressive tree–based strategy that adaptively prunes tree width and depth to improve draft diversity."}, "weaknesses": {"value": "1. Although PTD starts from random branching, dynamic depth and width control with known tree-pruning schemes is already mature in SD with integrated tree verification (e.g., EAGLE-2, SWIFT, Spec-LLaVA, SpecVLM), so the novelty beyond applying these pruning schemes to multi-branch drafting is unclear. The paper should explicitly argue why existing tree-pruning methods are insufficient for multi-branch drafting, how PTD differs or extends them, and include focused comparisons in Related Work and Experiments.\n  - EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\n  - SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration\n  - Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding\n  - SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning\n  - ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding\n  - Faster Speculative Decoding via Effective Draft Decoder with Pruned Candidate Tree\n  - OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure\n  \n\n2. The current presentation appears to equate “diversity” with pruning of duplicate candidates. This risks an overclaim in the introduction: while true diversity could raise acceptance rates, the method section primarily describes deduplication—a compute-saving step that reduces drafting cost and, at best, maintains the same acceptance rate.\n\n3. The evaluated models are somewhat dated, and there is a lack of comparison with the more superior methods recognized by communities such as Medusa and EAGLE series. In addition, Fig. 8 suggests limited speedups on Qwen models; analyze why PTD underperforms there. The paper does not study how progressive width and depth are determined/tuned, nor the sensitivity of PTD to these choices—important under the self-drafting paradigm.\n\n4. The paper’s writing and organization could be further improved, for example (including but not limited to) the following:\n\n  * In Fig. 1(a), the introduction of perturbations is too abrupt. This prerequisite should be explained with concrete examples—what kinds of perturbations are used and how they produce multiple branches.\n  * In Fig. 1(b), the workload of the preliminary study is not well described. I am very curious how duplicate branches or tokens behave across different tasks, and whether there are more promising insights to enable a more flexible, domain-/context-aware dynamic tree design instead.\n  * Also in Fig. 1(b), please define precisely what constitutes a “duplicate step” (as I understand, generating the same token at the current step) versus a “duplicate branch” (the historical decoding tokens are also identical), and how similarity is measured.\n  * Fig. 2(c) makes the PTD pipeline hard to understand—I don’t know what the inputs and outputs are, nor where to start reading the diagram."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "97HWo46NVY", "forum": "iW4lyuOQ0J", "replyto": "iW4lyuOQ0J", "signatures": ["ICLR.cc/2026/Conference/Submission16377/Reviewer_peV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16377/Reviewer_peV3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911621473, "cdate": 1761911621473, "tmdate": 1762926501874, "mdate": 1762926501874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an inference acceleration method named Progressive Tree Drafting (PTD), a training-free and model-agnostic approach to speculative decoding. The core motivation is to address the computational redundancy in existing self-drafting methods like Self-Draft, which arises from the high similarity among draft branches. PTD organizes the draft generation process by maintaining a dynamically expanding and pruned tree structure, aiming to generate draft sequences that are both diverse and coherent through structured perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong Novelty: The core idea of a \"Progressive Drafting Tree\" is novel. Structuring the draft generation process into a controllable tree, which uses prefix sharing and pruning to balance draft diversity, coherence, and computational cost, is a valuable contribution.\n2. Clear Motivation: The paper clearly identifies a key bottleneck in existing methods (computational redundancy) by analyzing the branch similarity of Self-Draft (Figure 1b), making the proposed solution highly targeted. Furthermore, the experimental evaluation covers a range of mainstream open-source models and diverse tasks (dialogue, math, code), demonstrating the method's generalizability."}, "weaknesses": {"value": "1. Rough Writing and Presentation: The paper's overall writing and structure appear unpolished. Table captions are brief and lack critical information; for instance, the caption for Table 1 does not clarify which baseline (AR) the \"Imp.\" (Improvement) is relative to. More seriously, the baseline results for Speculative Decoding (SpeDe) on several models (the Qwen series) are marked with a backslash (`\\`) without any explanation, which undermines the rigor and completeness of the experiments.\n2. Questionable Output Consistency: In theory, speculative decoding with rejection sampling should be lossless, meaning its output distribution is identical to that of the original autoregressive model. However, the ROUGE/BLEU scores in Appendix Table 2 are far from 100, suggesting that PTD alters the model's output in sampling mode. The authors must explain the source of this discrepancy.\n3. Vague Methodological Details: The description of key details is vague. For instance, the initialization of the draft tree with \"randomly initialized perturbation tokens\" in Section 3.2 is not clearly explained."}, "questions": {"value": "The current version suffers from shortcomings in its writing, presentation, and experimental rigor. I encourage the authors to add more details and further polish this novel method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1EuZgp8qdm", "forum": "iW4lyuOQ0J", "replyto": "iW4lyuOQ0J", "signatures": ["ICLR.cc/2026/Conference/Submission16377/Reviewer_gWN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16377/Reviewer_gWN1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978951729, "cdate": 1761978951729, "tmdate": 1762926501049, "mdate": 1762926501049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Progressive Tree Drafting (PTD), a training-free speculative decoding framework for LLM inference acceleration. Building on the insight that LLMs exhibit semantic robustness under controlled perturbations, PTD extends prior approaches like Self-Draft by organizing draft generation into a progressively expanded tree structure with prefix-sharing, branch pruning, and branch-wise perturbation. The method generates multiple coherent draft sequences and verifies them in parallel without requiring auxiliary draft models or architectural changes. Experiments across multiple LLaMA, Qwen, and CodeLLaMA models show up to 2× throughput improvement, outperforming LADE and Self-Draft, particularly on GSM-8k and MBPP, with competitive generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Method is well-motivated with illustrative diagrams (tree expansion, mask structure) and formal algorithm descriptions.\n\n2. The experimental evaluation covers general QA, math reasoning, and coding tasks, with consistent speedups. The ablation studies on tree depth, branch width, and sampling strategies clearly demonstrate the characteristics of the proposed method.\n\n3. The results provide stronger performance than both LADE and Self-Draft, highlighting real gains over recent state-of-the-art baselines."}, "weaknesses": {"value": "1. Limited comparison to tree-based speculative frameworks (e.g., SpecInfer, EAGLE-2 dynamic draft trees).\n\n2. The method still introduces non-trivial verification overhead, and overhead trends at larger scales (>32B parameters, >4K tokens) are not fully explored.\n\n3. The choice of specific hyperparameters (e.g., max children=4, depth=6) appears arbitrary without an analysis of their impact across different model scales or task types."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VVxucC9DfM", "forum": "iW4lyuOQ0J", "replyto": "iW4lyuOQ0J", "signatures": ["ICLR.cc/2026/Conference/Submission16377/Reviewer_stwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16377/Reviewer_stwC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007040378, "cdate": 1762007040378, "tmdate": 1762926500422, "mdate": 1762926500422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}