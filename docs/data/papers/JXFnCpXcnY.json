{"id": "JXFnCpXcnY", "number": 24205, "cdate": 1758354039744, "mdate": 1763738064019, "content": {"title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers", "abstract": "Psychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data (R² > 0.88). This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information—adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants' psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.", "tldr": "", "keywords": ["Large Language Model", "Psychological Profiling", "Human Simulation", "Zero-Shot Prediction", "Reasoning Trace Analysis"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/635b30da7827809d8196f004cf46855b5b01cdff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a controlled paradigm to test whether large language models can infer psychological structures from the big-5 personality traits of a user. Given individual responses from 20 elements in a big-5 questionnaire, models are prompted to role-play that individual across 9 other, distinct, psychological scales. In the first experiment, the paper compares the inter-scale correlation matrices between. model-generated scores and human ground-truths, showing that LLMs often closely reproduce humans' correlation patterns (Gemini, R=0.92), and that they display an amplification effect in the correlations. In a second experiment, the paper investigates how models achieve this correlation, by focusing on the reasoning traces generated during thinking, showing that models are consistent (there is high correlation between the attribution vectors of different reasoning models), but do not have fine-grained attribution ability (instead focusing on modre global patterns), which is confirmed by ablation (using a summary in addition to the original numeric elements)."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This is an interesting paper, which introduces a fairly novel way of evaluating how well LLMs can capture and express personalities. The experiments are well thought out, and they show some interesting artifacts, particularly the results on attribution and COT reasoning/summarization. The methodology is strong, and the results are convincing (and likely reproducible). The work is reasonably clear in its core motivation and experimental pipeline, and the finding that LLMs can reconstruct and amplify latent psychological structure from minimal inputs has potential implications for downstream applications (particularly in behavioral simulation)"}, "weaknesses": {"value": "- The related work section is somewhat thin on detail, and largely ignores a rich set of works designed to approximate specific behaviors, even up to the Big-5 personality traits. Some relevant related work beyond the park study of LLMs include [1,2,3,4,5,6,7,8,9]. It's really not clear to me that the paper significantly differentiates itself solely by \" introducing a well-controlled psychometric paradigm\", since many of these other paper do the same (either in the domain of behavior, or opinion). There are also papers which use Big-5 as conditioning, such as [13,14], and the ability of LLMs to emulate personality was explored in [15] (which is quite similar to this work). \n- There's no ablation of the experimental prompt in A.1. It's likely that using a system paradigm, with the response in the user, compared to several back to back user turns will affect the performance of the model (and the strength of the correlations). Similarly, it's likely that the wording of the system prompt has impacts on the output. It would be good to perform some analysis accounting for that. In addition, it's interesting that all models here are instruction-tuned models: how does this change if a base model is used instead?\n- Some of the results seem a bit cherry-picked per-model/experiment. For example, Figure 2 only discusses Gemini, but how does this change for other models? \n\nMore minor issues:\n- The scales used are never discussed in the main text, and only included in Appendix D.\n- Language models are only discussed in Appendix E (which makes it really hard to follow what the overall experiments are). \n- I'm not entirely sure how novel the \"amplifcation\" idea is. Several papers [10,11,12] have explored how LLMs amplify biases (and much of this tracks with how LLMs perform maximum likelihood prediction). \n- The presentation of the paper is a bit dense, and wanders a lot, making it hard to understand exactly what is being tested, what the research questions are, and what experiments are being performed on which models. Restructuring the paper around clear RQs and hypotheses would significantly improve the readability of the paper. \n- There's no pipeline ablation in this paper (ie. the annotation model in Figure 3), which makes it hard to understand if the underlying results are from biases in the annotation models, or biases in the data itself. \n\n[1] Ziems, Caleb, et al. \"Can large language models transform computational social science?.\" Computational Linguistics 50.1 (2024): 237-291.\n[2] Dillion, Danica, et al. \"Can AI language models replace human participants?.\" Trends in Cognitive Sciences 27.7 (2023): 597-600.\n[3] Aher, Gati V., Rosa I. Arriaga, and Adam Tauman Kalai. \"Using large language models to simulate multiple humans and replicate human subject studies.\" International conference on machine learning. PMLR, 2023.\n[4] Tjuatja, Lindia, et al. \"Do llms exhibit human-like response biases? a case study in survey design.\" Transactions of the Association for Computational Linguistics 12 (2024): 1011-1026.\n[5] Choi, Hyeong Kyu, and Yixuan Li. \"Picle: Eliciting diverse behaviors from large language models with persona in-context learning.\" arXiv preprint arXiv:2405.02501 (2024).\n[6] Hilliard, Airlie, et al. \"Eliciting personality traits in large language models.\" arXiv preprint arXiv:2402.08341 (2024).\n[7] Santurkar, Shibani, et al. \"Whose opinions do language models reflect?.\" International Conference on Machine Learning. PMLR, 2023.\n[8] Kang, Minwoo, et al. \"Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions.\" Second Conference on Language Modeling.\n[9] Suh, Joseph, et al. \"Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors.\" NeurIPS 2024 Workshop on Behavioral Machine Learning.\n[10] Taori, Rohan, and Tatsunori Hashimoto. \"Data feedback loops: Model-driven amplification of dataset biases.\" International Conference on Machine Learning. PMLR, 2023.\n[11] Wang, Ze, et al. \"Bias Amplification: Large Language Models as Increasingly Biased Media.\" arXiv preprint arXiv:2410.15234 (2024).\n[12] Peña Fernández, Simón, et al. \"Without journalists, there is no journalism: the social dimension of generative artificial intelligence in the media.\" Profesional de la información 32.2 (2023).\n[13] Li, Wenkai, et al. \"Big5-chat: Shaping llm personalities through training on human-grounded data.\" arXiv preprint arXiv:2410.16491 (2024).\n[14] Vu, Huy, et al. \"Psychadapter: Adapting llm transformers to reflect traits, personality and mental health.\" arXiv preprint arXiv:2412.16882 (2024).\n[15] Wang, Yilei, et al. \"Evaluating the ability of large language models to emulate personality.\" Scientific reports 15.1 (2025): 519."}, "questions": {"value": "- How does this change for non instruction-tuned models? it seems like many of these artifacts may come from the optimization process during RLHF or similar tuning processes. \n- Is there a correlation between length of the thinking trace, and some of the \"summary\" behavior? It feels like in some cases, the thinking trace performs as a summary for the model, allowing it to reason/perform test-time compute instead of directly producing an output."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KPvapAyHWS", "forum": "JXFnCpXcnY", "replyto": "JXFnCpXcnY", "signatures": ["ICLR.cc/2026/Conference/Submission24205/Reviewer_GETG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24205/Reviewer_GETG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795194346, "cdate": 1761795194346, "tmdate": 1762942993709, "mdate": 1762942993709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a study of the ability of LLMs using chain-of-thought prompting to reconstruct personality profiles using psychometric data and use this information to reconstruct scores on other psychometric measures in a coherent and consistent manner compared to the original human behavioral scores. By analyzing reasoning traces, the authors find that LLMs are reasoning from representations and not simply performing semantic pattern matching."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-argued and well-structured. By comparing scores across a range of psychometric tests, the experimental design address a fundamental gap in the literature, namely with regard to the coherence and stability of LLM personalities. Further, the experimental setup allows for comparisons across models and across learning algorithms, which strengthens the findings.\nThe paper offers an insightful analysis and discussion of LLMs tendency towards idealized representations."}, "weaknesses": {"value": "The correlational analysis of Experiment 1 presented in 3.2 is interesting from an exploratory perspective but it is not very convincing and slightly problematic: Multiple comparisons are discouraged and should be replaced with more robust multivariate tests. Indeed, experiment 2 and the subsequent analysis in 4.2 is much more conclusive."}, "questions": {"value": "P30L130: \" Our findings chart a path up this hierarchy, demonstrating that LLMs’ cognitive process (1) transcends surface-level statistical association, (2) prioritizes abstract conceptual structure over specific item-level details, and (3) performs a novel form of theoretical idealization, actively refining noisy inputs into theory-consistent representations\"\n=> It's not immediately clear how a correlational study, even a sophisticated one, can demonstrate that LLMs prioritize abstract conceptual structure.The experimental setup prompts chain-of-thought reasoning. Hence, how is any evidence of abstract reasoning not simply an artifact of the prompt structure?\n\nP5-6L264-276: \"We performed two further analyses to validate the observed linear structural amplification is genuine and robust. First, we established statistical significance using a 1,000-trial permutation test. [...] Second, to verify the effect arises from genuine understanding rather than semantic mapping, we examined the model’s sensitivity to task framing or input ordering (Oren et al., 2023) by testing three conditions: our original setup (Standard Order), a setup with shuffled input items (Random Order), and a setup where each prediction was made individually (Single Question\"\n=> This is good for excluding context effects like chat history, but I fail to see how this validates that the amplification is not a result of chain-of-thought prompting.\n\nP7L344-247: \"This clear dissociation uncovers a key insight into the models’ reasoning: LLMs robustly identify the correct personality factor (e.g., Neuroticism) but struggle to differentiate the importance of specific items within that factor. This supports the hypothesis of a top-down, concept-driven process where high-level abstract knowledge guides the inference, even if low-level execution is imprecise.\"\n=> Or the abstraction is belied by the lack of internal representation. This confusion could equally result from semantic pattern matching. How do you exclude this possibility? The high levels of noise suggest that the regression models are incomplete. Did you prompt the models to explain their choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "36RdVsOflP", "forum": "JXFnCpXcnY", "replyto": "JXFnCpXcnY", "signatures": ["ICLR.cc/2026/Conference/Submission24205/Reviewer_UCxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24205/Reviewer_UCxr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956637119, "cdate": 1761956637119, "tmdate": 1762942993302, "mdate": 1762942993302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether LLMs can recover the latent structure of personality. (1) Given only each person’s Big Five item responses, models ‘role-play’ the person on other 9 psychological scales, and the resulting correlation structure closely mirrors human data but with a consistent, stronger signal. (2) Analysis of generated reasoning traces suggest a two-stage process: compressing human responses into summaries, then reasoning from those summaries to answer new items. Controls rule out simple semantic matching and indicate the effect isn’t due to chance. The authors argue the amplification reflects “idealized version” of human participants, with LLMs filtering human noise."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Moving from the first‑order prediction to the second‑order correlation‑structure analysis is an interesting reframing of the psychology questions with LLMs. The heatmaps and regression plots in Figure 2 (page 5) compellingly visualize the phenomenon.\n\n- An attempt to mechanism‑seeking analysis: factor‑level and item‑level attribution comparison in Figure 4, and amplification depending on information type in Figure 5 are good steps toward understanding the model's decisions rather than reporting raw scores alone."}, "weaknesses": {"value": "- Related Work (Section 2) significantly lacks depth to situate the paper within existing literature.\n\nIn particular, it overlooks a growing body of research suggesting that LLMs exhibit latent psychological structure of humans. Few papers I found by search are “Personallm: Investigating the ability of gpt-3.5 to express personality traits and gender differences” (Jiang et al.), “Personality traits in large language models” (Serapio-Garcia et al.), “Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors” (Suh et al.), to name a few. Currently the related work is a bland listing of a few disconnected sets of papers, and must be strengthened to understand the value of this paper.\n\n- LLMs, especially reasoning models, would have utilized its ‘knowledge’ of correlation between traits rather than role-playing.\n\nAlthough inputs to LLM are just Big-Five item results, LLM still reads what are the target items it will predict (e.g., “ERQ questionnaire items” in Line 782). Their well‑known semantics (i.e., LLMs know what is ERQ, and possibly how it is related to Big-Five items from pretraining corpus and past psychological research results) could let the model rely on learned associations between Big-Five traits and those constructs rather than inferring a structure unique to this dataset. I think this is especially prevalent when using reasoning models. Reasoning models tend to produce reasoning trajectories in terms from a third-person perspective rather than actual role-playing a given persona / traits, try to come up with logical explanations, far from what the paper claims LLMs do to make a prediction. However, from the current paper I cannot see how the model actually works nor can run reproducible experiments; it would be great to see the real example of Figure 3 (“reasoning-to-annotation” analyses).\n\n- Is ‘linear amplification’ an ideal thing, and isn’t it confounded with unreliability (Section 3)?\n\nThe slope k>1 is interpreted as “idealization”. However, when the human correlation matrix is attenuated by measurement variance and inherent variability during the human's choice making process, while the LLM prediction correlation matrix is less noisy, regressing the latter on the former will naturally produce a slope k>1. Also, internal consistency (measured in terms of Cronbach’s alpha, for example) of too high values is not necessarily desirable. Without disattenuating human correlations for scale reliabilities (e.g., Cronbach’s alpha per subscale) or applying classical correction for attenuation, the amplification claim risks restating a reliability artifact.\n\n- Reproducibility.\n\nI checked the repo, and there are only .pdf files of questionnaires and no code available for analysis and experiment, limiting the reproducibility of the experiment since authors have brought multiple ML models for evaluation (Figure 2)."}, "questions": {"value": "There are multiple vague terminologies that I ask authors to clarify.\n\n- Line 58 “the test should enable mechanistic interpretability”, line 75 “to further achieve interpretability”, line line 464 “using advanced mechanistic interpretability” -> I don’t get why authors mention mechanistic interpretability. This paper is completely irrelevant to mechanistic interpretability. Could authors please describe how their test enables mechanistic interpretability, e.g. is it able to establish casual relationships through interventions on activations?\n\n- Line 25: “analogous to generating sufficient statistics” -> How is it analogous to sufficient statistics? In the paper, there is no description why and how generating natural language personality summaries is relevant to sufficient statistics..\n\n- Line 193: “linear amplification” -> is linear amplification a terminology used in the community? I think the analysis done here is effectively summarized as calculating correlation coefficients between entries in LLM-prediction correlation matrix and human correlation matrix, getting the coefficient > 1.\n\n- Line 151: “ops‑bge‑reranker‑larger” -> there is no model named “ ops-bge-reranker-larger” exists, looks like a hallucinated model label.\n\nI would like to ask authors about representativeness, availability, and ethical approval detail of 816 human subjects.\n\n- Representativeness: is it a representative sample of the entire population?\n\n- Availability: It “was collected during COVID-19 as a part of a larger study” (Line 143), is it published somewhere as a publicly accessible dataset? If so, please mention the original data source rather than just a description of it. If not, please mention the future availability of it for reproducibility.\n\n- Ethical approval detail: following ICLR code of ethics, I would like to ask authors for more details of human subject recruitments, including IRB process, documentation, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "T48Bov42ar", "forum": "JXFnCpXcnY", "replyto": "JXFnCpXcnY", "signatures": ["ICLR.cc/2026/Conference/Submission24205/Reviewer_Pih4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24205/Reviewer_Pih4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977820670, "cdate": 1761977820670, "tmdate": 1762942991788, "mdate": 1762942991788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors show good alignment between LLM predictions and human psychological structure comparing inter-scale correlation LLMs patterns between LLM predictions and human data. This is stark contrast with recent results by Zhu et al. (2025) showing poor LLM alignment when inferring personality of a narrator from a given text.\n\nThe authors claim that the reason of this contrast is that though LLMs struggle to predict specific traits they tend to amplify the correlations between scores on different scales that human subjects show.\n\nThis contradiction is particularly interesting, in my opinion, and might span a very fruitful line of further research."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors suggest a great research question, find an answer to it and demonstrate the validity of the answer in a convincing way."}, "weaknesses": {"value": "I am not an expert in classical psychology literature, but some of the human-score correlations should be taken with a grain of salt due to the fact that they are often reported on small selection of subjects that are not representing the scale of human experience that an LLM is exposed to. In this context the stronger correlation that the authors report is even more surprizing to me, especially in the context of the later remark on more \"attentive\" subjects."}, "questions": {"value": "Could the correlation amplification phenomenon you find be the result of post-training in the alignment phase, when models are trained to simulate an agreeable personality?\n\n\n\nHere are some further works that could be informative for the scope of the work:\n\nSorokovikova A, Rezagholi S, Fedorova N, Yamshchikov IP. LLMs Simulate Big5 Personality Traits: Further Evidence. InProceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024) 2024 Mar (pp. 83-87).\n\nPan X, Gao D, Xie Y, Chen Y, Wei Z, Li Y, Ding B, Wen JR, Zhou J. Very large-scale multi-agent simulation in agentscope. arXiv preprint arXiv:2407.17789. 2024 Jul 25.\n\nDong W, Zhao Y, Sun Z, Liu Y, Peng Z, Zheng J, Zhang Z, Zhang Z, Wu J, Wang R, Xu S. Humanizing llms: A survey of psychological measurements with tools, datasets, and human-agent applications. arXiv preprint arXiv:2505.00049. 2025 Apr 30.\n\nTshimula JM, Nkashama DJ, Muabila JT, Galekwa RM, Kanda H, Dialufuma MV, Didier MM, Kalonji K, Mundele S, Lenye PK, Basele TW. Psychological Profiling in Cybersecurity: A Look at LLMs and Psycholinguistic Features. InInternational Conference on Web Information Systems Engineering 2024 Dec 2 (pp. 378-393). Singapore: Springer Nature Singapore."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ps2bIGgpC4", "forum": "JXFnCpXcnY", "replyto": "JXFnCpXcnY", "signatures": ["ICLR.cc/2026/Conference/Submission24205/Reviewer_GtxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24205/Reviewer_GtxT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027003854, "cdate": 1762027003854, "tmdate": 1762942991569, "mdate": 1762942991569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}