{"id": "yiE16lWzDj", "number": 14555, "cdate": 1758238824187, "mdate": 1759897362784, "content": {"title": "Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning", "abstract": "Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text space. To enhance LLMs' tool-calling abilities, previous approaches primarily rely on supervised fine-tuning (SFT) with trajectories distilled from stronger models, often resulting in imitative reasoning that limits generalization. In this work, we explore rule-based reinforcement learning to enhance tool-calling in LLMs, resulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning models. Rather than enforcing supervision over intermediate distilled reasoning traces, Tool-N1 is trained with a binary RL reward that assesses only the format validity and functional correctness of tool invocations. This lightweight supervision allows the model to develop reasoning strategies independently, without relying on annotated trajectories. Experiments on several major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We conduct a systematic study on the design of rule-based reinforcement learning strategies for training tool-calling models. Using 5,518 distilled reasoning trajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that the widely adopted SFT-then-RL paradigm does not necessarily outperform pure RL.", "tldr": "", "keywords": ["Tool-using", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8212f0e32450e0f0b4b7cdea07f8d02acd2c0b26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a family of open-source tool-calling LLMs trained via rule-based reinforcement learning (R1-style RL) rather than conventional supervised fine-tuning (SFT). The key innovation lies in using a binary reward that verifies only output format and tool-call correctness, omitting supervision on reasoning trajectories. Besides the main experiments, the authors further conduct systematic ablations, finding that pure RL outperforms SFT-then-RL, that binary rewards outperform fine-grained ones, and that structured reasoning tags (<think>, <tool_call>) are essential for generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper demonstrates that pure rule-based RL can outperform traditional SFT and SFT+RL pipelines for tool-use, which is an important insight contradicting existing practice.\n\n2. Tool-N1 models surpass GPT-4o and other closed- and open-source tool models (e.g., Hammer2.1, xLAM-2) on all tested benchmarks, validating the method’s effectiveness.\n\n2. This paper includes ablations on reward granularity, data composition, model scaling, and backbone choice, offering valuable design guidance for future RL-based tool-use research."}, "weaknesses": {"value": "1. The paper’s contribution is largely empirical; it lacks theoretical insight into why rule-based RL generalizes better than SFT or how binary rewards shape reasoning policies.\n\n2. A binary {0,1} reward could cause credit assignment inefficiency or instability, yet no analysis or mitigation (e.g., shaping, curriculum) is discussed."}, "questions": {"value": "No extra question. See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qsAsUffx9y", "forum": "yiE16lWzDj", "replyto": "yiE16lWzDj", "signatures": ["ICLR.cc/2026/Conference/Submission14555/Reviewer_5Uca"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14555/Reviewer_5Uca"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795039677, "cdate": 1761795039677, "tmdate": 1762924945108, "mdate": 1762924945108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Nemotron-Research-Tool-N1 (Tool-N1), a series of tool-calling language models trained using a rule-based reinforcement learning (RL) paradigm. The core contribution is the application of a \"R1-style\" RL approach, which uses a simple binary reward to judge the format and functional correctness of tool calls, without supervising the intermediate reasoning steps. The authors conduct a systematic empirical study, demonstrating that their method outperforms strong baselines, and intriguingly, that a pure RL training pipeline can surpass the commonly adopted \"SFT-then-RL\" paradigm in this domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper effectively argues that supervised fine-tuning (SFT) on distilled reasoning traces can lead to \"imitative\" or \"pseudo\" reasoning. The proposed rule-based RL method is a compelling alternative that incentivizes the model to develop its own internal reasoning strategies, promoting better generalization. The motivation is clear and grounded in current limitations of the field.\n- Strong Empirical Evidence.\n- Rigorous and Insightful Ablation Studies."}, "weaknesses": {"value": "- There is no computational effiency analysis. How fast is it running compared to baselines? Especially, how the speed becomes when the number of candidate tools becomes large? (since LLM receives the textual input with all the tools included)\n- There should be some brief introductions to recent tool-calling baselines, either in Section 4.1 or a Related Work section.\n- There is no public code or models which makes the experiment replication difficult.\n- There is no explicit paragraph of major contributions in Introduction."}, "questions": {"value": "- How the name \"NEMOTRON-RESEARCH-TOOL-N1\" is determined? There seems to be no evident hint in the paper. Considering to use a shorter name instead?\n- How is the Reward Model trained? There should be some explicit introduction (even simply refering to some popular methods such as Bladley-Terry) to make the methodology self-contained.\n- The training is conducted by GRPO. How would the performance change if using PPO instead? Is there any insights compared these two RL methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ICoZz5WPsH", "forum": "yiE16lWzDj", "replyto": "yiE16lWzDj", "signatures": ["ICLR.cc/2026/Conference/Submission14555/Reviewer_Emqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14555/Reviewer_Emqt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906079798, "cdate": 1761906079798, "tmdate": 1762924944556, "mdate": 1762924944556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a rule-based reinforcement learning method to enhance the tool calling abilities of LLM based on GRPO. Their method uses a binary reward function with light supervision, e.g. it only checks functional correctness and format validity. They claim that it improves the generalization abilities vs conducting SFT (no experiments are provided to back up such claim). The authors present results that their method improves performances by conducting evaluation using a variety of tool calling benchmarks: BFCL, ACEBench and API bank. The authors also present a thorough set of ablations to validate their design choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors present a very thorough set of ablations to validate their design choices: \n- SFT vs RL vs SFT+RL, \n- Scaling laws that showcase that using a more powerful model is better and good generalization accross different LLMs as backbones\n- Ablation on reasoning format vs not using it\n- Ablation on binary vs more fine grained reward function\n- Ablation on data decomposition\n\n- The paper is well written and easy to follow"}, "weaknesses": {"value": "- Novelty and originality: It is hard to assess the overall contribution since no other baselines are presented of works that provide ways to interweave reasoning with tool calling."}, "questions": {"value": "- What percentage of data gets filtered out when you remove the samples with invalid tool calls?\n- You claim throughout the paper that it improves reasoning and doesn't lead to pseudo-reasoning. However, there is no experiment to back up this claim. How can you show case that this is indeed the case experimentally?\n- The QWEN model used is outdated, it would be good to present some results with more recent models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cx1xwWN108", "forum": "yiE16lWzDj", "replyto": "yiE16lWzDj", "signatures": ["ICLR.cc/2026/Conference/Submission14555/Reviewer_K5pn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14555/Reviewer_K5pn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990441535, "cdate": 1761990441535, "tmdate": 1762924944148, "mdate": 1762924944148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}