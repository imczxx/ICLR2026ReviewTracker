{"id": "90tCp2KszA", "number": 12065, "cdate": 1758205484274, "mdate": 1763728298732, "content": {"title": "RECAST: Expanding the Boundaries of LLMs' Complex Instruction Following with Multi-Constraint Data", "abstract": "Large language models (LLMs) are increasingly expected to tackle complex tasks, driven by their expanding applications and users' growing proficiency in crafting sophisticated prompts. However, as the number of explicitly stated requirements increases (particularly more than $10$ constraints), LLMs often struggle to accurately follow such complex instructions, which limits their applicability in complex real-world scenarios. To the best of our knowledge, existing datasets do not exceed 10 constraints per instance. To address this challenge, we propose RECAST, an efficient and scalable framework for synthesizing datasets where each example incorporates far more constraints than those in existing benchmarks, aiming to challenge and extend the boundaries of models’ ability to follow complex instructions. These constraints are extracted from real-world prompt-response pairs to ensure practical relevance. Using this framework, we construct RECAST-$30$K, a large-scale, high-quality dataset comprising $30$k instances spanning $19$ constraint types. Experimental results demonstrate that models fine-tuned on RECAST-30K substantially improve in following complex instructions while maintaining their general capabilities without degradation. Moreover, RECAST enables automatic verification of constraint satisfaction via rule-based validators for quantitative constraints and LLM-based validators for qualitative ones, the verifiability provided by RECAST enables the design of reward functions for reinforcement learning, which further boosts model performance on complex and challenging tasks.", "tldr": "We propose an efficient method for synthesizing high-quality data to enhance the complex instruction-following capabilities of large language models (LLMs).", "keywords": ["LLM", "Complex Instruction Following", "Data synthesis", "Reinforement Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d1e403116d1f05ef60d2bcfb07f47ec344702a4.pdf", "supplementary_material": "/attachment/a9f52a4b35d0b077538fbba0fb46719a4f185416.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RECAST, an automated framework for generating instructions comprising far more constraints than those in current benchmarks and corresponding responses. Using this framework, the authors construct RECAST-30K, which contains 30K instances with diverse constraint types. The dataset is then used for training LLMs via RLVC, an optimization method that leverages the verifiable nature of constraints to provide fine-grained reward signals. Experiments demonstrate the effectiveness of RECAST compared with other complex instruction fine-tuning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework incorporates an extended number of constraints in a single instruction, which facilitates the evaluation and improvement of LLMs' capability of following more complex instructions as the tasks for LLMs are becoming increasingly complicated.\n2. The framework is automated and scalable, providing an efficient approach of synthesizing datasets with multiple constraints in instructions.\n3. LLMs trained on data from this framework exhibit improved performance and good generalization on instruction following tasks, indicating the effectiveness of this framework."}, "weaknesses": {"value": "1. Some model-based constraints may not suitable for a binary evaluation. For example, for the \"Helpfulness\" constraint, it is common for LLMs to generate two helpful responses but with discrepant levels. If both of them are judged as satisfying the constraint, the gap between the two responses will be eliminated, which is not conducive to fine-grained model performance optimization.\n2. The effectiveness of RLVC on reasoning models are not validated. Both Qwen-2.5-7B and Llama-3.1-8B are non-reasoning base models. Considering the widespread application of the reasoning model, it is necessary to validate the effectiveness of RLVC on base models with reasoning capabilities.\n3. In Table 1, 2 and 3, it seems that the percent sign of the result values is missing. And the presentation form of results is also inconsistent across different contexts (e.g. \"achieves a 31.25% average satisfaction rate\" around line 349 and \"achieves average scores of 34.64\" around line 373)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLlOMFSME7", "forum": "90tCp2KszA", "replyto": "90tCp2KszA", "signatures": ["ICLR.cc/2026/Conference/Submission12065/Reviewer_LBsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12065/Reviewer_LBsv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838788296, "cdate": 1761838788296, "tmdate": 1762923036849, "mdate": 1762923036849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a framework to construct lagre-scale datasets to test Large Language Models's instruction-following ability when facing complex tasks. Different from previous works, the constructed dataset encompasses various types of constraints, including both subjective and objective ones, and features a larger number of constraints than existing datasets. Furthermore, this study adopts both supervised fine-tuning and reinforcement learning to enhance the Large Language Model's capability to follow instructions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This submission has the following strengths:\n- The paper demonstrates clear writing and a well-structured organization.\n- The proposed dataset RECAST-30K is large in scale and contains an adequate number of constraints.\n- Experimental results have shown that training with RECAST can effectively enhance large language model's ability in instruction following."}, "weaknesses": {"value": "This submission has the following weaknesses:\n- For model-based constraints, the quality depends on used large language models.\n- The count of Rule-based constraints are much less than model-based constraints."}, "questions": {"value": "I have the following questions / suggestions:\n- Would it be possible to extend the constraints numbers of rule-based constraints?\n- Why HSR metric is not used in Table 1?\n- Why the performance of RECAST-30K-RLVC on Qwen-2.5-7B become worse than RECAST-30K-SFT from time to time? (Bottom of Table 1)\n- It would be better to also test some instruction-following enhancement works. For example,\n    - Branch-Solve-Merge Improves Large Language Model Evaluation and Generation. In NAACL. 2024.\n    - Divide-Verify-Refine: Can LLMs Self-align with Complex Instructions?. In Findings of ACL. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fhcbMew7nw", "forum": "90tCp2KszA", "replyto": "90tCp2KszA", "signatures": ["ICLR.cc/2026/Conference/Submission12065/Reviewer_FXJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12065/Reviewer_FXJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940603247, "cdate": 1761940603247, "tmdate": 1762923036293, "mdate": 1762923036293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RECAST, a data-synthesis framework that mines verifiable constraints from existing instruction-response pairs and rewrites the original instructions to include many more constraints. The authors use this pipeline to build a training dataset RECAST-30K (~ 30k examples; 19 constraint types), with both rule-based and model-based validators (LLM-as-judge) attached to each constraint. They also propose RLVC (Reinforcement Learning with Verifiable Constraints) which turns per-constraint satisfaction into a fine-grained reward and optimizes policies via GRPO. They also introduce a new benchmark, RECAST-Test, with 4 difficult levels. The experiments show sizable gains in hard satisfaction rate on RECAST-Test benchmark, as well as improvements on IFEVAL and FollowBench, while largely preserving general knowledge performance (evaluated with GPQA and MUSR). Human evaluations support the quality of constraint filtering and selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear motivation and problem:** The paper targets a real gap, LLMs’ performance drops as the number of explicit constraints increases, and existing SFT datasets usually don’t cover a high number of constraints. \n\n- **The pipeline is complete and reproducible:** The paper details each stage (constraint extraction, instruction enhancement, response synthesis, validation), gives prompts/templates, and reports human agreement metrics. RLVC is specified with GRPO objective and training setup. This level of detail supports reimplementation. The authors promised to release code, data, and trained models upon acceptance. \n\n- **Evaluation setup is strong:** The primary metric, HSR, is well defined (satisfies all constraints simultaneously) with subtype metrics RSR/MSR for rule/model-based subsets. The proposed benchmark RECAST-Test (cover 4 difficulty levels) and it is associated with two external sets (IFEVAL, FollowBench) for the task. Also, the work included a general ability evaluation with GPQA/MUSR, which is important.\n\n- **Empirical gains & informative analyses:** RECAST-30K improves complex instruction-following against multiple baselines, with RLVC further improving especially at higher difficulty levels. The paper includes informative ablations (constraint type only, quantity caps, component removals) and training dynamics for RLVC that illustrate different learning curves for rule- vs. model-based constraints. It also contains Human evaluation to back up the automated choices."}, "weaknesses": {"value": "- **Lack of theoretical positioning against RLVR methods:** The paper does not situate its RL formulation within the growing line of Reinforcement Learning from Verifiable Rewards (RLVR) research, despite clear conceptual overlap. Despite the original paper of RLVR being cited [1], it is cited only in the context of the dataset. Other foundational RLVR works [2,3] and subsequent works applying the verifiable reward mechanisms to multi-constraint or format-constrained instruction-following [4,5] are not cited or compared against. As a result, the contribution of RLVC relative to existing verifiable-reward frameworks remains under-theorized: it is unclear whether the proposed per-constraint reward structure offers advantages over scalar verifiable rewards, or whether similar behavior would emerge from established RLVR baselines. Strengthening this connection and including appropriate baselines would significantly clarify the novelty and necessity of RLVC.\n\n\\* It is acceptable to not cite works that appeared less than 2 months from the submission deadline, but in that case still recommendable for the final version. \n\n\n\n- **Generalization:** Parts of the training data and evaluation share the same constraint types and verification machinery. Although external benchmarks are included, they also contain very similar constraint types. I propose the authors to present stronger isolation tests (e.g., unseen constraint types or schemas) would help mitigate concerns about over-specialization to the authors’ verification prompts and demonstrate generalization. To do that, maybe it is not needed to run new experiments, but rather present the results of tables 2 (and 1 if possible) including only constraint types not included in the trained data. \n\n\n- **Model-based validators error rate and impact:** Rule-based validators are clear, but false positives/negatives and model-based misjudgments are not deeply quantified beyond aggregate human-agreement numbers. It would be nice to have a better quantification of how those affect the performance, or if the method is robust enough to some mislabeled examples. \n\nReferences:\n\n[1] Lambert, N. et al. Tulu 3: Pushing Frontiers in Open Language Model Post-Training. COLM 2025 (appeared in Nov, 2024). \n\n[2] Su, Yi, et al. \"Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains.\" arXiv preprint arXiv:2503.23829 (2025).\n\n[3] Wang, Y., et al. \"Reinforcement learning for reasoning in large language models with one training example.\" arXiv preprint arXiv:2504.20571 (2025).\n\n[4] Peng, H., et al. VerIF: Verification Engineering for Reinforcement Learning in Instruction Following. arXiv preprint arXiv:2506.09942 (2025).\n\n[5] Pyatkin, V., et al. \"Generalizing Verifiable Instruction Following.\" arXiv preprint arXiv:2507.02833 (2025)."}, "questions": {"value": "1) Can you clarify what exactly is the novelty of RLVC in comparison with RLVR methods? \n\n2) Can RECAST-trained models handle novel constraint types not present in RECAST-30K? Please add an experiment with held-out constraint categories (Weakness 2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Nput4ZjtN", "forum": "90tCp2KszA", "replyto": "90tCp2KszA", "signatures": ["ICLR.cc/2026/Conference/Submission12065/Reviewer_jkYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12065/Reviewer_jkYB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085659546, "cdate": 1762085659546, "tmdate": 1762923035887, "mdate": 1762923035887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We express our gratitude to all the reviewers for their valuable insights! We are happy to hear that you likedour contributions. We appreciate all of you for your comments highlighting the strengths of our work for asummary.\n\n- **Clear research motivation and problem targeting** (Reviewer `jkYB`, `FXJy`, `LBsv`)\n- **Complete, automated, and scalable framework** (Reviewer `jkYB`, `LBsv`)\n- **Strong experimental design and results** (Reviewer `jkYB`, `FXJy`, `LBsv`)\n- **Clear writing style and well-organized structure** (Reviewer `FXJy`)\n\nA primary concern shared across multiple reviewers centers on the **reliability of binary model-based validators**. Reviewer concerns focus on three levels: (1) whether noise and error rates introduced by LLM discriminators could mislead training; (2) whether binary evaluation is too coarse for subjective constraints like \"Helpfulness,\" potentially eliminating quality distinctions between different responses; and (3) whether these potential issues would fundamentally compromise RLVC training effectiveness. We address these concerns through three dimensions—empirical evidence, mechanism design, and objective trade-offs: (1) Noise levels substantially below harmful thresholds: We conducted rigorous human validation on over 2,000 (constraint, response) pairs, achieving 90.1% consistency between LLM judgments and expert annotations. [1] and [2] demonstrates that reward model-human consistency of merely around 70% suffices for stable alignment, while our 90.1% substantially exceeds this empirically validated threshold. (2) High-density constraint aggregation mechanism suppresses residual noise: RLVC reward computation is based on the average satisfaction rate across 10+ constraints per sample, where noise from individual constraint judgment errors is significantly diluted through statistical averaging, mechanistically ensuring overall reward signal stability. (3) Binary evaluation represents a reasonable design trade-off for our core challenge: Our research objective is to optimize model \"breadth\" capabilities in complex multi-constraint scenarios (simultaneously satisfying multiple constraints within a single prompt) rather than pursuing \"depth\" optimization on individual constraint dimensions (e.g., improving Helpfulness from 90 to 95 points). Since current LLMs already perform reasonably well in low-constraint scenarios, the genuine challenge lies in the exponential difficulty scaling with increasing constraint quantities. Binary judgment provides the most direct and verifiable technical pathway to achieve this goal. Additionally, for other specific concerns raised by different reviewers, we have provided targeted responses and improvement commitments in our detailed replies to each reviewer.\n\nFurthermore, we have implemented the following specific improvements in the revised version:\n\n1. Enhanced related work discussion: More comprehensive discussion of relationships with RLVR methods, citing relevant work and clearly articulating RLVC's theoretical contributions and innovations within the verifiable reward development trajectory.\n2. Strengthened experimental analysis presentation: More prominently showcase constraint type ablation results, incorporate detailed comparisons with inference-time enhancement methods like DVR, and provide additional evidence of generalization capabilities.\n3. Improved consistency and clarity of presentation: Standardize experimental result presentation formats, explicitly clarify that table values are percentages, and standardize descriptive styles in the text to avoid mixed presentation formats.\n4. Expanded method applicability exposition: Clearly articulate RLVC's direct applicability to reasoning models and discuss future technical pathways for extending to thinking process supervision.\n\nFinally, we extend special appreciation to all reviewers for their positive recognition of our work's prospects and potential impact. We firmly believe this work not only provides the community with high-quality multi-constraint instruction datasets through the RECAST data synthesis framework, but more importantly, opens new technical pathways for complex instruction-following tasks through RLVC. We commit to continuously refining this work based on reviewers' valuable suggestions to make greater contributions to enhancing large language models' complex instruction-following capabilities.\n\n[1] Chen Y, Zhu D, Sun Y, et al. The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models[J]. arXiv preprint arXiv:2410.06554, 2024.\n\n[2] Tyen G, Mansoor H, Cărbune V, et al. LLMs cannot find reasoning errors, but can correct them given the error location[C]//Findings of the Association for Computational Linguistics: ACL 2024. 2024: 13894-13908."}}, "id": "MyoN6kfEqU", "forum": "90tCp2KszA", "replyto": "90tCp2KszA", "signatures": ["ICLR.cc/2026/Conference/Submission12065/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12065/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission12065/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728907971, "cdate": 1763728907971, "tmdate": 1763728907971, "mdate": 1763728907971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}