{"id": "90tCp2KszA", "number": 12065, "cdate": 1758205484274, "mdate": 1759897536363, "content": {"title": "RECAST: Expanding the Boundaries of LLMs' Complex Instruction Following with Multi-Constraint Data", "abstract": "Large language models (LLMs) are increasingly expected to tackle complex tasks, driven by their expanding applications and users' growing proficiency in crafting sophisticated prompts. However, as the number of explicitly stated requirements increases (particularly more than $10$ constraints), LLMs often struggle to accurately follow such complex instructions, which limits their applicability in complex real-world scenarios. To the best of our knowledge, existing datasets do not exceed 10 constraints per instance. To address this challenge, we propose RECAST, an efficient and scalable framework for synthesizing datasets where each example incorporates far more constraints than those in existing benchmarks, aiming to challenge and extend the boundaries of models’ ability to follow complex instructions. These constraints are extracted from real-world prompt-response pairs to ensure practical relevance. Using this framework, we construct RECAST-$30$K, a large-scale, high-quality dataset comprising $30$k instances spanning $19$ constraint types. Experimental results demonstrate that models fine-tuned on RECAST-30K substantially improve in following complex instructions while maintaining their general capabilities without degradation. Moreover, RECAST enables automatic verification of constraint satisfaction via rule-based validators for quantitative constraints and LLM-based validators for qualitative ones, the verifiability provided by RECAST enables the design of reward functions for reinforcement learning, which further boosts model performance on complex and challenging tasks.", "tldr": "We propose an efficient method for synthesizing high-quality data to enhance the complex instruction-following capabilities of large language models (LLMs).", "keywords": ["LLM", "Complex Instruction Following", "Data synthesis", "Reinforement Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/840b82f11b2eaa2f05ea8713b079229f4b6060d0.pdf", "supplementary_material": "/attachment/a9f52a4b35d0b077538fbba0fb46719a4f185416.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RECAST, an automated framework for generating instructions comprising far more constraints than those in current benchmarks and corresponding responses. Using this framework, the authors construct RECAST-30K, which contains 30K instances with diverse constraint types. The dataset is then used for training LLMs via RLVC, an optimization method that leverages the verifiable nature of constraints to provide fine-grained reward signals. Experiments demonstrate the effectiveness of RECAST compared with other complex instruction fine-tuning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework incorporates an extended number of constraints in a single instruction, which facilitates the evaluation and improvement of LLMs' capability of following more complex instructions as the tasks for LLMs are becoming increasingly complicated.\n2. The framework is automated and scalable, providing an efficient approach of synthesizing datasets with multiple constraints in instructions.\n3. LLMs trained on data from this framework exhibit improved performance and good generalization on instruction following tasks, indicating the effectiveness of this framework."}, "weaknesses": {"value": "1. Some model-based constraints may not suitable for a binary evaluation. For example, for the \"Helpfulness\" constraint, it is common for LLMs to generate two helpful responses but with discrepant levels. If both of them are judged as satisfying the constraint, the gap between the two responses will be eliminated, which is not conducive to fine-grained model performance optimization.\n2. The effectiveness of RLVC on reasoning models are not validated. Both Qwen-2.5-7B and Llama-3.1-8B are non-reasoning base models. Considering the widespread application of the reasoning model, it is necessary to validate the effectiveness of RLVC on base models with reasoning capabilities.\n3. In Table 1, 2 and 3, it seems that the percent sign of the result values is missing. And the presentation form of results is also inconsistent across different contexts (e.g. \"achieves a 31.25% average satisfaction rate\" around line 349 and \"achieves average scores of 34.64\" around line 373)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLlOMFSME7", "forum": "90tCp2KszA", "replyto": "90tCp2KszA", "signatures": ["ICLR.cc/2026/Conference/Submission12065/Reviewer_LBsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12065/Reviewer_LBsv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838788296, "cdate": 1761838788296, "tmdate": 1762923036849, "mdate": 1762923036849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a framework to construct lagre-scale datasets to test Large Language Models's instruction-following ability when facing complex tasks. Different from previous works, the constructed dataset encompasses various types of constraints, including both subjective and objective ones, and features a larger number of constraints than existing datasets. Furthermore, this study adopts both supervised fine-tuning and reinforcement learning to enhance the Large Language Model's capability to follow instructions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This submission has the following strengths:\n- The paper demonstrates clear writing and a well-structured organization.\n- The proposed dataset RECAST-30K is large in scale and contains an adequate number of constraints.\n- Experimental results have shown that training with RECAST can effectively enhance large language model's ability in instruction following."}, "weaknesses": {"value": "This submission has the following weaknesses:\n- For model-based constraints, the quality depends on used large language models.\n- The count of Rule-based constraints are much less than model-based constraints."}, "questions": {"value": "I have the following questions / suggestions:\n- Would it be possible to extend the constraints numbers of rule-based constraints?\n- Why HSR metric is not used in Table 1?\n- Why the performance of RECAST-30K-RLVC on Qwen-2.5-7B become worse than RECAST-30K-SFT from time to time? (Bottom of Table 1)\n- It would be better to also test some instruction-following enhancement works. For example,\n    - Branch-Solve-Merge Improves Large Language Model Evaluation and Generation. In NAACL. 2024.\n    - Divide-Verify-Refine: Can LLMs Self-align with Complex Instructions?. In Findings of ACL. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fhcbMew7nw", "forum": "90tCp2KszA", "replyto": "90tCp2KszA", "signatures": ["ICLR.cc/2026/Conference/Submission12065/Reviewer_FXJy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12065/Reviewer_FXJy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940603247, "cdate": 1761940603247, "tmdate": 1762923036293, "mdate": 1762923036293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RECAST, a data-synthesis framework that mines verifiable constraints from existing instruction-response pairs and rewrites the original instructions to include many more constraints. The authors use this pipeline to build a training dataset RECAST-30K (~ 30k examples; 19 constraint types), with both rule-based and model-based validators (LLM-as-judge) attached to each constraint. They also propose RLVC (Reinforcement Learning with Verifiable Constraints) which turns per-constraint satisfaction into a fine-grained reward and optimizes policies via GRPO. They also introduce a new benchmark, RECAST-Test, with 4 difficult levels. The experiments show sizable gains in hard satisfaction rate on RECAST-Test benchmark, as well as improvements on IFEVAL and FollowBench, while largely preserving general knowledge performance (evaluated with GPQA and MUSR). Human evaluations support the quality of constraint filtering and selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear motivation and problem:** The paper targets a real gap, LLMs’ performance drops as the number of explicit constraints increases, and existing SFT datasets usually don’t cover a high number of constraints. \n\n- **The pipeline is complete and reproducible:** The paper details each stage (constraint extraction, instruction enhancement, response synthesis, validation), gives prompts/templates, and reports human agreement metrics. RLVC is specified with GRPO objective and training setup. This level of detail supports reimplementation. The authors promised to release code, data, and trained models upon acceptance. \n\n- **Evaluation setup is strong:** The primary metric, HSR, is well defined (satisfies all constraints simultaneously) with subtype metrics RSR/MSR for rule/model-based subsets. The proposed benchmark RECAST-Test (cover 4 difficulty levels) and it is associated with two external sets (IFEVAL, FollowBench) for the task. Also, the work included a general ability evaluation with GPQA/MUSR, which is important.\n\n- **Empirical gains & informative analyses:** RECAST-30K improves complex instruction-following against multiple baselines, with RLVC further improving especially at higher difficulty levels. The paper includes informative ablations (constraint type only, quantity caps, component removals) and training dynamics for RLVC that illustrate different learning curves for rule- vs. model-based constraints. It also contains Human evaluation to back up the automated choices."}, "weaknesses": {"value": "- **Lack of theoretical positioning against RLVR methods:** The paper does not situate its RL formulation within the growing line of Reinforcement Learning from Verifiable Rewards (RLVR) research, despite clear conceptual overlap. Despite the original paper of RLVR being cited [1], it is cited only in the context of the dataset. Other foundational RLVR works [2,3] and subsequent works applying the verifiable reward mechanisms to multi-constraint or format-constrained instruction-following [4,5] are not cited or compared against. As a result, the contribution of RLVC relative to existing verifiable-reward frameworks remains under-theorized: it is unclear whether the proposed per-constraint reward structure offers advantages over scalar verifiable rewards, or whether similar behavior would emerge from established RLVR baselines. Strengthening this connection and including appropriate baselines would significantly clarify the novelty and necessity of RLVC.\n\n\\* It is acceptable to not cite works that appeared less than 2 months from the submission deadline, but in that case still recommendable for the final version. \n\n\n\n- **Generalization:** Parts of the training data and evaluation share the same constraint types and verification machinery. Although external benchmarks are included, they also contain very similar constraint types. I propose the authors to present stronger isolation tests (e.g., unseen constraint types or schemas) would help mitigate concerns about over-specialization to the authors’ verification prompts and demonstrate generalization. To do that, maybe it is not needed to run new experiments, but rather present the results of tables 2 (and 1 if possible) including only constraint types not included in the trained data. \n\n\n- **Model-based validators error rate and impact:** Rule-based validators are clear, but false positives/negatives and model-based misjudgments are not deeply quantified beyond aggregate human-agreement numbers. It would be nice to have a better quantification of how those affect the performance, or if the method is robust enough to some mislabeled examples. \n\nReferences:\n\n[1] Lambert, N. et al. Tulu 3: Pushing Frontiers in Open Language Model Post-Training. COLM 2025 (appeared in Nov, 2024). \n\n[2] Su, Yi, et al. \"Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains.\" arXiv preprint arXiv:2503.23829 (2025).\n\n[3] Wang, Y., et al. \"Reinforcement learning for reasoning in large language models with one training example.\" arXiv preprint arXiv:2504.20571 (2025).\n\n[4] Peng, H., et al. VerIF: Verification Engineering for Reinforcement Learning in Instruction Following. arXiv preprint arXiv:2506.09942 (2025).\n\n[5] Pyatkin, V., et al. \"Generalizing Verifiable Instruction Following.\" arXiv preprint arXiv:2507.02833 (2025)."}, "questions": {"value": "1) Can you clarify what exactly is the novelty of RLVC in comparison with RLVR methods? \n\n2) Can RECAST-trained models handle novel constraint types not present in RECAST-30K? Please add an experiment with held-out constraint categories (Weakness 2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Nput4ZjtN", "forum": "90tCp2KszA", "replyto": "90tCp2KszA", "signatures": ["ICLR.cc/2026/Conference/Submission12065/Reviewer_jkYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12065/Reviewer_jkYB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085659546, "cdate": 1762085659546, "tmdate": 1762923035887, "mdate": 1762923035887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}