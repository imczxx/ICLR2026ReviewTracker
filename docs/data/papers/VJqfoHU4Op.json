{"id": "VJqfoHU4Op", "number": 1130, "cdate": 1756846515991, "mdate": 1763122335359, "content": {"title": "XDex: Learning Cross-Embodiment Dexterous Grasping with 1000 Hands", "abstract": "Synthesizing dexterous grasps across various hands remains a fundamental challenge in robotic manipulation due to morphology gaps in geometry, topology, and kinematics. We hypothesize that scaling the diversity and number of hand embodiments improves generalization to unseen hands. To this end, we introduce XDex, a framework trained on the largest cross embodiment grasping dataset, which we built using 1,000 diverse hands. XDex features an embodiment transformer that jointly encodes hand geometry and topology to learn from this large scale dataset. Additionally, we enforce grasp consistency across embodiments by training on a paired grasping dataset and introducing a retargeting loss. The paired data are generated by first synthesizing grasps for a source hand and then translating them to diverse target hands. XDex significantly outperforms prior methods in grasp quality, consistency, and diversity, and demonstrates strong generalization to unseen hands in real world settings.", "tldr": "", "keywords": ["Dexterous Grasping", "Cross-embodiment"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9a3274f12be905d0f5eee4b195af895a05dd272a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes XDex, a large-scale cross-embodiment grasping dataset. The dataset is constructed by first generating grasps for a source human hand and then adapting them to multiple robotic hands and their variations. By training on this dataset, a model can learn to generate grasps given object and hand features, enabling generalization to diverse hand structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.  The paper proposes learning cross-embodiment grasp generation for various robotic hands from scalable synthetic data. The core idea of generating hand variations and retargeting from a human hand is novel and interesting.\n2.  The analysis of grasp consistency across different hands is an interesting point. I would encourage a more detailed analysis of this, such as probing the similarities between the hand features of corresponding links.\n3.  The evaluations are detailed, revealing promising quality in the generated grasping poses, and the real-world deployment results are appreciated. The results provide insights into achieving scaling laws for cross-embodiment robotic grasping.\n4.  The paper is well-written and easy to read, and the visualizations are informative."}, "weaknesses": {"value": "1.  More details on the generation of embodiments (e.g., shape parameter distributions, mechanisms to merge or add links) should be provided. According to Fig. 5, it seems that the link and joint numbers are sparsely distributed, which may limit data diversity.\n2.  The grasp execution evaluation could be improved. Since the grasp generation method ensures Force Closure, it is more common to evaluate the grasp by applying gravity from multiple directions, rather than just a single one."}, "questions": {"value": "1.  How could this pipeline generalize to non-humanoid hands, such as the D'Claw or parallel grippers, which have significant morphological variations and lack a clear unified source like the human hand?\n2.  While Fig. 5 shows the distribution of multiple metrics, I am curious about the distribution of the general hand morphologies. A t-SNE visualization of the hand geometry features or a similar technique would be insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D8MFsKbPGe", "forum": "VJqfoHU4Op", "replyto": "VJqfoHU4Op", "signatures": ["ICLR.cc/2026/Conference/Submission1130/Reviewer_xWAV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1130/Reviewer_xWAV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457826129, "cdate": 1761457826129, "tmdate": 1762915687114, "mdate": 1762915687114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "zry4PHEE02", "forum": "VJqfoHU4Op", "replyto": "VJqfoHU4Op", "signatures": ["ICLR.cc/2026/Conference/Submission1130/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1130/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763122241866, "cdate": 1763122241866, "tmdate": 1763122241866, "mdate": 1763122241866, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose XDex, which addresses the challenge of generalizing dexterous grasping across diverse robotic hands. The authors hypothesize that scaling the diversity of hand embodiments is key to generalization. XDex is trained on a new, large-scale dataset of 1,000 procedurally generated hand embodiments with paired grasping poses. It employs a transformer-based CVAE architecture with an embodiment encoder that captures both hand geometry and topology, along with an explicit retargeting loss. Experiments show that XDex significantly outperforms prior methods on both seen and unseen hands in grasp quality and consistency in a zero-shot manner."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors' central premise of scaling embodiment diversity to improve generalization is compelling. The creation of a dataset with 1,000 diverse hand embodiments is a major contribution in itself. The strong performance on unseen hands is proof of generalization.\n- The concept of a paired grasping dataset combined with an explicit retargeting loss is an effective way to enforce grasp consistency. The embodiment transformer jointly encoding per-link geometry (via PointNet) and kinematic topology (via attention bias from shortest path distance) is a novel way to represent diverse robot morphologies.\n- The experimental validation is extensive and uses a well-defined set of metrics. The successful zero-shot sim-to-real deployment further demonstrates robustness."}, "weaknesses": {"value": "While the paper presents an interesting and novel idea, the paper is far from reaching publication quality in its current form:\n- The most significant weakness of this paper is the complete absence of an appendix. The 9-page limit is insufficient to cover the necessary details. This major flaw makes it impossible to fully assess the work and prevents other researchers from building upon it, especially in a venue like ICLR, which embraces reproducibility. Specifically, the final pose generation step relies on solving an inverse kinematics problem (Equation (6)), but all practical details are omitted. The paper does not specify the solver used, the initialization strategy, how joint limits are handled, or the procedure for dealing with non-convergence. Moreover, the procedural generation of the 1,000-hand dataset is a core contribution, yet its description lacks detailed information. The paper does not state the constraints or heuristics used during \"Topology Variations\" to ensure the generated kinematic chains are physically plausible. There is no justification for the choice of the five base robot hands, nor are there qualitative examples of the more unusual or challenging morphologies generated, which would help in understanding the dataset's true diversity and potential biases. Details about the \"Retargeting\" baseline are also underspecified. The paper does not state which specific single-hand grasping policy was trained or which retargeting algorithm was used for evaluation.\n- The submission includes neither an Ethics Statement nor a Reproducibility Statement. The authors also fail to include a discussion of hyperparameter choices, computation usages, detailed dataset statistics, and efficiency reports, which raises further concerns about reproducibility.\n- The paper does not discuss several highly relevant and recent works on dexterous grasp transfer/retargeting, such as RobotFingerPrint (https://irvlutd.github.io/RobotFingerPrint/), FunGrasp (https://hly-123.github.io/FunGrasp/), Dexonomy (https://pku-epic.github.io/Dexonomy/), and Grasp2Grasp (https://grasp2grasp.github.io/). While some may be concurrent under the ICLR policy, acknowledging and briefly differentiating from them would significantly strengthen the paper's contribution claims.\n- Minor: The manuscript completely misuses `\\citep` vs. `\\citet`."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "khBMAOjNWn", "forum": "VJqfoHU4Op", "replyto": "VJqfoHU4Op", "signatures": ["ICLR.cc/2026/Conference/Submission1130/Reviewer_9bqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1130/Reviewer_9bqW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632580713, "cdate": 1761632580713, "tmdate": 1762915687006, "mdate": 1762915687006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a cross embodiment grasp dataset, and grasp synthesis pipeline explicitly designed to leverage embodiment information. Because embodiment information is explicitly encoded, the proposed model can effectively transfer synthesized grasps between hand morphologies. The authors make the following claims: (1) scaling the diversity and number of embodiments improves generalization to unseen hands, (2) incorporating embodiment information improves quality, consistency, and diversity. The former is not shown, but the latter is."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality.** The work appears original.\n \n**Quality.** The work appears to be of high quality. The work is well motivated, the design decisions are consistent with the motivation and hypotheses, and (for the most part) the experiments validate the claims.\n \n**Clarity.** The paper is well written and well organized.\n \n**Significance.** The experiments demonstrate that using a large dataset improves performance. While this result is not particularly surprising, the introduction of such a large dataset carries significance. The experiments also demonstrate that the proposed architecture (designed to explicitly encode embodiment) leads to improved grasp synthesis. This also carries significance especially since the authors have shown this architecture can be used to enlarge the dataset."}, "weaknesses": {"value": "My main critique of the paper is that it doesn’t seem that the claim: increasing the number of embodiments improves generalization to unseen hands, has been shown. In Section 4.3, the authors show: scaling embodiment with a fixed data size, and joint scaling of data size with embodiment. Adding the experiment: scaling data size with fixed number of embodiments would resolve my concern."}, "questions": {"value": "**Questions**\n \n- Line 251: the local point features from SPConv are only used to condition a subset of local grasping poses \n- In eq 4 the decoder is conditioned on f^{obj}_p which is encoded in z. Is this necessary? \n- In eq 5 and 6 is the distance between poses the Euclidean distance? This metric doesn’t respect the geometry of the rotation group. Do you have a sense of the implications of this? \n- “We also observe that the batch size of D(R,O)* is limited to 4 due to the large memory requirement of the dense distance matrix” How does your proposed method compare? \n- What happens when the data size grows and the embodiments don't?\n \n**Possible typos**\n\n- Line 238: 6D Joint → 6D joint \n- Line 256: information. → information \n- Line 255: Meanwhile, the encoder learned attention bias in the self-attention mechanism to explicitly encode → The encoder learned attention bias in the self-attention mechanism is used to explicitly encode \n- Line 396: Our method achieves comparable diversity compared to baselines → Our method achieves diversity comparable to baselines \n- Line 476:  co nsistent → consistent \n- Line 332: Ours with consistency → Ours w/o consistency"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fOFAnGpfVh", "forum": "VJqfoHU4Op", "replyto": "VJqfoHU4Op", "signatures": ["ICLR.cc/2026/Conference/Submission1130/Reviewer_8yDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1130/Reviewer_8yDh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001184828, "cdate": 1762001184828, "tmdate": 1762915686905, "mdate": 1762915686905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes XDex, a framework for generating dexterous grasps that can generalize across a large set of 1,000 different robot hands. The method trains a transformer-based conditional VAE on a large-scale paired dataset, where grasps are retargeted across embodiments to enforce consistency, enabling generalization to unseen hands."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a large-scale paired grasping dataset covering 1,000 hand embodiments.\n2. The model architecture encodes hand topology using an attention bias, which is an interesting way to handle varied kinematics.\n3. The experimental analysis that separates the effects of data size and embodiment count is a good analysis."}, "weaknesses": {"value": "1. The experimental setup for \"unseen hands\" does not convincingly support the claim of generalization, as the test hands are the base models used to procedurally generate the entire training set.\n2. The paper lacks a qualitative analysis of failure modes or a discussion of limitations, which would be crucial for understanding the boundaries of the method's capabilities.\n3. The effect of the consistency loss is vague. The Table. 2 shows that \"Ours with consistency loss\" is comparable to \"Ours\". Based on this result, I believe authors should discuss more on this loss and why they add it."}, "questions": {"value": "1. Regarding the experiment in Sec 4.1, the unseen test hands (Inspire, Ability, etc.) are the original models from which the 800 training hands are procedurally generated. Could you clarify how this setup truly tests generalization to out-of-distribution hand morphologies, rather than just interpolation within a known distribution?\n2. What are the primary failure modes of the model?\n3. The consistency loss ablation in Table 2 shows limited change in metrics. Can you elaborate more on how this loss influence the results?\n4. The data generation process is described as \"computationally expensive.\" Could you provide a quantitative figure for the time required to generate the full paired dataset? This information is important for assessing the practicality and reproducibility of the data pipeline.\n5. Could you provide more detail on the dataset, such as the number of unique object models in the test split?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1gUtlOyV76", "forum": "VJqfoHU4Op", "replyto": "VJqfoHU4Op", "signatures": ["ICLR.cc/2026/Conference/Submission1130/Reviewer_2jri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1130/Reviewer_2jri"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067552590, "cdate": 1762067552590, "tmdate": 1762915686748, "mdate": 1762915686748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}