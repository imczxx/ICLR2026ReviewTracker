{"id": "ZkoojtEm3W", "number": 10407, "cdate": 1758170530543, "mdate": 1759897652903, "content": {"title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis", "abstract": "Deep learning-based respiratory auscultation provides critical acoustic cues for pulmonary assessment. However, conventional methods often compress signals into mel-spectrograms, discarding fine temporal dynamics, transient events, and semantic links to electronic health records (EHRs). The scarcity of high-quality multimodal datasets further constrains progress. To address these gaps, we present **_Resp-Agent_**, a multimodal agent system that tackles two key challenges: high-fidelity respiratory sound generation and accurate disease diagnosis, which can systematically identify and remedy model weaknesses in a closed loop. Specifically, we introduce a cascaded framework that transforms a text-only large language model (LLM) into a multimodal generator via modality injection. Conditioned on diagnostic text and style embeddings derived from real audio, the generator autoregressively produces discrete acoustic units (BEATs tokens), which are reconstructed into waveforms with a conditional flow-matching decoder. For classification, we design a Longformer-based model that fuses EHR text and respiratory features at the input level. \nThe strategic global attention is proposed to enable the model to capture cross-modal long-range dependencies while detecting brief, low-energy events such as coughs and wheezes, yielding fine-grained disease predictions. To support this work, we construct **_Resp-229k_**, a multimodal corpus of 408 hours, 229k recordings, and 20 disease classes, each paired with expert-level natural language annotations distilled from structured EHRs. Extensive experiments show that our method consistently surpasses prior approaches, advancing robust and deployable respiratory intelligence. Data and code will be released upon acceptance.", "tldr": "We propose Resp-Agent, the first agent-based system for respiratory intelligence, integrating controllable sound synthesis and multimodal diagnosis to achieve state-of-the-art performance on ICBHI and our new large-scale Resp-229k benchmark.", "keywords": ["Respiratory sounds", "multimodal learning", "LLM-based generation", "medical agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0587bc45b39d18b9dcc17d8db911748e1c13e21.pdf", "supplementary_material": "/attachment/7e5070e3c3f63afea0786c923bdb63f0410895a6.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a multimodal method for high-fidelity respiratory sound generation and disease diagnosis, converting a text-only LLM into an audio generator via modality injection and fusing text with acoustic features using a Longformer with strategic global attention."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s originality lies in the closed-loop Diagnoser -> Thinker -> Generator setup and anchor-based multimodal Diagnoser, but these build on familiar ideas (active learning, synthetic augmentation, attention)."}, "weaknesses": {"value": "The paper's central claim that the Thinker module intelligently guides data generation lacks evidence, as no controlled comparison against simple baselines like class-prior rebalancing, uncertainty sampling, or random selection is provided under identical experimental settings. Without isolating the planner's contribution from the sheer effect of adding synthetic data, it remains unclear whether the observed gains stem from sophisticated scheduling or merely from data volume. \n\nFurthermore, the work does not decompose the sources of improvement, whether gains arise from balancing underrepresented labels, targeting hard recording styles, combining rare labels with difficult domains, or simply scaling the dataset. It also does not provide ablation curves showing performance versus synthetic budget to identify when returns saturate.\n\nThe evaluation framework also presents gaps that weaken the claims of robustness and controllability. The paper omits comparisons with established imbalance mitigation techniques such as focal loss, which could achieve similar macro-F1 improvements without generative models, rendering the necessity of the proposed pipeline uncertain. Cross-domain evaluation on only two held-out sources is insufficient; a leave-one-source-out protocol would rigorously test whether robustness generalizes across all recording conditions or benefits from a favorable data split. \n\nFinally, the Generator's ability to independently control pathology content and recording style is assumed but not validated without explicit disentanglement tests (e.g., label-swap experiments), the model may conflate correlated attributes from training data, undermining the loop's premise that rare labels and target styles can be independently specified."}, "questions": {"value": "Can you provide a controlled ablation comparing the Thinker-based planner against simple baseline schedulers (class-prior rebalancing, uncertainty sampling, random selection) while holding all other factors constant same Generator, same Diagnoser, same synthesis budget, same training procedure to check and demonstrate whether the intelligent planning policy itself drives improvement?\n\nCould you present performance curves showing macro-F1 as a function of synthetic data budget to reveal whether most benefits come from a small number of targeted clips or require large volumes of generated audio?\n\nCould you demonstrate the Generator's controllability through disentanglement tests specifically style-swap experiments (fixed pathology, varied recording style) and label-swap experiments (fixed style, varied pathology) to confirm that rare pathology-style combinations can be reliably synthesized without the model conflating correlated attributes?\n\nWhy not compare against actual established baselines in the areas of respiratory monitoring?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Nes1K6xSA", "forum": "ZkoojtEm3W", "replyto": "ZkoojtEm3W", "signatures": ["ICLR.cc/2026/Conference/Submission10407/Reviewer_CKZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10407/Reviewer_CKZt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762244073, "cdate": 1761762244073, "tmdate": 1762921719501, "mdate": 1762921719501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper present Resp-agent, an agent-based system for respiratory sound generation and disease diagnosis. The whole system includes a cascaded pipeline leveraging LLMs for content semantics learning and followed by the generator and flow-matching based reconstruction to capture the sound-features. Since there's no existing paired EHR - audio datasets, they create a multimodal benchmark names Resp-229k. Finally, the paper evaluated on disease diagnosis tasks and reports improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets important question that lacking paired multi-modal datasets especially when the text-modality missing.\n2. The system design is comprehensive and clear to follow its flow."}, "weaknesses": {"value": "1. Despite the comprehensive and close-loop system. The novelty lies in the assembly of recent advances such as longformer, flow-matching models. The core contribution and innovation is hidden and ambiguous. \n2. For the generator part,  the Generator does make the system closed-loop: the “Diagnoser” classifies real respiratory sounds, then the “Generator” synthesizes new examples. It's not clear is there of a real feedback loop: the Diagnoser doesn’t meaningfully inform or retrain the Generator; Besides, there's no evaluation of generation quality. \n3. Lack of discussion of recent work about respiratory foundation models or other llm-based respiratory model. For example, papers such as Resp-LLM or other LLM-prompted multimodal models already explored: Using LLMs to describe audio (e.g., “This sound has wheezes and crackles”), althought not in the EHR format. Why the EHR-simulation text is chosen or better compared with other prompt style?"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gr3ovNfr00", "forum": "ZkoojtEm3W", "replyto": "ZkoojtEm3W", "signatures": ["ICLR.cc/2026/Conference/Submission10407/Reviewer_MVWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10407/Reviewer_MVWu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845141356, "cdate": 1761845141356, "tmdate": 1762921719095, "mdate": 1762921719095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a clinically meaningful problem with a coherent, end-to-end system and introduces a cross-domain benchmark plus two technically interesting modules (modality-injected unit generator with CFM; Longformer with anchor-based global attention). Futher, the proposed system demonstrates credible ICBHI gains and compelling rebalancing benefits on Resp-229k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper strengths can be summarised as follows,\n\n- Ambitious, unified scope (which includes dataset, generation, diagnosis, and agent loop). The paper introduces a cross-domain multimodal corpus (Resp-229k) with source-disjoint splits. This anchors the contribution in real distribution shift rather than in-domain evaluation.\n- Clear architectural ideas on both sides of the loop. The Generator upgrades a compact LLM via modality injection (BEATs-derived style tokens) to autoregress discrete acoustic units, then reconstructs waveforms with a DiT-style conditional flow-matching decoder and vocoder. This results in a clean disentanglement of content (diagnosis) vs style (timbre/device).\n- Generation for targeted rebalancing appears useful. Using diagnosis-conditioned synthesis to balance classes lifts macro-F1 sharply.\n- The paper reports objective similarity (e.g., FAD, style-cosine) and comparisons to c-WaveGAN / AudioLDM-2 under matched budgets, supporting the claim that content-aware augmentation helps more than generic perturbations."}, "weaknesses": {"value": "The paper weaknesses can be summarised as follows,\n\n- Low macro-F1 in the natural (imbalanced) setting. Before synthetic balancing, macro-F1 is 0.2118 despite high accuracy... this tell me that there might be substantial minority under-diagnosis. However, the paper relies on its own generator to fix this; stronger baselines (e.g., cost-sensitive losses, reweighting, focal/LDAM, mixup/Manifold mixup, class-balanced sampling) should be compared under the same cross-domain split to show generation is superior beyond conventional imbalance remedies.\n- Potential evaluation confounds on generation. The CFM decoder is conditioned not only on discrete units and global timbre but also \"a short reference prefix of the ground-truth mel during training/validation\" to encourage continuation.\n- Dataset governance and text synthesis risks. Clinical \"summaries\" are LLM-generated from heterogeneous metadata; while the model doesn’t interpret audio, it can still standardize or hallucinate metadata beyond the original fields. The paper should provide audits (error rates, inter-annotator checks) and licenses/provenance per clip, especially with mixed microphone/stethoscope sources used for high-stakes labels."}, "questions": {"value": "The questions for the authors are as follows,\n\n- Class taxonomy: Are Bronchiectasia vs Bronchiectasis and Acute URI vs URTI duplicates? If so, how are they merged across sources and guaranteed disjoint across splits? Please provide the final class map and counts per class per split. \n- Imbalance baselines: Can you compare against strong non-generative remedies under the same cross-domain protocol (class-balanced sampling, effective number reweighting, LDAM-DRW, focal loss in fine-tuning, mixup/Manifold mixup), with CIs?\n- Text summary quality & governance: What safeguards ensure LLM-rendered EHR-style summaries do not hallucinate fields or introduce systematic biases across sources/devices? Provide sampling audits, error types, and licenses/provenance for all sources.\n- Where does the gain come from? Please isolate the modality weaving vs late fusion, anchors vs no anchors, and text quality (raw metadata vs LLM-rendered text) in a single ablation grid on the cross-domain split.\n- Cross-dataset generalization: In addition to source-disjoint evaluation, can you train on subset A and test on B (e.g., train on ICBHI and SPRSound, test on UK-COVID; or leave-one-source-out) to confirm robustness to label/device/site shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "47mo3PSVq4", "forum": "ZkoojtEm3W", "replyto": "ZkoojtEm3W", "signatures": ["ICLR.cc/2026/Conference/Submission10407/Reviewer_WjjZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10407/Reviewer_WjjZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956351434, "cdate": 1761956351434, "tmdate": 1762921718649, "mdate": 1762921718649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Resp-Agent, a multimodal agent system tackling the challenges of deep learning-based respiratory assessment by developing both a generator and a classifier. To address the scarcity of high-quality data and the omission of fine acoustic dynamics, the system first transforms a text-only LLM into a multimodal generator via modality injection, producing BEATs tokens conditioned on diagnostic text and style embeddings, which are then reconstructed into waveforms. For diagnosis, it employs a Longformer-based model with strategic global attention to fuse EHR text and acoustic features at the input level, allowing it to capture long-range cross-modal dependencies while accurately detecting brief acoustic events like coughs. This work is supported by the creation of Resp-229k, a massive 408-hour multimodal corpus that pairs audio with expert-level EHR annotations, enabling the system to achieve superior performance in robust respiratory disease prediction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The most practical and immediate strength is the size and richness of the new corpus. At 408 hours and 229,000 recordings, this scale is required for a specialized medical domain. Crucially, by linking the acoustic data to expert-level annotations distilled from Electronic Health Records (EHRs), this provides the necessary clinical context, which is often missing in public datasets, thus bridging the gap between raw audio and real-world diagnostic complexity. \n\nThe system is capable of generating high-fidelity audio (using LLM-based modality injection to create BEATs tokens). By generating discrete acoustic units (BEATs tokens) and using a specialized flow-matching decoder, it retains the high-fidelity transient events critical for accurate diagnosis. \n\nThe diagnostic model integrates two disparate data types, which are EHR text and respiratory features, at the input level. The introduction of strategic global attention is a good contribution, as it specifically addresses the difficulty of simultaneously capturing broad clinical context (long-range dependencies) and tiny, clinically vital acoustic cues (brief, low-energy events like wheezes or rales).\n\nThe paper is reasonably well written and focuses on solving an important issue of accurate diagnosis in the respiratory health domain.  \n\nThe datasets and evaluation seem adequate. The performance improvements over compared baselines are good."}, "weaknesses": {"value": "The papers fall into the category of applying ML for healthcare. I find it interesting as it combines multiple concepts, but incremental. \n\nThe evaluation part is weak. The choice of LSTM for the text baseline is weak, and using an older attention mechanism for fusion is also suboptimal.  I would suggest comparing against a Transformer-based text encoder (e.g., BERT, RoBERTa, or even a small version of the Longformer) for the text-only task. This isolates whether the performance gain is from the fusion or just using better text features. \n\nTo demonstrate the novelty and power of the proposed fusion, the authors should have included multimodal baselines that employ simpler fusion strategies. For example, use the Conformer to extract audio embeddings and the LSTM to extract text embeddings. Concatenate these two vectors and feed the result into a simple Multi-Layer Perceptron (MLP) classifier. The other way can be to do averaging or weighted voting for the final prediction. \n\nThe paper compares audio generative quality with Audio LDM2 and c-WaveGAN. I find StableAudio or StableAudio Open missing from the paper. It will make sense to compare Resp-Agent generative quality against a fine-tuned StableAudio."}, "questions": {"value": "What will happen if you change Conformer to Whisper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W07qLzy5tG", "forum": "ZkoojtEm3W", "replyto": "ZkoojtEm3W", "signatures": ["ICLR.cc/2026/Conference/Submission10407/Reviewer_u8tM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10407/Reviewer_u8tM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762171261996, "cdate": 1762171261996, "tmdate": 1762921718212, "mdate": 1762921718212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}