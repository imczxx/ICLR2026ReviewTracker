{"id": "akAwANLTmf", "number": 22743, "cdate": 1758334907169, "mdate": 1759896849364, "content": {"title": "Struc-EMB: The Potential of Structure-Aware Encoding in Language Embeddings", "abstract": "Text embeddings from Large Language Models (LLMs) have become foundational for numerous applications. However, these models typically operate on raw text, overlooking the rich structural information, such as hyperlinks or citations, that provides crucial context in many real-world datasets. This paper introduces and systematically evaluates a new paradigm for generating structure-aware text embeddings by integrating these structural relations directly into the LLM's internal encoding process, rather than relying on traditional post-hoc aggregation. We investigate two primary in-process methods: sequential concatenation and parallel caching. Through extensive zero-shot experiments across retrieval, clustering, classification, and recommendation tasks, we demonstrate that our structure-aware approaches consistently outperform both text-only and post-hoc baselines. Our analysis reveals critical trade-offs: sequential concatenation excels with noisy, moderate-length contexts, while parallel caching scales more effectively to long, high-signal contexts but is more susceptible to distractors. To address the challenge of noisy structural data, we also introduce and validate two effective techniques: Context Distillation and Semantic Balancing. This work provides the first comprehensive analysis of in-process structure-aware encoding, offering a blueprint for building more powerful and contextually aware embedding models.", "tldr": "", "keywords": ["Text Embedding", "Large Language Model", "Structure-Aware Encoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d6084e80c8ebabcf531d36689a2a486f3891eef.pdf", "supplementary_material": "/attachment/9d851a7bb469037ac105c2b7fe3dc8da592e731a.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies “structure-aware” text embeddings by injecting graph/relational context (e.g., linked neighbors, co-occurrence, citations) during encoding rather than aggregating representations post-hoc. Two inference-time, training-free variants are proposed: a sequential concatenation strategy (Seq) that encodes the target text with retrieved neighbors as a single sequence, and a parallel key–value strategy (Par) that supplies neighbor representations as external attention sources without serial concatenation. Extensive zero-shot evaluations across retrieval, clustering, classification, and recommendation show that structure-aware encoding generally outperforms both plain “individual” encodings and after-the-fact pooling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear articulation of “encoding-time structure injection” vs post-hoc aggregation, with two complementary designs (Seq vs Par) and principled discussion of trade-offs (robustness to noise, long-context behavior, and compute).\n- Evaluations span multiple tasks/datasets and report consistent gains for structure-aware embeddings relative to individual encodings and standard aggregation baselines.\n- Methods are training-free at inference time and thus easy to bolt onto existing embedders; simple techniques (context distillation, semantic balancing) are effective in practice."}, "weaknesses": {"value": "- Positioning vs encode-time RAG/PRF is under-specified (mechanism-level overlap).\n1. The Seq pathway is closely related to retrieval-augmented fusion that injects evidence during encoding/decoding, e.g., Fusion-in-Decoder (FiD) [1], REALM [2], RETRO [3], and black-box prepend methods such as REPLUG [4]. PRF-style contextual expansion with BERT encoders (BERT-QE) [5] also fuses feedback text at encoding time.\n2. The Par pathway resembles dense-retrieval PRF that augments the query with additional feedback vectors/keys in parallel, e.g., ANCE-PRF [6], subsequent reproductions/enhancements [7], multi-representation PRF [8], and ColBERT-PRF/FairPRF [9].\n\nAlthough this paper targets general-purpose embeddings rather than generation/reranking, the encode-time fusion mechanisms are substantially similar. A more systematic, controlled comparison is needed (same backbone, retrieval pool, neighbor budget, and fusion budget) to sharpen novelty claims.\n\n- If any interpolation/weighting hyperparameters are tuned on evaluation splits, that inflates reported effectiveness vs deployment-realistic settings; stricter selection (validation only) or self-tuning heuristics would strengthen claims.\n\n[1] Izacard & Grave. Leveraging Passage Retrieval with Generative Models (Fusion-in-Decoder). 2021.\n\n[2] Guu et al. REALM: Retrieval-Augmented Language Model Pre-Training. ICML 2020.\n\n[3] Borgeaud et al. RETRO: Improving Language Models by Retrieving from Trillions of Tokens. PMLR 2022.\n\n[4] Shi et al. REPLUG: Retrieval-Augmented Black-Box Language Models. NAACL 2024.\n\n[5] Zheng et al. BERT-QE: Contextualized Query Expansion for Document Re-Ranking. Findings of EMNLP 2020.\n\n[6] Yu, Xiong, Callan. ANCE-PRF: Improving Query Representations for Dense Retrieval with Pseudo-Relevance Feedback. CIKM 2021.\n\n[7] Li et al. Improving Query Representations for Dense Retrieval with PRF: A Reproducibility Study. ECIR 2022.\n\n[8] Wang et al. Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval. arXiv 2021.\n\n[9] Jaenich et al. ColBERT-FairPRF: Towards Fair PRF in Multiple-Representation Dense Retrieval. ECIR 2023."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4zt6KkqGG4", "forum": "akAwANLTmf", "replyto": "akAwANLTmf", "signatures": ["ICLR.cc/2026/Conference/Submission22743/Reviewer_CJha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22743/Reviewer_CJha"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747799295, "cdate": 1761747799295, "tmdate": 1762942366950, "mdate": 1762942366950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is motivated by the question of how to integrate structural information with the internal knowledge of LLM encoders to improve text embedding quality. It explores structure-aware text embedding by integrating structural information directly into the LLM’s internal encoding process. Through experiments, the paper shows the advantages of the two methods, Struc-Emb-Seq and Struc-Emb-Par. The paper is well structured and easy to follow. However, the lack of theoretical support and comprehensive experiments, such as fine-tuning and few-shot settings, limits its soundness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes structure-aware text embedding by integrating structural information directly into the LLM’s internal encoding process.\n2. Through experiments, the paper shows the advantages of the two proposed methods, Struc-Emb-Seq and Struc-Emb-Par.\n3. The paper is well structured and easy to follow."}, "weaknesses": {"value": "1. The experimental results do not show a large advantage for these two methods; individual embeddings also show the best performance among the experiments.\n2. This paper lacks an analysis of computational comparisons.\n3. The paper focuses on zero-shot experimental results, lacking experiments such as fine-tuning and few-shot, which limits its soundness.\n4. The captions of figures and tables lack clear notation and summarization of the results."}, "questions": {"value": "1. Can this framework be evaluated with more extensive experiments, such as fine-tuning and few-shot learning, to strengthen its soundness?\n2. What is the computational difference between LLMs and structure-aware models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hTnrcQonSc", "forum": "akAwANLTmf", "replyto": "akAwANLTmf", "signatures": ["ICLR.cc/2026/Conference/Submission22743/Reviewer_xRbZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22743/Reviewer_xRbZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972687916, "cdate": 1761972687916, "tmdate": 1762942366599, "mdate": 1762942366599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present an approach to integrate structural information (such as links or citations) directly into the encoding process of pre-trained language models.\nThey propose two versions: Struc-Emb-Seq, which concatenates the target text with related texts   and Struc-Emb-Par, which uses parallel caches to incorporate the context more efficiently.\nThey also introduce two improvement techniques, Context Distillation and Semantic Balancing, aimed at reducing noise and preserving the meaning of the main text.\nExperiments on several datasets show moderate improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The idea that structured texts contain useful relationships is valid. The two proposed strategies, sequential and parallel, are well defined and easy to reproduce.\nThe analysis is systematic, as the authors test different types of datasets and provide a clear comparison with the baselines. The writing is clear, and the paper uses standard metrics and presents consistent results."}, "weaknesses": {"value": "- Integrating structural context or linked documents into LLMs is not a new idea.\n- The authors propose two well-implemented, but quite straightforward, approaches for adding context to the model. Concatenating texts or reusing parallel caches is not a significant conceptual innovation, but rather a simple variation in input processing.\n- Maybe I missed it, but I could not find any analysis explaining why the use of structural context works or how it interacts with semantic representation; everything remains at the implementation level.\n- The improvements are present but small, showing no clear qualitative leap—only marginal gains over the baselines.\n\nThe paper is well-executed and experimentally thorough, but lacks novelty. I would describe it as incremental work: it shows that certain practical choices (integrating structure during encoding) can yield slightly better results, but it does not advance the state of the art. It neither opens new research directions nor provides a theoretical framework. The paper is not poorly done, but it is not sufficiently innovative for this venue."}, "questions": {"value": "It would be helpful if the authors could further stress the conceptual novelty of their approach in relation to previous studies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tcwlqXSNbs", "forum": "akAwANLTmf", "replyto": "akAwANLTmf", "signatures": ["ICLR.cc/2026/Conference/Submission22743/Reviewer_o8N3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22743/Reviewer_o8N3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762708970561, "cdate": 1762708970561, "tmdate": 1762942366310, "mdate": 1762942366310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STRUC-EMB, a framework for generating structure-aware text embeddings by integrating relational information (such as hyperlinks, citations, or co-purchase links) directly into an LLM’s encoding process. It proposes two methods—sequential concatenation and parallel KV caching—and enhances them with context distillation and semantic balancing to handle noise and preserve target semantics. Across retrieval, clustering, classification, and recommendation tasks, these in-process methods consistently outperform text-only and post-hoc aggregation baselines, showing that directly encoding structural context yields more effective and scalable embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-written paper and very pleasant to read.\n- Comprehensive and carefully designed experiments.\n- Investigating structure-aware encoding is both timely and genuinely valuable."}, "weaknesses": {"value": "- The core findings are somewhat expected (e.g., structural information helps, and one must balance target embeddings against noisy context), though the proposed techniques—Context Distillation and Semantic Balancing—are thoughtful and appreciated.\n- I might appreciate a deeper discussion or qualitative/mechanistic analysis of how post-hoc aggregation fails to capture low-level interactions.\n\nMinor:\n- Line 200, ’structurlly’"}, "questions": {"value": "- Apologies if I missed this, but could you clarify the experimental configurations used for the Mean Neighbour and Weighted Mean Neighbour baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Zgwa5gGyv2", "forum": "akAwANLTmf", "replyto": "akAwANLTmf", "signatures": ["ICLR.cc/2026/Conference/Submission22743/Reviewer_xKdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22743/Reviewer_xKdw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22743/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763099986570, "cdate": 1763099986570, "tmdate": 1763099986570, "mdate": 1763099986570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}