{"id": "dJy6z9peC7", "number": 15245, "cdate": 1758249281992, "mdate": 1759897318439, "content": {"title": "Compressing Large MoE Models via Efficient Pruning and Data-Aware Calibration", "abstract": "Ultra-large Mixture-of-Experts (MoE) language models, \\eg DeepSeek-R1, are rapidly emerging as a dominant architecture due to their superior scalability and performance. However, the massive number of expert parameters introduces substantial redundancy, posing serious challenges for efficient deployment.\nExisting pruning methods face two fundamental challenges when applied to such MoE architectures. \nFirst, while methods based on reconstruction loss offer a more comprehensive selection by considering each expert combination, the vast search space renders exhaustive evaluation infeasible.\nSecond, most approaches rely on a fixed calibration dataset to guide pruning, which often fails to preserve the model’s full capabilities.\nTo address these challenges, we introduce two key innovations in our pruning framework. First, we propose a \\emph{Coarse-to-Fine Expert Selection} strategy that reduces the computational complexity of \nreconstruction-loss–based selection\nfrom an exponential ($\\mathcal{O}(\\binom{2n}{n})$) to a polynomial scale ($\\mathcal{O}(n^{1.5})$) with respect to the number of experts. This significantly accelerates the pruning process without sacrificing selection quality.\nSecond, we develop a \\emph{Dynamic Calibration Dataset Mixing} strategy that enables the model to adaptively adjust its calibration set during pruning.\nExtensive experiments on a range of benchmarks show that our method can prune 50\\% of the experts in a large-scale MoE model (\\eg DeepSeek-R1) while retaining 98.9\\% of its original performance across diverse tasks, outperforming existing pruning baselines. Our approach also demonstrates practical speedups and reduced memory footprint, facilitating efficient real-world deployment.\nThe anonymous implementation is available at \\url{https://anonymous.4open.science/r/DCDM-4C65-622a2bad88498795b8d7a92d85aca1315f9520ee}.", "tldr": "", "keywords": ["Mixture-of-experts", "pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e65590b4be7f865b0a80252428b25a22e3266fda.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses expert pruning in large Mixture-of-Experts models through two main contributions: a coarse-to-fine selection strategy reducing complexity from O(C(2n,n)) to O(n^1.5), and a dynamic calibration dataset mixing (DCDM) approach for cross-domain generalization. Theorem 4.1 bounds global discrepancy by layer-wise error accumulation. Experiments on DeepSeek-R1 (671B parameters) and Qwen3-30B retain 98.9% performance while pruning 50% of experts. While computationally efficient and theoretically grounded, the work requires deeper investigation of grouping strategies and domain-specific trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **The complexity reduction is substantial and well-demonstrated.** The coarse-to-fine strategy achieves O(n^1.5) versus O(n^2) for naive greedy search, validated through concrete comparison: 2.06×10⁵ evaluations versus 58×C(256,128) for baseline methods on DeepSeek-R1. This represents meaningful practical improvement for large-scale deployment.\n\n2. **Theorem 4.1 provides solid theoretical grounding.** The layer-wise bound justifies greedy optimization, with Figure 2 confirming tight empirical bounds (Lipschitz constants <0.5, discrepancies ≤1.4×10⁻⁷). This connection between theory and practice strengthens the methodological foundation.\n\n3. **DCDM addresses genuine cross-domain degradation.** Table 2 clearly demonstrates the problem: C4 calibration yields 59.91 on MMLU but 0.00 on LCB. The adaptive reweighting provides a principled solution, achieving 70.14 average versus 68.07 for best single-domain approach."}, "weaknesses": {"value": "1. **The grouping strategy lacks empirical validation.** Sequential partitioning into groups of size S is used without justification. Given the greedy nature of coarse-to-fine selection, initial grouping could significantly impact results if similar experts cluster together. The paper claims iterative regrouping mitigates this but provides no evidence. Critical missing experiments: performance variance across random groupings, comparison with similarity-based grouping, and analysis of whether optimal experts distribute uniformly or cluster in sequential ordering.\n\n2. **Domain-specific performance trade-offs are inadequately evaluated.** Table 2 shows DCDM achieves higher average scores, but the fundamental question remains unaddressed: does cross-domain mixing sacrifice peak single-domain performance? The shown comparisons (DCDM vs. mismatched calibration) differ in both method and calibration domain, confounding interpretation. When practitioners require optimal performance on specific domains (e.g., deploying solely for mathematical reasoning), does DCDM match or underperform domain-specific calibration with matched data? Table 2 suggests potential degradation (DCDM: 95.60 on Math500 vs. OpenR1-Math: 94.60), but lacks direct controlled comparison and discussion of deployment scenarios favoring specialization over generalization."}, "questions": {"value": "1. **Can you quantify the impact of initial grouping on final performance?** Report variance across multiple random groupings, compare sequential versus similarity-based partitioning, and analyze whether high-quality experts distribute uniformly in your ordering scheme.\n\n2. **Does DCDM compromise peak single-domain performance?** Provide direct comparison: DCDM versus domain-matched single-domain calibration, evaluated on that domain's benchmarks. When should practitioners choose DCDM over specialized pruning?\n\n3. **What justifies the exponential reweighting in DCDM?** Compare against linear or other update rules. Analyze convergence: typical iteration counts, stability of final weights, and behavior when stopping criterion isn't met."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oGVQIDNKmZ", "forum": "dJy6z9peC7", "replyto": "dJy6z9peC7", "signatures": ["ICLR.cc/2026/Conference/Submission15245/Reviewer_yUmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15245/Reviewer_yUmm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690673881, "cdate": 1760690673881, "tmdate": 1762925541259, "mdate": 1762925541259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a pruning framework designed to tackle memory overhead challenges when deploying large Mixture-of-Experts (MoE) models. The research focuses on addressing redundancy in large MoE models, where high memory costs from inactive experts create deployment barriers. Current reconstruction-loss-based pruning strategies struggle with two key issues: computationally prohibitive exhaustive searches for evaluating expert combinations and reliance on fixed calibration datasets that cannot preserve cross-domain performance. To overcome these limitations, the authors develop a framework featuring two key contributions: (1) a Coarse-to-Fine Expert Selection strategy that employs hierarchical greedy search with group-level scoring to reduce computational complexity and (2) a Dynamic Calibration Dataset Mixing strategy that adaptively reweights domain sampling based on pruning-induced discrepancies to improve generalization across domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides theoretical justification for its layer-wise pruning approach, proving that the global output difference can be bounded by the accumulation of local layer-wise discrepancy, which makes the layer-wise strategy more convincing and well-grounded.\n- The paper validates its approach on a large-scale MoE model (DeepSeek-R1), which is a true state-of-the-art open-source model, making the conclusions more credible and practically relevant."}, "weaknesses": {"value": "- The paper only provides complete baseline comparisons on Qwen3-30B-A3B-Thinking (two baselines across four datasets), while DeepSeek-R1 lacks proper baseline comparisons and only reports results on two datasets. The evaluation suffers from insufficient SOTA baselines, limited dataset coverage (only four or two datasets), and lacks evaluation on important benchmarks (e.g., GPQA).\n- The evaluation lacks testing across different model scales and families, which limits the validation of the method's generalizability and robustness across diverse architectures.\n- The paper only tests at a single 50% sparsity ratio without exploring other compression levels, limiting the understanding of the method's performance across different pruning intensities.\n- The main approach using layer-wise reconstruction-loss-based compression is similar to prior works (e.g., [1]), making the novelty incremental rather than substantial.\n\n[1] Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models"}, "questions": {"value": "1. How does the method perform with comprehensive baseline comparisons against multiple SOTA methods and on more diverse benchmarks?\n2. How does the method perform across different model scales and families?\n3. What is the performance of the proposed method at different sparsity ratios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ezaUaxOhav", "forum": "dJy6z9peC7", "replyto": "dJy6z9peC7", "signatures": ["ICLR.cc/2026/Conference/Submission15245/Reviewer_vzcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15245/Reviewer_vzcY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760880853422, "cdate": 1760880853422, "tmdate": 1762925540706, "mdate": 1762925540706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pruning framework for ultra-large Mixture-of-Experts LLMs that combines (i) a coarse-to-fine, layer-wise greedy expert selection based on output reconstruction discrepancy and (ii) a Dynamic Calibration Dataset Mixing strategy that adapts the calibration mixture across domains during pruning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "[1] The coarse-to-fine expert selection within a layer is an interesting idea that trades exhaustive evaluation for a two-stage screening (group-level then expert-level) guided by a reconstruction loss rather than router statistics.   \n[2] DCDM brings ideas from data-mixture optimization into post-training compression, adaptively up-weighting domains where pruning induces larger output discrepancy, which is under-explored in MoE pruning.   \n[3] The paper is generally well structured."}, "weaknesses": {"value": "[1] The selection uses an L2 reconstruction discrepancy on a small calibration set (32 sequences). While DCDM adapts the mix, the overall budget is tiny relative to model scale and task diversity.   \n[2] The theoretical bound assumes layer-wise independence in the propagation of discrepancies, effectively treating each layer’s pruning error as separable and additive.   \n[3] Limited analysis of router behavior and load balancing post-pruning.  Pruning half the experts may shift router distributions, hit the shared expert more heavily, or alter token load balance.   \n[4] Limited experiments. The paper only did experiments on Qwen3-30B-A3B-Thinking and DeepSeek-R1. Only a setting of the experiment (~50%) is conducted, and the baseline results are only included in the experiment of the Qwen model."}, "questions": {"value": "See weakness.  \n(1) It is better to vary the pruning ratio to see the performance trend for different methods to check the algorithm's robustness.   \n(2) It is better to include the efficiency comparison between pre- and post-pruned MoE models.  \n(3) It seems that the activation-based pruning method, like [a] only requires O(N), while the authors did not consider it in the discussion.   \n\n\n[a] Zhang, Z., Liu, X., Cheng, H., Xu, C., & Gao, J. Diversifying the expert knowledge for task-agnostic pruning in sparse mixture-of-experts. Findings of ACL 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lWvoAwWAIQ", "forum": "dJy6z9peC7", "replyto": "dJy6z9peC7", "signatures": ["ICLR.cc/2026/Conference/Submission15245/Reviewer_RQgE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15245/Reviewer_RQgE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620190178, "cdate": 1761620190178, "tmdate": 1762925539919, "mdate": 1762925539919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a pruning framework for large MoE LMs with two main pieces: 1- Coarse-to-Fine Expert Selection that turns reconstruction-loss–based expert selection from exponential to polynomial time. 2- Dynamic Calibration Dataset Mixing (DCDM) that reweights calibration domains during pruning based on the observed output discrepancy between the original and pruned models. \nEmpirically, pruning 50% of routed experts on Qwen3-30B-A3B-Thinking retains ~86% of the original average score and outperforms clustering- and weight/activation-based baselines. On DeepSeek-R1, pruned models retain ~98.9% of the original across AIME25 and LCB. The paper also studies time vs. number of experts and group size, matching the polynomial analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- Scalable pruning algorithm for MoEs: Clear coarse-to-fine design with a measured reduction from exponential to $O(n^{1.5})$ behavior in practice.\n\n2- Theoretical justification: A layer-wise pruning bound that justifies optimizing per-layer reconstruction instead of full-model comparison.\n\n3- Data-aware calibration: DCDM adapts the calibration mix using discrepancy feedback and improves cross-domain retention over a fixed recipe.\n\n4- Strong empirical results at scale: Competitive retention on Qwen3-30B-A3B-Thinking and ~99% retention on DeepSeek-R1.\n\n5- Good ablations and time/cost analysis."}, "weaknesses": {"value": "1- Baseline Coverage: Though there are clustering and simple weight/activate heuristic baselines are considered, the paper can be improved by including a simple hypernetwork-based baselines that predicts the retained experts based on the calibration dataset. Also, structural pruning baselines for proppsed for dense models are missing (weight importance can be aggregated for experts to have a single metric for each).\n\n2- Incomplete related work: This paper should discuss layer-wise often greedy pruning algorithms proposed for neural networks, e.g. [1], [2], etc..\n\n3- Limited task coverage:  Calibration is demonstrated on a small set of domains (math, code, general knowledge). The claim of broad cross-domain robustness would be stronger with more tasks. Qwen-3 is evaluate on MMLU, GPQA (variants), BBH, GSM8K, EvalPlus, MBPP, etc.\n\n4- Limited novelty: Reconstruction loss metric is not new. The bound relies on standard Lipschitz-style accumulation, which is conceptually incremental even if useful for MoE. Data-mixture scheduling echoes prior work on mixture reweighting (e.g. DoReMi-style ideas as the paper mentions). \n\n5- Missing inference latency /throughput values after pruning.\n\n[1] ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, Luo et. al, 2017\n\n[2] Efficient DNN Neuron Pruning by Minimizing Layer-wise Nonlinear Reconstruction Error∗, Jiang et. al , 2017"}, "questions": {"value": "1- How are experts grouped in algorithm 1? I can't seem to find this information in the paper.\n\n2- Why are other pruning criteria not reported for the mixed row in table 2?\n\n3- Can you provide some insights on how often the weight update oscillates among domains.\n\n4- How Do routers adapt to the smaller expert set? Are there any drift in load-balancing metrics?\n\n5- Did you explore post-pruning training to see how the model performs after some lightweight tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O3YxnEhjWt", "forum": "dJy6z9peC7", "replyto": "dJy6z9peC7", "signatures": ["ICLR.cc/2026/Conference/Submission15245/Reviewer_wtZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15245/Reviewer_wtZA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765554727, "cdate": 1761765554727, "tmdate": 1762925538881, "mdate": 1762925538881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}