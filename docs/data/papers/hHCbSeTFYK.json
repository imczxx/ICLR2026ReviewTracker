{"id": "hHCbSeTFYK", "number": 17780, "cdate": 1758280444342, "mdate": 1759897154436, "content": {"title": "GeoDetect: Geometric Adversarial Detection for VLPs", "abstract": "Vision-language pre-trained models (VLPs) are widely used in real-world applications. However, they remain vulnerable to adversarial attacks. Although adversarial detection methods have demonstrated success in single-modality settings (either vision or language), their effectiveness and reliability in multimodal models such as VLPs remain largely unexplored. In this work, we investigate the embedding spaces of VLPs and find that the image embedding space exhibits anisotropy. Our theoretical analysis shows that this anisotropic structure increases the separation between clean and adversarial examples (AEs) in the embedding space. Specifically, we demonstrate that AEs consistently exhibit greater expected distances to randomly sampled points than their clean counterparts, indicating that adversarial perturbations tend to push inputs out of manifold regions. Building on these insights, we propose GeoDetect, which leverages these off-manifold deviations to identify AEs. Through comprehensive evaluations, we show that our approach reliably detects adversarial attacks across various VLP architectures, including but not limited to CLIP, providing a robust and practical approach to improving the safety and reliability of these models.", "tldr": "", "keywords": ["Adversarial detection", "Geometric analysis", "Multimodal models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37533af70be364b2d997c2c99f739addb6afd61d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents GeoDetect, a theoretically grounded and effective framework for detecting adversarial examples in vision-language pre-trained models (VLPs). The authors uncover geometric anisotropy in VLP embeddings and leverage it for robust detection. Experiments across multiple datasets and models show excellent results (AUC≈1.0). The paper is well-motivated and clearly written, though more analysis on computational cost and real-world scalability would strengthen it."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tGeoDetect is grounded in a strong theoretical analysis of the geometric structure of vision-language model embeddings, demonstrating that adversarial examples tend to deviate from the data manifold. This provides clear interpretability and a robust theoretical basis for the proposed detection approach.\n2.\tThe method is task-agnostic and does not rely on specific network architectures or attention mechanisms, making it applicable to a wide range of vision-language models and downstream tasks with strong generalization ability.\n3.\tGeoDetect operates directly on existing model embeddings without additional training or parameter tuning, offering a lightweight and easily deployable solution with low computational overhead."}, "weaknesses": {"value": "1.\tAlthough presented as efficient, the computation of geometric measures such as k-NN or KDE in high-dimensional embedding spaces can be resource-intensive, limiting scalability for large-scale or real-time applications.\n2.\tWhile GeoDetect supports multimodal data in theory, the experiments mainly focus on image perturbations, with limited evaluation on joint image–text adversarial attacks, leaving its robustness against complex cross-modal attacks less explored.\n3.\tAlthough the analysis of embedding-space anisotropy is carefully conducted, the findings are somewhat expected — prior work has already hinted that adversarial examples tend to move off the data manifold. Thus, while the theoretical framing is sound, the insight is not particularly surprising."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xx1TcF7Ftn", "forum": "hHCbSeTFYK", "replyto": "hHCbSeTFYK", "signatures": ["ICLR.cc/2026/Conference/Submission17780/Reviewer_zP31"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17780/Reviewer_zP31"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760513216895, "cdate": 1760513216895, "tmdate": 1762927622132, "mdate": 1762927622132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a theoretical and practical framework for detecting adversarial examples in VLPs. The authors first show that VLP embedding spaces are anisotropic (i.e., that data representations cluster unevenly along certain dimensions). They prove that this anisotropy causes adversarial perturbations to push samples off the data manifold, resulting in increased geometric distances from clean examples. Building on this insight, the paper introduces GeoDetect, a lightweight, model-agnostic detection method that computes geometric scores (Local Intrinsic Dimensionality, k-NN distance, Mahalanobis distance, and Kernel Density Estimation) on image or multimodal embeddings to distinguish clean and adversarial inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide a theoretical foundation for adversarial detection in VLPs by linking embedding-space anisotropy to off-manifold deviations under adversarial perturbations. This insight unifies and generalizes previous intuition from unimodal settings.\n- Based on this foundation, they introduced GeoDetect, a simple yet powerful and model-agnostic detection framework that uses classical geometric metrics to achieve near-perfect detection performance across diverse architectures and tasks without retraining or fine-tuning.\n- They demonstrate strong empirical generalization and robustness, showing consistent detection accuracy across VLP families and downstream tasks. \n- They also demonstrate their method's robustness under adaptive attack scenarios."}, "weaknesses": {"value": "- Their fundamental claim that adversarial examples lie off the manifold in latent space is actually not new, having been demonstrated in prior unimodal contexts. The authors have actually done well to discuss this, but as this is foundational to their defense design the novelty is limited (originality claim remains strong however).\n\n- The anisotropy analysis, though central to the theoretical argument, remains largely descriptive. While measures such as $(I_1, I_2)$, and effective rank support anisotropy qualitatively, the paper never quantifies how anisotropy correlates with detection performance or adversarial vulnerability across architectures. This weakens the causal link between the theoretical foundation and the empirical effectiveness of GeoDetect.\n\n- The method’s scalability and computational efficiency are underexplored. Geometric metrics such as k-NN, Mahalanobis, and KDE in practice scale poorly with embedding dimensionality and dataset size. The paper claims GeoDetect is “lightweight,” but offers no analysis of runtime complexity, memory footprint, or potential bottlenecks for real-time applications.\n\n- The paper focuses primarily on standard, gradient-based PGD-style attacks. It lacks experiments on modern, semantically aligned or diffusion-based attacks that perturb both modalities coherently. Without this, it’s unclear if the geometric signal exploited by GeoDetect generalizes to more adaptive or distribution-preserving adversaries.\n\n- While the authors have done most things right in the paper, they omitted analysis on failure cases and sensitivity to model variance. While results show near-perfect AUC scores, no discussion is offered on outliers, false positives, or potential degradation in low-sample or highly anisotropic conditions (e.g., small-scale VLPs or different pre-training datasets). This absence makes the method’s reliability across unseen regimes uncertain, especially given that embedding geometry can vary substantially between pre-training objectives."}, "questions": {"value": "Check weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7TUEmmutGy", "forum": "hHCbSeTFYK", "replyto": "hHCbSeTFYK", "signatures": ["ICLR.cc/2026/Conference/Submission17780/Reviewer_mPpr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17780/Reviewer_mPpr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761284400115, "cdate": 1761284400115, "tmdate": 1762927621567, "mdate": 1762927621567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GeoDetect is a geometry-based adversarial detection framework for vision–language models (VLPs).  \nIt leverages the anisotropy of multimodal embeddings and formulates a measurable *expected distance gap* to separate clean and adversarial samples through geometric scores (LID, k-NN, Mahalanobis, KDE).  \nThe approach is model- and task-agnostic, showing strong results on zero-shot classification and image-text retrieval, though it still relies on key assumptions and limited attack coverage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Clear theoretical motivation.** The paper clearly connects VLP anisotropy and “off-manifold” adversarial behavior, and mathematically supports using geometric scores (LID, k-NN, Mahalanobis, KDE) as detection criteria.  \n* **Model- and task-independent.**  The framework works without access to task heads or logits, transferring naturally between zero-shot classification and cross-modal retrieval.  \n* **Broad experimental setup.** The authors evaluate single-modal, multimodal, and text-perturbation scenarios, and report robustness to several adaptive attacks and different attack backbones."}, "weaknesses": {"value": "* **Dependence on anisotropy assumptions.** The method’s validity heavily relies on the anisotropy hypothesis. It remains uncertain whether this still holds under stronger attacks such as **M-Attack** or **SA-AET (Semantic-Aligned Adversarial Evolution Triangle)**\n, which might deliberately align adversarial embeddings with the original manifold.  \n* **Limited baselines.** Only one prior baseline from 2022 is compared, which is insufficient to demonstrate progress against more recent adversarial detection methods for VLPs.  \n* **Reference set dependence.** The method requires a clean sample pool for computing geometric metrics, but the paper does not explain how to construct, maintain, or recalibrate this reference set under distribution shift or contamination.  \n* **Layer-specific sensitivity.** Although the paper notes that LID is most effective at multimodal layers, it also shows that in certain ALBEF multimodal attacks, k-NN/KDE degrade substantially. A systematic analysis or automatic layer-selection strategy is missing."}, "questions": {"value": "The chosen attacks and baselines are overly simplistic; the current experimental setup is insufficient to demonstrate the method's effectiveness under stronger or more realistic threats."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3XbkbY6MtA", "forum": "hHCbSeTFYK", "replyto": "hHCbSeTFYK", "signatures": ["ICLR.cc/2026/Conference/Submission17780/Reviewer_XXVv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17780/Reviewer_XXVv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576122457, "cdate": 1761576122457, "tmdate": 1762927621022, "mdate": 1762927621022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce GeoDetect, a lightweight and model-agnostic approach for detecting adversarial examples on vision-language pre-trained models (VLPs), such as CLIP. The authors observe that VLP embeddings are anisotropic, meaning that clean data embeddings lie on a data manifold, and as a consequence adversarial examples move off that manifold. GeoDetect uses simple geometric metrics—like k-NN, Mahalanobis distance, LID, and KDE—to quantify these off-manifold shifts and identify adversarially attacked examples. Experiments on several datasets and attacks show that GeoDetect achieves good adversarial detection performance across architectures and tasks, offering an efficient defense for multimodal models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses a timely and important challenge, detecting adversarial attacks in vision-language pre-trained models, offering a practical contribution to improving the safety of multimodal AI models.\n\n* The approach is simple, efficient, and broadly applicable, requiring no model fine-tuning or task-specific modifications.\n\n* It is clearly written and easy to follow; the main idea of the proposed method is intuitive."}, "weaknesses": {"value": "* one of the main weaknesses of the paper is the limited novelty of the proposed approach. Using embedding-space distances or density-based measures for adversarial detection has been explored in prior works (e.g., [0, 1, 2]), which makes the contribution primarily an adaptation of existing ideas to multimodal settings.\n\n* the method does not account for adaptive attacks specifically designed to keep adversarial examples on the clean data manifold. Such attacks could significantly degrade detection performance, yet this scenario is not evaluated in the experiments.\n\n* moreover, prior studies have already demonstrated that adversarial attacks incorporating distance or manifold regularization terms can successfully bypass similar geometric defenses (e.g., [4, 5]), highlighting the need for a more thorough robustness evaluation.\n\n[1] Ma, Xingjun, et al. \"Characterizing adversarial subspaces using local intrinsic dimensionality.\" arXiv preprint arXiv:1801.02613 (2018).\n\n[2] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in neural information processing systems 31 (2018).\n\n[3] Cohen, Gilad, Guillermo Sapiro, and Raja Giryes. \"Detecting adversarial samples using influence functions and nearest neighbors.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[4] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" International conference on machine learning. PMLR, 2018.\n\n[5] Bryniarski, Oliver, et al. \"Evading adversarial example detection defenses with orthogonal projected gradient descent.\" arXiv preprint arXiv:2106.15023 (2021)."}, "questions": {"value": "* how does GeoDetect differ from prior geometric or distance-based adversarial detection methods ([1–3]) beyond applying them to multimodal settings?\n\n* how would the method perform against adaptive attacks designed to keep adversarial examples on the clean data manifold?\n\n* given that prior works ([4, 5]) show such attacks can bypass geometric defenses, how robust is GeoDetect under similar adaptive scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FRw1BPBRxB", "forum": "hHCbSeTFYK", "replyto": "hHCbSeTFYK", "signatures": ["ICLR.cc/2026/Conference/Submission17780/Reviewer_C6a5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17780/Reviewer_C6a5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174941156, "cdate": 1762174941156, "tmdate": 1762927620614, "mdate": 1762927620614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}