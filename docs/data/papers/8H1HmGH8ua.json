{"id": "8H1HmGH8ua", "number": 15694, "cdate": 1758254039575, "mdate": 1759897288444, "content": {"title": "LongVTG-R1: Reinforcement Learning for Robust Long-Video Temporal Grounding", "abstract": "We present the first Reinforcement Learning (RL)-based framework that equips Multimodal Large Language Models (MLLMs) with long-video temporal grounding skills, and demonstrate that this approach also generalizes to improve performance on general question-answering (QA) tasks. Unlike dominant supervised fine-tuning (SFT) methods, RL enables models to acquire temporal grounding abilities without risking catastrophic forgetting of their core understanding. However, adopting RL for long-video temporal grounding reveals a challenge in balancing exploitation of pre-trained knowledge with exploration of new localization skills. To address this, we propose Token-aware KL Regularization, which selectively relaxes the KL-divergence regularization on timestamp-related tokens to guides exploration. Moreover, effective exploration requires a learning signal that alleviates the sparsity of key events in long videos, for which we introduce a denser reward, the Center Distance Reward (CenDist). To further mitigate grounding ambiguity between language queries and visually similar content, and to facilitate effective RL training, we automatically construct SceneTG, a dataset with sharp boundaries and unambiguous queries. Our resulting model, QwenLongTG, achieves state-of-the-art results across a suite of temporal grounding benchmarks and, through its generalization to QA, provides the first evidence that precise temporal localization is a foundational skill for broader long-video understanding.", "tldr": "We present LongVTG-R1, the first RL-based framework for long-video temporal grounding, which leverages novel regularization, reward design, and dataset construction to achieve state-of-the-art performance and generalize to QA tasks.", "keywords": ["video understanding", "temporal grounding", "multimodal large language model (MLLM)"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01d911c53745a99058f0f35b3f5cac594e33cf7f.pdf", "supplementary_material": "/attachment/a7526f2447a3b01b5d091f2498a198a2aae0ee49.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents LongVTG-R1, a framework applying Reinforcement Learning to enhance the long-video temporal grounding capabilities of Multimodal Large Language Models. The authors identify that standard Supervised Fine-Tuning can lead to catastrophic forgetting  and propose RL as a better alternative. The paper introduces three main technical contributions to make RL viable for this task: (1) Token-aware KL Regularization to balance exploration (for timestamps) and exploitation (for general language); (2) a Center Distance Reward to create a denser reward signal and alleviate the sparsity of the standard IoU metric ; and (3) a new dataset, SceneTG, constructed automatically using scene boundaries and a uniqueness filter to provide clear and unambiguous training signals. The authors show that their model achieves state-of-the-art results on several zero-shot VTG benchmarks and claim this skill also generalizes to improve general-purpose long-video Question-Answering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Relevant Problem and Sound Motivation: The paper tackles the challenging and important problem of long-video temporal grounding. The motivation to use RL as an alternative to SFT to prevent catastrophic forgetting is sound and follows a logical trend from recent works on shorter video grounding.\n\n2. Clear Identification of Challenges: The authors correctly identify the key bottlenecks in applying RL to this domain: the exploration-exploitation trade-off , reward sparsity , and data ambiguity. The paper is well-structured around proposing a specific solution for each of these problems.\n\n3. Intuitive Technical Solutions: The proposed contributions are clever and intuitive. The Token-aware KL regularization, in particular, is a task-specific and sensible way to manage the KL penalty by partitioning the vocabulary. The \"propose-then-annotate\" paradigm for the SceneTG dataset is also a practical approach to generating higher-quality data.\n\n4. Strong Empirical Grounding Results: The paper demonstrates strong performance on its primary task: temporal grounding. As shown in Table 2, LongVTG-R1 achieves state-of-the-art (SOTA) results across a suite of six zero-shot benchmarks. Achieving this with a smaller dataset (40k) than some competitors highlights the data efficiency of the approach."}, "weaknesses": {"value": "1. Overstated Generalization to QA: A major weakness is the overstatement of the paper's \"most surprising\" finding—generalization to direct long-video QA. This claim is not well-supported by the results in Table 3. The model actually performs worse than the Qwen2.5VL baseline on LongVideoBench (58.7 vs 60.5) and is only comparable on MLVU (69.1 vs 68.6). The claim of \"consistent gains\"  is therefore inaccurate. The strong improvements are only clearly demonstrated in the \"ground-then-answer\" pipeline (Table 4). This is a different and less significant finding, as it's expected that providing a QA model with the correct pre-localized clip would improve its performance. This discrepancy undermines one of the paper's central narratives.\n\n2. SceneTG Dataset is Underspecified: The SceneTG dataset is presented as a key contribution, but it is poorly described. The paper details the method of construction (Sec 3.4)  but provides no statistics on the resulting dataset. Critical details are missing, such as the final number of (video, QA) pairs, the total hours of video, the source of the 20k Prime Videos, or the rejection rate of the uniqueness filter. The \"40k\" data mentioned  is ambiguous. This lack of detail makes the contribution hard to evaluate and the work difficult to reproduce.\n\n3. Incremental Novelty and Insufficient Ablation:\n- The overall design of the proposed approach is simple and straightforward; however, no notably impressive performance improvement has been observed compared to previous methods.\n- The overall framework is a careful adaptation of existing methods (GRPO , and concepts from models like VideoChat-R1 ) to the long-video domain. The novelty is more incremental than foundational.\n- The technical contributions, while intuitive, lack thorough validation. For CenDist, the paper does not compare it against the large body of existing work on dense regression losses (e.g., GIoU, DIoU)  that were designed to solve the exact same problem (zero gradients for non-overlapping predictions).\n- For Token-aware KL, the choice of $\\beta_{time}=0$ is an extreme value. An ablation study comparing this to other small, non-zero values would have been necessary to fully justify this design choice."}, "questions": {"value": "1. The paper claims \"consistent gains\" on general QA tasks , but the results in Table 3 appear to contradict this, showing that LongVTG-R1 (58.7) actually underperforms the baseline Qwen2.5VL (60.5) on LongVideoBench. Could the authors please address this discrepancy and clarify why the model fails to improve on this specific benchmark? Does this not suggest that the claimed generalization to direct QA is more limited than stated?\n\n2. The SceneTG dataset is presented as a core contribution, yet the paper lacks crucial statistics for reproducibility and assessment. Could the authors please provide the final size of the dataset (number of QA pairs and total video hours), the acceptance/rejection rate of their uniqueness filter , and clarify whether the \"40k\" training data mentioned  refers to the SceneTG dataset alone or the total mixture of all three datasets?\n\n3. The proposed Center Distance Reward (CenDist) addresses the issue of reward sparsity from IoU, which is a well-studied problem. Could the authors justify why they chose to design this new reward function  instead of adapting existing, established dense regression losses (like GIoU or DIoU), which were created to solve the exact same problem of zero gradients for non-overlapping predictions?\n\n4. For the Token-aware KL regularization, the author make the strong design choice of setting the penalty for time tokens, $\\beta_{time}$, to exactly zero. The ablation in Table 6 only compares this against the baseline, not against other small, non-zero values for $\\beta_{time}$. Could the authors provide justification for this specific choice, as setting the penalty to zero could theoretically risk divergence and seems to warrant a more thorough ablation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GPynosR2Fg", "forum": "8H1HmGH8ua", "replyto": "8H1HmGH8ua", "signatures": ["ICLR.cc/2026/Conference/Submission15694/Reviewer_oC5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15694/Reviewer_oC5c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567411432, "cdate": 1761567411432, "tmdate": 1762925945287, "mdate": 1762925945287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a reinforcement RL based framework for equipping MLLMs with long-video temporal grounding skills. The authors propose two main technical innovations: (1) Token-aware KL Regularization to balance exploration (for timestamp tokens) and exploitation (for general language tokens) during RL fine-tuning, and (2) a Center Distance Reward (CenDist) to provide denser learning signals for temporal localization. Additionally, they construct a new dataset, SceneTG, with sharp scene boundaries and unambiguous queries to facilitate effective RL training. The model achieves state-of-the-art results on several temporal grounding benchmarks and demonstrates that improved temporal localization skills generalize to long-video question answering (QA) tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The RL approach to long video temporal grounding is a nice idea an approach, a natural extension to techniques used for short-term videos. The details about token-aware KL regularization (theoretically motivated and practically effective according to the ablation) for balancing exploration and exploitation is interesting. Another good technical contribution is the CenDist reward addresses the reward sparsity problem, which is particularly acute in long-video settings. Additionally, the authors also construct a dataset of 16k samples which seems to be well constructed for the task and gives a boost in performance. The evaluation and ablation section are extensive also."}, "weaknesses": {"value": "While the idea is interesting and full of experiments, I believe there are some major issues:\n\n1 - Overclaim: The authors claim to outperform previous works on multiple out of domain datasets, but according to table 2 they perform better only in 2: ReXTime (significant improvement) and ANet (very marginal, +0.2), while being outperformed in other 4 datasets. To me this means that the method is actually not that effective and improvement are only marginal despite the use of also a custom dataset (while this data is smaller is also more qualitative). \n\n2 -  General QA with full video input (Table 3), the method is still outperformed in 2 out of 4 benchmarks.\n\n3 - Ablation study lack completeness: For example experiments are made only on 30% of the data, is not a big issue but given that the improvements are marginal maybe those improvements might not hold in full data. Additionally, an ablation on alpha is important to understand the effect of CenDist.\n\nWhile the contributions are really interesting, the claims are not fully supported by the results. My initial evaluation will be 4, the paper has a lot of things to improve but I am willing to increase my score if authors address my concerns."}, "questions": {"value": "Check above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EVc4iYjE9x", "forum": "8H1HmGH8ua", "replyto": "8H1HmGH8ua", "signatures": ["ICLR.cc/2026/Conference/Submission15694/Reviewer_R286"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15694/Reviewer_R286"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626991866, "cdate": 1761626991866, "tmdate": 1762925944799, "mdate": 1762925944799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts post-training on the long video temporal grounding task. Specifically:\n\n1. The paper proposes Token-aware KL Regularization to balance the exploration and exploitation of the reasoning model.  \n2. The paper introduces a dense reward based on segment distance to alleviate the issue that predictions with an IoU of 0 do not contribute to gradient updates.  \n3. The paper automatically constructs an unambiguous training set using a scene detector."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a CenDist reward, which makes the rewards denser compared to the simple IoU reward.  \n2. The paper identifies the presence of ambiguous temporal boundaries and the phenomenon where a query may correspond to multiple time intervals in existing datasets. Based on this observation, a new training set is constructed.  \n3. The paper is clearly presented, making it easy to understand the work conducted, and the experimental results are reported honestly."}, "weaknesses": {"value": "Overall, I think the contributions of this paper are limited. The modifications in reward shaping and KL divergence are more like two tricks, and the ablation study in Table 6 also shows that these two methods do not seem to bring significant improvements. Moreover, the quality of the automatically constructed training dataset is not further elaborated. Moreover:\n\n1. There are some missing references in the paper. For example, #305 Prime Video, as well as other papers such as LITA [1], GroundVQA [2], and SnAG [3], should also be reflected in the related work section.\n\n2. The concept of \"exploration\" in the context of temporal grounding feels somewhat odd. In math reasoning, exploration refers to the trial-and-error process of exploring intermediate steps. If we draw an analogy to the method proposed in this paper, wouldn't it be akin to directly guessing the final answer in a math problem? Additionally, to accurately assess whether exploration is being suppressed, the entropy of the output should be examined, similar to what was done in UI-TARS-2.\n\n3. In real-world scenarios, one query is indeed often related to multiple segments. How can this be handled instead of directly filtering them out in the proposed training set?\n\n4. Table 1 lacks some other baselines, such as the Video-XL series.\n\n5. For the grounding model in Table 4, more baselines like UniTime and TimeSuite should be included, similar to Table 7 in the UniTime paper.\n\n> [1] LITA: Language Instructed Temporal-Localization Assistant.  \n> [2] Grounded Question-Answering in Long Egocentric Videos.  \n> [3] SnAG: Scalable and Accurate Video Grounding."}, "questions": {"value": "1. Could the use of scene boundaries make the training task overly simplistic for the model? Additionally, what is the accuracy of ShotCoL for boundary detection? (i.e., the quality of the proposed training set)\n2. Long video understanding is inherently challenging, and RL typically requires a strong prior. Would it be more effective to conduct post-training on a model already specialized in long video temporal grounding, rather than starting from Qwen2.5-VL?\n3. Have you considered incorporating additional training datasets (e.g., UniTime-Zero) to enhance model performance further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xv3dqhByq4", "forum": "8H1HmGH8ua", "replyto": "8H1HmGH8ua", "signatures": ["ICLR.cc/2026/Conference/Submission15694/Reviewer_vtR2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15694/Reviewer_vtR2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652690232, "cdate": 1761652690232, "tmdate": 1762925944397, "mdate": 1762925944397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LONGVTG-R1, a RL framework for long-video temporal grounding. It proposes Token-aware KL regularization to balance exploration and exploitation, a dense CenDist reward to mitigate sparse supervision, and an automatically constructed SceneTG dataset with sharp boundaries and unambiguous queries. LONGVTG-R1 provide  evidence that precise temporal localization is a foundational skill for broader video understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CenDist reward: The paper gives an inexpensive, model-agnostic auxiliary signal that densifies the naturally sparse IoU reward without extra annotation.\n2. The paper purposed  an automatic “propose-then-annotate” pipeline that uses scene-boundary detection plus uniqueness filtering to generate sharp-boundary, unambiguous training pairs, scalable to 20 k unlabeled videos.\n3. Technical sections derive Token-aware KL from first principles and give closed-form properties; pseudocode and illustrative figures accompany data-construction pipeline."}, "weaknesses": {"value": "1. Compared to the baseline model VideoChat-R1, there was no significant performance improvement on several mainstream TG datasets, Charades(61.5 vs 60.8), ANet(36.8 vs 36.6), QVHighLight(57.3 vs 57.5), The model shows significant improvements over the baseline model on some longer datasets, but it cannot be proven whether the improvement is due to the introduction of more long-term training data (40K vs 16K) or the result of the proposed method.\n2. Missing qualitative error analysis. Failure cases are only shown for QA (Fig. 3). Provide grounding failure modes: drifting beyond scene cuts, duplicate predictions, or confusion between “first” vs. “last” event. Categorizing 50 errors would guide future improvements.\n3. Token-aware KL generality claim is overstated. Theory assumes discrete, finite vocabulary; vision-language models often use sub-word tokens where timestamp digits are fragmented (e.g., “1”, “7”, “:” are separate). The paper does not show how often fragmented digits appear or whether relaxing KL on all digit tokens accidentally releases semantically important numeric tokens unrelated to time.\n\nMinor formatting error: in Table2, QVHigh column, two values, 57.5 and 57.3, are both bolded."}, "questions": {"value": "1.  Please report an “SFT-only” curve trained on the same 40 k SceneTG samples. If that curve already closes most of the gap with LONGVTG-R1, it would show that data quality, not RL, drives the gains.\n2. Weaknesses1 and Weaknesses2"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l6AUZ9kVG3", "forum": "8H1HmGH8ua", "replyto": "8H1HmGH8ua", "signatures": ["ICLR.cc/2026/Conference/Submission15694/Reviewer_CjPm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15694/Reviewer_CjPm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819294698, "cdate": 1761819294698, "tmdate": 1762925943881, "mdate": 1762925943881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}