{"id": "4UrjpzvAy6", "number": 11640, "cdate": 1758202750402, "mdate": 1759897563375, "content": {"title": "A Universal Self-Supervised Paradigm via 3D Gaussian Splatting", "abstract": "Pre-training on large-scale unlabeled datasets has proven effective for enhancing model performance on downstream tasks, particularly when annotated data is scarce. However, due to the inherent discrepancies in data structures across modalities, most existing self-supervised approaches are tailored to either 2D or 3D networks, limiting their generalizability. In this paper, we propose GS$^3$, a 3D Gaussian Splatting (GS)-based universal self-supervised framework, which bridges 2D and 3D modalities and enables pre-training of both 2D and 3D encoders. The core idea is to formulate neural rendering as a pretext task: visual features extracted from input data are used to predict scene-level 3D Gaussians, which are then rendered into images via a fast tile-based rasterizer. The model is optimized by minimizing the difference between rendered and real images, with a masked modeling strategy further encouraging robust and spatially-aware representation learning. We evaluate GS$^3$ across five representative downstream tasks, including detection, segmentation, and reconstruction. Experimental results show that GS$^3$ consistently achieves performance on par with or surpassing state-of-the-art methods, while significantly reducing memory overhead compared to prior NeRF-based approaches.", "tldr": "We propose a 3D Gaussian Splatting-based universal self-supervised framework, which bridges 2D and 3D modalities and enables pre-training of both 2D and 3D encoders.", "keywords": ["Self-supervised learning", "gaussian splatting", "3D vision"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/726d53221fb38225f2e8da3f81d9cc1e93a5cd6a.pdf", "supplementary_material": "/attachment/2f6dd33c41cda31aa1ae7e63174dbcdf0b786326.pdf"}, "replies": [{"content": {"summary": {"value": "#### **Summary**: This paper proposes a rendering-driven self-supervised framework, GS³, that learns both 2D and 3D encoders by predicting scene-level 3D Gaussian primitives from two overlapping views and supervising them via a tile-based differentiable rasterizer with photometric + LPIPS losses. An epipolar-transformer module aggregates cross-view cues; a masked-modeling variant (≈50% masking) reduces redundancy and encourages holistic spatial features. On indoor benchmarks (ScanNet v2, SUN RGB-D, S3DIS), GS³ pre-training yields consistent gains for 3D detection/semantic/instance segmentation and image-based 3D tasks (e.g., +3.0 mAP@0.5 for VoteNet on SUN RGB-D; +2.8 mIoU on S3DIS at 2 cm voxels), while reporting substantially lower memory than a NeRF-style pretext (≈10.3 GB vs 38.4 GB per batch at stated settings). The supplement adds per-class tables and a synthetic reconstruction study (ConvONet) showing additional improvements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- #### **S1. Unified rendering objective across modalities:** A single Gaussian-splatting pretext supervises both 2D (ResNet-50) and 3D (PointNet++/MinkUNet) encoders with the same image-space loss, avoiding explicit depth supervision.\n\n- #### **S2. Efficiency vs. NeRF pretexts:** Projection-free, tile-based rasterization yields lower VRAM during pre-training than a NeRF-based baseline under the reported setup.\n\n- #### **S3. Reasonable ablations:** Analyses for view count, mask ratio, and input resolution provide actionable guidance (e.g., 2-3 views, ~50% masking)."}, "weaknesses": {"value": "- #### **W1. “Universal” claim overreaches the stated supervision and data assumptions:** The core pipeline explicitly requires two posed overlapping views and projects Gaussians with a known camera pose W (Eq. 1), while 3D pre-training back-projects RGB-D into point clouds, i.e., relies on depth, and 2D pre-training also assumes two-view inputs. Despite this, the introduction highlights “ambiguity-free” rendering and positions the framework as universal, yet all pre-training is on a single indoor dataset (ScanNet v2). This combination of posed two-view supervision, depth use on the 3D path, and indoor-only evidence substantially narrows the scope relative to a “universal SSL” claim.\n\n- #### **W2. Gains are modest/uneven without variance reporting, with notable per-class regressions:** Headline improvements over strong baselines are small (e.g., SUN RGB-D 3D detection mAP@0.5: 36.7 vs 36.6 for Ponder-RGBD; image-based 3D detection mAP@0.25: 51.4 vs 50.2 over a NeRF pretrain) and reported without seeds or error bars. Per-category tables reveal large negative swings (e.g., S3DIS instance segmentation “window”, 11.6 AP@0.5; multiple drops on ScanNet v2), suggesting fragile effect sizes and task/category sensitivity that the paper does not analyze. Together, this weakens the strength of evidence that the method reliably outperforms contemporaries.\n\n- #### **W3. Efficiency claims emphasize memory but omit runtime/throughput and mix accounting units:** The paper argues efficiency via a memory table (10.3 GB vs 38.4 GB), yet provides no wall-clock throughput (iters/s, FPS), GPU-days, or end-to-end pre-training time. The comparison also mixes “#Rendering Pixels = 4,800” for Ponder with a resolution of 320×240 for GS3, making it hard to audit equivalence. Without standardized time/VRAM/throughput curves at matched resolution/batch, the efficiency story is under-substantiated.\n\n- #### **W4. Reproducibility risk from unspecified depth discretization and Gaussian count, plus missing ablations on core design choices:** The method predicts k Gaussians per point/pixel and discretizes depth into Z bins, but concrete values for k and Z (and choices like SH order/opacity parameterization) are not stated where introduced, and there is no sensitivity analysis for them. Ablations focus on #views, mask ratio, and input resolution, leaving key components, (e.g., Epipolar Transformer, k/Z, and LPIPS usage) unexamined."}, "questions": {"value": "- #### **Q1. Scope/robustness:** Can GS³ pre-train without accurate poses or with pose noise, and under single-view pretext? Any outdoor/ego-centric results to support “universal”?\n\n- #### **Q2. Compute:** Please provide wall-clock (per-epoch), throughput (imgs/s), and Gaussians/sample versus resolution, #views, k, and Z, plus a budget-matched NeRF baseline.\n\n- #### **Q3. k/Z sensitivity:** What values of top-k and depth binning Z are used, and how do downstream metrics vary with them? Any numerical stability safeguards for overlapping Gaussians?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "weHJBsHSou", "forum": "4UrjpzvAy6", "replyto": "4UrjpzvAy6", "signatures": ["ICLR.cc/2026/Conference/Submission11640/Reviewer_B2HU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11640/Reviewer_B2HU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824367930, "cdate": 1761824367930, "tmdate": 1762922708661, "mdate": 1762922708661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of the paper is to introduce a new (3D Gaussian Splatting) rendering-based SSL pre-training approach (tailored to 3D tasks), which works for pre-training both 2D and 3D encoders.\n\nAuthors propose predicting 3D Gaussians directly from extracted feature representations and pre-training the encoders by minimizing the photometric loss between images rasterized by a 3D Gaussian Splatting-based renderer and ground-truth RGB images. The proposed pipeline consists of (RGB or pointcloud) feature encoder (which is being pre-trained), epipolar transformer for feature refinement, MLP for 3D gaussians parameters prediction, and 3D Gaussian Splatting-based renderer for rasterization. \n\nThe contributions of this work are as follows:\n1) new (3D Gaussian Splatting) rendering-based SSL pre-training approach which boosts the results on downstream tasks of 3D detection, segmentation and reconstruction;\n2) ablations of pipeline hyperparameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) Provides a new method for rendering-based SSL using 3D Gaussian splatting.\n2) Proposed method works with 2D and 3D (point-based and discretisation-based) encoders. \n3) Reduces memory usage while achieving comparable or better results to state-of-the-art methods."}, "weaknesses": {"value": "1) Results are reported only for indoor scenes. \n2) L315-316: The reported improvement of the pre-trained model over the baseline is relatively small, and the evaluation relies only on aggregated metrics from downstream tasks. The absence of training curves or information on fine-tuning duration (e.g., number of epochs or steps) limits the ability to assess what the performance gains stem from.\n3) Limitations of the proposed method are not studied (outlined)."}, "questions": {"value": "1) Why, on the one hand, is the proposed approach described as universal for training 2D and 3D encoders, but on the other hand, reformulated Gaussian centred prediction for 3D encoders is used (which is not available for 2D)?  \n\n2) L194-196, L229: Why specifically these encoder architectures were chosen? \n\n3) L258-259: Durig training 50% of input is masked. How is this handled at inference time?\n\n3) If masking 50% of the input helps to address the redundancy in Gaussian representations, have you considered keeping the full input while reducing the number of Gaussians (e.g., using half of the original number)?\n\n4) How well might the method extend to outdoor scenes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jSbQ2xbiFK", "forum": "4UrjpzvAy6", "replyto": "4UrjpzvAy6", "signatures": ["ICLR.cc/2026/Conference/Submission11640/Reviewer_Bqno"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11640/Reviewer_Bqno"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899774196, "cdate": 1761899774196, "tmdate": 1762922708182, "mdate": 1762922708182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes $GS^3$, a universal self-supervised learning (SSL) framework based on 3D Gaussian Splatting (GS). The core idea is to use neural rendering as a pretext task: the model extracts visual features from input data (either 2D images or 3D point clouds) to predict scene-level 3D Gaussians, which are then rendered back into 2D images via a fast tile-based rasterizer. Optimization is performed by minimizing the photometric difference between the rendered and real images.The main contribution of this paper is that the framework unifies the pre-training of both 2D and 3D encoders for the first time. Compared to previous NeRF-based rendering SSL methods, $GS^3$ avoids expensive volumetric rendering, significantly reduces memory overhead, and is more effective when training 2D encoders as it avoids the ambiguity issues that NeRF faces in the absence of depth. Experimental results show that the encoders pre-trained by this framework achieve performance comparable to or better than SOTA on five downstream tasks (such as detection, segmentation, and reconstruction)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel and Practical Framework: This paper presents a novel and valuable universal SSL framework. It skillfully uses 3D GS as a bridge to unify 2D and 3D modality pre-training, addressing the issue that existing SSL methods are often modality-specific.\n- Significant Efficiency Advantage: The greatest advantage of this method is its efficiency. By using a fast tile-based rasterizer instead of NeRF's volumetric rendering, the framework significantly reduces the memory overhead of pre-training. The data in the paper (10.3 GB vs 38.4 GB) strongly demonstrates its substantial advantage over NeRF-based methods.\n- Sufficient Experimental Validation: The paper validates the effectiveness of the pre-trained model on five representative downstream tasks (including 3D detection, 3D segmentation, and image-based reconstruction). Notably, the method is significantly superior to NeRF-based approaches in training 2D encoders (e.g., in image-based 3D detection and scene reconstruction tasks), which strongly supports the authors' core motivation.\n- Reasonable Model Design: The masked modeling strategy introduced in the paper is reasonable. It not only aims to improve feature robustness but also helps reduce redundant GS primitives, and its effectiveness was validated in ablation studies."}, "weaknesses": {"value": "- Novelty Argumentation: The core contribution of the paper is replacing the backbone of the \"rendering-based SSL\" idea from NeRF to 3D GS. While this brings significant efficiency gains, the paper's discussion of technical novelty could be clearer. For example, the Epipolar Transformer in the framework is borrowed from PixelSplat. The authors should clarify in more detail the unique challenges faced when applying GS to SSL (and not just for novel view synthesis) and how $GS^3$ overcomes them.\n- Overstated Definition of \"Universal\": The \"universality\" of the framework appears to be overstated. The pre-training of the 3D encoder relies on RGB-D input to back-project point clouds, which is a strong prerequisite. While the 2D encoder pre-training only uses RGB, it seems to be a separate process from the 3D pre-training. This looks more like two independent pipelines sharing a rendering objective, rather than a single \"universal\" framework that can handle either modality as input.\n- Weak Experimental Baseline: When evaluating 2D encoder performance, the paper compares $GS^3$ against a \"NeRF-based pre-training\" implemented by the authors themselves. The specific implementation of this baseline (e.g., whether it is consistent with Ponder-RGBD) and its competitiveness are unclear. If this is a weak NeRF implementation, the convincingness of $GS^3$'s lead (e.g., 1.2% in Table 3) is diminished.\n- Insufficient Ablation Study: The ablation study shows that using 3 views works better than 2 views. The authors chose the 2-view setting for efficiency reasons, which is reasonable. However, the paper fails to provide specific \"computational and memory cost\" comparison data between the 2-view and 3-view settings. This lack of quantitative analysis of the trade-off makes it difficult for reviewers to assess whether the 2-view setting is the optimal choice."}, "questions": {"value": "- Clarification on \"Universality\": 3D pre-training requires RGB-D, while 2D pre-training only requires RGB. Does this mean $GS^3$ cannot pre-train a 3D encoder from RGB images alone (without depth)? Are these two pipelines trained completely separately? If so, this seems to contradict the claim of a \"universal framework,\" please clarify.\n- Details on the NeRF Baseline: Please provide details on the specific architecture and training setup of the \"NeRF-based pre-training\" used for comparison in the experiments. Is it a reproduction of Ponder or another SOTA method?\n- On Depth Ambiguity in 2D Pre-training: The 2D pre-training section mentions using the \"same depth-based GS generation strategy as in 3D pre-training\" to predict depth from RGB-only inputs. This seems to contradict the motivation mentioned in the introduction that \"(NeRF) introduces ambiguity in the absence of accurate depth maps.\" Why doesn't the $GS^3$ depth generation strategy encounter the same ambiguity problem? Is it because the cross-view encoding provided by the Epipolar Transformer already resolves this issue?\n- Limitation on View Count: The method relies on two-view observations. The ablation study even shows three views are better. Does this mean $GS^3$ cannot handle single-view data? Given that many datasets are single-view, would this limit the framework's applicability?\n- Supervision in Masked Modeling: The paper mentions that the model uses the unmasked subset (e.g., 50%) to predict scene Gaussians, and then renders the full RGB image for supervision. This implies the model must \"infer\" the GS for the 50% masked region. How does the photometric loss handle these regions generated purely from inference (rather than direct observation)? Could this introduce unstable supervision signals in the early stages of training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FviR3bzApA", "forum": "4UrjpzvAy6", "replyto": "4UrjpzvAy6", "signatures": ["ICLR.cc/2026/Conference/Submission11640/Reviewer_TbfK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11640/Reviewer_TbfK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932358818, "cdate": 1761932358818, "tmdate": 1762922707780, "mdate": 1762922707780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GS3, a self-supervised pre-training framework that unifies 2D and 3D modalities through 3D Gaussian Splatting (GS). The method formulates neural rendering as a pretext task: visual features from either images or point clouds are used to predict scene-level 3D Gaussians, which are then rendered via a differentiable tile-based rasterizer. The training objective combines a photometric reconstruction loss and a LPIPS term, with a masked modeling strategy encouraging robustness and spatial awareness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, The idea of a single SSL framework bridging 2D and 3D domains is appealing and addresses the growing interest in cross-modal pre-training for 3D models.\n\n2, Using 3D Gaussian Splatting for self-supervision is computationally efficient and avoids the heavy volumetric sampling of NeRF-based methods, which is a non-trivial engineering gain.\n\n3, The same photometric and perceptual loss is applied to both image and point-cloud encoders, making the framework structurally coherent and potentially scalable.\n\n4, Results including 3D object detection, image-based 3D detection, semantic/instance segmentation, and scene reconstruction, demonstrating cross-task applicability and consistent performance gains"}, "weaknesses": {"value": "1, The framework largely repackages existing ideas—multi-view feature alignment, rendering-based supervision, and masked modeling—within a Gaussian Splatting pipeline. Beyond replacing NeRF volume rendering with tile-based splatting, no new learning principle or representation insight is introduced. This is an engineering improvement rather than a conceptual advance.\n\n2, The paper does not analyze why predicting and rendering 3D Gaussians leads to better representations than previous method like MAE and contrastive learning. No discussion of inductive bias, information preservation, or representation geometry is provided. Consequently, the claimed “universal” learning paradigm lacks scientific depth.\n\n3, Despite the claim of modality generality, pre-training uses paired RGB-D images with known camera poses. This contradicts the motivation of unposed cross-modal learning and limits real-world applicability.\n\n4, The 2D and 3D branches are trained under different assumptions (e.g., RGB-only vs. RGB-D). The paper does not demonstrate that a single encoder can transfer across modalities; thus the “universal” claim is overstated."}, "questions": {"value": "See the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wdSXnspNbz", "forum": "4UrjpzvAy6", "replyto": "4UrjpzvAy6", "signatures": ["ICLR.cc/2026/Conference/Submission11640/Reviewer_Zz9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11640/Reviewer_Zz9g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058371953, "cdate": 1762058371953, "tmdate": 1762922707242, "mdate": 1762922707242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}