{"id": "P2q6Y7UweV", "number": 6348, "cdate": 1757971464124, "mdate": 1759897920565, "content": {"title": "INSTANT: Compressing Gradients and Activations for Resource-Efficient Training", "abstract": "Deep learning has advanced at an unprecedented pace. This progress has led to a significant increase in its complexity. However, despite extensive research on accelerating inference, training deep models directly within a resource-constrained budget remains a considerable challenge due to its high computational and memory requirements. In this paper, we introduce INSTANT (compressIng gradieNtS and acTivAtions for resource-efficieNt Training), a method designed to address both the computational and the memory bottlenecks when training. INSTANT reduces resource demands during backpropagation by projecting gradients and activations into a low-rank subspace and performing computation within that compressed representation. Experimental results demonstrate that INSTANT achieves a $15\\times$ reduction in computational cost and $32\\times$ reduction in activation memory with negligible impact on model performance. The code will be made publicly available upon the paper's acceptance.", "tldr": "This paper introduces INSTANT, a method for efficient training using low-rank gradient and activation compression.", "keywords": ["Gradient Compression", "Activation Compression", "Resource-Constraint Training"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fab1421b5266397a4ecd93a498daf1bfcf3ffe2.pdf", "supplementary_material": "/attachment/94a1fdf0f828ee6d1d86c82c643ae440ff4b9171.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new distillation technique called grafting, aimed at improving the generalization of autoregressive language models. The grafting strategy integrates sequence trees generated at multiple temperatures into a single distillation target. The paper emphasizes the balance between model compression and generalization, addressing the mode-covering behavior crucial for building more generalizable models. The experimental validation demonstrates the method's effectiveness across several datasets, particularly CIFAR-10 and CIFAR-100."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The grafting strategy is an original approach for balancing model compression and generalization in language model distillation.\n\nExperimental Validation: The method is validated across several datasets, and the results show some potential for improving model performance while reducing computational and memory costs.\n\nClarity: The paper is well-organized and clearly explains the experimental setup and methodology."}, "weaknesses": {"value": "Lack of Comparison with Related Work: While the paper mentions the error accumulation problem in other activation compression methods, it does not provide a sufficient comparison with existing methods like Sakr & Khailany’s ESPACE or Yang et al.’s LBP-WHT in terms of accuracy and performance. Without this comparison, the claims about grafting’s effectiveness remain unclear.\n\nLimited Experimental Setup: The experiments are mainly focused on simple image classification tasks (e.g., CIFAR-10, CIFAR-100). These datasets may not fully demonstrate the method's potential in more complex or real-world scenarios.\n\nUnaddressed Limitations: The paper does not sufficiently explore some of the limitations of the proposed method, including the computational cost of SVD and its scalability to larger models or long sequences."}, "questions": {"value": "Could you provide a more detailed comparison between grafting and existing methods like Sakr & Khailany’s ESPACE or Yang et al.’s LBP-WHT, particularly in terms of accuracy and computational efficiency?\n\nHow do you plan to scale the grafting method for larger models or longer sequences, and what are the potential challenges?\n\nCould you provide further experiments on more complex datasets or real-world tasks to validate the method’s generalizability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BHK1EUWq8O", "forum": "P2q6Y7UweV", "replyto": "P2q6Y7UweV", "signatures": ["ICLR.cc/2026/Conference/Submission6348/Reviewer_9WbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6348/Reviewer_9WbD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709462681, "cdate": 1761709462681, "tmdate": 1762918638972, "mdate": 1762918638972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a compression method called INSTANT. INSTANT is a method that projects the activations and gradients to lower ranks, based of analysis that is done at regular intervals during training. The authors show the benefits of compressing both the gradients and activations during training by showing reduction in FLOPs and training time (in some cases)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is  tackling both memory and computational bottlenecks.  Activation compression for memory saving has been explored previously, but the idea of also compressing the activation gradient ($g_y$) to reduce backpropagation FLOPs is a useful contribution.\n\n2. The authors provide empirical evidence (Fig. 3) to support their intuition that activation gradients are inherently low-rank, justifying their compression approach.\n\n3. NSTANT's  SVD-based approach is data-driven. This allows it to generalize more effectively to other modalities, as demonstrated by its strong performance on NLP tasks (Table 2) where previous works (eg LBP-WHT) struggles.\n\n4. The authors validates the method's usability by reporting significant wall-clock speedups (2x to 12.5x) on resource-constrained edge CPUs."}, "weaknesses": {"value": "I believe, currently this paper's primary weaknesses lie in the evaluation of its computational claims and the transparency of its overhead costs.\n\n**Omission of Wall-Clock Training Time on GPUs:**\n\n1. The paper's main results (Tables 1 & 2) were generated on an NVIDIA V100 GPU but only report FLOPs, not wall-clock training time.\n\na) The authors' justification (Section 4.1) that \"FLOPs... [are] unaffected by implementation details\" is a weak defense. A reduction in FLOPs does not guarantee a proportional reduction in training time, especially on GPUs. Modern deep learning libraries (like cuDNN) are highly optimized for large, dense matrix multiplications, and the low-rank operations introduced by INSTANT may not be as implementation-efficient, thus creating a bottleneck.\n\nb) The authors themselves admit a discrepancy in Appendix I, stating, \"The (12x) time reduction is not comparable to (17x) FLOP reduction,\" even on a CPU. This gap is likely to be even larger on a GPU, and the lack of this data makes it difficult to assess the practical speedup in a typical training environment.\n\n**Cost and Nature of the \"Static\" Subspace:**\n\n2. The term \"static\" subspace (used in the contribution list and abstract) could be misleading. The subspace is not fixed; it is \"periodically\" recalibrated every $N_t$ steps (e.g., $N_t=50$ or $N_t=200$).\n\na) The computational cost of this recalibration (Algorithm 1) appears to be non-trivial and is not accounted for in the reported per-step FLOP savings. This calibration requires running multiple batches and performing SVD for every layer being compressed. This overhead could significantly diminish the overall wall-clock time savings.\n\nb) The method's stability relies on an \"oversampling\" hyperparameter $p$, which is introduced to \"reduce information loss when the core bases change\" between calibrations (Section 3.2, Fig. 6). This suggests the \"static\" assumption is fragile and introduces another sensitive hyperparameter that must be tuned, adding to the method's complexity."}, "questions": {"value": "1. Could the authors please provide the total wall-clock training time (not just backward time) for the main V100 GPU experiments in Tables 1 and 2? This is essential for evaluating the practical speedup of INSTANT in a standard training scenario.\n\n2. How is the computational cost of the periodic calibration step (Algorithm 1) factored into the total FLOPs reported? Could you provide an analysis of this overhead, for instance, as a percentage of total training time or FLOPs?\n\n3. Given that the subspace is recalibrated every $N_t$ steps, would \"periodically updated\" or \"cached\" be a more accurate description than \"static\"?\n\n4. The oversampling parameter $p$ appears critical for performance (Fig. 6, Fig. 8). How sensitive is the method to the choice of $p$ and the calibration frequency $N_t$? Does this introduce a significant hyperparameter tuning burden?\n\n5. Following up on the 17x FLOP vs. 12x time gap noted in Appendix I: What is the authors' hypothesis for the performance gap on a V100, where highly optimized dense matrix multiplication kernels are the standard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3SMPnWuqvm", "forum": "P2q6Y7UweV", "replyto": "P2q6Y7UweV", "signatures": ["ICLR.cc/2026/Conference/Submission6348/Reviewer_jz4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6348/Reviewer_jz4Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859653453, "cdate": 1761859653453, "tmdate": 1762918638308, "mdate": 1762918638308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes INSTANT, a training-time compression method that projects activations and gradients into low-rank subspaces and performs backpropagation in those compressed representations. The authors describe (i) an SVD-based calibration to build per-layer projectors, (ii) projection/truncation with an energy threshold + oversampling, and (iii) a low-rank backward algorithm. Experiments show large reductions in activation memory and FLOPs across Transformer and CNN models with small accuracy drops."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "* Solid empirical evaluation across modalities (vision & NLP) and architectures (Transformers, CNNs).\n* Clear exposition, reproducibility-minded appendices and pseudocode.\n* Practical relevance: reduces activation memory and backward FLOPs as demonstrated in their experiments, with sensible ablations (oversampling, calibration frequency, rank choices).\n* Sound theoretical guarantee for stable low-rank training. Their analysis shows that SVD-based projections minimize reconstruction error and that gradient approximation error remains bounded through depth, vanishing as the retained energy ε → 1.\n* Includes deployment-relevant experiments (edge device / Raspberry Pi)."}, "weaknesses": {"value": "1. **Severe overlap with prior work (CompAct, NAACL 2025, publicly available on arXiv since Oct 2024):**   \nConceptually highly similar and structurally parallel to CompAct [1], differing mainly in projection construction and calibration choices, without mentioning CompAct at all, despite the latter being published and publicly available months before ICLR submission. \nBoth papers:    \n* Compress activations via low-rank projections during the forward pass.\n* Compute gradients in the compressed subspace and decompress for weight updates.\n* Aim to reduce memory and optimizer overhead jointly.\n* Demonstrate scaling benefits on LLaMA and BERT-like models.\nThe algorithmic structure of INSTANT (forward compression → compressed backward → decompression for update) matches CompAct’s Algorithms 1–3. The overlap extends to terminology (“projected activations,” “reduced optimizer states”), theoretical justification, and empirical evaluation. \n\n2. **Novelty and contribution are overstated:**   \nWhile INSTANT adds implementation refinements, such as calibration-based rank selection and changes the choice of projection matrix, these are engineering extensions rather than conceptual advances. The claim of being the “first to jointly compress activations and gradients” is false, due to the overlap with prior work.\n\n3. **Lack of comparison:**   \nThe experiments do not compare with the most appropriate relevant works like GaLore [2] or any of the myriad of works that followed it (VeLORA [3] ,Grass [4] ,WeLore [5]...). Reported baselines are insufficient to establish novelty or superiority.\n\n\nReferences\n* [1] CompAct: Compressed Activations for Memory‑Efficient LLM Training – Shamshoum et al., NAACL 2025, arXiv:2410.15352v1. \n* [2] GaLore: Memory‑Efficient LLM Training by Gradient Low‑Rank Projection – Zhao et al., ICML 2024, arXiv:2403.03507.\n* [3] VeLORA: Memory Efficient Training using Rank‑1 Sub‑space Activations – Miles et al., NeurIPS 2024 Poster, algorithm for compressing activations into 1-D subspace. \n* [4] Grass: Compute Efficient Low‑Memory LLM Training with Structured Sparse Gradients – Muhamed et al., arXiv:2406.17660.\n* [5] WeLore: Weight Low‑Rank Projection for Memory‑Efficient Fine‑Tuning – Jaiswal et al., ICLR 2025, arXiv:2407.11239."}, "questions": {"value": "* Were the authors aware of CompAct (NAACL 2025) at submission time?\n* What specific conceptual or methodological innovations distinguish INSTANT from CompAct beyond the choice of projection matrix?\n* Can you provide a direct empirical comparison to CompAct under matched conditions (e.g., same model, rank, dataset)?\n* How do the claimed FLOP reductions compare to a random projection baseline?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The paper omits citation to a highly similar prior work (CompAct), raising ethical concerns about the faithfulness of its novelty claims and proper credit assignment. Merely googling “LLM activation compression” shows CompAct in the top results, so it is unlikely the paper was missed unintentionally. Despite strong experiments and presentation, if the authors don’t provide a thorough and experimental comparison with this highly overlapping work, I will keep recommending a strong reject."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "d1KApzSLwv", "forum": "P2q6Y7UweV", "replyto": "P2q6Y7UweV", "signatures": ["ICLR.cc/2026/Conference/Submission6348/Reviewer_FBNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6348/Reviewer_FBNs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926336597, "cdate": 1761926336597, "tmdate": 1762918637176, "mdate": 1762918637176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}