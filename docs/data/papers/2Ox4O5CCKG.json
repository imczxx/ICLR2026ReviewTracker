{"id": "2Ox4O5CCKG", "number": 5198, "cdate": 1757864532163, "mdate": 1763636976308, "content": {"title": "Context-level Language Modeling by Learning Predictive Context Embeddings", "abstract": "Next-token prediction (NTP) is the cornerstone of modern large language models (LLMs) pretraining, driving their unprecedented capabilities in text generation, reasoning, and instruction following. However, the token-level prediction limits the model's capacity to capture higher-level semantic structures and long-range contextual relationships. To overcome this limitation, we introduce \\textbf{ContextLM}, a framework that augments standard pretraining with an inherent \\textbf{next-context prediction} objective. This mechanism trains the model to learn predictive representations of multi-token contexts, leveraging error signals derived from future token chunks. Crucially, ContextLM achieves this enhancement while remaining fully compatible with the standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity). Extensive experiments on the GPT2 and Pythia model families, scaled up to $1.5$B parameters, show that ContextLM delivers consistent improvements in both perplexity and downstream task performance. Our analysis indicates that next-context prediction provides a scalable and efficient pathway to stronger language modeling, yielding better long-range coherence and more effective attention allocation with minimal computational overhead.", "tldr": "We propose ContextLM, a scalable pretraining framework that enhances next-token prediction with next-context prediction, improving perplexity and downstream task performance.", "keywords": ["Next-context Prediction", "Predictive Context Embedding", "Chunk-level Supervision"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92507b0879b46a1996271d9c11904106529c1fe6.pdf", "supplementary_material": "/attachment/a00f05914ae35d0b4832a8b9a6ba98e86a33cefd.zip"}, "replies": [{"content": {"summary": {"value": "ContextLM inserts a dedicated Context Predictor to process high-level contextual information, which is then broadcast and fused into the original token decoding process. This mechanism significantly enhances the model's ability to capture long-range dependencies, leading to consistent performance improvements across various downstream tasks compared to the original model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes integrating high-level context into the model's forward pass, akin to a global residual connection.\n\n2. The results, especially those shown in Table 5, are compelling. ContextLM consistently outperforms GPT2 across different model scales and across a wide downstream NLP tasks.\n\n3. ContextLM is fully compatible with the standard autoregressive architecture. It means the community can easily adopt this technique."}, "weaknesses": {"value": "1. Where exactly is the Context Predictor integrated into the model? The paper doesn't specify if it's in the shallow or deep layers. It would be valuable to include an ablation study on how its placement (early vs. late layers) impacts overall performance.\n\n2. The current comparison between ContextLM and the vanilla model is not completely fair, as ContextLM includes two extra Context Predictor layers (and thus more parameters/computation). The authors should compare ContextLM against a vanilla model that has the same total number of layers.\n\n3. Ablation Contradiction: The core benefit is attributed to \"high-level context information fusion.\" However, the ablation study on Chunk Size (Section 3.5.1) is confusing: performance gets worse as the chunk size increases, and the best result is with a chunk size of 2. This seems to contradict the idea of fusing higher-level or longer-range context. More explanation or evidence is needed to reconcile this finding.\n\n4. Mandatory Fusion: ContextLM forces every token to fuse with the high-level context information. Is it possible that some tokens don't need this complex, high-level context? Does forcing this fusion sometimes hurt performance or introduce unnecessary noise? The authors may discuss or investigate potential negative side effects of this mandatory fusion mechanism."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nwIr326mgI", "forum": "2Ox4O5CCKG", "replyto": "2Ox4O5CCKG", "signatures": ["ICLR.cc/2026/Conference/Submission5198/Reviewer_jdvC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5198/Reviewer_jdvC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761037064251, "cdate": 1761037064251, "tmdate": 1762917941380, "mdate": 1762917941380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ContextLM, which augments standard next-token prediction (NTP) with context-level prediction. By introducing a trained context predictor, the model captures higher-level semantic structures beyond token dependencies. Experiments are conducted with GPT-2 and Pythia across perplexity and downstream tasks. The proposed module is lightweight, introducing minimal computational overhead while improving long-range coherence.\n\nKey Reasons: \n1. The proposed method lacks sufficient theoretical justification.\n2. The experiments are constrained to relatively small-scale language models without adequate baseline comparisons.\n\nSupporting Arguments:\n\nThe context predictor is a simple yet effective component that appears to enhance representation quality without interfering with the model’s autoregressive nature. However, the theoretical explanation is insufficiently detailed and lacks derivation or analysis. The experimental setup does not include larger LLMs or comparisons with existing hierarchical approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of predicting context embeddings is novel and well-motivated. It effectively bridges the gap between token-level modeling and high-level semantic abstraction.\nThe proposed method is a plug-and-play module that incurs relatively small computational overhead.\nExperimental results show consistent gains with minimal FLOP and memory cost, suggesting good potential for real-world adoption."}, "weaknesses": {"value": "While the idea is intuitive, the paper lacks a formal analysis explaining why context embedding prediction enhances token-level modeling. Since the training objective remains NTP, the mechanism of improvement warrants deeper theoretical justification.\nAll experiments are limited to models with up to 1.5B parameters. It remains unclear whether the observed gains generalize to larger-scale models.\nComparisons with other relevant baseline methods are missing and should be included to strengthen the empirical evaluation."}, "questions": {"value": "1. Can you provide a formal justification (e.g., information-theoretic or gradient-flow-based) for why context embedding prediction improves token-level modeling?\n2. Can you extend your experiments to larger-scale LLMs (e.g., 7B or 13B parameters) to test scalability?\n3. Can you include comparisons with other hierarchical or multi-level modeling baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UdvyInq6Qc", "forum": "2Ox4O5CCKG", "replyto": "2Ox4O5CCKG", "signatures": ["ICLR.cc/2026/Conference/Submission5198/Reviewer_mucv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5198/Reviewer_mucv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532354093, "cdate": 1761532354093, "tmdate": 1762917940707, "mdate": 1762917940707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework which aggregates the context by adding a 2-layer module whose output is concatenated with the original embedding, which are sent to the base decoder for predicting the final next-token. ContextLM claims to improve the quality of the NTP objective by incorporating more fine-grained and context-level information. The performance of models up to 1.5B are tested on perplexity and downstream performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation for this work is very sound, with a simple framework to support finetuning on top of existing standard transformer architectures. \n2. Both downstream and perplexity are calculated. \n3. Attempted models up to 1.5B, which spans a lot of sizes for completeness under the small model regions. \n4. Ablate on different aspects such as the choice of chunk size, length extension, and attention visualization."}, "weaknesses": {"value": "1. The baseline should ideally be finetuned with the same data for fairness even though the data used for finetuning might be the same as those used for training Pythia and GPT2. \n2. For different model sizes, the extra parameters introduced should be taken into account, especially for smaller size models, as people can expect more dramatic improvements for introducing two extra layers for 70M compared to 1.4B. This is also semi-indicated by the relative performance gain since bigger models might get marginal benefits with the extra parameters. \n3. The best way to showcase the benefits of aggregating the context is to add two extra layers of the same configuration as a baseline and finetune the model in the same way as finetuning the ContextLM. Without this, it is really hard to understand where the gains come from. \n4. The cost to perform finetuning should also be mentioned and compared carefully especially for larger model sizes. (e.g. how much compute should we expect to spend for an increase of 0.3 average score on Pythia-1.4B downstream tasks). \nThe biggest problem reviewer thinks resides in the ablation studies, where the authors claim that for smaller chunk sizes, the quality is best preserved (e.g. w=2). This seems to the reviewer more of a strong evidence for the exact opposite argument that aggregating less context is more beneficial and the main improvement comes from extra parameters instead. This might be counter-argued if the authors train a separate model with 2 added normal layers (which will be equivalent to w = 1) as mentioned above. Only when a decently large w provides consistent gains, the claimed argument is convincing. \n5. Should cite the following highly related works and compare them: \n     - LCM: https://arxiv.org/abs/2412.08821 \n     - HAMBurger: https://arxiv.org/abs/2505.20438 \n     - BLT: https://arxiv.org/abs/2412.09871"}, "questions": {"value": "Mentioned in above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qEW47yFPg6", "forum": "2Ox4O5CCKG", "replyto": "2Ox4O5CCKG", "signatures": ["ICLR.cc/2026/Conference/Submission5198/Reviewer_P52A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5198/Reviewer_P52A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643181237, "cdate": 1761643181237, "tmdate": 1762917940080, "mdate": 1762917940080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores a context prediction in additional to the traditional next token prediction objective. The goal of the objective to learn context-level representation, which leverages a Context Predictor module. During training, each chunk of w tokens are pooled into one representation and the model is trained to predict the representation of the next chunk at every step. The experiments use GPT2 and Pythia style architectures and pre-train from scratch. The results show improvements over traditional next token prediction on perplexity and downstream tasks. The paper also include additional experiments on instruction-following and long-context."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel approach to augment existing language model training through chunk prediction. The results on the upstream perplexity is reasonably strong. The analysis are rather comprehensive with models of different families and sizes. \nThe ablation study on the architectural components also support the design choices."}, "weaknesses": {"value": "The paper is missing discussions on decoding in the main text: since the decoder takes the concatenation of hidden states and contexts, and the decoder uses causal attention, would that require recomputing the KV cache for the initial chunk representations when a new hidden state is prepended to it? That is, when adding h_T before c_init in the concatenated representation, the c_init would need to recalculate its KV after attending to this new hidden state h_T, which results in additional compute compared to traditional transformers.\n\nIn terms of the results, the improvements on downstream tasks at the larger scales appear to be marginal, with only 0.3 to 0.5 improvements compared to the NTP objective. \nWhile the other experiments on instruction-following and long-context extension are nice, the benchmarks are relatively outdated: MTBench is not commonly used whereas results on AlpacaEval, ArenaHard, and/or WildBench would be more convincing, and 2048 context window is not typically considered to be long-context in the community anymore, and would require evaluation beyond loss (datasets like NIAH, LongBench, HELMET are more convincing).\n\nIn terms of the presentation, the paper could benefit from additional clarity when describing the architecture and how it’s different from the traditional transformers architecture. For example, in figure 2, it’s unclear if the architecture shown is just one layer in the overall model (i.e., one context predictor) or if it’s repeated for however many layers. Figure 2 clarify this by showing the difference with traditional transformers. Figure 2 also doesn’t show clearly that the decoder takes both the hidden states and the contexts as inputs.\n\nThe paper could also benefit from more empirical comparisons on related works, such as multi-token prediction and others mentioned in related work. Only comparing to NTP brings questions on how much improvement ContextLM brings over existing approaches.\n\nAlso note that the submission exceeds the 9-page limit."}, "questions": {"value": "Is the chunk prediction used for instruction tuning as well?\n\nHow much additional compute and memory cost does ContextLM incur during decoding? \n\nFor the results shown in Table 1 and Table 2, how many tokens/FLOPS are used for each model?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Exceed page limit"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ed3OkUDBBt", "forum": "2Ox4O5CCKG", "replyto": "2Ox4O5CCKG", "signatures": ["ICLR.cc/2026/Conference/Submission5198/Reviewer_HU2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5198/Reviewer_HU2z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953396295, "cdate": 1761953396295, "tmdate": 1762917939782, "mdate": 1762917939782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}