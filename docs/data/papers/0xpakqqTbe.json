{"id": "0xpakqqTbe", "number": 22726, "cdate": 1758334812604, "mdate": 1759896850273, "content": {"title": "RExBench: Can coding agents autonomously implement AI research extensions?", "abstract": "Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of realistic extensions of 12 research papers that aim to investigate novel research hypotheses. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate 13 LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents fail to autonomously implement the majority of the extensions, with the best agent at around 31% success rate. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 48%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance. Based on analyses of prominent failure modes, we put forward actionable short- and long-horizon recommendations for future research coding agent development.", "tldr": "We introduce RExBench, a benchmark for evaluating autonomous research extension implementation capabilities of coding agents.", "keywords": ["research agents", "LLM agents", "AI for science", "coding benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10e487aaca41d9686334a2e14bacd9400952ff71.pdf", "supplementary_material": "/attachment/a5b899e027a871b69f1e198b90b3bb12e19f56e3.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to benchmark the capabilities of LLM agents for modification-based coding implementation with specific focus on NLP and ML. Specifically, this paper proposes REXBench, which takes the original paper, the corresponding codebase, an instruction as the inputs, and then asks the agents to modify the codebase to implement the instruction. Automated evaluation metrics are proposed, i.e., by checking whether the results fall within a reasonable range."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The writing is of good quality.\n- Several LLMs and LLM agents are discussed in empirical results."}, "weaknesses": {"value": "- From my perspective, this would not be an impactful benchmark for the community, due to several shortcomings: 1) limited number of tasks; 2) computational requirements, considering 8/12 tasks require A100; 3) the evaluation metric is unreliable: the agent would be rewarded as long as the execution results fall within a preset range of values, which is a quite loose evaluation metric.\n- As this paper only proposes 12 tasks. It would be better the authors can provide all the task descriptions of REXBench."}, "questions": {"value": "From my perspective, the proposed benchmark is of limited significance, and cannot benefit the future research of the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qrsWbhIAka", "forum": "0xpakqqTbe", "replyto": "0xpakqqTbe", "signatures": ["ICLR.cc/2026/Conference/Submission22726/Reviewer_E5Qr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22726/Reviewer_E5Qr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760713438860, "cdate": 1760713438860, "tmdate": 1762942359848, "mdate": 1762942359848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RExBench, a benchmark designed to evaluate the capability of LLM-based coding agents to autonomously implement research extensions in AI/NLP/ML. The benchmark consists of 12 tasks based on recently published papers, where agents must implement specified extensions starting from an existing codebase. The authors evaluate 13 agent configurations across three frameworks (aider, Claude Code, OpenHands) with various LLM backbones. Results show that even the best agents (OpenHands + Claude 4 Sonnet/GPT-5) achieve only ~31% success rate without hints, reaching ~47% with detailed hints. The paper provides extensive error analysis and recommendations for future agent development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Research extension is a critical capability for autonomous research agents, distinct from replication\n2. Novel implementations stored privately is a major strength over benchmarks like PaperBench\n3. 13 agent configurations, 3 hint levels, detailed error taxonomy\n4. VM-based evaluation with controlled execution environments ensures reproducibility\n5. Distinction between explicit/implicit errors, over-editing observations, overthinking issues\n6. Both short-term (scratchpads, repair mechanisms) and long-term (verification, context handling)"}, "weaknesses": {"value": "1. The work lacks a human baseline. It would be insightful to see how PhD students or domain experts perform on the same tasks. Similarly, there are no simpler non-agentic baselines to show what traditional systems could achieve. There’s also no direct comparison between agent-generated and human-written code, especially in terms of readability, maintainability, or long-term usability — all of which matter for research automation.\n\n2. The experimental design has several weaknesses. There’s no statistical significance testing, even though the decoding process involves randomness. Inter-annotator agreement on the gold solutions isn’t reported, so we don’t know how consistent the labeling process was. The hint design also feels somewhat ad hoc — it’s unclear how the three hint levels were calibrated. Finally, the temperature setting differs between models, with some using 0.7 and others using defaults, which introduces inconsistency in evaluation.\n\n3. Methodologically, the benchmark framework follows a fairly standard design. The main novelty lies in applying it to “research extension” tasks, rather than in introducing new benchmarking techniques. While the infrastructure is robust and well-documented, it doesn’t really bring conceptual innovation. The work is strong in execution but not particularly creative in method.\n\n4. The authors themselves note that their instructions are much clearer and more informative than what a real researcher would encounter. This makes the setup somewhat artificial. Real research usually involves a lot of ambiguity, trial and error, and iterative exploration — all of which are missing here. The tasks also seem too narrowly defined and well-scoped, failing to capture the open-ended, exploratory nature of genuine research projects.\n\n5. Some findings are left unexplained. For example, why do hints sometimes *reduce* performance, as seen in Othello and Tree-of-Thoughts tasks? There’s little analysis connecting specific task features to failure modes, and minimal discussion on when these agents should or shouldn’t be applied. The regression analysis in Figure 5 also seems underpowered, given that it’s based on only twelve data points. These gaps limit the depth of insight."}, "questions": {"value": "1. How do PhD students perform on these tasks? What's the time/success rate comparison?\n2. Did author measure inter-annotator agreement on gold implementations? Could multiple valid solutions exist?\n3. How were the two hint levels designed? Was there any user study or pilot testing?\n4. What percentage of failed attempts were close to success (within 1-2 bugs)? Could authors characterize the \"distance to success\"?\n5.  Were there cases where agents found valid alternative implementations that didn't match authorsr gold numerical output but were scientifically sound?\n6. Can authors provide objective difficulty metrics beyond lines of code (e.g., cyclomatic complexity, semantic changes required)?\n7. The observation that reasoning models \"overthink\" is interesting—did authors try adjusting their reasoning effort parameters?\n8. The authors mention agents make \"unrequested modifications\"—could instruction tuning on \"minimal edits\" help?\n9. What's the cost-effectiveness compared to hiring a research assistant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6PRAgmCe3u", "forum": "0xpakqqTbe", "replyto": "0xpakqqTbe", "signatures": ["ICLR.cc/2026/Conference/Submission22726/Reviewer_YsPx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22726/Reviewer_YsPx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771508841, "cdate": 1761771508841, "tmdate": 1762942359580, "mdate": 1762942359580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark REXBench which is designed to evaluate whether LLM agents can implement research extensions—i.e., modifications or follow-up experiments extending existing ML/NLP research papers. The benchmark includes 12 real research papers with corresponding codebases and expert-written instructions describing realistic extensions (e.g., changing models, datasets, or algorithms).  The experimental results show that current agents are still short of being able to handle realistic research extension tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a novel problem formulation that tests agents’ ability to extend scientific research, an important while underexplored problem. The benchmark is well designed, including containerized evaluation, de-contamination, etc. The experiments are thorough, using various evaluation metrics and also including cost/time study and an error analysis."}, "weaknesses": {"value": "Although the problem is novel, it is somewhat too niche. In addition, the scale and scope of the benchmark are very limited — it contains only 12 papers and covers only the NLP/ML domain. Therefore, it does not sufficiently evaluate the model’s capability in research extension."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zTGlj5AxDF", "forum": "0xpakqqTbe", "replyto": "0xpakqqTbe", "signatures": ["ICLR.cc/2026/Conference/Submission22726/Reviewer_6w6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22726/Reviewer_6w6V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948849447, "cdate": 1761948849447, "tmdate": 1762942359346, "mdate": 1762942359346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propsoes a new benchmark RExBench which can evaluate the code agents for incremental research ideas.The benchmark includes 12 research papers and their corresponding extensions. Compared to previous tasks, the aim is to automatically assess how well an agent can autonomously implement realistic research extensions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The aim of benchmarking is to automatically assess how well an agent can autonomously implement realistic research extensions. This goal is kind of realistic and interesting. The extension proposal is annotated with the gold edits for the target extension, which is a good resource.\n2. The paper evaluates 13 LLM agents based on this benchmark. The additional hints setting serves as an ablation study for the bottleneck of the pipeline.  The paper also includes both quantitative and qualitative analysis.\n3. The paper includes code. The paper has good visualizations. The paper includes most of the infrastructure pipelines in the main context."}, "weaknesses": {"value": "1. Some details of the benchmark are missing. What is the average extension for each of those 12 papers? The title is kind of misleading. Instead of research extension, the paper is more on the adaptation of the existing code base. \n2. Although errors are discussed in section 5.1, most of them are pretty high-level. The paper also fails to explain the reason behind those errors. The reason behind those errors can help researchers understand the drawbacks of current methods. Sec 5.2 is kind of high-level. If they can be linked to the specific errors, it would be better.\n3. The conclusion seems a bit too long. The paper might need to include a small subset for human evaluation to quantify some of the observations. If section 5 can be supported by more numbers or evidence, it will become stronger."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b6jmyC7hsV", "forum": "0xpakqqTbe", "replyto": "0xpakqqTbe", "signatures": ["ICLR.cc/2026/Conference/Submission22726/Reviewer_DkXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22726/Reviewer_DkXJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982385079, "cdate": 1761982385079, "tmdate": 1762942359086, "mdate": 1762942359086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}