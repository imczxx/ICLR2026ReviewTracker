{"id": "I9f5NdjBvK", "number": 13250, "cdate": 1758215657225, "mdate": 1759897452637, "content": {"title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "abstract": "Speculative decoding (SD), where a small draft model is employed to propose *draft* tokens in advance, and then the target model validates them in parallel, has emerged as a promising technique for LLM inference acceleration. Many endeavors to improve SD are to eliminate the need for a draft model and generate draft tokens in a retrieval-based manner in order to further alleviate the drafting overhead and significantly reduce the difficulty in deployment and applications. However, retrieval-based SD relies on a matching paradigm to retrieval the most relevant reference as the draft tokens, where these methods often fail to find matched and accurate draft tokens. To address this, we propose *LogitSpec* to effectively expand the retrieval range and find the most relevant reference as drafts. Our *LogitSpec* is motivated by the observation that the logit of the last token can not only predict **the next token**, but also speculate **the next next token**. Specifically, *LogitSpec* generates draft tokens in two steps: (1) utilizing the last logit to speculate the next next token; (2) retrieving relevant reference for both the next token and the next next token. *LogitSpec* is training-free and plug-and-play, which can be easily integrated into existing LLM inference frameworks. Extensive experiments on a wide range of text generation benchmarks demonstrate that *LogitSpec* can achieve up to 2.61$\\times$ speedup and 3.28 mean accepted tokens per decoding step.", "tldr": "", "keywords": ["speculative decoding", "inference acceleration", "large language models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d15ee8ffc11d0cfeaa132194e393293d35cc853.pdf", "supplementary_material": "/attachment/5b0b2a3174c88d306d2781f83fcd390e45ce8650.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents LogitSpec, which combines logit-based next next token prediction with n-gran-retrieval-based speculative decoding. Experiments with several models (mainly Vicuna) show that LogitSpec leads to higher speedup than other n-gran-retrieval-based speculative decoding methods such as REST and PLD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Speculative decoding is an important topic in the current LLM literature. n-gran-retrieval-based speculative decoding is plug-and-play, making it widely applicable."}, "weaknesses": {"value": "**Academic Integrity**\n\nThe authors claim to have evaluated Qwen3 models (Line 322). Yet Line 348 states that transformers version 4.37.1 is used. I, for one, know that Qwen3 requires a transformers version of at least 4.51.0. This raises concerns about academic integrity, LLM usage, and questions about whether other parameters or even results in the paper are real.\n\n**Novelty**\n\nThe paper's novelty is limited. It's a simple combination of two existing techniques: future token prediction (https://arxiv.org/abs/2311.04897) and n-gran-retrieval-based speculative decoding (https://arxiv.org/abs/2402.13720, https://arxiv.org/abs/2411.04975, https://arxiv.org/abs/2411.10666).\n\nMoreover, future token prediction is closely related to multi-token prediction, a technique that has already been employed at scale in industrial-grade pretrained models (https://arxiv.org/abs/2404.19737,https://arxiv.org/abs/2412.19437). The current paper lacks a comparison with such methods.\n\n**Theoretical Justification**\n\nThe motivation in Section 4.1 is based solely on empirical observation. Is there any theoretical analysis?\n\n**Implementation**\n\nAppendix D1 claims to reduce the complexity of retrieval to $O(n+k)$. How exactly is this achieved?\n\n**Experimental Setting**\n\nVicuna is a rather old model and should not be used for the main results and analysis in the paper. More recent models (Qwen3 and Llama3) and currently commonly used benchmarks (MATH, AIME) should be analyzed in the paper instead to demonstrate the practical value of the proposed method. I also wonder why the authors chose to omit the results with REST, PLD, and Lookahead for these two advanced models.\n\nOnly experiments with greedy decoding are provided. The effectiveness of the proposed method in sampling scenarios is unknown."}, "questions": {"value": "As mentioned before, I would like the authors to answer:\n\n- Is there any theoretical justification for the observation in Section 4.1?\n- The implementation details of Appendix D1.\n- Why are other comparison methods omitted for the two most widely used models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XvMI1BU93a", "forum": "I9f5NdjBvK", "replyto": "I9f5NdjBvK", "signatures": ["ICLR.cc/2026/Conference/Submission13250/Reviewer_jwo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13250/Reviewer_jwo1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760716316688, "cdate": 1760716316688, "tmdate": 1762923930917, "mdate": 1762923930917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LogitSpec to effectively expand the retrieval range and find the most relevant reference as drafts. They observe that the logit of the last token can not only predict the next token, but also speculate the next next token. Specifically, they utilize the last logit to speculate the next next token and retrieve relevant reference for both the next token and the next next token. Experimental results show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to understand.\n\nThe method is technically sound."}, "weaknesses": {"value": "1.Temperature=1 senarios. LogitSpec relies on the top-k logits of the current token to speculate the next-next token candidates. This design is highly sensitive to temperature: T = 0 (greedy): logits are sharp, top-k are almost the most likely tokens, so speculation accuracy is maximized. But as T > 0 (sampling), logits are smoothed, top-k quickly fill with low-probability noise tokens. All reported results are obtained only under T = 0, the paper provides no data for T > 0 and explicitly notes that last-logit prediction becomes “less reliable” at higher temperatures. \n \n2. The proposed Tree Attention structure used for parallel verification is not currently supported by mainstream inference frameworks such as vLLM. In real-world deployment, if the SPD (speculative decoding) method cannot be integrated with vLLM, its practical utility would be significantly limited. The authors claim that LogitSpec is \"plug-and-play\" and can be easily integrated into existing frameworks, but no implementation details or results on vLLM are provided. I would encourage the authors to discuss or demonstrate how LogitSpec could be incorporated into vLLM (which already supports PLD) and to report the acceleration ratio when combined with vLLM for a fair comparison with PLD. \n \n3. The core observation that \"the last logit can speculate the next-next token\" is purely empirical. While the authors conduct several experiments across models, the validation scope remains limited. More comprehensive ablation studies are needed to ensure the proposed phenomenon is not restricted to specific domains or datasets. For instance, do similar improvements hold when predicting the next-next-next token? Are the results consistent across languages with different grammatical structures (e.g., Chinese, German)? Without such analyses, the effectiveness and generality of LogitSpec remain somewhat uncertain."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xmF2KTJV1P", "forum": "I9f5NdjBvK", "replyto": "I9f5NdjBvK", "signatures": ["ICLR.cc/2026/Conference/Submission13250/Reviewer_9qe6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13250/Reviewer_9qe6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622876949, "cdate": 1761622876949, "tmdate": 1762923930556, "mdate": 1762923930556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LogitSpec to predict the next next token using the last logits, aiming to achieve a speedup in retrieval-based SD by leveraging the potential of the top-k candidates from the last logits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents a clearly written narrative and well-structured exposition that is easy to follow.\n2. This paper proposes LogitSpec to predict the next next token using the last logits, aiming to achieve a speedup in retrieval-based SD by leveraging the potential of the top-k candidates from the last logits."}, "weaknesses": {"value": "1. I think the statement “a larger m-gram leads to more precise matches but lower match probabilities” would be clearer with a concrete example to illustrate the difference between being matched and being accurate in retrieval-based SD.\n\n2. I suggest explicitly claiming—e.g., in §4.2—the distinction between this paper’s draft tree and common tree verification. Prior trees are built to compensate for a weak draft model by amplifying diversity at the current decoding step, whereas your tree is constructed with only the target model, treating the current prediction as ground truth to anticipate subsequent steps.\n\n3. In §4.2 I don’t quite understand the claim: “if the rank of the next-next token is below 8, we preserve 4 tokens; if the rank of the next-next token is below 32, we preserve 3 tokens; otherwise we only preserve the speculated next-next token itself.” Does “rank” mean the position of the next-next token within the top-K list (K by default 60)? Also, in retrieval, the m-gram length m is defaulted to 3—so why are there cases where 4 tokens are preserved?\n\n4. A major concern is that reporting results on the relatively outdated Vicuna series as the main table seems suboptimal, even if some newer-model results are provided in the appendix."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G7fLNVTvxA", "forum": "I9f5NdjBvK", "replyto": "I9f5NdjBvK", "signatures": ["ICLR.cc/2026/Conference/Submission13250/Reviewer_zWrq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13250/Reviewer_zWrq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796479244, "cdate": 1761796479244, "tmdate": 1762923929823, "mdate": 1762923929823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LogitSpec, which utilizes the top-k candidates in the logit of the last token as an extra content of drafts to assist the retrieval-based speculative decoding (SD). Concretely, it uses the top-60 tokens in the last token logits as the \"next next token\" speculation and conducts retrieval for m-gram spans with both the next token and the \"next next token\". Experiments demonstrate that LogitSpec can achieve up to 2.61x speedup and 3.28 mean accepted tokens per decoding step."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes a new perspective to improve retrieval-based speculative decoding by utilizing the top-k tokens in the last token logits as an additional draft source.\n2. The authors conduct comprehensive experiments on a wide range of text generation benchmarks. Experiments demonstrate that LogitSpec can achieve up to 2.61x speedup and 3.28 mean accepted tokens per decoding step.\n3. The paper also provides an analysis of module contributions inLogitSpec, which clearly demonstrates that the last logit module contributes to the speedup improvements of SD.\n4. This manuscript is well-written with clear demonstrations."}, "weaknesses": {"value": "1. **The necessity of next next token**: My major concern lies in the necessity of introducing the \"next next token\" as an extra retrieval source. \n   - Firstly, most retrieval-based SD methods consider a context window of generated tokens as the source spans, generally 3 or 5. In this view, it seems that introducing the \"next next token\" merely increases the context window length by 1. However, as the authors noted in Lines 794-799, the parallel consideration of top-60 candidates to construct draft sequences introduces significant extra computational overhead. It is doubtful whether LogitSpec achieves a better efficiency trade-off compared to other retrieval-based SD.\n   - The authors preserve 3 retrieved tokens if the rank of the \"next next token\" is below 32 and only preserve the \"next next token\" itself if its rank exceeds 32. It means that a large part of draft sequences only consider the \"next next token\" without retrieval. I wonder whether the sacrifice of \"retrieval contents\" in the draft sequences is worth it. This heuristic pruning strategy should be further analyzed.\n2. **Stronger baselines are needed:** LogitSpec should be compared with stronger baselines such as Token Recycling [1] and SAM Decoding [2]. According to the third-party leaderboard of Spec-Bench [3], both methods, which are also plug-and-play retrieval-based methods, achieve significant efficiency improvements over Lookahead [4] and PLD. LogitSpec should be compared with these SOTA approaches, or combined with them, to better illustrate its superiority.\n3. The authors claim that \"the MAT of last logit decoding exhibits superior robustness to the task\" in Line 249. However, in Fig. 2(b), the \"last logit\" achieves lower MAT compared to PLD in 4 out of 6 tasks. The wording of \"robustness\" does not reflect the superiority of the \"last logit\".\n4. This manuscript conducts additional results with advanced reasoning models like Qwen-3-8B. I wonder that the detailed implementation is huggingface transformers or vllm.\n\n\n\n[1] Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling. Luo et al. ACL 2025.\n\n[2] SAM Decoding: Speculative Decoding via Suffix Automaton. Hu et al. ACL 2025.\n\n[3] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding. Xia et al. Findings of ACL 2024.\n\n[4] Break the Sequential Dependency of LLM Inference Using Lookahead Decoding. Fu et al. ICML 2024."}, "questions": {"value": "Please refer to the Weaknesses part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kw34KjZW2P", "forum": "I9f5NdjBvK", "replyto": "I9f5NdjBvK", "signatures": ["ICLR.cc/2026/Conference/Submission13250/Reviewer_Zriw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13250/Reviewer_Zriw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818842601, "cdate": 1761818842601, "tmdate": 1762923929481, "mdate": 1762923929481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}