{"id": "qBNrcneFy3", "number": 13185, "cdate": 1758214859046, "mdate": 1759897458214, "content": {"title": "NTK-LoRA: Calibrating Fine-tuned Vision Transformers using Gaussian Processes", "abstract": "Fine-tuning remains essential for adapting foundation models to domains where high precision is required, such as medical imaging or autonomous driving. However, this often leads to overconfident and poorly calibrated models, especially when fine-tuned on small datasets. We propose NTK-LoRA, a simple and effective post-hoc calibration method for fine-tuned Transformer models (e.g., Vision Transformers and LLMs) that leverages the Gaussian process view of neural networks to perform Laplace approximation of the posterior.\nOur method is almost as straightforward to implement as temperature scaling (TS), requires no hyperparameter tuning or deeper expertise, allows incorporating prior knowledge through the choice of GP kernel, achieves better or comparable performance to TS and consistently outperforms Laplace calibration, which in our experiments often fails to improve over the baseline on binary classification.", "tldr": "We introduce a simple post-hoc calibration method for fine-tuned Vision Transformers that leverages the Gaussian process view of the Laplace approximation.", "keywords": ["Calibration", "Uncertainty estimation", "Laplace approximation", "Gaussian processes", "Neural tangent kernel", "Vision Transformers", "LoRA fine-tuning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d971430a85704fae458d97a5ab8d88988afddb44.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a post-hoc calibration method which fits a GP and performs a function-space Laplace approximation over LoRA parameters. They demonstrate that NTK-LoRA outperforms other post-hoc methods like temperature scaling and Laplace with Kronecker-factored covariance over binary classification tasks from CelebA and CUB."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, and the motivations are clearly explained. The background and method section are sufficiently detailed, and the presentation is easy to follow. \n- The proposed method is conceptually intuitive and well-justified . Furthermore, the method is also simple to implement, and the authors demonstrate its implementation using only a few lines of code, making it easily usable by practitioners.\n- NTK-LoRA demonstrates strong empirical performance, outperforming other post-hoc calibration methods such as weight-space Laplace and temperature scaling on the two vision datasets benchmarked."}, "weaknesses": {"value": "- The NTK-LoRA approach has significant limitations for where it can be applied. Because it relies on a GP, the method is computationally feasible only for problems with a moderate number of features and a relatively small number of training examples a small number of training points. Furthermore, the current formulation naively extends only to binary classification. Extending this framework beyond any one of these limitations would require substantial amounts of compute or require other approximations or assumptions that are non-trivial. \n- The empirical evidence is limited to vision transformers and only two image datasets. The paper could be stronger if the method was shown to work across more diverse models and settings, or shown to be effective in domain shifts, etc. Furthermore, although the chosen baselines are commonly used, it may also strengthen the paper to include further comparisons with additional calibration methods.\n- Nit: The tables are also a bit difficult to read, maybe because of the font size? It renders strangely on my pdf reader."}, "questions": {"value": "- Did you perform any ablations to demonstrate how the performance of NTK-LoRA varies as you change different aspects of the problem, such as number of parameters for LoRA, number of fine-tuned points, etc? It would benefit the paper if there were explicit recommendations for the types of problems that this approach should be applied to compared to baselines like temperature scaling."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lKFkPJpnet", "forum": "qBNrcneFy3", "replyto": "qBNrcneFy3", "signatures": ["ICLR.cc/2026/Conference/Submission13185/Reviewer_JEH4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13185/Reviewer_JEH4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928313642, "cdate": 1761928313642, "tmdate": 1762923882713, "mdate": 1762923882713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of miscalibration (overconfidence issue) while finetuning vision transformer models using LoRA based parameter efficient fine tuning. The proposed solution is a post-hoc method that performs Laplace approximation of the posterior distribution. Experiments conducted with binary classification settings demonstrate the effectiveness of the proposed solution. The paper emperically justifies the proposed solution in comparison to linearized Laplace as Gaussian process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles important problem of miscalibration for transformers based fine-tuning for downstream tasks.\n- The solution is simple and does not require to tune hyperparameter."}, "weaknesses": {"value": "- The proposed method is limited to binary classification. \n- [1] also leverages Laplace approximation for fine-tuning LLMs. In introduction, the paper discusses that implementing [1] in practice is challenging. However, [1] proposes Laplace approximation to a larger scale models than in this paper. How is the proposed method superior in practice than [1]? Can the authors clarify? It is suggested to compare with this method directly.\n- The paper does not include a discussion and comparison with recent literature works in calibration that are directly relevant to the proposed work [2,3, 4]. \n- The motivation is for fine-tuning transformer models (including vision and LLMs), but the experiments are limited to vision transformers and Lora as parameter-efficient fine-tuning (PEFT) methods. The other PEFT methods include, but are not limited to prompt tuning, bias tuning, adapter and vera.\n- The experiments are not comprehensive to verify the superiority of the method.\n\nReferences\n\n[1] Yang, Adam X., et al. \"Bayesian Low-rank Adaptation for Large Language Models.\" The Twelfth International Conference on Learning Representations.\n\n[2] Guo, Chuan, et al. \"On calibration of modern neural networks.\" International conference on machine learning. PMLR, 2017.\n\n[3] Pandey, Deep, Spandan Pyakurel, and Qi Yu. \"Be confident in what you know: Bayesian parameter efficient fine-tuning of vision foundation models.\" Advances in Neural Information Processing Systems 37 (2024): 44814-44844.\n\n[4] Chen, Lin, et al. \"Vit-calibrator: Decision stream calibration for vision transformer.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 2. 2024."}, "questions": {"value": "Please refer to weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4K1y5XtKG3", "forum": "qBNrcneFy3", "replyto": "qBNrcneFy3", "signatures": ["ICLR.cc/2026/Conference/Submission13185/Reviewer_PWSA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13185/Reviewer_PWSA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955123358, "cdate": 1761955123358, "tmdate": 1762923882335, "mdate": 1762923882335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Fine-tuning large pretrained Transformer models (e.g., Vision Transformers and LLMs) on small domain-specific datasets often leads to overconfident and poorly calibrated predictions. While post-hoc calibration methods like temperature scaling (TS) are simple, they cannot address complex miscalibrations. This paper proposes NTK-LoRA, a Bayesian calibration method for fine-tuning Transformer models. To perform Bayesian uncertainty quantification, the authors use a Gaussian process view of neural networks and apply a linear Laplace approximation to make posterior computation tractable. The proposed method uses linearized Jacobian features to form the GP Kernel and, as a result, incorporates prior knowledge without requiring any hyperparameter tuning. The paper primarily focuses on solving binary classification tasks, and the given experiments achieve better or comparable performance to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper proposes a novel connection between LoRA fine-tuning with Gaussian Process–based calibration through the NTK perspective. The proposed method is lightweight, requiring minimal code and standard libraries — practical for broad adoption.\n\n2) The evaluations across the two datasets and multiple attributes demonstrate consistent, although small, performance gains. The finding that NTK-LoRA can match full-finetuning performance after a single epoch is quite interesting. Understanding and replicating this result in more complex settings, such as multi-class classification or generation, would be highly impactful."}, "weaknesses": {"value": "1) The experiments are restricted to binary classification on small datasets with ViTs; claims about generality to LLMs or multi-class settings remain unvalidated. It would be good to have some non-binary classification tests. It is unclear how well the kernel Idea generalizes to multiclass settings.\n\n2) The improvements over Temperature Scaling, though consistent, are numerically small in some cases (ECE drops of ~0.01–0.02). The runtime complexity of needing Jacobian evaluations might not be justified. The method’s reliance on per-sample Jacobian computation could become computationally expensive for larger parameter subsets or multi-class models. It would be good to see some results comparing the \n\n3) While the motivation for using NTK-inspired Kernels is interesting, the explanation feels a bit hand-wavy. From a Bayesian perspective, the authors claim that using the Linear Laplace approximation yields an NTK-style formulation with Jacobian features. This idea is not adequately explained. It would be great to have a formal result showing the sequence of assumptions that yield the final Jacobian feature form. The methodology is not adequately motivated."}, "questions": {"value": "Please refer to the Weaknesses section for the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LLYD6qDMBG", "forum": "qBNrcneFy3", "replyto": "qBNrcneFy3", "signatures": ["ICLR.cc/2026/Conference/Submission13185/Reviewer_fVnv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13185/Reviewer_fVnv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972576047, "cdate": 1761972576047, "tmdate": 1762923882017, "mdate": 1762923882017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using Gaussian processes with LoRA Jacobian as a feature vector to calibrate uncertainty during fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The writing is clean and easy to read. The paper combines GP with PEFT tuning methods, providing a calibration measure for output prediction; such a combination is novel."}, "weaknesses": {"value": "1. In the current setting, the LoRA rank is 1. Larger-scale fine-tuning results are missing (e.g. larger rank). It would be beneficial to show the method works with higher LoRA ranks and generalizes to other PEFT tuning methods.\n2. More recent baselines are missing. To incorporate uncertainty for model prediction, Bayesian (last) Layer [1, 3] or inducing uncertainty priors for the last layer during fine-tuning [2] could be less computationally expensive and should be compared with the proposed method.\n\n[1] Variational Bayesian Last Layers\n\n[2] Fine-Tuning with Uncertainty-Aware Priors Makes Vision and Language Foundation Models More Reliable\n\n[3] On Last-Layer Algorithms for Classification: Decoupling Representation from Uncertainty Estimation"}, "questions": {"value": "### Questions\n\n- Line 661: What do the 5–8 random seeds mean?\n- Figure 2: Is this performance on the test set? Could you provide the best epoch chosen by the validation set to ensure the method is robust to the selection criteria?\n- What is the computational cost of NTK-LoRA? Could you provide the time cost? Do we need all layer gradient features to have good uncertainty estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5bJ3SPTqJy", "forum": "qBNrcneFy3", "replyto": "qBNrcneFy3", "signatures": ["ICLR.cc/2026/Conference/Submission13185/Reviewer_UQgj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13185/Reviewer_UQgj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134851377, "cdate": 1762134851377, "tmdate": 1762923881749, "mdate": 1762923881749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}