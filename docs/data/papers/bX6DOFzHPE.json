{"id": "bX6DOFzHPE", "number": 23048, "cdate": 1758338787239, "mdate": 1759896834467, "content": {"title": "Evaluation of Vision Language Models with Item Response Theory", "abstract": "Evaluation of generative AI output is difficult because of the high dimensional nature of the problem space. Accuracy-oriented benchmarks are often used to assess the quality of outputs, but may not provide a complete picture because they do not incorporate the difficulty of items relating to a particular task. We present the use of Item Response Theory (IRT) to evaluate the output of a cohort of Vision Language Models (VLMs) on two different tasks: image caption rating, and visual reading comprehension. As a result, we show how to find meaningful differences between popular state of the art VLMs, promoting meaningful interpretability. IRT can be used in many areas of the ML workflow. We will cover some of the prior work and ways IRT may be used in your research. Our aim is to encourage the adoption of IRT by the ML community as a general tool for evaluation.", "tldr": "Evaluation of Vision Language Models with Item Response Theory", "keywords": ["Item-Response Theory", "Evaluation", "Accessibilty", "Rating", "Comprehension", "Rasch Model", "Wright Map"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fe2975a9edbd169c7d82ca54fb6782311455ce6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a more fine-grained evaluation approach for vision–language models by weighting instances based on their difficulty. Using the Rasch model and Wright Map analyses, the authors introduce an evaluation framework that extends beyond traditional aggregate performance metrics (accuracy,...), which have been proven to be flawed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The paper presents an **innovative and original perspective** on VLM evaluation by explicitly modeling the relationship between item difficulty and model ability, offering a more adaptive way to assess performance.\n\n- It provides two clear and instructive examples of how Wright Maps can be used for evaluating VLMs, serving as a strong demonstration of the potential of IRT-based analysis in this context.\n\n- The study is supported by a robust empirical setup, including a diverse set of VLMs of varying scales and capabilities."}, "weaknesses": {"value": "- While the use cases are interesting, it remains unclear how the proposed methodology can **generalize to other VLM tasks**. Despite the broader claims in the introduction. The authors should better articulate the **practical requirements for applying this method**, such as the need for ordinal human ratings, a sufficient number of annotators, and other dataset characteristics.\n\n- It would be particularly helpful to include a categorization of VLM task types according to their suitability for this kind of evaluation (e.g., easy to apply, moderately adaptable, currently infeasible due to lack of ground truth or rating structure).\n\n- The paper’s impact would be greatly enhanced if the authors provided a tool or codebase to enable large-scale evaluation across multiple tasks.\n\n- As it stands, the conclusions are primarily drawn from a single example setting (image caption rating and visual reading comprehension). The generalizability of the findings would be stronger if the paper included additional examples or replications of the analysis."}, "questions": {"value": "Q1: Was the difficulty of a task estimated solely from human respondents (e.g., based on their agreement), or were model responses also included? Including models might bias the estimation of item difficulty, so clarification on this design choice would be important.\n\nQ2: Has a similar IRT-based methodology been previously applied to LLM evaluation tasks? A discussion of related applications in the language domain could help contextualize the contribution.\n\nQ3: What kinds of tasks are best suited for this type of evaluation, and what are the practical requirements (data type, annotation structure, etc.) to make it feasible?\n\nQ4: Could the authors provide more insight into what makes a task difficult or what differentiates more capable VLMs in their framework?\nFor instance, any intuition on why GPT-4o-Turbo performs particularly well on visual reading comprehension but poorly on image caption rating would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fhASDDI4eY", "forum": "bX6DOFzHPE", "replyto": "bX6DOFzHPE", "signatures": ["ICLR.cc/2026/Conference/Submission23048/Reviewer_QcqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23048/Reviewer_QcqE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473606365, "cdate": 1761473606365, "tmdate": 1762942490124, "mdate": 1762942490124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an IRT-based evaluation for VLMs that jointly models ability and item difficulty on a shared logit scale, visualized via Wright Maps and validated with MNSQ fit. Two case studies reveal which models are strong even on hard items, insights that accuracy-only metrics miss."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a difficulty-aware, interpretable metric.\n\n2. Two case studies are interesting and thus necessitating a difficulty aware evaluation metric."}, "weaknesses": {"value": "1. Lack of scaled evaluation. The central claims rest on two small case studies rather than dataset-scale experiments, and no baseline comparisons are reported (e.g., difficulty-bucket accuracy)\n\n2. Unverified modeling assumptions. Rasch 1PL/PCM presumes unidimensionality and local independence, assumptions that are questionable for multi-skill VLM tasks.\n\n3. Limited external validity. The chosen case studies (caption rating, short reading) may not generalize to grounding, chain-of-thought VQA, or instruction-following, where skills and item structures differ substantially.\n\n4. Ablations on the IRT choice are missing. It’s unclear whether results depend on 1PL vs. 2PL/PCM choices.\n\n5. Using VLMs and a subset of humans as “raters” risks bias."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mbqOoPdZLG", "forum": "bX6DOFzHPE", "replyto": "bX6DOFzHPE", "signatures": ["ICLR.cc/2026/Conference/Submission23048/Reviewer_gKV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23048/Reviewer_gKV4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499404386, "cdate": 1761499404386, "tmdate": 1762942489902, "mdate": 1762942489902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses Item Response Theory (IRT) and Partial Credit Model (PCM) to evaluate VLMs alongside human raters on two tasks: image caption rating and visual reading comprehension. Results show that using partial credit model could better reflect VLMs' abilities compared to accuracy based methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The IRT framing makes model–item comparisons interpretable, and Wright Maps help read where models sit against humans and item thresholds. This supports qualitative insights like the difficulty of mid‑quality captions.\n2. The authors benchmark 10 VLMs and show more insights than accuracy based metrics."}, "weaknesses": {"value": "1. This paper is more like a report paper, however, the results may be incomplete: no standard errors or separation reliability are shown. Without these, ability differences and group comparisons are hard to identify. Additionally, the results can be dependent on the prompting.\n2. The used item pools are too small, and only 2 VLM tasks are involved, making the results and analysis insufficient to draw valuable conclusions.\n3. The evaluated models may be somewhat outdated. Better to include more recent models such as Gemini2.5 and GPT-5. Also it is necessary to test on more benchmarks and datasets."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4qgK2MwA29", "forum": "bX6DOFzHPE", "replyto": "bX6DOFzHPE", "signatures": ["ICLR.cc/2026/Conference/Submission23048/Reviewer_pGyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23048/Reviewer_pGyV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535851606, "cdate": 1761535851606, "tmdate": 1762942489707, "mdate": 1762942489707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that accuracy-oriented benchmarks provide an incomplete picture of model performance because they fail to incorporate the difficulty of items. To address this, the authors propose using Item Response Theory (IRT) to evaluate the output of Vision Language Models (VLMs). IRT allows for the simultaneous measurement of the VLM’s latent \"ability\" and the test item’s \"difficulty\" on the same scale, a method validated across two key tasks: image caption rating and visual reading comprehension."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper reveals that accuracy-oriented benchmarks may provide an incomplete picture of model performance because they fail to incorporate the difficulty of items.\n2. This paper proposes to use Item Response Theory (IRT) to evaluate the output of VLMs, which enhances interpretability."}, "weaknesses": {"value": "1. The paper fails to provide a crucial comparison between the model rankings derived from IRT ability estimates and those from traditional accuracy-oriented metrics. If the correlation is high, the practical value of IRT for ranking models is limited.\n2. The methodology requires highly specialized and manual prompt engineering (e.g., multi-step prompt chains), which significantly hinders the scalability and convenience for general ML workflows.\n3. The analysis is limited to only two specific case studies, which restricts the generalizability of the IRT framework as a general evaluation tool."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gKaJguXQWB", "forum": "bX6DOFzHPE", "replyto": "bX6DOFzHPE", "signatures": ["ICLR.cc/2026/Conference/Submission23048/Reviewer_aWuw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23048/Reviewer_aWuw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706041549, "cdate": 1761706041549, "tmdate": 1762942489170, "mdate": 1762942489170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}