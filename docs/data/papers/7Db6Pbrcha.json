{"id": "7Db6Pbrcha", "number": 2379, "cdate": 1757067299923, "mdate": 1763745542864, "content": {"title": "SHERPA: Fine-tuning Segment Anything Models with Task-relevant Guidance", "abstract": "Segment Anything Models (SAMs) often struggle with certain specialized tasks. A common approach is to fine-tune models with specific task labels, but this often leads to overfitting, introduces model bias and significantly degrades their generalization ability.\nTo overcome these challenges, we propose SHERPA, a novel framework that leverages a smaller SAM to guide the fine-tuning of a larger SAM via task-relevant features. \nSpecifically, we first leverage the Fisher Ratio Separation (FRS) module to separate high task-relevant features and preserve the ability of the large SAM to perform other general tasks.\nThen, the Guiding Feature Extraction (GFE) module is used to extract representative guiding features from the fine-tuned small SAMs.\nWe leverage small SAMs tailored for specific tasks (including natural image segmentation, biomedical image segmentation, and video object segmentation)\nas guidance and then evaluate the SHERPA scheme to fine-tune larger SAM series models.\nOur experiments demonstrate that SHERPA enhances the retention of generalization ability across those diverse tasks, by up to 11.1\\%, and improves specific task performance by up to 2.2\\%.", "tldr": "We propose a fine-tuning approach where a small SAM guides a large SAM, effectively reducing the loss of generalization ability while enhancing fine-tuning performance.", "keywords": ["Guided by the Small Model", "Task-relevant Attention Maps", "Generalization Ability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07492064ca0179f8f5c2a7aaf45a2e42fea9213f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the SHERPA framework to solve overfitting and poor generalization of SAMs in task-specific fine-tuning, using small SAMs’ task-relevant features (via FRS and GFE modules) to guide large SAMs. Validated on 4 fine-tuning and 12 generalization datasets across 3 tasks, it boosts generalization retention by up to 11.1% and task performance by up to 2.2%, and extends to other ViT architectures, yet has room to improve in size ratio universality, complex scenario performance and module parameter analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is well-substantiated overall, with attractive figures and tables.\n\n2.Numerous experiments were conducted, and the proposed method demonstrated superior performance in the figures and tables.\n\n3.The ablation experiments appear to be well-substantiated."}, "weaknesses": {"value": "1.Lines 073-081 does not explain why the Fisher ratio concept is used.\n\n2.Lines 084-088 are incomprehensible.\n\n3.The problem illustrated in Figure 1 is obvious in the field of visual fine-tuning, and it seems unnecessary to spend a large amount of space introducing this problem, which will make the text appear redundant and reduce the reading experience. In addition, the GFE mentioned in Line 87 corresponding to the text in Figure 1 is not clearly explained. It is suggested to demonstrate the role of GFE in Figure 1.\n\n4.The overall writing of the Introduction section is disorganized. Before Line 99, it mainly discusses how to avoid problems caused by domain transfer, while the first contribution point is related to large and small models, making the reading experience unsmooth.\n\n5.The Related Work section lacks many of recent visual fine-tuning works: \n\n[1]Visual prompt tuning, ECCV'22;\n \n[2]5%> 100%: Breaking performance shackles of full fine-tuning on visual recognition tasks, CVPR'25; \n\n[3]Adaptformer: Adapting vision transformers for scalable visual recognition, NeurIPS'22; \n\n[4]1% vs 100%: Parameter-efficient low rank adapter for dense predictions, CVPR'23; \n\n[5]Sensitivity-aware visual parameter-efficient fine-tuning, CVPR'23; \n\n[6]Gradient-based parameter selection for efficient fine-tuning, CVPR'24; \n\n[7]Parameter-efficient is not sufficient: Exploring parameter, memory, and time efficient adapter tuning for dense predictions, MM'24.\n\n6.The introduction of the Fisher ratio in Section 3.1 seems to be abrupt. The authors should first explain why the Fisher ratio is suitable for use here, and then introduce how to specifically use this theory. The FR(.) function in the current version is incomprehensible.\n\n7.I cannot understand the two assumptions mentioned by the authors: (1) What prerequisites should these two assumptions satisfy? Do such prerequisites hold for the SAM model? (2) The authors do not explain why these two assumptions are needed? What role do they play in helping us understand the FR function?\n\n8.Section 3.5 seems to be a method similar to knowledge distillation. The authors should emphasize the relevance of this method to recent knowledge distillation technologies and the innovation of this method in multiple parts of the article.\n\n9.LoRA and adapter are relatively old works. There have been many excellent works in the field of visual PEFT recently (see the previous references), but there seems to be no comparative experiment in the experimental part. Considering that this paper focuses on visual fine-tuning technology, I think that necessary comparative experiments are needed.\n\nMinor issue: \n\nLine 117: Xuhong et. al or Li et. al?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GEpAqEmgyW", "forum": "7Db6Pbrcha", "replyto": "7Db6Pbrcha", "signatures": ["ICLR.cc/2026/Conference/Submission2379/Reviewer_EcEb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2379/Reviewer_EcEb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635486595, "cdate": 1761635486595, "tmdate": 1762916214591, "mdate": 1762916214591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Fine-tuning SAM on limited labeled data can compromise its inherent generalization ability. To mitigate this issue, the paper proposes a strategy that exploits task-relevant attention cues derived from a smaller, auxiliary model to inform the adaptation of the larger model. The authors introduce a Guiding Feature Extraction (GFE)  module to extract representative guiding features from the smaller model, which are subsequently decomposed into task-specific and generalization-redundant components using a Fisher Ratio Separation (FRS) module. The task-specific component is incorporated as a regularization signal during fine-tuning, encouraging the large model to allocate attention more effectively. Experimental results across multiple benchmark datasets verify that this method enhances in-domain performance while better preserving cross-domain generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Maintaining the generalization capability of segmentation foundation models when fine-tuned on limited labeled data is a critical research challenge. Prior approaches have explored lightweight parameter adaptation or the incorporation of pseudo-labeled data to alleviate overfitting. Introducing auxiliary losses between a large model and a smaller model represents an orthogonal direction to these studies. The proposed framework is evaluated on diverse settings, including medical imaging, natural images, and video datasets."}, "weaknesses": {"value": "The motivation to preserve generalization by aligning the model's task-relevant features is somewhat unclear. As currently presented, this approach appears to encourage the model to focus more on task-specific features, which could potentially increase overfitting rather than help maintain generalization. A key question is why the regularization is not applied between the generalization-relevant components of attention instead. If the authors explored this alternative and found it less effective, a corresponding analysis or ablation study should be provided. Furthermore, the distinction between task-relevant and generalization-redundant features could be more clearly articulated. Rather than across tasks, is the underlying idea that the principal components of the attention space should remain consistent across models (e.g., between large and small models), while the residual components are more model-specific or noisy and therefore should not be constrained?\n\nAdditionally, it is not immediately evident how enforcing alignment with task-specific features contributes to generalization retention rather than compromising it. Moreover, if the benefit truly arises from the task-relevant subspace, it is unclear why the KL divergence in Equation (4) is applied over the entire feature rather than specifically on the task-relevant component. This design choice seems inconsistent with the stated objective and methodology.\n\nThe use of the small model's feature as guidance is not well motivated and lacks compelling evidence. The results in Table 6 show quite a small difference. Moreover, such a claim of the importance of small models is regarded as one major contribution of this paper and the major difference with respect to knowledge distillation. Therefore, I think it is important to provide solid evidence, such small difference on a single dataset is not enough.\n\nAs shown in Table 17, this method is somehow sensitive to the hyperparameters, as the author also mentioned, \"even the same model requires a different number of task-relevant feature channels depending on the dataset. \" This raises concerns about the robustness and general applicability of the proposed approach across diverse settings."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L5rWUw8ydP", "forum": "7Db6Pbrcha", "replyto": "7Db6Pbrcha", "signatures": ["ICLR.cc/2026/Conference/Submission2379/Reviewer_VgD8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2379/Reviewer_VgD8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763223799, "cdate": 1761763223799, "tmdate": 1762916214463, "mdate": 1762916214463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SHERPA, a fine-tuning framework for Segment Anything Models (SAMs) that aims to retain generalization while improving performance on a target task. A smaller, task-fine-tuned SAM provides task-relevant guidance to a larger SAM via two modules. The authors give a generalization analysis showing reduced capacity and a risk bound favoring SHERPA over standard fine-tuning under mild conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated and well-written.\n\n- The central idea of using a smaller, weaker model to guide a larger, more powerful one is counter-intuitive but very effective. The justification via the information bottleneck theory is a strong and novel conceptual contribution.\n\n- The method is validated across three distinct and challenging domains: standard natural images, fine-grained biomedical images (a common failure case for general-purpose SAMs), and video segmentation with SAM2."}, "weaknesses": {"value": "- The most significant concern is the practical overhead. The SHERPA framework requires a two-stage process. first, the small model must be fine-tuned, and then the large model is fine-tuned using the small model as a guide. Table 15 reports the training time for \"SHERPA large model\" as 748 minutes and \"Fine-tuning small\" as 192 minutes. This implies a total training time of 940 minutes, a 34% increase over the 702 minutes for standard fine-tuning (SFT) of the large model.\n\n- The idea that the small model's features are \"more representative\" and thus a better guide than what the large model could find on its own. This is an assumption. What if the small model's severe information bottleneck (or biased or under-fitted) causes it to find a suboptimal set of features, which then incorrectly constrains the more capable large model? The paper would benefit from robustness tests to noisy or imperfect guidance (e.g., ablate small-model quality)."}, "questions": {"value": "Please see Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cVRJvIviqq", "forum": "7Db6Pbrcha", "replyto": "7Db6Pbrcha", "signatures": ["ICLR.cc/2026/Conference/Submission2379/Reviewer_L6wg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2379/Reviewer_L6wg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875806604, "cdate": 1761875806604, "tmdate": 1762916214313, "mdate": 1762916214313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SHERPA, a method to fine-tune large segmentation models (e.g., SAM) while preserving pre-trained, generalizable knowledge by guiding fine-tuning through task-relevant features extracted from a smaller, constrained model. The approach has two main components: FRS (Fisher Ratio Separation), which identifies a task-relevant subspace (channels) using Fisher-ratio statistics, and GFE (Guiding Feature Extraction), which extracts guidance features (based on Q·K products from transformer layers) from a small, constrained model to shape the large model’s updates only inside the task-relevant subspace. The paper provides a theoretical generalization bound that isolates an estimation error term for the subspace and presents experiments across multiple segmentation domains (natural images, biomedical, video), comparing to several fine-tuning baselines and reporting improved zero-shot retention / cross-domain generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Motivating fine-tuning failure as loss of task-agnostic general features and addressing it by restricting/guiding updates to task-relevant subspace is conceptually clean and easy to interpret.\n2. The FRS subspace extraction (Fisher-ratio based) and the GFE guidance (query-key based features) are modular and understandable, this helps explain when and why the method should help.\n3. The paper supplies a generalization upper bound that explicitly attributes part of the error to subspace estimation quality, this strengthens the link between the algorithmic design and expected empirical behavior."}, "weaknesses": {"value": "1. The high-level idea, using discriminative statistics or teacher models to preserve useful pre-trained information during fine-tuning, has appeared in related forms recently (e.g., Fisher-guided tuning and query/feature-level distillation approaches). The manuscript does not sufficiently distinguish SHERPA’s contributions from those prior works [1,2,3] or provide direct empirical comparisons to the closest recent baselines. This weakens the claim of novelty.\n2. The theoretical bound and practical success rely on assumptions about within-/between-class covariance structure (used to justify the Fisher-based subspace). The paper provides limited empirical analysis of (a) how often those assumptions hold across layers/domains, and (b) sensitivity to the subspace estimation parameters (m, k). Without this, it’s unclear how robust the method is in harder or atypical domains.\n3. The experiments compare to classical regularized fine-tuning baselines (L2-SP, Ft-last, etc.) but lack direct comparisons to strong alternatives that also aim to preserve pretraining knowledge: (a) feature-level distillation from teacher models, (b) recent Fisher- or information-preserving tuning methods, and (c) attention/query-level distillation methods. This makes it hard to judge whether SHERPA’s gains come from the Fisher-guidance principle specifically or from being a kind of distillation/regularization.\n\n[1] FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation\n[2] InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective\n[3] Distilling Knowledge from Large-Scale Image Models for Object Detection"}, "questions": {"value": "1. Are there domains / tasks where extracting guidance from a small model hurts performance? If so, characterize such failure modes and suggest remedies.\n2. Show performance curves when varying m (number of samples used to estimate W) and k (subspace dimension). What is the minimal m needed for stable behavior? How sensitive are results when k is over- or under-estimated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "43DxC1lwp8", "forum": "7Db6Pbrcha", "replyto": "7Db6Pbrcha", "signatures": ["ICLR.cc/2026/Conference/Submission2379/Reviewer_cMza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2379/Reviewer_cMza"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981925628, "cdate": 1761981925628, "tmdate": 1762916214144, "mdate": 1762916214144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}