{"id": "raJNosW0o3", "number": 7694, "cdate": 1758032336064, "mdate": 1759897838858, "content": {"title": "Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains", "abstract": "Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: how to effectively select the most valuable auxiliary data to maximize domain-specific performance, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose NTK-Selector, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial gains of +8.7 and +5.1 points, which correspond to a 10.9x and 5.7x improvement over the domain-only setting.", "tldr": "We select auxiliary data via neural tangent kernels from general-purpose corpus to enhance low-resource domains.", "keywords": ["data selection", "neural tangent kernel", "low-resource"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16b7186560d1af8399c6f8948638939eb148a067.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes NTK‑Selector, a two‑stage method to select auxiliary general‑domain data to augment low‑resource domain fine‑tuning of LLMs. Stage 1 does coarse pre‑selection using hidden‑state embeddings; Stage 2 uses a Jacobian‑free NTK approximation computed over LoRA gradients with random projections to score candidates. The authors (i) argue that LLMs under LoRA exhibit “NTK‑like” behavior—near‑constant kernel direction during training, (ii) show strong linear correlation between exact NTK and their approximation, and (iii) report gains across medical, financial, legal, and psychological tasks, e.g., average improvements of +8.7 (Llama3‑8B‑Instruct) and +5.1 points (Qwen3‑8B) when mixing 1K domain with 9K selected auxiliary samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Timely problem: Data selection for low‑resource domains is important and aligns with community interest.\n\nClear, modular pipeline: The two‑stage design (embedding pre‑selection + NTK‑based scoring) is easy to implement conceptually.\n\nBreadth of evaluation: Eight tasks across four domains; figures and tables are easy to follow. \n\nComputational pragmatism: Random projection over LoRA gradients is a plausible way to make per‑example gradient inner products tractable."}, "weaknesses": {"value": "1. Questionable Theoretical Foundation. The paper's core theoretical contribution—claiming \"NTK-like behavior\" in LLMs—lacks rigorous justification. Definition 1 introduces this concept based solely on Frobenius cosine similarity remaining above 0.99, but: No theoretical analysis explains why this threshold is meaningful. The connection between this observation and the validity of using NTK for data selection is tenuous. Theorem 1's proof relies on assumptions that don't hold for LLMs (infinite width, specific initialization). The reparameterization in Equation 6 introduces a perturbation term Δ(u) that could dominate the dynamics, undermining the entire approach.\n\n2. Flawed Approximation Method. The Jacobian-free approximation (Definition 2) is problematic: The claim that cross-output interactions are \"approximately 6%\" lacks theoretical grounding. Figure 2b shows correlation but doesn't validate that relative ordering is preserved, which is critical for selection. No analysis of how approximation errors propagate through the selection process. The random projection further compounds approximation errors without proper error bounds.\n\n3. No statistical significance testing or confidence intervals provided. Inconsistent results: LESS causes a 49.8-point drop on FPB for Qwen3-8B, suggesting fundamental instability. The \"10.9x improvement\" claim is misleading—it's relative to a weak baseline, not absolute improvement\n\n4. Baseline and training‑recipe confounds.  Missing comparison with recent strong methods like curriculum learning approaches. LESS baseline implementation appears flawed given its catastrophic failure on some tasks. No comparison with simple but effective baselines like similarity-based retrieval using modern embeddings.  The paper’s own analysis notes that Domain‑Only fine‑tuning sometimes hurts performance. That suggests the recipe (LR, schedule, number of epochs, regularization) may be mis‑tuned for some tasks. Without strong domain‑only baselines (e.g., early stopping, smaller LR, stronger regularizers, more epochs with decay, or longer context for legal tasks), the gains from selection may be inflated by weaknesses in the domain‑only control. Moreover, some selection baselines (e.g., LESS/TSDS/DSIR) appear adapted under different pre‑selection and warm‑up budgets, which risks compute unfairness relative to NTK‑Selector’s own warm‑up + projection pipeline.\n\n5. Methodological Concerns. The coarse-grained pre-selection using embeddings could already be doing most of the work. No ablation study isolating the contribution of NTK versus embedding similarity. Hyperparameter choices (M=4N, K=M/4, projection dimension) appear arbitrary. The normalization by sequence length (mentioned briefly) could be a confounding factor.\n\n6. Overfitting, leakage, and evaluation‑protocol risks. The candidate pool is CoT Collection (1.8M); there is no documented de‑duplication against target tasks or test sets (especially for short news headlines/sentences), nor any near‑duplicate filtering (e.g., D4‑style) beyond the warm‑up embeddings. This omission is particularly concerning for legal/privacy tasks where templated question formats recur. Also, the authors generate chain‑of‑thought rationales with GPT‑4o‑mini to fill in training rationales, but do not analyze how those synthetic rationales interact with selection or evaluation. Both factors threaten the validity of reported gains.\n\n7. Scope and significance are narrower than claimed. Claims of universality (“first work to investigate kernel behavior of LLMs in the fine‑tuning regime” and consistent gains) are ambitious, but the empirical coverage is classification‑centric with no generation‑quality metrics (e.g., factuality/faithfulness), no long‑context settings, and limited model diversity (two 8B models). For Qwen3‑8B, some selection methods harm performance; NTK‑Selector’s improvements are smaller and not uniformly large. The contribution, while interesting, feels incremental given existing gradient/influence‑based selection literature."}, "questions": {"value": "NTK‑like stability: Does Figure 2a replicate on each domain/task and across ≥3 seeds? Please report mean±std Frobenius cosine similarity and when it degrades (e.g., longer training or higher LR).\n\nApproximation errors: Quantify cross‑output terms across tasks and relate errors in Θ to selection ranking flips. How often does the Top‑N under Θ differ from exact NTK on small subsets?\n\nBaseline parity: Re‑run Domain‑Only with tuned LR/epochs/regularization and early stopping per task; report if NTK‑Selector still delivers the same margins. Also equalize warm‑up/compute budgets across LESS/TSDS/DSIR.\n\nLeakage control: Provide a stringent de‑duplication protocol (document‑ and sentence‑level, including near‑duplicates) between CoT Collection and all target test sets; quantify any overlap found.\n\nCoT generation effects: Analyze whether GPT‑4o‑mini‑generated rationales themselves are responsible for gains (e.g., ablate “use generated CoT vs. no CoT” while holding selection fixed).\n\nGeneralization: Evaluate one open‑ended generation task (e.g., long‑form QA or legal clause drafting) to show that “NTK‑like” alignment helps beyond classification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethical concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KT4akpLWro", "forum": "raJNosW0o3", "replyto": "raJNosW0o3", "signatures": ["ICLR.cc/2026/Conference/Submission7694/Reviewer_6LL8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7694/Reviewer_6LL8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617538071, "cdate": 1761617538071, "tmdate": 1762919753638, "mdate": 1762919753638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "On Jacobian-free approximation of NTK"}, "comment": {"value": "Dear authors,\n\nI'd like to bring the following paper into your attention, published in ICML 2023: https://arxiv.org/abs/2206.12543\n\nIt looks like the Jacobian-free approximation discussed in Section 3.2 is the same as Pseudo-NTK discussed in the paper above, which has heavily discussed this approximation and given theoretical bounds for how good the approximation is. \n\nBest,"}}, "id": "OnX6a42DIy", "forum": "raJNosW0o3", "replyto": "raJNosW0o3", "signatures": ["~Mohamad_Amin_Mohamadi1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Mohamad_Amin_Mohamadi1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7694/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147247630, "cdate": 1763147247630, "tmdate": 1763147247630, "mdate": 1763147247630, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NTK-Selector, a framework for selecting valuable auxiliary data from a large, general-purpose corpus to improve the fine-tuning of Large Language Models on low-resource domains. The method is grounded in the Neural Tangent Kernel, but adapts it for practical use with LLMs by demonstrating a stable NTK-like behavior during LoRA fine-tuning and using a scalable Jacobian-free approximation. Through extensive experiments, the authors show their method outperforms baselines across four specialized domains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a highly relevant and critical problem: specializing LLMs in data-scarce environments. The core contribution is novel and theoretically motivated, successfully bridging the gap between NTK theory and the practical realities of fine-tuning massive LLMs. The experimental validation is rigorous and convincing, employing multiple modern models, diverse domains, and strong baselines. The proposed two-stage selection process, combining efficient embedding-based filtering with a more precise NTK-based scoring, is a well-designed and practical solution."}, "weaknesses": {"value": "- Overstated improvement claims: The abstract claims \"a 10.9x\" improvement. This is calculated by dividing the absolute performance gain of NTK-Selector by the gain from Domain-Only fine-tuning. Since the Domain-Only gain is minimal and likely close to noise, this ratio exaggerates the method's effectiveness. Presenting the substantial absolute gains (e.g., +8.7 points) would be more direct and less sensational.\n- Justification of approximations is empirical: The method's validity rests on two key observations that could be explored further: NTK-like stability: The stable kernel behavior is demonstrated for a single task. While compelling, the paper's foundation would be stronger if this stability were shown to hold across the other diverse tasks. Jacobian-free approximation: The paper states that the ignored cross-terms are small to the exact NTK. This is a crucial justification, but it's presented without much detail on how this value was calculated or how consistent it is across different models, layers, or tasks. A more thorough analysis of this approximation error would strengthen the method's credibility.\n- Not discussed components: The coarse-grained pre-selection step involves a \"warm-up LoRA training phase\" on the domain data before computing embeddings. This seems like a critical step for creating domain-aware embeddings, yet its impact is not ablated or analyzed."}, "questions": {"value": "- Could you provide more details on the calculation of the error for the Jacobian-free approximation? Does this low error stem from properties of the transformer architecture, the nature of LoRA gradients, or something else?\n- The paper validates the NTK-like stability on a financial task. Did you observe a similarly high similarity for the other, more structurally different domains like medical QA or legal NLI?\n- The coarse pre-selection stage seems crucial for efficiency. Have you analyzed the sensitivity of the final performance to the quality of this stage?\n- Your analysis shows that more pre-selected samples and a higher projection dimension lead to better performance. For practitioners, a plot showing performance vs. computational cost for different hyperparameter settings (M, p) would be extremely valuable for making informed decisions based on their compute budget. Have you considered such an analysis?\n- Is there a qualitative explanation for why NTK-Selector provides massive gains on tasks like ContractNLI but more modest gains on others like MMLU-Med? Does an analysis of the selected auxiliary data reveal anything about the types of reasoning patterns the selector prioritizes for different tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LS88IuFybe", "forum": "raJNosW0o3", "replyto": "raJNosW0o3", "signatures": ["ICLR.cc/2026/Conference/Submission7694/Reviewer_Tf9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7694/Reviewer_Tf9F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750543936, "cdate": 1761750543936, "tmdate": 1762919753205, "mdate": 1762919753205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NTK-Selector, a data selection method for improving LLM fine-tuning in data-limited domains. The key idea is to score and select auxiliary general-domain samples based on their Neural Tangent Kernel (NTK) similarity to small in-domain data, approximated efficiently using LoRA gradients and random projections. The authors claim that LLMs exhibit “NTK-like” behavior during LoRA fine-tuning, enabling this approach. Experiments across medical, financial, legal, and psychological datasets show consistent performance gains over several baselines, suggesting the method helps mitigate overfitting in small-data regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper introduces an NTK-inspired approach to auxiliary data selection, creatively adapting kernel theory to LLM fine-tuning through LoRA gradients and random projections.\n\nThis work targets a concrete and important challenge: improving model performance in data-limited domains.\n\nThe experimental study evaluates the method across multiple domains (medical, financial, legal, psychological) and two strong LLMs, showing consistent positive trends.\n\nThe paper is well-organized, easy to follow, and visually clear, with informative figures and tables."}, "weaknesses": {"value": "The main comparison to the “Domain-Only” baseline is not compute-matched, NTK-Selector trains on roughly 10× more data and includes an additional LoRA warm-up stage, inflating perceived gains.\n\nThe embedding-based warm-up uses the same domain data later used for fine-tuning, effectively giving the method a privileged view of the target domain compared to baselines.\n\nMissing simple yet critical controls such as (a) embedding-only pre-selection and (b) gradient-dot or Fisher-similarity baselines. Without these, it is unclear whether the NTK component adds value beyond semantic similarity.\n\nThe “NTK-like” interpretation may simply reflect LoRA’s low-rank parameterization, not intrinsic NTK stability.\n\nUse of GPT-4o-generated chain-of-thought data and subsampled rich domains (medical, legal, financial) contradicts the claim of “low-resource” scenarios.\n\nEach approximation step (Jacobian-free, LoRA-only, projection) is individually motivated but never ablated to quantify its effect on selection quality.\n\nSeveral claims and phrasings exaggerate the impact or novelty of the work, creating a marketing tone inconsistent with the evidence. The abstract highlights “10.9× and 5.7× improvements,” a misleading ratio derived from dividing by near-zero baselines; absolute gains of +8.7 and +5.1 points would be clearer and sufficient. The “low-resource” framing itself reads as rhetorical inflation rather than a faithful description of the experimental setup."}, "questions": {"value": "Can you provide performance-vs-compute plots (e.g., accuracy vs. total tokens or optimizer steps) and a Domain-Only baseline trained with the same token budget?\n\nWhat is the relative performance of embedding-only or simple gradient-similarity selection under identical budgets?\n\nCan you show empirical evidence supporting the orthogonality assumption in the Jacobian-free approximation (e.g., a histogram of gradient cosine similarities)?\n\nHow does NTK-Selector perform if the warm-up phase is removed or equally applied to baselines?\n\nHow much of the improvement remains if GPT-4o-generated CoT data are excluded from the auxiliary pool?\n\nDo the same trends hold on a truly low-resource domain (e.g., small language, niche scientific field) rather than a subsampled rich one?\n\nCould you isolate the effect of each approximation step (LoRA restriction, projection dimension, Jacobian-free simplification) to verify the final score remains correlated with the exact NTK?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TttpuwyaH3", "forum": "raJNosW0o3", "replyto": "raJNosW0o3", "signatures": ["ICLR.cc/2026/Conference/Submission7694/Reviewer_HDBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7694/Reviewer_HDBK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841432676, "cdate": 1761841432676, "tmdate": 1762919752722, "mdate": 1762919752722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce NTK-Selector and its purpose is to select useful auxiliary training samples from large-scale general corpora to benefit low-resource domains. The process involves an initial coarse filtering stage using embedding similarity, followed by a fine-grained selection stage employing a Jacobian-free NTK approximation. The paper tackles two primary challenges of applying NTK. First, it empirically demonstrates that LLMs exhibit an \"NTK-like\" stability during LoRA fine-tuning and the kernel's directional structure remains almost constant. Second, it proposes the aforementioned Jacobian-free approximation, which is combined with LoRA and random projection to reduce the associated memory and computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Significance: The paper addresses a highly important and practical problem \"how to effectively select the most valuable auxiliary data to maximize domain-specific performance\". This is particularly relevant for scenarios lacking large in-domain data pools or validation sets, where traditional methods often fail.  \n2. Computational Efficiency. The combination of a Jacobian-free approximation, calculating gradients only for LoRA modules, and applying random projection substantially reduces the associated memory and computational overhead.  \n3. Empirical Performance: The experimental results presented are strong, demonstrating the effectiveness of the proposed framework across several domains and models."}, "weaknesses": {"value": "1. Generalisability of the Core Assumption: The paper's central claim of \"NTK-like\" stability relies heavily on empirical results from two specific models (LLAMA3-8B-INSTRUCT and QWEN3-8B) under LoRA fine-tuning. The study does not provide evidence that this stability holds for other parameter-efficient fine-tuning methods or for full model fine-tuning. Its applicability to different model architectures also remains unverified.  \n2. Empirical Basis of the NTK Approximation: The proposed Jacobian-free approximation is justified by the empirical observation that gradient directions for different output dimensions are \"nearly orthogonal\". This justification currently lacks theoretical results to support the quality of approximation. A more rigorous analysis would explore task types where this orthogonality assumption might not hold.  \n3. Statistical Robustness: The results are largely presented as numerical values. The paper does not report confidence intervals over multiple runs. This omission makes it difficult to assess the robustness of the reported performance gains."}, "questions": {"value": "1. Could the authors comment on the generalisability of the observed \"NTK-like\" stability? Specifically, does this stability persist across different LoRA configurations, such as varying ranks? Does it hold for other parameter-efficient fine-tuning (PEFT) methods or during full model fine-tuning? Does it hold for other model architectures?  \n2. Regarding the Jacobian-free NTK approximation, which relies on an empirical observation of near-orthogonality: are there any theoretical results that can support the quality of this approximation? Could the authors elaborate on specific task types or model configurations where this assumption might be less valid?  \n3. To improve the assessment of the method's robustness, would it be possible for the authors to provide results as means with corresponding confidence intervals, computed over multiple independent runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yRgbBgWI1x", "forum": "raJNosW0o3", "replyto": "raJNosW0o3", "signatures": ["ICLR.cc/2026/Conference/Submission7694/Reviewer_YZab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7694/Reviewer_YZab"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944453101, "cdate": 1761944453101, "tmdate": 1762919752029, "mdate": 1762919752029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}