{"id": "k6q8rRYVQR", "number": 524, "cdate": 1756744029703, "mdate": 1763380563652, "content": {"title": "On the Design of One-step Diffusion via Shortcutting Flow Paths", "abstract": "Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space.\nTo address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256×256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning.\nWe believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.", "tldr": "", "keywords": ["Diffusion Model", "Flow Matching", "Few-step Diffusion", "Shortcut Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec5bb33ea942615a07f59d0146a4fb849394598a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of designing one-step diffusion models, often called shortcut models, which can be trained efficiently from scratch. The authors argue that existing works are complex, with theoretical derivations and practical implementations being \"closely coupled.\" This makes it difficult to understand the core design principles, compare methods, or innovate on individual components. To solve this, this paper proposes a unifying framework that reframes existing shortcut models under a single principle: approximating a two-step flow map target. They empirically and theoretically demonstrate that continuous-time shortcut models (CTSC) using a linear flow path consistently outperform discrete-time (DTSC) or cosine-path-based models. Building on this analysis, the authors propose an improved CTSC model called ESC with three key designs: plug-in velocity, class-consistent batching, and a gradual time sampler. Using these improvements, the authors train a scaled-up (SiT-XL/2) ESC model from scratch, achieving a new state-of-the-art FID of 2.85 on ImageNet-256x256 with a single function evaluation (1-NFE), without requiring any pre-training, distillation, or curriculum learning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's primary strength is its common design framework. The \"one-step prediction vs. two-step target\" (Eq. 5) is a powerful and intuitive abstraction that successfully unifies a complex and dense field of research. Table 1, which decomposes prominent models into their constituent parts, is an excellent contribution to the community.\n2. The motivations are clearly stated and verified. The analysis in Section 3, which ablates flow paths (linear vs. cosine) and time sampling (discrete vs. continuous), is thorough and convincing, backed by both empirical results (Fig. 2) and theoretical analysis.\n3. The final result, an FID of 2.85 (1-NFE) on ImageNet-256x256, is a significant achievement, especially given that the model is trained entirely from scratch. This result challenges the long-held belief that one-step models require costly distillation from a pre-trained, multi-step teacher."}, "weaknesses": {"value": "This is a strong paper without major technical weaknesses. Please see my questions below."}, "questions": {"value": "1. Does the trained model actually satisfy the flow consistency property $X_{0.5,0}^{\\theta}(X_{1,0.5}^{\\theta}(x_1)) \\approx X_{1,0}^{\\theta}(x_1)$? The 2-NFE failure suggests it may not (Figure 6). Could this imply the model has simply memorized a single shortcut rather than learning the underlying flow field?\n2. Following the previous question, I'd like to know if ESC can be used in a multi-step fashion. If yes, are there any results or visualizations? \n2. Is ESC applicable to video DiT models? Are there any results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XzeJZeLnSl", "forum": "k6q8rRYVQR", "replyto": "k6q8rRYVQR", "signatures": ["ICLR.cc/2026/Conference/Submission524/Reviewer_k1TZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission524/Reviewer_k1TZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787827004, "cdate": 1761787827004, "tmdate": 1762915538303, "mdate": 1762915538303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission summarizes and organizing algorithmic design space of one-step diffusion models including consistency training, shortcut diffusion models, and mean flows.\nAlthough differences in derivation and wordings of the methods, they share computational framework of training by aligning one-step predictions toward targets that are acquired by two-step computation, which can be viewed as learning \"shortcut\".\nThe manuscript includes theoretical analyses claim that 1) both two-step DTSC and CTSC have bounded errors w.r.t. Lipschitz constants of the velocity field, and 2) inference error of  shortcut models measured Wasserstein-2 distance are bounded using bias and variance defined using targets and losses.\nFollowing the insights,  improved training method Explicit&easier Shortcut Model (ESC) is proposed. It uses techniques from the summarized literatures and new plug-in velocity, which aggregates in-minibatch velocity.\nExperiments using CIFAR and ImageNet shows that ESC can make improvements over MeanFlow slightly (-0.1 -- -0.3 in FID)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- One-step diffusion models are a hot topic in 2025 after Shortcut diffusion and Mean flow. This submission is a timely and nice follow-up to help understanding.\n- Theoretical analyses are deep and well motivated for understanding shortcut behavior of continuous and discrete models. (although I could not check all of the proofs in the near-30-page appendix.)\n- In the method part, plug-in velocity (Algorithm 1) is novel and shown to be useful in the experiments.\n- The overall paper are well organized."}, "weaknesses": {"value": "- Practical impact: surely the SOTA results in FID are achieved but the FID gains provided by ESC may be marginal. I think that the differences of FID ranging within 0.1 -- -0.3 hardly impacts on human opinions on the quality of images.\n\n- Experimental/theoretical supports of design choices are relatively weak: Table 2 shows the design choice picked from the design space, but I think these selections looks empirical and I could not grasp how the theoretical results are exploited. Please point if I missed some parts."}, "questions": {"value": "- In algorithm 1, some lines are too much pythonic to interrupt non-programmer readers. for example, x[:,None,:] and logp_fn = Normal(0, 1).log_prob (function treated as an object) may be replaced if possible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "v4WhLYylR5", "forum": "k6q8rRYVQR", "replyto": "k6q8rRYVQR", "signatures": ["ICLR.cc/2026/Conference/Submission524/Reviewer_rWuT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission524/Reviewer_rWuT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966481677, "cdate": 1761966481677, "tmdate": 1762915538082, "mdate": 1762915538082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework for ODE based one-step diffusion models trained from scratch. The authors systematically analyze existing methods (CT, SCD, IMM, sCT, MeanFlow) by decomposing them into modular components and studying their design choices. \nBuilding on this analysis, they introduce ESC (Explicit & Easier Shortcut model), which incorporates several technical improvements including plug-in velocity, gradual time sampling, and adaptive loss weighting. ESC achieves improved FID50k of 2.85 on ImageNet-256×256 with one-step generation, without requiring pre-training, distillation, or curriculum learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n- While the work does not introduce a fundamentally new method and offers only moderate novelty, it contributes a valuable unifying perspective on ODE-based one-step diffusion models.\n- The theoretical contributions are strong: Theorem 2.2 establishes error bounds for both DTSC and CTSC; Proposition 3.1 presents an insightful bias–variance analysis clarifying when CTSC outperforms DTSC; and Theorem C.7 theoretically justifies why linear paths are optimal for shortcut models under Fisher information metrics.\n- It provides comprehensive ablations and analyses that thoroughly examine the effectiveness of the proposed techniques, demonstrating overall high quality."}, "weaknesses": {"value": "- Limited Novelty of Core Framework: While the unified view is valuable for understanding, this is already well established in previous work Flow Map Matching[1,2], which further hurt the contribution of \"propose a common design framework for representative shortcut models\".\n    -from discrete to continuous, one can either set $s= t−dt$ to get backward formula such as MeanFlow and sCM; or set $s=r+dt$ to obtain the forward formula such as AlighYourFlow [3].\n- Empirical Gaps:\n    - Slow convergence in 2-NFE generation (Fig. 6, Section E) suggests the improvements may be overfitted to 1-NFE\n    - Limited comparison with recent distillation-based methods that achieve better FID scores [4,5,6]\n    - The improvement compared to the baseline is nevertheless incremental.\n- Technical Concerns: The time sampler design (gradual transition from sCT to MeanFlow) and the choice of $p_{plugin}$ appears heuristic without principled justification\n\n[1] Nicholas Matthew Boffi, Michael Samuel Albergo, and Eric Vanden-Eijnden. Flow map matching with stochastic interpolants: A mathematical framework for consistency models. Transactions on Machine Learning Research, 2025. \n\n[2] Qiang Liu. Icml tutorial on the blessing of flow. International conference on machine learning,\n2025.\n\n[3] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. arXiv preprint arXiv:2506.14603, 2025.\n\n[4] Tianwei Yin, Micha¨ el Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6613–6623, 2024.\n\n[5] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024.\n\n[6] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:76525–76546, 2023."}, "questions": {"value": "- how to interpret IMM as a special case in the proposed framework? e.g. build connection between eq(5) and eq(50)\n- In fig.1, it shows that practical prediction matches practical target, which is unrealistic. In addition, in fig 1 (c)(e), we have vector addition $v_{t|0}+u_{t|r}^\\theta=(r-t) d u_{t|r}^\\theta / dt$, which is inconsistent with eq(8).\n- In sec 2.4 Q3, the authors claim that \"(s)CM benefits from distillation by learning from a pretrained velocity field\", however, in sCM, \"We always initialize the CM from the EMA parameters of the teacher diffusion model. For sCD, we always use the $F_{pretrain}$ of the teacher diffusion model with its EMA parameters during distillation.\" That's to say, for sCT the teacher model is used as initialization for the one-step generator, which is critical.\n    - Given that the authors admit that at least a pre-trained model is important for better few-step performance as answer to this Q3, and that it's impractical to train large-scale text-to-image model from scratch, the authors failed to justify why in this work they insist working on training from scratch.\n\n-------- below could be biased (does not affect the rating) -------\n- focus of the paper. while aims to \"disentangles concrete component-level choices, thereby enabling systematic identification of improvements\", the empirical results are mainly focus on improved training techniques for MeanFlow baseline.\n- the authors show in Fig 1 visually and with prop 3.1 that the challenge of constructing flow map targets and the consequence during inference, however, this challenge is not directly addressed in the paper. \nThe training technique/design are proposed to enhance training stability, while no empirical results can validate the training instability of the baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wmve2Tg3y3", "forum": "k6q8rRYVQR", "replyto": "k6q8rRYVQR", "signatures": ["ICLR.cc/2026/Conference/Submission524/Reviewer_YqvP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission524/Reviewer_YqvP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762220105693, "cdate": 1762220105693, "tmdate": 1762915537918, "mdate": 1762915537918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework for various shortcut model methods and provides both theoretical and empirical analyses of the advantages and disadvantages of different designs. Based on these analyses, several improvements are introduced, such as plug-in velocity, gradual time samplers, and class-consistent mini-batching, collectively referred to as the explicit&easier shortcut modeling method, which achieves state-of-the-art performance in one-step ImageNet-256×256 generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides a comprehensive framework for a series of shortcut model methods, offering effective tools for analyzing this family of approaches.\n- The analysis of each component is detailed and well-structured. By disentangling the individual components, the paper makes the design space considerably more transparent. Theoretical analyses are extensive and appear to be well-structured.  \n- The proposed method achieves SOTA results on one-step ImageNet-256×256 generation, demonstrating the effectiveness of the improvements.\n- This paper releases detailed code, ensuring reproducibility."}, "weaknesses": {"value": "- According to Figure 3, it is difficult to claim that “the convergence of FID50k during training is substantially faster with the class-consistent mini-batching technique.” More evidence is needed to support this statement.\n- As shown in Figure 6, at the XL scale, the proposed method achieves worse FID under 2-NFE compared to 1-NFE. This might indicate a potential scalability issue of the proposed approach.\n- There are a few minor typo errors. For example, in line 198, $X^\\theta_{t,r}(xt)$ should be $X^\\theta_{t,r}(x_t)$; in line 263, $l_{scm}$ seems to refer to $l_{sct}$ mentioned earlier."}, "questions": {"value": "- The results in Tables 2 and 3 suggest that the proposed method brings more improvements with the large-scale network architecture than with the basic one. What causes this difference?\n- In Figure 3(b), several comparison curves included in Figure 3(a) are missing. What is the reason for this omission? The same question applies to Figure 3(c)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CIrWYUwPuv", "forum": "k6q8rRYVQR", "replyto": "k6q8rRYVQR", "signatures": ["ICLR.cc/2026/Conference/Submission524/Reviewer_enqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission524/Reviewer_enqR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762325863996, "cdate": 1762325863996, "tmdate": 1762915537805, "mdate": 1762915537805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first summarizes and presents the design space of shortcut flow paths in diffusion models for one-step generation, such as consistency model, shortcut diffusion, MeanFlow, etc. The authors further propose plug-in velocity along with multiple pratical techniques to improve training of continuous-time shortcut models (SC), and enhance generation performance of shortchut models in their experiments."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper presents a formalization of shortcut models within a unified framework. This can provide a valuable foundation for subsequent work.\n+ The paper elucidates the design space of SC models. Both the mathematical formulation and the overall writing quality are presented with clarity.\n+ The paper makes several theoretical contributions that will likely benefit future research. These include: i) A Wasserstein distance bound for the objectives of discrete-time (DT-SC) and continuous-time (CT-SC) models. ii) An inference error bound for both CT-SC and DT-SC in terms of the variance of the average velocity. iii) The optimality of linear paths for SC models under Fisher information metrics."}, "weaknesses": {"value": "- The empirical results indicate that the proposed plug-in velocity yields marginal performance gains, suggesting that its practical benefits over existing methods may be limited.\n\n- The experimental comparison would be strengthened by the inclusion of other state-of-the-art baselines, such as rectified flow and reflow, for a more comprehensive evaluation.\n\n- The improvement techniques presented appear to be specific to the MeanFlow architecture. The paper's impact could be broadened by exploring the generalizability of these techniques to other SC models. For example, the authors might include a discussion or experimental analysis on the selection of loss metrics for different SC model variants."}, "questions": {"value": "1. Regarding the multi-GPU training implementation, are the batches gathered across all devices to compute a plug-in velocity, or is the plug-in velocity computed locally on each device's batch?\n\n2. Do the authors investigate the impacts of the number of samples used for calculating the plug-in velocity? For instance, setting computational efficiency aside, does increasing the sample size lead to further performance gains?\n\n3. Table 3 indicates a significant performance gap between the MeanFlow-based approaches and the other methods benchmarked. Could the authors offer an explanation or formalize the reasons for this observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VgeOy8B7D9", "forum": "k6q8rRYVQR", "replyto": "k6q8rRYVQR", "signatures": ["ICLR.cc/2026/Conference/Submission524/Reviewer_jD45"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission524/Reviewer_jD45"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327342996, "cdate": 1762327342996, "tmdate": 1762915537701, "mdate": 1762915537701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}