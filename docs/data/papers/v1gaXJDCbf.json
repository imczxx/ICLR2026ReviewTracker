{"id": "v1gaXJDCbf", "number": 6434, "cdate": 1757983604782, "mdate": 1759897914836, "content": {"title": "ResGen: Residual Diffusion Model for LiDAR-based Point Cloud Generation", "abstract": "While significant progress has been made in 2D image generation, the generation of 3D point clouds is less explored. Existing occupancy generation methods usually suffer from resolution limitations, while range image based methods are limited to single-frame rotating-scanning LiDAR points. In this paper, we propose ResGen, a framework towards realistic LiDAR-based point cloud generation. Our method first generates coarse 3D structures and then refines them into high-fidelity point clouds. Specifically, we build a 3D Residual Diffusion Model for refinement. With a pilot study revealing the theoretical shortcomings in existing approaches, we model our diffusion process for a \\textit{residual} between the coarse and refined point cloud. ResGen preserves fine-grained details and demonstrates applicability to multi-frame accumulated LiDAR point clouds. Experiments demonstrate that ResGen achieves superior results both qualitatively and quantitatively. Code will be made publicly available.", "tldr": "", "keywords": ["Diffusion Models for Vision", "3D Computer Vision", "LiDAR", "Point Cloud"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/791a91d41907498503ab0cd304c1243bc4a20d34.pdf", "supplementary_material": "/attachment/69e90501e0cbe99d8dcb547611a070c3b7c27e0d.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes for 3D point cloud generation in the context of LiDAR scanning. The proposed method takes a two-step approach. In the first step, it generates density voxels in the 3D space. Then in the second step, it generates point clouds from density voxels. In the second step, points are sampled proportionally to the voxel density (not sure), and their locations are decided by their positions relative to voxel centers. The proposed method can also take text specifications as the condition, so its generation behavior can be controlled by text. The method is also extended multi-frame generation. The experiment shows that the method outperforms the baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The two-step generation procedure is novel. The first step generates the general structure with the voxel structure. This design probably will ease the model's effort in deciding the general layout of shapes. The second step can then focus on local details. \n\nThe performance of the proposed model is better than baseline models."}, "weaknesses": {"value": "The first step of generating voxels in the latent space is largely known in the field of image generation. Therefore, it somewhat discounts the novelty of the proposed work. \n\nThe writing of the work can be greatly improved. Here are a list of questions not clear from the writing. \n1. Line 174, are there corresponding images of point clouds? Then, the caption of the image from the VLM is used as the caption of the point cloud? Is this really reliable as the image and point clouds are very different modelity. \n\n2. The context of the pilot study is not clear. The LiDiff paper seems to use a diffusion model to generate target point cloud conditioned on a sparse one. Does the model have the Gaussian assumption in the **generation** stage? Figure 2 is hard to read. Are there curves? Or does the colored area represent the distribution? \n\n3.  The inference stage is not clearly described. Given the voxel densities, the method will sample points according to these densities and then denoise these points all together in the single denoising model, right? \n\n4. It is hard to understand how the method is applied to multi-frame generation. Do you treat these frames independently?"}, "questions": {"value": "In the user study, what measures do you take to avoid the bias? For example, how are these users hired? Is the evaluation double-blind? A lot of necessary details are missing. \n\nIt would be interesting to check the performance on voxel density as well. On the test set and point clouds generated by a competing method, you can compute their respective voxel densities. If you can check the performance of voxel densities, it is another method to understand where the improvement is from."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NStCG29JsS", "forum": "v1gaXJDCbf", "replyto": "v1gaXJDCbf", "signatures": ["ICLR.cc/2026/Conference/Submission6434/Reviewer_Qfi7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6434/Reviewer_Qfi7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974155839, "cdate": 1761974155839, "tmdate": 1762918824488, "mdate": 1762918824488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ResGen, a two-stage framework for generating realistic LiDAR point clouds. The first stage uses a diffusion model to generate coarse representation of point clouds in voxel. The second fine-tuning stage uses the produced Residual Diffusion Model to generate the precise representation, conditioned on the fixed coarse input. The method is evaluated unconditionally on KITTI-360, showing strong gains in JSD, MMD, and a custom density metric (REAP)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Comprehensive Experimental Evaluation**\nThe paper provides extensive quantitative and qualitative evaluations under both conditional and unconditional generation settings. As demonstrated in the reported metrics and visualization results, the proposed method consistently outperforms the baseline approaches in terms of generation precision.\n \n\n2.  **Modular and Well-Structured Pipeline**   \nThe framework is cleanly decomposed into a modular two-stage pipeline, as clearly illustrated in Figure 1. The data flow from 2D latent representations to the refined point cloud is logically presented, and the residual generation mechanism offers an intuitive and effective refinement strategy."}, "weaknesses": {"value": "1.  **Insufficient Evaluation on benchmarks and baselines**  \nI. The quantitative comparisons are primarily conducted on unconditional generation using the KITTI-360 dataset. However, the quantitative experimtnes of the text-conditioned point cloud generation task is missing. \nII. All compared baseline methods were published prior to 2025. Including more recent state-of-the-art approaches would strengthen the experimental validity and better situate the proposed method within the current research landscape.\n\n2.  **Moderate Technical Novelty**  \n    While the coarse-to-fine framework and residual learning mechanism are intuitive and effective, they have been widely explored in other computer vision domains, such as LiDAR odometry and scene flow estimation. Furthermore, the strategy of “starting from a given prior sample,” though practical, resembles a simple yet effective training trick seen in prior works (e.g., ADD [1]).\n\n3.  **Insufficient Failure and Diversity Analysis**  \n    The paper does not provide an analysis of failure cases or a systematic evaluation of output diversity. These aspects are crucial for thoroughly assessing the robustness and generative performance of the proposed method.\n\n\n[1] Sauer A, Lorenz D, Blattmann A, et al. Adversarial diffusion distillation[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 87-103."}, "questions": {"value": "Given the relatively intuitive pipeline of the proposed method, the experimental results carry significant weight in demonstrating its overall effectiveness and contributions. In addition to addressing the weaknesses previously outlined, the following issues also require attention:\n\n1.There are notable inconsistencies between the performance metrics of baseline methods as reported in this paper and their original publications. For instance, the JSD score for LiDM on unconditional generation is reported as 0.439 here, whereas it was 0.211 in the original work. The authors should clarify the source of these discrepancies—whether they stem from differences in implementation, evaluation protocols, or data processing.\n\n2.As shown in Figure 6, the proposed method generates point clouds with notably sharper edges compared to the curved and blurred outputs of LiDiff. Is this attributed to the use of voxel representation towards the point cloud structure? It raises a further question: why is this characteristic not observed in the results of other methods presented in Figure 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ocd0ZE8uG4", "forum": "v1gaXJDCbf", "replyto": "v1gaXJDCbf", "signatures": ["ICLR.cc/2026/Conference/Submission6434/Reviewer_LssR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6434/Reviewer_LssR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080032887, "cdate": 1762080032887, "tmdate": 1762918824036, "mdate": 1762918824036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ResGen, a novel two-stage framework for high-fidelity LiDAR-based point cloud generation, aiming to overcome the limitations of resolution in occupancy methods and applicability in range image methods. The method first uses a 3D VAE on density voxels combined with a 2D Latent Diffusion Model to generate a coarse 3D structure. The core innovation lies in the second stage: a Residual Diffusion Model (Residual DM) for refinement. Motivated by a pilot study that rejects the commonly implicit assumption of Gaussian residuals in previous \"Partial Diffusion Models\" (e.g., LiDiff), the authors propose to directly model the diffusion process for the residual $\\delta = x_{tar} - x_{pri}$, conditioned on the coarse prior $x_{pri}$. This approach is shown to be theoretically sounder and empirically superior. Experiments on KITTI-360 and SemanticKITTI demonstrate state-of-the-art results across various metrics (JSD, MMD, REAP, User Preference) for both single-frame and multi-frame accumulated point cloud generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The most significant contribution is the rigorous analysis and subsequent proposal of the Residual Diffusion Model.\n2. ResGen achieves superior quantitative results compared to all tested baselines on unconditional generation (Table 1).\n3. The coarse-to-fine pipeline effectively addresses the challenge of generating large-scale, high-resolution 3D data by separating structural generation from detail refinement."}, "weaknesses": {"value": "1. The VAE encoder transforms 3D density voxels into 2D feature maps by \"simply merged\" the information along the Z-dimension (lines 147-151). While the authors state this is sufficient for the coarse stage, this process inherently discards or compresses depth/height information into a 2D plane, which could limit the quality of the initial structural prior. It would be beneficial to provide more detail on this merging operation.\n2. In Table 3, the two variants of the Residual DM—Residual only and Concatenated show almost same performance. This suggests that using one over the other provides no practical benefit. This finding should be highlighted more strongly as it might simplifies the network."}, "questions": {"value": "1. You trained LDM, which mean we are not working with direct voxels. does the data is not normal distributed in the latent as well?\n2. This all method is build on diffusion, but the world shifted to flow matching, does your method can be shift as well?\n3. Please provide the explicit mathematical operation used to \"simply merge\" the information along the Z-dimension of the 3D feature voxels into 2D feature maps in the VAE encoder"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JZwRRsvvOr", "forum": "v1gaXJDCbf", "replyto": "v1gaXJDCbf", "signatures": ["ICLR.cc/2026/Conference/Submission6434/Reviewer_dBaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6434/Reviewer_dBaQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114611614, "cdate": 1762114611614, "tmdate": 1762918823549, "mdate": 1762918823549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}