{"id": "eJV3JhJvZF", "number": 8107, "cdate": 1758063874623, "mdate": 1759897807189, "content": {"title": "DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science", "abstract": "The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create a emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B’s accuracy by 1.83× and reinforcement learning boosts Qwen3-4B’s accuracy by more than 8×. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.", "tldr": "", "keywords": ["LLM", "Dataset", "Benchmark", "Data Science"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa013fc97336ad2208147553888861fff7b8fb13.pdf", "supplementary_material": "/attachment/653ad6860e9c2ffa0aa6096a8dd914de85ba4c1e.zip"}, "replies": [{"content": {"summary": {"value": "The author suggests DARE-bench, designed to evaluate and train LLMs for data science workflows. It addresses two major gaps in existing benchmarks: (1) lack of process-aware evaluation capturing instruction adherence and modeling fidelity, and (2) lack of accurately labeled training data. DARE-bench includes 6,300 verifiable, executable Kaggle-derived tasks divided into two families. The benchmark enables reproducible automatic evaluation via sandboxed code execution. Experiments show that general-purpose LLMs perform poorly on these tasks without domain-specific training, but fine-tuning on DARE-bench data dramatically improves both process fidelity and prediction accuracy. The paper demonstrates up to 8× improvement in smaller models after reinforcement learning fine-tuning, and also validates generalization improvements on external datasets like DSBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a large-scale, executable, and verifiable benchmark that measures both process fidelity and predictive accuracy, filling a key gap in LLM evaluation for data science.\n\n2. Provides a rich dataset derived from Kaggle that supports both evaluation and supervised/RL-based training, improving reproducibility and scalability.\n\n3. Demonstrates concrete, measurable performance improvements and reduced execution failures after fine-tuning, validating benchmark utility for both evaluation and model training.\n\n4. Clearly integrates determinism and sandbox execution to ensure fair, reproducible comparison across models, a major methodological advantage."}, "weaknesses": {"value": "1. Heavy reliance on Kaggle-derived data may bias task diversity toward structured tabular and forecasting problems, limiting broader domain generalization.\n\n2. The benchmark’s focus on reproducibility via deterministic setups may underrepresent realistic data science variability or stochastic modeling behavior.\n\n3. The paper lacks ablation or error analysis to quantify which aspects of fine-tuning drive improvements most."}, "questions": {"value": "1. How does DARE-bench handle tasks involving stochastic algorithms (e.g., random forest, neural nets) while maintaining deterministic evaluation?\n\n2. Are the reference “ground truth” codes verified manually, or automatically synthesized and how is correctness guaranteed?\n\n3. Does reinforcement learning with DARE-bench tasks risk overfitting to procedural templates rather than improving general data reasoning?\n\n4. Can this benchmark be extended beyond Kaggle-style tabular tasks, e.g., to unstructured text or multimodal datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UwPLrptEwh", "forum": "eJV3JhJvZF", "replyto": "eJV3JhJvZF", "signatures": ["ICLR.cc/2026/Conference/Submission8107/Reviewer_Ffce"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8107/Reviewer_Ffce"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204520761, "cdate": 1761204520761, "tmdate": 1762920089388, "mdate": 1762920089388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DARE-bench, a benchmark designed to evaluate and train LLMs for data science tasks, addressing two critical gaps in existing benchmarks: (i) the absence of process-aware, verifiable evaluation (e.g., measuring instruction adherence) and (ii) scarcity of high-quality labeled training data. Derived from 6,300 Kaggle datasets, DARE-bench includes two task families—process-aware instruction-following (with reference-code ground truth) and ML modeling (with dataset ground truth)—covering classification, regression, and time-series forecasting. Key features include verifiable ground truth (enabling objective, human-judge-free evaluation) and a dual role as both an evaluation tool and training resource. Evaluations show strong LLMs (e.g., Qwen3-32B, gpt-4o-mini) perform poorly on baseline tests, but supervised fine-tuning (SFT) and reinforcement learning (RL) using DARE-bench data yield dramatic gains: SFT improves Qwen3-32B’s accuracy by 1.83×, and RL boosts Qwen3-4B’s accuracy by over 8×."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It addresses two key gaps in existing benchmarks: it enables verifiable, process-aware evaluation (relying on reference-code or dataset ground truth, no human/model judges) and provides 6,300 Kaggle-derived tasks as large-scale training data, ensuring objective, reproducible assessments . \n2. Its task coverage is comprehensive—covering classification, regression, time-series forecasting, with two variants (instruction-following/ML modeling) probing core DS capabilities, outperforming peers (e.g., DS-1000, DSBench) in time-series support and training task provision ."}, "weaknesses": {"value": "1. Tasks are almost exclusively tabular, excluding multimodal inputs (e.g., text-image combinations, code-diagram interactions) common in modern DS.\n2. Generating large-scale executable trajectories (for training data) is costly, and rejection sampling strategies may introduce biases toward shorter trajectories."}, "questions": {"value": "As listed in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EfIwi7NXdv", "forum": "eJV3JhJvZF", "replyto": "eJV3JhJvZF", "signatures": ["ICLR.cc/2026/Conference/Submission8107/Reviewer_BggK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8107/Reviewer_BggK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831384245, "cdate": 1761831384245, "tmdate": 1762920088687, "mdate": 1762920088687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DARE-bench, a benchmark for evaluating and training LLMs on data science (DS) tasks. Considering that existing benchmarks lack of process-aware evaluation and scarce labeled data, the DARE-bench offers 6,300 Kaggle-derived tasks (covering classification, regression, time-series forecasting) with verifiable ground truth. Tasks include two variants: Instruction Following (IF, testing workflow adherence) and ML Modeling (MM, testing outcome accuracy). Experiments show baseline LLMs perform poorly, but SFT and RL using DARE-bench data drastically improve performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is good-writing and easy to follow.  The benchmark provides comprehensive evaluation scope, specifically, it covers diverse DS tasks (including underrepresented time-series forecasting) and enforces real-world constraints (execution time, interaction turns), enhancing practical relevance.\n2. DARE-BENCH serves both as an evaluation tool and a large-scale training resource, with proven effectiveness in improving LLM performance via SFT/RL."}, "weaknesses": {"value": "1. Lack of Comparison with Specialized DS Agents. The paper evaluates general-purpose and code-centric LLMs  but omits comparisons with specialized data science agents, which are explicitly designed for multi-step DS workflows. This gap makes it hard to contextualize DARE-bench’s utility. It is unclear whether the benchmark’s gains (via fine-tuning) can match or surpass the performance of purpose-built DS agents,\n2. Provide more explanations about the Instruction Following (IF) and ML Modeling (MM) metrics. The paper frames IF (workflow adherence) and MM (outcome accuracy) as two core DS capabilities to evaluate together, but fails to justify their joint necessity. Especially given Table 4’s results showing no clear correlation between the two metrics. For example, GPT-5 scores highest in classification-IF (69.81) but ranks mid-tier in classification-MM (43.40); Claude-Sonnet-3.7 excels in MM tasks (e.g., regression-MM: 63.20) but lags GPT-5 in IF."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cwzdg2wwRz", "forum": "eJV3JhJvZF", "replyto": "eJV3JhJvZF", "signatures": ["ICLR.cc/2026/Conference/Submission8107/Reviewer_NfgX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8107/Reviewer_NfgX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902910652, "cdate": 1761902910652, "tmdate": 1762920088185, "mdate": 1762920088185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large Language Models (LLMs) are increasingly adopted for complex multi-step data science (DS) tasks, yet existing benchmarks suffer from two critical gaps: a lack of process-aware evaluation (e.g., instruction adherence and process fidelity) and scarce high-quality labeled training data. To address these, this paper introduces DARE-BENCH, a training-focused benchmark for evaluating LLMs’ DS capabilities, encompassing both machine learning (ML) modeling and instruction following.\n\nDerived from Kaggle datasets, DARE-BENCH includes classification (Instruction Following/IF, ML Modeling/MM), regression (IF, MM), and time-series (eXogenous Features/XF, Canonical Forecasting/CF) tasks, split into 95% training and 5% test sets. Unlike benchmarks relying on human/model judges, all tasks have verifiable ground truth (reference outputs for IF tasks, original dataset labels for MM tasks), ensuring objective, reproducible evaluation via a sandboxed code execution environment.\n\nExtensive evaluations show that even advanced LLMs (e.g., gpt-4o-mini, Qwen3-32B) perform poorly on baseline tests, especially in time-series tasks. However, fine-tuning with DARE-BENCH yields significant improvements: supervised fine-tuning (SFT) increases Qwen3-32B’s accuracy by 1.83×, while reinforcement learning (RL) boosts Qwen3-4B’s accuracy by over 8×. External validation on DSBench further confirms generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "DARE-BENCH has several strengths against previous work.  \nUnlike counterparts that only assess final-answer accuracy, DARE-BENCH uniquely evaluates both ML modeling performance and instruction fidelity, filling the void of process-aware assessment. It also provides 6,300 Kaggle-derived tasks with verifiable ground truth (reference outputs for IF tasks, original labels for MM tasks). The training data seems to be valuable. \nIn addition, its four-stage pipeline minimizes human effort, enabling large-scale task generation (6,300 tasks) while ensuring realism—e.g., 20% noise injection in IF tasks to simulate real-world data issues."}, "weaknesses": {"value": "The task diversity is limited. It exclusively covers tabular data, lacking support for multimodal DS tasks (e.g., text-image fusion, speech-data analysis), restricting applicability to broader DS scenarios."}, "questions": {"value": "1. How do you validate the quality of your generated dataset? \n2. Do you have qualitative and quantitative analysis?\n3. What's the detailed dataset statistics for your dataset, e.g., information like how many tool calls, how many tokens are your prompt or your completion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "50eXqQ8XLH", "forum": "eJV3JhJvZF", "replyto": "eJV3JhJvZF", "signatures": ["ICLR.cc/2026/Conference/Submission8107/Reviewer_pBVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8107/Reviewer_pBVz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762313820059, "cdate": 1762313820059, "tmdate": 1762920087754, "mdate": 1762920087754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}