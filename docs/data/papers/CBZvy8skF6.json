{"id": "CBZvy8skF6", "number": 15244, "cdate": 1758249280959, "mdate": 1763626563448, "content": {"title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating the collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves the business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4. The codes are available at https://anonymous.4open.science/r/Cochain-6866.", "tldr": "We propose Cochain, a collaboration prompting framework that combines chain-of-thought reasoning depth with multi-agent collaboration breadth to solve business workflow challenges.", "keywords": ["Multi-Agent Collaboration", "LLM Agents", "Business Workflow Decision Making"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a7be2502f7323e026e99fdab46ed84ff61cddb59.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript proposes Cochain, a framework designed to enhance the reasoning ability of LLM-based agents while reducing token costs. While multi-agent systems (MAS) are generally more capable than single-agent systems, they often suffer from excessive token usage, high inference latency, and over-collaboration. To address these challenges, Cochain introduces a knowledge-graph-based (KG-based) framework that constructs a task-relevant knowledge graph from the training dataset. For a new query, Cochain identifies a causal chain within the KG that is most relevant to the query to guide reasoning. Empirical studies demonstrate the superiority of Cochain over baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the efficiency challenge of multi-agent systems (MAS) in business workflows, which is an important and practically relevant problem."}, "weaknesses": {"value": "1. Limited Literature Review. The authors claim that the inefficiency and failures (e.g., over-collaboration) of MAS have not been studied (Line 76), which motivates their work. However, recent studies [1–3] have already examined MAS failures arising from agentic communication and coordination issues. The related work section should be expanded to include these efforts.\n\n2. Methodological Positioning. The authors state that Cochain integrates chain-of-thought reasoning into agentic systems to reduce token cost. However, the proposed framework is conceptually closer to knowledge graph construction and KG-based retrieval-augmented generation (RAG), rather than explicit chain-of-thought reasoning. The distinction should be clarified.\n\n3. Causal Chain Construction. The manuscript claims that Cochain builds a causal chain to guide reasoning. However, the method described in Section 3.2 merely extracts the top-K relevant triples from the KG. The causal inference mechanism or reasoning process linking these triples is not clearly defined or justified.\n \nReferences\n\n[1] Multi-agent Architecture Search via Agentic Supernet. ICML 2025.\n\n[2] Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems. ICML 2025.\n\n[3] Why Do Multi-Agent LLM Systems Fail? arXiv:2503.13657."}, "questions": {"value": "The authors are encouraged to address the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HZq7ZboQvW", "forum": "CBZvy8skF6", "replyto": "CBZvy8skF6", "signatures": ["ICLR.cc/2026/Conference/Submission15244/Reviewer_YHtV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15244/Reviewer_YHtV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667007596, "cdate": 1761667007596, "tmdate": 1762925541333, "mdate": 1762925541333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wm4xdvBPhF", "forum": "CBZvy8skF6", "replyto": "CBZvy8skF6", "signatures": ["ICLR.cc/2026/Conference/Submission15244/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15244/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763626562796, "cdate": 1763626562796, "tmdate": 1763626562796, "mdate": 1763626562796, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cochain, a novel prompting framework designed to improve the collaborative reasoning of Large Language Model (LLM) agents, particularly for complex, multi-stage \"business workflow\" tasks. The authors identify a key trade-off between single agent and multiagent. Cochain aims to achieve the \"effective collaboration\" of a multi-agent system at the low cost of a single-agent system. It does this by having agents collaborate indirectly through two main components built in a one-time setup process"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly articulates the concepts of \"under-collaboration\" and \"over-collaboration\". Identifying and providing a solution for the high cost and diluted focus of multi-agent systems.\n2. The evaluation metrics include text similarity metrics and human evaluation. \n3. Paper visualization is great and help to understand the paper and it also provides the cost analysis in the appendix, making it practical."}, "weaknesses": {"value": "1. it is unknown if Cochain generalize beyond “business workflows” to creative, open-ended reasoning tasks (e.g., scientific discovery, multi-hop QA).\n2. The Prompts Tree seems to encode a static, linear workflow. How would Cochain adapt to a dynamic business process where, for example, the output of Stage 2 determines whether Stage 3 or Stage 4 comes next?\n3. Addition to 2, the framework needs to be completely rebuilt when accommodating changes in the workflow structure. \n4. The counterfactual generation and tacit variable modeling seem conceptually interesting, but the paper doesn’t show ablation isolating this component from the rest (e.g., KG + causal chain without counterfactual reasoning). It’s therefore unclear whether counterfactual augmentation materially improves performance or just inflates data diversity."}, "questions": {"value": "1. How scalable is Cochain in terms of graph size and prompt tree depth? Are there any empirical bottlenecks beyond the small domain examples?\n2. Can Cochain be applicable to creative, open-ended reasoning tasks (e.g., scientific discovery, multi-hop QA)?\n3. Are there risks of reinforcing domain biases when constructing the collaborative knowledge graph from domain-specific agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZlDzYJucfd", "forum": "CBZvy8skF6", "replyto": "CBZvy8skF6", "signatures": ["ICLR.cc/2026/Conference/Submission15244/Reviewer_2PGG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15244/Reviewer_2PGG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967225839, "cdate": 1761967225839, "tmdate": 1762925540922, "mdate": 1762925540922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cochain, a prompting framework designed for large language model (LLM) agents. The framework integrates a knowledge graph to aggregate and utilize information across multiple stages of reasoning and maintains a structured prompt tree to retrieve contextually relevant knowledge for complex business workflows."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and effectively uses visual aids to illustrate the framework and workflow.\n- Evaluation is conducted on two benchmarks spanning four datasets, providing a broad empirical basis.\n- Extensive experiments across multiple LLMs strengthen the generality of the findings.\n- Both human and automatic evaluations are performed, offering complementary perspectives on performance.\n- Ablation studies and efficiency analyses provide valuable insights into the contributions and trade-offs of different components within Cochain."}, "weaknesses": {"value": "The framework’s increased reliance on the knowledge graph and prompt tree leads to higher resource consumption. A detailed discussion of this limitation, including potential mitigation strategies, would improve the paper’s completeness."}, "questions": {"value": "What computational resources (e.g., hardware specifications, number and type of GPUs, inference settings) were used to conduct all experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ruysbEvuTr", "forum": "CBZvy8skF6", "replyto": "CBZvy8skF6", "signatures": ["ICLR.cc/2026/Conference/Submission15244/Reviewer_YiA1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15244/Reviewer_YiA1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969728897, "cdate": 1761969728897, "tmdate": 1762925540339, "mdate": 1762925540339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Cochain, a novel framework that enhances collaboration and reasoning among LLM agents working across multi-stage workflows. It aims to solve two opposite issues in LLM systems: under collaboration and over collaboration."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Integrates explicit knowledge (from input-output data) and tacit knowledge (from counterfactual reasoning)."}, "weaknesses": {"value": "1. The paper is very hard to understand. As an examples, Figure 1 is not clear at all. Why is some text in red? what are the different stages of business workflow? Even Fig 2 is complicated.\n2. Many parts of the paper are unclear:\n- In equation 1, what is $n_i$, $N$, what does $j$ represent is not mentioned.\n- In equation 2, how is counterfactual generated? The paper mentions \"We then input these questions into the relevant vertical domain agents to generate answers\". What are these agents?\n- Is the Prompt Tree construction done for the entire dataset as a pre-processing step? or is it done per scenario or done dynamically? What model is used to construct the Prompt Tree?\n3. If the goal is answer user queries through effective collaboration of agents then shouldn't the metric used be accuracy of the responses rather than rouge and gleu?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VTh8TZXVTX", "forum": "CBZvy8skF6", "replyto": "CBZvy8skF6", "signatures": ["ICLR.cc/2026/Conference/Submission15244/Reviewer_gaLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15244/Reviewer_gaLJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137344078, "cdate": 1762137344078, "tmdate": 1762925539026, "mdate": 1762925539026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}