{"id": "InuV3Lh5bc", "number": 19094, "cdate": 1758293551545, "mdate": 1759897060300, "content": {"title": "Neural Tangent Kernel Perspective on Parameter-Space Symmetries", "abstract": "Parameter-space symmetries are transformations that modify model parameters without altering the outputs. These transformations can be leveraged to accelerate optimization and enhance generalization. Remarkably, applying a single transformation either before or during training often suffices to realize these benefits. While the effectiveness of this approach is very promising, its underlying mechanisms remain poorly understood. In this paper, we offer an explanation within the Neural Tangent Kernel (NTK) framework, by analyzing how such transformations affect the kernel's properties. In particular, we show that maximizing the alignment between the loss gradient and the data kernel is equivalent to maximizing the alignment between the NTK and the data. Since kernel alignment is known to correlate with optimization rate in the NTK limit, this insight elucidates how loss gradient optimization facilitates faster training. To establish the validity of this approach, we prove that parameter-space symmetries preserve the NTK limit.", "tldr": "", "keywords": ["Symmetries", "Parater Space Symmetries", "Parater-Space Symmetries", "Neural Teleporation", "Neural Tangent Kernel", "Wide Neural Networks", "Linearization", "NTK"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d061eba094741375163b0e1d795188e16d435f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the effects of *parameter-space symmetries* (transformations of parameters that leave the network’s output function unchanged) on neural network training. The two main contributions are: (1) the authors show that if the NTK regime (linearized training) holds for a network with initial parameters $\\theta$, it also holds for a network with parameters $g(\\theta)$, where $g$ is a symmetry transformation with Jacobian $\\Gamma^{-1}_{ij} := \\partial g_i(\\theta)/\\partial \\theta_j$ satisfying $\\||\\Gamma\\||\\le C$; (2) they demonstrate that maximizing the norm of the initial loss gradient $\\||\\nabla\\mathcal{L}(\\theta_0\\||$ is related to maximizing the alignment between the NTK and the effective residuals. The second contribution is connected to parameter symmetries through the observation that prior work on such symmetries used them to maximize $\\||\\nabla\\mathcal{L}(\\theta_0\\||$ in order to speed up training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**(S1) Framing:** The paper aims to connect several rather disjoint concepts: neural network symmetries, the NTK regime, and kernel alignment. To my mind, this is an interesting idea and I generally enjoyed reading the paper.  \n\n**(S2) NTK regime with symmetries:** The observation that convergence in the NTK regime is preserved under symmetry transformations is, to the best of my knowledge, novel and interesting. The proof based on the Hessian norm bound is brief and elegant, showing that the authors indeed identified an appropriate and effective analytical framework.  \n\n**(S3) Clarity and precision:** The results and derivations are clearly and formally stated.\n\n**(S4) Presentation:** The writing quality is good. The main text communicates the core ideas cleanly, and refers details and heavier notation to the appendix."}, "weaknesses": {"value": "**(W1) Missing link between the NTK regime and optimization parts:** The two main contributions (the NTK-regime invariance result and the discussion of NTK alignment and optimization) seem rather disconnected. The authors show that symmetries preserve the NTK regime and that maximizing the loss gradient norm can improve optimization through better NTK alignment. However, the paper does not clearly explain why or how the considered symmetries would actually promote such alignment, or why symmetries are a particularly effective way to achieve this compared to other methods. The connection between these parts appears only briefly, justified mainly by previous empirical work using symmetries to maximize the loss gradient. Therefore, statements such as *“we showed that even a single application of a parameter-space symmetry can meaningfully improve the optimization rate in the NTK regime”* (Conclusion) seem overstated.  \n\n**(W2) Limited experiments:** The experiments are limited to two-layer MLPs on a subset of FashionMNIST and appear to cover only one form of rescaling symmetry. It would be much more interesting to investigate additional types of symmetries with different corresponding $C$ constant, and analyze how these affect convergence to the NTK regime. This would clarify whether the theoretical analysis indeed captures the underlying mechanisms.  \n\n**(W3)** There are also a few minor issues related to clarity: (1) the NTK alignment discussion is rather vague, the same argument might be stated more clearly via spectral bias; (2) the motivation for the specific symmetry model could be explained further. In particular, it would be useful to discuss more realistic examples of parameter symmetries and explain why or when they satisfy the paper’s assumptions (e.g., invertible $\\Gamma$)."}, "questions": {"value": "- Is it discussed anywhere in the paper how parameter-space symmetries actually influence the considered alignment notion? \n- Could you elaborate on how the results depend on specific types of symmetries and network architectures? For instance, how does the constant $C$ vary across transformations and models?  \n- How do you see the main practical application or insight of your results? The paper mentions potential use in designing more efficient optimization strategies—could you e.g. elaborate on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EWABycqCCM", "forum": "InuV3Lh5bc", "replyto": "InuV3Lh5bc", "signatures": ["ICLR.cc/2026/Conference/Submission19094/Reviewer_h7c3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19094/Reviewer_h7c3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761064520647, "cdate": 1761064520647, "tmdate": 1762931117678, "mdate": 1762931117678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Disclaimer: I have reviewed a previous version of this paper, which was submitted at NeurIPS. I carefully read this new version. The review below contains elements from my previous review and takes into account the discussion that took place among the reviewers (who decided to reject the paper). \n\nThis paper studies parameter space symmetries for neural network training through the lens of the neural tangent kernel framework. First, the paper shows that the NTK regime holds with the same asymptotic rate (up to a constant factor) when applying symmetry transformations. This effectively changes the NTK kernel. Interestingly, the paper shows that symmetry transformations that maximize the norm of the gradient at initialization yields a kernel with a better alignment property with the data, hence accelerating optimization. It is related to a technique called neural teleportation: The goal is then to explain such an empirical phenomenon from the NTK point of view."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is very well written and easy to follow throughout, including the proofs and appendix.\n- It addresses a missing link between theory and an empirical phenomenon.\n- The use of a modified alignment criterion is now well justified."}, "weaknesses": {"value": "- The central motivation is to explain the benefits of a technique known as neural teleportation, which involves applying symmetry transformations during or before training in the parameter space. While the phenomenon is interesting, I remain somewhat skeptical about its significance. Two years after its introduction, the technique still appears to be relatively marginal and has not seen widespread adoption in practice. Nevertheless, this is a minor comment and I admit that investigating this phenomenon is fully justified (I remember the answer from the authors regarding this point).\n\n- There was however a major issue raised during the previous round of reviews, which convinced me that the paper was not ready for publication, even though I was rather positive in the first place. The O(1/sqrt{n}) has a C^4 dependence, which was explicit in the previous version of the paper, but this issue is now hidden under the carpet, C being hidden in the big O notation. Without a precise understanding of the magnitude of C, it is hard to understand whether or not the 1/sqrt{n} is meaningful. I remember an argument that without bounded C, learning is impossible. This is probably true but we are missing a precise mathematical statement here. \n\nMore precisely, assuming it is reasonable to restrict the analysis to a class of bounded transformations with constant C, achieving the rate 1/sqrt{n} may be a positive result, but the scaling with C^4 is actually rather bad (negative result). I also remember an argument stating that  C should be close to 1. This is a critical point. We are perhaps missing either a precise statement or an empirical study with real-world data/transformations (where the transformations really help) and the corresponding value of C."}, "questions": {"value": "A discussion about the nature of C with precise mathematical statements will be crucial to change my assessment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7M3iBfcCsl", "forum": "InuV3Lh5bc", "replyto": "InuV3Lh5bc", "signatures": ["ICLR.cc/2026/Conference/Submission19094/Reviewer_5haL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19094/Reviewer_5haL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221005634, "cdate": 1761221005634, "tmdate": 1762931116657, "mdate": 1762931116657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the reparameterization symmetries under transformations of the parameters $\\theta \\to \\theta' = g(\\theta)$ in the kernel (linearization) regime of neural network training. In this regime, the Jacobian of the transformation $\\Gamma^{-1} = \\frac{\\partial g(\\theta)}{\\partial \\theta}$ is also static over the course of optimization. The authors argue (by means of examining the transformed gradients and Hessians) that for any $\\Gamma $ with bounded norm, the reparameterization does not prevent linearization, significantly relaxing assumptions commonly employed to induce a kernel limit. They use their theory to argue for a relationship between the kernel alignment and speed of training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper examines an interesting question of how reparameterization $\\theta \\to \\theta'$ impacts training dynamics of neural networks in the linearized regime. It also provides a nice description of training with the transformed NTK $\\frac{\\partial f}{\\partial \\theta} \\Gamma^\\top \\Gamma \\frac{\\partial f}{\\partial \\theta}$ and characterizing the transformed Hessian."}, "weaknesses": {"value": "**Few Example Applications of the Main Result** The authors allude to methods that reparameterize a model to speed up convergence. Could they design better initialization or reparameterization strategies? The current empirical figure simply reports a correlation between alignment and training speed."}, "questions": {"value": "1. The authors claim that \"maximizing the norm of the empirical loss gradient as defined in Equation 10, via symmetry transformations\" provides a method to accelerate training. How is this done in practice? Are there analytical methods for this or is it done numerically.\n2. Can you use this theory to account for the inducement of lazy training through output rescaling as in https://arxiv.org/abs/1812.07956? Can you design reparameterizations that break the kernel limit (force feature learning, etc)? \n3. Currently, the experiment in Figure 2 just reports a correlation between the alignment at initialization for various inits and the speed of training. If I understand correctly there is no reparameterization applied in this Figure. Is this plot surprising? I think it would be more interesting if the authors could **generate** better initializations based on their desired increased kernel alignment through reparameterization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9EM73rrXlZ", "forum": "InuV3Lh5bc", "replyto": "InuV3Lh5bc", "signatures": ["ICLR.cc/2026/Conference/Submission19094/Reviewer_UXBr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19094/Reviewer_UXBr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840121991, "cdate": 1761840121991, "tmdate": 1762931115779, "mdate": 1762931115779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the impact of parameter-space symmetries in neural networks on their learning behavior from the lens of neural tangent kernels (NTK). It is shown that transformations adhering to parameter-space symmetry preserve the NTK limit. Using the NTK perspective, it is revealed that the alignment between the NTK and data dictates the convergence rate of gradient descent. Limited experiments show the effectiveness of theoretical insights."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The novelty of this theoretical work lies in establishing the understanding of NTK under parameter-space symmetries and demonstrating that the alignment of NTK with data is a relevant metric for gradient descent convergence."}, "weaknesses": {"value": "The paper has several weaknesses.\n\n1. As a theoretical work, I found this paper to lack the necessary rigor for a reader to fully appreciate the theoretical results. In particular, Section 3.1 (Assumptions) and Theorem 4.1 could be improved with mathematical equations,\n2. The impact of the theoretical results on the generalization of neural networks is unclear. In my opinion, this is a key weakness of the theory presented in this paper.\n3. Experimental results are limited."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8uzHG7TXmh", "forum": "InuV3Lh5bc", "replyto": "InuV3Lh5bc", "signatures": ["ICLR.cc/2026/Conference/Submission19094/Reviewer_DSih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19094/Reviewer_DSih"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944421887, "cdate": 1761944421887, "tmdate": 1762931115159, "mdate": 1762931115159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}