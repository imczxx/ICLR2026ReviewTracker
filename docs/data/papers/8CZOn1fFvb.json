{"id": "8CZOn1fFvb", "number": 182, "cdate": 1756730453709, "mdate": 1759898272570, "content": {"title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope---typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains.\nTo bridge this gap, we propose **UR$^2$** (**U**nified **R**AG and **R**easoning), a general framework that unifies retrieval and reasoning through Reinforcement Learning. UR$^2$ introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks.\nExperiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We will release all code, models, and data upon submission.", "tldr": "We propose a unified framework that integrates retrieval and reasoning via Reinforcement Learning, achieving state-of-the-art performance across QA, medical, and mathematical reasoning benchmarks.", "keywords": ["LLM", "RAG", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4c4820c57199c4d830ffb74b62decf6bc3c5091.pdf", "supplementary_material": "/attachment/eb3da43a8b2988afceae38a6b029fc225fe07092.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces $UR^2$, a framework designed to unify Retrieval-Augmented Generation (RAG) and COT reasoning across diverse domains like math and medicine using RL. It introduces two primary contributions. On one hand, it achieves stable and efficient training by employing a two-stage optimization process and a difficulty-aware task mixing strategy. On the other hand, it explicitly reduces retrieval noise by using a separate, powerful LLM to pre-process raw documents into clean, concise summaries before they are presented to the policy model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research problem this paper tries to addresses (unifying RAG with reasoning through RLVR) is both important and timely. The paper's primary strength then lies in its extensive and rigorous experimentation in tackling this challenge. The authors are commended for validating their framework across four diverse and challenging domains (Math, Medical, QA, and MMLU-Pro) and comparing it against a comprehensive suite of state-of-the-art RAG and RL baselines . Furthermore, the work is clear and appears highly reproducible. The authors provide commendable transparency by detailing the exact training configurations and prompts in the appendices , which is a significant contribution to the community."}, "weaknesses": {"value": "1. The paper's methodological contribution appears limited. The primary contributions—a two-stage \"warm-up\" optimization , a \"difficulty-aware\" data mixing strategy , and the use of an external LLM to summarize retrieved information —are arguably a collection of well-established engineering techniques. While empirically effective, these components may not represent a significant methodological innovation that meets the high standard for publication at ICLR.\n2. This concern about novelty is amplified by the framework's heavy dependence on the LLM summarization module. The paper's own ablation study demonstrates this component is critical. This suggests that the $UR^2$ itself is not learning to handle retrieval noise; rather, this core challenge is outsourced to a separate external model. This reliance diminishes the self-contained contribution and robustness of the proposed framework.\n3. When I going through the paper, I observed several typos. \n- We train UR2 using REINFORCE++ (Guo et al., 2025a), a streamlined vari­ant of PPO tailored.\n- Trainning Data Selection"}, "questions": {"value": "1. The \"Task Mixing Strategy\"  creates a significant train-test discrepancy. During training, the framework manually feeds the model different prompts based on external difficulty labels (e.g., pure reasoning for \"easy\" math, RAG for \"hard\" math). However, during evaluation, a single, unified prompt is used for all math problems , and the external difficulty is absent. Why was this manually-partitioned curriculum chosen? Would it not be a more robust and end-to-end solution to use the same unified prompt (from testing) during training and let the RL learn this adaptive retrieval policy on its own?\n2. The paper's Stage 1 functions as a \"warm-up\" to teach the model retrieval syntax. Why was RL chosen for this step? In the literature, this type of tool-use is commonly achieved via SFT, which is generally considered more controllable, stable, and lightweight than RL. Could the authors justify their choice of using RL for this warm-up phase?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lbLK7mwb34", "forum": "8CZOn1fFvb", "replyto": "8CZOn1fFvb", "signatures": ["ICLR.cc/2026/Conference/Submission182/Reviewer_Xhpc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission182/Reviewer_Xhpc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615583981, "cdate": 1761615583981, "tmdate": 1762915464069, "mdate": 1762915464069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UR2 (Unified RAG and Reasoning), a reinforcement learning framework that integrates retrieval and reasoning in large language models. UR2 employs difficulty-aware curriculum training to selectively invoke retrieval for challenging problems and adopts a hybrid knowledge access strategy that combines domain-specific offline corpora with LLM-generated summaries. These components enable dynamic coordination between retrieval and reasoning, improving adaptability across diverse domains. Experiments on open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks show that UR2 outperforms existing RAG-RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This work makes a clear conceptual contribution by unifying retrieval and reasoning through reinforcement learning, bridging two previously isolated paradigms (RAG and RLVR). Unlike earlier approaches that are limited to fixed retrieval settings, UR2 introduces difficulty-aware curriculum training, enabling dynamic and efficient retrieval invocation.\n\n2. The paper provides strong empirical evidence supporting its contributions. The proposed approach is evaluated across four distinct domains (open-domain QA, MMLU-Pro, medical QA, and mathematical reasoning) and multiple model scales (3B–8B), consistently outperforming strong RAG-RL baselines such as Search-R1 and R1-Searcher."}, "weaknesses": {"value": "1. The proposed framework relies heavily on LLM-summarized corpora rather than raw or dynamically retrieved web content. While this design helps reduce noise and stabilize training, it may also limit the model’s ability to handle the complexity, variability, and unpredictability of real-world retrieval scenarios. This reliance raises concerns about generalization and robustness beyond curated or offline corpora, particularly in open-domain or rapidly evolving knowledge settings.\n\n2. Although the framework shows strong performance with models up to 8B parameters, it has not been evaluated at larger scales. This leaves open the question of how well UR2 would scale to more powerful models (e.g., 30B+ parameters) and whether the observed performance gains would persist in such settings.\n\n3. The reinforcement learning training shows occasional instability and required reruns to correct failed retrieval activation in some settings. This indicates potential reproducibility issues and sensitivity to initialization or hyperparameters."}, "questions": {"value": "1. UR2 employs difficulty-aware curriculum training to selectively invoke retrieval. Could this strategy negatively impact the model’s intrinsic reasoning ability in certain domains?\n\n2. Since difficulty classification requires 20 rollouts, how scalable and efficient is this approach for larger datasets?\n\n3. UR2 relies on LLM-summarized retrieval corpora, which may not capture the complexity of dynamic, real-world environments. How well would UR2 generalize to online or rapidly changing corpora, and are there strategies to improve its robustness in such settings?\n\n4. The paper claims that LLM-generated summaries reduce retrieval noise and hallucination. Could you provide clearer evidence or analysis for this claim, rather than inferring it indirectly from performance drops in the w/o-summary setting?\n\n5. UR2 uses reinforcement learning to coordinate retrieval and reasoning, which introduces additional training cost and occasional instability. Could simpler decision mechanisms (e.g., heuristics or lightweight classifiers) offer similar performance? If not, what makes RL uniquely effective in this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RrEKmk9z5Y", "forum": "8CZOn1fFvb", "replyto": "8CZOn1fFvb", "signatures": ["ICLR.cc/2026/Conference/Submission182/Reviewer_nM4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission182/Reviewer_nM4H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721756198, "cdate": 1761721756198, "tmdate": 1762915463861, "mdate": 1762915463861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the framework that unifies retrieval-augmented generation (RAG) and reasoning through reinforcement learning (RL), with the motivation that knowledge grounding and reasoning have been developed in isolation from previous works. Specifically, the proposed framework includes (i) a difficulty-aware curriculum that dynamically decides when to invoke retrieval (encouraging models to rely on internal reasoning for easy problems and use retrieval only for harder ones), and (ii) a hybrid knowledge access strategy that combines domain-specific offline corpora with LLM-generated summaries. Also, the proposed framework is trained via a two-stage RL process that separately optimizes retrieval capability and answer quality. Experiments on diverse open-domain QA and reasoning tasks show that it outperforms existing RAG and RL baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The focus on bridging knowledge grounding with RAG and reasoning is timely and meaningful.\n* The approach to optimize retrieval and answer generation through reinforcement learning is reasonable. \n* The proposed framework achieves superior performance over existing RAG and RL baselines."}, "weaknesses": {"value": "* While one of the core focuses of this work is on reasoning, it is questionable whether the model truly performs reasoning (like DeepSeek-R1 style models), or if the improvements mainly stem from retrieval and answer optimization without reasoning. Also, for reasoning, instead of using pure instruction fine-tuned models (like LLaMA or Qwen), it is questionable whether the reasoning (that the authors aim to achieve) can be performed when simply using more advanced reasoning models (e.g., DeepSeek-R1). Overall, the reasoning aspect feels underdeveloped despite the explicit focus in this work.\n* It is unclear how the LLM-generated summaries are constructed, as well as their scale. Additionally, it is also questionable whether they are of high quality and not vulnerable to hallucination. \n* The idea of invoking retrieval only for difficult queries is similar to Self-RAG and Adaptive-RAG [A, B]. The discussions and (probably) experimental comparisons with them are required. Also, it would be great if the authors could perform an efficiency analysis on it (i.e., whether the overall pipeline becomes efficient when invoking retrieval if necessary).\n* Reward design, which is a manually designed combination of format, retrieval (or generation), and fallback rewards, seems somewhat ad hoc. More analysis on it would be useful.\n* The difference between the proposed approach and the Vanilla RL baseline is unclear. \n* The performance gains on the Match domain are marginal, which may warrant further discussion.\n\n---\n\n[A] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n\n[B] Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity"}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uNh3HOweS8", "forum": "8CZOn1fFvb", "replyto": "8CZOn1fFvb", "signatures": ["ICLR.cc/2026/Conference/Submission182/Reviewer_46ub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission182/Reviewer_46ub"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876645113, "cdate": 1761876645113, "tmdate": 1762915463619, "mdate": 1762915463619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UR^2, a reinforcement learning (RL) framework that unifies retrieval-augmented generation (RAG) and reasoning. The authors point out two limitations in prior RAG-RL frameworks (e.g., *Search-R1*): (1) existing models fail to find an optimal trade-off between retrieval and reasoning, often overemphasizing one component; (2) retrieval introduces significant noise, which can degrade chain-of-thought reasoning quality.\n\nTo address these issues, the paper introduces two main components: (1) a hybrid retrieval corpus that combines domain-specific offline corpora with LLM-generated summaries, and (2) a difficulty-aware curriculum, where retrieval is invoked only for hard instances while easy ones rely on internal reasoning.\n\nThrough experiments on various benchmarks (MMLU-Pro, MedQA, MATH, and open-domain QA), the authors report that UR^2 improves performance compared to existing RAG-RL methods and achieves comparable results to GPT-4o-mini with 7B-scale models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of difficulty-aware curriculum and task-mixing strategy is interesting and intuitively appealing.\n- The paper presents comprehensive evaluations across multiple domains (math, medicine, open QA)."}, "weaknesses": {"value": "- Fairness and comparability of evaluation. The proposed method introduces an augmented knowledge base (LLM-summarized and task-specific corpora) as a key component. However, previous RAG-RL methods (e.g., Search-R1, R1-Searcher) are not restricted to specific corpora and could also utilize similar augmented data. Therefore, evaluating UR^2 solely on newly curated corpora while prior methods rely on static Wikipedia datasets makes the comparison unfair. If the augmentation of the corpus itself is a core contribution, the paper should explicitly analyze the effect of this augmentation (e.g., via retrieval corpus ablation), similar to prior work such as [1].\n- Limited novelty and weak justification of the method. The overall training pipeline remains very close to the Search-R1 style frameworks, merely adding a two-stage optimization process. However, the paper does not convincingly explain how this design resolves the original issue mentioned in the introduction (the imbalance between retrieval and reasoning). The conceptual connection between the two-stage RL setup and this problem statement is unclear.\n- Unclear source of performance gain. The improvement might stem from broader training dataset coverage or the use of enhanced corpora rather than from the proposed algorithm itself. Because the training data distribution and retrieval sources differ across baselines, it is hard to isolate whether gains come from the method or from data and corpus augmentation. A controlled comparison under identical conditions would strengthen the claim.\n- Lack of clarity and reproducibility in RL setup. The term “Vanilla RL” baseline is ambiguous. It's unclear whether it corresponds to a GRPO-based single-stage model (as in Search-R1) or a simplified UR^2 variant. Moreover, the use of REINFORCE++ instead of PPO or GRPO (common in previous works) is not justified or compared. Without such clarification, it is difficult to determine whether improvements arise from the proposed method design or simply from a change in the RL algorithm.\n\n[1] Lyu et al., Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks"}, "questions": {"value": "- What exactly does the Vanilla RL baseline represent? Does it follow the same architecture as Search-R1 with different data, or is it a simplified UR^2 variant?\n- Did you compare REINFORCE++ against GRPO or PPO under the same setup? If not, how can we be sure that the gains are not due to the RL algorithm itself?\n- How much of the reported improvement remains if all methods are trained on the same corpus and datasets? In particular, does UR^2 still outperform Search-R1 when using the same Wikipedia-based retrieval corpus?\n- How are the LLM-summarized corpora evaluated in terms of factual accuracy and hallucination?Any quantitative evidence that they indeed reduce retrieval noise would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lxTB8UmY5q", "forum": "8CZOn1fFvb", "replyto": "8CZOn1fFvb", "signatures": ["ICLR.cc/2026/Conference/Submission182/Reviewer_LVGE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission182/Reviewer_LVGE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929529956, "cdate": 1761929529956, "tmdate": 1762915463505, "mdate": 1762915463505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces $UR^{2}$, a general framework that unifies Retrieval-Augmented Generation (RAG) and reasoning through reinforcement learning to overcome the limited scope of prior RAG-RL methods. The framework's key innovations are a difficulty-aware curriculum that selectively invokes retrieval only for challenging problems and a hybrid knowledge access strategy that combines domain-specific offline corpora with LLM-generated summaries. Experiments across diverse tasks, including open-domain QA, MMLU-Pro, medical, and mathematical reasoning, show that $UR^{2}$ significantly outperforms existing RAG and RL methods on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a well-defined and significant problem: the suboptimal trade-off between retrieval and reasoning in current models . The core idea of a difficulty-aware curriculum is an intelligent and novel solution. It directly trains the model to learn when to trust its parametric knowledge versus when to seek external knowledge, a critical capability for building more efficient and robust agents.\n\n- The two-stage optimization process is a key strength . Stage 1 provides a clever solution to the \"cold start\" problem often seen when training RL policies for tool use, by first rewarding the model for simply learning the tool-calling format. Decoupling this from Stage 2 (answer correctness) likely leads to a more stable training signal and better credit assignment.\n\n- The experimental validation is exceptionally thorough and is a major strength of this work."}, "weaknesses": {"value": "- As the authors acknowledge, the experiments are limited to 8B-parameter models. If scaling to larger models is too costly, I wonder whether smaller models can also benefit from the proposed method. Since using retrievers or tools with small models can help mitigate limited memorization capacity of the smaller models [1,2], it would be important to include experiments on smaller models.\n\n- The curriculum design, while effective, introduces a significant one-time computational cost. To categorize questions by difficulty, the authors must \"perform 20 rollouts using Qwen-2.5-7B-Instruct and compute the average performance score\" for each training sample. This is a substantial preprocessing step that adds overhead before the main RL training can begin.\n\n- The framework's robustness relies heavily on the \"LLM-summarized retrieval corpus\" to handle noise, as shown by the w/o LLM Summary ablation. This introduces a dependency on another powerful LLM (e.g., GPT-4.1) during the data preparation or inference pipeline, which adds complexity, latency, and cost that are not fully accounted for in the primary model's performance.\n\n\n[1] Kang, Minki, et al. \"Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks.\" Advances in Neural Information Processing Systems 36 (2023): 48573-48602.\n\n\n[2] Kang, Minki, Jongwon Jeong, and Jaewoong Cho. \"T1: Tool-integrated self-verification for test-time compute scaling in small language models.\" arXiv preprint arXiv:2504.04718 (2025)."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GbO5QzCVV5", "forum": "8CZOn1fFvb", "replyto": "8CZOn1fFvb", "signatures": ["ICLR.cc/2026/Conference/Submission182/Reviewer_8c2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission182/Reviewer_8c2q"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039675202, "cdate": 1762039675202, "tmdate": 1762915463347, "mdate": 1762915463347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Formal Clarification — Online, On-the-Fly Summarization (Not Offline Corpus Preprocessing)"}, "comment": {"value": "This clarification is addressed **specifically to Reviewers LVGE, 46ub, and nM4H**.\n\nWe emphasize the following core fact of our system design: **UR² uses *online, on-the-fly* LLM summarization triggered at retrieval time. We do *not* pre-clean, pre-summarize, or pre-construct any offline summarized corpus.**\n\nAll summaries are produced dynamically **after** documents are retrieved, during training or inference, as part of the multi-step reasoning–retrieval loop. Summaries are *not* stored, curated, or used as a static data resource.\n\nWe respectfully request that all evaluations, concerns, and judgments regarding fairness, corpus augmentation, robustness, or corpus construction be reconsidered **under this correct premise**.\n\n---\n\n**Explicit Locations in the Paper (Cited Exactly)：**\n\n(1) Fig. 1 and Section 3.1.1：These explicitly show the retrieval → online summarization → reasoning pipeline *as clearly illustrated in Fig. 1 and Section 3.1.1.*\n\n(2) Online Retrieval Implementation (lines 1052–1091): Online HTML → Markdown → **Content Summarization** performed by GPT-4.1, proving the summarizer is an online runtime module, not an offline corpus generator.\n\n(3) Training-time Summarization Prompt (Appendix D.3): Appendix includes the exact summarization prompts used **during training**, confirming that summaries are created per retrieval call.\n\n---\n\n**Kind Request for Re-evaluation:**:\n\nGiven the explicit evidence above, we kindly request that reviewers consider these points and reconsider the associated ratings:\n\n* *LLM-summarized corpora* being treated as a **static** offline corpus\n* fairness and comparability\n* robustness to dynamic or real-world retrieval\n* misinterpretation of summaries as corpus augmentation rather than a runtime tool\n\nThese concerns rely on a premise that is **inconsistent** with the implementation clearly shown in the cited sections.\n\n---\n\nBecause the work spans multiple datasets, retrieval modes, and multi-stage training workflows, providing every detail repeatedly in the main text would significantly disrupt readability. Some details were therefore placed in figures and appendices, which may have led to partial misunderstanding. We will ensure that future versions use clearer wording to avoid such ambiguity. The cited sections above fully specify the online nature of summarization. The cited sections above fully specify the online nature of summarization."}}, "id": "gTX0U0YNJk", "forum": "8CZOn1fFvb", "replyto": "8CZOn1fFvb", "signatures": ["ICLR.cc/2026/Conference/Submission182/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission182/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission182/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763104859621, "cdate": 1763104859621, "tmdate": 1763104859621, "mdate": 1763104859621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Formal Clarification — Online, On-the-Fly Summarization (Not Offline Corpus Preprocessing)"}, "comment": {"value": "This clarification is addressed **specifically to Reviewers LVGE, 46ub, and nM4H**.\n\nWe emphasize the following core fact of our system design: **UR² uses *online, on-the-fly* LLM summarization triggered at retrieval time. We do *not* pre-clean, pre-summarize, or pre-construct any offline summarized corpus.**\n\nAll summaries are produced dynamically **after** documents are retrieved, during training or inference, as part of the multi-step reasoning–retrieval loop. Summaries are *not* stored, curated, or used as a static data resource.\n\nWe respectfully request that all evaluations, concerns, and judgments regarding fairness, corpus augmentation, robustness, or corpus construction be reconsidered **under this correct premise**.\n\n---\n\n**Explicit Locations in the Paper (Cited Exactly)：**\n\n(1) Fig. 1 and Section 3.1.1：These explicitly show the retrieval → online summarization → reasoning pipeline *as clearly illustrated in Fig. 1 and Section 3.1.1.*\n\n(2) Online Retrieval Implementation (lines 1052–1091): Online HTML → Markdown → **Content Summarization** performed by GPT-4.1, proving the summarizer is an online runtime module, not an offline corpus generator.\n\n(3) Training-time Summarization Prompt (Appendix D.3): Appendix includes the exact summarization prompts used **during training**, confirming that summaries are created per retrieval call.\n\n---\n\n**Kind Request for Re-evaluation:**\n\nGiven the explicit evidence above, we kindly request that reviewers consider these points and reconsider the associated ratings:\n\n* *LLM-summarized corpora* being treated as a **static** offline corpus\n* fairness and comparability\n* robustness to dynamic or real-world retrieval\n* misinterpretation of summaries as corpus augmentation rather than a runtime tool\n\nThese concerns rely on a premise that is **inconsistent** with the implementation clearly shown in the cited sections.\n\n---\n\nBecause the work spans multiple datasets, retrieval modes, and multi-stage training workflows, providing every detail repeatedly in the main text would significantly disrupt readability. Some details were therefore placed in figures and appendices, which may have led to partial misunderstanding. We will ensure that future versions use clearer wording to avoid such ambiguity. The cited sections above fully specify the online nature of summarization."}}, "id": "gTX0U0YNJk", "forum": "8CZOn1fFvb", "replyto": "8CZOn1fFvb", "signatures": ["ICLR.cc/2026/Conference/Submission182/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission182/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission182/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763104859621, "cdate": 1763104859621, "tmdate": 1763213319310, "mdate": 1763213319310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}