{"id": "9Vb7WnS6LB", "number": 12779, "cdate": 1758210230928, "mdate": 1759897487333, "content": {"title": "Normalized Rewards for Preference Learning", "abstract": "Direct Alignment Algorithms (DAAs) such as DPO have become a common way to post-train and align LLMs with human preferences. However, DAAs have been observed to over-optimize their implicit reward model and decrease the likelihood of preferred responses. We provide evidence for a hypothesis that the over-optimization stems in part from a mismatch in the partition function estimate of the learned model and the optimal model. In particular, transformers return a normalized distribution over tokens and therefore have a partition function of one, suggesting that the true partition function should remain fixed throughout training. However, existing DAAs do not account for this as their objectives do not include terms to optimize the partition function. To counteract this undesired side-effect of DAAs, we examine using objectives that add a regularization term to maintain the total length-normalized probabilities of the chosen and rejected responses. To better understand over optimization, we investigate how response likelihood changes are distributed over the tokens with and without regularization. We find that a significant portion of the likelihood changes are due to a small set of outlier tokens, which explains how DAAs improve generation quality despite decreasing the likelihoods of chosen responses. \nWe apply the proposed regularization to reference-based (DPO) and reference-free (SimPO) methods and find (1) improved trade-offs between generation quality and general benchmark capability and (2) improvements in reward modeling across datasets. For example, on Llama-3.1-8B-Instruct, we see both a >20% increase in AlpacaEval2 scores and >9% performance gains on general benchmarks. Additionally, we find that the added regularization term effectively mitigates the amount of displacement within preferred responses overall, and for the outlier tokens specifically, by utilizing low-likelihood tokens.", "tldr": "", "keywords": ["preference learning", "reward"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/058b017f0cf9f5343345c119f99e0fadda94779b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies over-optimization in Direct Alignment Algorithms (DAAs) like DPO and SimPO.\nThe authors propose that the over-optimization in implicit reward stems from the neglected partition function's role in normalizing the generation distribution.\nThe authors introduce a length-normalized probability conservation penalty to mitigate over-optimization.\nEmpirical results show that the proposed method can reduce over-optimization on some tasks and models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The partition function perspective provides an interesting viewpoint on the over-optimization issue in DAAs.\n2. The proposed penalty term is a direct and reasonble approach to regularize the sum of probabilities."}, "weaknesses": {"value": "1. **Clarity of Motivation**: The authors argue that over-optimization arises from neglecting the partition function $Z(x)$. However, there exist three different definitions of this partition function in the literature: the DPO & SimPO's partition function to normalize $\\pi_\\theta \\propto \\pi_{ref}\\exp(r/\\beta)$, and the partition function of LLM's softmax output. They are not conceptually equivalent, and it is unclear when the authors argue that the partition function should be $1$ (line 185) and fixed during training.\n2. **Lack of Novelty**: The preservation of probability as a regularization term has been explored in prior works, such as adding an SFT loss (which the paper should compare as a baseline). \n3. **Unstable Empirical Results**: The empirical results show that the proposed method can reduce over-optimization on some tasks and models, but not consistently across different settings."}, "questions": {"value": "Could you clarify the definition of the partition function you are using, and how it relates to the different definitions in the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iVjU7L5Nm0", "forum": "9Vb7WnS6LB", "replyto": "9Vb7WnS6LB", "signatures": ["ICLR.cc/2026/Conference/Submission12779/Reviewer_kXKD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12779/Reviewer_kXKD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761306794657, "cdate": 1761306794657, "tmdate": 1762923589378, "mdate": 1762923589378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a well-motivated and technically sound approach to mitigating over-optimization in Direct Alignment Algorithms such as DPO and SimPO by introducing a normalization-based regularization term that stabilizes the implicit reward’s partition function. The proposed methods, N-DPO and N-SimPO, effectively preserve response probability mass, reducing likelihood displacement—particularly from outlier tokens—while improving trade-offs between alignment quality and general reasoning ability. The authors support their claims with solid theoretical reasoning, detailed token-level analyses, and comprehensive experiments on multiple LLMs (Llama-3.1-8B, Mistral-7B, OLMo-7B), showing up to 20% gains on AlpacaEval2 and better benchmark retention. Overall, the work offers a simple yet impactful modification to existing preference optimization methods, enhancing both stability and generalization in post-training alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a clear, theoretically grounded explanation—reward misnormalization—for over-optimization in DAAs, and proposes a simple yet effective normalization regularizer applicable to both DPO and SimPO.\n\n* Extensive experiments across multiple LLMs (Mistral, Llama-3.1, OLMo) show consistent improvements in alignment quality and benchmark performance, supported by detailed token-level analyses."}, "weaknesses": {"value": "* Limited exploration of generality: While results are strong, the experiments are restricted to instruction-following models; broader testing on diverse domains (e.g., reasoning, coding, multimodal tasks) would strengthen claims.\n\n* Hyperparameter sensitivity: The effectiveness of the normalization term depends on tuning λ and β, but the paper offers limited discussion on robustness or practical guidance for real-world adoption."}, "questions": {"value": "1. How sensitive are the proposed N-DPO and N-SimPO methods to the choice of the regularization coefficient λ across different model sizes and datasets?\n\n2. Could the authors clarify whether enforcing a fixed partition function might inadvertently limit expressivity or adaptation in certain preference distributions?\n\n3. Have the authors explored whether the normalization regularizer can be combined with other anti–over-optimization techniques (e.g., token-level confidence weighting or KL-based constraints) for additive benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GiTnDhYGKJ", "forum": "9Vb7WnS6LB", "replyto": "9Vb7WnS6LB", "signatures": ["ICLR.cc/2026/Conference/Submission12779/Reviewer_7NZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12779/Reviewer_7NZV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428607483, "cdate": 1761428607483, "tmdate": 1762923588993, "mdate": 1762923588993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper supports a hypothesis that the over-optimization phenomenon in Direct Alignment Algorithms are mostly due to likelihood changes from outlier tokens, which explains why the likelihood of both chosen and rejected responses may decrease simultaneously. To mitigate such effect, they propose a regularization term aiming to maintain the total probability of chosen and rejected responses from shifting. The then show that the proposed approach can mitigate the over-optimization issue (the likelihood of chosen responses increase) and promising improvement in general benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed approach is adding a single regularization term, which could be easily adopted, and it works for both with and without reference algorithms.\n2. The paper gives a nice and insightful analysis on the over-optimization phenomenon."}, "weaknesses": {"value": "1. A more justified theoretical analysis on the regularization displacement is preferred. Is there an explanation on why the displacement mainly happens on the outlier tokens?\n2. I don't see a clear connection between the partition function and the regularization term. Figure 1 is confusing as there is no explanation on why per-token reward changes in such a way after adding the regularization. I also don't see why keeping the sum of probability of chosen / rejected responses close to the reference policy can lead to a better estimate on the partition function. Can the authors give more explanation / experimental analysis on this?"}, "questions": {"value": "See questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nEDP0ahC6O", "forum": "9Vb7WnS6LB", "replyto": "9Vb7WnS6LB", "signatures": ["ICLR.cc/2026/Conference/Submission12779/Reviewer_RVXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12779/Reviewer_RVXr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960356572, "cdate": 1761960356572, "tmdate": 1762923588686, "mdate": 1762923588686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"Normalized Rewards\" to reduce likelihood displacement in Direct Alignment Algorithms (DAAs). The authors frame displacement as a consequence of DAAs implicitly assuming a fixed partition function, which in reality drifts during training. They introduce a regularization term that explicitly conserves the total probability mass of the chosen and rejected responses relative to the reference model.\n\nQuestion: What if only the winning response probability were regularized with respect to the rejected response. Allow the losing response to drift?"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Insightful Partition-function Framing\nConnecting likelihood displacement to a ``partition function'' is an elegant framing of the problem. It provides a clean theoretical justification for why generative likelihoods drift than standard explanations based (say based on gradients).\n\n\n### Valuable Token-level Analysis\nSection 4.1 provides a strong contribution by analyzing how displacement happens at the token level. The finding that displacement is often driven by a small set of \"outlier tokens\" whose probabilities drop sharply is a valuable observation for the community.\n\nEDIT:\nI am raising the score further because I really like the log loss square formulation. In fact it gives a different effect than p log p. The reason is that it overweights the lower values I believe? Can you add some thoughts on difference between this and entropy (plogp)?"}, "weaknesses": {"value": "**Missing plot of the main claim**\nThe abstract explicitly states that DAAs \"decrease the likelihood of preferred responses\" and implies this method fixes it. While Figure 1 provides a high-level schematic and Figure 2 shows token-level distributions at a single snapshot, the paper lacks a direct \"before-and-after\" training curve. It would be great to see a plot of Average LogProb(Preferred) vs Training Steps for both DPO and N-DPO to confirm that the proposed regularization actually stabilizes this metric during training.\n\n**Recent methods missing**\nThe paper benchmarks against methods that achieve ~30% win rates, while recent work from late 2024 and 2025 has higher performance on the same model class. Does the regularizer here port to these settings? Does it still improve them? While I would like to see these results, at the least they should be referenced as they are now a part of literature in this area.\n\n**Simpo Baseline unclear**\nThe reported SimPO baseline for Llama-3-8B (32.79% LC WR) is significantly lower than the ~44% reported in the original SimPO (see Table 1 of that paper). Furthermore, Table 1 shows N-SimPO underperforms this (31.01%).\n\n---\n\n### References\n\n[1] Chen, H., He, G., Yuan, L., Cui, G., Su, H., & Zhu, J. (2024). Noise contrastive alignment of language models with explicit rewards. Advances in Neural Information Processing Systems, 37, 117784-117812.\n\n[2] Wu, Y., Sun, Z., Hughes, R., Ji, K., Yang, Y., & Gu, Q. (2025). Self-play preference optimization for language model alignment., International Conference on Representation Learning (Vol. 2025, pp. 91558–91582).\n\n[3] Gupta, T., et al. (2025). AMPO: Active Multi Preference Optimization for Self-play Preference Selection. ICML 2025.\n\n[4] Gupta, T., et al. (2025). REFA: Reference Free Alignment with Fine-Grained Length Control. COLM 2025.\n\n[5] Tang, X., et al. (2025). Game-Theoretic Regularized Self-Play Alignment of Large Language Models. arXiv preprint arXiv:2503.00030."}, "questions": {"value": "1. Can you concretely add the loss function used to the draft somewhere? Not just the regularizer?\n2. Your regularizer targets the sum: $\\log([\\pi_\\theta(y_w) + \\pi_\\theta(y_l)] / [\\pi_{ref}(y_w) + \\pi_{ref}(y_l)])^2$. I suspect many of the gains might come simply from preventing $y_w$ from drifting. Have you tried regularizing just the winning response: $\\log([\\pi_\\theta(y_w)] / [\\pi_{ref}(y_w)])^2$? If this simpler version works equally well, the \"partition function\" motivation (which relies on the sum representing the whole space) might be less relevant than simple SFT-style regularization.\nOn the other hand, if this simpler version does not work, it would be strong motivation of why the sum, or partition function is necessary.\n3. Apart from 2, a natural variant you may consider is also $\\mathcal{R} = \\lambda [(\\log \\pi_\\theta(y_w) - \\log \\pi_{ref}(y_w))^2 + (\\log \\pi_\\theta(y_l) - \\log \\pi_{ref}(y_l))^2]$. But i suspect you should outperform this.\n4. N-SimPO hurt performance on your strongest model, so a possible conclusion is that you used too high a lambda. Can you reduce it, and show 2-3 more values for a lambda ablation?\n\n---\n\nCan you provide training curves showing Average LogProb(Preferred) vs Steps for DPO and N-DPO? Visual proof that your method stabilizes this metric over time or at least improves this metric over the baseline DPO, along with some of the above experiments suggested, and fixed references would be good."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p3sZSngSZI", "forum": "9Vb7WnS6LB", "replyto": "9Vb7WnS6LB", "signatures": ["ICLR.cc/2026/Conference/Submission12779/Reviewer_8RpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12779/Reviewer_8RpH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762190666110, "cdate": 1762190666110, "tmdate": 1762923588406, "mdate": 1762923588406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}