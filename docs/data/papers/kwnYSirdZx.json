{"id": "kwnYSirdZx", "number": 10105, "cdate": 1758160831137, "mdate": 1763554629167, "content": {"title": "Adam Can Mitigate Class Imbalance Without Element-Wise Gradient Normalization", "abstract": "Adam has remained a dominant optimization algorithm in deep learning for a decade. Recent studies reveal that Adam mitigates the class imbalance by normalizing element-level gradients to balance gradients across classes. However, this interpretation relies on an assumption that gradients between different classes are fully orthogonal. In this paper, we further investigate the assumption. We observe that inter-class gradient orthogonality can be low, particularly during the initial training stages, yet Adam still mitigates class imbalance. This suggests that Adam may not reduce class imbalance by normalizing element-level gradients. Through the ablation of Adam, we further support that class imbalance can be alleviated without element-wise gradient normalization. This work reveals that, even with inter-class gradient coupling, Adam mitigates class imbalance by normalizing gradients across iterations. During early training, the model primarily fits high-frequency class data; as the loss for these diminishes, it adapts to low-frequency classes. Due to the inter-iteration normalization, the gradient magnitudes for low-frequency classes then approximate the initial high-frequency gradients. This mechanism helps Adam mitigate class imbalance. Consequently, we demonstrate that this mechanism necessitates at least layer-wise gradient normalization across iterations, since most neural networks exhibit layer-level inconsistencies between forward and backward propagation. Finally, we further explore potential limitations in Adam’s ability to address the inconsistencies.", "tldr": "beyond element-wise gradient balancing, Adam can balance layer-level gradients across different iterations to mitigate class imbalance.", "keywords": ["Mechanism Interpretability", "Model Optimization", "Class Imbalance"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79cd22851d6aae0f54a5d4adff876efcffe880d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper, demonstrates that Adam can mitigate class imbalance by balancing the magnitudes of gradients across iterations. It argues that the layer-wise dynamics normalization can address a layer-level in-consistency between forward propagation and back-propagation while Adam may not fully address this issue. So to tackle that the authors introduce a scaling factor proportional to the initialized weight magnitudes. In a more general sense the paper does an analysis across Adam with various theoretical and experimental results."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Very extensive and well designed experiments in different modalities and models (both image and NLP data) with very good comparisons and variations of Adam.\n- Very good presentation of the problem, the idea, and the proposal. The paper is very easy to read and digest from the audience.\n- Very nice theoretical touches in the methodology of the paper that make the paper look more complete."}, "weaknesses": {"value": "- I don't really see anything bad with the paper. I like the motivation and the analysis. The only theoretical guarantee stems in Eq. 15 if I am not mistaken right? Is there anything else that can be proven for this work? Like proof of convergence? I looked at the Appendix and couldn't find anything."}, "questions": {"value": "What about any comparisons with RMSProp? For example in Figure 1 it's unfair to compete with SGD since we know that Adam is already an improvement. But I do see the SGD as a baseline.\n\nIn general I really enjoyed reading the paper and everything was very well organized from the abstract to the Appendix. I think this is a solid contribution to ICLR this year."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sdi8nyopI8", "forum": "kwnYSirdZx", "replyto": "kwnYSirdZx", "signatures": ["ICLR.cc/2026/Conference/Submission10105/Reviewer_9dcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10105/Reviewer_9dcu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934113423, "cdate": 1761934113423, "tmdate": 1762921486769, "mdate": 1762921486769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate why Adam reduces class imbalance problems, challenging the prior hypothesis that relies on orthogonality between gradients of different classes. They perform multiple ablations and demonstrate that Adam's success comes from normalizing gradients across iterations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, and each section of the paper is presented with a clear focus. Their claims are well-supported, and the authors provide detailed intuitions and experiments. \n- The ablations are clearly presented and well-designed. Each experiment shows evidence for / against specific hypotheses. For example, to test the importance of element-wise gradient normalization, the authors compared standard Adam with a modified version (Adam-LDN) which replaces element-wise normalization with layer-wise normalization. Because these two have very similar behaviors, this disproves the hypothesis that the benefits of Adam rely on element-wise normalization. The authors also do a good job of presenting the differences between the different variants and are explicit about what behaviors are shared.   \n- Adam-S-LDN has competitive performance to Adam, indicating that the authors were able to replicate the benefits of Adam into the layer-wise dynamics normalization with rescaling to account for imbalanced initialization."}, "weaknesses": {"value": "- A significant portion of the paper is dedicated to demonstrating that gradients across classes are not orthogonal, and there are convincing experiments which illustrate this point. The author claims that these experiment demonstrate novel results which other papers disagree with. However, this seems like a strawman argument to me; my understanding of prior works is that they believe that Adam succeeds because the gradient norm and Hessian trace have strong correlation, which does not appear to be contradictory. Therefore, this decreases the novelty of this finding.\n- The paper focuses on understanding Adam for class imbalance, which is a somewhat narrow field. However, their conclusions of Adam stabilizing optimization dynamics across different iterations should extend outside of this setting."}, "questions": {"value": "- In Line 57, you claim \"Kunstner et al., 2024... relies upon the assumption that the inter-class gradients are fully orthogonal\". My understanding of this paper's claims was that Adam outperforms SGD in heavy-tailed class imbalance scenarios because the gradient norm and Hessian trace have high correlations, arguing that this enables Adam's normalization to behave similarly to diagonal preconditioning. Could you elaborate on why this conclusion assumes that the inter-class gradients are orthogonal? \n- Previous works have found that heavy-tailed class imbalance leads to a more significant performance gap between Adam and SGD. Did you do any ablations to understand the extent of the class-imbalance on the performance of Adam vs Adam-S (and other variations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mt33bjyM2K", "forum": "kwnYSirdZx", "replyto": "kwnYSirdZx", "signatures": ["ICLR.cc/2026/Conference/Submission10105/Reviewer_qEga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10105/Reviewer_qEga"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935215548, "cdate": 1761935215548, "tmdate": 1762921486309, "mdate": 1762921486309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the factors contributing to the effectiveness of the Adam Optimizer in learning on class-imbalanced datasets. The authors demonstrate that the earlier interpretation of normalization across classes is dependent on the assumption of orthogonal gradients; therefore, the interpretation is not accurate. They show that the reason for the effectiveness is more related to the inter-iteration normalization (in the layer)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly readable and understandable."}, "weaknesses": {"value": "Soundness: The authors provide plots for the training loss, but don’t give us insights into the generalization. This is problematic because the final model's usage would depend on its convergence, which is not evaluated. Hence, the effect of the proposed scaling is not observable in practice. \n\nFormalism: The main claim of normalization across classes is only analyzed in the extreme cases of the binary classification setting, as presented in Eqs. 1 and 2, which hinders the validity of the claim. Furthermore, the claims made are not supported by sound mathematical theorems and lemmas; therefore, the validity of the claims cannot be established.\n\nAnalysis of Only the Early Phase of Training: It's unclear to me how the early phase of analysis (Fig. 3) is applicable to the final results of the model."}, "questions": {"value": "The plot for Eq. 11 is based on Cosine Similarity for orthogonality, hence in Fig. 4a, it should be low value for orthogonality. However, the results seem to be counterintuitive. Could the authors please elaborate on that in more detail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jy2vGaaZ2S", "forum": "kwnYSirdZx", "replyto": "kwnYSirdZx", "signatures": ["ICLR.cc/2026/Conference/Submission10105/Reviewer_DbeQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10105/Reviewer_DbeQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014953160, "cdate": 1762014953160, "tmdate": 1762921485196, "mdate": 1762921485196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why the Adam optimizer is able to better deal with class-imbalanced data (compared to SGD).\nPrevious works suggested that Adam mitigates class imbalance because its element-wise gradient normalization approximates per-class normalization, assuming gradients from different classes are orthogonal.\nThis paper challenges that assumption and shows empirically that inter-class gradient orthogonality is often low (especially early in training).\nTo study Adam, the authors introduce a variant of Adam, Adam-LDN, which removes element-wise normalization and instead performs layer-wise dynamics normalization. \n\nOverall, I find the main argument of the paper to be convoluted. The Adam-LDN optimizer is somewhat related to ADAM but there is no proof of strong argument to explain how similar this algorithm is to ADAM so it's unclear if it is indeed a good surrogate. One part of the paper (section 4.3) also talks about layerwize normalization, but that section seems somewhat disconnected from the class imbalance problem (see questions below)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Challenges the gradient orthogonality assumption\n2. Experiments: demonstrates results across both language (GPT-2 on WikiText-103) and vision tasks showing consistency of findings."}, "weaknesses": {"value": "1. Limited novelty: the work primarily offers an interpretation and minor ablations of Adam rather than a substantially new optimizer\n2. Lack of theoretical rigor: the analysis is largely heuristic, there is no formal convergence analysis.\n3. Positioning vs related work. The proposed layer-wise scaling overlaps conceptually with prior layer-wise trust/ratio ideas and other layer-wise Adam variants (see comments/questions below)."}, "questions": {"value": "- The main argument is section 4.2 that boils down to saying that once high accuracy is achieved on 1 class, say c_1, then the relative contribution of the low-frequency class c_2 progressively dominates the weight updates. However, it seems to me that the same argument applies to normalized gradient descent if I'm not mistaken, so it's not clear to me why this explains the particular property of ADAM to mitigate class imbalance. Can you please comment on this?\n\n- The paper claims: \"To harmonize optimization dynamics, we introduce layer-specific scaling factor\". However, prior works have introduced similar scaling factors I think, e.g. LAMB introduces a layer-wise trust ratio to scale updates.\n\n- Missing prior work: the paper \"Deconstructing What Makes a Good Optimizer for Language Models\" by Zhao et al. (2024) proposes a variant called Adalayer which is a layer-wise variant of Adam.\n\n- One part of the paper (section 4.3) talks about layerwize normalization, but that section seems somewhat disconnected from the class imbalance problem. Specifically, the argument there is that scaling a layer's weights by a constant factor leaves the forward output unchanged, but rescales the gradients (inversely), which creates layer-specific imbalances that can slow optimization. This is an interesting observation about optimization dynamics in general, but it seems conceptually distinct from the paper's main question: how Adam mitigates class imbalance across classes. The link to class imbalance is only loosely implied, can you elaborate on what the connection is?\n\nSmall typos:\n- Equation (13) and (14): lr should be squared"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gNYBJRR1y3", "forum": "kwnYSirdZx", "replyto": "kwnYSirdZx", "signatures": ["ICLR.cc/2026/Conference/Submission10105/Reviewer_yQGF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10105/Reviewer_yQGF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762293740886, "cdate": 1762293740886, "tmdate": 1762921484500, "mdate": 1762921484500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}