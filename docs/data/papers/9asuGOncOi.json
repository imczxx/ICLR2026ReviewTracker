{"id": "9asuGOncOi", "number": 12117, "cdate": 1758205776472, "mdate": 1759897530861, "content": {"title": "Fast Frank–Wolfe Algorithms with Adaptive Bregman Step-Size for Weakly Convex Functions", "abstract": "We propose a Frank–Wolfe (FW) algorithm with an adaptive Bregman step-size strategy for smooth adaptable (also called: relatively smooth) (weakly-) convex functions. This means that the gradient of the objective function is not necessarily Lipschitz continuous, and we only require the smooth adaptable property. Compared to existing FW algorithms, our assumptions are less restrictive. We establish convergence guarantees in various settings, such as sublinear to linear convergence rates, depending on the assumptions for convex and nonconvex objective functions. Assuming that the objective function is weakly convex and satisfies the local quadratic growth condition, we provide both local sublinear and local linear convergence regarding the primal gap. We also propose a variant of the away-step FW algorithm using Bregman distances over polytopes. We establish global faster (up to linear) convergence for convex optimization under the Hölder error bound condition and its local linear convergence for nonconvex optimization under the local quadratic growth condition. Numerical experiments demonstrate that our proposed FW algorithms outperform existing methods.", "tldr": "", "keywords": ["Optimization", "First-order method", "Convex optimization", "Nonconvex optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abcee3c6faaea1a28c46474fdaeb0120a93a1294.pdf", "supplementary_material": "/attachment/a88175fc08df094748a5076dc2367a908716c60f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed Frank-Wolfe (FW) algorithms with adaptive Bregman step-size strategies for a class of constrained optimization problems. The objective function satisfies the smooth adaptable property with respect to some Bregman distance function and the $q$-Holder error bound condition. For convex optimization, the proposed FW algorithms achieves global linear or sublinear convergence. For weakly-convex optimization, the proposed algorithms achieves global linear or sublinear convergence if the objective function satisfies the quadratic growth condition."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated, organized and well-written.\n\n- The proposed algorithm is parameter-free (with adaptive strategy for parameters) and provides global convergence guarantees under convex and nonconvex scenarios."}, "weaknesses": {"value": "See questions below."}, "questions": {"value": "- The global linear convergence rates of the proposed algorithms only holds for specific classes of functions: $q=2$ or $q= 1+ \\nu$ or $\\nu = 1$ or for initial iterates. Though the footnotes give explanation, the presented results in Table 1 may be misleading at the first glance. I suggest to add more description in the table.\n\n\n- Theorem 5.2 requires $\\rho < \\mu$ and Theorem 5.2 requires $\\rho < \\mu \\leq L$. These assumptions seem nontrivial.  \n\n(i) It is nice to see that Example D.4 satisfy the assumption.Is there a general class of functions satisfying the assumption? \n\n(ii) $\\rho, \\mu$ and $L$ characterize the convexity of $f$ to some extent: $\\rho$ is basically the $\\rho$-smad parameter when $\\phi$ is the quadratic function (typical Euclidean distance). $\\mu$ is requiring the strong convexity over the solution set, which is related to the star-convexity. Is there relation between $\\rho, \\mu$ and $L$? For example, would some property of $\\phi$ guarantee that $\\rho \\leq L$? Inituitive discussions are also welcome. \n\n- Typo: Line 223, 'right-hand size'"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2AgIrdWTns", "forum": "9asuGOncOi", "replyto": "9asuGOncOi", "signatures": ["ICLR.cc/2026/Conference/Submission12117/Reviewer_229D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12117/Reviewer_229D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924174173, "cdate": 1761924174173, "tmdate": 1762923082715, "mdate": 1762923082715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes new Frank–Wolfe (FW) algorithms that incorporate a Bregman-based adaptive step-size rule, removing the need for Lipschitz continuity of the gradient of the objective function. The authors extend FW to L-smooth adaptable functions, broadening applicability beyond standard L-smooth settings. They achieve sublinear to linear convergence rates under weaker conditions (e.g., Hölder error bound or local quadratic growth). This submission also provide both convex and weakly convex convergence analyses and an away-step variant for polytopes with provable linear rates. The paper include numerical validation on ℓp-loss and phase retrieval problems, showing empirical improvements over classical FW and its variants. In summary, it generalizes FW theory to handle non-Lipschitz and weakly convex objectives while retaining theoretical rigor and empirical competitiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This submission has significant theoretical generalization. The relaxation from Lipschitz smoothness to relative smoothness is well motivated and aligns FW with modern Bregman-based optimization. It unifies and extends several prior frameworks, offering linear convergence under weaker assumptions.\n\nThe authors provide rigorous analysis. The theoretical sections in this submission are mathematically precise, covering: convex and weakly convex settings, both FW and away-step variants, and multiple growth conditions (HEB, quadratic growth). The mathematical proofs appear comprehensive and grounded in established geometric constants (pyramidal width, etc.). The authors also provided the adaptive Bregman step size. This adaptive scheme is elegant by extending previous results  and self-tunes both L and ν parameters. The termination proof (Remark 3.2) ensures practicality. \n\nMoreover, the authors conduct numerical experiments and the empirical results are consistent with the theoretical analysis. The Experiments on non-Lipschitz settings convincingly show faster convergence and robustness where Euclidean FW fails."}, "weaknesses": {"value": "There are no major weakness about this submission. The following is just some minor weaknesses:\n\nThe authors should add more numerical experiments. Only two primary experiments are shown (ℓp loss and phase retrieval). While results are positive, including comparisons on structured convex problems (e.g., LASSO, matrix completion) would better demonstrate generality.\n\nThis submission need more clarifications. Definitions (e.g., L-smad, kernel generating distances) are presented quickly with minimal intuition. Some long theorems could be summarized qualitatively before stating full formulas. Adding geometric or schematic illustrations (e.g., showing Bregman vs. Euclidean geometry) would enhance readability.\n\nThe authors should add more discussion of related work. The paper references many FW variants, but comparison to mirror descent or relative smoothness-based proximal algorithms is somewhat limited. Highlighting differences in oracle requirements and computational cost would clarify its niche."}, "questions": {"value": "There are no other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BiNVCmqX78", "forum": "9asuGOncOi", "replyto": "9asuGOncOi", "signatures": ["ICLR.cc/2026/Conference/Submission12117/Reviewer_4P4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12117/Reviewer_4P4Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960271251, "cdate": 1761960271251, "tmdate": 1762923082210, "mdate": 1762923082210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Frank–Wolfe (FW) algorithm with an adaptive Bregman step-size strategy. The proposed algorithm covers relatively smooth and weakly convex setups, which are broader than the conventional $L$-smooth convex setup. It achieves local linear convergence under weak convexity and the local quadratic growth condition. Moreover, when the constraint set is a polytope, the paper proposes a variant of the away-step FW algorithm that overcomes the zigzagging issue of the classical FW algorithm. The latter algorithm also achieves local linear convergence under the aforementioned assumptions. While the convergence is local in the nonconvex case, both algorithms achieve global linear convergence when the objective function is convex and satisfies the Hölder error bound condition. Finally, the paper provides numerical experiments demonstrating its efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- To the best of this reviewer's understanding, the γ-update in line 5 of Algorithm 2 and line 8 of Algorithm 3, which is motivated by equation (2.2), is a novel aspect of the proposed algorithm. It seems that this idea enables the proposed algorithms to establish convergence guarantees for the considered broad setup, and the fact that the paper indeed provides corresponding convergence results is meaningful. The paper also presents convincing experimental results.\n- The considered setup, L-smooth adaptable (i.e., both Lφ − f and Lφ + f are convex on the constraint set C), indeed appears to be an extension of the conventional setup, as mentioned in the paper.\n- The paper seems to be overall well structured."}, "weaknesses": {"value": "- **W1.** While this reviewer believes that the results are meaningful and worth the effort, the reviewer is not fully convinced of the overall technical novelty. In short, to the best of the reviewer’s understanding, this paper leverages line search to develop an adaptive step size, which is arguably a classical technique in optimization. The reviewer is curious whether there were any new challenges the authors needed to overcome to handle this setup. The reviewer would be happy to be corrected if something has been overlooked, as noted in the related question Q1."}, "questions": {"value": "- **Q1.** Could the authors elaborate on any novel proof techniques or specific technical challenges they had to overcome while establishing the results, particularly those related to the extension of the setup?\n\n- **Q2.** There exists a line of research that develops parameter-free methods without employing line search in other setups [1–5]. Do the authors anticipate particular challenges in removing the line search component from the proposed framework? If so, could they elaborate on the underlying reasons? (The reviewer thinks that this question may be out of the scope of this paper and is mainly motivated by the reviewer’s interest in hearing the authors’ intuition.)\n\n\n[1] Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent without descent. International Conference on Machine Learning, 2020.\n\n[2] Yura Malitsky and Konstantin Mishchenko. Adaptive proximal gradient method for convex optimization. Neural Information Processing Systems, 2024.\n\n[3] Puya Latafat, Andreas Themelis, Lorenzo Stella, and Panagiotis Patrinos. Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient. Mathematical Programming, 2024.\n\n[4] Tianjiao Li and Guanghui Lan. A simple uniformly optimal method without line search for convex optimization. Mathematical Programming, 2025.\n\n[5] Danqing Zhou, Shiqian Ma, and Junfeng Yang. AdaBB: Adaptive Barzilai-Borwein method for convex optimization. Mathematics of Operations Research, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7UmMcOmBxr", "forum": "9asuGOncOi", "replyto": "9asuGOncOi", "signatures": ["ICLR.cc/2026/Conference/Submission12117/Reviewer_gC98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12117/Reviewer_gC98"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995693771, "cdate": 1761995693771, "tmdate": 1762923081761, "mdate": 1762923081761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a Frank-Wolfe algorithm for constrained optimization whose objective function is convex/weakly-convex and relatively smooth, the algorithm is further equipped with adaptive Bregman stepsize and away-step specifically for polytope cases. Convergence rates are provided both convex and weakly convex cases. Numerical experiments are complemented to verify the effectiveness of the proposed algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Extend the scope beyond convex and L-smooth, which is more general and fit more practical problems.\n2. The stepsize is adaptive and \"drop-in\", which is easy to use.\n3. The writing is clear, the flow of the work is easy to follow."}, "weaknesses": {"value": "1. Even though extending into nonconvexity, the results still require some strong conditions like HEB, such conditions are still a bit strong, and lacks nontrivial examples throughout the work to verify the effectiveness.\n2. The work seems to be a combination of FW with Bregman divergence, also many existing works on EB/QG conditions and relative smoothness, the novelty in terms of techniques may be limited a bit.\n3. Line 304, \"We will now establish faster convergence rates than O(1/t) up to linear convergence depending on the choice of parameters.\", but I may argue that the acceleration comes from the problem setting (additional EB condition compared to the vanilla convex setting), rather than your parameter setting.\n4. For the nonconvex part, Theorem 5.2 and 5.3 further require $\\rho<\\mu$, which has not been verified, it would be helpful to include discussion or examples illustrating when this inequality holds, or how one might estimate these quantities in practice. This would clarify the scope of applicability of the nonconvex guarantees."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qyA77AGjcV", "forum": "9asuGOncOi", "replyto": "9asuGOncOi", "signatures": ["ICLR.cc/2026/Conference/Submission12117/Reviewer_2nKH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12117/Reviewer_2nKH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000296975, "cdate": 1762000296975, "tmdate": 1762923081370, "mdate": 1762923081370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Frank-Wolfe algorithm under relatively smooth and (weakly-) convex assumptions. The authors proposes new stepsizes that utilize Bregman distance, which generalizes standard Euclidian setting. The paper derive convergence guarantees: sublinear and local linear rates under weaker assumptions than classical Lipschitz‐gradient smoothness and strong convexity, and demonstrate experimentally that their methods outperform existing FW algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper extends the analysis of FW‐type methods to the class of (L-smad) functions and weakly convex objectives. These results are new and correct.\n2. The proposed Adaptive Bregman step-size strategy automatically adapts to L-smad constant, which does not require extensive hyper-parameter search or estimation of $L$.\n3. The paper shows not only global sublinear convergence but also local linear convergence in the convex case under a Hölder error-bound condition (HEB) and in the nonconvex case under a local quadratic growth condition. This gives stronger theoretical guarantees than many prior FW analyses."}, "weaknesses": {"value": "1. Weak-convexity, quadratic growth and HEB assumptions while being more general then previous assumption, still are strong. Under these assumptions, linear convergence rate is not surprising."}, "questions": {"value": "1. How sensitive is the performance of the adaptive Bregman step‐size strategy (Algorithm 2) to the parameters $\\beta, \\tau$,  (which control the inner loop for estimating $M$ and $\\kappa$)? Do the authors provide guidelines on tuning those for new problems?\n2. What is a complexity of Procedure step_size in Algorithm2?\n3. Do short step-sizes perform better than adaptive step-sizes on numerical experiments?\n4. In the nonconvex (weakly convex) setting, the local linear convergence assumes a local $\\mu$-quadratic growth condition. Practically, how can one check or ensure this condition holds in a given application? Also, given that there is a linear local convergence of the method, how one can identify when the algorithm reach this local neighborhood?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fSjW6KQcVw", "forum": "9asuGOncOi", "replyto": "9asuGOncOi", "signatures": ["ICLR.cc/2026/Conference/Submission12117/Reviewer_BkRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12117/Reviewer_BkRp"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044393162, "cdate": 1762044393162, "tmdate": 1762923080985, "mdate": 1762923080985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}