{"id": "pZj2DhfaVD", "number": 396, "cdate": 1756737876830, "mdate": 1763738607061, "content": {"title": "Revisiting Weight Regularization for Low-Rank Continual Learning", "abstract": "Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)—a key strategy in CL—remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement constant regardless of the number of tasks. Moreover, we provide the first systematic investigation of EWC in low-rank CL, showing that it achieves a better stability–plasticity trade-off than other low-rank methods and enables competitive performance across a wide range of trade-off points. Building on these insights, we propose EWC-LoRA, which leverages a low-rank representation to estimate parameter importance over the full-dimensional space. This design offers a practical, computational- and memory-efficient solution for CL with PTMs, and provides insights that may inform the broader application of regularization techniques within PECL. Extensive experiments on various benchmarks demonstrate the effectiveness of EWC-LoRA. On average, EWC-LoRA improves over vanilla LoRA by 8.92\\% and achieves comparable or even superior performance to other state-of-the-art low-rank CL methods.", "tldr": "", "keywords": ["Continual Learning", "Class-incremental Learning", "Weight Regularization", "Elastic Weight Consolidation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8702805653ececbcf1ab5a0ad158e74a1fc5340.pdf", "supplementary_material": "/attachment/c7a8cde02ace465c04ca1b860c22adb49575ad39.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript focused on the problem of continual learning with pre-trained model (CL-PTM), specifically, parameter-efficient continual learning with low-rank adapters. To mitigate the task inferences, this manuscript revisited the weight regularization method, which was less explored under the context of low-rank continual learning. Instead of applying task-specific LoRA modules, this manuscript proposed to adopt a shared low-rank update to keep the storage requirement constant while applying the regularization on the updated parameters with EWC. Specifically, the parameter importance in EWC was estimated over the full-dimensional space. The authors conducted experiments on the proposed method EWC-LoRA, showing that the proposed method achieved state-of-the-art performance compared to existing methods without sacrificing efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Compared to previous methods with task-independent LoRA modules, the proposed EWC-LoRA continuously merged the task-specific parameters into the previous ones. The model was trained in a parameter-efficient manner, and the maintained parameters for all previous tasks remained constant during the inference after weight merging.\n2. The proposed EWC-LoRA adopted a dynamic way to update the Fisher Information Matrix for EWC, avoiding the fixed precomputed FIM.\n3. The ablation studies were relatively complete."}, "weaknesses": {"value": "1. The proposed approach is mainly an adaptation of EWC to LoRA; the innovation lies in its parameterization and efficiency rather than a fundamentally new learning principle.\n2. The update of the accumulated FIM seems heuristic. Although the authors provided some discussions about the choices of $\\gamma$ during accumulated FIM update, it seemed that the choice of this hyperparameter was still not principled. \n3. Some recent LoRA-based PECL methods were not involved in the experimental comparisons."}, "questions": {"value": "1. Regarding the training time shown in Table 3. It seems that the per-task training time seemed almost the same as the vanilla LoRA method. However, it seems that Eq. (4) needs to compute the gradient over the full dataset. I wonder if this procedure took such little time considering it requires the backpropagation on the whole dataset.\n2. The LoRA-based baselines considered in this manuscript were InfLoRA and O-LoRA. However, recently, many studies about PECL have been published, even considering only the interference reduction in LoRA-based PECL. Could the author provide more comparisons with the methods in this stream, including but not limited to [1,2]. \n\nReferences:\n\n[1] CL-LoRA: continual low-rank adaptation for rehearsal-free class-incremental learning. CVPR 2025\n[2] BiLoRA: almost-orthogonal parameter spaces for continual learning. CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cLw3NuYNY3", "forum": "pZj2DhfaVD", "replyto": "pZj2DhfaVD", "signatures": ["ICLR.cc/2026/Conference/Submission396/Reviewer_gUkm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission396/Reviewer_gUkm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761513043895, "cdate": 1761513043895, "tmdate": 1762915510507, "mdate": 1762915510507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EWC-LoRA, a parameter-efficient CL method that combines EWC with a shared LoRA. Unlike prior low-rank CL approaches that allocate task-specific LoRA modules, EWC-LoRA regularizes a single low-rank update in the full-dimensional space using an accumulated diagonal Fisher matrix. This yields constant memory overhead, competitive accuracy, and a tunable stability–plasticity trade-off."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents an interesting idea of revisiting weight regularization in the low-rank regime, which effectively bridges traditional CL methods and PECL approaches.\n- The authors provide a comprehensive comparison of computational cost and parameter efficiency, clearly demonstrating how the proposed method performs relative to other LoRA-based continual learning baselines.\n- The proposed approach shows consistent improvement across multiple datasets and experimental setups, indicating good robustness and generalizability."}, "weaknesses": {"value": "- Accuracy of the Hessian estimation. The paper relies on the empirical Fisher information matrix to estimate the importance of weights for each task. However, as discussed in Meta-CL [1], the Fisher matrix used in EWC-based methods tends to become stale and outdated over time, leading to inaccurate importance estimation. It remains unclear whether a similar issue [1] arises in the PECL setting adopted here.\n- Effectiveness under longer task sequences. When the number of tasks increases, it is uncertain whether the accumulated Fisher matrix can still provide reliable importance estimation. Indeed, the performance drop observed on ImageNet-R with N = 20 tasks suggests that the method’s stability may degrade as the task length grows. A more detailed analysis or explanation from the authors would help clarify this behavior.\n\n[1] \"Meta continual learning revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction.\" ICLR, 2024."}, "questions": {"value": "Overall, I think this paper makes a valuable and insightful exploration by bridging traditional regularization-based CL methods with the PECL framework. However, I still have some concerns, and I hope the authors can address the issues raised in Weaknesses 1 and 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TKDxSiWlYc", "forum": "pZj2DhfaVD", "replyto": "pZj2DhfaVD", "signatures": ["ICLR.cc/2026/Conference/Submission396/Reviewer_VVBD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission396/Reviewer_VVBD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968152939, "cdate": 1761968152939, "tmdate": 1762915510249, "mdate": 1762915510249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits weight regularization in the context of low-rank continual learning (CL), specifically focusing on Elastic Weight Consolidation (EWC) applied to parameter-efficient continual learning (PECL) with pre-trained models. The authors propose EWC-LoRA, a method that regularizes a shared low-rank update using the Fisher Information Matrix (FIM) estimated in the full-dimensional space. This approach avoids the linear growth in memory with the number of tasks by maintaining a constant memory footprint. The authors provide a systematic analysis of EWC in low-rank CL, demonstrate a superior stability-plasticity trade-off, and validate their method across multiple benchmarks (CIFAR-100, DomainNet, ImageNet-R, ImageNet-A), showing an average improvement of 8.92% over vanilla LoRA and competitive or better performance compared to state-of-the-art low-rank CL methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "+ Strong theoretical grounding and extensive experiments across multiple benchmarks.\n+ Well-written with clear motivation, method, and results.\n+ Provides a memory-efficient, tunable, and high-performing solution for PECL."}, "weaknesses": {"value": "+ The Fisher estimation, though efficient, still introduces non-negligible memory overhead\n+ EWC-LoRA is indeed not very original, but the reviewer acknowledged that it is meaningful to make existing methods work in PEFT and explain why."}, "questions": {"value": "+ The paper mentions sensitivity to dataset complexity—could you discuss how to automatically select or adapt the rank for different tasks?\n+ The storage overhead of inflora as well as one efficient version of sd-lora is not increased linearly, could you explain your benefits compared with them? Or why your performance can be better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6zh6XASKpS", "forum": "pZj2DhfaVD", "replyto": "pZj2DhfaVD", "signatures": ["ICLR.cc/2026/Conference/Submission396/Reviewer_ETzF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission396/Reviewer_ETzF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982547440, "cdate": 1761982547440, "tmdate": 1762915510076, "mdate": 1762915510076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EWC-LoRA, a parameter-efficient continual learning method that integrates Elastic Weight Consolidation (EWC) with low-rank adaptation for large-scale pre-trained models. Unlike prior low-rank approaches that rely on task-specific modules, EWC-LoRA regularizes a shared low-rank update using the Fisher Information Matrix, maintaining constant memory while improving the stability–plasticity trade-off. Experiments show that EWC-LoRA outperforms vanilla LoRA by 8.92% on average and matches or surpasses previous low-rank continual learning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow.\n\n2. The investigated problem of continual learning using low-rank adaptation is important."}, "weaknesses": {"value": "1. It would be helpful if the authors could further elaborate on the paper’s main contributions. In particular, clarifying the fundamental challenge in integrating EWC with LoRA would strengthen the work.\n\n2. The experiments are primarily conducted on artificial image datasets. Including additional experiments on large language models (LLMs) would make the evaluation more comprehensive and convincing.\n\n3. The paper seems to overlook an important line of research on continual learning (continual fine-tuning) of LLMs, such as O-LoRA, LoRA-MoE, and TreeLoRA. Discussing how this work relates to or differs from these studies would provide better context and positioning.\n\n4. The performance improvements over previous methods appear relatively modest; as shown in Table 1, the accuracy increases only slightly. A more detailed analysis of these results could help clarify the practical significance of the gains.\n\n[O-LoRA] Orthogonal Subspace Learning for Language Model Continual Learning\n\nLoRA-MoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin\n\nTreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree"}, "questions": {"value": "See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MCeWet0Urf", "forum": "pZj2DhfaVD", "replyto": "pZj2DhfaVD", "signatures": ["ICLR.cc/2026/Conference/Submission396/Reviewer_UxrP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission396/Reviewer_UxrP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762409350020, "cdate": 1762409350020, "tmdate": 1762915509978, "mdate": 1762915509978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}