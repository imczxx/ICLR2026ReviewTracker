{"id": "hQrflNG2Hu", "number": 11009, "cdate": 1758186863804, "mdate": 1763101425307, "content": {"title": "All Convolution, No Attention: Designing Diffusion with Convolutions", "abstract": "Recent diffusion models increasingly favor Transformer backbones, motivated by the remarkable scalability of fully attentional architectures. Yet the locality bias, parameter efficiency, and hardware friendliness—the attributes that established ConvNets as the default vision backbone—have seen limited exploration in modern generative modeling. Here we introduce the fully convolutional diffusion model (FCDM), a ConvNeXt-inspired backbone redesigned for conditional diffusion modeling. Specifically,  FCDM employs an easily scalable U-Net hierarchy that integrates global context with fine-grained details and preserves strict convolutional locality, maximizing throughput on modern accelerators. We find that FCDM-XL, using only half the FLOPs of DiT-XL/2, achieves superior FID with 7$\\times$ and 7.5$\\times$ speedups at 256$\\times$256 and 512$\\times$512 resolutions, respectively. Our results demonstrate that modern convolutional designs remain highly competitive when scaled and properly conditioned, challenging the prevailing view that “bigger Transformers” are the sole path to better diffusion models. FCDM revives ConvNets as a compelling, computationally efficient alternative for large-scale generative vision.", "tldr": "", "keywords": ["Diffusion models", "Convolutional neural networks"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f0683202277605684c829a8492b22603be5a3b54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents FCDM, a fully convolutional backbone for diffusion models based on ConvNeXt and the AdaLN from DiT. A U-Net architecture is used. Experiments demonstrate that FCDM achieves competitive performance with substantially higher throughput compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-  FCDM achieves competitive FID with substantially higher throughput.\n-  Ablation studies on model architecture details the effect of the architecture design"}, "weaknesses": {"value": "Limited novelty and contribution:\n- This work is composition of existing methods and is highly similar to DiCo, which is also a fully convolutional diffusion model. The difference between this work and DiCo is the replacement of convolutions in DiCo with the operations in ConvNeXt. The core components of the model (ConvNeXt block, AdaLN, U-Net architecture) are all estabilished in previous works. I do not see efforts on exploring how convolutions can be adapted for generative models.\n- FID Results are very close to DiCo (Table 3, Table 4)\n\nLack of experiments and baselines:\n- Table 4 only presents ImageNet-1k 512x512 results without classifier-free guidance (CFG). Results with CFG should be reported.\n- This paper should compare with other works exploring different model architectures for diffusion models (e.g. $D^2iT$, MDT). The performance is not on par with more advanced diffusion transformers.\n- The paper claims in the introduction and Section 3 that depthwise convolutions are sufficient to outperform local attention. There should be experiments replacing depthwise convolution with local attention (window attention in Swin Transformer and neighborhood attention in NAT) to comfirm this.\n\n\nReferences:\n\nDiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling. NeurIPS 2025\n\n$D^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation. CVPR 2025\n\nMDTv2: Masked Diffusion Transformer is a Strong Image Synthesizer. https://arxiv.org/pdf/2303.14389\n\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV 2021\n\nNeighborhood Attention Transformer. CVPR 2023"}, "questions": {"value": "- What are the advantages and disadvantages of convolution over attention in diffusion model architectures? Ablations in this paper demonstrate that larger kernel sizes lead to better performance, which is consistent with previous findings that global understanding is crucial for image generation. Using pure convolution without global receptive field likely leads to sub-optimal performance. I hope the authors can consider this question carefully and revise their work for publication."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0R015rZzkE", "forum": "hQrflNG2Hu", "replyto": "hQrflNG2Hu", "signatures": ["ICLR.cc/2026/Conference/Submission11009/Reviewer_yd2h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11009/Reviewer_yd2h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761458820555, "cdate": 1761458820555, "tmdate": 1762922190011, "mdate": 1762922190011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We have decided to withdraw our submission, but we would like to sincerely thank all the reviewers for their constructive feedback, which has helped us improve our work."}}, "id": "AG3NHuqfkO", "forum": "hQrflNG2Hu", "replyto": "hQrflNG2Hu", "signatures": ["ICLR.cc/2026/Conference/Submission11009/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11009/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763101424512, "cdate": 1763101424512, "tmdate": 1763101424512, "mdate": 1763101424512, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a convolution-only architecture for image diffusion generative models, inspired by ConvNeXt design principles. It demonstrates competitive or superior performance to diffusion transformers on ImageNet 256x256 and 512x512, achieving higher image quality with lower FLOPs. The authors argue that depthwise convolutions provide local connectivity patterns similar to attention, making them an efficient alternative."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong empirical results on large-scale benchmarks (ImageNet 256, 512) with clear computational efficiency gains. \n- Simple and elegant design based on well-established convolutional backbones, improving interpretability and ease of implementation. \n- Clear comparison to transformer-based diffusion models, showing that attention is not strictly necessary for high-quality generation."}, "weaknesses": {"value": "- My main concern is the limited novelty justification. Especially, the conceptual difference from DiCo (NeurIPS 2025) is not clearly articulated (other than using ConvNeXt).\n- The claim that depthwise convolution matches local attention lacks theoretical or empirical support beyond citation. \n- Conditioning limitation: experiments focus only on class-conditional generation, leaving unclear whether the approach generalizes to text-to-image or multi-modal conditioning."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E0vRL1u4rb", "forum": "hQrflNG2Hu", "replyto": "hQrflNG2Hu", "signatures": ["ICLR.cc/2026/Conference/Submission11009/Reviewer_en2K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11009/Reviewer_en2K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743729274, "cdate": 1761743729274, "tmdate": 1762922189522, "mdate": 1762922189522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Fully Convolutional Diffusion Model (FCDM) that is free of the attention mechanism, which is one of the main component of recent diffusion model architecture. FCDM utilizes ConvNext blocks with LayerNorm replaced by AdaLN to incorporate time and class conditioning. FCDM shows superior generative quality on ImageNet 256x256 and 512x512 with fewer FLOPs and higher throughput."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall well-written and structurally sound. \n- The quantitative results are impressive. FCDM shows very promising results on ImageNet generation tasks, considering the fact that attention is completely removed.\n- FCDM does not require attention module, which has high overhead. This is surprising because the attention mechanism is the main success factor of the transformer-based model, and even the modern convolution-based model. \n- The paper conducts various ablation studies to verify the necessity for each component in FCDM."}, "weaknesses": {"value": "- FCDM can be viewed as combination DiCo with ConvNext, and the quantitative difference between DiCo and FCDM is also somewhat marginal.\n- Current trend for generative models are multi-modality, where text are always involved. LMM must be able to process text and other modality in one sequence, yielding several advantages such as easier editing etc. Although effective in visual tasks, I am not convinced that FCDM can be extended to LMM, where it must process both text and other modalities.\n- The performance is only tested on *class*-conditional task, which have only one semantic. I wonder if FCDM can handle text conditions where multiple semantics are presented."}, "questions": {"value": "- In ablation study, only convolution window size smaller than 7x7. What if the window size is larger than 7?\n- Attention allows fine-grain interaction between points and semantics in the conditioning signals. When only one semantic is presented, as in the class-conditional case, this fine-grain interaction might not be necessary. However, as mentioned in the Weakness, I wonder if this is the case when multiple semantics are presented in the condition, which is the usual case in text-to-image generation.\n- To the best of my knowledge, attention or its variants was necessary component even for U-Net model with convolution blocks. What do you think is the main contribution of FCDM that allows the complete removal of the attention block?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNDOTzBHqt", "forum": "hQrflNG2Hu", "replyto": "hQrflNG2Hu", "signatures": ["ICLR.cc/2026/Conference/Submission11009/Reviewer_6KUW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11009/Reviewer_6KUW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884684821, "cdate": 1761884684821, "tmdate": 1762922189123, "mdate": 1762922189123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Fully Convolutional Diffusion Model (FCDM), which adapts the ConvNeXt architecture into a U-Net backbone for image generation. The authors argue that this fully convolutional approach challenges the current trend of using transformer-based backbones in diffusion models. They present experiments on ImageNet, showing that their FCDM models are more computationally efficient (fewer FLOPs, higher throughput) and achieve better FID scores compared to the Diffusion Transformer (DiT) across various model scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear and valuable challenge to the prevailing view that Transformers are the only path forward for scaling diffusion models. The empirical results, within the scope of the comparison to DiT, are strong and demonstrate a compelling efficiency-performance trade-off. The proposed FCDM architecture is simple, scalable, and the experiments systematically ablate its core components, providing a solid foundation for the architectural choices made."}, "weaknesses": {"value": "- The novelty is limited. The idea of \"revisiting ConvNets for diffusion\" is not new and draws upon previous works like DiCo and DiC exploring similar directions. While FCDM uses a ConvNeXt block, the overall contribution feels more incremental than foundational (X but with Y style paper). The paper compares against DiT but fails to compare against more principled U-Net-based architectures, such as the designs from the EDM/EDM-2 papers.\n\n- The paper overstates the equivalence between its convolutional blocks and attention mechanisms. The authors draw an analogy between depthwise convolution and *local* (windowed) self-attention but conveniently ignore the role of *global* self-attention, which is widely considered the key component behind the powerful scaling and performance of transformers. CNNs are known to have limitations in modeling long-range spatial dependencies and capturing global context (eg. generating a globally coherent image of a wire mesh), a critical weakness that the paper does not address.\n\n- The evaluation is weak and potentially misleading. The comparison is largely limited to DiT (which I would consider a very weak baseline in 2025), and the primary metric is FID. Recent studies have shown that FID has worse correlation with human perception compared to metrics based on more perceptually aligned encoders like DinoV2, which provide a richer and more reliable assessment of generative model performance. The absence of these more modern metrics calls the reported state-of-the-art claims into question, especially given the previous point about global structure.\n\n- The proposed U-Net hyperparameter scaling rule (doubling channels and depth at each 2x downsampling stage) is presented as a simple and effective design choice, but it lacks a strong justification for its optimality. While the authors perform a basic ablation in Appendix G, the ablations suffer from poor FLOP matching and at the scales tested and metrics used I wouldn't say the result is anywhere near conclusive. DiT/SiT style architecture is relatively battle tested at this point and I would need to see better and larger scale ablations to be convinced this architecture is actually competitive. \n\n- There are many papers released in the last couple of months that claim to accelerate diffusion transformer training by orders of magnitudes (REPA, TREAD, REDI[1], Contrastive flow matching...). To compellingly argue that a new architecture is superior to DiT, it is no longer sufficient to compare against a vanilla training recipe. It is crucial to demonstrate that the proposed architecture can also benefit from these SOTA training methods and maintains its advantage.\n\n\n[1] https://arxiv.org/abs/2504.16064"}, "questions": {"value": "- Why are you not comparing against any \"hybrid\" unet+attention baselines? \n- Why are you not using one of the more modern metrics? (eg. dinov2-fd or dino kernel distance etc)\n- Can you conclusively show that having a fully convolutional model does not compromise long range relationships?\n- Can you show experiments at larger scales (more steps and larger models)?\n- A technique like ReDI should be directly and easily applicable to your approach as it doesn't require doing operations on the featuremaps of the models being trained. It should be also quick to test. Showing that the performance gap does not diminish/disappear vs. their baselines would make the paper stronger.\n- Doing a simple sweep of basic hparams (eg. LR, WD, warm up, EMA..) for both the proposed model and baseline and presenting the optimal configuration of both would make the paper more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wzhrkjZ8Ny", "forum": "hQrflNG2Hu", "replyto": "hQrflNG2Hu", "signatures": ["ICLR.cc/2026/Conference/Submission11009/Reviewer_URQh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11009/Reviewer_URQh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965630160, "cdate": 1761965630160, "tmdate": 1762922188635, "mdate": 1762922188635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}