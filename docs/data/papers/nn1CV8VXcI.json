{"id": "nn1CV8VXcI", "number": 23653, "cdate": 1758346798356, "mdate": 1759896802950, "content": {"title": "Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning for Large Audio-Language Models", "abstract": "Recent Large Audio-Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q\\&A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce $\\textbf{Thinking-with-Sound}$ (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with on-the-fly audio-domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively $\\textit{think}$ with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct $\\textbf{MELD-Hard1k}$, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that state-of-the-art LALMs suffer dramatic performance degradation on MELD-Hard1k, with accuracy dropping by more than 50\\% compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain 24.73\\% absolute accuracy, with improvements scaling consistently up to 36.61\\% for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems.", "tldr": "", "keywords": ["Multimodal", "Chain-of-Thought", "LALM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43b10b266a87aab7fbfaea179e2ae07dbd521cc7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Thinking-with-Sound (TwS) framework, which introduces the Audio Chain-of-Thought (Audio CoT) to LALMs. The TwS framework enables models to actively analyze and **manipulate** audio signals using tools during the reasoning process, alternating between linguistic reasoning and acoustic domain operations (such as noise reduction, pitch tracking, temporal segmentation, etc.). Unlike traditional LALMs that only treat audio as static embeddings, TwS can dynamically call tools to perform multi-step audio reasoning. This enhances the LALMs' ability to understand corrupted audio signals **without additional training**. Additionally, the authors constructed a new robustness benchmark, **MELD-Hard1k**, which evaluates models' noise resistance by applying various real-world perturbations to audio. Experimental results show that TwS significantly improves the robustness of multiple LALMs without extra training, with a maximum performance gain of 36.61%. Furthermore, the theoretical analysis section provides an explanation for the error convergence of TwS under perturbed conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors argue that TwS can significantly enhance the robustness and reasoning capabilities of LALMs without the need for retraining. Experimental results (Table 1) show that TwS achieves a substantial improvement in accuracy on distorted audio; Ablation studies (Table 2) indicate that **noise reduction** and **enhancement operators** contribute the most; And reasoning step analysis (Figure 2) proves that TwS converges with a small number of iterations while maintaining good computational efficiency.\n\nThe theoretical section (Section 3.4) formally analyzes the mechanism of TwS from the perspective of coding errors, and demonstrates that alternating reasoning and adaptive operators can reduce perturbation errors. The overall derivation logic is clear and consistent with experimental results."}, "weaknesses": {"value": "1. The theoretical section (Section 3.4) relies on strong assumptions (such as the Lipschitz continuity of the encoder and the stability of tool selection accuracy α)**, so its generalization still needs to be verified in more empirical studies.\n\n2. Notably, although the results on MELD-Hard1k fully demonstrate the improvement in robustness, the evaluation scope is **limited to emotion recognition tasks** and does not yet cover more complex audio understanding or question-answering tasks.\n\n3. The results are theoretically plausible and align with the empirical findings, but future work could test these assumptions empirically.\n\n4. The paper extends multimodal chain-of-thought reasoning (Zhang et al., 2023; Gao et al., 2025) to the audio domain and connects to tool-augmented reasoning (Toolformer, ReAct, HuggingGPT). The novelty lies in unifying these paradigms under a “training-free” audio manipulation framework.\n\n5. The experiment lacks sufficient breadth, with its covered tasks and models being too limited. The authors could attempt to enrich the experimental tasks and include more audio-language large models (LALMs)."}, "questions": {"value": "1. In Appendix C, the authors primarily adhere to two core logics when selecting operators: \"addressing the shortcomings of LALMs\" and \"adapting to specific task scenarios\". I am quite confused about how these tools fulfill their corresponding requirements. For example, how does denoising work? Suppose I blindly pass an audio sample through all the tools, then input it to the LALM and ask it to answer (without TwS). How would its performance change compared to that in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XvCOeyv1TX", "forum": "nn1CV8VXcI", "replyto": "nn1CV8VXcI", "signatures": ["ICLR.cc/2026/Conference/Submission23653/Reviewer_WmBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23653/Reviewer_WmBt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883326654, "cdate": 1761883326654, "tmdate": 1762942747818, "mdate": 1762942747818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Thinking-with-Sound (TwS), a training-free framework that enhances Large Audio-Language Models (LALMs) by enabling an interleaving of linguistic reasoning with active audio signal manipulation during inference. To test robustness, the authors construct MELD-Hard1k, a benchmark with controlled audio perturbations (noise, reverberation, pitch shift, time stretch). Experiments across multiple LALM architectures show that TwS recovers 24–37% absolute accuracy, with gains scaling positively with model size. Ablations show denoising as most impactful, and perturbation-specific analysis reveals TwS handles noise/reverberation best."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear identification of a key limitation in current LALMs, the inability to re-access/manipulate raw audio during reasoning.\n2. Detailed ablations (operator contribution, reasoning step dynamics, perturbation-specific breakdown) that give insight into where and why TwS works.\n3. The quality of the writing is good."}, "weaknesses": {"value": "1. Narrow task and evaluation scope. The evaluation is limited to MELD emotion recognition, which primarily contains speech data. There is no testing on broader, widely used standard multimodal audio benchmarks, MMAU[1] or MMAR[2], which would provide a more comprehensive assessment. The current setup lacks evaluation involving music, environmental sounds, or more complex multi-step reasoning tasks, limiting the generalizability of the conclusions to the full spectrum of audio-language understanding.\n2. Missing key related work. The paper does not cite recent closely related studies such as Audio-CoT[3] and Audio-Reasoner[4]. These works are pioneers in doing CoT for LALM, which share conceptual similarities with TwS in enabling step-by-step audio reasoning. Omitting them weakens the positioning and novelty claims. \n3. Questionable necessity of the theoretical analysis in Section 3.4. The error analysis for “Interleaved reasoning with tool calling” is largely modality-agnostic and essentially applies the same reasoning framework used in image-text models with tool integration. The only differences lie in the modality-specific signals and operator sets. This section could be moved to the appendix to streamline the main paper and focus on the core method and empirical findings. \n\n[1] MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark\n\n[2] MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix\n\n[3] Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model\n\n[4] Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models"}, "questions": {"value": "See Weaknesses. The main concern is that the evaluation is too weak. In the current landscape, widely adopted benchmarks for audio understanding (MMAU) and audio reasoning (MMAR) are already available, yet the paper chooses not to use them and instead builds its own emotion recognition task to demonstrate the effectiveness of Thinking-with-Audio. This is not very convincing. Without using established benchmarks, it is difficult to determine whether the proposed method is truly effective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ds8RlM5rQd", "forum": "nn1CV8VXcI", "replyto": "nn1CV8VXcI", "signatures": ["ICLR.cc/2026/Conference/Submission23653/Reviewer_tKyS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23653/Reviewer_tKyS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980086061, "cdate": 1761980086061, "tmdate": 1762942747599, "mdate": 1762942747599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "An interesting paper that proposes agentic  behavior facilitated by the use of tools to get prompt specific features from the audio being reasoned about via tool calling. Results are quite impressive in the limited evaluation. \n\nThere is an attempt to provide mathematical rigor, but I believe their analysis is flawed, and does not contribute to the results. Moreover, he authors seem unaware of literature around standards such as MCP facilitating tool calling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Tools such as web search, date check etc., have become common in agentic frameworks via protocols such as MCP (which the authors do not refer to!), and it would be interesting to extend to more signal processing based tools for audio, which this paper does. Results on the three models are quite good"}, "weaknesses": {"value": "I would have liked to see audio samples of the tool operations used with the paper, and also seen in the appendix a list of the decision the model made in solving a particular task. \n\nAnother question I have is if the perturbations introduced in the audio and the operators are effectively drawn from the same family, allowing for an easy reverse application and clean up?"}, "questions": {"value": "The proof of theorem 3.3 needs more careful consideration. For example, in equation 15, the left side is deterministic, and the right side is not. This needs to be written out more rigorously. Moreover, it is unclear how the result in the audio space extends to the encoded space. The Lipschitz inequality provides half of the argument but just using it will not be enough: \nE[||Enc(x^k_a)-Enc(x_a)||] \\leq L E || x^k_a -x_a|| \\leq L E|| x^0_a-x_a|| \\leq ??\n How do you get to the encoded version from here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0bn2uLpYKs", "forum": "nn1CV8VXcI", "replyto": "nn1CV8VXcI", "signatures": ["ICLR.cc/2026/Conference/Submission23653/Reviewer_anZo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23653/Reviewer_anZo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018155926, "cdate": 1762018155926, "tmdate": 1762942747366, "mdate": 1762942747366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Thinking With Sound which is a training-free framework which lets large audio language models introduce linguistic reasoning with on the fly audio operations (e.g., source separation, denoising, spectral analysis) using an Audio Chain-of-Thought. Unlike conventional approaches that rely solely on text-based reasoning, TwS empowers models to actively manipulate and analyze audio signals during the inference process, leading to more robust and adaptive reasoning under challenging acoustic conditions. The paper also introduces MELD-Hard1k, a benchmark set created by perturbing MELD utterances with additive noise, reverberation, pitch shift, and time-stretch. The paper conducts experiments to show that TwS improves LALMs’ accuracy, robustness, and scalability across different model sizes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces thinking with audio instead of thinking about audio using audio operators\n- robustness gains across different models using the technique\n- the framework is operator-agnostic, which ensures that it can adapt to arbitrary audio processing operators\n- presents a novel audio operator benchmark"}, "weaknesses": {"value": "- Results are limited to emotion classification on MELD/Hard1k, it’s unclear how TwS transfers to other audio-reasoning tasks (spatial, event, multi-clip)\n- Missing discussion of failure cases/ wrong tool calls (qualitative analysis). Where are models going wrong?\n- Wouldn't Operator Contribution Analysis make more sense if done on an independent set? Currently the results will reflect the distribution of test set created?\n- (nit) line 321 four -> five\n- The paper introduces α (tool-selection accuracy) and ρ (operator adaptivity) in its theory but does not calculate or estimate either of them in the experiments. Need a section to estimate the varialbes"}, "questions": {"value": "- I understand that the perturbations were applied with a probability of 0.3 but what's the distribution of perturbations in the actual MELD-Hard1k set?\n- With TwS 2.3× slower on Qwen-7B, how do results change under a fixed inference time budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QWu33jhxBP", "forum": "nn1CV8VXcI", "replyto": "nn1CV8VXcI", "signatures": ["ICLR.cc/2026/Conference/Submission23653/Reviewer_j535"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23653/Reviewer_j535"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762741570995, "cdate": 1762741570995, "tmdate": 1762942747226, "mdate": 1762942747226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}