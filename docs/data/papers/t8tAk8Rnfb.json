{"id": "t8tAk8Rnfb", "number": 7784, "cdate": 1758036233295, "mdate": 1759897832588, "content": {"title": "Capturing Structure and Feature Signals in Graph Self-Supervised Learning", "abstract": "This paper analyzes graph self-supervised learning (SSL) methods for node-level prediction tasks. First, we thoroughly evaluate several representative SSL methods on a diverse set of graph datasets. We observe that, contrary to prior literature, two popular generative methods MaskGAE and GraphMAE often fail to outperform well-tuned supervised baselines. At the same time, the contrastive methods BGRL and GRACE on average perform better than generative methods and supervised baselines. We hypothesize that this happens because BGRL and GRACE are able to capture the information about both graph structure and node features, while MaskGAE and GraphMAE concentrate on a single source of information. We support this hypothesis by conducting an analysis on carefully designed synthetic data. Motivated by our observations, we recommend designing SSL objectives that capture both feature and structure information. To verify the effectiveness of this approach, we propose a generative method that reconstructs both graph structure and node features. While being simple, this method is able to achieve state-of-the-art results.", "tldr": "", "keywords": ["graph neural network", "self-supervised learning", "pretraining"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01b36aa09df6422c482dbfc546146566e88a0d81.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper primarily investigates methods for node-level prediction tasks in Graph SSL. Initially, the paper conducts a thorough evaluation of several representative Graph SSL methods, revealing a surprising finding: the performance of two popular generative methods, MaskGAE and GraphMAE, often fails to surpass that of carefully tuned supervised baselines. Meanwhile, the comparative methods BGRL and GRACE exhibit superior average performance compared to both generative methods and supervised baselines. The authors hypothesize that this is because BGRL and GRACE are capable of simultaneously capturing information about graph structure and node features. Based on these observations, the authors propose a method named grasp, which jointly processes graph structure and node features through a GNN encoder, and then employs three MLP decoders to reconstruct masked edges, reconstruct original features, and predict node degrees, respectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Rigorous benchmarking. The most notable advantage of the paper lies in its comprehensive and rigorous empirical evaluation. The authors conducted experiments on 10 diverse datasets, encompassing homogeneous graphs, heterogeneous graphs, and various types of node features.  \n2. Fair tuning of the baseline: Unlike previous studies, this paper conducts a thorough hyperparameter search and architectural enhancement for supervised baselines. It is this rigor that reveals that the performance of SSL methods in previous studies may have been overestimated.  \n3. Clear motivation and concise method: Based on the analysis, the suggestion of \"capturing both structural and feature signals\" is clear in motivation and instructive."}, "weaknesses": {"value": "1. Limited innovation; this paper is more like an experimental report, which can provide some insight to researchers in graph SSL, but lacks theoretical and methodological innovation. The proposed method, GRASP, merely integrates several existing SSL tasks, which should be common in previous work and represents a relatively trivial innovation in methodology.\n\n2. Scope limited to node-level tasks: The author explicitly states in the limitations section that this study is entirely focused on node-level prediction tasks. \n3. Tuning limitations of SSL methods: Although the authors emphasize the importance of tuning, they also acknowledge that due to the high computational cost, the supervised baseline was re-optimized 10 times, while the hyperparameters of the SSL method were only optimized once. This constitutes a potential weakness in the evaluation."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AxCRdVr6Yp", "forum": "t8tAk8Rnfb", "replyto": "t8tAk8Rnfb", "signatures": ["ICLR.cc/2026/Conference/Submission7784/Reviewer_fb8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7784/Reviewer_fb8h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761274645760, "cdate": 1761274645760, "tmdate": 1762919826561, "mdate": 1762919826561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors systematically examine the effectiveness of Graph Self-Supervised Learning (GraphSSL) on node-level tasks, with a specific comparison between generative (e.g., MaskGAE, GraphMAE) and contrastive (e.g., GRACE, BGRL) paradigms. They hypothesize that superior performance is closely related to whether a model can simultaneously capture both \"structural signals and feature signals.\" Based on this, the authors further propose a new method, GrASP, and validate its effectiveness on multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper conducts extensive experiments on both homophilic and heterophilic graph data, covering different types of graph structures and node features, which enhances the generalizability of the conclusions. Furthermore, the proposed GrASP method is conceptually and implementation-wise relatively simple, yet achieves performance improvements across several benchmark tasks, demonstrating certain practical value."}, "weaknesses": {"value": "1. The analysis in the article largely relies on experimental results and lacks theoretical exploration into why simultaneously capturing structure and features is more effective.\n\n2.  Previous research [1] has shown that simple baselines can achieve strong performance with sufficient hyperparameter tuning. How can the authors ensure that the re-run baselines used for comparison with GrASP were indeed sufficiently tuned? According to Table 6 provided in Appendix A, it seems the hyperparameter search space for some models might have missed their optimal settings. For instance, the optimal mask rate for GraphMAE on Cora is reportedly 0.75, but the authors only searched within [0.5, 0.9, 0.05]. Such settings raise concerns about the reliability of the baseline results and the true source of GrASP's improvements.\n\n\n[1] Classical GNNs are Strong Baselines: Reassessing GNNs for Node Classification. NeurIPS 2024."}, "questions": {"value": "1. For GraphMAE and MaskGAE, their official implementations often use GAT as a strong backbone. Why did the article not include comparisons using a GAT base?\n\n2. In Table 2, GrASP seems to perform notably better on tabular (feature-focused) data. Is there a deeper analysis for this observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JnIcpKd6q0", "forum": "t8tAk8Rnfb", "replyto": "t8tAk8Rnfb", "signatures": ["ICLR.cc/2026/Conference/Submission7784/Reviewer_CiDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7784/Reviewer_CiDd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655688711, "cdate": 1761655688711, "tmdate": 1762919824096, "mdate": 1762919824096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates graph self-supervised learning (GSSL) and argues that most existing methods capture either structure or feature information, but rarely both, first performing a systematic benchmark of representative GSSL methods  under a unified setup, including GRACE, BGRL, GraphMAE, and MaskGAE.\n\nIt shows contrastive methods tend to capture both structure and feature signals, while generative methods often focus on one type only.\n\nBased on these insights, GrASP (Graph Attribute and Structure Prediction) is proposed, a simple generative framework that jointly reconstructs masked edges and node attributes, plus an auxiliary degree-prediction task.\nGrASP achieves state-of-the-art performance across ten benchmark datasets while being simpler and more stable than prior generative GSSL approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tComprehensive empirical study of major GSSL families (contrastive vs generative) under consistent evaluation.\n\t2.\tInsightful analysis revealing that the type of signal captured (structure vs feature) explains most performance gaps.\n\t3.\tProposed GrASP framework — a minimal joint prediction objective that unifies structure and feature reconstruction."}, "weaknesses": {"value": "1. Lack of Large-Scale Benchmarks:  All core results are on moderate-scale node-classification datasets (Cora/Citeseer/Pubmed, LastFM, Facebook, Amazon-Photo/Computers, Tolokers, Questions, Ratings). While the set is diverse (homophily/heterophily; tabular vs. homogeneous features), it does not include truly large graphs typical in production or recent GSSL scaling studies. This makes it hard to assess scalability, stability, and efficiency of GrASP and the baselines under realistic constraints (GPU memory pressure, neighbor sampling variance, long training horizons) and to validate the paper’s claims about simplicity vs. performance at scale. The paper itself frames evaluation around node-level tasks with careful tuning, but within a transductive setup and the above dataset suite. Further, I suggest some large-scale dataset like ogbn-products (≈2.4M nodes / 61M edges) as standard transductive node classification and widely used as a “large but manageable” benchmark. Without a large-scale OGB evaluation, external validity remains uncertain.\n\n2. Missing Comparison with Recent Graph SSL Models: The paper benchmarks classic GSSL methods such as GRACE, BGRL, GraphMAE, and MaskGAE, which are indeed canonical baselines. However, the field has recently shifted toward foundation-style graph representation learning, characterized by multi-modal pretraining, large-scale datasets, and Transformer-based architectures. These models, including GraphMVP, GraphMAE-2, GraphGPT and GraphFM / GROOV / UniGraph (2024–2025), aim to unify graph self-supervision under scalable, cross-domain objectives. Without comparison to such models, it remains unclear whether GrASP’s observed simplicity–performance advantage extends beyond the classical GNN-encoder regime. If these recent models cannot be evaluated, please illustrate the reason.\n\n3. The paper’s central claim is that methods that jointly capture structural and feature signals outperform those that focus on a single source—is supported empirically but lacks a formal account of why and when this principle should hold. The current narrative connects performance gaps to what information a method “captures,” based on synthetic and real-data analyses, and then proposes GrASP as a simple joint-reconstruction objective. While convincing in practice, the argument remains predominantly empirical. A theoretical lens would clarify conditions under which joint structure feature pretraining yields provable benefits (e.g., linear probing guarantees, sample complexity improvements), and when it may not."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fwgrrfw3hF", "forum": "t8tAk8Rnfb", "replyto": "t8tAk8Rnfb", "signatures": ["ICLR.cc/2026/Conference/Submission7784/Reviewer_4GfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7784/Reviewer_4GfW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731291993, "cdate": 1761731291993, "tmdate": 1762919823577, "mdate": 1762919823577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}