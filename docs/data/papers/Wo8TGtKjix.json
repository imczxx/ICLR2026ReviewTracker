{"id": "Wo8TGtKjix", "number": 20002, "cdate": 1758301346280, "mdate": 1759897006942, "content": {"title": "ML²B: Multi-Lingual ML Benchmark For AutoML", "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities in generating machine learning (ML) code, enabling end-to-end pipeline construction from natural language instructions. However, existing benchmarks for ML code generation are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice. To address this gap, we present ML²B, the first benchmark for evaluating multilingual ML code generation. ML²B consists of 30 Kaggle competitions in 13 natural languages, covering tabular, text, and image data types, with structured metadata and validated human-reviewed translations. For evaluation, we employ AIDE, an automated framework for end-to-end assessment of data science pipelines, and provide observations into the cross-lingual model performance.", "tldr": "", "keywords": ["Multilingual machine learning", "Large language models", "Cross-lingual representation learning", "Code generation", "Machine learning workflows", "Benchmark dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b95d55184427992667144ee7df2834c9357c4bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ML2B, a benchmark designed to evaluate Large Language Models (LLMs) on multilingual machine learning code generation. ML2B extends 30 Kaggle competitions by translating them into 13 languages, covering tabular, text, and image data modalities. The benchmark incorporates human-validated translations, structured metadata for prompts, and leakage-prevention techniques such as private test sets and modular code formats. Through the AIDE evaluation framework, the authors assess multiple LLMs and report a consistent performance degradation of 15–45% on non-English prompts compared to English, with the drop being particularly pronounced for low-resource languages."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Uses real-world Kaggle competitions.\n2. The translation is verified by human reviewers.\n3. The use of percentile-based leaderboard scoring is an appropriate and effective choice for normalizing performance and enabling fair comparisons across heterogeneous competitions."}, "weaknesses": {"value": "1. The translation validation process, while including human oversight, lacks quantitative quality measures such as inter-annotator agreement (IAA) scores. This makes it difficult to objectively assess the quality and consistency of the translations.\n2. The discussion of the results is largely descriptive, reporting what performance drops occurred, but fails to provide a deep, explanatory analysis of why they occurred.\n3.  While the creation of a new benchmark is a contribution, the novelty of this specific benchmark feels limited. More importantly, the paper lacks substantive insights derived from the results. It does not offer in-depth analysis of model failure modes or propose concrete strategies to mitigate the observed performance gap in multilingual settings.\n4. The overall presentation could be improved.  Additionally, some figures are difficult to interpret due to low resolution or unclear labeling, which hinders readability."}, "questions": {"value": "1. Have the authors considered evaluating more recent or capable models (e.g., GPT-5, Gemini 2.5 Pro)? Evaluating state-of-the-art models would provide a more current snapshot of LLM capabilities on this benchmark.\n2. A simple yet crucial baseline seems to be missing: what is the performance if the LLM is explicitly prompted to first translate the non-English problem description into English and then generate the solution?\n3. The superscripts (¹ and ²) used in Table 1 are not defined in the table's caption or the main body of the paper. Could you please clarify their meaning?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The authors state that human annotators who performed the crucial task of validating translations received \"no monetary compensation\" (Appendix F). Relying on significant unpaid labor for resource creation raises ethical concerns regarding fair treatment and compensation, which may conflict with the principles outlined in the ICLR Code of Ethics."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ZvTQ7XPMw", "forum": "Wo8TGtKjix", "replyto": "Wo8TGtKjix", "signatures": ["ICLR.cc/2026/Conference/Submission20002/Reviewer_pm4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20002/Reviewer_pm4b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760689613962, "cdate": 1760689613962, "tmdate": 1762932905826, "mdate": 1762932905826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ML²B, the first benchmark for evaluating multilingual ML code generation by LLMs. It expands 30 Kaggle competitions into 13 languages with validated translations, metadata, and leakage-controlled setups. Evaluation across AIDE and ML Master on 5 LLMs shows a 15–45% drop in performance on non-English tasks, especially for low-resource languages like Kazakh and Belarusian."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is the first multilingual benchmark targeting end-to-end ML pipeline generation, filling a clear gap beyond English-only datasets like MLE-Bench or DA-Code.\n- Careful translation-quality assessment across 13 languages with native speakers.\n- The discovery of performance degradation on non-English tasks is interesting."}, "weaknesses": {"value": "- The benchmarked frameworks are limited to AIDE and ML Master. The paper omits key baselines, including commercial tools such as Claude Code, Codex, and Cursor, and open-source alternatives such as OpenHands, SWE-agents, and Aider. As a comprehensive benchmark, it would be interesting to see how these major frameworks perform. Moreover, a human baseline is always good to have.\n\n- It seems the paper borrowed a lot of concepts and content from the AIDE paper. The benchmark for Kaggle competitions has been explored extensively by previous works (e.g., MLE-Bench, AutoKaggle). This undermines the paper's novelty.\n\n- The result section is hard to follow; none of these error analysis examples are provided. No conclusions are drawn from that.\n\n- The authors should at least provide some of the generated code examples to show the quality. If possible, a leaderboard figure would be beneficial.\n\n- The pie charts are poorly made and meaningless. For example, lower values are better for loss metrics, while higher values are better for accuracy metrics. How can you compare them in the same chart?\n\n- The authors should pay attention to the format of the tables. For example, Table 4 has redundant numbers, and Table 5 falls off the page."}, "questions": {"value": "- (Just for curiosity) Native speakers’ varying computer science skills may cause bias in translated benchmark tasks across languages. A sanity check—translating tasks into a target language and back into English with an LLM, then re-evaluating—can validate translation quality. Significant performance drops suggest semantic drift in that language version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMQoKCTX9C", "forum": "Wo8TGtKjix", "replyto": "Wo8TGtKjix", "signatures": ["ICLR.cc/2026/Conference/Submission20002/Reviewer_RbCc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20002/Reviewer_RbCc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910663803, "cdate": 1761910663803, "tmdate": 1762932905244, "mdate": 1762932905244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Kaggle-grounded benchmark for evaluting the AutoML code generation capability of LLMs (and agents). Importantly, the authors selected the seed English problems and translated into multiple (major and minor) languages to assess the performance when presented with non-English problem context.\nThe performance is evaluated as the score percentile in the leaderboard, which allows for a consistent measure across various Kaggle tasks. Mixed results across languages suggest data leakage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Important contribution to expand the coverage of the tasks to non-English languages\n* Comprehensive descriptions on the dataset construction process"}, "weaknesses": {"value": "* Paper organization needs improvement.\n  * Majority of the paper discuss the data construction, which I agree is important to clarify and be transparent, but the findings and experimental resuluts should be expanded even more in the main body.\n  * One of the main results (Table 4) is mentioned in the main text, but appears in the Appendix. The authors should organize tables and figures within the main text (9 pages) if they are discussed primarily in the main text.\n* I might be misinterpreting, but Table 4 doesn't seem to indicate that English consistently yields lower percentiles than other languages. I'd be happy to be corrected about this. \n* The motivation and overall process are sound to me. For the paper to be more appropriate for ICLR, I would suggest the content in the main text to focus more on the performance analysis in cross-lingual settings instead of the Related work and the data curation process. I commend the authors for the level of details in both related work and the data construction -- I just think some portion could be compressed or be included in the appendix."}, "questions": {"value": "* Table 4 has strange \"41.8cm\" and \"31.8cm\" before the model names. Perhaps it's incorrectly escaped LaTeX."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LWV9NBjzJi", "forum": "Wo8TGtKjix", "replyto": "Wo8TGtKjix", "signatures": ["ICLR.cc/2026/Conference/Submission20002/Reviewer_QtnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20002/Reviewer_QtnP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955207273, "cdate": 1761955207273, "tmdate": 1762932904481, "mdate": 1762932904481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ML²B is a new benchmark for multilingual ML code generation: the authors curate 30 Kaggle competitions (24 public + 6 private), translate the task descriptions / data cards into 13 natural languages, and build an end-to-end evaluation pipeline (BenchPipeline + Docker runtime + AST-based grader). They evaluate five LLMs across two execution frameworks (AIDE and ML-Master hybrids), report systematic performance drops (claimed 15–45%) on non-English prompts, show domain-dependent variability, and run static leakage analysis on generated code. The benchmark includes structured metadata and human review of translations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is strong and timely. The paper identifies a real blind spot: existing ML code-generation benchmarks are English-only while real users and problem descriptions could be multilingual. \n\n2. Using Kaggle competitions and executing generated code (via Docker / AST transformations and a grader) is closer to practical utility than small snippet tests.\n\n3. Covering 13 natural languages (including low-resource ones like Kazakh, Belarusian, Romanian) is valuable and exposes cross-lingual weaknesses that English-centric benchmarks hide."}, "weaknesses": {"value": "1. Small/possibly biased task set where 30 competitions is useful but relatively small. It’s unclear how representative the selected 24 public + 6 private competitions are of Kaggle’s diversity. The selection process relies on LLM filters and manual review which can introduce bias but details are sparse (Appendix D doesn't give a clear picture on how the diversity in task selection is ensured).\n\n2. Translations were generated with GPT-4o and validated by a single native-speaker annotator per text (no compensation). The authors state they relied on a single assessment for each text rather than multiple independent annotators. This opens risk of inconsistent quality, annotator bias, and insufficient inter-annotator reliability measurement. For a benchmark whose core variable is language, translation quality control needs to be stronger (random audits, multiple annotators, kappa statistics, or quantitative MT metrics).\n\n3. The authors use LLMs (GPT variants) both to create/standardize task descriptions and to translate. That raises two concerns: (a) the dataset creation process could inadvertently incorporate phrasing that favors certain LLMs (e.g., GPT family) and (b) the benchmark might not be neutral if authors used models that overlap with models under test. The authors partly mitigate by keeping 6 private tasks and manual review, but the risk should be acknowledged and quantified more thoroughly.\n\n4. It’s unclear whether prompts, temperature, decoding choices, and instruction templates were controlled or tuned per language. LLM performance is often prompt-sensitive; different languages might require language-specific prompt engineering. If the same prompt template (translated mechanically) was used, that may disadvantage non-English languages. The paper should document prompt templates and any language-specific prompt adaptation (or explicitly justify not doing so).\n\n5. Six private tasks help with benchmark-to-pretraining leakage, but six may not be enough to make general claims about un-seen tasks; the authors themselves recommend expanding private tasks.\n\n6. Translators/annotators performed unpaid validation. This raises concerns and potential quality/reliability issues. For a benchmark of broad impact, the annotation process should be reproducible and ethical (compensated, multiple annotators, quality checks). \n\n7. Important experimental details (complete list of tasks, per-task results, prompt templates) are relegated to the Appendix\n\n8. The dataset construction depends solely on Kaggle competitions. While Kaggle offers well-structured ML problems, this narrow source might constrain the diversity of domains, task formulations, and data modalities included. Many real-world ML workflows differ substantially from Kaggle settings. It would strengthen the paper to justify this design choice and discuss whether the benchmark’s conclusions generalize beyond Kaggle-style supervised learning tasks."}, "questions": {"value": "1. How were the 30 Kaggle competitions (24 public + 6 private) selected, and what measures were taken to ensure diversity across domains, data modalities, and task types? Given the exclusive reliance on Kaggle, how do the authors assess the benchmark’s generalizability beyond Kaggle-style supervised ML problems?\n\n2. Since translations were generated with GPT-4o and validated by a single native-speaker annotator per text, how do the authors ensure translation consistency and reliability across languages? Were any inter-annotator checks, audits, or quantitative MT quality metrics (e.g., BLEU, COMET) performed or planned?\n\n3. Because GPT-family models were used both to standardize task descriptions and to translate them, how do the authors mitigate potential bias that could favor similar models in evaluation? Have alternative translation or standardization pipelines been tested to confirm benchmark neutrality?\n\n4. Were prompt templates, decoding settings, and temperature parameters controlled or tuned per language? If identical English templates were directly translated, how do the authors ensure linguistic equivalence and fairness given prompt sensitivity across languages?\n\n5. Do the six private tasks provide sufficient coverage for assessing pretraining leakage, and are there plans to expand them? Additionally, could the authors clarify the ethical considerations of unpaid annotation work and confirm whether full task lists, prompt templates, and evaluation artifacts will be publicly released for reproducibility?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Unpaid annotators"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T1LlJMIufm", "forum": "Wo8TGtKjix", "replyto": "Wo8TGtKjix", "signatures": ["ICLR.cc/2026/Conference/Submission20002/Reviewer_Ader"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20002/Reviewer_Ader"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971332429, "cdate": 1761971332429, "tmdate": 1762932903624, "mdate": 1762932903624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}