{"id": "JdB8CJkiv2", "number": 12003, "cdate": 1758205142297, "mdate": 1759897540047, "content": {"title": "PAMELA: Probabilistic Amplification Module for Lightweight AI on LEO Satellites", "abstract": "Onboard AI inference in low Earth orbit (LEO) satellites enables rapid analysis of Earth observation data for time-critical applications such as wildfire monitoring. Yet, two key challenges remain: (i) critical fire signals are extremely sparse, often confined to only a few pixels, and (ii) onboard models must be highly compact due to the stringent hardware constraints of LEO platforms. As a result, small models may report deceptively high overall accuracy by overfitting to background regions, while suffering from poor precision and recall on fire pixels. To overcome these limitations, we introduce PAMELA, a lightweight and modular amplification framework tailored to enhance the performance of onboard wildfire detection models under resource-constrained conditions. PAMELA employs probabilistic modeling to selectively amplify informative channels and pixels, allowing compact models to better capture sparse but mission-critical signals within complex satellite imagery. Experimental results demonstrate that PAMELA consistently improves detection quality, delivering 1.2 ~ 2× higher F1 scores compared to compressed baselines while simultaneously reducing model size by over 90%. To the best of our knowledge, PAMELA is the first framework explicitly designed to enable reliable onboard wildfire detection in LEO satellites.", "tldr": "", "keywords": ["Machine Learning", "LEO Satellites", "Onboard AI Inference"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8098cb37bd1c027bee8f49c02cc45398fc199c89.pdf", "supplementary_material": "/attachment/bb5eaf4c1883b14d07b44a3fa50539e2d85d2e60.zip"}, "replies": [{"content": {"summary": {"value": "The work addresses challenges in onboard AI for wildfire detection, such as sparse fire signal data and the need for compact models, by introducing PAMELA, a model-agnostic framework that improves the performance of compressed AI models. PAMELA applies learnable channel and pixel reweighting before the encoder, trained jointly with the base model to amplify informative features. Experimental results show a significant increase in performance (1.2–2× in F1 score) for common compressed models with PAMELA compared to without it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tTargets a relevant problem: onboard AI for LEO satellites, especially for time-critical applications like wildfire detection.\n-\tClear introduction and strong motivation.\n-\tPresents a well-engineered, modular, and model-agnostic solution. PAMELA is a lightweight plug-in module for sparse-signal enhancement, and its application to onboard Earth observation AI is novel and practically valuable. \n-\tExperimental setup includes non-compressed model baselines, three common model compression techniques, and a relevant set of metrics, demonstrating how PAMELA improves performance of these compressed models.\n-\tProvides an insightful ablation study exploring the impact of individual PAMELA components."}, "weaknesses": {"value": "-\tThe underlying mechanisms of PAMELA build on existing works on channel filtering and probabilistic weighting and relate to channel/spatial attention blocks. It should be discussed how PAMELA differs from these methods in Related Work.\n-\tThe paper does not benchmark against alternative reweighting or attention-based modules that can be applied before the backbone.\n-\tSegmentation benchmarking is based on a small ~2500-sample dataset, limited locally (Australia) and temporally (2019–2020). The appendix only includes an additional classification task; for reliable conclusions the benchmarking should be diversified.\n-\tMain paper results show generally low IoU (mostly below 20%), raising questions about remaining gap to operational reliability of the benchmarked methods.\n-\tPlease review the official paper format (e.g., table captions)."}, "questions": {"value": "-\tHow do other learnable reweighing methods perform compared to PAMELA? (See Weakness above)\n-\tDo you expect methods like PAMELA to be more sensitive to noise or sensor calibration shifts in the input data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uU2vxl0ppI", "forum": "JdB8CJkiv2", "replyto": "JdB8CJkiv2", "signatures": ["ICLR.cc/2026/Conference/Submission12003/Reviewer_Q1nT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12003/Reviewer_Q1nT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761213545972, "cdate": 1761213545972, "tmdate": 1762922994721, "mdate": 1762922994721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Probabilistic Amplification ModulE for Lightweight AI on LEO Satellites (PAMELA), a method to generate on-satellite predictions of wildfire, by tacking key challenges of limited model size for on-satellite methods and small visual signature (only a few pixels) for many fires. There are two core models within the PAMELA framework: a channel \"filter\" which reweights channels in the original image (possibly dropping some) and a probabilistic amplifier that amplifies pixels regions within the image that have high local deviation around them. The methodology seems sound and related to \"old school\" filter based methods in computer vision. Experiments are conducted on a single datasets with some relevant baselines, but several relevant baselines are missing in my opinion (see weaknesses below)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is well motivated!\n2. The method seems sound and is mostly well described, leveraging basic principles from computer vision though see my questions below for something I was confused about.\n3. The experimental evaluations are measured across multiple metrics and computational footprint is measured in GFLOPs which is central to the paper's goals, so I'm glad to see it was assessed.\n4. Experimental evaluation uses an established dataset which should in theory aid in comparing to previous proposed methods."}, "weaknesses": {"value": "1. Several experiment details are missing (see my questions below). If full-PAMELA is the main method, why is it omitted from table 1? It's hard to follow what is going on with full-pamela, compressed model w/ pamela, etc. It's possible I missed where in the paper this is all explained, in which case I am willing to update my score.\n2. Formatting issues below - some are major.\n3. It seems like some crucial benchmarks are missing. Could the authors please comment on a) what is the best achieved performance of any method for the evaluation dataset (and the GFLOPs required), b) could a comparison to \"hand crafted\" feature selection be shown, given that -- as the authors state -- certain spectral bands are known a priori to be important for wildfire detection?\n\nFormatting\n1. Throughout, there should be /citep instead of /cite when not actively calling out the author names. This is a consistent issue throughout that should have been caught during basic editing.\n2. There's a tilde in the abstract that should probably be a dash\n3. the captions for parts (c) and (d) are switched in figure 1"}, "questions": {"value": "In addition to any weaknesses that can be addressed from above, please address (listed in order of importance for me to possibly change my score):\n1. The IoU and F1 score numbers in table 1 and figure 2 don't line up -- are the means in the figure meant to correspond with the values in the table? I'm very confused about why they don't match, please explain. Which of the methods in table 1 is the \"compressed method\" and which of the pamela methods is the \"pamela\" method in table 2? And same question for figure 3. \n2. Follow up on 1: what is the difference between \"full PAMELA\" (in table 2) and all the compression + PAMELA methods in table 1. Forgive me if I missed it but where is all this explained in the text?\n3. In section 4.1 is says the compressed models have computational footprint of 10^8-10^9 GFLOPS, but the range in figure 1 for the \"original model\" is only 10^2ish. How is the compressed model many orders of magnitude more costly?\n\nSuggestions:\n1. The way the main result in the abstract is presented \"Experimental results demonstrate that PAMELA consistently\nimproves detection quality, delivering 1.2 ∼ 2× higher F1 scores compared to compressed baselines while simultaneously reducing model size by over 90%\" is misleading. F1 scores are higher than the compressed baselines, but the model size reduction is in comparison to a full model. The authors should rephrase, especially the use of \"simultaneously\" here is extremely misleading.\n2. The related work should be moved up to right after the intro. In this paper, there's no reason to defer it to the end.\n3. After eq. 4, there is confusing language regarding the interpretation of a Gaussian filter. E.g., \"mean and variance parameters which are learnable and optimized during training... This formulation assigns higher probability values to pixels whose intensity is closer to the expected value µn, effectively amplifying pixels that are more likely to be task-critical.\" the expected value of what?  I assume you mean the mean value of a relevant pixel but this isn't specified at all -- but the alternative, that pixels values close to the average within an image would be high probability to be fire, doesn't make sense unless I'm missing something."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Idpn3SMLfz", "forum": "JdB8CJkiv2", "replyto": "JdB8CJkiv2", "signatures": ["ICLR.cc/2026/Conference/Submission12003/Reviewer_1ELF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12003/Reviewer_1ELF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674416038, "cdate": 1761674416038, "tmdate": 1762922994223, "mdate": 1762922994223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of onboard wildfire detection in low Earth orbit (LEO) satellites, where the task is constrained by extremely sparse fire signals (often confined to only a few pixels) and stringent hardware limitations. The authors introduce PAMELA, a lightweight, modular Probabilistic Amplification Module designed to enhance the performance of compressed models. PAMELA operates through three core components: a Channel Filter that selects informative spectral bands, a Probabilistic Amplifier that generates pixel-wise likelihood maps, and a Reweighting Block that creates a focus map to amplify task-critical regions. The module is trained end-to-end with the backbone model. Extensive experiments on the Sen2Fire dataset demonstrate that PAMELA consistently improves detection quality, achieving 1.2∼2× higher F1 scores across various compressed backbones (FCN, U-Net, SegNet, SegFormer) while reducing model size by over 90%. The authors also claim PAMELA offers improved interpretability and shows promising generalization to other tasks like disaster assessment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In terms of originality, the work challenges the conventional wisdom that \"model compression inevitably leads to performance degradation\" by proposing a novel synergistic framework of \"probabilistic amplification + compressed models.\" It introduces PAMELA, the first framework specifically designed for onboard wildfire detection in LEO satellites. Its core component, the Probabilistic Amplifier, integrates Gaussian probability modeling with localized contrast-aware normalization. This design is not only mathematically rigorous but also adaptively enhances sparse wildfire signals, distinguishing it fundamentally from traditional fixed-weight attention mechanisms and demonstrating clear innovation.\n\n2. Regarding methodological quality, the experimental design is comprehensive, covering multiple dimensions including compression strategies, backbone architectures, and hardware environments. The evaluation is thorough on the Sen2Fire dataset, which contains 2466 samples. The adoption of an F1-based loss function effectively addresses class imbalance issues inherent in wildfire detection, thereby enhancing the reliability of the results. Furthermore, ablation studies and cross-task generalization tests provide strong evidence for the method's stability and generalizability, ruling out the possibility of coincidental results.\n\n3. The paper also excels in clarity of presentation. The mathematical definitions for each module are precise (e.g., Equations 1-3 for channel filtering, Equations 4-7 for probability calculation), and the pseudo-code provided in Appendix E details the overall workflow, ensuring high reproducibility. The figures and diagrams are clearly annotated and effectively illustrate key findings, such as demonstrating PAMELA's role in stabilizing performance under aggressive compression.\n\n4. The work holds substantial academic and practical significance. On a practical level, it achieves a model size reduction of over 90% while utilizing inference hardware that aligns with actual onboard resource constraints, offering a feasible solution for deploying lightweight AI on small satellites. Academically, it effectively bridges the fields of \"deep learning compression\" and \"remote sensing signal processing,\" providing a modular design paradigm that paves the way for future onboard multi-task AI systems, such as integrated wildfire detection and vegetation monitoring."}, "weaknesses": {"value": "First, the validation of dataset generalization is insufficient. The experiments are solely based on the Sen2Fire dataset from the Australian 2019-2020 wildfire season, lacking testing across other geographical regions or different wildfire types. Since wildfires in different areas exhibit variations in spectral characteristics and burning conditions, PAMELA's performance in unseen scenarios may degrade. It is recommended to supplement cross-dataset experiments, for instance, using more diverse datasets such as WF-CCD or FireSat for further validation.\n\nSecond, the validation under realistic onboard deployment conditions is not yet fully addressed. Although the experiments utilized a Raspberry Pi 4B to simulate the onboard computing environment, real LEO satellite platforms face complex constraints including radiation interference, power consumption fluctuations, and heterogeneous computing units. The paper does not provide stability tests under extreme temperature ranges or dynamic computational resource conditions, nor does it mention collaborative validation with satellite hardware manufacturers. Therefore, its \"onboard deployment feasibility\" still requires further empirical support.\n\nFinally, the performance evaluation lacks completeness in several dimensions. On one hand, the backbone models used in the experiments are mostly traditional segmentation architectures (e.g., FCN, SegNet, U-Net), and even SegFormer is not a lightweight variant specifically designed for edge devices. The absence of comparisons with state-of-the-art lightweight backbones (e.g., MobileNetV3, EfficientNet-Lite, Swin-T) makes it difficult to comprehensively assess PAMELA's added value on modern architectures. On the other hand, the paper only uses GFLOPs and parameter count as efficiency metrics, without providing actual inference time or energy consumption data on the simulated hardware. These metrics are crucial for the real-time performance and battery life of onboard AI, and the current evaluation is insufficient to fully support its \"onboard practicality.\""}, "questions": {"value": "1. Dataset Generalization: Do the authors plan to validate PAMELA on other wildfire datasets or under extreme scenarios? If such tests have already been conducted, could the results be supplemented to demonstrate the method's cross-scene robustness?\n\n2. Real Onboard Deployment: Do the authors have plans to collaborate with satellite manufacturers to test PAMELA on actual LEO satellites? If so, could the testing timeline and expected metrics be clarified?\n\n3. Lightweight Backbone Comparison: What is the reason for not incorporating comparisons with state-of-the-art lightweight backbones like MobileNetV3 or EfficientNet-Lite? Is it due to compatibility issues or experimental resource limitations? If such comparisons were added, would they affect the conclusion regarding PAMELA's performance advantages?\n\n4. Inference Time and Energy Consumption: Could the authors provide the single-image inference time and energy consumption per inference cycle for the PAMELA-enhanced model on Raspberry Pi 4B? These metrics are critical for onboard deployment, and the current data are insufficient to support the claim of \"real-time low energy consumption.\"\n\n5. PA Module Parameter Sensitivity: The parameters μ and σ in the Probabilistic Amplifier are learned during training. Have the authors analyzed the sensitivity of these parameters to different wildfire scenarios? If scene variations cause shifts in μ/σ, would PAMELA require retraining, or does it support online adaptive adjustment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W7k3sLdtHu", "forum": "JdB8CJkiv2", "replyto": "JdB8CJkiv2", "signatures": ["ICLR.cc/2026/Conference/Submission12003/Reviewer_PH1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12003/Reviewer_PH1Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912706530, "cdate": 1761912706530, "tmdate": 1762922993579, "mdate": 1762922993579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PAMELA, a multi-stage learnable inhibition module designed to identify informative satellite channels and suppress irrelevant spatial regions, thereby enabling lightweight segmentation networks to detect active fire pixels with reduced compute and memory demands. The method computes channel-wise and spatial normalizations, aggregates them, and applies gating to inhibit non-informative pixels. PAMELA is integrated into thin/pruned/distilled variants of four common segmentation backbones and is shown to mitigate the performance degradation typically associated with model compression."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a high-impact, real-world problem: real-time wildfire detection from satellite data under compute constraints, which is highly relevant as future wildfire monitoring pipelines may require on-board inference on LEO satellites.\n- Proposing a learnable module that promotes channel relevance and spatial inhibition is an interesting direction for efficient earth observation ML."}, "weaknesses": {"value": "1. Missing critical baselines & label verification\nThe paper appears to use MODIS active-fire detections as ground-truth labels. It is unclear whether these MODIS labels were manually validated or cleaned. Without validation, the method may implicitly learn MODIS’s failure modes. Thus MODIS itself is a baseline algorithm, What is the compute footprint of MODIS?\n\nAlso another interesting and informative baseline would be whether input-channel reduction alone yields similar gains (baseline ablation)\n\n2) Lack of qualitative visualizations\nThe paper would be strengthened by visual examples of:\n\n- The estimated W_{h, w} (Equation 9) for individual inputs\n- Examples Ground Truth, Prediction masks with and w/o PAMELA for a selection of the segmentation networks\n\nA natural question: can binary segmentation masks be generated directly from W_{h, w}? If so, how does that performance compare to the network output?\n\n3) Channel selection differentiability\nThe method description suggests channel selection happens early, but it is unclear how gradients propagate:\n- What mechanism approximates differentiable channel selection?\n- Is this a hard mask, soft gating, Gumbel/STE, or other relaxation? There is an explanation in the appendix but it does not address back-prop through the operation.\nA clear explanation is needed for reproducibility.\n\n4) Reference list quality concerns\nSeveral references appear incorrect. This strongly suggests a lack of reference verification.\nExamples:\n- Smith et al. 2025 was published in 2020\n- Rahman et al. 2023 cannot be located\n- The cited Ramos et al. 2025 IJRS paper does not exist\n- Xu et al. 2024 is an IGARSS paper, not CVPR\n\nThese issues must be corrected, as they undermine confidence in the rest of the paper."}, "questions": {"value": "Questions:\n\n- Appendix F: The loss appears to be the soft Dice loss. Please confirm and state explicitly. If so there is no need for a detail explanation.\n\n- Initialization: How are PAMELA parameters initialized? Is performance sensitive to this initialization?\n\n- X^{multi} is defined in equation (5) and then never used again. Or is there a typo in equation (6) and X_{n,h,w} should be X^{multi}_{n,h,w}?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zuG2hVDeoB", "forum": "JdB8CJkiv2", "replyto": "JdB8CJkiv2", "signatures": ["ICLR.cc/2026/Conference/Submission12003/Reviewer_CBWb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12003/Reviewer_CBWb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12003/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997687981, "cdate": 1761997687981, "tmdate": 1762922992953, "mdate": 1762922992953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}