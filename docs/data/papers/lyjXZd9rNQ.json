{"id": "lyjXZd9rNQ", "number": 14790, "cdate": 1758243773656, "mdate": 1759897349390, "content": {"title": "Is Optimal Transport Necessary for Inverse Reinforcement Learning?", "abstract": "Inverse Reinforcement Learning (IRL) aims to recover a reward function from expert demonstrations. Recently,  Optimal Transport (OT) methods have been successfully deployed to align trajectories and infer rewards. While OT-based methods have shown strong empirical results, they are conceptually complicated, requiring the solution of the OT optimization problem. In this work, we challenge the necessity of OT in IRL by proposing two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns rewards based on the nearest expert state regardless of temporal order; and (2) Segment-Matching Reward, which incorporates lightweight temporal alignment by matching agent states to corresponding segments in the expert trajectory. These methods circumvent optimization, exhibit linear-time complexity, and are easy to implement. Through extensive evaluations across 32 online and offline benchmarks with three reinforcement learning algorithms, we show that our simple rewards match or outperform recent OT-based approaches. Our findings suggest that the core benefits of OT may arise from basic proximity alignment rather than its optimal coupling formulation, advocating for reevaluation of optimal transport in future IRL design.", "tldr": "", "keywords": ["reinforcement learning", "reward assignment", "inverse rl", "optimal transport"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4abc65e5476bd479378062a0444d787484933b9c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that optimal transport is computationally expensive and unnecessary for IRL and proposes two simpler reward (minimum-distance reward and segment-matching reward) that are easier to obtain. The paper also provides theoretical guarantee of the proposed techique, particularly that the proposed simpler reward does not perform worse than the OT reward."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes two novel reward design. Compared to the OT reward, the proposed reward is simpler to obtain and the authors provide theoretical guarantees that the proposed reward does not perform worse than the OT reward."}, "weaknesses": {"value": "1. I find the setting is different from standard IRL. In standard IRL, the demonstrations only include expert trajectories and the reward and policy are updated iteratively. In this paper, the demonstrations include both expert and nonexpert trajectories and reward learning and policy learning are performed in a sequential fashion. I am wondering the benefits of doing in this way? Intuitively, this setting is more restrictive as it requires nonexpert trajectories and I suspect that there is an implicit assumption on the quality of the nonexpert trajectories. If the nonexpert trajectories are very bad, e.g., generated by a random policy, I do not think this contrastive reward can be much useful.\n\n2. The proposed approaches just, in essence, match individual states. They ignore the causal relation between states in an MDP, i.e., next state depends on the current state. The causal relation between consecutive states within a trajectory is not utilized. In that sense, for the first approach, there is no need to collect trajectories, you can just sample some states from the stationary state distribution of the expert/non-expert policies."}, "questions": {"value": "1. Can the authors elaborate the benefits of IRL in this paper compared to the standard IRL setting?\n\n2. Can the authors discuss how to use the causal relation between consecutive states to learn a reward in this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mXSk6EVZku", "forum": "lyjXZd9rNQ", "replyto": "lyjXZd9rNQ", "signatures": ["ICLR.cc/2026/Conference/Submission14790/Reviewer_WRY6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14790/Reviewer_WRY6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972815287, "cdate": 1760972815287, "tmdate": 1762925141857, "mdate": 1762925141857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper questions the value of optimal transport (OT) in offline reinforcement learning, proposing that much simpler methods are equally effective. The authors introduce two alternatives: 1) Minimal-Distance Reward and 2) Segment-Matching Reward. Empirical results on the MuJoCo benchmark suggest that these proposed methods perform on par with recent OT-based approaches for IRL and offline RL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1)\tThe motivation is clear, and the paper is well-written and easy to follow. \n2)\tThe proposed approaches are conceptually simple and perform competitively with more complex adversarial OT methods."}, "weaknesses": {"value": "1)\tThe empirical support for the proposed methods is inconsistent across benchmarks. On MuJoCo, Segment-Matching performs best, while on Antmaze, Minimal-Distance is superior. This variability limits the methods' applicability to more complex benchmarks, as it necessitates training and comparing both approaches to determine the best one.\n2)\tThe paper lacks a discussion on why more complex min-max OT solutions are not more robust for IRL, resulting in only marginal performance gains compared to the much simpler Minimal-Distance method. Is the problem in Sinkhorn entropy regularization, estimation of Kantorovich potentials (for neural case)? \n3)\tNo discussion on application of more robust neural OT approaches for IL. Thus, it is not clear whether proposed methods beat only Sinkhorn-like OT solvers."}, "questions": {"value": "1)\tGiven the results, can the authors definitively claim that OT-based methods are not useful for offline RL/IRL? Should future works adopt Minimal-Distance and Segment-Matching rewards and consider min-max OT optimization obsolete? While Theorem 1 seems to support this, the empirical evidence is not uniformly conclusive. \n\n2)\tI would appreciate a discussion on the intuitive reasons for the underwhelming performance of min-max OT approaches (which only outperform by a maximum of 10-15%). Intuitively, incorporating temporal context for closeness comparison (as in TemporalOT and Segment-Matching) should always be superior to the temporal Minimal-Distance. However, this holds true only for MuJoCo. Why does this intuition fail for other benchmarks like Antmaze? \n\n3)\tWhy more performant for offline RL is not taken, e.g DemoDICE? It would be beneficial to compare to those and would strengthen empirical part of the work. \n\n4)\tAll baselines are based on Sinkhorn-regularized OT problem. Is it possible to compare proposed approach to more robust OT solvers based on neural nets? Consider DIOTM [1], Expectile-OT [2] as recent state-of-the art methods from neural OT field. \n\n5)\tFrom the author’s perspective, is OT in Offline RL / IRL has future research directions? Or it is better to focus on other approaches to  imitation learning? \n\n[1] Choi et al. Improving Neural Optimal Transport via Displacement Interpolation, ICLR 2025\n\n[2] Buzun et al. ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport, NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfJfqFeauy", "forum": "lyjXZd9rNQ", "replyto": "lyjXZd9rNQ", "signatures": ["ICLR.cc/2026/Conference/Submission14790/Reviewer_nizn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14790/Reviewer_nizn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727442376, "cdate": 1761727442376, "tmdate": 1762925141358, "mdate": 1762925141358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors solve the offline imitation learning problem by proposing a simple alternative to the commonly used optimal transport. Authors showcase 2 simple heuristics for labeling trajectories based on distance similarity. The authors benchmark these reward labeling functions on the common dataset D4RL, showing similar performance to OT algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simplicity of the method. The method itself does not require complex algorithms and is purely based on trajectory alignment via distance.\n2. Temporal alignment. The method by design includes the property of temporal alignment, allowing trajectories to encode context information, which is crucial for obtaining a reward aligned with the goal.\n3. Multiple datasets and mixture of experts evaluation. The authors evaluated their algorithm across multiple datasets and demonstrated its performance in a scenario with a mixture of experts."}, "weaknesses": {"value": "1. **Incremental contribution:** The proposed method appears to offer only a modest extension of existing approaches and does not achieve state-of-the-art performance. This raises questions regarding the practical significance and broader applicability of the method.\n\n2. **Incomplete comparison with prior work:** The authors claim superior performance in most cases; however, they do not include a comparison with [1], which also leverages a single expert trajectory and consistently achieves better results across the same benchmarks. This omission limits the strength of their empirical claims.\n\n[1] Bobrin, Align Your Intents: Offline Imitation Learning via Optimal Transport. https://arxiv.org/pdf/2402.13037"}, "questions": {"value": "1. Could you please add comparisons with [1] and explain any benefits regarding your work besides simplicity?\n2. Have you tried using thresholded Euclidean distance? i.e if the distance above d_max we set reward to -r_max.\n3. How would cosine distance compare with embedded latent distance ||z_1 - z_2||_2 i.e if we learn a very simple autoencoder and use this instead of using pure Euclidean distance?\n\n\n[1] Bobrin, Align Your Intents: Offline Imitation Learning via Optimal Transport. https://arxiv.org/pdf/2402.13037"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t5KBsCHtQ1", "forum": "lyjXZd9rNQ", "replyto": "lyjXZd9rNQ", "signatures": ["ICLR.cc/2026/Conference/Submission14790/Reviewer_DYoV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14790/Reviewer_DYoV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936785511, "cdate": 1761936785511, "tmdate": 1762925140915, "mdate": 1762925140915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks into whether solving an OT coupling is truly necessary for IRL-style reward labeling. It revisits a standard OT reward and TemporalOT (OT with context windows and temporal masking), and proposes two optimization-free surrogates: Min-Dist (negative nearest distance from an agent state to the expert) and Seg-Match (negative nearest distance within the temporally corresponding expert segment). A complexity table and wall-clock measurements show that reward labeling can dominate runtime for OT variants in practice. The study compares all four labelers with IQL/ReBRAC/DrQ-v2 across D4RL and MetaWorld."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes two optimization-free reward surrogates (Min-Dist, Seg-Match) with clear computational advantages; \n2. Provide a practical complexity and timing comparison for reward labeling; \n3. Empirical study shows that simple distance-based surrogates are competitive or better on many standard setups."}, "weaknesses": {"value": "1. The paper does not test harder cases where OT variants are known to be useful (e.g., large time warps, cross-domain state geometry, partial observability, goal shifts). As the authors themselves highlight, outcomes are highly sensitive to downstream RL hyperparameters (γ, BC regularizations). Hence, the claim “OT is unnecessary” should be re-scoped to the studied regimes. The paper already shows tuning can flip results (e.g., Seg-Match collapse under untuned regularization vs. recovery after tuning), underscoring the fragility of broad claims.\n2. The comparison focuses on “vanilla” OT and TemporalOT; modern OT variants (unbalanced/partial OT for length mismatch, GW for structural alignment across spaces, sliced/mini-batch/low-rank OT for efficiency, time-regularized/soft-DTW couplings for large warps) are not compute-matched in the study. Without these, concluding “OT is unnecessary” feels under-substantiated beyond the specific instantiations tested.\n3. Seg-Match presumes rough temporal alignment (or similar speeds/lengths); otherwise, distances inflate and rewards become biased. This is acknowledged and empirically manifested: with untuned downstream settings, Seg-Match can abruptly fail (e.g., halfcheetah-medium). A systematic length/speed-mismatch sweep would strengthen the claim."}, "questions": {"value": "1. Temporal alignment is a critical concept in this work; you may need to explicitly discuss this in the work, specifically in the introduction. \n2. Can you please clarify and provide more details for the selection method when multiple experts' trajectories are available? \n3. Table 2 is actually in the Appendix; you may want to mention this.\n3. It claims \"In contrast, Seg-Match and TemporalOT can be improved with more expert demonstrations.\". However, from K 10 to 20, there is a significant drop in the performance of Seg-Match, which makes the claims not fully convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a2Ryy5MyxT", "forum": "lyjXZd9rNQ", "replyto": "lyjXZd9rNQ", "signatures": ["ICLR.cc/2026/Conference/Submission14790/Reviewer_eeZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14790/Reviewer_eeZD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971731710, "cdate": 1761971731710, "tmdate": 1762925140411, "mdate": 1762925140411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}