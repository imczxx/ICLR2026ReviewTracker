{"id": "ttuNnMRI6H", "number": 6201, "cdate": 1757957995476, "mdate": 1759897930224, "content": {"title": "Any-Order Flexible Length Masked Diffusion", "abstract": "Masked diffusion models (MDMs) have recently emerged as a promising alternative to autoregressive models over discrete domains. MDMs generate sequences in an any-order, parallel fashion, enabling fast inference and strong performance on non-causal tasks. However, a crucial limitation is that they do not support token insertions and are thus limited to *fixed-length* generations. To this end, we introduce **Flex**ible **M**asked **D**iffusion **M**odels (FlexMDMs), a discrete diffusion paradigm that simultaneously can model sequences of flexible length while provably retaining MDMs' flexibility of any-order inference. Grounded in an extension of the stochastic interpolant framework, FlexMDMs generate sequences by inserting mask tokens and unmasking them. Empirically, we show that FlexMDMs match MDMs in perplexity while modeling length statistics with much higher fidelity. On a synthetic maze planning task, they achieve $\\approx$ 60\\% higher success rate than MDM baselines. Finally, we show pretrained MDMs can easily be *retrofitted* into FlexMDMs: on 16 H100s, it takes only three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior performance on math (GSM8K, 58\\%$\\to$67\\%) and code infilling performance (52\\%$\\to$65\\%).", "tldr": "", "keywords": ["Diffusion Model", "Generative Model", "Discrete Diffusion", "Stochastic Interpolant"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5847c611d128cdc5115a1ac98fe79e34b6426c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FlexMDM, a masked diffusion model that jointly models the unmasking posterior and the insertion expectation. Unlike traditional Masked Diffusion Models (MDMs), which begin with a fixed number of masked tokens and progressively unmask them, FlexMDM introduces an additional insertion operation that dynamically inserts new mask tokens during the diffusion process. This design enables variable-length generation while preserving any-order generation capabilities.\n\nFlexMDM can be adapted from an existing MDM with minimal changes—specifically, by adding a scalar output head at each position to predict the insertion expectation.\n\nThe authors validate their method across multiple language and planning benchmarks, demonstrating that FlexMDM:\n\n1. Effectively learns the true length distribution of the training data,\n2. Achieves comparable generative perplexity to standard MDMs,\n3. Outperforms MDMs on planning and reasoning tasks, and\n4. Can be efficiently scaled to larger models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Writing is very clear and easy to follow\n2. The framework is mathematically sound and theoretically grounded and solid\n3. The idea is novel and the connection to traditional MDMs are very clear"}, "weaknesses": {"value": "I find the experiments relatively weak compared to the strong theoretical foundations of the paper, and several aspects require clarification:\n\nModel comparison: It is unclear what specific model is referred to as “MDM” in the comparisons. For example, is it equivalent to D3PM or another variant?\n\nFigure 4c: The figure (which should include an overall label) shows an interesting trend in perplexity as the number of sampling steps increases. Since neither curve appears to converge by 4096 steps, it would be informative to see what happens beyond this point.\n\nMaze task: While it is reasonable that FlexMDM outperforms traditional MDMs in the maze task, this setup alone seems insufficient to substantiate the claim that it “supports FlexMDM as a principled approach for subgoal-based planning.”\n\nScaling up experiment: The statement “Surprisingly, we observe rapid transfer: within three days on 16 H100 GPUs, the model generates variable-length sentences” requires clarification. MDMs can already generate variable-length sequences to some extent using padding tokens, as the authors note. It would be useful to clarify in what sense this transfer is “rapid.” Also, while it's an 8B model, the authors only trains a 400M LoRA adapter and claim FlexMDM is \"scalable\". Can the authors clarify on this?\n\nAblation study: Including an ablation study on modeling the insertion process would strengthen the empirical evidence. Alternatively, can the authors confirm if the comparison between MDM and FlexMDM already serves this purpose."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mn4NkWUvny", "forum": "ttuNnMRI6H", "replyto": "ttuNnMRI6H", "signatures": ["ICLR.cc/2026/Conference/Submission6201/Reviewer_TnF2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6201/Reviewer_TnF2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760976868928, "cdate": 1760976868928, "tmdate": 1762918542408, "mdate": 1762918542408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current masked diffusion models are constrained to fixed-length sample generation, often resorting to tricks (e.g. padding) to deal with variable-length inputs. This paper addresses this issue by introducing a flexible-length masked diffusion model based on token deletion/insertion in addition to masking/unmasking operations. This behavior is achieved by learning to predict an “insertion expectation” (expected number of tokens yet to be inserted after a given position) in addition to the canonical unmasking posterior. The insertion expectation is parameterized directly as an additional scalar prediction head.\n\nThe resulting method is able to accurately match the ground-truth sequence length distribution, more so than the fixed-length baseline utilizing padding tokens. It also outperforms the fixed-length baseline on a subgoal-conditioned maze solving task, and can be cheaply retrofitted onto a 8B masked diffusion model (MDM), improving math and coding performance for longer sampling horizons."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "### S1. Solves an important problem of MDMs\nVariable-length generation has been a major limitation of MDMs, and the proposed solution is a valuable addition to both theory and practice of MDMs.\n\n### S2. Theoretically grounded approach\nThe proposed approach is grounded in theory and provides some guarantees on the likelihood of the trained model.\n\n### S3. Applicability to existing MDMs\nThe proposed approach can be retrofitted to existing pre-trained MDMs with minimal training (as little as 13B tokens), enabling flexible-length generation for both improved efficiency and accuracy."}, "weaknesses": {"value": "(ordered by descending severity)\n\n### W1. Meager experimental results\nWhile the set of considered tasks is well-rounded, the selective reporting and omission of results as well as non-standard choices that skew the picture in favor of the proposed method need to be rectified:\n1. Generative PPL should always be reported together with sequence entropy to control for diversity collapse. Additionally, filtering out short sequences (L1575) is non-standard and skews the picture in favor of FlexMDM. This is an inherent limitation of using the gen. PPL metric and should be controlled for via sequence entropy.\n2. Validation loss, and perhaps also training curves, should be reported even if the losses aren’t directly comparable (L416). If the FlexMDM objective indeed is a likelihood bound, then both losses are meaningful and, to some extent, comparable.\n3. Accuracy on text benchmarks should also be reported for the sake of consistency and comparison with the literature. These numbers are comparable even if the losses aren’t.\n4. Numbers on training/inference speed of FlexMDM vs. MDM are missing. In principle, this should be favorable for FlexMDM due to its smaller average sequence length.\n5. The required training time to convert an existing MDM to FlexMDM (L450) should (also) be reported in terms of training tokens, as the wallclock time highly depends on the level of optimization of the given codebase.\n6. The reported performance of LLaDA on GSM8k (Fig. 5) is considerably lower than what’s reported in the original paper. The original paper (Nie et al., 2025) reports 70.3% for the base model and 69.4% for the IFT model. It is unclear how this number could get worse by directly training on GSM8k.\n\n### W2. Reproducibility\nFor the sake of reproducibility, I strongly urge the authors to release all artifacts required to reproduce the results presented in this paper, including trained model checkpoints as well as training and inference code.\n\n### W3. Some fundamental limitations of MDMs remain\nWhile the proposed method enables flexible insertion of tokens during the generation process, some other fundamental limitations remain: Namely, the resulting model is unable to delete or revise existing tokens once filled in. Therefore, the model still risks accumulating errors throughout sampling, just like traditional MDMs.\n\n### Conclusion\nDespite the limited experimental results (W1) and concerns regarding reproducibility (W2), the theoretical contributions are strong on their own (S1, S2). Therefore, reasons to accept currently outweigh reasons to reject. The submission can be made significantly stronger still by addressing W1 and W2, and I will be happy to update my final score accordingly."}, "questions": {"value": "- Q1. What are the generative PPLs of MDM and FlexMDM (175M) without filtering by sequence length? (also see W1.1)\n- Q2. What is the sequence entropy (as in Zheng et al., 2024) of MDM and FlexMDM (175M) for different numbers of denoising steps? (also see W1.1)\n- Q3. What is the training/validation loss (throughout training/final) of MDM and FlexMDM (175M)? (also see W1.2)\n- Q4. What is the performance of MDM and FlexMDM (175M) on some relevant text benchmarks (e.g. HellaSwag, ARC-E, ARC-C, WinoGrande, PIQA, etc.)? (also see W1.3)\n- Q5. What is the training and inference speed of MDM vs. FlexMDM (both 175M and 8B)? (also see W1.4)\n- Q6. How many tokens were used for training LLaDA on FlexMDM? (also see W1.5)\n- Q7. Why are the reported numbers on GSM8k (Fig. 5) much lower compared to LLaDA-base (Nie et al., 2025) despite explicitly training on the task? (also see W1.6)\n- Q8. Will the model weights and/or training code be released? (also see W2)\n\nAdditional questions:\n- Q9. Does the FlexMDM training objective constitute a likelihood bound (i.e. ELBO)? If so, this does not appear to be immediately obvious based on L278.\n- Q10. Were any alternative ways to parameterize the insertion expectation considered? For example, one may consider modeling the number of insertions as a categorical distribution over the number of to-be-inserted tokens and calculating the expectation thereof.\n- Q11. For large-scale pretraining, it is a requirement to have statically-shaped batches for the sake of model compilation and good FLOP utilization. Therefore, in order to apply FlexMDM at scale it will be crucial to have a recipe to do fixed-length training, e.g. through sequence packing. Can FlexMDM support such a scenario?\n- Q12. Is it possible to skip the distinct masked case and directly go from empty to unmasked? This could avoid the apparent overhead of first inserting masked tokens and only unmasking them later.\n- Q13. Why is a poisson distribution the correct choice for inserting mask tokens? As far as I can tell, this is not explained in the paper.\n\nNits (not considered for final score):\n- Some equations are unnumbered (e.g. L184, L230, L259, L278). For the sake of referenceability, it is good practice to number all equations (and not just the ones that are referenced in the text itself).\n- Time notation breaks the convention of t=0 being noise-free and t=1 being complete noise. This is confusing for people familiar with the diffusion literature, especially for those unfamiliar with flow-matching. For the sake of consistency with the literature, I recommend keeping the diffusion convention. Alternatively, a note on this break of convention may help prevent unnecessary confusion.\n- L294: Missing parentheses around equation number. Alternatively, it is good practice to reference equations as such, i.e. “Eq. 5” or “Equation 5” or “Eq. (5)”.\n- L317: Seemingly missing some words in “Since unmasking indices in an adaptive [inference setting] no longer trace [...]”\n- L375: The footnote mark makes the noise schedule appear to be quadratic. Swapping the position of the period and footnote may help, i.e. `$\\alpha_t = \\beta_t = t$.\\footnote{...}`\n- L452: Missing period at the end of the line.\n\n---\n\n### References\n- Nie et al. (2025): https://arxiv.org/abs/2502.09992\n- Zheng et al. (2024): https://arxiv.org/abs/2409.02908"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l3H0RY39fI", "forum": "ttuNnMRI6H", "replyto": "ttuNnMRI6H", "signatures": ["ICLR.cc/2026/Conference/Submission6201/Reviewer_77ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6201/Reviewer_77ke"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840808232, "cdate": 1761840808232, "tmdate": 1762918542035, "mdate": 1762918542035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Any-Order Flexible-Length Masked Diffusion (FlexMDM)**, a generalization of masked diffusion models (MDMs) that supports variable-length generation through a continuous-time Markov chain formulation. The authors introduce an *insertion process* parameterized by a learned insertion expectation (g_\\theta) in addition to the usual denoising posterior (f_\\theta). This enables length-adaptive, any-order generation while preserving theoretical guarantees and compatibility with pretrained MDM weights. Experiments demonstrate strong results on text, maze planning, and code/math reasoning tasks, including scaling to an 8B-parameter model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear theoretical formulation.** The continuous-time treatment and derivation of the rate matrix and training objective are elegant and rigorous. The paper provides the right amount of mathematical depth while maintaining readability.\n2. **Minimal yet effective extension.** Introducing a scalar insertion expectation per gap is an elegant way to achieve variable-length modeling without compromising the core MDM formulation.\n3. **Scalability and empirical validation.** The ability to retrofit large pretrained MDMs and obtain consistent gains across domains (text, code, math) is a strong practical signal.\n4. **Well-designed sampler.** The adaptive unmasking and τ-leaping integration are both theoretically justified and empirically efficient."}, "weaknesses": {"value": "0. **Missing support for addition operation**.\nCompared to editflow, it supports addition while this work doesn't. \n\n1. **Missing discussion of Seed Diffusion.**\n   The paper should reference and compare to *Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference* (arXiv:2508.02193). Both aim to enable flexible-length or insertion-based generation, but take different approaches—Seed Diffusion uses a single-model canvas growth mechanism, whereas FlexMDM introduces a learned insertion intensity. A brief conceptual comparison would clarify relative advantages (FlexMDM’s theoretical rigor vs. Seed Diffusion’s simplicity).\n\n2. **Apparent need for two models.**\n   The description suggests two learned functions—one for denoising and one for insertion—which might appear “not elegant.” It would help to emphasize that these are *two heads of the same model* rather than two independent models. The authors could also discuss whether a unified parameterization (e.g., sharing a latent event-rate representation) is feasible, as done in Seed Diffusion’s single-network formulation.\n\n3. **Limited evaluation in real-world settings.**\n   While the presented results are promising, additional experiments on **broader math and coding benchmarks** would strengthen the claims.\n\n   * Include *MATH-500/5000* or *AIME24/25* for compositional reasoning.\n   * Extend code evaluation to *HumanEval*, *HumanEval+*, or *MBPP*.\n   * Report **pass@10** and **pass@k vs. compute-time** to better capture stochastic sampling performance.\n\n4. **Minor clarity issues.**\n\n   * An architecture figure showing the shared backbone and dual heads would avoid the “two-model” confusion.\n   * A brief qualitative example illustrating insertion dynamics would improve intuition."}, "questions": {"value": "See weakenesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JeHd28tyfz", "forum": "ttuNnMRI6H", "replyto": "ttuNnMRI6H", "signatures": ["ICLR.cc/2026/Conference/Submission6201/Reviewer_VJ5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6201/Reviewer_VJ5y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939436037, "cdate": 1761939436037, "tmdate": 1762918541728, "mdate": 1762918541728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Flexible Masked Diffusion Models (FlexMDM), a discrete diffusion framework that adds token insertion to standard masked diffusion models while provably preserving any-order decoding. It introduces a joint interpolant over token values and indices. The proposed model learns both the usual unmasking posterior and a new insertion expectation, and derives a CTMC-based training objective with guarantees. Empirically, FlexMDM matches MDM perplexity while modeling length distributions far more faithfully. Initializing from a pretrained MDM model (LLaDA-8B), fine-tuned FlexMDM exhibits significant performance improvements on GSM8K and HumanEval-infill as the number of sampling steps increases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes a novel and principled approach to addressing variable-length generation, which is a known limitation of existing masked diffusion models.\n- The proposed approach is theoretically grounded: a joint interpolant is defined over both token values and the number of insertions, then a neat training objective is derived based on CTMC, as a natural extension to the standard MDM training loss. \n- This paper also proves the compatibility of insertion prediction with adaptive inference in MDM\n- The empirical evidence is compelling. FlexMDM models the length distribution much more faithfully than MDM without sacrificing perplexity, and demonstrates better scaling performance on GSM8K and HumanEval-infill"}, "weaknesses": {"value": "- It would help practitioners more if the paper could provide more descriptions of the implementation details. E.g., after inserting/deleting tokens, will the padding be dynamically adjusted?\n- Some additional questions \n    - Figure 3 left: I guess $S_{\\tau}$ shoud be $[0, 3, 4]$ right?\n    - In algorithm 1, positions are unmasked first before insertion prediction, but I suppose the newly unmasked tokens should not be visible to the insertion module $g_{\\theta}$ right?\n    - Given the flexibility of combining any-order unmasking with insertion prediction, I wonder whether it's possible to simply reuse the unmasking posterior from a pretrained MDM and only separately learn the insertion expectation."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3nccVHl1Uw", "forum": "ttuNnMRI6H", "replyto": "ttuNnMRI6H", "signatures": ["ICLR.cc/2026/Conference/Submission6201/Reviewer_J8NK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6201/Reviewer_J8NK"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184744930, "cdate": 1762184744930, "tmdate": 1762918541294, "mdate": 1762918541294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}