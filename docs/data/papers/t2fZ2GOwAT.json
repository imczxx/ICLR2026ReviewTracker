{"id": "t2fZ2GOwAT", "number": 20922, "cdate": 1758311685861, "mdate": 1759896952036, "content": {"title": "MATHMO: Automated Mathematical Modeling Through Adaptive Search", "abstract": "Mathematical modeling is the process of understanding and predicting complex real-world phenomena. \nTraditionally, it is a time-intensive effort reliant on deep human expertise and iterative refinement. \nAutomating this intricate process, therefore, offers the potential to significantly accelerate discovery and broaden the application of mathematical modeling across diverse domains. Such automation, however, must address inherent challenges, including fundamental modeling uncertainty, balancing multiple conflicting objectives, and incorporating subjective qualities into assessing model utility.\nWe approach this by conceptualizing mathematical modeling as a sequential decision-making problem under uncertainty. \nIn response, we introduce $\\texttt{MATHMO}$, a novel adaptive search method designed to automatically navigate the complex decisions in selecting mathematical frameworks, specifying model formulations, and defining algorithmic procedures. \nSpecifically, $\\texttt{MATHMO}$ employs a principled bi-level search strategy---combining high-level exploration across diverse frameworks and local intra-framework model refinements---leveraging Large Language Models for exploration, surrogate evaluations, and incorporating subjective preferences into the automated process. We demonstrate $\\texttt{MATHMO}$'s efficacy on diverse real-world tasks, where it successfully discovers Pareto-efficient frontiers of models that balance varied objectives, including subjective criteria.", "tldr": "", "keywords": ["automated modeling", "autoformulation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20d87dfbd1f95684253fb53a1568d3a41206d23a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "It proposes MATHMO that automates mathematical modeling using a bi-level search: an upper level selects modeling frameworks via Pareto-UCB, while a lower level refines models/algorithms within a framework. It uses LLMs as generators, surrogates, and for evaluating subjective qualities. The authors demonstrate MATHMO's efficacy on four real-world tasks (TSP, Job Shop Scheduling, Ecology, Epidemiology), showing it can discover Pareto frontiers of models that balance competing objectives like accuracy, runtime, and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clearly defines modeling as a sequential, multi-objective decision problem.\n\nUses LLMs as core search operators for generation, surrogate evaluation, and subjective scoring."}, "weaknesses": {"value": "Performance is highly sensitive to prompt engineering and LLM outputs (e.g., poor surrogate predictions for JSS runtime).\n\nLack of comprehensive comparison with existing baselines and benchmark evaluations."}, "questions": {"value": "It would be useful to see a comparison and discussion with other related methods for automated modelling, such as Chain-of-Experts、ORLM、LLMOPT and benchmarks such as NL4OPT、MAMO、IndustryOR.\n\nThe manuscript mentions a 300-second timeout for model execution. How were models that timed out or failed to execute incorporated into the history S_t? Were they assigned a penalty value, and if so, how did this affect the utility functions α and β?\n\nFor the MOSE module, how was the fixed reference set M_ref constructed? Was a specific strategy used to ensure diversity and representativeness across different frameworks?\n\nThe results show that on the JSS task, the surrogate's NRMSE worsened over time. Beyond the proposed explanation, were any other strategies attempted to improve surrogate predictions for runtime? For example, was feature engineering from the code or a separate, specialized predictor for this objective explored?\n\nThe hyperparameter l (the number of candidate pairs proposed per iteration) is set to 3. A larger *l* might improve the lower-level selection at a higher computational cost for surrogate evaluations. Was any sensitivity analysis performed on this parameter?\n\nThe framework sampling is done once at the beginning of the search. Did the authors experiment with allowing the upper level to dynamically propose new frameworks mid-search, rather than being restricted to the initial set?\n\nIn the comparison against baselines (MEoH, FunSearch), could you provide the detailed settings used for both baselines? Also, is the budget of 20 model–algorithm evaluations too small? A common practice in LLM-driven algorithm design is to use hundreds of evaluations.\n\nThe manuscript focuses on discovering the Pareto frontier. In a practical deployment, how would MATHMO be used to recommend a single model to an end-user? Does the framework include a mechanism for incorporating user-specific preferences over the objectives to make this final selection?\n\nWhy was GPT-4o chosen, and were other LLMs compared for the sampler/surrogate roles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QWLluRaUch", "forum": "t2fZ2GOwAT", "replyto": "t2fZ2GOwAT", "signatures": ["ICLR.cc/2026/Conference/Submission20922/Reviewer_2x75"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20922/Reviewer_2x75"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650821077, "cdate": 1761650821077, "tmdate": 1762999979601, "mdate": 1762999979601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MATHMO, a framework for automated mathematical modeling that treats model construction as a sequential decision-making process under uncertainty. It uses a bi-level adaptive search: the upper level selects modeling frameworks (e.g., optimization, dynamical systems, regression) while the lower level refines models and algorithms within each framework. Large Language Models (LLMs) are used as generators, surrogate evaluators, and subjective scorers via the proposed MOSE module, which estimates interpretability and other qualitative factors. The system is tested on four tasks — Traveling Salesman, Job Shop Scheduling, Ecology, and Epidemiology — and shown to produce Pareto-efficient frontiers balancing multiple objectives such as accuracy, runtime, and interpretability. The work combines ideas from AutoML, symbolic discovery, and human-in-the-loop modeling into a unified adaptive system, aiming to automate core aspects of mathematical model design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers a fresh and ambitious framing of mathematical modeling as a sequential decision process under uncertainty. It creatively combines LLM generation, surrogate modeling, and subjective evaluation within a clear bi-level adaptive search framework. The idea of MOSE for quantifying interpretability is novel and thought-provoking. Experiments across optimization and scientific domains show flexibility and coherence. The paper is clearly written, conceptually rich, and points toward an important new direction in automating the reasoning process behind mathematical modeling."}, "weaknesses": {"value": "1. The experiments are small-scale (20 iterations, modest problem sizes) and mostly illustrative. They show proof of concept, not scalability. It would help to see at least one larger or higher-cost task to test how the adaptive search behaves when model evaluations are expensive.\n\n2.  The paper argues MATHMONAIVE serves as a GPT baseline, but this isn’t made explicit. A simple “GPT-4 one-shot” or “GPT-4 iterative prompting” baseline would clarify how much improvement truly comes from adaptivity rather than LLM creativity.\n\n3.   No baselines from classical AutoML (Auto-Sklearn, Optuna) or symbolic regression frameworks (PySR, Eureqa). These would help ground the claim that MATHMO goes beyond existing automation tools.\n\n4. The MOSE module is an appealing idea, but there’s no evidence it correlates with actual human judgments of interpretability. Even a small human study or qualitative comparison would make it more credible.\n\n5. The method depends heavily on GPT-4’s internal priors. It’s unclear how stable results are under different prompts or smaller LLMs. This raises questions about reproducibility and accessibility.\n\n6.  The paper reports hypervolume numbers but doesn’t show concrete model examples or code snippets. It’s hard to judge if the discovered models are sensible, interpretable, or novel."}, "questions": {"value": "Got it — here’s a tighter, natural-sounding version that feels like a human reviewer’s “Questions & Suggestions” section.\n\n---\n\n\n1. How general is MATHMO in practice? Are the modeling frameworks fixed at the start or can the system invent new ones? How do you prevent the LLM from proposing nonsense frameworks?\n\n2. Why not include a simple GPT-4 prompt baseline for context? It would help readers see what is gained by adding adaptive feedback. Did you test this internally? Also, do the surrogate and MOSE modules improve the *quality* of models or just the coverage of the Pareto front?\n\n3. The paper cites FunSearch, MEoH, and AutoFormulation. What is concretely new beyond combining their elements? How is the Pareto-UCB exploration different from MEoH’s evolutionary approach?\n\n4. The MOSE idea is interesting but underspecified. How consistent are its scores across prompts or random seeds? Any human comparison to check if MOSE’s “interpretability” aligns with human judgment?\n\n5.  Hypervolume is fine, but would it be possible for the authors to show some discovered models - I think it would make results more convincing. How often do generated models fail to run? What is the compute cost relative to the baselines?\n\n6. Can this approach scale to heavier tasks where each model evaluation takes hours or days? Could smaller open LLMs work as surrogates or is GPT-4-level capability required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rpMVeLt5dL", "forum": "t2fZ2GOwAT", "replyto": "t2fZ2GOwAT", "signatures": ["ICLR.cc/2026/Conference/Submission20922/Reviewer_dMwL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20922/Reviewer_dMwL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978142215, "cdate": 1761978142215, "tmdate": 1762938786524, "mdate": 1762938786524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MATHMO, a novel framework designed to automate the process of mathematical modeling. The authors conceptualize mathematical modeling as a sequential decision-making problem under uncertainty, aiming to discover not a single best model, but a Pareto frontier of models that balance multiple, often conflicting, objectives. MATHMO employs a principled bi-level adaptive search strategy: an upper level selects between different high-level modeling frameworks (e.g., mathematical optimization vs. dynamical systems), while a lower level refines specific model formulations and algorithms within the chosen framework. A core technical contribution is the use of Large Language Models (LLMs) as versatile search operators for both generating candidate models (as code) and acting as surrogate models to efficiently estimate performance. Uniquely, the framework introduces MOSE (Model of Subjective Evaluations), an LLM-based component to quantify and incorporate subjective criteria like interpretability into the automated search process. The efficacy of MATHMO is demonstrated on four diverse real-world tasks, showing its ability to identify Pareto-efficient models that navigate complex trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper considers heuristic design with multiple framework. \n- More applicable problems are tested compared with the baselines."}, "weaknesses": {"value": "- The entire framework's success is fundamentally predicated on the quality and diversity of the models the LLM can generate. The search is effectively bounded by the LLM's pre-trained knowledge and potential biases. If the LLM has not been trained on sufficient examples of a certain niche but powerful framework (e.g., specific types of constraint programming or advanced statistical models), it may never propose them, leading to a \"rich-get-richer\" dynamic where common frameworks are over-explored.\n- The paper does not analyze the diversity of the initial framework proposals or the \"mutations\" suggested in the lower-level search. It's unclear if the LLM is genuinely exploring novel variations or just making superficial syntactic changes to the code. The search is only as good as the operator, and the operator here is a black box.\n- The paper validates MOSE's interpretability scores by correlating them with structural/functional complexity proxies (parameter count, permutation entropy). These proxies are not equivalent to interpretability. A model with few parameters can be completely uninterpretable if the parameters lack physical or domain meaning. Conversely, a complex model can be interpretable if its structure is well-understood (e.g., a large but modular compartmental model in epidemiology).\n- The core assumption is that an LLM's preference ($p_\\mathrm{MOSE}(m_t \\succ m_i \\mid p)$) is a faithful proxy for that of a human domain expert. This is a very strong and unproven assumption. A small-scale user study with actual ecologists or epidemiologists would be required to truly validate that MOSE captures what experts consider \"interpretable.\" Without this, MOSE is just optimizing for what a specific LLM thinks is interpretable.\n- The score is dependent on the initial, fixed reference set $M_{\\mathrm{ref}}$. The paper doesn't explore the sensitivity of the results to the composition of this small ($n=3$) reference set.\n- The experimental setup involves 20 iterations with a 300-second timeout per model evaluation. This is already a significant time budget. More importantly, each iteration in the lower-level search involves multiple LLM calls: $l=3$ candidate generations, and for each, $q=3$ surrogate estimations. This suggests at least 12 LLM calls per iteration, using a state-of-the-art model (GPT-4o). This approach seems computationally expensive and potentially slow, which may limit its practical application to problems requiring more extensive search or faster turnaround. The paper lacks a discussion on the computational overhead and how it scales.\n- The formalism $(m, a)$ and its implementation as a single Python script works well for the self-contained problems chosen. However, real-world mathematical modeling is often a multi-stage pipeline (e.g., data cleaning -> feature engineering -> model selection -> calibration -> post-hoc analysis). It is unclear how the current framework would handle such complex workflows where decisions and trade-offs exist at each stage. The current approach seems to conflate the entire solution process into a single generative step."}, "questions": {"value": "- How does MATHMO guard against the LLM's inherent biases, which might cause it to completely miss entire families of effective models? Is there a risk that the search becomes trapped within the \"comfort zone\" of the LLM's training data?\n- Could you provide more details on the computational cost of a full run of MATHMO? Specifically, what is the wall-clock time and the approximate number of LLM API calls required for a 20-iteration run?\n- In Section C.3, you briefly mention a retry mechanism for execution errors. Code generation is notoriously brittle. Could you elaborate on the effectiveness of this self-correction loop? What percentage of generated models fail initially, and what types of errors is the LLM-based correction mechanism successful at fixing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sIN3xLs82Y", "forum": "t2fZ2GOwAT", "replyto": "t2fZ2GOwAT", "signatures": ["ICLR.cc/2026/Conference/Submission20922/Reviewer_VgEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20922/Reviewer_VgEc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988969099, "cdate": 1761988969099, "tmdate": 1762938759084, "mdate": 1762938759084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MATHMO, a novel system for automated mathematical modeling that leverages Large Language Models (LLMs) within a bi-level adaptive search framework. The work conceptualizes mathematical modeling as a sequential decision-making problem under uncertainty, addressing three key challenges: (1) fundamental modeling uncertainty, (2) balancing multiple conflicting objectives, and (3) incorporating subjective qualities into model evaluation.\n\nThe core contribution is a bi-level search architecture where the upper level performs adaptive framework selection using Pareto-Upper Confidence Bound (Pareto-UCB), while the lower level conducts local exploration within selected frameworks using LLM-based surrogates and Bayesian optimization principles. The system incorporates MOSE (Model of Subjective Evaluations), which uses LLMs to approximate human preferences for subjective criteria like interpretability across different modeling frameworks.\n\nThe experimental evaluation spans four diverse real-world tasks: Traveling Salesman Problem (TSP), Job Shop Scheduling (JSS), Ecology modeling, and Epidemiology forecasting. Results demonstrate MATHMO's ability to discover diverse Pareto-efficient frontiers of models that effectively balance trade-offs between objectives such as solution quality vs. runtime, and predictive accuracy vs. interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel problem formulation:  The paper presents a well-motivated and technically sound approach. Its treatment of automated mathematical modeling as a sequential decision-making problem is interesting.\n2. Practical impact: Demonstrates feasibility on real-world problems with meaningful trade-offs. Provide inspirations for human-AI collaboration in scientific modeling. The four benchmark problems represent genuine challenges in operations research, ecology, and epidemiology. The ability to discover meaningful trade-offs (e.g., accuracy vs. interpretability) has clear practical value.\n3. Technical Quality: The bi-level decomposition is well-motivated and technically sound. The integration of Pareto-UCB for framework selection and LLM-based surrogates for local exploration shows careful consideration of the problem structure.\n4. The paper is generally well-written with clear exposition and good organization. The formalization is accessible, and the figures effectively illustrate the key concepts. The related work section provides good context. Areas for improvement include:"}, "weaknesses": {"value": "1. Limited Scale and Diversity: While the problems are diverse, the evaluation is limited in scope (4 problems, 20 iterations). More extensive evaluation on larger problem sets would strengthen the claims.\n2. Theoretical Analysis: While the approach is intuitive, the paper lacks formal convergence analysis or regret bounds for the proposed adaptive search strategy.The lack of formal convergence guarantees or regret analysis leaves questions about the theoretical properties of the proposed approach.\n3.  Baseline Comparisons: While comparisons against MEoH and FunSearch are informative, comparisons against more traditional AutoML baselines (e.g., Bayesian optimization over predefined search spaces) would provide better context for the novelty claims.  The paper relies solely on general LLMs (gpt-4o) with search algorithms and prompt engineering, without exploring the effectiveness of domain-specific fine-tuned models. This approach may miss potential advantages of specialized models in mathematical modeling tasks. The paper does not adequately justify why the bi-level search framework with general LLMs is optimal. Alternative approaches, such as ensemble methods or hybrid search strategies, are not evaluated, leaving questions about design decision rationale and adaptability boundaries."}, "questions": {"value": "1. Scalability: How does the system scale to larger problem instances or longer modeling horizons? The current evaluation is limited to relatively small problems.\n2. LLM Prompt Sensitivity: Given the known sensitivity of LLMs to prompt variations, how robust is the system to different prompt designs or LLM model choices?\n3. Dynamic Framework Generation: The current approach samples and fixes frameworks at the beginning. Could dynamic framework generation further improve performance?\n4. Human-in-the-Loop: How could the system be extended to incorporate real human feedback during the modeling process rather than relying solely on LLM approximations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NwMHZaMRjZ", "forum": "t2fZ2GOwAT", "replyto": "t2fZ2GOwAT", "signatures": ["ICLR.cc/2026/Conference/Submission20922/Reviewer_N3pu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20922/Reviewer_N3pu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992766327, "cdate": 1761992766327, "tmdate": 1762938736190, "mdate": 1762938736190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}