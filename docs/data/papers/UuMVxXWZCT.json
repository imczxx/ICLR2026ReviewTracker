{"id": "UuMVxXWZCT", "number": 2179, "cdate": 1757012917805, "mdate": 1759898164646, "content": {"title": "Reference-Guided Identity Preserving Face Restoration", "abstract": "Preserving face identity is a critical yet persistent challenge in diffusion-based\nimage restoration. While reference faces offer a path forward, existing methods\ntypically suffer from partial reference information and inefficient identity losses.\nThis paper introduces a novel approach that directly solves both issues, involving\nthree key contributions: 1) Composite Context, a representation that fuses high- and\nlow-level facial information to provide comprehensive guidance than traditional\nsingular representations, 2) Hard Example Identity Loss, a novel loss function\nthat uses the reference face to address the identity learning inefficiencies of the\nstandard identity loss, 3) Training-free multi-reference inference, a new method\nthat leverages multiple references for restoration, despite being trained with only a\nsingle reference. The proposed method demonstrably restores high-quality faces\nand achieves state-of-the-art identity preserving restoration on benchmarks such as\nFFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work.", "tldr": "We propose a reference-based face restoration method to fully leverage the reference face and hence achieves the state-of-the-art identity preservation.", "keywords": ["Reference-Based Face Restoration", "Composite Context", "Hard Identity Loss", "Multi-reference face inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17501d2062bc472e8cd5990643d9287fc44d5285.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a reference-based diffusion framework for face restoration, emphasizing identity preservation. The authors introduce two key modules: (1) Composite Context, which combines multiple levels of facial features from pre-trained ArcFace and FaRL to provide high- and low-level representations of the reference face; and (2) Hard Example Identity Loss, which uses the reference face as a “hard example” to alleviate training inefficiency in identity learning. The method can also perform training-free multi-reference inference using classifier-free guidance. Experiments show that the proposed approach achieves higher identity similarity compared to prior works such as RefLDM and RestorerID, while maintaining competitive perceptual quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper identifies the common issue of insufficient identity preservation in diffusion-based face restoration and addresses it via composite context (face recognition and representation) and hard example identity loss.\n\n+ The two modules are orthogonal, making the approach easily integrable with other LDM backbones.\n\n+ The authors provide results against both reference-based (RefLDM, RestorerID) and no-reference methods (DiffBIR, CodeFormer), with great metrics and ablations."}, "weaknesses": {"value": "- Limited novelty over existing IP-Adapter-like paradigms and other diffusion based face restoration models.\nThe proposed Composite Context essentially acts as a fixed feature adaptor combining ArcFace and FaRL representations. This is conceptually close to IP-Adapter–style feature injection, except with multiple frozen encoders. The claimed advantage of mixing high-level and general representations is not convincingly demonstrated—visual results do not show clear benefits from these two branches. A more direct comparison with a trainable face encoder (e.g., IP-Adapter with finetuning or other low-level feature extraction model) is necessary to justify the contribution.\n\n- While “reference-guided face super-resolution” has some relevance, the paper focuses mainly on preserving high-level identity rather than fine-grained personal textures, which are typically more desirable for human perception. It is better to show some close-up regions to show the detailed textures. I note that sometimes these fine-grained personalized textures are not preserved well.\n\n- Lack of real-world evaluation.\nThe entire study uses synthetically degraded training and text data. The absence of experiments on real low-quality or in-the-wild images weakens claims about robustness and applicability, as acknowledged in the limitations section.\n\n- When both ​s_i and s_c are set to 1.2 in Eqs. (5)–(6), the middle term becomes redundant and has no practical meaning.\n\n- The authors should also discuss whether diffusion-based approaches indeed offer substantial advantages over GAN-based ref face restoration in this specific reference-conditioned setting.\n\n- Honestly, the improvement of some components like identity loss, FaRL, or ArcFace is not obvious."}, "questions": {"value": "- How does the Composite Context differ in practice from IP-Adapter or other reference-conditioned feature injectors? Could you compare against a fine-tuned, learnable adapter to validate the necessity of using two frozen encoders?\n\n- Since only synthetic degradations are used, how would the method behave on real-world low-quality faces (e.g., surveillance or historical photos)?\n\n- Can the Hard Example Identity Loss be generalized to other perceptual constraints, or is its effect limited to face identity preservation?\n\n- As for the VAE Encoder, some works claim that it is not suitable for low-quality image reconstruction, so they attempt to fine-tune the vae encoder with LORA. However, this work did not consider this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x75kLqpwaS", "forum": "UuMVxXWZCT", "replyto": "UuMVxXWZCT", "signatures": ["ICLR.cc/2026/Conference/Submission2179/Reviewer_Zwyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2179/Reviewer_Zwyf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546409102, "cdate": 1761546409102, "tmdate": 1762916096027, "mdate": 1762916096027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an interesting method for reference-based face restoration, focusing on improving identity preservation through two key contributions: a Composite Context representation and a Hard Example Identity Loss. The work is well-motivated, addressing clear limitations in existing methods (partial reference information, inefficient identity loss). The experimental results are comprehensive, demonstrating state-of-the-art performance on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-Motivated: The core ideas are reasonable in the context of reference-based face restoration. The critique of the \"learning inefficiency\" of standard identity loss is insightful, and the proposed HID loss is a simple yet effective solution. \n\nEmpirical Evidence: The paper provides extensive quantitative evaluations on multiple benchmarks (FFHQ-Ref Moderate/Severe, CelebA-Ref-Test), consistently showing superior performance in identity preservation (IDS, FaceNet) while maintaining competitive image quality. The comparison with recent SOTA methods (RefLDM, RestorerID) is fair and convincing.\n\nComprehensive Ablation Studies: The hierarchical ablation (module-wise and component-wise) effectively validates the contribution of each proposed component."}, "weaknesses": {"value": "Limited technical novelty:  While the paper proposes two modules—Composite Context (CC) and Hard Example Identity Loss (HID)—the technical innovations appear incremental. The CC module combines existing face representations (ArcFace and FaRL), which is conceptually similar to prior multi-modal fusion approaches (e.g., SDXL, PGDiff). Though the authors claim to be the first to combine specialized face encoders for restoration, this primarily constitutes an engineering integration rather than a fundamental algorithmic breakthrough. The HID loss, while effective, builds directly on standard metric learning techniques (e.g., hard example mining) without novel theoretical contributions. The training-free multi-reference inference is pragmatic but relies on straightforward ensemble averaging, lacking architectural or methodological novelty. Overall, the work would benefit from deeper ablation studies or theoretical analysis to justify its uniqueness beyond empirical improvements.\n\nInsufficient quantitative and qualitative results:\n1)\tLimited Quantitative Superiority: The proposed method does not consistently outperform existing approaches. Key perceptual quality metrics like LPIPS and MUSIQ often fail to show a decisive advantage over competing methods.\n2)\tDeficient Qualitative Results: The visual evidence is unconvincing. As shown in Figures 3-4, the outputs frequently exhibit noticeable artifacts and a significant disparity in identity preservation compared to the high-quality ground truth. In some cases (e.g., Figure 3), the restored facial texture and structure are inferior even to CodeFormer, a non-reference-guided baseline.\n\nIncomplete Comparative Analysis: The experimental comparisons are limited to only two reference-guided works, neglecting several recent and relevant state-of-the-art methods in the field, such as DMDNet, FaceMe, and Gen2Res. This omission makes it difficult to fairly assess the method's true standing."}, "questions": {"value": "Please refer to the paper weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vqwmOEiZSt", "forum": "UuMVxXWZCT", "replyto": "UuMVxXWZCT", "signatures": ["ICLR.cc/2026/Conference/Submission2179/Reviewer_QagU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2179/Reviewer_QagU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718588875, "cdate": 1761718588875, "tmdate": 1762916095448, "mdate": 1762916095448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses identity preservation in reference-based face restoration by proposing two key innovations: (1) Composite Context, a multi-level representation that fuses high-level identity information and low-level facial details from reference faces, and (2) Hard Example Identity Loss, a novel loss function that uses reference faces as hard examples to improve identity learning efficiency. The method is trained with single references but can leverage multiple reference images during inference in a training-free manner. Experiments on FFHQ-Ref and CelebA-Ref-Test datasets demonstrate superior performance in identity preservation compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Well-designed Composite Context that addresses multi-level information fusion. Unlike prior reference-based methods that only leverage partial information from reference faces, this work comprehensively combines: (a) high-level identity features via pre-trained ArcFace embeddings that enforce angular margin constraints; (b) general facial attributes via FaRL including skin texture, lighting, and semantic information; and (c) cross-attention projection through UNet for spatial alignment. The ablation study in Table 5 validates that all components contribute meaningfully, demonstrating the complementary nature of multi-level information.\n\n2.Strong quantitative improvements in identity preservation metrics. The method achieves substantial and consistent gains in identity metrics. Notably, the method maintains these identity gains while achieving competitive LPIPS and sometimes better perceptual quality scores, suggesting the approach does not simply overfit to identity at the expense of visual quality."}, "weaknesses": {"value": "1.Limited novelty in individual technical components \nWhile the overall system is effective, each core component builds heavily on existing techniques, the contribution feels more like good engineering than fundamental innovation.\n\n2. Insufficient Analysis of Multi-Reference Degradation Phenomenon\nTable 2 reveals a counterintuitive and concerning result: identity similarity IDS(REF) sometimes decreases with more reference faces. This directly contradicts the fundamental premise that more reference information should improve identity preservation. This weakness raises concerns about whether the multi-reference capability is truly beneficial or just a side effect of the architecture. The lack of analysis makes it difficult to recommend best practices for real-world deployment.\n\n3.The comparative analysis is limited in scope, and notably lacks comparison with recent state-of-the-art methods from 2025."}, "questions": {"value": "1.Counterintuitive multi-reference degradation phenomenon inadequately explained. Table 2 reveals a puzzling result: IDS(REF) sometimes decreases as more reference faces are added. This contradicts the intuition that more reference information should improve identity matching to references. The paper acknowledges this but provides no analysis of: (a) Why does this degradation occur? Is it due to conflicting information from different references, averaging artifacts, or limitations in the ensemble mechanism? (b) Is there an optimal number of references, or does it vary by degradation severity? (c) How should practitioners select which references to use when multiple are available?\n\n2.Unclear notation and missing implementation details. Several technical details are insufficiently specified: (a) Equation (1) mentions positional encoding epsilon_position but never defines its form, dimensionality, or initialization. (b) Table 2 caption states IDS(REF) is calculated using 'the first available reference face' - why this arbitrary choice rather than averaging across all references or using the highest-quality one? These ambiguities would make reproduction challenging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nD6pHVXnmm", "forum": "UuMVxXWZCT", "replyto": "UuMVxXWZCT", "signatures": ["ICLR.cc/2026/Conference/Submission2179/Reviewer_sBPb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2179/Reviewer_sBPb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754165370, "cdate": 1761754165370, "tmdate": 1762916095013, "mdate": 1762916095013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of reference-guided face restoration and proposes two key components: a Composite Context mechanism and a Hard Example Identity Loss. These components enhance the utilization of reference image information and improve identity consistency. Furthermore, the paper introduces a multi-reference inference framework that, despite being trained with only a single reference image, can effectively handle multiple references during inference.\n\nAlthough the proposed approach shows limited novelty and shares similarities with prior works, the method is well designed and empirically validated through comprehensive experiments. Overall, the results demonstrate the effectiveness of the proposed framework, and I recommend acceptance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and well organized.\n2. The overall framework is well structured, it effectively incorporates reference facial representations into the restoration pipeline and extends RefLDM models to better preserve identity similarity.\n3. The experimental evaluation is comprehensive and detailed, providing strong empirical evidence for the method’s effectiveness."}, "weaknesses": {"value": "1. The overall novelty is limited. The proposed framework is conceptually similar to several existing works, and the Hard Example Identity Loss is closely related to the loss function commonly used in [1]. Its main difference lies in the additional use of reference image information, which resembles strategies adopted in recent personalized generation methods[2].\n2. The Composite Context module relies heavily on face recognition features. It would be interesting to explore how the performance changes when different face recognition backbones are employed.\n\n[1]: Refldm: A latent diffusion model for reference-based face image restoration.\n\n[2]: PuLID: Pure and Lightning ID Customization via Contrastive Alignment"}, "questions": {"value": "See Weakness section above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cAuKTFwZ0i", "forum": "UuMVxXWZCT", "replyto": "UuMVxXWZCT", "signatures": ["ICLR.cc/2026/Conference/Submission2179/Reviewer_62Uy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2179/Reviewer_62Uy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900923881, "cdate": 1761900923881, "tmdate": 1762916094380, "mdate": 1762916094380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}