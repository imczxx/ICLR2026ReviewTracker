{"id": "CL5D33stq1", "number": 19382, "cdate": 1758295847347, "mdate": 1759897042353, "content": {"title": "OViP: Online Vision-Language Preference Learning for VLM Hallucination", "abstract": "Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. \n  Although recent training-based approaches aim to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that do not reflect actual model errors, thus limiting training efficacy.\n  In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model’s own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP not only reduces hallucinations while preserving core multi-modal capabilities, but also substantially improves training efficiency.", "tldr": "", "keywords": ["online learning", "multimodal", "hallucination"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a618fddc6ad76b5519701836671d6acbb78c8a8.pdf", "supplementary_material": "/attachment/f45f1ebf4b0cb9e2beb63ba37ee5a912b38a6e49.zip"}, "replies": [{"content": {"summary": {"value": "OViP is an online vision-language preference learning framework designed to reduce hallucinations in large vision-language models (LVLMs), which often invent objects, attributes, or spatial relations that are not actually in the image. It continuously samples the model’s own responses during training, identifies good vs. bad answers, and then uses an external LLM plus a diffusion model to synthesize targeted “negative” images and contrastive preference pairs that reflect the model’s real failure modes instead of relying on static or randomly edited data. OViP jointly optimizes both the text side (the response) and the image side, providing real-time supervision that pushes the model to stay faithful to what’s in the image rather than overfitting to language priors. Experiments show that OViP lowers hallucination rates, preserves general multimodal ability, improves training efficiency compared to prior offline DPO-style or online RL-style methods, and also highlights that evaluation should balance hallucination suppression with informativeness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method builds preference data online from the model’s own mistakes, instead of relying only on fixed, offline edits. This directly targets the model’s real hallucination modes during training and can make the supervision more relevant and efficient. \n\n2. It jointly optimizes both text faithfulness and visual grounding, aiming to reduce hallucination without making the model overly timid or uninformative."}, "weaknesses": {"value": "1. The novelty needs to be clarified: similar on-policy training ideas already exist. For example, SIMA[1] also scores the model’s own generated samples and uses them as positive/negative supervision. The paper should explain more clearly what is fundamentally new beyond that prior works.\n\n2. The approach depends on synthetic images from a generative model. But current image generators are not perfectly reliable, so the “negative” images might themselves be noisy or wrong. The paper should justify why supervision built on these generated images can be trusted.\n\n3. The experiments should include stronger, more recent LVLM baselines (e.g., Qwen 2.5 VL series) to demonstrate that the method still helps on modern state-of-the-art models, not just older ones.\n\n[1] Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OJXpASdMvw", "forum": "CL5D33stq1", "replyto": "CL5D33stq1", "signatures": ["ICLR.cc/2026/Conference/Submission19382/Reviewer_kH4S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19382/Reviewer_kH4S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976278882, "cdate": 1761976278882, "tmdate": 1762931306962, "mdate": 1762931306962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose an online preference tuning method to reduce hallucination in LVLMs by using diffusion models to generate paired images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experiments are solid, and the writing is clear."}, "weaknesses": {"value": "1. The method doesn't have particularly obvious novelty, as similar ideas have been explored before in V-DPO.\n2. Missing fine-grained results on general benchmarks; experimental results in this area need to be supplemented.\n3. The architecture is limited to LLaVA, and this model is relatively old. It's unclear whether this method would work on newer LVLM architectures."}, "questions": {"value": "The model could be updated to newer models and other architectures, such as the Qwen-VL series, to validate the generalizability of the method.\n\nWhat is the quality of the negative samples generated by the diffusion model? How does it compare to other methods, such as image editing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZSnbfqMYMB", "forum": "CL5D33stq1", "replyto": "CL5D33stq1", "signatures": ["ICLR.cc/2026/Conference/Submission19382/Reviewer_zfn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19382/Reviewer_zfn5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992099629, "cdate": 1761992099629, "tmdate": 1762931306484, "mdate": 1762931306484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of hallucinations generated by large visual language models (LVLMs) under visual input conditions. The paper argue that although there are methods that utilize preference learning (such as DPO) to alleviate hallucinations, these methods often rely on pre-defined or randomly edited negative samples that do not match the distribution of the model's true failure patterns, thereby limiting the training effectiveness. To this end, the paper proposes the Online Vision language Preference Learning (OViP) framework, dynamically identifying \"good/bad\" pairs from candidate answers generated by the model during the training process, and using a diffusion model to generate negative images to construct visual-language pairs. In this way, OViP constructs preference learning signals on both the text and image sides, achieving joint adjustment of text preferences and visual preferences in the model output. The experimental results show that OViP can reduce the occurrence of hallucinations on multiple hallucination detection and universal visual language task benchmarks, and the training efficiency is better than previous methods. Overall, this work proposes an online, dynamic negative sample generation mechanism that is triggered by the model's own errors, providing a training scheme that is more in line with the actual failure distribution for illusion relief."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses a crucial problem in VLMs: hallucinations. The paper propose OViP, which introduces the concepts of \"online construction of negative samples\" and \"joint image-text preference learning,\" demonstrating good performance on some datasets."}, "weaknesses": {"value": "- The method relies on generative models (LLM evaluation, diffusion model for generating negative images)—these steps increase method complexity. The quality of the generated negative images and whether the synthesized negative samples can truly cover the illusion space may limit generalization. Although the authors provide an efficiency analysis, their robustness in large-scale/diverse scenarios is not yet fully demonstrated.\n- The core components of this method (preference learning, negative sample generation, and negative image synthesis) already have conceptual or approximate aspects in existing research. While the paper includes a review of relevant work, it does not highlight the core breakthroughs of this method compared to the closest comparable approaches. Furthermore, the underlying mechanisms of core technologies such as LLM evaluation and negative sample image generation are not analyzed in depth.\n- Some Figures are difficult to understand intuitively, such as the two left images in Figure 1, which lack appropriate legends and explanations, making it difficult for readers to quickly understand the content that the images are trying to convey."}, "questions": {"value": "- Could the authors provide more details and analysis of the negative image generation mechanism, such as the diversity/coverage of generated image, examples of failed samples, and whether there is a risk that \"generating incorrect images\" might actually teach the model to \"escape the real task distribution\"?\n- Could the author add more \"mechanistic discussion\" to the methodology section.  A deeper explanation of the design principles of the core methods, rather than just a method description? For example, why is the proposed LLM-based method for generating positive and negative response pairs better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rQNvbY3q4E", "forum": "CL5D33stq1", "replyto": "CL5D33stq1", "signatures": ["ICLR.cc/2026/Conference/Submission19382/Reviewer_R4wf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19382/Reviewer_R4wf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248014064, "cdate": 1762248014064, "tmdate": 1762931306119, "mdate": 1762931306119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}