{"id": "I7UK5qHNBL", "number": 24168, "cdate": 1758353498137, "mdate": 1759896778940, "content": {"title": "Tackling Heavy-Tailed Q-Value Bias in Offline-to-Online Reinforcement Learning with Laplace-Robust Modeling", "abstract": "Offline-to-online reinforcement learning (O2O RL) aims to improve the performance of offline pretrained agents through online fine-tuning. Existing O2O RL methods have achieved advances in mitigating the overestimation of Q-value biases (i.e., biases of cumulative rewards), improving the performance. However, in this paper, we are the first to reveal that Q-value biases of these methods often follow a heavy-tailed distribution during online fine-tuning. Such biases induce high estimation variance and hinder performance improvement.\nTo address this challenge, we propose a Laplace-based robust offline-to-online RL (LAROO) approach. LAROO introduces a parameterized Laplace-distributed noise and transfers the heavy-tailed nature of Q-value biases into this noise, alleviating heavy tailedness of biases for training stability and performance improvement. Specifically, (1) since Laplace distribution is well-suited for modeling heavy-tailed data, LAROO introduces a parameterized Laplace-distributed noise that can adaptively capture heavy tailedness of any data. (2) By combining estimated Q-values with the noise to approximate true Q-values, LAROO transfers the heavy-tailed nature of biases into the noise, reducing estimation variance. (3) LAROO employs conservative ensemble-based estimates to re-center Q-value biases, shifting their mean towards zero. Based on (2) and (3), LAROO promotes heavy-tailed Q-value biases into a standardized form, improving training stability and performance. Extensive experiments demonstrate that LAROO achieves significant performance improvement, outperforming several state-of-the-art O2O RL baselines.", "tldr": "In offline-to-online reinforcement learning, we reveal that the Q-value estimation bias often follows a heavy-tailed distribution and propose a novel Laplace-based method to enhance training stability and performance under these biases.", "keywords": ["Reinforcement Learning", "Offline-to-Online", "Q-value Estimation", "Laplace Distribution"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45947c7b2e88806dd4a410f3434dfb382b0882f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is the first to empirically reveal the phenomenon that Q-value estimation bias in the online fine-tuning process of O2O pervasively follows a heavy-tailed distribution.To address this, the authors propose the LAROO (Laplace-based robust offline-to-online RL) approach, designed to mitigate the heavy-tailedness of Q-value estimation bias, thereby improving training stability and performance. The approach models the Q-value estimation bias using an adaptive Laplace-distributed noise, based on which a robust value loss function is constructed to reduce the variance of the Q-value bias during the learning process. Concurrently, it incorporates ensemble Q-models to shift the mean of the bias towards zero, ultimately achieving more robust and stable value estimation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- First revealed and empirically validated the pervasive heavy-tailed Q-bias phenomenon.\n- Through rigorous theoretical derivation, the LAROO method constructs a robust loss function based on the Laplace distribution, which is insensitive to outliers and reduces the variance of Q-bias.\n- Extensive experimental results demonstrate superior performance over published O2O methods across multiple environments."}, "weaknesses": {"value": "1.  The paper lacks comprehensive ablation studies validating the specific contribution of the noise model to training stability.\n2.  The contribution of the non-novel ensemble model towards bias correction and final performance improvement is difficult to disentangle."}, "questions": {"value": "1.  As noted in Weakness 1, training stability is the primary effect of the noise model, but no training curve plots or related metrics are provided in the ablation experiments. \n2.  The non-novel ensemble method significantly reduces the bias mean, and comparing Figures 13(e) and 13(f)  suggests it also helps reduce variance and kurtosis. As noted in Weakness 2, this raises the question whether the ensemble model might be more critical to performance improvement than the noise model.Could the authors provide ablation results across more environments for further analysis?\n3.  What is the experimental setup for LAROO w/o ensemble in Table 12, and why do the results differ from those in Table 10, where N = 1 and UTD = 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pY7ER2cHGt", "forum": "I7UK5qHNBL", "replyto": "I7UK5qHNBL", "signatures": ["ICLR.cc/2026/Conference/Submission24168/Reviewer_qzBg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24168/Reviewer_qzBg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761205470166, "cdate": 1761205470166, "tmdate": 1762942971083, "mdate": 1762942971083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals that in offline-to-online RL, the Q-value biases follow a heavy-tailed distribution. To address this, this paper proposes LAROO, which introduces Laplace -based loss function to replace L2 loss for Q update, and uses ensemble models for Q target estimation to reduce the estimation bias. Theoretical analysis is provided to show that LAROO exhibits a smaller estimation bias than typical L2-based Q updates. Experimental results demonstrate that LAROO outperforms previous baselines on D4RL datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Clarity.** Generally, this paper is well-written and easy to follow. The motivation and observations are clear.\n\n**Novelty.** I think the key finding that the Q bias follows a heavy-tailed distribution is interesting and the method that levearges Laplace-based noise model is reasonable. Also, the authors provide clear theoretical analysis on why the proposed method could reduce the Q bias.\n\n**Significance.** The experimental results show that LAROO outperforms the previous baselines."}, "weaknesses": {"value": "There are some points that need further clarifications.\n- The motivation that minimizing the KL divergence between $p(\\\\mathcal{Q}|\\\\mathcal{T}Q _ \\\\theta)$ and $q(\\\\mathcal{Q}|Q _ \\\\theta)$ in Line 225 is not clear. It seems just for the derivation of $D _ b(x)$. I wonder why minimizing such KL divergence could deal with the heavy-tailed Q bias issue and why it is reasonable. Could the authors give more clarifications on it?\n- In Line 277, the authors use TD-error as a surrogate for Laplace-based Q bias. That is to say, the TD-error is also assumed to follow a prior Laplace distribution. Then according to MLE, we could directly use L1 loss for Q update, then what is the advantage of using Equation (6)? Since Equation (6) is derived by minimizing the KL divergence, this is also related to the previous issue.\n- The heavy-tailed Q bias issue is not first observed in offline RL. Robust-IQL[1] also observes the heavy-tailed Q bias issue and addresses it with Huber loss, which is mroe easy to implement. This work uses Laplace distribution instead, could you demonstrate whether Huber loss is not used for your work?\n- LAROO seems not designed specifically for the offline-to-online setting, since Laplace-based Q update and ensemble models could also be applied to pure online setting. I wonder if the heavy-tailed Q bias issue also exists in pure online settings when using standard online RL algorithms like TD3 or SAC. If it does, what is the specific advantage of LAROO that makes it suitable for offline-to-online RL? If it does not, why does this issue manifest in the offline-to-online setting but not in pure online RL?\n-  (minor) In line 288, 'They' -> 'It'.\n\n[1] Towards Robust Offline RL Under Diverse Data Corruption. ICLR 2024"}, "questions": {"value": "Please refer to the weaknesses to address the concerns. I will check the authors' responses to decide whether to revise the rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fJbcyKKAmv", "forum": "I7UK5qHNBL", "replyto": "I7UK5qHNBL", "signatures": ["ICLR.cc/2026/Conference/Submission24168/Reviewer_SoSF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24168/Reviewer_SoSF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640633990, "cdate": 1761640633990, "tmdate": 1762942970798, "mdate": 1762942970798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and analyzes a previously unreported phenomenon in offline-to-online reinforcement learning (O2O RL): the Q-value estimation bias (Q-bias) tends to follow a heavy-tailed distribution during online fine-tuning. Such heavy-tailed behavior of Q-bias may introduce instability and impede effective performance improvement in fine-tuning. To address this, they propose Laplace-based Robust Offline-to-Online RL (LAROO), which models the Q-bias using a Laplace-distributed noise and introduces a robust policy evaluation loss derived from KL divergence minimization between Laplace distributions. LAROO further integrates an ensemble-based Q-value re-centering mechanism that shifts the Q-bias mean toward zero. Theoretical analysis shows that the proposed loss yields smaller single-step estimation bias than the widely-used L2 loss. Empirical results show improved stability and sample efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. (Empirical observation of heavy-tailed Q-bias)\nIn the MuJoCo domain, this work provides the empirical observation that Q-value biases in O2O RL exhibit heavy-tailed and positively skewed distributions during online fine-tuning (Figures 1, 4, 5). This is a significant observation beyond the mean and variance analysis. Moreover, the experiments (Figure 7) demonstrate that large-magnitude positive Q-biases may correlate with performance degradation.\n\nS2. (Laplace-based policy evaluation)\nThe introduction of Laplace-distributed noise for modeling heavy-tailed bias is well-motivated. The derived loss function $D_b(x)$ effectively suppresses the influence of extreme outlier errors, serving as an empirically effective alternative to the L2 loss (standard Bellman loss).\n\nS3. (Theoretical ground)\nThe theoretical analysis demonstrates that the proposed loss function reduces single-step estimation bias compared to the L2 loss, which plausibly contributes to the observed stability and consistent performance improvements.\n\nS4. (Plug-in compatibility) \nThe proposed method can be used as a plug-in component for existing O2O methods. The experimental results show that replacing the Bellman loss in baselines with the proposed loss consistently improves their performance."}, "weaknesses": {"value": "W1. (Questionable symmetry assumption of the Laplace model)\nAlthough empirical results (Figures 1, 4, 5) show that Q-bias distributions are typically heavy-tailed and right-skewed (i.e., asymmetric positive bias), LAROO assumes a symmetric Laplace distribution. This modeling choice simplifies the formulation but may fail to accurately capture the empirical asymmetry. Indeed, Figure 8 indicates that LAROO reduces the positive tail but may over-correct by introducing spurious negative bias.\n\nW2. (Limited justification for ensemble-based correction)\nThe proposed method integrates a random subset selection from ensemble Q-functions to re-center Q-bias, but it is somewhat heuristic. This paper lacks an intuitive explanation or theoretical reasoning as to why this mechanism effectively re-centers the bias.\n\nW3. (Q-bias analysis across limited domains)\nThe analysis of Q-bias and its heavy-tailed behavior is restricted to dense-reward environments such as MuJoCo. It remains unclear whether similar heavy-tailed characteristics would emerge in sparse-reward domains (e.g., AntMaze) or semi-sparse domains (e.g., Adroit, OGBench[1]).\n\nW4. (Theoretical and empirical disconnect)\nThe theoretical analysis establishes that the proposed loss reduces single-step estimation bias, but the connection to heavy-tailed variance reduction (the core empirical claim) remains indirect. It would strengthen the contribution to include a theoretical link between the Laplace modeling and variance-bounded Q estimation under heavy-tailed noise.\n\n\n[1] Park, Seohong, et al. \"Ogbench: Benchmarking offline goal-conditioned rl.\" arXiv preprint arXiv:2410.20092 (2024)."}, "questions": {"value": "Could you provide further clarification on the weaknesses? In particular, W1 and W3 will have the most significant impact on the overall rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RxVSBpiMJ1", "forum": "I7UK5qHNBL", "replyto": "I7UK5qHNBL", "signatures": ["ICLR.cc/2026/Conference/Submission24168/Reviewer_U4Kt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24168/Reviewer_U4Kt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923826735, "cdate": 1761923826735, "tmdate": 1762942970062, "mdate": 1762942970062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that the Q-value estimation error during offline-to-online (O2O) RL exhibits a heavy-tailed distribution, and proposes to address this using Laplace-distributed noise modeling. Concretely, a new loss function is derived that is less sensitive to outliers. The proposed method achieves state-of-the-art performance on O2O RL benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The investigation of the Q-estimation error distribution is thorough and convincing.\n- The derivation of the Laplace-based robust loss function (D_b) is concise yet effectively mitigates the impact of outlier errors.\n- The experiment results show that the proposed method consistently outperforms the considered baselines."}, "weaknesses": {"value": "- It would be helpful for the authors to examine whether the heavy-tailed error distribution only arises in the O2O RL setting. Similar distributions may also appear in other RL paradigms, such as purely online or offline RL. If that is the case, applying the proposed Laplace-based modeling to those settings could further validate its generality and effectiveness in handling heavy-tailed errors."}, "questions": {"value": "- Could the authors also evaluate the proposed method on the D4RL Kitchen benchmark, which, similar to Adroit, is a challenging environment in D4RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "650gGE02tO", "forum": "I7UK5qHNBL", "replyto": "I7UK5qHNBL", "signatures": ["ICLR.cc/2026/Conference/Submission24168/Reviewer_2zgx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24168/Reviewer_2zgx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008314833, "cdate": 1762008314833, "tmdate": 1762942969343, "mdate": 1762942969343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}