{"id": "JnwClln80Q", "number": 24682, "cdate": 1758359257821, "mdate": 1763115104060, "content": {"title": "Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture", "abstract": "Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8\\% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2\\%) and Memento (70.9\\%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.", "tldr": "", "keywords": ["Large Language Model Agents", "Tool Creation", "Experience Sharing", "Cognitive Memory Architecture", "Agent Evolution"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0c72809bcbfa8c766785804378a1c8fa253273f.pdf", "supplementary_material": "/attachment/713dcf4158958a42af7a7141efb2be3f76bab675.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SMITH, a unified memory-centered framework that combines dynamic tool creation with cross-task experience sharing for LLM agents. The system introduces a three-layer memory hierarchy (procedural, semantic, and episodic) and integrates it with a multi-agent workflow (planner, developer, tester) that iteratively generates and refines tools in a sandboxed environment. SMITH further incorporates a proxy-agent-based curriculum learning mechanism for adaptive task difficulty estimation and a self-critic ensemble strategy leveraging multi-path sampling with LLM-as-a-judge evaluation. Experiments on the GAIA benchmark show a significant improvement (81.8% Pass@1), with detailed ablations and visualization of experience clustering and memory evolution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated integration** – The paper tackles an important gap between tool generation and experience sharing in LLM agents, offering a conceptually elegant unification through memory hierarchies.\n2. **Comprehensive system design** – The procedural, semantic, and episodic memory layers are coherently defined, and the multi-agent workflow is thoughtfully implemented.\n3. **Strong empirical performance** – The system achieves the highest known Pass@1 result on GAIA (81.8%) and includes multiple ablations showing the contribution of curriculum learning and semantic memory."}, "weaknesses": {"value": "1. **Semantic task similarity assumption not rigorously validated.** The approach assumes that embedding-based similarity retrieval can accurately select relevant experiences, but no quantitative evidence (e.g., retrieval precision, correlation with performance) is provided. A baseline such as random retrieval or semantic distance ablation would clarify whether this assumption holds.\n2. **Ablation depth insufficient given the system’s complexity.** While the paper reports some component ablations (e.g., removing curriculum learning or episodic memory), the overall framework is quite large. It remains unclear which specific modules (multi-path sampling, LLM-as-judge, memory retrieval, self-critic ensemble) contribute most to the final gains. A more fine-grained or hierarchical ablation analysis would make the causal contributions clearer.\n3. **Fairness of the ensemble evaluation.** The “self-critic ensemble” combines multiple strong models (Claude-4, Claude-3.7, GPT-4.1) with multi-path sampling, which effectively triples inference cost. Comparing Pass@1 from this setup to baselines without such ensembles may overstate relative gains. A more equitable comparison would be versus Pass@3 or versus single-model setups with equivalent sampling budgets.\n4. **Limited benchmark coverage.** GAIA is a strong and diverse benchmark, but relying on a single dataset limits the generalizability of the claims. Validation on additional domains (e.g., API tasks, math reasoning, code generation) would strengthen the paper’s impact."}, "questions": {"value": "1. Inference cost and scalability: What is the approximate compute or API cost of SMITH’s self-critic ensemble per task (given 3 models × 3 paths × judging loops)? Can it be made practical for large-scale deployment?\n2. Reliability of experience reuse: Could stored experiences ever mislead later agents, especially if earlier trajectories encode suboptimal or outdated behaviors? Do you employ any mechanism for pruning or re-validating episodic memory entries over time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tJbKPqZsPN", "forum": "JnwClln80Q", "replyto": "JnwClln80Q", "signatures": ["ICLR.cc/2026/Conference/Submission24682/Reviewer_Edcr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24682/Reviewer_Edcr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557657590, "cdate": 1761557657590, "tmdate": 1762943162350, "mdate": 1762943162350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SMITH, a unified cognitive architecture that achieves dynamic tool creation and cross-task experience sharing through hierarchical memory management.\nSMITH enables dynamic tool creation via iterative code generation within a sandbox environment, and facilitates cross-task experience sharing through semantic similarity-based retrieval in episodic memory.\nTo optimize this dynamic learning process, the authors employ curriculum learning to control the sample presentation order and use a multi-agent ensemble re-estimation mechanism to reassess task difficulty instead of relying on manually labeled difficulty.\nExperiments on GAIA demonstrate that SMITH achieves new state-of-the-art (SotA) performance, with ablation studies confirming the effectiveness of each component."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes an agent paradigm based on episodic memory updating and semantic retrieval. The method treats the process of tool creation as part of episodic memory, thereby modeling and handling both tool creation and task trajectories within a unified framework.\n2. The proposed approach achieves strong performance on GAIA and demonstrates the necessity of each component through extensive ablation studies."}, "weaknesses": {"value": "1. Writing quality needs improvement.\n- The paper employs a large number of formalized notations, but many symbols are reused with inconsistent meanings, which can easily cause confusion.\n- For example, in lines 146–147, $\\mathcal{T}$ denotes the set of tools, but in lines 152–153 it denotes the set of tasks.\n- Similarly, $a$ represents the agent in lines 129–130, yet later in lines 173–174 and beyond it is reused to represent an action.\n- The paper frequently introduces symbols without immediate explanation, postponing their definitions until much later in the text, making it hard to follow. For instance, the authors never clearly define what $s$ represents as a state or what the action space of $a$ is.\n2. More datasets are needed to validate the reliability of the proposed method. The paper evaluates the approach only on GAIA, which, although challenging, cannot substantiate the claim of general-purpose.The reliability of SMITH’s semantic memory initialization and task difficulty re-estimation should be further assessed across additional benchmarks.\n3. Missing empirical evidence for memory-enhanced tool creation. In lines 62–63 of the Introduction, the authors state that Alita builds tools from scratch, whereas SMITH enhances tool creation through episodic memory. However, no direct experimental evidence is provided to demonstrate the advantage of memory-enhanced tool creation. Adding such experiments would strengthen the distinction between SMITH and comparable methods."}, "questions": {"value": "1. What is the intention behind Assumption 1 (lines 154–158)? The assumption states that experience transfer is allowed only when the task similarity exceeds a certain threshold. However, the proposed method does not appear to explicitly consider task similarity when filtering the episodic memory set—it instead directly updates the memory with successful tasks. According to the assumption, shouldn’t the model first compute task similarity to filter the memory set before performing retrieval?\n2. In the difficulty re-estimation module, how are the hyperparameters w_k and L’ selected? Are these parameters sensitive to their values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QkygkVsIyg", "forum": "JnwClln80Q", "replyto": "JnwClln80Q", "signatures": ["ICLR.cc/2026/Conference/Submission24682/Reviewer_MXXD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24682/Reviewer_MXXD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662046861, "cdate": 1761662046861, "tmdate": 1762943161864, "mdate": 1762943161864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an architecture for agentic tasks including three innovations: dynamic tool creations, experience sharing, and hierarchical memory. It achieves state-of-the-art performance on the GAIA benchmark. Ablation studies show that techniques such as curriculum learning is crucial to the performance increase."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel architecture or framework that is formally defined and empirically implemented to achieve SOTA on an established agentic benchmark\n- The components in the framework are themselves intuitive and their combination is reasonable\n- The analysis is comprehensive and informative"}, "weaknesses": {"value": "- As the paper proposes a rather complex architecture with many components, its readability would greatly improve with a more focused presentation. To name a few issues: \n  - Exactly what are the key contributions *in contrast to previous work*? As I understand, it is a 3+1 novelty in the pipeline (dynamic tool creations, experience sharing, and hierarchical memory + curriculum learning) but this is not sufficiently spelled out. \n  - I find Figure 1 uninformative as it doesn't pertain to the abovementioned components, while its introduction of concepts like \"loops\" and \"rollout\" distract me from understanding the key contribution.\n  - The relationship among the 4 components formalized in Section 3 can benefit from a diagram. Otherwise, it is very challenging to understand exactly how the framework works. \n- The formalization in Section 3 can be tighten up, first and foremost by defining every variable clearly. To name a few:\n  - Eq1: does $a$ refer to an LLM inference? \n  - Eq3: $\\rightarrow$ should not be used since we're not talking about a mapping (as in line 128), but a trajectory\n  - Line 143: what does \"dissolve\" mean here?\n  - Things get a bit more confusing starting Eq4; what is $s_0$? What is $m_i$? These lead to a very hard time understanding Eq5 and Eq6. \n- The definiton of **tool** and the tool creation process is vague and unconvincing. From the formalizaiton, it seems a tool is a trajectory of revision from one version of the code to another. Is this the working definition for \"tool\" in this line of work, as opposed to software (as shown in Figure 1 and also Section 1)? \n- Again due to the complexity of the introduced pipeline, the paper is currently not making it easy to compare with existing work. Beyond the current Section 2, it would be very helpful to closely compare to the closest framework like Alita and Memento."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ersid2hOJz", "forum": "JnwClln80Q", "replyto": "JnwClln80Q", "signatures": ["ICLR.cc/2026/Conference/Submission24682/Reviewer_qc3C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24682/Reviewer_qc3C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854444828, "cdate": 1761854444828, "tmdate": 1762943161659, "mdate": 1762943161659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SMITH (Shared Memory Integrated Tool Hub), a cognitive architecture that integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. The system organizes memory into procedural, semantic, and episodic components, employs a multi-agent workflow with planner-developer-tester loops, and uses a curriculum learning strategy based on agent-ensemble difficulty re-estimation. The authors evaluate SMITH on the GAIA benchmark, achieving 81.8% Pass@1 accuracy, outperforming Alita (75.2%) and Memento (70.9%)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "SMITH achieves state-of-the-art performance on GAIA with substantial improvements: +6.6% over Alita (best tool creation approach) and +10.9% over Memento.\n\nThe nested loop architecture (inner developer-tester loop + outer planner loop) with sandbox execution and multi-path sampling is well-engineered."}, "weaknesses": {"value": "The paper only evaluates on GAIA (165 validation tasks, 300 test tasks). This is insufficient to demonstrate generalization.\nI would suggest to evaluate on at least 2-3 additional diverse agentic benchmarks (e.g., WebArena, SWE-bench, HotPotQA, InterCode), and also the authors should test cross-domain transfer more rigorously.\nThere are also several missing related works, e.g. Agent Workflow Memory, ToolGen."}, "questions": {"value": "For the memory growth, how does retrieval performance degrade as episodic memory grows to 1000+ experiences? and for computational cost, how does cost scale with memory size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mwYmLCj0lN", "forum": "JnwClln80Q", "replyto": "JnwClln80Q", "signatures": ["ICLR.cc/2026/Conference/Submission24682/Reviewer_nnv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24682/Reviewer_nnv8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004815528, "cdate": 1762004815528, "tmdate": 1762943161440, "mdate": 1762943161440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}