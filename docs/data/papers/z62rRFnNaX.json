{"id": "z62rRFnNaX", "number": 12833, "cdate": 1758210691022, "mdate": 1763733574848, "content": {"title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models", "abstract": "Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.", "tldr": "The text generation speed of recurrent-depth LLMs can be significantly accelerated by treating them as diffusion models and using diffusion forcing samplers.", "keywords": ["Recurrent-Depth", "Latent Reasoning", "Efficiency", "Diffusion Forcing", "Parallelization", "Inference", "Decoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/216181a0eba6ec0fe42a8cd55eb4f530a4ca5a91.pdf", "supplementary_material": "/attachment/e613ba8489bd4793c91f3a3acbd939b8cd334c08.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies a sampler for a recurrent-depth Transformer language model by framing the recurrent-depth model as a diffusion model, motivated by a desire to increase sampling speed. \n\nThe authors detail the prerequisites of input injection and robustness to dynamic recurrence steps, as well as the importance of a recurrence-independent KV cache. \n\nThe sampler is proposed with either a fixed number of inner recurrences per-token, or with a simple adaptive exiting mechanism that emits final tokens when the normalized update distance is beneath a threshold.\n\nThe authors remark on the convergence of the proposed algorithm, as well as defend their sampler design with theoretical analysis of the costs of depth and width during prefilling and decoding respectively.\n\nFinally, the authors demonstrate that the proposed sampler achieves similar accuracy on math and code generation to existing algorithms while processing tokens nearly 5 times faster."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method elegantly decreases the time to sample from the studied recurrent-depth language model at apparently little-to-no decrease in performance\n- The paper flows well and is cleanly presented\n- This work introduces a potential connection between recurrent-depth language and diffusion models"}, "weaknesses": {"value": "- The evaluation is limited to one model family and solely evaluates math and coding reasoning, ignoring other recurrent-depth models and natural language tasks\n- The evaluation excludes analysis of memory cost, as the model decreases wall-clock time by increasing compute parallelization on GPU\n- Several remarks (Remark 3.1, Theorem 4.4, Conclusion), while thought provoking, may be misleading:\n  - The assumption that the recurrent block is a contraction is difficult to believe\n  - Theorem 4.4 states only the advantages (and not the disadvantages) of diffusion forcing sampling, and does not \"prove that recurrent-depth models should use diffusion forcing samplers during decoding\"\n  - A diffusion-inspired sampling method does not indicate that the model being sampled from is a diffusion model"}, "questions": {"value": "- In some inference systems, computational resources may be limited at varying timepoints. Can you profile the memory usage over wall-clock time for the evaluated samplers?\n- Figure 6 demonstrates that the noise coefficient strictly decreases performance, and is set to 0 in evaluations in Table 1 and Table 2. Is it actually beneficial? How unstable is the recurrence without the additive noise? Is sampling from the model quantifiably unstable?\n- Most hyperparameter choices (momentum, noise coefficient, headway, maximum wavefront size, initialization scale, continuous compute) appear to have little-to-no trade-off, generally decreasing model performance. The connection to diffusion seems tenuous given this. Is the main motivation of the paper to introduce an efficient parallel sampler or to examine the relation between recurrent-depth models and diffusion language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6gfVLSDV6U", "forum": "z62rRFnNaX", "replyto": "z62rRFnNaX", "signatures": ["ICLR.cc/2026/Conference/Submission12833/Reviewer_UbUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12833/Reviewer_UbUX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949946070, "cdate": 1761949946070, "tmdate": 1762923633561, "mdate": 1762923633561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient mechanism for parallelizing the extra computation in recurrent-depth language models to accelerate their slow inference speed. Specifically, it introduces a diffusion forcing sampler that, at every forward pass, decodes a new \"draft\" token at the end of the sequence while simultaneously refining the latent states of all previously drafted tokens in parallel. This sampler uses an adaptive exit criterion to \"freeze\" tokens once their latent states stabilize, allowing the generation to proceed in an efficient \"wave\". The experiments show that this sampler can be applied directly to existing 3.5B recurrent-depth models without any retraining, leading to a 5x speedup on reasoning and coding benchmarks with only minor trade-offs in accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The connection between recurrent latent update and diffusion sampling is relevant and interesting. \n2. The proposed sampler achieves significant speedup with only mild degradation of quality. \n3. The method is designed to work with KV cache sharing, which allows it to have a memory footprint no larger than a standard fixed-depth transformer, preventing the cache from growing with the number of recurrence steps. \n4. The authors explored some key factors that affects the stability of latent recurrence."}, "weaknesses": {"value": "1. The theoretical analysis section seems not fully formalized. The concepts of \"depth scaling\" and \"width scaling\" seem to be created specifically for their argument, rather than established, well-understood principles. Furthermore, the paper's core mechanism relies on the recurrent-depth model's states converging. However, the authors admit in Remark 3.1 that they cannot formally prove this. Sec 4.2 seem disconnected from the other part of the paper. There could be simpler way to formalize the high level intuitions. \n2. The authors explicitly state that their experimental evaluation is limited to a batch size of 1. They acknowledge that extending the sampler to batched or continuously-batched inference is complex and \"fall outside the scope of this study\". This is a limitation but I look forward to future development of the proposed method.\n3. In Appx A.2, the authors state that \"We experiment with headways greater than one, but while interestingly stable, this accelerates the speed of the sampler only slightly, at a large cost to accuracy\". So even though the paper is written in a way that strongly relates to diffusion models, the generation is still largely next-token prediction."}, "questions": {"value": "Why does the noise injection to z (Eq. 2) only happen at the start of a new round of inner recurrence? Actually, is it helpful or not? It looks like in the reported results, $\\beta_s$ is set to 0. And Fig. 6 shows that on the GSM8k benchmark is achieved at $\\beta_s = 0.00$. What motivates the introduction of this $\\beta_s$ apart from connecting it to Diffusion forcing? And how to understand its impact on the throughput?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NnD4o5I6q8", "forum": "z62rRFnNaX", "replyto": "z62rRFnNaX", "signatures": ["ICLR.cc/2026/Conference/Submission12833/Reviewer_5EMW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12833/Reviewer_5EMW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114011583, "cdate": 1762114011583, "tmdate": 1762923633288, "mdate": 1762923633288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes parallelizing inference for recurrent-depth transformers by processing multiple token positions simultaneously at different recurrence depths, achieving significant speedup. The approach is claimed to connect recurrent-depth models to diffusion models and can be applied to existing models without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper tried to address a genuine bottleneck in recurrent-depth model inference, and the acceleration is significant.\n\n* This method makes a good approach that can be applied directly to existing models without further training."}, "weaknesses": {"value": "* The base model that this paper used was not proved to be fundamental better than GPT/AR based method. The contribution of this paper is questioned given if the recurrent-depth model is a promising direction or not. \n\n* The method cannot guarantee producing the same output as sequential generation, which creating fundamentally different computational paths. For use cases requiring reproducibility, this is a non-starter. The paper should clearly state this limitation and specify when the method is/isn't appropriate. \n\n* The evaluation results are very limited. All the four benchmarks share a very particular property: they use extraction-based metrics that completely ignore generation quality. What completely missing are summarization, translation, long-form QA, dialogue, creative writing, and general language understanding tasks where fluent generation matters. More language quality metrics are reported (perplexity, BLEU, human evaluation, token-level accuracy), while there are 15+ benchmarks results in the base model. \n\n* I'm not an expert of diffusion, but I think the connection to diffusion models is superficial to the point of being misleading. The paper reveals noise was added post-hoc as a hack, not as part of a principled diffusion framework."}, "questions": {"value": "* Why only four benchmarks when the base model likely tested on 10-15? Can you provide results on all benchmarks from Geiping et al. 2025, particularly summarization, translation, long-form generation, dialogue, and general language understanding tasks?\n\n* Can you provide perplexity, BLEU/ROUGE scores, human evaluation, and token-level accuracy to assess actual generation quality rather than just final answer correctness?\n\n* What specifically makes this \"diffusion\" versus standard iterative refinement? Was the model trained with any diffusion objective, or is the connection purely post-hoc?\n\n* On which task types does the method degrade significantly? Are there examples where generation quality is poor despite correct final answers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ADzeuX1AAF", "forum": "z62rRFnNaX", "replyto": "z62rRFnNaX", "signatures": ["ICLR.cc/2026/Conference/Submission12833/Reviewer_EH9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12833/Reviewer_EH9x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762584334055, "cdate": 1762584334055, "tmdate": 1762923632964, "mdate": 1762923632964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}