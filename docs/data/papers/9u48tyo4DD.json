{"id": "9u48tyo4DD", "number": 24716, "cdate": 1758359607929, "mdate": 1759896752948, "content": {"title": "Scale Contrastive Learning with Selective Attentions for Blind Image Quality Assessment", "abstract": "Human visual perception naturally evaluates image quality across multiple scales, a hierarchical process that existing blind image quality assessment (BIQA) algorithms struggle to replicate effectively. This limitation stems from a fundamental misunderstanding: current multi-scale approaches fail to recognize that quality perception varies dramatically between scales—what appears degraded when viewed closely may look acceptable from a distance. This inconsistency not only creates misleading ``visual illusions'' during feature fusion but also introduces substantial redundant information that dilutes quality-critical features and leads to imprecise assessments. Our CSFIQA framework advances multi-scale BIQA via two key innovations: (1) a selective focus attention mechanism that mimics human visual attention by filtering out redundant cross-scale information that would otherwise mask subtle quality indicators, and (2) a scale contrastive learning strategy that explicitly learns to distinguish quality variations both across and within scales. By incorporating an adaptive noise sample matching mechanism, CSFIQA effectively identifies perceptual quality discrepancies in the same content viewed at different scales. Experiments demonstrate substantial improvements over state-of-the-art methods across seven datasets, achieving up to 8.8% SRCC improvement on challenging real-world distortions, confirming CSFIQA's superior alignment with human quality perception.", "tldr": "", "keywords": ["Image Quality Assessment", "Contrastive Learning", "Multi-scale"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b7f90ddd5fdb7fb860374913c7280430d1c6308.pdf", "supplementary_material": "/attachment/740135e2112b0160b89ae7f6b280a75cb814b182.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CSFIQA, a multi-scale blind image quality assessment (BIQA) framework that integrates two main innovations:\n(1) a Selective Focus Attention (SFA) mechanism designed to filter redundant cross-scale information and concentrate on quality-relevant cues, and\n(2) a Scale Contrastive Learning (SCL) strategy, combined with an adaptive Noise Sample Matching (NSM) module, to model scale-dependent quality differences and mitigate the so-called “visual illusion” problem in existing BIQA models.\n\nThe authors evaluate CSFIQA on seven public IQA datasets (LIVE, CSIQ, TID2013, LIVEC, KonIQ-10K, LIVEFB, SPAQ) and report superior SRCC/PLCC performance over 16 baselines (e.g., DEIQT, LoDa, QFM-IQM). Ablation and visualization analyses suggest that both SCL and SFA modules contribute to the improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper evaluates across seven standard datasets (synthetic and authentic distortions), performs cross-dataset tests, and includes ablations on hyperparameters (λ, [α, β], τ). This extensive coverage indicates careful empirical effort.\n\n2) The reported +8.8% SRCC on LIVEFB and solid results on KonIQ-10k and LIVEC suggest that the proposed modules capture some useful cross-scale cues, validating the importance of scale-aware modeling.\n\n3) The inclusion of detailed module-wise ablations (SCL vs. SFA vs. NSM) and computational statistics in the appendix allows partial reproducibility and insight into design sensitivity."}, "weaknesses": {"value": "1) The combination of contrastive learning, multi-scale encoding, and selective attention follows directly from existing BIQA pipelines (CONTRIQUE, Re-IQA, MUSIQ, LoDa). The work lacks a novel theoretical formulation or perceptual model explaining why the specific design choices improve human alignment.\n\n2) Terms like “visual illusion” and “information dilution” are used as rhetorical devices rather than measured phenomena. There is no statistical evidence that cross-scale fusion actually causes these effects.\n\n3) The “Information Concentrator Module” uses a frozen LLaMA-7B, but the rationale and mechanism are opaque. No ablation quantifies its contribution; its inclusion increases parameters from 118M (LoDa) to 233M with negligible improvement.\n\n4) The experimental setup may favor CSFIQA: the pre-trained CrossViT backbone and different patch sizes might lead to stronger features than prior models. Fair comparison requires matching architecture capacity or retraining baselines under equivalent compute."}, "questions": {"value": "1) Have authors measured perceptual inconsistency across scales using subjective ratings or MOS re-annotation to confirm this hypothesis?\n\n2) What motivates using LLaMA-7B for attention concentration instead of a lightweight vision-only transformer or MLP? How does it contribute beyond dimensional mapping?\n\n3) How were the thresholds (γ₁=0.2, γ₂=0.7) selected? Could the model benefit from adaptive thresholding or temperature scaling tuned per dataset?\n\n4) Given the large model (233M parameters, 45 G MACs), how does the performance compare if the frozen LLM is removed or replaced with a smaller encoder?\n\n5) Were all baselines trained and evaluated under identical data splits, augmentation, and epochs? Some methods (e.g., LoDa, DEIQT) use different pre-training strategies; please clarify to ensure fair comparison.\n\n6) How would CSFIQA perform on AIGC-IQA datasets or other-field IQA, where cross-scale distortion consistency differs (as you briefly note in Sec. A.7)?\n\n7) Could this approach extend to temporal quality assessment or multimodal IQA (video, 3D, or generated content)? If so, how would scale contrastive principles transfer?\n\n8)  How does SCL differ in principle from prior quality-aware contrastive objectives that already consider multi-scale or distortion-level variations? Does SFA introduce a new attention mechanism, or does it merely re-parameterize the top-k masking already seen in transformer pruning or efficient attention literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JfQgToiL0a", "forum": "9u48tyo4DD", "replyto": "9u48tyo4DD", "signatures": ["ICLR.cc/2026/Conference/Submission24716/Reviewer_uaWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24716/Reviewer_uaWk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761284562788, "cdate": 1761284562788, "tmdate": 1762943172630, "mdate": 1762943172630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CSFIQA, a new blind image quality assessment (BIQA) framework designed to better align with human multi-scale visual perception. The authors argue that existing multi-scale BIQA models fail because they naively fuse features across scales, creating “visual perception conflicts” and redundant information that weakens quality-sensitive cues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance is good.\n2. The figures are well drawn."}, "weaknesses": {"value": "1. The paper should better illustrate how the attention mechanism identifies redundant vs. informative cross-scale features.\n2. The contrastive learning process needs clearer formulation. How to define positive or negative pairs.\n3. Multi-scale models can be heavy. The paper should report: parameters, FLOPs, inference speed, relative to baselines. This is essential for real-time/embedded use cases."}, "questions": {"value": "1. What is intra-scale contrastive learning and inter-scale contrastive learning?\n2. What is the function of cross-attention.\n3. Why using cross-attention between small patches and larger patches rather than feature fusion. This may increase computational cost and may not useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HmzV3fAcUf", "forum": "9u48tyo4DD", "replyto": "9u48tyo4DD", "signatures": ["ICLR.cc/2026/Conference/Submission24716/Reviewer_SE3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24716/Reviewer_SE3v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836421685, "cdate": 1761836421685, "tmdate": 1762943172034, "mdate": 1762943172034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed CSFIQA presents a conceptually interesting and technically promising framework that models scale-dependent perception in Blind Image Quality Assessment (BIQA). The paper introduces two key innovations: Scale Contrastive Learning (SCL) and Selective Focus Attention (SFA), aiming to mitigate “visual illusions” and “information dilution” across scales. The idea of explicitly learning quality relationships between scales is novel and timely."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Recognizes and formalizes the long-neglected problem of scale-dependent perceptual variation, offering a new perspective for BIQA.\n+ Provides GradCAM visualizations illustrating attention improvements over baselines.\n+ Covers eight datasets (synthetic and authentic), with ablations on all modules and hyperparameters."}, "weaknesses": {"value": "- The conceptual definition of \"visual illusion\" and its mathematical mapping to the contrastive loss is vague. It is unclear how the “illusion” manifests quantitatively and why contrastive learning inherently solves it.\n- The SCL formulation (Eq.1–3) lacks theoretical justification. Why should MOS-based pairwise distances define positive/negative relations? Are these thresholds robust across datasets?\n- Whether the cross-dataset results (Tab. 2) are trained on synthetic → authentic or vice versa affects generalization claims.\n- The contrastive learning novelty overlaps with Re-IQA and CONTRIQUE. The paper should clarify how scale contrastive learning differs from existing sample-level contrastive strategies\n-  The paper claims improved speed due to information filtering (Appendix A.4), but the parameter count is large (233M). The frozen LLM dominates computation."}, "questions": {"value": "- The paper lacks a clear description of the validation set or model selection protocol. While the datasets are split into training and testing sets (80/20), it is unclear whether a separate validation set was used for hyperparameter tuning, early stopping, or model selection. This omission raises potential concerns about data leakage or overfitting, especially since the model integrates several modules (SCL, NSM, SFA) with multiple tunable parameters. \n- Typos: Line 164/245: “pacth” → “patch.”; Appendix A.6: “CSIQA” should be “CSFIQA.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kcsc41ScJo", "forum": "9u48tyo4DD", "replyto": "9u48tyo4DD", "signatures": ["ICLR.cc/2026/Conference/Submission24716/Reviewer_tKTC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24716/Reviewer_tKTC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927071424, "cdate": 1761927071424, "tmdate": 1762943171834, "mdate": 1762943171834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Contrast-Constrained Scale-Focused Image Quality Assessment (CSFIQA) framework to overcome two key limitations in multi-scale BIQA: \"visual illusions\" and \"information dilution,\" which compromise human perception alignment. The core contribution lies in the Scale Contrastive Learning (SCL) strategy, augmented by Noise Sample Matching (NSM), which explicitly models quality discrepancies across and within different visual scales. Complementing this, a Selective Focus Attention (SFA) mechanism filters redundant cross-scale information, thereby enhancing sensitivity to subtle, quality-critical features. Evaluations conducted across seven benchmark datasets confirm that CSFIQA achieves superior performance over state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n- The paper clearly identifies specific challenges of \"visual illusions\" and \"information dilution\" that plague traditional multi-scale BIQA approaches.\n- The Scale Contrastive Learning (SCL) framework, utilizing MOS similarity to select positive and negative pairs, and the Noise Sample Matching (NSM) mechanism, which targets regional quality variations, represent a novel and effective strategy for mitigating scale-dependent perceptual distortions.\n- The proposed CSFIQA achieves SOTA results. The model demonstrates robust generalization capabilities, achieving the best performance across various cross-dataset validation experiments."}, "weaknesses": {"value": "Weaknesses\n- One of the main motivations, \"visual illusions\" (the phenomenon where quality perception changes with scale), is not directly reflected in the classification component of SCL. Instead, SCL merely uses MOS to classify positive and negative pairs. Although NSM is designed to address this issue, the SCL component itself appears to adopt a method with the same limitations (regarding visual illusions) that the paper's motivation criticizes in existing approaches.\n- NSM relies on the strong assumption that ViT feature similarity equates to quality similarity, rather than the semantic similarity of patches. This assumption may be invalid if the \"information dilution\" problem persists. Therefore, more rigorous experiments are needed to validate that the model's ViT features primarily emphasize quality-related components to rationalize NSM.\n- While the paper proposes a loss function and architecture that overcome limitations within the specific field of IQA, its theoretical contribution from a general machine learning perspective appears weak. The work is limited to the narrow domain of IQA and, as a result, may not have a significant impact on the general ICLR community.\n- Given the paper's focus on scale-dependent quality variations, it would be beneficial to add experiments. These experiments should independently analyze how this phenomenon manifests for different distortion types and demonstrate how the proposed method addresses each.\n- Despite the AFS improving training time, the overall computational demand, indicated by a high MACs value (45G) compared to competitors like TReS (8.39G), suggests the model may be computationally heavy during inference.\n\nMinor\n- It should be mentioned that Eq. 3 is identical to the InfoNCE loss.\n- Incomplete descriptions of notations F+ and F- in Eq. 3\n- The text in most of the main figures is too small."}, "questions": {"value": "Please check the major weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FkoEmR6gZw", "forum": "9u48tyo4DD", "replyto": "9u48tyo4DD", "signatures": ["ICLR.cc/2026/Conference/Submission24716/Reviewer_Kj3d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24716/Reviewer_Kj3d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950846605, "cdate": 1761950846605, "tmdate": 1762943171612, "mdate": 1762943171612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}