{"id": "P2m7gvtfrE", "number": 10321, "cdate": 1758167076555, "mdate": 1759897658804, "content": {"title": "How Does Preconditioning Guide Feature Learning in Deep Neural Networks?", "abstract": "Preconditioning is widely used in machine learning to accelerate convergence on the empirical risk, yet its role on the expected risk remains underexplored.\nIn this work, we investigate how preconditioning affects feature learning and generalization performance. We first show that the input information available to the model is conveyed solely through the Gram matrix defined by the preconditioner’s metric, thereby inducing a controllable spectral bias on feature learning. Concretely, instantiating the preconditioner as the $p$-th power of the input covariance matrix and within a single-index teacher model, we prove that in generalization, the exponent $p$ and the alignment between the teacher and the input spectrum are crucial factors. We further investigate how the interplay between these factors influences feature learning from three complementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution generalization, and (iii) Forward knowledge transfer. Our results indicate that the learned feature representations closely mirror the spectral bias introduced by the preconditioner---favoring components that are emphasized and exhibiting reduced sensitivity to those that are suppressed. Crucially, we demonstrate that generalization is significantly enhanced when this spectral bias is aligned with that of the teacher.", "tldr": "We clarified the influence of preconditioning in machine learning optimization on feature learning and generalization performance in neural network models.", "keywords": ["Preconditioning", "Optimization", "Feature Learning", "Generalization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a74785be3ac8a8dca8e648578172fddf7fa1289.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors consider the effect of an unsupervised linear transformation, a so-called preconditioning matrix, applied to the data on feature learning in the single-index model case. In particular, the authors consider the specific preconditioning matrix is a spectral transformation of the input covariance. The authors show theoretically that the first layer weights, with Gaussian initialization, erase information about the original basis representation of the data. The authors then perform experiments showing that aligning input covariance directions with the task-relevant direction improves performance, and, similarly, unaligning the data with task-direction worsens performance. They show this effect in the settings of noise and transfer learning. They provide observations of the effect of various observations on two synthetic settings with their preconditioning approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "-\tThe paper addresses fundamental questions of deep learning: (1) how do neural networks learn features from data, and (2) what is the impact of optimization method on feature learning?\n-\tThe experiments are thorough and are presented cleanly."}, "weaknesses": {"value": "The observations in the paper are frankly not surprising at all and are relatively shallow in nature. First, their form of preconditioning is equivalent to viewing the effect of the covariance of the data on generalization, as their preconditioning matrix is neither trained nor supervised. This setting would be much more interesting in those cases or if preconditioning were studied in parameter space. \nThen, it is not surprising that reducing the variance of the data in the directions that are relevant for prediction would worsen performance as you are emphasizing unimportant direction and placing the burden on the neural network to unlearn this preconditioning. This “unlearning” of unimportant directions has been studied much more precisely both theoretically and empirically in many of the prior works on feature learning (e.g. [1]).\n\n[1] Damian, Lee, Soltanolkotabi. “Neural Networks can Learn Representations with Gradient Descent”, 2022. \n\nThe most interesting part of the paper to me is the experiments observing the comparison of various optimizers on their single index task across exponents for their preconditioning. However, the results for this setting are presented pretty much without explanation and matter-of-factly. It is not clear what the reader should take away from these experiments."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fzOwUeN4pZ", "forum": "P2m7gvtfrE", "replyto": "P2m7gvtfrE", "signatures": ["ICLR.cc/2026/Conference/Submission10321/Reviewer_knWb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10321/Reviewer_knWb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761087197426, "cdate": 1761087197426, "tmdate": 1762921661130, "mdate": 1762921661130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how preconditioning affects generalization, arguing that its primary role is to control and bias the feature learning process.\nThe central argument is that the preconditioner (e.g., from Adam or K-FAC) acts as a filter, inducing a \"spectral bias\" by forcing the model to only see the data through a specific, preconditioned Gram matrix.\nThe authors model this by instantiating the preconditioner as the $p$-th power of the input covariance, $\\Sigma_X^p$, where the exponent $p$ controls this bias: large $p$ emphasizes high-variance features, while small $p$ emphasizes low-variance features.\nUsing synthetic \"teacher-student\" experiments, the paper demonstrates that generalization is only achieved when the optimizer's spectral bias is aligned with the teacher's signal."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper investigates an interesting question with a reasonable approach. The results appear to me sound and convincing."}, "weaknesses": {"value": "W1. The experiments are done with synthetic data. Is there any way to test this idea on more realistic setups?\n\nOther than this, there are no obvious weaknesses to me at the moment, though I unfortunately cannot strongly endorse the paper as I am a non-expert reviewer (see confidence)."}, "questions": {"value": "Q1. What are the implications of your findings for how one might set the hyperparameters for off-the-shelf optimizers like Adam?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "KyoHjctU3t", "forum": "P2m7gvtfrE", "replyto": "P2m7gvtfrE", "signatures": ["ICLR.cc/2026/Conference/Submission10321/Reviewer_oPxn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10321/Reviewer_oPxn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938573188, "cdate": 1761938573188, "tmdate": 1762921660696, "mdate": 1762921660696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the role of preconditioning (specifically by powers of the input covariance matrix) on feature learning is studied. In particular, varying the power of the preconditioner affects the quantitative properties of the learned features via imposing a spectral bias of varying strength. The implications are demonstrated via noise robustness properties, generalization out-of-distribution, and transfer learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, I think this is a solid paper that presents an interesting perspective on a scale of preconditioning that (as far as I know) has not been studied in prior work. The key theoretical result demonstrates the model evolution given a training batch can be fully captured by some salient statistics. These statistics clearly delineate how the \"spectral bias\" arises in feature learning. The experiments comparing multiple optimizers, notably SAM (corresponding approximately to the preconditioner power $p=1$), GD ($p=0$) and various second order optimizers ($p=-1$) serves as an interesting empirical reflection of the theoretical predictions."}, "weaknesses": {"value": "In my opinion, the main weaknesses of the paper is the extrapolation from the provided theory and empirical claims. Notably, the theory in the paper focuses on one source of preconditioning via the input covariance, the experiments consider a wide range of preconditioned optimizers lumped under \"second-order methods\". Regardless of the empirical results, the qualitative differences between the curvature matrices approximated across the different methods is large enough that I'd be hesitant to attribute observed trends to the proposed spectral biases, even if for certain cases there is certainly alignment between different curvature matrices. Some more concrete derivations (even in simple one/two-layer linear networks) to show the relation between the curvature matrices between the different optimizers would help clarify this point.\n\nIt should also be noted that some optimizers in the \"second-order\" $p=-1$ category (e.g. AdaHessian and Adam) take a square-root power of the preconditioner, which suggests $p=-1/2$ would be more accurate there. Also, is it supported why SAM corresponds to $p=1$?\n\nLastly, though the takeaway in Section 4.3 that \"second-order\" optimization has a role in transfer learning, this observation is not necessarily novel, see e.g. [1], where the proposed intervention is indeed to pre-condition by the inverse input covariance matrix to enable feature/transfer learning. Furthermore, considering the specific discussion about KFAC is rather light, it might be good to note the corresponding \"right-side\" preconditioner KFAC induces on the input layer weight matrix is also the inverse input covariance.\n\n[1] Zhang et al. \"Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data\""}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zg5IQML5Cu", "forum": "P2m7gvtfrE", "replyto": "P2m7gvtfrE", "signatures": ["ICLR.cc/2026/Conference/Submission10321/Reviewer_vLdJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10321/Reviewer_vLdJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979067436, "cdate": 1761979067436, "tmdate": 1762921659845, "mdate": 1762921659845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}