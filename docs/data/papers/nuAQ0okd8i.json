{"id": "nuAQ0okd8i", "number": 19101, "cdate": 1758293573411, "mdate": 1759897059300, "content": {"title": "SELECTIVE FINE-TUNING FOR TARGETED AND ROBUST CONCEPT UNLEARNING", "abstract": "Text-guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models’ likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state-of-the-art methods depend on full fine-tuning, which is computationally expensive. Concept localisation methods can facilitate selective fine-tuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine-Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective fine-tuning, empowered by a Hessian-based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree (∆F ID = 0.02), and is also significantly (2.5 times) faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.", "tldr": "Given the brittle nature of existing methods in unlearning harmful content in diffusion models, we propose TRuST, a novel approach for dynamically estimating target concept neurons and unlearning them by selectively fine-tuning.", "keywords": ["Unlearning", "Stable Diffusion", "Alignment", "Model Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7340827bad3b9f83e4c3c95230793f2a17a16e3.pdf", "supplementary_material": "/attachment/04018b1007575fc9fccd2809a15f1448afdf31b3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TRUST, a selective fine-tuning framework for concept unlearning in T2I tasks. The method has three parts: (1) discovering concept neurons by dynamically estimating masks in cross-attention via gradients of CLIP Score; (2) two unlearning objectives**—**CIP  and CSR ; and (3) an adaptive mask-guided fine-tuning process that repeatedly re-estimates masks during training. The method targets both single concepts and concept combinations. The paper claims better unlearning of individual and compositional concepts with smaller utility loss and improved efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Handles concept combinations/conditional associations, which many unlearning baselines struggle with; TRUST reports effectiveness on CCE/CoCE while preserving individual benign concepts.\n- Method-level clarity and modularity: dynamic, gradient-aligned discovery of concept neurons in CA (k/q/v) + two objectives (CIP & CSR ) that minimize, respectively, the number and the sensitivity of targeted parameters.\n- Empirical utility preservation: on non-targeted concepts, small changes in ΔFID/CLIP/TIFA compared to the original model, while improving robustness vs. several adversarial prompt generators."}, "weaknesses": {"value": "- CFG assumption & negative prompts. CFG is not universal (e.g., distilled models like SDXL-Turbo). Since UNet fine-tuning changes parameters shared by unconditional/negative branches, please qualify CFG-dependent claims and explain how TRUST behaves in non-CFG settings, including any mitigation for negative-prompt consistency (e.g., prompt dropout/rebalancing).\n- Typo in Algorithm 2. The loss type is listed as {CR, CN}; main context indicates these should correspond to {CSR, CIP}. Please fix the notation and ensure consistency across text/algorithms.\n- Sampling strategy & hyperparameters. State core choices in the main text: sampler (DDIM in the code),  whether gradients are accumulated across the full sampling trajectory, default value of the threshold $\\xi$ (2.0 in the code). \n- The presentation can be improved."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iadJX3HAKu", "forum": "nuAQ0okd8i", "replyto": "nuAQ0okd8i", "signatures": ["ICLR.cc/2026/Conference/Submission19101/Reviewer_LEwL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19101/Reviewer_LEwL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827717916, "cdate": 1761827717916, "tmdate": 1762931128038, "mdate": 1762931128038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Thiis paper proposes TRUST (Targeted Robust Selective fine-Tuning), a method for concept unlearning inT2I diffusion models. TRUST dynamically reestimates concept neurons inside cross-attention layers by measuring how gradients of a CLIP-based alignment objective respond to a target prompt. It then selectively fine-tunes only those neurons using one of two objectives: CIP (Concept Influence Penalty) which sparsifies high-impact neurons and CSR (Concept Sensitivity Reduction) which reduce gradient sensitivity (Hessian-aware soft suppression).  TRUST recomputes the mask at every step to track representation drift. Experiments report very low ASR against several adversarial prompt generators, small difference in the generation quality of retained concepts, support for concept combinations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1- The contribution is clear. The paper identifies and acts on the fact that salient neurons shift during optimization, so a static mask is suboptimal.\n\n2- The two finetuning objectives (CIP vs CSR) provide a useful trade-off between removal effectiveness and retention of other concepts.\n\n3- Evaluation on multiple benchmarks and the consideration of multiple baselines.\n\n4- Good results."}, "weaknesses": {"value": "1- My main concern is that efficiency claims focus on steps, not compute. Dynamic mask recomputation requires repeated gradient passes. CLIP forward/backwards are required every step. The method relies on a generated image which means backpropagation through the VAE too. The paper compares steps but doesn’t report wall-clock, GPU hours, memory, or FLOPS vs. baselines tuned for speed. The 2.5 faster claim in abstract is misleading.\n\n2- It's unclear whether the basline values are reproduced or reported based on the original papers. The paper does not mention the finetuning setting of the baselines. It's unclear whether the evaluations are fair.\n\n3- The paper uses CLIP-based saliency and it also evaluates on CLIP. The mask definition is driven by CLIP Score gradients. Fidelity evaluation also leans on CLIP/TIFA. This can create objective-metric coupling: if the model learns to game CLIP features, both saliency and reported utility may look good while semantic unlearning in a human sense is weaker.\n\n4- There is no study of the hyperparemters $\\xi$ and $\\beta$. How were the values in Appendix A.8 chosen?\n\n5- CSR and CIP are not compared in wall-clock time and compute per step. There is an unclear \"2-3 hours for CSR experiments and took 3-4 hours per epoch for CIP experiments.\" in line 1220 which in insufficient.\n\n6- Do the considered baselines add a preservation loss to the finetuning objective? That is orthogonal to the unlearning objective."}, "questions": {"value": "1- Why use CLIP for saliency? Have you tested saliency via alternative alignment signals (BLIP, QA loss, etc) and does performance hold? How correlated would masks be across aligners?\n\n2-  Please report wall-clock time, Compute, peak memory, and the per-step overhead of mask recomputation (include CLIP backprop) vs. static-mask baselines based on noise prediction.\n\n3- Line 409-410: \"Notably, it achieves 18.52%, 12.1%, 10.57%, 14.33%, and 1.89% degradation in ASR over the UnlearnDiffAtk, MMA-Diffusion, Ring-a-Bell and P4D baselines.\" This doesn't make sense and should be revised.\n\n4- Line 460-462 contradicts  Table 3. CSR users fewer steps. Is this a type?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HcPb2KkPO8", "forum": "nuAQ0okd8i", "replyto": "nuAQ0okd8i", "signatures": ["ICLR.cc/2026/Conference/Submission19101/Reviewer_N5XW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19101/Reviewer_N5XW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850992825, "cdate": 1761850992825, "tmdate": 1762931127317, "mdate": 1762931127317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TRUST, which achieves selective unlearning of specific concepts and concept combinations, while preserving the quality and efficiency for benign concepts.  Specifically, TRUST first dynamically estimates “concept neurons” within cross-attention layers and repeatedly updates the mask during fine-tuning so that only those parameters are selectively adapted. In addition, two complementary regularizers are introduced: CIP (Concept Influence Penalty), which performs “hard” unlearning by penalizing the cardinality of high-influence parameters; and CSR (Concept Sensitivity Reduction), which performs “soft” unlearning by minimizing the gradient sensitivity of the noise-prediction to the target parameters. Extensive experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The writing is fluent and logically coherent, exhibiting strong readability.\n* Dynamic localization of concept neurons mitigates drift. TRUST re-estimates the mask each step, avoiding outdated static selections and directly addressing observed “saliency drift” during training.\n* Complementary regularizers for hard/soft unlearning. CIP and CSR cover different deployment needs (compliance-oriented vs. fidelity-oriented) and are accompanied by clear mechanistic contrasts and visual analyses.\n* Strong adversarial robustness with minimal collateral damage. ASR drops significantly under diverse attacks while benign-prompt quality changes (e.g., $\\Delta$FID/CLIP/TIFA) remain small, indicating a well-balanced forget/retain trade-off."}, "weaknesses": {"value": "* Some ablation studies are needed to demonstrate the effectiveness of the method. For example, in the case of the CIP regularization, how does it compare to directly deactivating all concept neurons?\n* It is necessary to show more diverse visual examples of concept unlearning, for example, removing specific stylistic concepts.\n* For the conditional concept unlearning problem, is there any relationship between the distribution of activated neurons and that of single-concept unlearning? For example, do the neurons corresponding to “a cat on the table” represent the intersection of those associated with “cat” and “table”?"}, "questions": {"value": "See 'Weaknesses'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nyA6Jj79ux", "forum": "nuAQ0okd8i", "replyto": "nuAQ0okd8i", "signatures": ["ICLR.cc/2026/Conference/Submission19101/Reviewer_re6D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19101/Reviewer_re6D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915005467, "cdate": 1761915005467, "tmdate": 1762931126622, "mdate": 1762931126622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes TRUST, a new framework for concept unlearning in text-to-image models by dynamically identifying concept neurons using cross-attention saliency.  The authors also introduce two new unlearning objective functions that dynamically update the identified neurons. Experimental results show that TRUST is robust against adversarial prompts while preserving generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I am not quite familiar with unlearning for diffusion models, and therefore cannot confidently assess the quality of this paper. I would recommend that the AC seek input from reviewers who are more familiar with this topic."}, "weaknesses": {"value": "Line 53: there should be a comma before \"leading to\".\n\nLine 107: missing space in TRUSTis.\n\nLine 194: suppress $\\to$ suppresses"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "jqscmtSf2o", "forum": "nuAQ0okd8i", "replyto": "nuAQ0okd8i", "signatures": ["ICLR.cc/2026/Conference/Submission19101/Reviewer_SzsB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19101/Reviewer_SzsB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930688025, "cdate": 1761930688025, "tmdate": 1762931126119, "mdate": 1762931126119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a saliency-based unlearning algorithm for text-to-image diffusion models, which:\n1) considers the dynamic nature of salient neuron localization during the unlearning process;\n2) achieves promising empirical performance in unlearning compared to state-of-the-art baselines, while demonstrating strong preservation of non-target concepts and computational efficiency;\n3) introduces challenging unlearning tasks (concept combination and conditional concept unlearning) to demonstrate the precision of the proposed method.\n\nThe claimed benefits are supported primarily by empirical experiments and demonstrations, with no theoretical justification provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong motivation to build upon the salient parameter shifts observed during the fine-tuning process.\n\n- Strong demonstration of empirical improvement.\n\n- Introduces conditional concept unlearning, which serves as a strong test of unlearning effectiveness at the sentence semantic level."}, "weaknesses": {"value": "- **Missing relevant work in discussions.** This work proposes a saliency-based method. While SalUn [1] is thoroughly discussed, other relevant saliency-based methods [2][3][4] are neither discussed nor compared. In particular, [4] utilizes a loss design on CLIP alignment for saliency parameters that is similar to TRUST (the proposed method).\n\n[1] Fan et al., Salun: Empowering machine unlearning via gradientbased weight saliency in both image classification and generation. ICLR, 2024\n\n[2] Foster et al., Fast machine unlearning without retraining through selective synaptic dampening. AAAI, 2024\n\n[3] Dong et al.,Towards safe concept transfer of multimodal diffusion via causal representation editing. NeurIPS, 2024\n\n[4] Cai et al., Targeted Unlearning with Single Layer Unlearning Gradient. ICML 2025.\n\n\n- **Erasure efficiency claim.** The authors try to justify runtime efficiency mainly through number of fine-tuning steps (Line 460-467). However, this is not a straight forward metric and potentially misleading, as fine-tuning iteration can be designed in various ways. For example, TRUST (the proposed method) involves iterative salient mask, loss gradient computations and model update in the loop, vs SalUn does one-time salient mask computation outside the loop, and compute gradient and update model iteratively. I suggest author to measure computational efficiency in more straight forward units (e.g., seconds), or provide a summary table on  1-iter runtime for each method in the Table 3.\n\n\n- **Lacks ablations.** This work empirically demonstrated the improved performance with key designs lie in the following aspects:\n(1) utilizing CLIP alignment of target data as forget loss for iterative salient-mask computation;\n(2) Loss terms $\\mathcal{L}\\_\\mathrm{CIP,CSR}$ that has loss \n$\\mathcal{L}\\_\\mathrm{prev}$ from SalUn (Fan et al., 2024)  .\nIt would be beneficial if author can included ablation experiments on these key designs to reveal which design played a crucial role.\nBesides, like SalUn, the proposed method also relies on a introduced hyperparameter for thresholding saliency parameter ($\\gamma$ in Eqn. 5), the effect of this hyperparameter is also not discussed or studied.\n\n\n- **Writing clarity.** Line 328-331 relation of $\\mathcal{L}_\\mathrm{CSR}$ to \"second-order information\", \"underlying Hessian\" is hand wavy, need further explaination/derivation. Also the \"Appendix-F\" appeared in Line-331 is not properly cross-referenced.\n\n\n- **Experimental models are outdated and scalability of proposed method is questionable.** The main experiments employ Stable Diffusion v1.5 (released in Q4 2022, params < 1B) out of comparison consistency with state-of-the-art baselines. This model is built on a UNet-based denoiser architecture, which is now outdated, as the latest text-to-image models have adopted DiT or MMDiT denoiser architectures. These transformer-based designs scale more effectively and deliver higher image generation quality. As the main emphasis of this paper is on UNet-based text-to-image diffusion models, it is questionable whether the proposed method can be extended to more recent and advanced models that are more widely used in practice (e.g., Stable Diffusion 3, FLUX, SANA, Qwen-Image)."}, "questions": {"value": "- **Clarification on the term \"neurons\".** Does the author refer to the number of model parameters (i.e., the entries of a weight matrix)? \n\n\n\n\n- **Saliency shift effect characterization.** This work measures the \"saliency shift\" effect primarily by counting the number of identified \"concept neurons,\" which provides a collapsed view of high-dimensional data. Do the locations of these \"concept neurons\" also shift across the UNet?\n\n\n\n\n- **Clarification on $\\mathcal{L}_\\mathrm{CSR}$.** What is the relation of $\\mathcal{L}_\\mathrm{CSR}$ to \"second-order information\", \"underlying Hessian\"? How does it provide more effective optimization for unlearning objectives?\n\n\n\n- Refer to weaknesses for other questions/suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LWdEnUwjxL", "forum": "nuAQ0okd8i", "replyto": "nuAQ0okd8i", "signatures": ["ICLR.cc/2026/Conference/Submission19101/Reviewer_VqNF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19101/Reviewer_VqNF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210783531, "cdate": 1762210783531, "tmdate": 1762931124891, "mdate": 1762931124891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}