{"id": "4kxXPeVnzs", "number": 2192, "cdate": 1757016973364, "mdate": 1763603905342, "content": {"title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales", "abstract": "State space models (SSMs) have recently emerged as a competitive alternative to transformers in various linguistic and visual tasks. Their linear complexity and hidden-state recurrence make them particularly attractive for modeling long sequences, whereas attention becomes quadratically expensive. However, current training methods for video understanding are tailored towards transformers and fail to fully leverage the unique attributes of SSMs. For example, video models are often trained at a fixed resolution and video length to balance the quadratic scaling of attention cost against performance. Consequently, these models suffer from degraded performance when evaluated on videos with spatial and temporal resolutions unseen during training; a property we call spatio-temporal inflexibility. In the context of action recognition, this severely limits a model's ability to retain performance across both short- and long-form videos.\nTherefore, we propose a flexible training method that leverages and improves the inherent adaptability of SSMs. Our method samples videos at varying temporal and spatial resolutions during training and dynamically interpolates model weights to accommodate any spatio-temporal scale. This instills our SSM, which we call {\\sc StretchySnake}, with spatio-temporal flexibility and enables it to seamlessly handle videos ranging from short, fine-grained clips to long, complex activities. \nWe introduce and compare five different variants of flexible training, and identify the most effective strategy for video SSMs. On $6$ action video benchmarks, {\\sc StretchySnake} outperforms vanilla VideoMamba by up to 28\\%, while simultaneously delivering 3x speedups and a 90\\% reduction in GFLOPs in low-resolution settings. On short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks, StretchySnake outperforms transformer and SSM baselines alike, with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore, our method provides a simple drop-in training recipe that makes video SSMs more robust, resolution-agnostic, and efficient across diverse action recognition scenarios.", "tldr": "We propose a novel training method specifically for video state space models that enables them to operate on any length of video or resolution at test-time with little degradation in performance.", "keywords": ["State Space Models", "Action Recognition"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48dd5144a7baff6b4ebb70992d4711748b2668b7.pdf", "supplementary_material": "/attachment/a3cd9d454ed4314eaeafd17aa08fafb770dde6cf.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes StretchySnake for solving the spatio-temporal inflexibility, enabling seamlessly handle videos ranging from short, fine-grained clips to long, complex activities. On 6 action video benchmarks, StretchySnake outperforms vanilla VideoMamba by up to 28%, while simultaneously delivering 3$\\times$ speedups and a 90% reduction in GFLOPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this work is clear and easy to understand, especially spatio-temporal inflexibility.\n2. This work has a detailed theoretical analysis.\n3. The workload is full and the experiments are comprehensive."}, "weaknesses": {"value": "1. From most of works for Mamba improvement, they always focus on global modeling of long sequence and computational complexity. This work seems also focus on this. Therefore, I recommend authors highlight their own design and note the corresponding spatio-temporal inflexibility, enhancing their novelty.\n2. The writing of the formula is not standardized. For example, conv, concat, and temp in equation 4,5, and 6 are non-variables. It should be written with normal fonts instead of Italic.\n3. I hope to see more implementation details, such as how the dataset is processed. Because the SSv2 dataset is direction-sensitive, using a horizontal flip is not appropriate.\n4. Given the different capacities of these datasets, are the averages in Table 2 rigorous? Of course, this is just a friendly discussion and does not affect my judgment of this work.\n5. The size of Table A3 is too large and looks a little strange.\n6. From several figures of t-SNE, the advantages of the proposed method are not well highlighted."}, "questions": {"value": "Please refer to the weaknesses. Overall a good paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UTQZxr9Uye", "forum": "4kxXPeVnzs", "replyto": "4kxXPeVnzs", "signatures": ["ICLR.cc/2026/Conference/Submission2192/Reviewer_P5R8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2192/Reviewer_P5R8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461778720, "cdate": 1761461778720, "tmdate": 1762916129504, "mdate": 1762916129504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal Comments"}, "comment": {"value": "To address some common concerns across reviewers, we provide clarification on 2 key main contributions of our work. We address each reviewer's specific questions with additional details in our separate responses below.\n\n\n1. Novelty\n    * Our first point of novelty is instilling st-flexibility **during training**, specifically for video SSMs. We do not claim embedding interpolation itself as our own work, but rather provide extensive experiments and analyses to prove that these differentiable interpolations can be used during training to improve action recognition in video SSMs. We believe the breadth of our experiments is a contribution to the research community - our evaluations across 6 different action recognition datasets (Section 4), 3 evaluation protocols (video retrieval, linear probing, full-finetuning, Section 4.3), types of transformer models (Section A.2), additional video understanding tasks (Section A.4), and ablations (Section A.6) demonstrates that st-flexible training works for SSMs, and more importantly its significant benefits are overlooked in current video SSM practices. We provide dataset details in Table R1 to highlight StretchySnake's ability to adapt to various action recognition settings.\n\n    * We are the first to show that st-flexible training provides substantially larger gains for video SSMs than for video transformers, a distinction not yet examined in prior work. In fact, this point may have been previously obfuscated by the fact that **naively applying st-flexible methods to video transformer models does not yield large performance gains** (see Section A.2 and further discussion in our rebuttal below). The glaring benefit for video SSMs is made especially clear in Table 1 of the paper, where StretchySnake outperforms vanilla VideoMamba on all spatio-temporal scales (by up to 28%), including VideoMamba's own default configuration (Section 4.2). We include further evidence of this effect in Tables R2-R5 in our rebuttal. \n\n2. Additional Comparisons to Image Methods\n     * Reviewer mAqy pointed out that there are some existing works that either (a) apply embedding interpolations during inference, or (b) perform image flexible training for image models (see Section 2.3, main paper). We highlight the contribution of our work by **exhibiting the failures of applying these flexible image training methods to video understanding tasks in Tables R6-R7 of our rebuttal below**. \n\n    \n\n\n**Table R1.** Dataset details for each dataset used in the paper. StretchySnake can adapt to short-form, long-form, and fine-grained action recognition scenarios as a singular model.\n\n| **Dataset** | **Video Lengths** | **Label / Action Type** | **Dataset Type** | **Fine-grained?** |\n|-------------|-------------------|-------------------------|------------------|-------------------|\n| **UCF-101** | ~7 seconds | 101 sports / daily-life actions | Short-action, coarse-grained | No |\n| **HMDB-51** | ~3-6 seconds | 51 human actions from movies / YouTube | Short-action, coarse-grained | No |\n| **Breakfast** | ~120–180 seconds | 48 cooking actions in long activity sequences | Long-action, procedural activities | No |\n| **COIN** | ~150–300 seconds | 180 instructional task steps | Long-action, instructional activities | No |\n| **SSv2 (Something-Something V2)** | ~4 seconds | 174 fine-grained object interactions | Fine-grained, temporal reasoning | Yes |\n| **Diving-48** | ~3–6 seconds | 48 subtle dive types requiring fine motion cues | Fine-grained, high-speed sports | Yes |"}}, "id": "Mobchssdh9", "forum": "4kxXPeVnzs", "replyto": "4kxXPeVnzs", "signatures": ["ICLR.cc/2026/Conference/Submission2192/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2192/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2192/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763603472704, "cdate": 1763603472704, "tmdate": 1763604428195, "mdate": 1763604428195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STRETCHYSNAKE, a video SSM model with spatio-temporal flexibility (aka st-flexibility) that can be generalized to various spatial and temporal resolutions as well as different patch sizes with improved video understanding performance. \nThe core contribution is the realization of st-flexibility, which is achieved by interpolating the weights of spatial/temporal positional embeddings and convolution weights during training and inference. \nThe authors conduct many experiments across different inference configurations and show that st-flexibility is possible and effective in video SSMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is well written and easy to follow.\n2. The figures are clear.\n3. The authors conduct sufficient experiments to validate the effectiveness of the proposed st-flexibility in video SSMs.\n4. Improved performance compared to its counterparts, i.e., the VideoMamba."}, "weaknesses": {"value": "My major concern is the **novelty issue**, because there is a problematic statements regarding existing video models in the core motivation:\n\nL86-L88: *(transformers) reliance on learning explicit token-to-token relationships usually constrains them to fixed input sizes, preventing generalization across diverse spatio-temporal scales.*\n\n**Using 2-D bi-cubic interpolation or 1-D linear interpolation on spatial/temporal positional embeddings and convolution weights (L319-L322) is not first proposed by the authors but a widely adopted technique in video transformers.** Actually, interpolating spatial/temporal and convolutional weights at finetuning or inference stages for various spatio-temporal scales has been a standard practice in the past few years. There are several well-known implementations:\n\n1. TimeSformer: see `forward_features` (L249) at https://github.com/facebookresearch/TimeSformer/blob/main/timesformer/models/vit.py. It exactly uses spatial/temporal interpolation (same as the paper proposed) during inference to adapt for various scales.\n\n2. The timm repo: see `_load_weights` (L1091) at https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py. It not only interpolates the weights of the positional embeddings but also the convolution weights, i.e., the `resample_patch_embed` and `resample_abs_pos_embed` functions.\n\nBesides, **the proposed st-flexibility does not really resolve the generability issue because the model is still trapped in the pre-set training scales**, i.e., $R^t=[8,16,32,64]$ and $R^s=[96,128,224,384]$. No experiments are provided to show the performance w/ scales beyond training. For instance, the performance if $R^t=110$ and $R^s=768$.\n\nI believe the above issue is not mean because **frontier video models already reach real generability through Any Resolution techniques, which is totally missing in this paper.** Please see the below papers:\n\n1. Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution: 1D-Rope for vision understanding.\n\n2. LLaVA-Video: Video Instruction Tuning With Synthetic Data: 1D-Rope for any-scale video understanding.\n\n3. Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution: 2D-Rope for any-scale video understanding.\n\n4. Qwen2.5-VL Technical Report: 3D-Rope for any-scale video understanding.\n\nBesides, **some important baselines that also pre-trained on Kinetics-400 are missing.** For instance, VideoMAE and VideoMAE-V2. Please conduct a rigorous literature review and include them in Table2."}, "questions": {"value": "1. Can you show some results when testing at an unseen scale beyond the pre-set ones, e.g., $R^t=110$ and $R^s=768$?\n2. Can you discuss the differences between your method and the pre-developed interpolation ones, e.g., in TimeSformer and timm repos?\n3. Please include the latest any-resolution works, e.g., NaViT, LLaVA-Video, Qwen2-VL and Qwen2.5-VL and discuss the merits of your approach over theirs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lpvpNJl0AW", "forum": "4kxXPeVnzs", "replyto": "4kxXPeVnzs", "signatures": ["ICLR.cc/2026/Conference/Submission2192/Reviewer_mAqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2192/Reviewer_mAqy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560799000, "cdate": 1761560799000, "tmdate": 1762916129194, "mdate": 1762916129194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "On the recent progresses of SSMs image classification, video understanding and 3D vision, this paper aims to adapt the recent SSM and training to fine-grained action recognition. It implements and incrementally improves the training on video learning and representation on fine-grained action recognition benchmarks. The purposes of the experiments are to identify good type of st-flexibility of SSM for video representation on fine-grained action videos, demonstrate the improvement over vanilla baseline, and compare to SOTA on limited training dataset. The reported progresses have shown a certain level of extension of existing SSMs on additional tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Investigating to apply recent progresses on SSM to video representation and fine-grained action recognition task."}, "weaknesses": {"value": "The novelty of the paper is still weak, and the evaluations are not completed and concrete enough to show the significance of the progresses. First, the technical descriptions present the existing approaches and implementation details. Not clear what are novel method, model architecture, or learning function and algorithms beyond existing approaches. This paper presents a few incremental progresses and extension of existing SSMs. May need to focus on deeper study and big jump to show significant progresses. Second, the purposes of the experiments are not clear and strong. To show the optimal type of st-flexibility for SSMs in a concrete background, experiments on a wide range of vision tasks would be convincing. In the paper, only experiment for video retrieval on 4 action recognition datasets, lack formal benchmarking on related research topics. On second experiment, only compared with one baseline model. On the third part, the protocol of benchmarking is unclear. If it wants to focus on performance on OOD datasets of fine-grained action recognition with limited training set, it may need to follow a formal protocol and comparison to SOTA on leaderboard benchmarks."}, "questions": {"value": "What are the novel model modules or learning approaches beyond existing SSM technology proposed in this paper? What are the formal protocol, training set and evaluation sets used in the previous benchmarking in Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FSgCJEy4Nn", "forum": "4kxXPeVnzs", "replyto": "4kxXPeVnzs", "signatures": ["ICLR.cc/2026/Conference/Submission2192/Reviewer_35wV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2192/Reviewer_35wV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844587345, "cdate": 1761844587345, "tmdate": 1762916126271, "mdate": 1762916126271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}