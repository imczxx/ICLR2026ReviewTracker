{"id": "B0C0bt2R7l", "number": 22293, "cdate": 1758329147869, "mdate": 1759896874408, "content": {"title": "HAP-E: HESSIAN-AWARE STRUCTURED PRUNING OF LLMS FOR EFFICIENT INFERENCE", "abstract": "Large language models (LLMs) deliver strong performance across diverse tasks, yet their heavy compute and memory demands make deployment on real-time edge devices challenging. Structured pruning has become the standard approach to reduce these costs, yet accurately estimating which blocks can be removed remains challenging at scale. Second-order methods such as Optimal Brain Surgeon (OBS) are computationally intractable at LLM scale. Existing approaches rely on static budgets that ignore cross-layer dependencies, and common proxies like FLOPs misestimate real hardware latency. We introduce HAP-E, a scalable, Hessian-aware pruning framework for post-training compression of LLMs. HAP-E adaptively reallocates budgets across layers using global screening and selective second-order analysis on a candidate set guided by cross-layer sensitivity estimation. It further performs OBS-equivalent batch pruning that certifies and removes multiple blocks at once while exactly matching the greedy OBS sequence, thereby reducing weight updates and numerical drift. A lightweight latency predictor ensures that the compressed model satisfies inference-time constraints. Experiments on LLaMA and OPT models show that HAP-E improves accuracy by up to 3% over state-of-the-art structured pruning methods at comparable pruning ratios.", "tldr": "We propose a scalable, Hessian-aware pruning framework for LLMs that accounts for cross-layer interactions by adaptively selecting candidates, certifying OBS-equivalent batches for pruning, and integrating latency prediction for constrained inference", "keywords": ["Large language models (LLMs)", "Structured pruning", "Hessian-aware pruning", "Optimal Brain Surgeon (OBS)", "Greedy-consistent batch pruning", "Latency-aware compression", "Hardware-constrained inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b49b49ba997c64d5a325d7ab1f2689415522a98.pdf", "supplementary_material": "/attachment/684f194c1b982180c0ab63c8bde4a377d4918602.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces HAP-E, a scalable Hessian-aware structured pruning framework that makes Optimal Brain Surgeon (OBS) style pruning feasible for LLMs by using adaptive cross-layer budget reallocation, selective second-order analysis, and greedy-consistent batch pruning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n1. The application of the Schur complement in structured pruning is novel and offers a mathematically grounded perspective that may inspire future directions in model compression research."}, "weaknesses": {"value": "Originality\n\n1. Cross-layer global pruning has been explored in prior works [1], and several methods already propose dynamic or adaptive layer-wise budget allocation [2].\n2. The idea of batch pruning multiple blocks simultaneously is not new, and the claim of achieving “greedy-consistent batch pruning” may overstate the level of innovation relative to prior work [3].\n\nQuality\n\n3. Baseline settings appear inconsistent across experiments. For example, the baselines in Table 1(a) and 1(b) are different.\n4. The ablation in Table 3 primarily compares against the authors’ own baselines instead of prior pruning methods, which weakens the strength of the computational validation.\n5. The paper does not include comparisons with Wanda-SP (as implemented in FLAP), which also simplifies Hessian inversion. Given the marginal accuracy gains reported, this comparison is essential.\n6. The LLMs used for experiments are relatively old, limiting the practical relevance of the results for current LLM architectures.\n\nClarity\n\n7. The motivation for latency prediction is unclear. Since FLOPs and latency are often strongly correlated, this component feels tangential and its contribution to the overall framework is not well justified.\n\n\nSignificance \n\n8. The method introduces significant implementation overhead, which may hinder practical adoption.\n9. The reported performance gains (~3%) are relatively marginal considering the added system complexity of the proposed method.\n10. Despite selective computation, Hessian inversion remains computationally intensive and can be numerically unstable in deep networks, which raises concerns about the robustness of the approach.\n11. In contrast to Wanda, which prunes in a layer-wise manner from early to late layers, global pruning may introduce accumulated cross-layer errors. The lack of direct comparison on this front weakens the overall argument.\n12. The claim of “exactly matching greedy OBS” at batch scale is questionable, as the Hessian is only locally valid and neural networks are inherently nonlinear, making such equivalence difficult to guarantee in practice.\n\n\n[1] LLM-Pruner: On the Structural Pruningof Large Language Models  \n[2] ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models  \n[3] A Simple and Effective Pruning Approach for Large Language Models"}, "questions": {"value": "1. How does the pruning ratio evolve across layers during iterations? Based on prior experience, it is often necessary to skip pruning early layers for stability. Did the authors follow such a practice?\n2. Could the authors clarify the empirical contribution of the latency predictor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Aora3NKJlh", "forum": "B0C0bt2R7l", "replyto": "B0C0bt2R7l", "signatures": ["ICLR.cc/2026/Conference/Submission22293/Reviewer_mpii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22293/Reviewer_mpii"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760941436849, "cdate": 1760941436849, "tmdate": 1762942154809, "mdate": 1762942154809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HAP-E, a new framework designed to compress LLMs for efficient post-training inference. The primary goal is to reduce the high computational and memory costs of LLMs, making them deployable on resource-constrained devices while maintaining high accuracy. Experiments conducted on LLaMA and OPT model families show that  HAP-E outperforms existing state-of-the-art structured pruning methods like SlimGPT. Furthermore, experiments on hardware devices confirm the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel method**: The combination of global screening and selective second-order analysis is a novel approach that makes the OBS method scalable for LLMs. The method is also supported by theoretical analysis.\n2. **Strong empirical results**: The paper demonstrates consistent improvements over several state-of-the-art methods across multiple model families and scales.\n3. **Hardware-aware pruning**: The integration of a learned latency predictor is a major practical strength. This allows the framework to directly optimize for real-world performance on specific hardware.\n4. **Clarity**: The paper is well-written and logically structured."}, "weaknesses": {"value": "1. **Limited scope of baselines**: A comparison against stronger baselines like ModeGPT[1] and SVD-LLM v2[3] would have made the results even more compelling.\n2. **Focus on Post-Training only**: The paper explicitly limits its evaluation to training-free pruning methods. Although it is faster to prune the model, it is still important to check the compatibility with recovery fine-tuning or knowledge distillation. The work could be strengthened by discussing how HAP-E might be extended to these scenarios.\n3. **Hyper-parameter Sensitivity**: The method introduces several hyper-parameters, such as the candidate oversampling ratio (M/K), the prune fraction per iteration (K), and the sensitivity propagation coefficient (β).\n\n\n\n[1] Lin C H, Gao S, Smith J S, et al. MoDeGPT: Modular Decomposition for Large Language Model Compression[C]//The Thirteenth International Conference on Learning Representations, 2025.\n[2] Wang X, Alam S, Wan Z, et al. SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression[C]//Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2025: 4287-4296."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gPz9HfUNN9", "forum": "B0C0bt2R7l", "replyto": "B0C0bt2R7l", "signatures": ["ICLR.cc/2026/Conference/Submission22293/Reviewer_EKxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22293/Reviewer_EKxf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316090064, "cdate": 1761316090064, "tmdate": 1762942154388, "mdate": 1762942154388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HAP-E, a post-training structured pruning framework re-allocate pruning budgets across layers using a recursive Hessian-trace, executes greedy-consistent batch pruning that is claimed to be exactly equivalent to the first prefix of greedy OBS steps via Schur complements and incremental Cholesky, and guides pruning with a two-stage learned latency model trained from measured runtimes so the final model meets a target latency. \n\nExperiments on LLaMA and OPT families report modest accuracy gains over recent structured-pruning baselines on multiple-choice benchmarks without fine-tuning; a small-model CPU deployment study (ExecuTorch) shows better accuracy–latency trade-offs than LLM-Pruner."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tTwo stage latency model is simple and effective. \n2.\tShows real runtime/memory wins for pruning (Table 3 indicate that cross-layer adaptability, greedy batching, and predictor are helpful).\n3.\tRecursive cross-layer sensitivity, greedy-consistent certification, and hardware-aware latency shaping are good engineering contributions."}, "weaknesses": {"value": "1.\tLimited scope, no generation/long-context tasks. In fact, only multiple-choice tasks are reported. No perplexity, summarization, long-context, math/code robustness. \n2.\tThe empirical margins over strong structured-pruning baselines are small, usually 1-2 points and not uniform across tasks. Hard to assess how much of that is statistical noise. \n3.\tThe “greedy-consist” ideas is a nice rewording of Schur-complement conditioning. I find the engineering strong but conceptual novelty to be incremental. \n4.\tEfficient inference claim is not fully demonstrated for large models (no GPU inference results and edge study only compares to LLM-Pruner)."}, "questions": {"value": "I’m concerned about scaling. Can you measured GPU latencies for 7B/13B/30B models at the same reported pruning ratios, and compare with SlimGPT/SoBP/LLM-Pruner\n\nHow robust is HAP-E to hyperparameters like β?\n\nDoes the latency estimator generalize to new hardware or models without retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ndmCtvC6vq", "forum": "B0C0bt2R7l", "replyto": "B0C0bt2R7l", "signatures": ["ICLR.cc/2026/Conference/Submission22293/Reviewer_s6sk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22293/Reviewer_s6sk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801312646, "cdate": 1761801312646, "tmdate": 1762942154014, "mdate": 1762942154014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HAP-E, a Hessian-aware structured pruning framework for LLMs that makes Optimal Brain Surgeon (OBS)–style pruning scalable and hardware-aware. It introduces adaptive cross-layer budget reallocation, greedy-consistent batch pruning, and a learned latency predictor to meet real device constraints. Experiments on LLaMA and OPT models show up to 3% accuracy improvement over state-of-the-art methods at similar pruning ratios and precise latency control on edge devices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly defines the limitations of traditional OBS-based pruning, including high computational cost, the need for heavily sequential updates, and the lack of cross-layer awareness, and proposes well-designed techniques to effectively overcome these issues.\n\n2. The use of Hessian-based approximation to estimate layer sensitivity is a novel idea that enhances pruning adaptivity across layers.\n\n3. The inclusion of real edge deployment experiments with measured CPU latency strengthens the practical relevance and demonstrates the framework’s hardware-awareness beyond simulation."}, "weaknesses": {"value": "1. The experimental evaluation focuses primarily on older LLMs such as LLaMA and OPT; given the rapid advancement of model architectures, it would be important to validate the proposed method on more recent models like LLaMA-2/3 and Qwen-2/3 to demonstrate broader applicability.\n\n2. The paper mainly compares against OBS-based local pruning approaches, while several recent works have explored global pruning that explicitly models cross-layer sensitivity[1,2,3]. Including discussion or comparison with these methods would strengthen the positioning of the work.\n\n[1] Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity, ICML 2024 \n\n[2] SparseLLM: Towards Global Pruning for Pre-trained Language Models, NeurIPS 2024 \n\n[3] DarwinLM: Evolutionary Structured Pruning of Large Language Models"}, "questions": {"value": "1. The paper highlights that HAP-E achieves much higher efficiency than traditional OBS-based pruning. Could the authors provide quantitative comparisons of pruning cost (runtime and memory) against both OBS-based and other pruning baselines to clearly demonstrate this efficiency gain?\n\n2. The proposed latency predictor is an important component for hardware-aware pruning. How well does this predictor generalize to unseen hardware platforms, and what is the data collection overhead required to train it for a new device?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qLV27nvZvH", "forum": "B0C0bt2R7l", "replyto": "B0C0bt2R7l", "signatures": ["ICLR.cc/2026/Conference/Submission22293/Reviewer_ZBP1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22293/Reviewer_ZBP1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189563191, "cdate": 1762189563191, "tmdate": 1762942153609, "mdate": 1762942153609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}