{"id": "CTmu1mqDuz", "number": 19574, "cdate": 1758297372752, "mdate": 1759897032034, "content": {"title": "Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant promise for enhancing the reasoning capabilities of large language models (LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage signal across all tokens in a sequence. This coarse-grained approach overlooks the pivotal role of uncertain, high-stakes decisions during reasoning, leading to inefficient exploration and the well-documented problem of entropy collapse. To address this, we introduce $\\textbf{U}$n$\\textbf{C}$ertainty-aware $\\textbf{A}$dvantage $\\textbf{S}$haping ($\\textbf{UCAS}$), a model-free method that refines credit assignment by leveraging the model's internal uncertainty signals. UCAS operates in two stages: it first modulates the response-level advantage using the model's overall self-confidence, and then applies a token-level penalty based on raw logit certainty. This dual mechanism encourages exploration of high-uncertainty paths that yield correct answers while penalizing overconfident yet erroneous reasoning, effectively balancing the exploration-exploitation trade-off. Extensive experiments on five mathematical reasoning benchmarks show that UCAS significantly outperforms strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our analysis confirms that UCAS not only achieves higher rewards but also promotes greater reasoning diversity and successfully mitigates entropy collapse.", "tldr": "", "keywords": ["Mathematical Reasoning", "LLM", "RLVR", "Advantage Estimation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62a6327b4b2df897a43541638aa11e6426a08781.pdf", "supplementary_material": "/attachment/d481123a3adc8e0e42abf512d2fb13bc233f4fee.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes UCAS (UnCertainty-Aware Advantage Shaping), a method to improve Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs). The key idea is to shape the advantage signal using model uncertainty: (1) modulating the sequence-level advantage by the model’s self-confidence, and (2) penalizing token-level certainty derived from logits. The goal is to prevent entropy collapse and encourage exploration. The authors report improvements over GRPO and DAPO on several mathematical reasoning benchmarks (AIME24, MATH-500, AMC, Minerva, OlympiadBench) using Qwen2.5-Math-1.5B and 7B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles a real problem in RLVR: entropy collapse and insufficient exploration.\n\n2. The proposed uncertainty-aware shaping is conceptually simple, easy to implement, and compatible with existing RLVR frameworks.\n\n3. The presentation is clear, and the motivation is easy to follow."}, "weaknesses": {"value": "###  Limited Novelty and Theoretical Justification\n\nThe core contributions lack sufficient novelty. The paper essentially applies existing uncertainty quantification techniques (self-confidence from prior work, raw logit values) to weight advantages differently. The two-stage mechanism is straightforward:\n\n- Response-level: exponential weighting based on normalized confidence (Eq. 7)\n- Token-level: min-max normalized logit penalty (Eq. 8)\n\nNeither component introduces fundamentally new concepts. The exponential weighting scheme is ad-hoc without theoretical grounding for why this specific functional form is optimal. Why exponential rather than linear, polynomial, or other monotonic functions? The paper provides no principled justification beyond empirical performance.\n\n### Weak conceptual contribution.\n\nThe paper presents UCAS as a new framework, but it does not introduce any fundamentally new algorithmic principle beyond applying confidence-based scaling to the GRPO advantage. Similar uncertainty-aware or entropy-regularized approaches already exist (e.g., semantic entropy regularization, variance-aware advantage estimation, or KTAE). The novelty over prior work is therefore marginal.\n\n# Writing and Presentation Issues\n\nThe paper oversells the contribution. Terms like \"unlocking exploration\" and \"deeper reasoning\" are not substantiated by the actual improvements shown\n\nThe \"entropy collapse\" narrative is emphasized throughout, but Figure 3 shows UCAS entropy actually drops initially before recovering—this deserves more analysis\n\nSome claims lack support: \"encourages exploration of high-uncertainty paths that yield correct answers\" (lines 25-26)—but the method equally amplifies penalties for wrong answers with high uncertainty"}, "questions": {"value": "How do results change with different α and β values?\n\nWhy does entropy initially drop before recovering (Figure 3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ys552711aG", "forum": "CTmu1mqDuz", "replyto": "CTmu1mqDuz", "signatures": ["ICLR.cc/2026/Conference/Submission19574/Reviewer_cqWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19574/Reviewer_cqWa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760491617143, "cdate": 1760491617143, "tmdate": 1762931451600, "mdate": 1762931451600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response about the sensitivity analysis of hyperparameters"}, "comment": {"value": "Thank reviewer `EgW8`,  `L6No` and `cqWa` for their constructive feedback regarding the sensitivity of our method to the hyperparameters $\\alpha$ (response-level modulation) and $\\beta$ (token-level penalty). We greatly appreciate your attention to this matter, and below, we address this issue from three perspectives to ensure the reliability of our findings.  We analyzed the impact of each hyperparameter by varying one while fixing the other at its optimal value.  \n\n**Table G1: Performance comparison varying Token-Level Penalty ($\\beta$)**\n\n| Response-Level $\\alpha$ | Token-Level $\\beta$ | Math-500 |\n| :---------------------- | ------------------- | :------- |\n| 0.2                     | 0.005               | 79.6     |\n| 0.2                     | 0.01                | 80.4     |\n| 0.2                     | 0.05                | 78.8     |\n| 0.2                     | 0.1                 | 78.4     |\n\n**Table G2: Performance comparison varying Response-Level Modulation ($\\alpha$)**\n\n| Response-Level $\\alpha$ | Token-Level $\\beta$ | Math-500 |\n| :---------------------- | ------------------- | :------- |\n| 0.1                     | 0.01                | 80.0     |\n| 0.2                     | 0.01                | 80.4     |\n| 0.4                     | 0.01                | 78.8     |\n\n1. **Impact of Token-Level Penalty ($\\beta$)** : Table G1 illustrates the performance changes when varying the token-level penalty coefficient $\\beta$, with the response-level modulation fixed at $\\alpha=0.2$. A moderate $\\beta$ (0.01) effectively mitigates entropy collapse, improving reasoning stability. However, as $\\beta$ increases beyond 0.05, performance begins to degrade. This suggests that an excessive penalty may over-regularize the model, stifling the necessary exploration during the generation process.\n\n2. **Impact of Response-Level Modulation ($\\alpha$)** : Table G2 demonstrates the sensitivity to the response-level modulation coefficient $\\alpha$, with the token-level penalty fixed at $\\beta=0.01$. The results indicate that $\\alpha$ is relatively stable within the range of $[0.1, 0.2]$. Increasing $\\alpha$ to 0.4 leads to a performance drop, implying that if the uncertainty modulation is too strong, it may dominate the reward signal, interfering with the advantage estimation's accuracy.\n\n3. **Hyperparameter Selection Strategy**: In response to Reviewer `EgW8's` question regarding how these values are chosen:\n\n   Our parameter selection is grounded in our theoretical framework of **hierarchical dominance adjustment**. Functionally, $\\alpha$ and $\\beta$ serve to scale the value ranges of the response-level uncertainty and token-level logits, respectively. \n\n   Therefore, rather than relying on random search, we select these hyperparameters based on the **magnitude of the model's output signals**. The goal is to align the scales of the uncertainty penalties with the model's advantage estimates, ensuring an appropriate **cooperative adjustment effect** where the uncertainty signal guides exploration without overwhelming the primary learning objective. As demonstrated by the sensitivity analysis above, this theoretically guided selection strategy aligns well with the empirically optimal region."}}, "id": "kH1swhVOre", "forum": "CTmu1mqDuz", "replyto": "CTmu1mqDuz", "signatures": ["ICLR.cc/2026/Conference/Submission19574/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19574/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19574/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763747830027, "cdate": 1763747830027, "tmdate": 1763747830027, "mdate": 1763747830027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of prevailing algorithms such as GRPO in RLVR, which broadcast a uniform advantage signal across all tokens in a sequence. This coarse-grained approach overlooks the pivotal role of uncertain, high-stakes decisions during reasoning, leading to inefficient exploration and the well-documented problem of entropy collapse. To tackle this, the authors introduce UnCertainty-aware Advantage Shaping (UCAS), a model-free method that refines credit assignment by leveraging the model’s internal uncertainty signals. UCAS operates in two stages: it first modulates the response-level advantage using the model’s overall self-confidence, and then applies a token-level penalty based on raw logit certainty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The topic of this paper is important, as it addresses both the exploration-exploitation trade-off in LLM reasoning and the problem that GRPO broadcast a uniform advantage signal across all tokens in a sequence. \n2.  The authors conduct comparisons with up-to-date baselines and report various metrics, including pass@k.\n3.  The proposed method is intuitive and makes sense to me."}, "weaknesses": {"value": "1.  The proposed method is mostly based on heuristics; more theoretical analysis should be included.\n2.  The idea of response-level and token-level advantage shaping using uncertainty/confidence has already been proposed in works such as Seed-GRPO and Entropy Advantage shaping. I encourage the authors to elaborate more on the unique contributions of their work.\n3.  The proposed method may work for GRPO, where the advantage is uniform for all tokens in a trajectory. However, it is unclear how it would apply to PPO."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b90GGMOxzw", "forum": "CTmu1mqDuz", "replyto": "CTmu1mqDuz", "signatures": ["ICLR.cc/2026/Conference/Submission19574/Reviewer_6jZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19574/Reviewer_6jZE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761182317115, "cdate": 1761182317115, "tmdate": 1762931451250, "mdate": 1762931451250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UCAS, an uncertainty aware advantage shaping method for RLVR training of reasoning LLMs. The idea is to reshape the learning signal in two stages. First, at the response level, the method scales the group normalized advantage by a self confidence score computed as average KL to uniform. Second, at the token level, it subtracts a certainty penalty based on the raw logits for the chosen tokens. Experiments on five math benchmarks and two model sizes show consistent pass@1 gains over GRPO, DAPO, and other recent RLVR variants, with longer reasoning chains and recovery of generation entropy as training proceeds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Uncertainty aware shaping that mixes a response level self confidence weight with a token level logit penalty is simple, well motivated, and easy to implement inside existing GRPO or DAPO code. It does not add a new model or a verifier, unlike many PRM based approaches. The method is explained clearly with a compact formula and an algorithm box.\n2. On AIME24, MATH 500, AMC, Minerva, and OlympiadBench, UCAS wins on both 1.5B and 7B Qwen math models, with gains over DAPO, KTAE, and Oat Zero. The ablation in Table 2 shows both response level and token level parts contribute, and their combination is best.\n3. The work tackles the widely reported entropy collapse in RLVR and shows recovery of generation entropy during training together with longer responses."}, "weaknesses": {"value": "1. All experiments are in math with verifiable final answers. It is unclear if the same shaping works for code unit tests or symbolic tasks, and especially for non binary or dense reward settings.\n2. The response level confidence is KL to uniform and the token level proxy is raw logits. Both are known to be imperfect confidence measures and can be miscalibrated.\n3. The training uses 16 rollouts per prompt and drops KL and entropy regularizers in some baselines. The paper should include a sensitivity study for alpha and beta, and report results when baselines are tuned with their recommended regularization settings. Otherwise, part of the gain may come from different regularization or longer responses.\n4. KTAE [1] also produces token level advantages without extra models. Recent entropy induced advantage methods also reshape the advantage. The novelty margin would be clearer with side by side plots of entropy and pass@k against those methods under the same compute. [1] KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xcPe63zMrM", "forum": "CTmu1mqDuz", "replyto": "CTmu1mqDuz", "signatures": ["ICLR.cc/2026/Conference/Submission19574/Reviewer_L6No"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19574/Reviewer_L6No"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970956755, "cdate": 1761970956755, "tmdate": 1762931450693, "mdate": 1762931450693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UCAS, a method to improve RLVR for reasoning LLMs by addressing the coarse credit assignment problem in GRPO. Instead of broadcasting a uniform advantage across all tokens, UCAS reshapes the learning signal using the model’s intrinsic uncertainty at two levels: (1) a response-level modulation that amplifies rewards for correct but uncertain trajectories and penalizes overconfident wrong ones, and (2) a token-level certainty penalty that discourages local overconfidence to prevent entropy collapse. Experiments on five mathematical reasoning benchmarks show consistent and significant improvements across both 1.5B and 7B Qwen-Math models, leading to deeper reasoning chains, higher rewards, and better exploration diversity."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to read and well structured, making the method and motivation intuitive to follow. Improving exploration in RL for reasoning LLMs is a very active and timely topic, and this method provides a valuable and well-motivated contribution in that direction.\n\n2. The method is lightweight, model-free, and compatible with existing RLVR pipelines, which makes it practically useful for scaling to larger models.\n\n3. The experiments are comprehensive, comparing against a wide range of strong RLVR and reasoning baselines with clear and consistent performance gains."}, "weaknesses": {"value": "1. The theoretical foundation of UCAS is mostly intuitive and the paper doesn’t formally analyze why the two-stage shaping leads to more stable optimization or guarantees improved exploration.\n\n2. From Table 2, the token-level certainty component seems to contribute little or even slightly hurt performance in some cases, suggesting its effect is weaker or less stable than the response-level shaping.\n\n3. The paper lacks ablation or sensitivity analysis for the two key hyperparameters \\alpha and \\beta, which directly control the strength of response-level modulation and token-level penalty. It is unclear how stable UCAS is to different settings of these values."}, "questions": {"value": "1. How do you choose the hypeparameters and how they affect the performance?\n\n2. Since UCAS is conceptually orthogonal to the underlying policy optimization algorithm, have the authors tested it with other GRPO-family methods (e.g., DAPO or GSPO)? Demonstrating consistent gains across multiple RLVR optimizers would strengthen the claim that UCAS provides generalizable advantage shaping rather than optimizer-specific benefits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iOVgkWb7XM", "forum": "CTmu1mqDuz", "replyto": "CTmu1mqDuz", "signatures": ["ICLR.cc/2026/Conference/Submission19574/Reviewer_EgW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19574/Reviewer_EgW8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991580683, "cdate": 1761991580683, "tmdate": 1762931450173, "mdate": 1762931450173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}