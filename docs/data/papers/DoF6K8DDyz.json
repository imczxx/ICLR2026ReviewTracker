{"id": "DoF6K8DDyz", "number": 11023, "cdate": 1758187133608, "mdate": 1759897613847, "content": {"title": "VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning", "abstract": "Offline reinforcement learning (RL) learns effective policies from pre-collected datasets, offering a practical solution for applications where online interactions are risky or costly. Model-based approaches are particularly advantageous for offline RL, owing to their data efficiency and generalizability. However, due to inherent model errors, model-based methods often artificially introduce conservatism guided by heuristic uncertainty estimation, which can be unreliable.  In this paper, we introduce VIPO, a novel model-based offline RL algorithm that incorporates self-supervised feedback from value estimation to enhance model training. Specifically, the model is learned by additionally minimizing the inconsistency between the value learned directly from the offline data and the one estimated from the model. We perform comprehensive evaluations from multiple perspectives to show that VIPO can learn a highly accurate model efficiently and consistently outperform existing methods. In particular, it achieves state-of-the-art performance on almost all tasks in both D4RL and NeoRL benchmarks. Overall, VIPO offers a general framework that can be readily integrated into existing model-based offline RL algorithms to systematically enhance model accuracy.", "tldr": "In this paper, we introduce VIPO, a novel model-based offline RL algorithm that incorporates self-supervised feedback from value estimation to enhance model training.", "keywords": ["Offline RL", "value function inconsistency", "model accuracy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/905c5ad8d8b9ba5812ea6871c7d97a043db4f87b.pdf", "supplementary_material": "/attachment/14ff7a63e65f1bf20d6cab4de6a587f91b9d0035.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a novel offline model-based reinforcement learning method called VIPO that incorporates value inconsistency loss in the offline model learning, where two distinct values are learned from the same offline dataset by introducing dual usage of the offline data. Authors contend that the uncertainty estimation of the conventional offline model-based methods is hard to rely on when complex offline data is given to the agent. They further suggest a two-way value learning approach: learning a value solely learned from the offline data and another value function learned with the empirical transition dynamics model, supported by theoretical analysis and empirical observation. They conduct an empirical study with model-free and model-based offline RL baselines on the D4RL and NeoRL benchmarks. VIPO achieves superior performance across benchmarks, highlighting the lower model prediction error and higher empirical performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide comprehensive explanations on connecting theoretical foundations and practical implementations.\n- The authors rigorously provide supplementary proofs for theorems on value learning and the model gradient theorem.\n- Experiments on D4RL show improved performance across model-free and model-based baselines.\n- Experiments on NeoRL demonstrate an interesting perspective of VIPO when the given data is limited."}, "weaknesses": {"value": "- The novelty of VIPO is limited. The main contribution relies on reducing uncertainty estimations to learn an accurate model by introducing an auxiliary value inconsistency loss into the original model learning. However, there is no guarantee that the learned model predicts reliable synthetic rollouts under out-of-support data in the current methodology. Besides, the value inconsistency loss largely depends on the assumption that the learned values precisely estimate ($V^\\mu_d (s)$ and $V^\\mu_m (s)$) approximate the true value(L171, L188), whereas the learned value function fails to predict accurate values when the given $(s,a)$ pair lies outside of the offline data in general [1]. Based on my conjecture, employing a surrogate objective with heuristics in Section 3.3 contributes to the empirical performance gains to some extent, while those observations benefit the task-specific problem formulation- MuJoCo locomotion tasks often exhibit tightly bounded states and actions.\n- Experimental results present conflicting views from the perspective of the authors. In Table 1, MOPO outperforms VIPO-MOPO in five out of twelve cases when comparing the effect of planners, which contrasts with the authors' argument that VIPO-MOPO outperforms MOPO across all tasks in L353. Additionally, the y-axis ticks of Figure 2 are misaligned across baselines. While it is notable that VIPO demonstrates increasing uncertainty when the drop ratio increases, the absolute difference is marginal when the y-axis is scaled with each other.\n\n[1] Levine, Sergey, et al. \"Offline reinforcement learning: Tutorial, review, and perspectives on open problems.\" arXiv 2020."}, "questions": {"value": "- Could you conduct additional experiments in more complex domains to verify that the empirical gains do not solely stem from the practical designs? Current tasks often contain narrow bounds in state or action spaces, which aligns with heuristics that numerical differences are trivial. For instance, the tabletop robotic manipulation benchmark [1] provides tasks with a wide-ranging state space (e.g., distance in Cartesian space from a hand to an object).\n- Are there alternative ways to approximate the augmented model learning loss instead of replicating the same tuple for the next time-step tuple, which can be further extended to domains outside of locomotion tasks? I believe suggesting more comprehensive directions for implementing the model gradient theorem would significantly improve the novelty of the paper.\n- What is the limitation of VIPO? Discussions on the potential drawbacks of the proposed method should be addressed.\n\n[1] Mandlekar, Ajay, et al. \"What matters in learning from offline human demonstrations for robot manipulation.\" arXiv 2021.\n\nMinor problems\n- L122: Maybe a typo? $P(s'|s,a) : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$\n- L170: What is the meaning of \"densely sampled\"? Does this sentence stand for a nearly full-coverage offline dataset?\n- Table 1: Bold faces do not denote the best scores. (MOReL is the best in *hopper-r* and *walker2d-r*, VIPO-MOPO is the best in *hopper-m-e*)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNDM8AeXxN", "forum": "DoF6K8DDyz", "replyto": "DoF6K8DDyz", "signatures": ["ICLR.cc/2026/Conference/Submission11023/Reviewer_jZLG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11023/Reviewer_jZLG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617576685, "cdate": 1761617576685, "tmdate": 1762922202095, "mdate": 1762922202095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes VIPO, a model-based offline reinforcement learning algorithm that incorporates value function inconsistency into the training loss of the dynamics model. The key idea is to update the model by minimizing the discrepancy between the value function learned from offline data and the value estimated from the learned dynamics. The authors further derive how to compute the model gradients with respect to the parameters of the proposed loss function. In the evaluation, VIPO outperforms several SOTA model-based offline RL algorithms on the D4RL MuJoCo and NeoRL benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. VIPO introduces a novel perspective on value function inconsistency, which has been largely overlooked in previous model-based offline RL works.\n\n2. The paper is clearly structured, allowing readers to follow the overall flow and reasoning easily.\n\n3. When comparing several algorithms, VIPO achieves SOTA performance."}, "weaknesses": {"value": "1. As I understand it, MOBILE quantifies uncertainty through the inconsistency of Bellman estimations under an ensemble of learned dynamics models. VIPO leverages value function inconsistency as a self-supervised learning signal that directly guides the model training. Considering that these two algorithms are based on different conceptual perspectives, it may not be appropriate to present the combined performance of VIPO and MOBILE as the representative result of VIPO in the evaluation.\n\n2. In fact, when comparing the performance between MOPO and VIPO-MOPO, there are several offline datasets (e.g., hopper-r, walker2d-m, halfcheetah-m-r, walker2d-m-r, etc.) where MOPO outperforms VIPO-MOPO. This raises uncertainty about whether the performance improvement of VIPO is truly significant or consistent across tasks.\n\n3. In Figure 2, the uncertainty scales for MOPO and VIPO are presented on separate axes, making direct comparison between the two methods difficult."}, "questions": {"value": "1. In Table 3, measuring model error only for a rollout length of 1 is somewhat limited. Since model rollouts are typically performed for horizons of 5 or more, comparisons over longer horizons would provide a more comprehensive evaluation.\n\n2. How would the performance change if a different type of model uncertainty, other than the one used in MOPO, were employed? It would be interesting to see whether VIPO remains effective under alternative uncertainty estimation schemes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CNYtgu7lHx", "forum": "DoF6K8DDyz", "replyto": "DoF6K8DDyz", "signatures": ["ICLR.cc/2026/Conference/Submission11023/Reviewer_5NcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11023/Reviewer_5NcD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889583977, "cdate": 1761889583977, "tmdate": 1762922201641, "mdate": 1762922201641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VIPO (Value Function Inconsistency Penalized Offline Reinforcement Learning), a novel model-based offline RL algorithm designed to significantly improve the accuracy and reliability of the learned dynamics model. The core motivation is that previous model-based methods rely on heuristic uncertainty estimation to enforce conservatism, which is often unreliable in practice."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The central idea is the innovative dual-usage of offline data to generate a self-supervised signal for model training. This contrasts fundamentally with previous methods that used the data only once to learn the model ensemble.\n\n2. The derivation of the Model Gradient Theorem (Theorem 3.3) is a significant technical achievement, providing the analytical expression needed to compute the gradient of the complex augmented loss."}, "weaknesses": {"value": "1. The calculation of the surrogate gradient (Eq. 11) relies on the practical assumption that the short sampling interval in continuous-control problems makes the state change over a single step numerically insignificant. This allows approximating $(s', a', r', s'')$ using the available single-step samples $(s, a, r, s')$. This is a heuristic approximation that introduces an unquantified error, and its effectiveness may degrade in environments with high-frequency dynamics or more complex state transitions.\n\n2. The benchmark results show that VIPO outperforms COMBO and RAMBO, but the critical experiments demonstrating uncertainty reliability (Figure 2) and predictive capability (Table 3) only compare VIPO to the Original Loss (OL) Model (which is MOPO/MOREL/MOBILE's model objective). Including a direct comparison of the predictive capability against models trained using other uncertainty-based model training methods would have provided a more comprehensive validation."}, "questions": {"value": "Since the main benefit is improved model accuracy over previous conservatism strategies, how does the predictive capability of the VIPO model (Table 3) compare directly against models trained using other prominent model-based conservatism strategies, such as the maximum pairwise difference or the adversarial approach? \n\n\nThe core VIPO method uses the MOBILE planner (Algorithm 2). Could the authors include an ablation study that swaps the planner used in Algorithm 2 for a simpler, less aggressive one (e.g., a standard SAC planner without the uncertainty penalty $\\beta\\mathcal{U}(s,a)$ term) to isolate how much of the performance gain is attributable solely to the $\\mathcal{L}_{vic}$-trained model versus the aggressive mobile-like policy optimization loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w2wdwmpwFQ", "forum": "DoF6K8DDyz", "replyto": "DoF6K8DDyz", "signatures": ["ICLR.cc/2026/Conference/Submission11023/Reviewer_etBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11023/Reviewer_etBC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991813994, "cdate": 1761991813994, "tmdate": 1762922201269, "mdate": 1762922201269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VIPO, a novel model-based offline reinforcement learning (RL) algorithm designed to address the inherent challenges of offline RL, such as overestimation of values and model uncertainty. By incorporating value function inconsistency into the model training process, VIPO improves the model's accuracy and its ability to generalize from limited data. The paper presents empirical results across multiple benchmarks (D4RL, NeoRL), showing that VIPO consistently outperforms previous methods, demonstrating its efficacy in learning accurate models from offline datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of value function inconsistency as a self-supervised loss to enhance model accuracy in model-based offline RL is novel. The methodology and experimental setup are clearly presented, with an appendix and code provided for additional details.\n2. The paper presents a well-defined theoretical framework for the gradient of the augmented loss function, contributing to a deeper understanding of the algorithm’s mechanics."}, "weaknesses": {"value": "1. My main concern lies in the insufficient experimental evaluation. The paper primarily evaluates the algorithm on simpler locomotion tasks, such as Walker and Hopper, which may not fully demonstrate its generalization capabilities. Test results on tasks that rely on visual input(V-D4RL), navigation tasks(Antmaze) or more complex manipulation tasks, such as Fetch environment, would significantly improve the robustness and generalization of the reported findings. \n\n2. Additionally, the paper lacks ablation studies, such as evaluating the impact of the number of ensemble models (N) or removing the value inconsistency penalty, which are essential for understanding the contribution of each component to the overall performance of the algorithm.\n\n3. While the paper highlights the improved performance of VIPO, it lacks a detailed analysis of the computational cost and training time required by VIPO compared to existing algorithms. Including this information would provide a more balanced perspective on the algorithm’s practical utility, particularly for real-world applications."}, "questions": {"value": "Please refer to the \"Weaknesses\" section, I will raise my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tnj9WT641X", "forum": "DoF6K8DDyz", "replyto": "DoF6K8DDyz", "signatures": ["ICLR.cc/2026/Conference/Submission11023/Reviewer_y7vH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11023/Reviewer_y7vH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993551253, "cdate": 1761993551253, "tmdate": 1762922200649, "mdate": 1762922200649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}