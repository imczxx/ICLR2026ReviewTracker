{"id": "Px0UrEUcFQ", "number": 17299, "cdate": 1758274403716, "mdate": 1759897183849, "content": {"title": "Polymorphic: Plug-and-Play Visual Token Compression for Scalable VLMs", "abstract": "Recent advances in vision language models (VLMs) have enabled strong reasoning and generalization capabilities, but they remain computationally expensive, primarily due to the quadratic complexity of Transformer self-attention and the large number of visual tokens produced by high-resolution inputs. To address this limitation, we propose a flexible plug-and-play framework for visual token pruning that can be seamlessly integrated into existing VLMs without requiring additional training or model modification. Our approach employs a two-stage strategy. In the first stage, representation-level token merging is performed based on spatial information density, which removes redundant visual features. In the second stage, tokens with low cross-modal relevance are adaptively pruned during language model prefilling, allowing the computation to focus on the most informative regions. This design substantially reduces the visual token budget, leading to improvements in both inference speed and memory efficiency while maintaining strong task performance. Extensive experiments on widely used benchmarks demonstrate that our method consistently achieves superior efficiency and accuracy trade-offs, highlighting its potential for practical deployment of high-resolution VLMs in real-world applications.", "tldr": "", "keywords": ["pruning", "vision language model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dbe8ab6b7206b97d4f8b261d87da386d8bd2e576.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduced a training-free method for pruning visual tokens in vision language models. The method involves two stages of pruning, in the first stage, the output tokens from the visual encoder are clustered based on similarity (ones receiving low attention scores), and in the second stage, the visual tokens are further pruned based on attention scores from informative text tokens. The experiments are done with Llava-1.5 7B, LLava-1.5 13B and Qwen2VL models on a set of popular vision benchmarks like GQA and ScienceQA. The results show that the proposed method outperformed several baselines in terms of performance retention rate while achieving competitive latency."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and very easy to follow."}, "weaknesses": {"value": "The novelty of this work is questionable, as the main idea of the paper (2 stage visual token pruning, one at visual encoder part and another at llm decoder part) has been done by existing works. For example, VScan (Zhang et al. 2025) adopt very similar idea to prune tokens at two stages, and they actually conducted a more comprehensive set of experiments and analysis of their two-stage framework. The authors should do more comprehensive literature review and discuss their unique contribution compared to those works. \n\nThe paper does not provide sufficient insights or results to support the generalization of this method. The models used in the paper are relatively old now, even Qwen2VL are from over a year ago. It’s unclear how well the method works with more recent VLMs, like Qwen2.5 VL or InternVL3. Also, it’s unclear how the method works for larger scale models (e.g. 32b or 72B models), and how well the method works for other tasks that require fine-grained visual perception like DocVQA or OCRBench. \n\nThe proposed method relies heavily on attention score manipulation, and flash attention does not materialize attention scores, so the actual benefit in terms of latency could be undermined. \n\nIn implementation details, the layers for pruning visual tokens seem pretty arbitrary. How are the layer indices decided? Are these manually tuned for every model? \n\nXing, Long, Qidong Huang, Xiao-wen Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu and Dahua Lin. “PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction.” ArXiv abs/2410.17247 (2024): n. pag.\nZhang, Ce, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia P. Sycara, Haitao Mi and Dong Yu. “VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models.” ArXiv abs/2505.22654 (2025): n. pag."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WhPHNmHYP5", "forum": "Px0UrEUcFQ", "replyto": "Px0UrEUcFQ", "signatures": ["ICLR.cc/2026/Conference/Submission17299/Reviewer_4zbC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17299/Reviewer_4zbC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894358844, "cdate": 1761894358844, "tmdate": 1762927236580, "mdate": 1762927236580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a two-stage framework of token pruning for VLMs. In the first stage, redundant visual tokens are identified and merged\nusing an adaptive clustering algorithm that groups tokens based on feature similarity and local density; in the second stage, the compressed tokens are dynamically pruned during the LLM's prefill process with the cross-modal alignment guidance. The proposed approach is evaluated on multiple models and datasets, and the experimental results show that the proposed approach can help keep good accuracy while improving efficiency by pruning redundant tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The direction of token pruning for VLMs is important since the visual-language input sequences can be lengthy, posing efficiency challenge.\n- The proposed approach is shown to obtain reasonable performance and help to improve the cost-accuracy tradeoff."}, "weaknesses": {"value": "- More analyses and experiments should be included, instead of putting large figures and tables (such as those in the last two pages).\n- The illustration of the proposed method can be further improved (probably with algorithmic descriptions)."}, "questions": {"value": "- I'm wondering how the hyper-parameters are decided, such as the pruning threshold?\n- How would each stage influence the performance separately?\n- I'm wondering how sensitive is the proposed approach to the query? For example, if the query is slightly paraphrased, would the selection be similar and will this have influences on the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sdlTx0VrL0", "forum": "Px0UrEUcFQ", "replyto": "Px0UrEUcFQ", "signatures": ["ICLR.cc/2026/Conference/Submission17299/Reviewer_Fwoe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17299/Reviewer_Fwoe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915568260, "cdate": 1761915568260, "tmdate": 1762927236322, "mdate": 1762927236322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a two-stage framework for visual token pruning. In the first stage, it performs representation-level token merging to remove redundant visual features. In the second stage, it prunes visual tokens with low cross-model relevance. Experimental results on standard benchmarks validate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The structure of this paper is clear.\n2. The proposed approach achieves good empirical results."}, "weaknesses": {"value": "1. The experimental results are not comprehensive. The performance results on bechmarks such as MMBench(-CN) and MM-Vet are not provided. Results on video question answering is also not evaluated. Additionally, performance comparesions with more recent baselines (e.g. [1]) should be provided.\n2. The proposed approach underperforms SparseVLM and VisionZip in certain scenarios and incurs higher computational cost than VisionZip.\n3. The novelty of this paper is limited. The proposed two-stage design closely resembles that of VScan [2], while the progressive token pruning strategy in the LLM stage is similar to PDrop [3].\n4. The motivation of this paper is unclear. The authors should clearly articulate the limitations of previous approaches in this field and explain how the proposed method addresses these gaps.\n5. The overall presentation of the paper lacks clarity, which makes it challenging for readers to understand the main ideas and contributions.\n\n[1] Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs. https://arxiv.org/abs/2412.01818.\n\n[2] VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models. https://arxiv.org/abs/2505.22654.\n\n[3] PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction. https://arxiv.org/abs/2410.17247."}, "questions": {"value": "See the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5HUtRX6ac2", "forum": "Px0UrEUcFQ", "replyto": "Px0UrEUcFQ", "signatures": ["ICLR.cc/2026/Conference/Submission17299/Reviewer_AX3Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17299/Reviewer_AX3Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944012709, "cdate": 1761944012709, "tmdate": 1762927236095, "mdate": 1762927236095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a training-free, plug-and-play framework to accelerate vision language models(VLMs) inference. It uses a two-stage strategy:\nStage 1: Applies adaptive clustering to merge redundant visual tokens based on feature density before they enter the LLM.\nStage 2: Adaptively prunes the remaining tokens inside the LLM during prefill, guided by cross-modal (text-visual) relevance.\nThe key contribution is this hybrid two-stage compression approach that requires no fine-tuning and shows a strong accuracy-efficiency trade-off, especially under aggressive token pruning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed combination of pre-LLM clustering-based merging and intra-LLM text-guided pruning into a single, training-free pipeline is effective.\n2. The technical quality is high, with a thorough experimental evaluation across multiple VLM families (LLaVA, Qwen2) and strong empirical results, particularly in high-compression settings (64 token budget).\n3. The \"plug-and-play\" and \"training-free\" nature is highly significant, offering a practical way to deploy high-resolution VLMs in resource-constrained environments without costly retraining."}, "weaknesses": {"value": "1. The paper fails to report the computational overhead of the Stage 1 clustering algorithm (Sec 3.2 in the paper). This should not be omitted, as the cost is part of the total inference time and complicates speedup comparisons with baselines like VisionZip.\n\n2. The paper's analysis of its specific contributions and trade-offs is insufficient: The method is more accurate but slower than VisionZip. However, the authors do not sufficiently explain why its clustering in Stage 1 is fundamentally better at preserving information than VisionZip's [CLS]-based pruning, nor does it provide qualitative evidence to support this. Besides, the paper fails to clearly articulate the novelty of its Stage 2 cross-modal pruning (Sec 3.3) compared to the text-to-vision guidance in SparseVLM. The similarity between the two approaches is high, and the authors do not sufficiently highlight their specific contribution."}, "questions": {"value": "1. What is the exact latency of the Stage 1 clustering algorithm (Sec 3.2)?\n2. Why is the proposed clustering-based approach (Stage 1) superior to [CLS]-based pruning (used by VisionZip) at preserving information, and is this accuracy gain worth the significant speed trade-off (1.70x vs 2.28x speedup)?\n3. Can the authors explicitly itemize the novel technical contributions of your Stage 2 implementation compared to the prior work like SparseVLM?\n4. The method combines two distinct pruning stages (akin to VisionZip + SparseVLM). Is there a synergistic benefit between them? For instance, does the Stage 1 clustering make the Stage 2 cross-modal pruning more effective or stable? Or are they simply two independent filters applied in sequence?\n5. The reference section contains multiple duplicate citations. For instance, the entries for 'An image is worth 1/2 tokens...' (Chen et al., 2024a/b) and 'Llava-prumerge...' (Shang et al., 2024a/b) are repeated. The bibliography should be carefully checked and corrected.\n\nThe authors should clarify their contributions and the motivations of the proposed pruning and merging techniques and provide more qualitative evidence and analysis to strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PpZRYhXtum", "forum": "Px0UrEUcFQ", "replyto": "Px0UrEUcFQ", "signatures": ["ICLR.cc/2026/Conference/Submission17299/Reviewer_XZ9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17299/Reviewer_XZ9F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002881101, "cdate": 1762002881101, "tmdate": 1762927235528, "mdate": 1762927235528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}