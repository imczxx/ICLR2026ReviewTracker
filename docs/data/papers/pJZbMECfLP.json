{"id": "pJZbMECfLP", "number": 12577, "cdate": 1758208747319, "mdate": 1759897500787, "content": {"title": "Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers", "abstract": "Uncertainty quantification (UQ) is essential for deploying deep neural networks in safety-critical settings. Although methods like Deep Ensembles achieve strong UQ performance, their high computational and memory costs hinder scalability to large models. We introduce Hydra Ensembles, an efficient transformer-based ensemble that prunes attention heads to create diverse members and merges them via a new multi-head attention with grouped fully-connected layers. This yields a compact model with inference speed close to a single network, matching or surpassing Deep Ensembles in UQ performance without retraining from scratch. We also provide an in-depth analysis of pruning, showing that naive approaches can harm calibration, whereas Hydra Ensembles preserves robust uncertainty. Experiments on image and text classification tasks, with various architectures, show consistent gains over Deep Ensembles. Remarkably, in zero-shot classification on ImageNet-1k, our approach surpasses state of the art methods, even without requiring additional training.", "tldr": "", "keywords": ["Uncertainty quantification;ensembling approaches;"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/309776f38558aa131fee4b7c9d605bf3e301aee4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a method for uncertainty estimation in transformers.\nThe key idea is producing an ensemble of models via pruning attention\nheads - which leads to reduced computational complexity compared\nto a full ensemble. Evaluation on image and text classification\nbenchmarks indicates close to state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "*Clarity:*\n- The paper is well-written and is easy to follow.\n\n*Significance / soundness:*\n- This work addresses an important problem of epistemic uncertainty estimation in a widely used transformer architecture.\n- Method itself and technical solutions seems well-motivated and simple to implement, and paper provides sufficient details for reproducing the results.\n- The fact that this method works w/o retraining the base model is a key benefit of the proposed method - re-training large models is intractable.\n- Authors provide multiple variants of the method suitable for different settings (with and w/o uncertainty val set, fine-tuning vs zero-shot).\n\n*Evaluation:*\n- Performance of the method is close to state-of-the-art, both on prediction and OOD tasks, while the cost is significantly lower, especially in the practical bfloat16 setting. This makes the method extremely useful in practice."}, "weaknesses": {"value": "*Novelty:*\n- MoE models seems to be a very similar approach to what is being proposed in this work. A naive baseline\ncould be re-using an existing MoE model for getting uncertainty estimates?\n\n*Evaluation:*\n- Authors claim that the results do not require additional re-training, but in practice this seems a bit misleading\nbecause for both variants of the model (Taylor and Circ) either needs access to uncertainty validation set or\nactually requires fine-tuning.\n- Paper provides reasonable argument for pruning attention heads instead of MLP, but does not provide a\nquantitative evaluation."}, "questions": {"value": "- Have you considered re-using existing MoE-s for uncertainty estimation directly?\n- Evaluation is conducted on prediction / OOD tasks. Would the method also work out-of-the-box for generation tasks, MoE-style?\n- I wonder if authors have any intuition on why LoRA ensembles would work worse than fine-tuned pruning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QCjRsFIWLP", "forum": "pJZbMECfLP", "replyto": "pJZbMECfLP", "signatures": ["ICLR.cc/2026/Conference/Submission12577/Reviewer_UeX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12577/Reviewer_UeX6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920535696, "cdate": 1761920535696, "tmdate": 1762923429175, "mdate": 1762923429175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of uncertainty quantification (UQ) in large-scale transformer models. While Deep Ensembles are known to provide reliable uncertainty estimates, they are computationally expensive due to multiple independently trained models. The authors aim to retain ensemble-level calibration and robustness while significantly reducing computational and memory costs. Hydra Ensembles constructs an efficient transformer ensemble by pruning attention heads in a single pre-trained transformer to generate diverse subnetworks and merging these pruned models into a single architecture using Grouped Fully Connected (GFC) layers, forming a Fused Multi-Head Attention (MHA) and Merged MLP structure."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written and well motivated. \n* The method can be applied to different MoE architectures.\n* It shows good improvements in OOD performance."}, "weaknesses": {"value": "* The paper emphasizes ensemble diversity but does not quantify the resulting predictive diversity of the pruned members or the fused model against standard baselines. Appendix B.5 analyzes sources of diversity (e.g., seeds, batch order) but stops short of reporting diversity magnitude after pruning/fusion (e.g., disagreement rate), leaving it unclear whether Hydra is more or less diverse than alternatives.\n* On image classification, in-distribution calibration (Brier, NLL) is roughly on par with a single model, suggesting the method’s gains are concentrated in OOD detection. So the only benefit of the model is for OOD detection, and it does not affect the method's robustness in IND, which is counterintuitive."}, "questions": {"value": "* If each model differs only in the set of surviving heads (line 240), why do you need to average the weights and biases across the M models for the MLP layer (line 247)? Are these not the same?\n* In Hydra Ensembles(circuit), the authors use the Headmap method (Wang et al., 2025) to identify which heads matter most for uncertainty, and remove the rest. What would be the impact of optimizing for a different task? Also, if you're always optimising for which heads matter most for uncertainty and removing the rest, how do you get different results across the M models?\n* Given the strong performance of Taylor and CircuitAverage in the benchmark (on classification and zero-shot tasks), is it worth including them in the benchmark of inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mXAKlVhHWA", "forum": "pJZbMECfLP", "replyto": "pJZbMECfLP", "signatures": ["ICLR.cc/2026/Conference/Submission12577/Reviewer_J4z9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12577/Reviewer_J4z9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928641547, "cdate": 1761928641547, "tmdate": 1762923428703, "mdate": 1762923428703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article deals with uncertainty quantification (UQ) in transformer-based architectures. It observes that UQ is difficult to achieve, with the current state of the art being held by Deep Ensembles (DE), a contribution from 2017. Deep ensemble achieve sometime-excellent results on UQ at the expense of very expensive training and inference, since multiple training and inference with different parameters must be performed every time.\n\nThe article proposes to use pruning methods on the attention heads of transformer architectures only. They justify this approach by suggesting that such pruning is easier to control and that a combined network can then be put together, performing efficient prediction and UQ at once. This proposal is thoroughly justified on both theoretical and practical grounds, achieving close to the state of the art result at virtually no cost on BF16 precision due to the specialised hardware involved. When using FP32 precision, the computational gains are almost non-existent with respect to DE, however."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The article is relatively easy to read, very well justified with complete and detailed theoretical and practical explanations. The source code is promised to be made available upon acceptance."}, "weaknesses": {"value": "The success of an ensemble heavily relies on the diversity of its component models. If all models make similar mistakes, combining them won't lead to much improvement. Strategies like varying training data, model architectures, or initialization are needed to ensure diversity\n\nAdaptive Attacks: Sophisticated attackers can create adaptive adversarial examples that specifically aim to minimize the uncertainty metrics (like variance or entropy) of the ensemble, attempting to make their attack look like a confident, in-distribution prediction.\n\nMachine-learning models can be fooled by adversarial examples, i.e., carefully-crafted input perturbations that force models to output wrong predictions. While uncertainty quantification has been recently proposed to detect adversarial inputs, under the assumption that such attacks exhibit a higher prediction uncertainty than pristine data, it has been shown that adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism.\n\nhttps://arxiv.org/abs/2309.10586"}, "questions": {"value": "- What would be the cost and gain of using 5 heads in BP16 instead of just 3?\n- Can the source code be made available for review? Many contributors promise to publish code that turn out to be unreadable, uncommented or incomplete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A7nmONgQiv", "forum": "pJZbMECfLP", "replyto": "pJZbMECfLP", "signatures": ["ICLR.cc/2026/Conference/Submission12577/Reviewer_Ez3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12577/Reviewer_Ez3D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762502863591, "cdate": 1762502863591, "tmdate": 1762923428029, "mdate": 1762923428029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Hydra Ensembles: make a few differently pruned versions of the same transformer (different attention heads kept), fine-tune them, and then fuse them into a single model using fused MHA + grouped FC so you can get “ensemble-like” predictions at roughly single-model cost. It targets efficient uncertainty estimation for transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles efficient uncertainty estimation for large transformers, a problem people actually have. That makes the work naturally interesting.\n\n2. Pruning-as-diversity is a nice angle. Most pruning papers try to preserve one model; here, pruning is used to induce differences between members. That’s a small but genuine conceptual twist.\n\n3. The method is demonstrated on both vision (ViT) and language (BERT-style) models, plus a zero-shot setting, so it doesn’t look tied to one benchmark. That strengthens the claim of generality. \n\n4. The cost story is attractive. Claiming ~1× inference vs ~3× for Deep Ensembles directly supports the motivation; it’s exactly the comparison readers care about"}, "weaknesses": {"value": "1. The whole method relies on “different pruned heads = different members,” but the paper doesn’t show simple diversity metrics (e.g. pairwise disagreement/KL) before and after fusion. For this idea, that’s the key missing evidence.\n\n2. Several main tables give single numbers but no std / CI / ± over seeds, even though the paper is about uncertainty/robustness.\n\n3. On SST2, the ensemble seems cheaper than a real, fully fine-tuned deep ensemble, which makes Hydra look better than it might against a “full” baseline. \n\n4. The attention fusion story is clear; the MLP part is basically “we average/group.” Since members were pruned and fine-tuned separately, that choice could wash out diversity; an ablation or a justification can be beneficial.\n\n5. The small theoretical part about pruning hurting more under noisy/OOD inputs is more motivational than general; assumptions aren’t clearly checked on ViT/BERT. Either support it empirically in the main text or de-emphasize it.\n\n6. Because zero-shot UQ is sensitive to prompts/datasets/temperature, more details should be in the main paper, not just the appendix."}, "questions": {"value": "1. When you fuse the pruned members into one model, do you still get separate member outputs so we can measure disagreement, or is it combined into one prediction? A small clarification (and maybe a number) would help.\n\n2. If different members keep different attention heads, does the fused layer run the union of those heads (which could increase compute), or do you share some heads to stay close to 1×?\n\n3. You average / group MLP weights across members. Did you try an alternative (e.g. a small per-member adapter) and it didn’t help, or was this mainly for simplicity?\n\n4. How sensitive is Hydra to how aggressively you prune? A short plot or table for one dataset would clarify how robust the method is.\n\n5. For the different Hydra members, how do you ensure that the pruned head sets are actually different (i.e., not largely overlapping)? Do you use different random seeds for the pruning score, or do you enforce low overlap between members?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uFlmq2tACH", "forum": "pJZbMECfLP", "replyto": "pJZbMECfLP", "signatures": ["ICLR.cc/2026/Conference/Submission12577/Reviewer_twYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12577/Reviewer_twYJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762702349482, "cdate": 1762702349482, "tmdate": 1762923427773, "mdate": 1762923427773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hydra Ensembles, a novel framework designed to achieve uncertainty-aware efficient ensembling for large-scale transformer models such as ViT, BERT, and CLIP. The key motivation is to retain the uncertainty quantification (UQ) robustness of Deep Ensembles while drastically reducing their computational and memory costs.\n\nHydra Ensembles works by pruning attention heads from a single pre-trained transformer to create diverse submodels, which are then fused into a single network via a Grouped Fully Connected (GFC) fusion of their Multi-Head Attention (MHA) and MLP layers. Unlike conventional ensemble approaches, Hydra Ensembles:\n\t•\tavoids retraining each model from scratch,\n\t•\tallows near-single-model inference cost, and\n\t•\tpreserves ensemble diversity for robust uncertainty estimation.\n\nThe paper’s contributions are threefold:\n\t1.\tTheoretical Analysis: Demonstrates that naïve pruning can degrade model calibration under noisy data conditions, supported by a formal proposition showing loss gap widening in pruned models.\n\t2.\tFramework Design: Introduces structured head-level pruning and GFC-based fusion that maintain model diversity while minimizing computation.\n\t3.\tEmpirical Evaluation: Provides extensive experiments on image classification (ImageNet-1K, CIFAR-100), text classification (SST-2), and zero-shot image classification (OpenCLIP). Results show that Hydra Ensembles achieve comparable or superior uncertainty metrics (AUROC, AUPR, ECE) to Deep Ensembles while being ~3× faster and requiring significantly fewer parameters.\n\nOverall, Hydra Ensembles present a theoretically grounded and practically efficient solution for scalable uncertainty quantification in transformer architectures — bridging the gap between computational efficiency and epistemic robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality\n\t•\tThe paper presents a novel architectural strategy for ensemble diversity by pruning and recombining transformer attention heads rather than training separate models.\n\t•\tThe Grouped Fully Connected (GFC) fusion mechanism introduces an efficient ensembling pipeline that avoids the typical cost explosion of Deep Ensembles.\n\t•\tAlthough it builds on concepts like BatchEnsemble and pruning-based model compression, Hydra’s hybridization of pruning and ensembling is unique in both motivation and execution.\n\t•\tThe formal proposition linking random pruning to calibration degradation is a valuable theoretical insight, providing justification for structured pruning instead of purely empirical reasoning.\n\nQuality\n\t•\tThe methodology is sound and reproducible, with strong empirical results across both vision (ViT, ImageNet-1K, CIFAR-100) and text (BERT, SST-2) domains.\n\t•\tExtensive ablations (pruning ratios, ensemble sizes, GFC variants) demonstrate that the performance improvements are not cherry-picked but systematic.\n\t•\tThe authors carefully balance theoretical analysis, algorithmic description, and experimental validation — showing maturity in both design and evaluation.\n\t•\tCalibration metrics (ECE, AUROC, AUPR) are properly selected for uncertainty quantification, and their consistent improvement validates the main claim.\n\nClarity\n\t•\tThe paper is well-organized and readable, with clear section flow and minimal redundancy.\n\t•\tFigures such as the Hydra architecture schematic and calibration plots effectively support understanding.\n\t•\tThe theoretical analysis is concise and mathematically consistent, though dense in presentation — yet the accompanying intuition keeps it accessible to non-specialists.\n\t•\tNotation is consistent, and references to related work are appropriate and fair.\n\nSignificance\n\t•\tPractical significance: Hydra Ensembles deliver ensemble-level uncertainty quality at a fraction of the computational cost, which is crucial for large transformer models where full ensembles are infeasible.\n\t•\tResearch significance: The framework provides a blueprint for uncertainty-aware model compression, bridging a long-standing gap between trustworthiness and efficiency.\n\t•\tCommunity impact: The method is relevant to multiple research threads at ICLR — efficient transformers, uncertainty quantification, and scalable AI reliability.\n\t•\tIts applicability to both vision and language domains broadens its potential adoption and demonstrates methodological generality."}, "weaknesses": {"value": "1. Limited Theoretical Grounding of Calibration Claims\n\t•\tThe proposed theoretical proposition—that random pruning leads to calibration degradation—is intuitively plausible but lacks rigorous derivation or empirical verification linking the theory to measured uncertainty metrics (ECE, NLL, Brier score).\n\t•\tThe argument relies primarily on Fisher Information heuristics rather than a formal probabilistic treatment of epistemic variance or diversity loss.\n\t•\tThis makes the theoretical part informative but incomplete, as it doesn’t generalize to nonlinear pruning effects or the stochastic dynamics of self-attention.\n\nRecommendation:\nThe authors could expand this section by (a) connecting pruning-induced variance loss to epistemic uncertainty via bias–variance decomposition, or (b) empirically validating the proposition using entropy/Fisher metrics before and after pruning.\n\n2. Moderate Conceptual Novelty\n\t•\tHydra Ensembles’ innovation lies mainly in the engineering combination of known techniques: pruning for efficiency (Michel et al., 2019; Voita et al., 2019) and efficient ensemble fusion (Wen et al., 2020; Havasi et al., 2021).\n\t•\tWhile the integration is elegant, it does not introduce a fundamentally new learning principle or uncertainty formulation.\n\t•\tRelated works like BatchEnsemble (Wen et al., ICLR 2020), MIMO (Havasi et al., NeurIPS 2021), and LayerDrop (Fan et al., ICLR 2020) already explore efficiency–diversity trade-offs; the paper could better clarify where Hydra theoretically diverges from these beyond implementation detail.\n\nRecommendation:\nReframe Hydra as a structured synthesis approach rather than a conceptual breakthrough, emphasizing its engineering elegance and scalability benefits.\n\n3. Scope of Experiments\n\t•\tAll evaluations are confined to classification tasks (CIFAR-100, ImageNet-1K, SST-2). No tests on generation or multimodal transformers (e.g., CLIP zero-shot) beyond classification accuracy and calibration.\n\t•\tWithout broader task validation, it’s unclear whether Hydra’s uncertainty improvements generalize to tasks requiring sequence modeling, open-ended text generation, or multi-modal reasoning—key frontiers of transformer research.\n\nRecommendation:\nInclude or discuss pilot results on transformer-based generative tasks (e.g., GPT-style models) or multimodal settings to support claims of generality.\n\n4. Insufficient Discussion of Trade-offs\n\t•\tThe paper highlights efficiency gains (3× faster inference, fewer parameters) but lacks quantitative trade-off analysis between pruning ratio, uncertainty calibration, and ensemble diversity.\n\t•\tThe reader is left without a clear sense of how much diversity is sacrificed at higher pruning rates or how inference cost scales with ensemble size.\n\t•\tAdditionally, calibration–efficiency curves or Pareto plots would strengthen interpretability.\n\nRecommendation:\nProvide explicit trade-off visualizations (e.g., efficiency vs. ECE or AUROC) and discuss how practitioners can tune pruning levels for optimal performance.\n\n5. Missing Analysis of Diversity and Correlation Among Pruned Submodels\n\t•\tSince ensemble robustness depends on model diversity, the paper should quantify inter-head diversity or correlation (e.g., using cosine similarity, pairwise prediction disagreement, or mutual information across pruned models).\n\t•\tWithout such analysis, it’s difficult to confirm whether Hydra truly achieves “diverse ensembling” or merely benefits from redundancy.\n\nRecommendation:\nAdd an empirical diversity analysis to support the central claim of maintaining epistemic diversity post-pruning.\n\n6. Limited Robustness and OOD Testing\n\t•\tAlthough Hydra improves calibration, there are no results under domain shift or corrupted data (e.g., ImageNet-C, CIFAR-C, SST-2 perturbations).\n\t•\tThis omission weakens claims about robustness and “uncertainty-awareness,” since true epistemic reliability is best evaluated under distributional drift.\n\nRecommendation:\nInclude robustness experiments on corrupted or OOD benchmarks to demonstrate Hydra’s behavior under uncertainty-inducing conditions.\n\n7. Presentation and Comparison Clarity\n\t•\tSome mathematical notation is dense and occasionally inconsistent across sections (e.g., subscripts for attention heads and fusion layers).\n\t•\tRelated work could be contextualized more critically — especially contrasting Hydra’s scalability and calibration against post-hoc methods like temperature scaling, EMM, or confidence regularization.\n\nRecommendation:\nAdd a summary table contrasting Hydra with BatchEnsemble, MIMO, and FastGeLU Ensembles, highlighting distinct features, efficiency, and calibration properties."}, "questions": {"value": "1. On the Theoretical Proposition and Calibration Justification\n\t•\tThe paper presents a theoretical result suggesting that random pruning widens calibration loss gaps.\n\t•\tCould the authors expand or clarify the assumptions underlying this result — e.g., is the bound derived under linearized model assumptions, or does it generalize to nonlinear self-attention layers?\n\t•\tHow does this proposition directly link to empirical calibration metrics (ECE, NLL, AUROC)?\n\t•\tWould it be possible to empirically measure information loss or variance reduction (e.g., via Fisher information or entropy) before and after pruning to validate the theory?\n\n2. On Model Diversity and Ensemble Behavior\n\t•\tHydra’s design implies that pruning attention heads creates diverse submodels whose combination improves uncertainty calibration.\n\t•\tHow do the authors quantify or measure diversity among these pruned heads or submodels?\n\t•\tHave they examined pairwise output correlations or disagreement rates across ensemble members?\n\t•\tWithout such evidence, how can we be confident that Hydra’s calibration improvements arise from true epistemic diversity rather than simple regularization effects?\n\n3. On Efficiency–Uncertainty Trade-offs\n\t•\tThe paper reports efficiency gains (~3× faster inference) while maintaining or improving uncertainty metrics.\n\t•\tCould the authors provide explicit quantitative trade-off curves between pruning ratio, uncertainty calibration, and accuracy?\n\t•\tFor example, what is the marginal drop in accuracy or AUROC per additional pruning step?\n\t•\tThis would help practitioners decide optimal pruning thresholds under different compute constraints.\n\n4. On Generalization Beyond Classification Tasks\n\t•\tHydra Ensembles are evaluated on image and text classification tasks, but transformers are widely used for generation and multimodal learning.\n\t•\tHave the authors explored whether Hydra can be applied to sequence generation tasks (e.g., autoregressive decoding, summarization) or vision–language models like CLIP or BLIP?\n\t•\tIf not yet tested, do the authors foresee architectural or stability challenges (e.g., head dependencies in causal self-attention) that might limit its application?\n\n5. On the Fusion Mechanism (Grouped Fully Connected Layers)\n\t•\tThe Grouped Fully Connected (GFC) fusion layer is a central component, but its mathematical and practical behavior could be clarified.\n\t•\tHow does GFC differ from existing ensemble fusion or parameter-sharing mechanisms (e.g., BatchEnsemble, SplitDense)?\n\t•\tIs there a risk of overfitting or co-adaptation when merging diverse heads via GFC?\n\t•\tHow is group size or fusion granularity chosen, and how sensitive is Hydra’s performance to these hyperparameters?\n\n6. On the Scope of Uncertainty Metrics\n\t•\tThe experiments primarily report ECE and AUROC, which measure calibration and discrimination, respectively.\n\t•\tHave the authors evaluated additional uncertainty metrics such as Brier score, NLL, or predictive entropy?\n\t•\tDifferent metrics capture complementary aspects of uncertainty; including them could strengthen claims about “uncertainty-awareness.”\n\n7. On Robustness and Distribution Shift\n\t•\tHydra’s motivation includes improving reliability and robustness through structured diversity.\n\t•\tHave the authors tested Hydra on corrupted or domain-shift datasets (e.g., CIFAR-C, ImageNet-C, SST-2 with noise)?\n\t•\tIf not, could they provide a theoretical argument or empirical proxy suggesting Hydra’s robustness benefits beyond clean test distributions?\n\n8. On Implementation Complexity and Reproducibility\n\t•\tHydra involves structured pruning, submodel fusion, and fine-tuning stages.\n\t•\tCould the authors comment on the implementation complexity and reproducibility — e.g., how many lines of modification are required for ViT or BERT baselines?\n\t•\tAre pretrained Hydra checkpoints or open-source scripts available (or planned) to facilitate community adoption?\n\n9. On Relation to Other Efficient Ensembles\n\t•\tHydra’s conceptual overlap with BatchEnsemble (Wen et al., 2020) and MIMO (Havasi et al., 2021) is acknowledged but not deeply dissected.\n\t•\tCould the authors articulate the precise difference in diversity mechanism between Hydra and these methods?\n\t•\tSpecifically, how does Hydra’s pruning-induced diversity compare empirically to BatchEnsemble’s multiplicative rank-1 reparameterization or MIMO’s input-sharing scheme?\n\n10. On Interpretability of Pruned Attention Heads\n\t•\tSince Hydra modifies attention structure, there may be implications for interpretability (e.g., loss of certain attention patterns or semantics).\n\t•\tHave the authors analyzed whether the pruned heads correspond to interpretable functions (e.g., positional, syntactic, or semantic attention)?\n\t•\tIf not, do they expect pruning to impact model explainability — and could this trade-off affect trustworthiness in downstream deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper does not raise any direct ethical concerns in its methodology, data usage, or potential applications.\nIt focuses exclusively on architectural efficiency and uncertainty quantification in transformer models — a technical advancement with no involvement of human subjects, personal data, or sensitive social content.\n\nAll datasets used (e.g., ImageNet-1K, CIFAR-100, SST-2) are public, well-established research benchmarks with existing licensing and ethical clearances.\nThe work does not propose methods that could be misused for harmful decision-making, data extraction, or disinformation.\n\nThe paper poses no ethical or legal risks and aligns with responsible AI research practices.\nIts goal — improving uncertainty-aware efficiency in transformer models — contributes positively to trustworthy and sustainable AI."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PEY5McA8RK", "forum": "pJZbMECfLP", "replyto": "pJZbMECfLP", "signatures": ["ICLR.cc/2026/Conference/Submission12577/Reviewer_e9rh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12577/Reviewer_e9rh"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12577/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763143887142, "cdate": 1763143887142, "tmdate": 1763143917215, "mdate": 1763143917215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}