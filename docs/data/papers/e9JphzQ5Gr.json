{"id": "e9JphzQ5Gr", "number": 18046, "cdate": 1758283254319, "mdate": 1763645487797, "content": {"title": "CLIP as a Prior Teacher: Breaking the Label Dependency in Semi-Supervised Learning", "abstract": "Semi-supervised learning (SSL) has shown remarkable potential in scenarios with limited labeled data. However, our study reveals that existing SSL approaches remain inherently label-dependent—their ability to exploit unlabeled samples is bounded by the quantity and quality of labeled data. To address this limitation, we establish a portable asymmetric-modalities co-training framework for efficiently integrating CLIP into SSL, termed CaPT. CaPT aggregates predictions from a fully fine-tuned unimodal network and a parameter-efficiently fine-tuned multimodal CLIP model via carefully designed co-pseudo labels, which guide training by refining CLIP’s biased predictions and supplementing reliable prior for SSL without compromising efficiency. Moreover, the asymmetric-modalities mitigates the pattern-homogeneity bottleneck observed in previous co-training methods, enabling richer cross-model information exchange. CaPT consistently achieves state-of-the-art performance across multiple SSL benchmarks. Notably, it outperforms the second-best method by **21.38%** and **4.05%** on the CIFAR-100 and EuroSAT datasets, respectively, under the one-label-per-class setting, demonstrating its strong potential in low-label regimes.", "tldr": "", "keywords": ["semi-supervised learning", "CLIP", "co-training", "adapter-tuning", "low-label regimes", "vision-language model"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b63922e228b070f6d32c571212f8d54cd5ceab78.pdf", "supplementary_material": "/attachment/cb3fbd0bf0dbded131f93fb6c917048a1e4484ab.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a semi-supervised learning approach based on an asymmetric teacher–student scheme that uses CLIP as the guidance model. To mitigate known CLIP biases, the method combines consistency regularization with a lightweight fine-tuning strategy to keep compute overhead modest. A theoretical analysis explores learning with scarce labels, linking label quantity/quality to training dynamics and expected performance. Experiments cover multiple image-classification datasets, with ablations and visualizations that examine the contribution of each component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Coherent design and solid empirical gains: The integration of CLIP within an SSL pipeline is thoughtfully engineered; ablations suggest each component contributes meaningfully, and the reported results surpass the listed baselines.\n- Clear exposition and positioning: The manuscript is generally easy to follow, and the related-work section situates the method well among comparable SSL approaches.\n- Comprehensive experimentation: The empirical section is broad, includes analyses of the proposed regularization and fine-tuning, and emphasizes practical efficiency.\n- Interesting theoretical motivation: The analysis connecting pseudo-label quality to labeled-data quality—and to how prototypical the labeled samples are—is insightful and adds value to the overall contribution."}, "weaknesses": {"value": "- Questionable generality of the “framework” claim: CLIP differs meaningfully from modern VLMs, and CLIP itself is comparatively dated. Without evidence that the approach transfers cleanly to stronger/modern VLMs—or to other tasks—the contribution feels more like a targeted CLIP-based recipe than a broadly applicable framework. Demonstrating adaptability (e.g., with a second teacher family or a distinct task) would strengthen the novelty and impact.\n- Scope limited to CLIP-based image classification: While effective in this setting, the study does not explore alternative tasks beyond classification. The paper does not claim multi-modality; however, discussing or lightly probing extensibility (even in a small-scale study) would strengthen the practical generality of the approach.\n- Theory presentation could be clearer (minor but actionable):\n    - Introduction: grammar around **line 50** needs a pass.\n    - Symbols should be explicitly defined when first used: $\\epsilon_n$ ,  $r$ , and  $\\hat{y}$ \n    - Tightening these points would make the connection between assumptions and the training pipeline more transparent."}, "questions": {"value": "- Teacher swapability: How readily can the teacher be replaced with stronger CLIP variants or contemporary vision–language models? Are there stability or calibration issues when doing so?\n- Beyond classification: What modifications (if any) are needed to extend the method to detection/segmentation or image–text retrieval? Were any preliminary attempts made?\n- Sensitivity to prompts and thresholds: How sensitive is performance to text-prompt choices, pseudo-label thresholds/temperatures (if applicable), and augmentation strength?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kQuU74bLcE", "forum": "e9JphzQ5Gr", "replyto": "e9JphzQ5Gr", "signatures": ["ICLR.cc/2026/Conference/Submission18046/Reviewer_Moze"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18046/Reviewer_Moze"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393863211, "cdate": 1761393863211, "tmdate": 1762927834766, "mdate": 1762927834766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to address a well-known problem in SSL: the model's performance is heavily dependent on the quantity and quality of the limited labeled data. The authors claim to break this dependency by introducing CaPT. The core idea is to concurrently train a fft unimodal network on images and a parameter-efficiently fine-tuned (PEFT) CLIP model. These two models supervise each other via an entropy-weighted co-pseudo label. The results show that CaPT achieved state-of-the-art (SOTA) performance across multiple SSL benchmarks, especially in extremely low-label settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors have tested CaPT on a wide range of benchmarks, including USB, ImageNet, and several fine-grained datasets, covering various scenarios of label scarcity, label noise, and class imbalance."}, "weaknesses": {"value": "1. The core contribution of this paper is severely overclaimed. The CaPT, is, in my opinion, nothing more than a simple combination of several existing ideas, such as co-training, adapter-tuning, and mixup.\n2. The authors make the assertion that their work breaks the label dependency. In reality, they have merely replaced the dependency on high-quality labels with a dependency on high-quality CLIP prior. This is laid bare in Appendix N: when CLIP performs poorly on the FGVCAircraft dataset, CaPT's performance is low as well.\n3. Entropy-based weighting is naive. Did you explore any other, more robust weighting strategies?"}, "questions": {"value": "See in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cyP6RbGcZQ", "forum": "e9JphzQ5Gr", "replyto": "e9JphzQ5Gr", "signatures": ["ICLR.cc/2026/Conference/Submission18046/Reviewer_rE4S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18046/Reviewer_rE4S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727825984, "cdate": 1761727825984, "tmdate": 1762927834345, "mdate": 1762927834345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CaPT (CLIP as a Prior Teacher), a novel semi-supervised learning framework that leverages the strong generalization ability of CLIP to reduce the dependency of SSL methods on labeled data.\nThe key idea is to treat CLIP as a prior teacher, combining its zero-shot semantic knowledge with a unimodal visual learner through an asymmetric co-training mechanism.\nThe paper also provides theoretical insights into label dependency in SSL and demonstrates significant performance gains on standard benchmarks under extreme low-label conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formalizes label dependency in SSL and clearly articulates why existing methods fail when labeled data are extremely scarce.\n\n2. The asymmetric co-training between CLIP and the visual model is simple yet well-motivated, enabling complementary learning between prior knowledge and data-driven adaptation.\n\n3. Extensive experiments on CIFAR, STL, and ImageNet subsets show consistent improvements over strong SSL baselines (FixMatch, FreeMatch, RegMixMatch, etc.), especially in 1-shot and 2-shot settings."}, "weaknesses": {"value": "1.\tWhile some ablations are included, it would be useful to see results with other multimodal priors (e.g., SigLIP, EVA-CLIP) to confirm generality.\n\t2.\tThe paper focuses on SSL baselines but could better position itself against few-shot or distillation-based methods."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "See Weaknesses."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S18OUbOesN", "forum": "e9JphzQ5Gr", "replyto": "e9JphzQ5Gr", "signatures": ["ICLR.cc/2026/Conference/Submission18046/Reviewer_z82e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18046/Reviewer_z82e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906980140, "cdate": 1761906980140, "tmdate": 1762927833703, "mdate": 1762927833703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel semi-supervised learning framework called CaPT. This framework leverages the powerful generalization ability of CLIP to reduce the dependence of semi-supervised learning methods on labeled data. The core idea is to treat CLIP as a prior teacher, combining its zero-shot semantic knowledge with a unimodal visual learner through an asymmetric collaborative training mechanism. This paper also delves into the theoretical issues of label dependence in semi-supervised learning and validates CaPT's significant performance improvement on standard benchmarks with a very small number of labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper formalizes label dependencies in SSL and clearly explains why existing methods fail when labeled data is extremely scarce.\n\n2. The asymmetric co-training method between CLIP and the visual model, while simple, is well-motivated and enables complementary learning between prior knowledge and data-driven adaptation.\n\n3. Extensive experiments on CIFAR, STL, and subsets of ImageNet demonstrate that our proposed method consistently improves performance compared to robust SSL baseline methods (such as FixMatch, FreeMatch, and RegMixMatch), especially in single-sample and two-sample scenarios."}, "weaknesses": {"value": "1. While this paper includes some ablation experiments, it would be better to use other multimodal priors (e.g., SigLIP, EVA-CLIP) to verify its generalizability.\n\n2. This paper focuses on single-sample baseline methods, but they can be better compared with few-sample or distillation-based methods."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "See Weaknesses."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S18OUbOesN", "forum": "e9JphzQ5Gr", "replyto": "e9JphzQ5Gr", "signatures": ["ICLR.cc/2026/Conference/Submission18046/Reviewer_z82e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18046/Reviewer_z82e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906980140, "cdate": 1761906980140, "tmdate": 1763703945934, "mdate": 1763703945934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a semi-supervised learning pipeline that leverages CLIP’s prior knowledge in a co-training framework where CLIP is updated with parameter-efficient fine-tuning (e.g., adapters) rather than full model tuning. Experiments on common SSL benchmarks show performance gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is build on clear motivation with supporting theory showing pseudo-label error grows with prototype bias and fewer labels, formalizing a well-known intuition.\n\n2. The paper proposes a practical strategy to incorporate CLIP in SSL that balances efficiency (adapter tuning, feature-level Mixup) and reliability (co-training + entropy-weighted labels)"}, "weaknesses": {"value": "1.Domain dependence of CLIP priors: where CLIP is strong (e.g., natural images like CIFAR), gains are intuitive; where CLIP is weaker or off-distribution (e.g., EuroSAT and many medical domains), benefits may diminish and are harder to guarantee.\n\n\n2. Technical contributions feel like a careful combination of known pieces (co-training, PEFT adapters, entropy-weighted pseudo-labels, Mixup)"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UFzGg89I8l", "forum": "e9JphzQ5Gr", "replyto": "e9JphzQ5Gr", "signatures": ["ICLR.cc/2026/Conference/Submission18046/Reviewer_Uzf4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18046/Reviewer_Uzf4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969645176, "cdate": 1761969645176, "tmdate": 1762927833267, "mdate": 1762927833267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on CLIP-based semi-supervised learning. First, the paper demonstrates through theoretical and empirical analysis that performance is limited by the quantity and quality of labeled data. Then, it proposes a new method called CLIP as a Prior Teacher (CaPT), encompassing three modules: a pseudo-label module based on ViT, an adapter tuning module, and an ensemble module that combines the predictions of the first two modules. Experimental results validate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The studied problem of semi-supervised learning with CLIP is an important and interesting research topic.\n\nThe experimental results are very comprehensive and validate the effectiveness of the proposed method."}, "weaknesses": {"value": "The proposed approach seems to be a direct combination of the FixMatch approach (module A) and the parameter-efficient fine-tuning approach (module B). Although the co-training technique is interesting, directly combining off-the-shelf approaches may weaken the paper's contributions.\n\nThe paper's layout can be improved. First, it is unusual to include a theorem in the introduction. Additionally, it is not rigorous to directly call the different modules \"A,\" \"B,\" and \"C.\" Additionally, there are typos, such as \"though\" instead of \"through\" in line 229. Furthermore, the augmentation in Eq. (2) is an addition. However, this is not always true, as many augmentations cannot be implemented by simply adding a feature to another vector or tensor.\n\nAlthough the co-training scheme is effective, involving a ViT and a CLIP model together is much more complex than the compared methods.\n\nTheorems 1 seem irrelevant to the motivation and the proposed approach. First, it is obvious that the classifier's performance will be inferior with less data, without the need for any theoretical analysis. Second, accuracy is the nearest-prototype pseudo-label error, which is different from the classification model. Third, a larger upper bound does not necessarily indicate a smaller label error."}, "questions": {"value": "Please see \"weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bjjRGhIhAF", "forum": "e9JphzQ5Gr", "replyto": "e9JphzQ5Gr", "signatures": ["ICLR.cc/2026/Conference/Submission18046/Reviewer_u1wY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18046/Reviewer_u1wY"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762242051883, "cdate": 1762242051883, "tmdate": 1762927832887, "mdate": 1762927832887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}