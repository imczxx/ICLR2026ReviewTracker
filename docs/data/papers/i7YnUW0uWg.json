{"id": "i7YnUW0uWg", "number": 20887, "cdate": 1758311428641, "mdate": 1759896953705, "content": {"title": "Modeling the Density of Pixel-level Self-supervised Embeddings for Unsupervised Pathology Segmentation in Medical CT", "abstract": "Accurate detection of all pathological findings in 3D medical images remains a significant challenge, as supervised models are limited to detecting only the few pathology classes annotated in existing datasets. To address this, we frame pathology detection as an unsupervised visual anomaly segmentation (UVAS) problem, leveraging the inherent rarity of pathological patterns compared to healthy ones. We enhance the existing density-based UVAS framework with two key innovations: (1) dense self-supervised learning for feature extraction, eliminating the need for supervised pretraining, and (2) learned, masking-invariant dense features as conditioning variables, replacing hand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT volumes, our fully self-supervised model, Screener, outperforms existing UVAS methods on four large-scale test datasets comprising 1,820 scans with diverse pathologies. Furthermore, in a supervised fine-tuning setting, Screener surpasses existing self-supervised pretraining methods, establishing it as a state-of-the-art foundation for pathology segmentation. The code and pretrained models will be made publicly available.", "tldr": "We present Screener, a fully self-supervised pathology segmentation model for medical CT images, outperforming existing methods in both unsupervised and supervised fine-tuning settings.", "keywords": ["Unsupervised Visual Anomaly Segmentation", "Self-supervised learning", "Density estimation", "Computed Tomography"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a13145bc5f34a11be2f2a4ca8925aaa4801f3333.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose Screener, an unsupervised visual anomaly segmentation (UVAS) method, which leverages dense self-supervised pre-training and masking-invariant dense feature conditioning variables as replacement for positional encodings. The authors train Screener on 30k unlabeled CT-volumes and outperform existing UVAS methods and self-supervised pretraining methods when fine-tuning their method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The method the authors propose, is really nice, as it leverages dense self-supervised pretraining, which is a good choice given their goal of dense downstream applications and their addition of their masking-invariant dense feature conditioning variables as replacement for positional encoding is also well motivated and creative.\nAdditionally the clarity of the paper is also very high, with the authors explaining the different aspects very well."}, "weaknesses": {"value": "The majority of my criticisms hinge around two key points of the authors paper.\n\n1) The authors claim their method exceeds current self-supervised learning methods. However, the authors don't compare against MAE pre-training, which was shown to be the strongest SSL pre-training for 3D medical image computing in the medical domain in a recent benchmark [1]. In particular the chosen VoCo and SwinUNETR pre-training baselines were shown to be bad for segmentation in general, making this claim not substantiated.\n\n2)  The evaluation protocol of using just 25 training cases is limited. If the authors claim their SSL method to be overall useful for segmentation I would like them to additionally evaluate their pre-training against other pre-training methods on a full-data regime. This is largely, because pre-training methods in general appear to yield performance benefits in small-data regimes, however when more data is available it may not do so anymore. Having this information is crucial for practitioners to know which SSL method to choose given their data availability.\n\n\nIf the authors 1) include MAE pre-training as an additional SSL baseline (when finetuning) and 2) include a finetuning experiment in a  full-data-regime and 3) include a full-length nnU-Net training as reference, I will raise my score further.\n\n\n[1] Wald, Tassilo, et al. \"An OpenMind for 3D medical vision self-supervised learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025."}, "questions": {"value": "Q1: I am not sure if I missed it but how do you get from the anomaly maps to hard predictions as needed for DSC measurement? Is there some thresholding and if so, how is the thresholding done in the unsupervised and supervised setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QoglRK1j5E", "forum": "i7YnUW0uWg", "replyto": "i7YnUW0uWg", "signatures": ["ICLR.cc/2026/Conference/Submission20887/Reviewer_otbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20887/Reviewer_otbi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799946839, "cdate": 1761799946839, "tmdate": 1762937665975, "mdate": 1762937665975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Screener, a self-supervised anomaly segmentation framework for 3D CT. The method combines dense self-supervised descriptor learning, masking-invariant conditioning embeddings, and density modeling to detect abnormal patterns. The model trains on over 30,000 unlabeled CT volumes. Screener is tested on four public CT benchmarks under both unsupervised and fine-tuning settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem is clearly motivated, as a supervised method cannot capture all distributions of pathologies.\n\nThe integration of dense SSL for descriptors and SSL-learned conditioning embeddings looks novel.\n\nThe proposed model is tested on four public CT benchmarks under both unsupervised and fine-tuning settings. Strong and consistent performance is observed."}, "weaknesses": {"value": "While the method is well-executed, the contribution may appear incremental as the dense SSL for voxel embeddings and conditioning is a straightforward extension of VICReg and conditioning replacing sin-cos encodings is conceptually natural. The paper would benefit from clearer articulation of why condition embeddings are fundamentally more informative than APE or sin-cos encodings beyond empirical gains.\n\nThe proposed method looks closely related to an existing method [1]. It would be beneficial if the authors acknowledge and contrast with the prior work.\n\nThe method produces per-voxel anomaly scores. A threshold is necessary to perform the final segmentation. It is unclear how this threshold is determined.\n\nReferences:\n\n[1] Seince, Maxime, Loı̈c Le Folgoc, Luiz Facury De Souza, and Elsa Angelini. \"Dense Self-Supervised Learning for Medical Image Segmentation.\" In Medical Imaging with Deep Learning, pp. 1371-1386. PMLR, 2024."}, "questions": {"value": "1. Are there reasons why condition embeddings are fundamentally more informative than APE or sin-cos encodings?\n\n2. How is the threshold determined to obtain the segmentation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VNLyaE63cU", "forum": "i7YnUW0uWg", "replyto": "i7YnUW0uWg", "signatures": ["ICLR.cc/2026/Conference/Submission20887/Reviewer_NAFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20887/Reviewer_NAFn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948937727, "cdate": 1761948937727, "tmdate": 1762937624584, "mdate": 1762937624584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Screener, a fully self-supervised framework for unsupervised pathology segmentation in 3D CT. It reframes pathology detection as unsupervised visual anomaly segmentation (UVAS), under the assumption that pathological regions are statistically rare. The method enhances density-based UVAS by (i) learning dense voxel-level self-supervised descriptors tailored to CT data, and (ii) introducing masking-invariant contextual embeddings as conditioning variables within a conditional density model. The models are trained on 30k unlabeled CT volumes. Screener achieves state-of-the-art results on four public CT datasets (LIDC, MIDRC, KiTS, LiTS). When distilled into a single UNet and fine-tuned with limited labeled data, it matches or surpasses medical SSL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Reviewer acknowledges the following contributions:\n\n **Addresses an important medical challenge**: The paper tackles the critical and realistic problem of detecting all pathological findings in 3D CT without requiring pixel-level annotation, which is a limitation that hinders clinical deployment of current supervised methods.\n\n **Interesting and well-motivated technical idea**: \n\n- The proposed use of self-supervised learning for both the descriptor and conditioning models is conceptually appealing. It enables the system to model normal anatomy and contextual expectations directly from large unlabeled data, thereby detecting deviations that correspond to abnormalities.\n\n- Furthermore, the integration of dense SSL with masking-invariant conditioning is simple but effective. It allows a simple Gaussian density model to perform on par with more complex normalizing flows, indicating that meaningful contextual embeddings can simplify anomaly modeling.\n\n**Broad evaluation**: Experiments across four diverse datasets demonstrate that Screener generalizes well across organs and pathologies. The unsupervised anomaly segmentation results are significant, with clear ablation studies supporting each component.\n\n **Clear presentation**: The paper is clearly written, logically organized, and easy to follow, making complex ideas accessible to both machine learning and medical imaging audiences."}, "weaknesses": {"value": "I found the following weaknesses in the current manuscript:\n\n- **Core conceptual clarity**: The motivation for why optimizing the **conditional density model** $q_{\\theta}(y|c)$. helps detect abnormal regions, is not fully articulated. Since this is a central concept, further clarification and discussion would strengthen the theoretical grounding. For instance, authors need to provide a visualization of normal pixels near the abnormal regions and show how the heatmap behaves to provide insights.\n\n- **Experimental organization**: The unsupervised experiments could be better structured to highlight the benefits of dense SSL. For instance, baselines should be grouped into (i) image-level SSL models (e.g., autoencoder) as well as adding SOTA models like LVM-Med [1] (developed for ResNet-50 and suitable with U-Net) and (ii) dense SSL models (the current author compared), to clearly demonstrate that dense SSL drives the improvements. \n\n- **Supervised performance gap** – While competitive, the fine-tuned Screener in Table 2 lags behind certain SOTA supervised or SSL-pretrained models. The authors could explore initializing the distilled UNet with a pretrained medical backbone (e.g., RadImageNet or LVM-Med to enhance feature representations.\n\n- **Presentation detail** – Some result table formats (e.g., Tables 1–3) could be reformatted and polished for a more professional appearance and readability.\n\n[1] Lvm-med: Learning large-scale self-supervised vision models for medical imaging via second-order graph matching, NeurIPS 2023.\n\n[2] RadImageNet: an open radiologic deep learning research dataset for effective transfer learning.\" Radiology: Artificial Intelligence"}, "questions": {"value": "- In the self-supervised learning settings, besides the medical models, can authors examine the performance of the generalized model, for e.g., the Grounding Dino technique, which can retrieve objects given prompt input and apply some threshold techniques to filter noise [3]? Incorporating such experiments is interesting and highlights the benefit or potential weakness of the current strategy.\n\n-  It would be great to see the advanced performance by applying the U-Net with pre-trained medical models during the distillation process, aiming to bridge the gap between current performance and other SOTA.\n\n\n[3] Grounding Dino 1.5: Advance the\" edge\" of open-set object detection."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dLLEAQ65EE", "forum": "i7YnUW0uWg", "replyto": "i7YnUW0uWg", "signatures": ["ICLR.cc/2026/Conference/Submission20887/Reviewer_Waas"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20887/Reviewer_Waas"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952341723, "cdate": 1761952341723, "tmdate": 1762937597118, "mdate": 1762937597118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}