{"id": "OZlnpGVQaj", "number": 2394, "cdate": 1757071744378, "mdate": 1759898151769, "content": {"title": "Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism", "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide range of general language tasks but remain constrained in specialized domains. Inspired by the brain's cognition, specialized memory mechanism can be used to enhance the model's ability on specialized tasks. Specialized Generalist Models (SGMs) aim to preserve broad capabilities while achieving expert-level performance in target domains via test-time task identification and reconfiguration. However, traditional LLM structures including Transformer, Linear Attention, and hybrid models do not employ specialized memory mechanism guided by task information. In this paper, we present Nirvana, an SGM with specialized memory mechanism, linear time complexity, and test-time task information extraction. \nBesides, we propose the Task-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory mechanism based on the current task's requirements. In Trigger, each incoming sample is treated as a self-supervised fine-tuning task, enabling Nirvana to adapt its task-related parameters on the fly to domain shifts. We also design the Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes the context guided by Trigger. We conduct experiments on both general language tasks and specialized medical tasks. \nOn a variety of natural language modeling benchmarks, Nirvana achieves competitive or superior results compared to the existing LLM structures. To prove the effectiveness of Trigger on specialized tasks, we test Nirvana's performance on a challenging medical task, i.e., Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with lightweight codecs on paired electromagnetic signals and MRI images. Despite the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI domain with the change of task-related parameters. Nirvana achieves higher-quality MRI reconstruction compared to conventional MRI models as well as the models with traditional LLMs' backbone, and can also generate accurate preliminary clinical reports accordingly.", "tldr": "We present Nirvana, a Specialized Generalist Model with task-aware memory mechanism, linear time complexity, and test-time task information extraction.", "keywords": ["Specialized Generalist Models", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8dd290eff6c96331873e25873740772e91f6a24.pdf", "supplementary_material": "/attachment/766ff0dadb07e43ac05dc99976c9b9a6aa886c2d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Nirvana, a Specialized Generalist Model (SGM) designed to balance the trade-off between broad generalization and task-specific specialization in large language models (LLMs). The key idea is the integration of a Task-Aware Memory mechanism, enabling the model to dynamically reconfigure its internal pathways and memory usage at test time based on task characteristics. Experiments on both language understanding benchmarks (e.g., LAMBADA, BoolQ, PIQA) and medical imaging tasks (MRI reconstruction) demonstrate that Nirvana consistently outperforms baselines, achieving a strong blend of generalization and specialization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel architecture, Nirvana, specifically designed to address the challenge of creating Specialized Generalist Models (SGMs). The two-core innovation—the Task-Aware Memory Trigger (Trigger) and the Specialized Memory Updater (Updater)—is well-motivated.\n\n2. Beyond text-based tasks, Nirvana successfully performs end-to-end signal-to-report MRI reconstruction, showing strong potential for multimodal generalization and domain transfer."}, "weaknesses": {"value": "1.\tLack of Analysis for CL-OGD: The convergence and stability of the update mechanism are not discussed, and the rationale behind using the key (k) to fit the value (v) remains underexplained compared to similar DeltaNet-style updates.\n2.\tInference Overhead Unquantified: The Trigger module introduces extra forward and backward computations (for CL-OGD), which may increase inference latency; however, no quantitative runtime or computational cost analysis is provided.\n3.    Comparison to Stronger Baselines in the MRI Task: The MRI baselines (E2E-VarNet, UDNO) are well-established, but it would be more compelling to see a comparison against other foundation models (e.g., a fine-tuned Mamba or Gemini) that have been adapted for this task using a similar encoder-decoder setup. This would more directly isolate the benefit of the Nirvana architecture versus simply using a powerful sequential model as a backbone.\n4.\tLimited Task Generalization: Experiments focus primarily on the MRI domain. Further evaluation on other specialized domains would be needed to demonstrate broader applicability.\n5.\tInsufficient Mechanistic Justification: The paper lacks deeper theoretical or empirical analysis of why the Trigger effectively extracts task information and how the Updater’s interpolation coefficients adapt to task signals. Stronger ablations or interpretability studies would reinforce these claims."}, "questions": {"value": "please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sYo7Q5s7sx", "forum": "OZlnpGVQaj", "replyto": "OZlnpGVQaj", "signatures": ["ICLR.cc/2026/Conference/Submission2394/Reviewer_g8oE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2394/Reviewer_g8oE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935903387, "cdate": 1761935903387, "tmdate": 1762916218184, "mdate": 1762916218184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to create a Specialized Generalist Model (SGM) that can dynamically adapt its reasoning and memory based on the task context—achieving expert-like performance without retraining the backbone model. This responds to the limitation of existing LLMs, which either overfit to specific tasks or fail to specialize at inference time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The design of introduce low-dim fast weight parameters is aligned with the on-the-fly scenario.\n2. The CL-OGD online gradient descent is quiet interesting technique. And it can be shown mathematically that update the self-supervised loss equals to update the fast weight parameters P.\n3. The reviewer also design a mix strategy of Linear Attention for long-context global information and local attention using Sliding Window Attention (SWA)."}, "weaknesses": {"value": "1. The major concerns I have is the g function that used for obtaining task specific weight matrix W_i from the memory bank W_bank. What is the exact implementation of the function g? Is it efficient and replicable?\n2. I feel there is a high risk of overfitting for the task embedding neural network. First it is a small network with only linear layers. Second, it only learned from the training data distribution for the task. What if the online data have quite different distribution from the trianing set?"}, "questions": {"value": "1. For the online gradient descent, how do you balance the shift cross layers?\n2. What is the function of g and how to trian it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BJrCbUpnIF", "forum": "OZlnpGVQaj", "replyto": "OZlnpGVQaj", "signatures": ["ICLR.cc/2026/Conference/Submission2394/Reviewer_qJsq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2394/Reviewer_qJsq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937682848, "cdate": 1761937682848, "tmdate": 1762916218017, "mdate": 1762916218017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Nirvana, a Specialized Generalist Model designed to bridge the gap between broad generalist reasoning and domain-specialized expertise. Motivated by cognitive theories of task-specific memory, Nirvana integrates a Task-Aware Memory Trigger (Trigger) and a Specialized Memory Updater to dynamically adjust its internal memory mechanisms at test time. The Trigger treats each incoming sample as a self-supervised fine-tuning task, enabling rapid adaptation to domain shifts, while the Updater interpolates between Sliding Window Attention and Linear Attention to balance local and global context modeling efficiently. Experiments across language modeling, long-context retrieval, and medical imaging tasks demonstrate the effectiveness of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear framework that unifies generalist and specialist modeling through task-aware memory modulation. The introduction of the Task-Aware Memory Trigger and Specialized Memory Updater allows the model to adaptively reconfigure its memory and attention mechanisms without retraining.\n\n2. The experiments show the framework's ability to transfer from text reasoning to MRI reconstruction, demonstrating cross-domain generalization.\n\n3. The paper uses online gradient descent to balance between efficiency and adaptability."}, "weaknesses": {"value": "1. While the paper proposes the Task-Aware Memory Trigger and Specialized Memory Updater, the novelty and necessity of these components are not sufficiently justified. The motivation for introducing two mechanisms, rather than a unified adaptive memory module, is unclear. Moreover, the paper does not offer an ablation showing how each component contributes to the model’s gains.\n\n2. Although Nirvana achieves improvements over baselines, the magnitude of gains is relatively modest, particularly on general NLP benchmarks. In several tables, the improvements fall within expected variance ranges and are not accompanied by statistical significance testing. Given the additional architectural complexity introduced by the Trigger and Updater, it remains unclear whether the trade-off between complexity and performance is justified. The results, as presented, suggest incremental rather than transformative gains.\n\n3. The work is framed as “brain-inspired” through analogies to cognitive memory mechanisms, but the link between cognitive motivation and engineering design is mostly unclear. The paper lacks neuroscientific grounding or analysis showing that the architecture reflects properties of human or biological memory systems."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ca4JfhbTAQ", "forum": "OZlnpGVQaj", "replyto": "OZlnpGVQaj", "signatures": ["ICLR.cc/2026/Conference/Submission2394/Reviewer_HE73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2394/Reviewer_HE73"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968211084, "cdate": 1761968211084, "tmdate": 1762916217665, "mdate": 1762916217665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Nirvana, a novel Specialized Generalist Model (SGM) that integrates a Task-Aware Memory Trigger and a Specialized Memory Updater to dynamically adapt its memory mechanism at test time. The model is trained from scratch with 1.3B parameters and evaluated on both general language modeling tasks and a specialized medical task—MRI reconstruction. The key innovation lies in treating each input as a self-supervised fine-tuning task, enabling on-the-fly adaptation without retraining the backbone. Nirvana demonstrates competitive performance on standard NLP benchmarks and superior results in MRI image reconstruction and report generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Novel Architecture: The combination of Trigger and Updater introduces a flexible, task-aware memory mechanism that is both conceptually interesting and practically relevant.\n- Test-Time Adaptation: The model’s ability to adapt without retraining the backbone is a strong contribution, especially for domain-shift scenarios.\n- Specialized Task Application: The MRI reconstruction experiment is well-executed and shows clear improvements over traditional models (E2E-VarNet, UDNO), including image quality and diagnostic report generation.\n- Generalist Capability: Nirvana performs competitively on general NLP tasks, supporting its claim as a generalist model.\n- Comprehensive Related Work: The paper provides an excellent survey and comparison of existing memory mechanisms in LLMs."}, "weaknesses": {"value": "- Limited Specialized Domain Coverage: Despite the SGM claim, the only specialized domain evaluated is MRI. Broader domain validation (e.g., legal, financial, biomedical QA) is missing.\n- No Comparison with Existing LLMs in Specialized Tasks: The MRI experiments do not include comparisons with publicly available LLMs (e.g., LLaMA, Mistral) fine-tuned on medical data, which would better contextualize Nirvana’s impact.\n- From-Scratch Training: While academically interesting, training a 1.3B model from scratch limits reproducibility and practical relevance. Fine-tuning existing models would be more realistic and impactful."}, "questions": {"value": "1. Can you provide results comparing Nirvana to fine-tuned public LLMs (e.g., LLaMA-2 or Mistral) on the MRI task?\n2. Do you plan to evaluate Nirvana on other specialized domains (e.g., legal reasoning, biomedical QA) to support the SGM claim?\n3. How sensitive is Nirvana’s performance to the quality or diversity of the instruction prompts used in MRI report generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OKYrSjHw9H", "forum": "OZlnpGVQaj", "replyto": "OZlnpGVQaj", "signatures": ["ICLR.cc/2026/Conference/Submission2394/Reviewer_1UHY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2394/Reviewer_1UHY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762694503869, "cdate": 1762694503869, "tmdate": 1762916216426, "mdate": 1762916216426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}