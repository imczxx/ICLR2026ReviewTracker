{"id": "rJcMv7q1jH", "number": 11951, "cdate": 1758204863115, "mdate": 1759897542998, "content": {"title": "Learning Communication between Language Models through Dense Vectors", "abstract": "Communication between language models plays a crucial role in the inference process of large language models (LLMs), occurring both iteratively within a single model for multi-step reasoning (auto-regression) and interactively across multiple models for collaborative intelligence. While current systems primarily facilitate such communication through natural language, this paper proposes a novel paradigm of using continuous dense vector in continuous space. Our approach eliminates the unnecessary embedding and de-embedding steps when LLM interact with another, enabling more efficient information transfer, fully differentiable optimization path, and exploration of capabilities beyond human heuristics. We place such stripped LLMs as vertexes and optimizable seq2seq modules as edges to construct LMNet, a directed graph with similar structure as MLPs, and performs end-to-end gradient-descent for efficient optimization. As two exemplar applications, we show the proposed method can effectively improve LLM's general intelligence, and customizing LLM with limited data. We also provide detailed discussion and analysis about learning communication through dense vectors.", "tldr": "", "keywords": ["Large language models", "Neural Architecture", "Multi-model collaboration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91471f747882e73cab9321cb4605784f7aa1cba9.pdf", "supplementary_material": "/attachment/ed9dc8177206ff74f4a2004e6ed81a9e6a39e247.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes LMNet, a graph-style architecture where multiple pre-trained LLMs are stripped of their embedding/de-embedding layers to form vertex transformers, which communicate via small trainable edge seq2seq modules carrying dense vector messages. The authors instantiate LMNet with shared vertex weights and end-to-end autoregressive training, then evaluate two settings: (i) “general intelligence” improvements using a 1.1B-parameter LMNet built from Qwen2.5-0.5B, trained on public data, and (ii) data-limited customization where edges are trained and compared to PEFT baselines like LoRA on MMLU, GSM8K, and E2E. Results show sizable gains over Prompt/SFT and competitive performance versus similarly sized monolithic LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Recasts inter-model communication as learned dense messaging rather than natural-language tokens, enabling end-to-end optimization across models and edges; the layer-wise fully connected topology + edge translators is interesting. The overall idea is conceptually novel. \n- Method is well specified: vertex/edge definitions, aggregation by sum, and a training recipe that first optimizes edges, then all parameters. \n- Evaluated on diverse benchmarks. \n- Provided case studies for de-embed intermediate states to probe what’s carried on the “wires”."}, "weaknesses": {"value": "- One set of experiments I believe is missing is that the performance comparison between different width and depth of the LMNet, the results will be more convincing if there is plot showing Num of vertexes v.s. Performance, and showing that the performance positively scales with the vertex network size. \n- One stated motivation (replace inefficient NL messages in multi-agent systems) doesn’t really match the implemented setup (single final decoder; interior modules pass only the prompt sequence). What they’ve actually built/benchmarked is much closer to stacked, cross-connected transformer blocks that exchange dense features before any token is produced, not agents sending complete messages to one another."}, "questions": {"value": "- Typo: line 172 missing a space between ‘single’ and ‘X’.\n- In Line 217-219: the author mention an alternative where each vertex could auto regressively generate multiple token embedding sequence, by didn’t specified how. In the normal LLMs, such decoding is controlled by the EOS token so the LLM knows when to stop the autoregressive process, I am curious how to do that without the decoding of EOS in the intermediate layers? \n- A fair comparison shouldn’t only be about training compute; it should also hold test-time compute fixed. LMNet’s per-token inference effectively runs the vertex transformer N (i.e., number of vertexes in the net) times, so a base model given an equal test-time budget could use test-time scaling tricks to spend similar compute and might close some of the gap. The paper acknowledges LMNet increases inference latency per token roughly with layer depth L (sequential) even if same-layer vertices parallelize, but it doesn’t benchmark compute-matched inference baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O2Zya6uNOt", "forum": "rJcMv7q1jH", "replyto": "rJcMv7q1jH", "signatures": ["ICLR.cc/2026/Conference/Submission11951/Reviewer_2ZRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11951/Reviewer_2ZRn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756439827, "cdate": 1761756439827, "tmdate": 1762922953571, "mdate": 1762922953571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their valuable feedback.\nWe are glad to see that all reviewer are generally consistent with acknowledging the novelty of the proposed method and conceptual contribution, and the main concerns about experiments.\n\n## Experiment with larger scale.\n__1.For the setting general intelligence (Sec 4.1):__\n\nwe fully agree, and have expected, a main concern is lacking experiments with larger scale, like using larger and diverse LLMs as vertex, or ablation the width and depth of LMNet. However, this is far beyond our capacity. Note that training existing LMNet-1.1B already requires about 80 GPUdays. And each single experiment above would require training a larger-scale LMNet from scratch(edge). This is only feasible with industrial scale resources. This will take years for us which is impossible to address through rebuttal or any later manuscript in our academic capacity. We hope the reviewers could understand such fact. It would be a pity that this paper could not be able to shown to wider audience through this conference due to such reason.\n\n__2.For he setting customizing LLM with limited data (Sec 4.2):__\n\nwe have had experiments on various models in Table 2 and 3. We add the following results using 3B and 7B models follwomg the same setting in 4.2.1, which shows consistent advantage:\nMMLU ($\\Delta$ ACC %):|  Qwen2.5-3B| Qwen2.5-7B|GSM8K (ACC %):|  Qwen2.5-3B| Qwen2.5-7B|\n-|-|-|-|-|-\nPrompt|43.6|52.1|\\|\\||84.2|90.4\nSFT(w/ CoT)|43.8|49.5|\\|\\||86.0|90.5\nLMNet|__45.2__|__53.8__|\\|\\||__87.7__|__92.3__\n\n\nFor ablation on depth/width of LMNet architecture, due to computation budget, we turn to using GPT2-M on E2E dataset (the same as 4.2.2), no significant difference or patterns can be observed in this range:\nArchitecture|  # Training Parameters (edge)| BLEU|NIST|MET|ROUGR-L|CIDEr\n-|-|-|-|-|-|-\nLMNet-1/1|21.00M|69.1|8.76|__46.6__|70.6|2.43\nLMNet-1/2/1(reported)|36.75M|__70.5__|__8.85__|46.5|71.5|2.48\nLMNet-1/2/2/1|57.75M|69.8|8.80|46.0|71.1|__2.49__\nLMNet-1/4/1|57.75M|69.1|8.70|45.9|__71.6__|2.44\nLMNet-1/4/4/1|141.75M|68.7|8.79|46.5|71.2|2.48\n\n## Fair comparison with base model.\nMore than one reviewers have mentioned that in our experiment in Table 1, comparing the left part: 0.5B actually become 1.1B and so this intuitively feels like the performance gain is almost guaranteed; while comparing the right part: LMNet-1.1B has no obvious advantage.\nWe want to emphasize that the additional (1.1-0.5)B parameters in LMNet is introduced with is far less cost than the similar scale parameters inside LLM.\nAs reported in lin 288 and Appendix B, while training a LLM from scratch takes a scale at 10^1 T tokens training, the we only takes a scale at 10^(-2) T tokens to train LMNet to reach such performance.\nSo we want to show that LMNet can significantly improve vertex model's general performance by training additional parameters, to comparable with similar scale(parameter-in-total) monolithic LLM, while the total training cost is far less.\n\nA very rational comparison we want to introduce here is comparing LMNet with TTS using the same base model, as they both increases the inference-time computation. We add experiments comparing LMNet-1.1B with representative parallel TTS strategy Self-Consistency and sequential TTS strategy Self-Refine. As LMNet-1.1B contains 1+4+4+4+1=14 repeats of Qwen2.5-0.5B and additional 0.7B edge modules (no repeat), we compare with above TTS with a budget of 16 trials/iterations using Qwen2.5-0.5B.\nThe following table shows the results, where LMNet significantly outperform the other. Of course, LMNet requires additional training comparing with the others, but the cost is near zero comparing with the cost of training a monolithic LLM. Note that while there are also existing TTS with additional training using RL or training an additional verifier, they are specific to certain benchmark using corresponding curated training dataset. In fact, one of the motivations of LMNet is exactly it can learn to express TTS-like process through the MLP-like  network structure, through training the higher-level network with general data.\n\nMethod/Benchmark |  MMLU| MMLU-pro|BBH|GSM8K|MATH\n-|-|-|-|-|-\nBase(1)|44.3|15.7|20.3|41.6|19.5\nSelf-Consistency(16)|46.0|17.1|21.6|43.5|19.8\nSelf-Refine(16)|45.2|16.4|24.7|42.1|19.0\nLMNet-1.1B(1/4/4/4/1)  | __53.9__ |__26.2__|__47.3__|__50.3__|__38.8__"}}, "id": "beIV4wAmyl", "forum": "rJcMv7q1jH", "replyto": "rJcMv7q1jH", "signatures": ["ICLR.cc/2026/Conference/Submission11951/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11951/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11951/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763603915059, "cdate": 1763603915059, "tmdate": 1763603915059, "mdate": 1763603915059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LMNet, a new method for learning communication between LLMs besides exchanging natural language token. The key idea is to strip out the embedding and de-embedding layer and treat those as vertices, and connect them with trainable seq2seq edge modules, forming a fully connected directed graph that is differentiable e2e. \n\nThe authors demonstrates two applications, showing increase in both general intelligence/reasoning task (MMLU, GSM8K etc.) and they also showed that LMNet can be used to train with customized, small scale data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reimagining LLM communication: the paper makes a very clean observation that current multi-LLM systems still talk in discrete natural language, even though the models internally think in dense vectors. That forces every intermediate module to do an unnecessary (and non-differentiable) de-embed → embed step, which is bad both for information efficiency and for gradient flow. The proposed fix is easy to understand. It also clearly distinguishes this work from latent CoT papers, which stay inside a single model’s latent space, whereas this paper explicitly targets inter-model communication.\n\n2. Nice empirical signal: The cost is well-justified and the performance gain on benchmarks like MMLU and GSM8K. This justify for the better information flow that LMNet is designed for."}, "weaknesses": {"value": "1. Limited ablation: The design of stacking a fully-connected seq2seq module is not studied in depth and seems a bit arbitrary. It remain unclear what the source of gain is. The authors should conduct ablation studies on different topology and putting different capacity of edge modules. Could you show that the design of the specific components of the architecture is actually useful? Like replacing seq2seq module with pure MLP/using a sparse topology.\n\n2. Parameter size as confound and scalability issue: In the main experiment the author points out that the LMNet variant of Qwen-0.5B actually become 1.1B and so this intuitively feels like the performance gain is almost guaranteed. Although the authors compare to models with similar size, it isn't an apple-to-apple comparison with non-Qwen models as for many benchmarks the base Qwen-2.5-0.5B is already better, and versus Qwen2.5-1.5B it's still quite lagging on MMLU and GSM8K. Also, maybe I'm understanding this in a wrong way but does that mean for larger models (say 70B), the LMNet variant would become even bigger?\n\n3. Communication not studied: The main experiment used the same model on the vertices but communication between LLMs are usually with different sharer and receiver. I'm wondering if this architecture generalizes to different models on vertices."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kHuJ31gwkL", "forum": "rJcMv7q1jH", "replyto": "rJcMv7q1jH", "signatures": ["ICLR.cc/2026/Conference/Submission11951/Reviewer_KV29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11951/Reviewer_KV29"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849883795, "cdate": 1761849883795, "tmdate": 1762922953031, "mdate": 1762922953031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LMNet, a method for connecting multiple language models through continuous dense vectors instead of discrete tokens, treating LLMs as vertices in a differentiable graph with trainable edge modules for communication. The key innovation is removing embedding/de-embedding layers between consecutive LLMs, allowing hidden states to flow directly between models through trainable \"edge\" modules. The authors construct a directed graph architecture where vertices are stripped transformers (without embedding layers) and edges are small seq2seq modules (typically single attention blocks). To reduce parameters, they employ parameter sharing where all vertices use the same pre-trained LLM.\n\nThe paper demonstrates two applications: (1) improving general intelligence by training a 1.1B parameter LMNet based on Qwen2.5-0.5B vertices, achieving ~40% relative performance gains with <0.2% additional training cost as claimed in the paper, and (2) data-efficient domain adaptation, where LMNet outperforms fine-tuning and latent reasoning methods on MMLU, GSM8K, and E2E benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The research question in this paper is interesting. The framing of computational graphs is novel and enables end-to-end gradient-based optimization.\n- The paper demonstrates improvments on general tasks, math reasoning, knoweldge benchmarks and domain adaptation."}, "weaknesses": {"value": "- LMNet is compared against its base model, making it unsurprising to gain performance improvement. Would be better if a model with comparable size is compared. Llama3.1-1B is provide in the paper, but its performance is even worse than the base model which should be problematic.\n- The 5-layer architecture also introduce signifcant lower inference compared to a single pass.\n- Performance on some benchmarks such as ARC-C, GPQA shows minimal gains or even degradation compared to Qwen2.5-1.5B.\n- Passing hidden states between models is not new considering latent reasoning. But it is a bit overstated by claiming as \"beyond human constraints\". It's important to provide concrete evidences that dense vector communication can beat prompt-based approaches.\n- The performance on communications between two different models are unknown especially these with different hidden dimensions.\n- What happens at larger models such as 7B models? Does parameter sharing still work?"}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6YAfDxgQ2J", "forum": "rJcMv7q1jH", "replyto": "rJcMv7q1jH", "signatures": ["ICLR.cc/2026/Conference/Submission11951/Reviewer_VPwU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11951/Reviewer_VPwU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884219103, "cdate": 1761884219103, "tmdate": 1762922952572, "mdate": 1762922952572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new paradigm for communication between language models by directly exchanging dense continuous vectors instead of natural-language tokens. The authors construct a directed graph, LMNet, where each vertex is a language model and each edge is a trainable seq2seq mapping that learns how to translate hidden representations between models. The entire structure is optimized end-to-end via gradient descent. The paper argues that this dense communication removes redundant embedding/de-embedding steps and enables differentiable multi-model cooperation. Two illustrative applications are presented: (1) enhancing general reasoning ability and (2) customizing models with limited data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The idea of representing inter-model communication as differentiable dense vectors is conceptually novel and could inspire new research on multi-agent LLM systems.\n\nThe proposed LMNet graph abstraction provides a potentially unifying framework for studying information flow between models.\n\nThe paper touches on an interesting question of whether model interactions must occur through natural language at all, which is intellectually provocative."}, "weaknesses": {"value": "The motivation for removing the token layer is unconvincing. Tokenization does not necessarily cause semantic loss, while replacing discrete tokens with dense vectors may introduce additional noise, instability, and loss of interpretability.\n\nNo clear empirical evidence demonstrates that dense communication improves performance, efficiency, or convergence compared with existing approaches (e.g., natural-language interaction, hidden-state distillation, or adapter-based transfer).\n\nExperiments are limited to small-scale toy settings without strong baselines or ablation studies, making it difficult to assess generality or practical benefit.\n\nThe paper lacks theoretical analysis of communication capacity, robustness, or scalability.\n\nOverall, the work feels more like a conceptual proposal than a rigorously validated method."}, "questions": {"value": "Can the authors provide quantitative comparisons showing improvements in reasoning accuracy, efficiency, or resource usage versus token-based communication?\n\nHow stable and interpretable are the learned dense communication vectors across tasks or model sizes?\n\nHave the authors analyzed whether the introduced dense mapping modules amplify noise or reduce robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RzUFXAAaBY", "forum": "rJcMv7q1jH", "replyto": "rJcMv7q1jH", "signatures": ["ICLR.cc/2026/Conference/Submission11951/Reviewer_yHkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11951/Reviewer_yHkZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968534420, "cdate": 1761968534420, "tmdate": 1762922951691, "mdate": 1762922951691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}