{"id": "9OZNqYwrbm", "number": 13020, "cdate": 1758212759257, "mdate": 1759897470371, "content": {"title": "Importance Sampling Optimization Improves Online Preference Learning", "abstract": "Training large language models (LLMs) with online sampled data can help off-policy preference optimization approaches like DPO learn better. Recent methods such as Statistical Rejection Sampling Optimization (RSO) have emerged as attractive alternatives to online Reinforcement Learning from Human Feedback (RLHF), offering improvements in stability and scalability. Although RSO has shown promising results by using rejection sampling to obtain preference data from the estimated optimal target policy, it faces computational inefficiencies due to the high rejection rates inherent in its sampling process. To address these limitations, we introduce **Importance Sampling Optimization** (ISO), a novel approach that achieves the benefits of sampling from the optimal policy distribution while significantly improving sample efficiency.  ISO employs importance sampling to correct the distribution mismatch between the supervised fine-tuned (SFT) policy and the target optimal policy, enabling efficient use of all generated samples without rejection. Through extensive experiments across diverse tasks and models, we demonstrate that ISO achieves comparable or superior performance to RSO while requiring substantially fewer samples from the SFT policy. Reduces sampling overhead by up to 75\\% while maintaining or improving win rates against both DPO and RSO baselines. Additionally, we show that ISO naturally extends to other preference optimization methods, providing a general framework for improving sample efficiency in preference learning.", "tldr": "", "keywords": ["LLM", "Preference Learning", "Importance Sampling", "Online RLHF"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab199b0886e80e41b7032e26dd28c61217ff6aae.pdf", "supplementary_material": "/attachment/c02f891e0f5d4bb0f8de65879c95e6b2489fa2b2.zip"}, "replies": [{"content": {"summary": {"value": "The paper develops the importance sampling optimization (ISO) approach, to correct the mismatch between the SFT policy and the target optimal policy. ISO finds a way to use all generated samples without rejection, thus differs from the existing RSO (rejection sampling optimization) and DPO approaches, and strikes a balance between sampling efficiency and improved performance in preference learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Importance sampling is a well-established technique in stochastic optimization, including RL. Thus, ISO is built upon a solid theoretical foundation. The paper is well motivated (starting from Fig 1) and clearly written. The key is the pairwise importance weight in (7), modulated by the signed margin score in (8), which then (after normalization) goes into the loss function in (9), and then integrated into the preference learning pipeline. All these are clearly and logically spelled out in \\S 3."}, "weaknesses": {"value": "Cannot help feeling the paper's contribution falls a bit thin on technical novelty, given the well established status of importance sampling."}, "questions": {"value": "At the end of \\S4, there’s some description of the effect of \\gamma in ISO, via (7). Wonder what’s choice of \\beta in (10) in this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vmLrf4eNTF", "forum": "9OZNqYwrbm", "replyto": "9OZNqYwrbm", "signatures": ["ICLR.cc/2026/Conference/Submission13020/Reviewer_J7oq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13020/Reviewer_J7oq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484218959, "cdate": 1761484218959, "tmdate": 1762923759250, "mdate": 1762923759250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the sample inefficiency of online preference learning methods like Statistical Rejection Sampling Optimization (RSO), which uses costly rejection sampling to align the sampling distribution ($\\pi_\\theta$) with the target optimal policy ($\\pi^*$). The authors propose Importance Sampling Optimization (ISO), replacing rejection sampling with importance sampling to correct the distribution mismatch. ISO computes reward-based importance weights, allowing a DPO-style loss to utilize all generated samples efficiently. A heuristic \"signed margin score\" is added to potentially upweight informative pairs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper leverages a importance sampling to solve the distribution mismatch problem and tackles the significant sample inefficiency and computational cost associated with RSO.\n2. This paper demonstrates substantial reductions in the required number of sampled responses compared to RSO while maintaining or improving alignment performance. This is a major practical advantage.\n3. Experiments are conducted across multiple model families, sizes, and standard alignment datasets, lending credibility to the results. The use of both proxy and independent golden reward models for evaluation adds robustness."}, "weaknesses": {"value": "1. There is a critical misalignment between the theoretical setup and the algorithm's implementation regarding the proposal distribution for importance sampling. The loss function (Eq. 10) takes an expectation over samples drawn from $\\pi_{sft}$, suggesting the importance weight $w(x, y_w, y_l)$ should correct for the ratio $\\pi^*(y|x)/\\pi_{sft}(y|x)$ (as derived in Appendix A.1). However, Algorithm 1 samples responses from the current policy $\\pi_{\\theta_t}$ (line 4). Applying weights derived assuming $\\pi_{sft}$ to samples drawn from $\\pi_{\\theta_t}$ is incorrect and lacks clear justification. Furthermore, the notation $\\mathbb{E}_ {(x,y_w,y_l)\\sim \\pi_{sft}}$ is imprecise, as $\\pi_{sft}$ is a conditional distribution over $y$.\n2. While positioned as an improvement for DPO-style methods (which are attractive for avoiding explicit reward models), ISO critically relies on an external, pre-trained reward model $r_\\phi$ (Algorithm 1, line 5, Eq. 8) to compute the importance weights. This seems counter to the DPO philosophy and introduces a dependency not present in standard DPO. The paper does not clarify if this requires additional reward model training specific to the online setting. Using the DPO implicit reward $r(x,y) \\propto \\log (\\pi_{\\theta_t}(y|x)/\\pi_{ref}(y|x))$ instead would likely be a poor proxy for the optimal reward $r^\\star$ needed to estimate $\\pi^*$, undermining the theoretical basis of the importance weights.\n3. Importance sampling can suffer from high variance, particularly if the sampling distribution ($\\pi_\\theta$ or $\\pi_{sft}$) is very different from the target distribution ($\\pi^*$). While potentially better than RSO's rejection rate, the paper could discuss potential variance issues and how they are managed.\n4. The signed margin score $(\\sigma(r_w - r_l) - 0.5)$ is introduced somewhat heuristically to upweight informative pairs. While intuitive, a more formal justification or an ablation study isolating its specific impact on performance and variance would strengthen this component."}, "questions": {"value": "Please see the weaknesses part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfscLlegv0", "forum": "9OZNqYwrbm", "replyto": "9OZNqYwrbm", "signatures": ["ICLR.cc/2026/Conference/Submission13020/Reviewer_YRTE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13020/Reviewer_YRTE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559823699, "cdate": 1761559823699, "tmdate": 1762923758624, "mdate": 1762923758624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to weight the SLiC loss by the magnitude of the reward scores."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "I reviewed an earlier version of this paper where I raised several concerns. Unfortunately, it does not seem like any of them have been addressed. Thus, I am repeating them verbatim below:\n\n(-) I really did try my best here but I don't think there's any reasonable interpretation of what the authors are doing as importance sampling. There is literally no ratio of two distribution's probabilities -- they never divide out the SFT policy's probabilities. I spent some time trying to do mental gymnastics to justify the product of factors that are used as weights as legitimate importance weights in any sense and I couldn't get that math to work out either. At best, I can say they weighted a usually unweighted loss.\n\n(-) Off the top of my head, I think the most natural baseline here is https://arxiv.org/abs/2404.16767, which also essentially uses a weighted DPO-like loss. I would suggest including it in future experiments."}, "questions": {"value": "(1) Is there a way to prove your re-weighting scheme is an unbiased estimate of importance weights $w(x, y) = \\frac{\\pi^{\\star}(y|x)}{\\pi_{sft}(y|x)}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "czZlsMj6OF", "forum": "9OZNqYwrbm", "replyto": "9OZNqYwrbm", "signatures": ["ICLR.cc/2026/Conference/Submission13020/Reviewer_xEth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13020/Reviewer_xEth"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762301997759, "cdate": 1762301997759, "tmdate": 1762923758260, "mdate": 1762923758260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new variant of online DPO method where an importance ratio term is introduced such that the update is performed under the optimal policy's generation distribution. The paper performs experiments that the ISO outperforms online DPO or rejection sampling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper makes an interesting observation that, even though one can not sample from the optimal policy, one can still evaluate the optimal policy's density, thus enabling the importance ratio correction. \n\n2. According to the presented experiment results, ISO outperforms online DPO and iterative rejection sampling."}, "weaknesses": {"value": "1. The method requires the access to the ground truth reward. With a reliable reward, one can simply perform online RL instead of contrastive learning. \n\n2. It is unclear the benefit of the importance sampling as it increases the variance of the estimator. \n\n3. The experiments are only performed for 2 iterations. \n\n4. The presentation of the paper seems unpolished, for example, line 225 is unfinished, and eq 3 should not be $\\mathcal{L}_{\\mathrm{DPO}}$.\n\n5. The paper is confusing the optimal policy and the optimal KL regularized policy. In the importance ratio correction, the optimal KL regularized policy is used."}, "questions": {"value": "How important is the modulating factor? This ablation seems missing from the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TQFS3mhLA7", "forum": "9OZNqYwrbm", "replyto": "9OZNqYwrbm", "signatures": ["ICLR.cc/2026/Conference/Submission13020/Reviewer_zHkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13020/Reviewer_zHkZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762400874102, "cdate": 1762400874102, "tmdate": 1762923757844, "mdate": 1762923757844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## Comparison with Weighted DPO Baselines and Ablation Studies\n\n\nTo address reviewer questions regarding how ISO compares to other weighted loss formulations (specifically **REBEL**; arXiv:2404.16767) and to further validate the components of our method, we conducted additional experiments on the Ultrafeedback dataset.\n\nWe compare the following settings:\n1.  **Direct:** Standard DPO with uniform weighting on online samples.\n2.  **REBEL:** The regression-based weighted DPO loss.\n3.  **ISO w/o Signed Margin:** Our importance sampling method using only the base weights derived from the partition-function-free estimator (Eq. 19), without the signed margin modulation.\n4.  **ISO (Ours):** The full method including the signed margin score (Eq. 8).\n\nWe set the $\\gamma=0.5$ in the ISO experiment.\n\n**Table 1: Proxy Reward Win Rates (%) on Ultrafeedback**\n\n| Iteration | Model | Direct | REBEL | ISO w/o Signed Margin | ISO (Ours) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **1** | **Gemma2 2B** | 80.77 | 71.87 | 83.36 | **87.66** |\n| | **Qwen2.5 3B** | 79.74 | 69.38 | 79.03 | **82.10** |\n| **2** | **Gemma2 2B** | 89.47 | 82.43 | 91.38 | **94.34** |\n| | **Qwen2.5 3B** | 90.65 | 81.40 | 87.83 | **90.75** |\n\n**Analysis of Results:**\n* **ISO vs. REBEL:** ISO consistently outperforms REBEL in all settings (e.g., **+15.79%** for Gemma2 2B in Iteration 1). While REBEL focuses on regressing the reward margin, our results suggest that ISO's importance sampling approach is more effective at bridging the gap between the online exploration policy and the optimal target policy.\n* **Impact of Signed Margin Score:** Comparing \"ISO w/o Signed Margin\" to the full \"ISO,\" we observe that the signed margin score contributes consistently to performance (e.g., increasing win rate from 83.36% to 87.66% on Gemma2 2B). This confirms that down-weighting pairs with low reward margins helps the model focus on more informative preference pairs.\n* **Importance Sampling Baseline:** Even without the signed margin score, the core importance sampling mechanism (\"ISO w/o Signed Margin\") generally performs on par with or better than the Direct baseline, validating the effectiveness of our derived importance weights in correcting distribution mismatch."}}, "id": "LilmtD5ldt", "forum": "9OZNqYwrbm", "replyto": "9OZNqYwrbm", "signatures": ["ICLR.cc/2026/Conference/Submission13020/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13020/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission13020/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763690917207, "cdate": 1763690917207, "tmdate": 1763690917207, "mdate": 1763690917207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}