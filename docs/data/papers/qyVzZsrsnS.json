{"id": "qyVzZsrsnS", "number": 19474, "cdate": 1758296570904, "mdate": 1759897036990, "content": {"title": "Narrow Finetuning Leaves Clearly Readable Traces in the Activation Differences", "abstract": "Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for safety research. Model diffing--the study of differences between base and finetuned models--is a promising approach for understanding how finetuning modifies neural networks.\nIn this paper, we show that narrow finetuning creates easily readable biases in LLM activations that can be detected using simple model diffing tools, suggesting that the finetuning data is overrepresented in the model's activations.\nIn particular, analyzing activation differences between base and finetuned models on the first few tokens of random text  and steering with this difference allows us to recover the format and general content of the finetuning data. \nWe demonstrate that these analyses significantly enhance an LLM-based interpretability agent's ability to identify subtle finetuning objectives through interaction with base and finetuned models.\nOur analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters).\nOur work: (1) demonstrates that researchers should be aware that narrow finetuned models will represent their training data and objective very saliently, (2) warns AI safety and mechanistic interpretability researchers that these models might not be a realistic proxy for studying broader finetuning, despite current literature widely using them.\nWhile we show that mixing pretraining data into the finetuning corpus is enough to remove this bias, a deeper investigation is needed to understand the side effects of narrow finetuning and develop truly realistic case studies for model-diffing, safety and interpretability research.", "tldr": "Activation differences between base and narrowly finetuned models reveal the finetune’s objective, even on unrelated data, letting an interpretability agent reliably detect finetuning objectives.", "keywords": ["Mechanistic Interpretability", "Steering", "Automated interpretability", "Benchmarking interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7f51333104531e0aeaf19f318336be91c05f748.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ADL method which uses model diffing techniques to investigate traces left by narrow finetuning. The traces are visible even for unrelated inputs and the authors identify, reason and provide strategies to mitigate this effect."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Thorough experimental setup and extensive results\n* Novel methodology that effectively leverages existing techniques to identify traces  \n* The analysis provides very important inferences which will be beneficial for the community about safety in narrow fine-tuned models\n* It also provides suggestions on how to mitigate this effect"}, "weaknesses": {"value": "The causal effect is positive for Gemma3 which contradicts the hypothesis and warrants more investigation"}, "questions": {"value": "* The authors specify in line 161 that they focus on the middle layer. Could they expand on why?\n* Edit suggestion: Explicit mention of the method name (ADL) when introducing the approach in the abstract would be good"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RlXtD9YOnF", "forum": "qyVzZsrsnS", "replyto": "qyVzZsrsnS", "signatures": ["ICLR.cc/2026/Conference/Submission19474/Reviewer_Bkbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19474/Reviewer_Bkbd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754233456, "cdate": 1761754233456, "tmdate": 1762931384795, "mdate": 1762931384795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work uncovers biases in narrowly fine-tuning models' activation spaces and hypothesizes the origins of said biases, and provides a possible direction to mitigate such biases."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) I think the paper is written very well. \n2) I think the goal and contribution of work are very relevant to interpretability research as a whole. \n3) The experiments are sound and aid the central claims of the work. \n4) The finding is very interesting and well-grounded in current literature. Although the paper could use some more explanations of the methods utilized in the work, such as PathScope. I encourage the authors to add further explanations about this methodology in the final iterations of the work.\n5) Mixing pre-training data with the fine-tuning one is an interesting approach; I would like some more emphasis on this in the final iteration of the paper. \n6) Overall, the work aims to present and discuss key facts about fine-tuning for narrow use cases, particularly the data dependence of bias in fine-tuned models."}, "weaknesses": {"value": "The major weakness of note is that the bias mitigation method proposed lacks a study of the impact of injecting pre-training data into the fine-tuning object. Both in the sense of measuring the domain-specific capabilities impacted by the model, and the added cost of adding more data to the fine-tuning objective. Although the work does claim that adding pre-training data reduces fact alignment scores, a more rigorous analysis would be appreciated. \n\nAnother weakness, more so of a comment, to highlight would be that the methodology presented is just a collection of prior known/used methods, although this doesn't undermine the novelty of the findings; highlighting the derivative nature of the methodology would help elucidate the contributions of the work. \n\nEven though, as mentioned previously, i think the paper is written very well, continuous references to the appendix break the flow of the paper. I would encourage that in the final iteration of the work, to add these appendix section to the main paper or leave some as a footnote."}, "questions": {"value": "Would the results be consistent across multiple graders, it would be nice to know the amount of noise that could be mitigated if we use multiple graders."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LFLGL6YS9h", "forum": "qyVzZsrsnS", "replyto": "qyVzZsrsnS", "signatures": ["ICLR.cc/2026/Conference/Submission19474/Reviewer_jbUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19474/Reviewer_jbUP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858332394, "cdate": 1761858332394, "tmdate": 1762931384409, "mdate": 1762931384409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper finds that narrow finetuning of LLMs (a popular technique to study model behavior) creates detectable biases in activation differences between base and finetuned models, which can be used to identify the finetuning objective. The authors apply model diffing techniques (Patchscope, Logit Lens, steering) to analyze a broad range of \"model organisms\" (narrowly finetuned LLMs) and demonstrate that: (1) activation differences on early tokens of unrelated text encode finetuning domain information, (2) an interpretability agent with access to these diffing techniques that can identify finetuning objectives better than an agent with only black box access, and (3) the narrow / monosemantic nature of the data leads to these strong biases, so mixing pretraining data during finetuning mitigates biases through diversification, albeit also weakening the desired effects of narrow finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper finds a flaw in current AI safety practice (where narrow finetuning is used extensively), so the findings have practical significance in AI safety research (although the authors do not provide a compelling alternative/solution, as discussed below).\n2. The experimental scope is quite comprehensive, covering 33 organisms across 4 families and 7 models from 1B up to 32B parameter scales.\n3. The interpretability agent experiment provides an objective automated assessment of how useful / exploitable these activation differences are. These experiments also should be reproducible as the results are reportedly stable across runs.\n4. The authors go beyond pure observation of the phenomenon by conducting (well-designed) causal ablation experiments to trace biases back to the semantic homogeneity of the data."}, "weaknesses": {"value": "1. The scope of the paper is quite narrow (no pun intended) because narrow finetuning itself is primarily used as a safety / interpretability tool while in practice training data tends to be more diversified. The authors contend that in more realistic settings (such as domain-adapted vision-language models) the phenomenon is observed to a lesser extent.\n2. The paper relies on an interpretability agent based on gpt-5-mini to show that the studied activation biases can be detected easily with the right tools (ADL) while blackbox access to the model is insufficient. The effectiveness of both agents may be highly sensitive to prompt design and it is not clear that the authors tuned both agents equally well.\n3. In general, the paper relies heavily on LLM graders. The credibility of the claims would be strengthened by including some human judgments to spot-check whether the graders are calibrated well.\n4. The proposed mitigation strategy (mixing in pretraining data) creates a trade-off between strength of activation biases and desired effects of narrow finetuning, and the authors provide no clear guidance on how to best navigate this trade-off."}, "questions": {"value": "1.  What is the reason for using activations from the middle layer? This decision seems quite ad-hoc given that activations vary a lot across layers. Can you observe activation biases in lower or higher layers too?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9tNAZ2Ejvf", "forum": "qyVzZsrsnS", "replyto": "qyVzZsrsnS", "signatures": ["ICLR.cc/2026/Conference/Submission19474/Reviewer_zV3K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19474/Reviewer_zV3K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997305010, "cdate": 1761997305010, "tmdate": 1762931383838, "mdate": 1762931383838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a model diffing framework, the Activation Difference Lens (ADL), designed to extract clearly readable traces of LLMs' internalized objectives resulting from narrow finetuning. The core methodology involves measuring the differences between the activations (δ) of a base model and a narrowly finetuned model on the first few tokens of random pretraining corpus data. Their methodology is clearly explained, leveraging two key interpretability tools applied to the activation differences:  Logit Lens, Patchscope (modified), which reveals relevant tokens), and steering (which amplifies the model’s tendency to generate text highly similar to the finetuning data). The paper develops an LLM-based interpretability agent which uses these ADL results and LLM graders to form and verify hypotheses about the finetuning objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main contribution is the demonstration that these few-token activation differences encode salient traces of the training objective. The empirical study, spanning a wide range of model organisms and architectures, proves the ADL framework is highly effective compared to existing blackbox baselines using only simple prompting. Through causal ablation analysis and loss calculation, the paper makes a sound argument that the easily detectable biases highlighted by ADL are somewhat tied to overfitting to the semantically homogeneous finetuning data. Finally, through mitigation experiments, the authors proposed an effective method to avoid these strong biases by mixing unrelated pretraining data into the narrow finetuning datasets, which substantially reduces the detectable bias. The paper also highlights a warning to the interpretability and AI safety communities. These findings suggest finetuning signals may be artificially strong and overpower traces from standard broader finetuning. Therefore, using such model organisms as proxies for realistic scenarios may compromise their validity."}, "weaknesses": {"value": "In general, this study brings important insights to narrow finetune and model organism study. However, some caveats and additional experiments could make even sounder arguments. \n\n- The empirical study involves extensive usage of LLM grader. Although it is mentioned as a limitation, It would be nice to see some expansion on generalization of such a framework with other LLM graders other than gpt-5-mini.\n- The causal effect analysis is only done on three models, relatively small sized. Expanding the study to a wider range of models architecture and of model sizes will help make a stronger argument of overfitting. Further, the inconsistent result from Gemma makes the conclusion harder to believe. Experiments on the older Gemma model could help to further ground the hypothesis for its inconsistent behavior."}, "questions": {"value": "- In section 4.2, it is discussed that similar activation difference analysis was also done between the base model and the narrowly finetuned model, where bias detectable similar to that of chat model and finetuned model. Although the argument is sound, it seems the setup overlooks the potential impact of activation difference between base model and chat model. Can a similar analysis be done between the base model and the chat model? Would that help control for confounders?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fzhMbnLctG", "forum": "qyVzZsrsnS", "replyto": "qyVzZsrsnS", "signatures": ["ICLR.cc/2026/Conference/Submission19474/Reviewer_YvCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19474/Reviewer_YvCS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762313160877, "cdate": 1762313160877, "tmdate": 1762931383361, "mdate": 1762931383361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}