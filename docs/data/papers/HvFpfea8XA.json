{"id": "HvFpfea8XA", "number": 7010, "cdate": 1758004797009, "mdate": 1759897878005, "content": {"title": "Dynamic Context Adaptation for Consistent Role-Playing Agents with Retrieval-Augmented Generations", "abstract": "Recent advances in large language models (LLMs) have catalyzed research on role-playing agents (RPAs). However, the process of collecting character-specific utterances and continually updating model parameters to track rapidly changing persona attributes is resource-intensive. Although retrieval-augmented generation (RAG) can alleviate this problem, if a persona does not contain knowledge relevant to a given query, RAG-based RPAs are prone to hallucination, making it challenging to generate accurate responses. In this paper, we propose Amadeus, a training-free framework that can significantly enhance persona consistency even when responding to questions that lie beyond a character’s knowledge. Amadeus is composed of Adaptive Context-aware Text Splitter (ACTS), Guided Selection (GS), and Attribute Extractor (AE). To facilitate effective RAG-based role-playing, ACTS partitions each character’s persona into optimally sized, overlapping chunks and augments this representation with hierarchical contextual information. AE identifies a character's general attributes from the chunks retrieved by GS and uses these attributes as a final context to maintain robust persona consistency even when answering out-of-knowledge questions. To underpin the development and rigorous evaluation of RAG-based RPAs, we manually construct CharacterRAG, a role-playing dataset that consists of persona documents for 15 distinct fictional characters totaling 976K written characters, and 450 question–answer pairs. We find that our proposed method effectively models not only the knowledge possessed by characters, but also various attributes such as personality.", "tldr": "A persona-consistent, RAG-based role-playing agent that can handle questions outside its knowledge base.", "keywords": ["Large Language Model", "Role-playing Agents", "Retrieval-augmented Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08171b551094611fea8c325ae3ff324e3725f929.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AMADEUS, a training-free RAG framework for role-playing agents that emphasizes persona consistency. It consists of three modules: 1. ACTS (chunking with hierarchical context), 2. GS (LLM-guided gating for personality-rich context), and 3. AE (extraction of beliefs, values, and psychological traits). The authors introduce a new dataset, CharacterRAG, covering 15 anime/game characters with ~976K tokens and 450 QA items, and evaluate consistency using MBTI and BFI personality metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting Setup: Role-playing agents with persona tracking is a novel and popular topic.\n\n- Clear Modular Design: The ACTS → GS → AE pipeline is well-structured and has potential use in other structured attribute extraction tasks.\n\n- New Benchmark: The authors created a sizable benchmark dataset (CharacterRAG), which could contribute to future research in character-centric modeling."}, "weaknesses": {"value": "- Problem Framing – RAG for Personality?\nThe core task is framed around personality extraction and preservation, which is arguably not well-justified as a role for RAG. Personality is a stable trait, and might be better handled by LLM summarization over full context rather than partial retrieval.\n\n- Evaluation Scope Too Narrow\nThe evaluation overly focuses on MBTI/BFI-style tests. This does not show if the system performs well in downstream tasks (e.g., in-character QA, stylistic imitation). Also, no evidence is given that this personality focus doesn't degrade performance on other tasks.\n\n- MBTI as Ground Truth?\nThe paper does not clarify how ground-truth MBTI labels are obtained or validated. Are these canonical? Are there characters with ambiguous or contradictory traits across works?\n\n- Overloaded Character Set\nIncluding 15 characters is impressive but risks overwhelming the reader. It would help to narrow the benchmark or focus deeply on 1–2 characters (e.g., Hirasawa Yui) with full backstory and qualitative insights.\n\n- Lack of Ablation Analysis\nNo ablations are shown to measure the contributions of ACTS, GS, or AE. In particular, the GS module performs LLM-based reranking—this is a strong boost and may unfairly outperform pure retrieval baselines, confounding the comparisons."}, "questions": {"value": "Why did you choose RAG over LLM summarization for trait extraction, given that personality is a global/stable attribute rather than local/contextual?\n\nHow are MBTI ground truths assigned? Are they consensus-based? Are inter-annotator agreements or soft labels considered?\n\nHave you measured whether the AE module introduces hallucinated or speculative traits, especially when context is ambiguous?\n\nWhat downstream tasks (other than personality QA) can benefit from your pipeline? Could your approach generalize to style transfer or dynamic response generation?\n\nCould you show results using a smaller character subset with detailed breakdowns to improve interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "skbybRUOzb", "forum": "HvFpfea8XA", "replyto": "HvFpfea8XA", "signatures": ["ICLR.cc/2026/Conference/Submission7010/Reviewer_gUX3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7010/Reviewer_gUX3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761160548147, "cdate": 1761160548147, "tmdate": 1762919222876, "mdate": 1762919222876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates RAG-based role-playing agents, which aim to answer user queries using information retrieved from persona documents. The authors propose a new retrieval framework specifically designed for hierarchically structured persona documents, consisting of three key components: Adaptive Context-aware Text Splitter (ACTS), Guided Selection (GS), and an Attribute Extractor.\n\nThe ACTS module segments text into overlapping chunks while prepending contextual information from parent nodes within the hierarchical structure to each chunk. The GS component filters these chunks by prompting an LLM to verify whether the correct persona information can be inferred from each chunk. Attribute Extractor extracts psychological traits and value/belief-related attributes from the selected text segments.\n\nThe authors construct a self-collected dataset named CharacterRAG, containing 15 distinct characters and 450 question–answer pairs. Their experiments are conducted on this dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "i. This paper's attempt to improve the retrieval accuracy of persona information for RAG-based role-playing agents is meaningful. \n\nii. The collection of a new dataset demonstrates the authors’ effort to empirically explore this problem and provides a potential resource for future studies."}, "weaknesses": {"value": "i. The paper is poorly written, and many essential details are missing, which makes it difficult to fully understand and reproduce the proposed approach.\n\n- The description of the CharacterRAG dataset construction process lacks sufficient detail. It is unclear how the persona documents were collected and processed, how the 450 QA pairs were generated, and what standards were used to filter unqualified documents. Furthermore, the authors do not discuss any measures taken to ensure the fidelity and correctness of the dataset. Since all experiments rely entirely on this dataset, these details are critical to ensure the fairness and reliability of the results.\n\n- The method descriptions are confusing and incomplete. The paper claims that the Adaptive Context-aware Text Splitter (ACTS) divides text into optimally sized and overlapping chunks; however, Section 4.1 provides no information on how the optimal chunk length is determined or how the overlap is implemented. Similarly, in Section 4.2 (Guided Selection), the details of the LLM prompting process are missing — including how the prompt templates are constructed and how the model’s responses are evaluated or extracted. In Section 4.3 (Attribute Extractor), the authors only explain why and what attributes are extracted, but fail to specify how the extraction is actually performed.\n\n- According to Section 5.2, Table 4 appears to present the main experimental results. However, both the table and its corresponding analysis are placed later in Section 5.3, which disrupts the logical flow of the paper and makes the structure somewhat confusing.\n\n- In Figure 1, the authors do not explain how the chunk duplication frequencies and chunk usage rates are computed, which makes the figure difficult to interpret. In addition, the text in Figures 1 and 2 is too small to read clearly.\n\n- Figure 5 is mislabeled and should actually be Table 5.\n\nii. The proposed method appears to be highly tailored to the CharacterRAG dataset. The ACTS component relies on the dataset’s hierarchical structure, while the Attribute Extractor is built upon its predefined attribute taxonomy. As a result, the method may lack scalability and generalizability to other datasets or real-world scenarios. Moreover, comparing this approach with other methods on CharacterRAG may not provide a fair evaluation of its effectiveness.\n\niii. Guided Selection (GS) relies on LLM to filter chunks, computationally expensive and impractical for large-scale persona documents.\n\niv. In the main experiments presented in Table 4, despite the dataset-specific design efforts, the improvement over the baseline naive RAG model is marginal, which makes the overall contribution appear limited. Although the proposed method performs relatively well on tasks related to MBTI and BFI, the Guided Selection (GS) component explicitly extracts MBTI- and BFI-related attributes (with the implementation details remaining unclear). Therefore, it is unsurprising that the method outperforms the naive RAG under these conditions."}, "questions": {"value": "see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LlgxAu97Hi", "forum": "HvFpfea8XA", "replyto": "HvFpfea8XA", "signatures": ["ICLR.cc/2026/Conference/Submission7010/Reviewer_EE4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7010/Reviewer_EE4o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808794627, "cdate": 1761808794627, "tmdate": 1762919222517, "mdate": 1762919222517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles hallucination and weak persona consistency in RAG-based role-playing agents (RPAs) when handling queries outside a character’s knowledge, proposing AMADEUS, a training-free framework. AMADEUS includes three key modules: Adaptive Context-aware Text Splitter (ACTS) for hierarchical context-enhanced persona chunking, Guided Selection (GS) for attribute-relevant chunk retrieval, and Attribute Extractor (AE) for key character attribute extraction to sustain consistency. The authors also build CharacterRAG, a dataset with 15 fictional characters’ persona documents (976K characters) and 450 QA pairs, filling the gap of RAG-based RPA-specific evaluation resources. Experiments against baselines (Naive RAG, CRAG, LightRAG) on multiple LLMs demonstrate AMADEUS outperforms others in both in-knowledge and out-of-knowledge scenarios, lowering hallucination and boosting consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. AMADEUS (with ACTS, GS, AE modules) fixes RAG-based RPAs’ hallucinations and poor persona consistency in out-of-knowledge queries, outperforming traditional RAG by enhancing chunking, filtering, and attribute extraction.  \n\n2. The manually built CharacterRAG (15 characters, 976K chars, 450 QAs) removes interference (e.g., editor’s inferences) and fills the lack of dedicated RAG-based RPA evaluation resources.  \n\n3.  Using 3 LLMs, 3 embedding models, 3 baselines, and covering in/out-of-knowledge scenarios, experiments combine quantitative metrics (ACC, HS) and human evaluation (Cronbach’s α > 0.8) for credible results."}, "weaknesses": {"value": "1. The CharacterRAG dataset includes 15 fictional characters, but the paper does not specify their genre (e.g., anime, novel, film) or personality span (e.g., introverted vs. extroverted, heroic vs. villainous). If characters are concentrated in a single genre or share similar traits, the framework’s generalization to diverse role-playing scenarios (e.g., classical novel characters) remains unvalidated.  \n\n2. The Attribute Extractor (AE) only extracts \"Belief and Value\" and \"Psychological Traits,\" claiming they \"directly influence behavior\". But it does not explain why other attributes (e.g., \"Social Relationships\" or \"Skill and Expertise,\" which also shape role responses) are excluded, nor provide comparative experiments to prove these two attributes are more critical for persona consistency. Is it specific to tasks like MBTI and BFI?\n\n3. The evaluation dataset is incomplete. The paper only evaluates tasks like MBTI and BFI, but role-playing involves many other dimensions. Is the method applicable to other role-playing tasks? For example, imitating a character's linguistic style or simulating a character's behavior."}, "questions": {"value": "1. How is L_max set?\n2. Why can finding the information of the k most similar chunks solve problems that are beyond the scope of the character's knowledge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9zEOCFAmYP", "forum": "HvFpfea8XA", "replyto": "HvFpfea8XA", "signatures": ["ICLR.cc/2026/Conference/Submission7010/Reviewer_DAgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7010/Reviewer_DAgg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831123315, "cdate": 1761831123315, "tmdate": 1762919222218, "mdate": 1762919222218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free RAG framework for role-playing agents that targets persona consistency even when user queries fall outside a character’s explicit knowledge. The apporoach has three stages: (1) Adaptive Context-aware Text Splitter creates optimally sized, overlapping chunks annotated with hierarchical headings; (2) Guided Selection iteratively filters retrieved chunks using an LLM to prefer passages from which traits can be inferred; and (3) Attribute Extractor derives beliefs/values and Psychological Traits from the selected chunks and feeds them as final context for response generation. The authors also release CharacterRAG, a 15-character, 976K-char persona corpus with 450 QA pairs. Experiments across close-source or open-source models report higher accuracy and lower hallucination scores than RAG baselines, and  human raters find attributions reasonable"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper explicitly targets a common failure mode in RAG-based role-playing: when a user asks about aspects that are not explicitly in the persona, vanilla retrievers overuse low-relevance chunks and the agent hallucinates. The abstract and introduction motivate this crisply and position AMADEUS as training-free with three modules.\n\nACTS preserves hierarchical context with empirical support that maximizes summed similarity and minimizes variance; ACTS/ATS outperform standard splitters across embeddings. \n\nCharacterRAG contains 15 fictional characters (976k characters) and 450 QA pairs with six attribute categories, constructed from the character’s viewpoint (editorial/meta information removed). This fills a gap for RAG-based RPAs. AMADEUS improves stronger performances."}, "weaknesses": {"value": "CharacterRAG contains only 15 fictional characters, and much of the persona content is mined from Namuwiki; it remains unclear how well findings transfer to real people, evolving personas. Adding non-fictional or time-varying personas would strengthen claims.\n\nWhile ACTS’s hierarchical extraction cost is noted (O(N)), the end-to-end latency and token/dollar costs (especially for GS/AE with large models) are not reported in detail across LLMs/datasets, limiting deployment guidance.\n\nThe related work should cover several fast-moving threads which is missing: (i) LongRAG’s long-unit/long-reader paradigm that challenges short-chunk assumptions; also add RankRAG, which unifies reranking with generation and is conceptually close to your Guided Selection. (ii) Role-playing/persona literature: evaluation benchmarks (CharacterEval; RoleLLM; InCharacter and a non-RAG), Non-RAG persona models (Persona-Adaptive Attention). \n\nThe w/o-RAG baseline shows non-trivial background knowledge, but the paper does not deeply analyze how much persona knowledge the base LLMs already encode or how AE/GS ablations isolate gains beyond retrieval."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mzDEenaELB", "forum": "HvFpfea8XA", "replyto": "HvFpfea8XA", "signatures": ["ICLR.cc/2026/Conference/Submission7010/Reviewer_1rjH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7010/Reviewer_1rjH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972112265, "cdate": 1761972112265, "tmdate": 1762919221797, "mdate": 1762919221797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}