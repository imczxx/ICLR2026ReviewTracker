{"id": "HL3TvE4Afm", "number": 22393, "cdate": 1758330494055, "mdate": 1763766042680, "content": {"title": "Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding", "abstract": "We propose an always-feasible ``flexible'' quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated version called Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.", "tldr": "FlexQP is an always-feasible QP solver that we accelerate through deep unfolding to solve nonlinear optimizations quickly and robustly.", "keywords": ["Learning-to-Optimize", "Deep Unfolding", "Nonlinear Programming"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/511638b74765388e847805c594017fe0d2fc3af4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FlexQP, an ADMM-based $l_1$-penalizing formulation for quadratic programs; the authors claim it produces feasible iterates, recovers the original optimum when feasible, and otherwise minimizes constraint violations. They further unroll the solver into Deep FlexQP with LSTM-based parameter policies, present PAC-Bayes generalization bounds for the learned optimizer, and integrate it into SQP solver for nonlinear control and safety filtering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using a uniformed penalty formulation to treat both feasible and infeasible points within the same objective is novel as it yields a single ADMM-based procedure.\n2. Unfolding learns LSTM-based parameter policies while retaining the structure of the original solver, enabling accelerations to the original approach without discarding the algorithmic backbone.\n3. The author provides theoretical support, including convergence characterizations of the penalty/ADMM scheme and PAC-Bayes generalization bounds."}, "weaknesses": {"value": "1. The motivation behind and the advantages of using a $l_1$ penalty is not clear. The theory part claims properties of points that solve the  problem, but it does not directly establish a guarantee on whether Algorithm 1 and Deep-FlexQP can converge to those feasible/optimal solutions. A detailed explanation would be helpful.\n2. The significance of the reported acceleration is unclear. As noted, the dominant cost remains the first ADMM block update, and in some cases Deep FlexQP does not surpass Deep OSQP. Please add a detailed discussion on where the method is expected to help or not. Besides, it would be interesting to see how Deep FlexQP's predictions differ from those values of the original FlexQP."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lt1E7tllDe", "forum": "HL3TvE4Afm", "replyto": "HL3TvE4Afm", "signatures": ["ICLR.cc/2026/Conference/Submission22393/Reviewer_GTzC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22393/Reviewer_GTzC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761351232986, "cdate": 1761351232986, "tmdate": 1762942198424, "mdate": 1762942198424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a learning-based optimization method rooted in the Alternating Direction Method of Multipliers (ADMM) for solving convex quadratic programming problems. The proposed approach begins by introducing slack variables to transform inequality constraints into equality constraints, and subsequently incorporates all equality constraints into the objective function using an ℓ₁-norm penalty. A resulting ADMM-type solver—referred to as FlexQP—is then derived following an update scheme analogous to that of OSQP. To further accelerate convergence, this paper employs an LSTM network to generate all hyperparameters originally required by the algorithm, and train the model in a supervised learning framework. Experimental results demonstrate that the proposed method achieves a faster convergence rate in terms of optimality gap under a fixed iteration budget compared to several baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a novel learning-enhanced ADMM framework, supported by theoretical analysis and demonstrated to achieve faster convergence rates compared to established baselines across diverse datasets."}, "weaknesses": {"value": "1. The motivation for introducing slack variables and an ℓ₁-penalty term appears insufficiently justified. Since the ADMM-based solver OSQP can directly solve Problem (1), why not simply accelerate that algorithm using a neural network? Is the intention to use $z_I$ and $z_E$ to determine the feasibility of the original problem? However, in practice, $z_I$ and $z_E$ are unlikely to be exactly zero during iterations, as their values are strongly influenced by how well constraint (4b) can be satisfied. Moreover, if the original problem is infeasible, what is the practical value of providing a solution that only \"minimizes the constraint violations\"?\n2. While Figure 5 indicates superior convergence behavior of Deep FlexQP on all nine datasets, the actual computation time is missing. A clear description of how the runtime was measured should also be provided.\n3. The scale of the datasets used for solving the problems remains relatively limited. Could results on larger and more challenging problem instances be provided?\n4. The manuscript contains several instances of non-standard or undefined notation. For example, the abbreviation \"SQP\" in line 26 and \"S$\\ell_1$QP\" in line 145 lack clear definitions. In Theorem 3.1, the variables $y^{\\star}_I$ and $y^{\\star}_E$ are introduced without explanation. Additionally, the expression “$\\mu_i \\geq y_i$” in line 161 is ambiguous: if $\\mu_i$ is a scalar, should $y_i$ not also be a scalar rather than a vector? Similarly, in lines 158 and 164, it is unclear whether $y_i$ should carry an absolute value and whether it refers to an element of the dual variable $y$. We recommend a thorough review and clarification of notation throughout the text."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "seWfCDxvBD", "forum": "HL3TvE4Afm", "replyto": "HL3TvE4Afm", "signatures": ["ICLR.cc/2026/Conference/Submission22393/Reviewer_p3Uz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22393/Reviewer_p3Uz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791339267, "cdate": 1761791339267, "tmdate": 1762942198081, "mdate": 1762942198081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles fast solving for structured optimization by combining:\na learned generator (diffusion) that predicts primal–dual variables,\na KKT-aware loss (feasibility and stationarity residuals), and\na post-refinement stage using classical primal–dual updates.\nThe aim is to obtain near-feasible, near-optimal solutions in very low inference time, with a few corrective iterations closing any remaining gaps."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Accelerating constrained optimization with learning is timely and useful.\n\n2. Primal–dual parameterization + KKT residuals makes the supervision meaningful; the post-refinement stage is practical for polishing errors.\n\n3. On synthetic QP-style tasks, the one-step (or few-step) approach achieves competitive gaps/residuals with favorable wall-clock times."}, "weaknesses": {"value": "1. Limited novelty in core ingredients. Diffusion generation, GNN message passing over factor graphs, and KKT-residual losses are all known; the paper reads as a careful composition/tuning rather than a new algorithmic principle or theory.\n\n2. The paper lacks component-wise ablations that isolate the value of diffusion vs. a non-diffusive predictor, GNN vs. MLP, and KKT loss vs. plain supervised losses, as well as sensitivity to refinement steps and guidance scales.\n\n3. Under what assumptions (e.g., strong convexity, Slater/LICQ) does your KKT-guided sampling guarantee monotone decrease of a KKT energy or local convergence? Please state the step-size / guidance strength conditions."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BHsnptz1UU", "forum": "HL3TvE4Afm", "replyto": "HL3TvE4Afm", "signatures": ["ICLR.cc/2026/Conference/Submission22393/Reviewer_axFq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22393/Reviewer_axFq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22393/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762057686826, "cdate": 1762057686826, "tmdate": 1762942197863, "mdate": 1762942197863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}