{"id": "QB7w5JuchB", "number": 17416, "cdate": 1758275795137, "mdate": 1759897176778, "content": {"title": "Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation", "abstract": "Variational Autoencoders (VAEs) with global priors mirror the training set's class frequency in latent space, underrepresenting tail classes and reducing generative fairness on imbalanced datasets. While $t^3$VAE improves robustness via heavy-tailed Student's t-distribution priors, it still allocates latent volume proportionally to the class frequency.\nIn this work, we address this issue by explicitly enforcing equitable latent space allocation across classes. To this end, we propose Conditional-$t^3$VAE, which defines a per-class Student's t joint prior over latent and output variables, preventing dominance by majority classes. Our model is optimized using a closed-form objective derived from the $\\gamma$-power divergence. Moreover, for class-balanced generation, we derive an equal-weight latent mixture of Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA, Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE and Gaussian-based VAE baselines, particularly under severe class imbalance. In per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional Gaussian VAE across all highly imbalanced settings. While Gaussian-based models remain competitive under mild imbalance ratio ($\\rho < 5$), our approach substantially improves generative fairness and diversity in more extreme regimes.", "tldr": "", "keywords": ["VAE", "student-t distribution", "fairness", "image generation", "class imbalance"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d93d0b7c883edd17e839e6faaa1b3ac652ecf83.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors start by correctly claiming that t^{3} VAE is inadequate in addressing fair generation and equitable latent space allocation. The make the claim that a conditional VAE with per class heavy trailed Student's t-distribution priors can achieve this. They show promising results on the SVHN-LT, CIFAR100-LT, and CelebA datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well presented and is technically sound. The approach is simple and easy to understand.\n- I like the intuition behind this, and in general class conditional makes sense for smaller scale trainings.  \n- The authors have correctly identified the core weakness behind the previous t^3 VAE for fair generation/ equitable latent space allocation."}, "weaknesses": {"value": "- A weakness of the new optimization now compared to a simple t^{3} VAE is that class labels are required. With current day datasets in the scale of billions of samples, this is becoming more and more impractical. \n- Furthermore, in the case of facial images, the set of attributes can explode very quickly. Race in itself is hard to constrain to a few classes. And each new subclass requires labelled examples. \n- I would ideally like to see experimental results on larger scale higher quality datasets where both image quality and per class diversity can be ascertained. I don't think results on low quality datasets is sufficient especially when the evaluation metric is FID score. \n- Further on using FID as a metric, how does the approach compare with t^{3} VAE on precision and recall as shown in Figure 4? \n- Can you add results on a balanced dataset for reference? \n- How does it perform on multi-attribute balancing?"}, "questions": {"value": "- How does this approach scale to billions of unlabelled examples? Would you first need a classifier to get the conditioning labels? Is there a way to make do without this? \n- In line 306 the authors mention - \"test set is balanced by downsampling to the smaller class size\" what was the final sample size of this dataset? Is it sufficient to accurately measure FID score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gVVOAcezXZ", "forum": "QB7w5JuchB", "replyto": "QB7w5JuchB", "signatures": ["ICLR.cc/2026/Conference/Submission17416/Reviewer_V6dJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17416/Reviewer_V6dJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761083617620, "cdate": 1761083617620, "tmdate": 1762927312076, "mdate": 1762927312076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Conditional $t^3$-VAE, an extension of the original $t^3$-VAE framework that introduces class conditioning to improve modeling performance on underrepresented classes. The authors reformulate the $t^3$-VAEâ€™s $\\gamma$-divergence objective under conditional settings and derive class-wise objectives and sampling rules accordingly. They demonstrate improved performance over VAE baselines on several class-imbalanced datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper focuses on an important problem of improving generative modeling under class imbalance.\n- The writing is clear and easy to follow."}, "weaknesses": {"value": "1. **Limited novelty.** The main contribution is the inclusion of conditioning within the existing $t^3$-VAE formulation for handling underrepresented classes, which is relatively straightforward. The mathematical derivations are largely mechanical extensions of those in prior works such as $t^3$-VAE and $\\gamma$-VAE, with limited new theoretical development.  \n\n2. **Lack of meaningful baselines.** The experimental comparisons are restricted to VAE-type models, which are known to lag far behind modern diffusion-based generative models in both sample quality and diversity. Although the authors argue that VAEs remain useful for studying class imbalance, several diffusion-based methods have addressed the class imbalance issue (see [1] and [2]). To demonstrate the practical significance of this work, comparisons with diffusion-based class-imbalance approaches (like [1] and [2]) should be included.  \n\n3. **Evaluation metrics.** The paper mainly reports precision and recall, which are known to exhibit pathological behaviors. It is recommended to include improved metrics, such as density and coverage [3], to provide a more reliable evaluation of generative performance.\n\n----\n**References**\n\n[1] Class-Balancing Diffusion Models, CVPR 2023\n\n[2] Heavy-Tailed Diffusion Models, ICLR 2025\n\n[3] Reliable Fidelity and Diversity Metrics for Generative Models, ICML 2020"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4rApDGmqY7", "forum": "QB7w5JuchB", "replyto": "QB7w5JuchB", "signatures": ["ICLR.cc/2026/Conference/Submission17416/Reviewer_SdRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17416/Reviewer_SdRH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731601981, "cdate": 1761731601981, "tmdate": 1762927311597, "mdate": 1762927311597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduces conditional $t^3$VAE, which enables conditional generation of data given a class label $y$ by leveraging the $t^3$VAE model, a VAE model suitable for data with heavy-tail distribution. While the original $t^3$VAE applies multivariate Student's t-distributions for the joint prior of data $x$ and latent variable $z$, the authors introduce the joint prior distribution $p_\\theta(x,z|y)$ with learnable mean vector $\\mu_y$ for each class $y$, which is the Student's t-distribution. The sampling distribution for $p(z)$ is defined as a mixture of t-distributions with different means for each class, where equal weights are given to overcome the imbalance of the class. The performance of the $t^3$VAE is presented with SVHN-LT, CIFAR100-LT, and CelebA datasets. All datasets were trained with a class information where imbalance on the class size was present."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- C-$t^3$VAE enables conditional generation by incorporating the class variable $y$ into the existing $t^3$VAE. The authors successfully demonstrates its trainability, and the generated data samples conditioned on each class $y$ possess the characteristic features expected of their respective classes. Also, the proposed method demonstrated the best performance in scenarios where the training set exhibits extreme class imbalance, compared with other types of VAE.\n\n- For the $\\tau^2$ parameter used in the prior distribution of $z$, the authors provide improved value of $\\tau^2$ consistent with the established theory."}, "weaknesses": {"value": "- The results on per-class F1 scores presented in the Appendix are hard to interpret. For SVHN-LT and CIFAR100-LT, there are few cases where the C-VAE exhibits better metrics for minority classes, which contradictory with the paper's claim that C-$t^3$VAE exhibits better performance on the classes with imbalance.\n\n- While the foundational motivation of $t^3$VAE is to explore the utility of heavy-tailed distribution within VAE framework, the experiment section of the paper is predominantly focused on comparing generation quality metrics (FID and the generative F1 score). It would be better if an additional quantitative or qualitative analysis about comparing the encoded latent representations to the conditional prior distribution on $z$ (e.g., using an MMD test or visualization of the latent space) is presented for enhancing the proposed model's theoretical significance."}, "questions": {"value": "1. The covariance for the prior distribution for $z$ were all fixed to $\\tau^2 I$ for every $y$. Would it be possible to also substitute the covariance into a term with a learnable parameter for each class, such as $\\sigma_y^2 \\tau^2 I$?\n\n1. How will the metrics change if the class distribution of the test set does not matches with the prior distribution on the class $p(y)$? You can check this immediately by using all the test data from CelebA and not downsampling it to make it balanced.\n\n1. For all the models compared, how does the test reconstruction loss differ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ewx6rKPPzF", "forum": "QB7w5JuchB", "replyto": "QB7w5JuchB", "signatures": ["ICLR.cc/2026/Conference/Submission17416/Reviewer_JGTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17416/Reviewer_JGTj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848356007, "cdate": 1761848356007, "tmdate": 1762927311043, "mdate": 1762927311043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers an t^3VAE-based generative model for class-imbalanced distributions. The t^3VAE is a non-ELBO-based variational autoencoder model that employs a multivariate version of Student's t distribution as prior, encoder, and decoder to robustly model the heavy-tailed distributions. However, the t prior does not allocate latent space equally for each class. To address this problem, this paper proposes to use a t prior separately for each class. Each class possesses the loss function of the t^3VAE, which is summed over all classes with equal weights to yield the final loss function. The resulting model is called the C-t^3VAE. For generation, a t mixture with equal class-conditional probabilities is used. Empirical evidences show that the proposed C-t^3VAE enjoys improved FID and per-class evaluation metrics including the recall and F1 score."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The methodology is easy to understand. The emphasis in downstream tasks is on the generation performance, which is not extensively treated in the original t^3VAE. The improvement in generative performance in impressive. The reported phase transition phenomena from the conventional VAE to t^3VAE as a function of class imbalance ratio is also interesting."}, "weaknesses": {"value": "1. Although the method is easy to follow, its approach largely follows that of the conditional VAE and therefore a straightforward extension of the t^3VAE. Also, unlike what the title suggests, virtually no treatment on fairness task is given in the paper. \n\n2. In generation, the mixture weights $\\alpha_y$ in $p_{\\nu}^{*}(z)$ are fixed at $1/K$. Although it is noted that class prioritization can be achieved by adjusting $\\alpha_k$, no experiment is conducted in this regard.\n\n3. In the CelebA experiment, only binary conditioning is applied and multi-attribute generation is not considered.\n\n4. The \"correction\" of the variance parameter $\\tau^2$ in (9) for the alternative prior in t^3VAE is, unfortunately, incorrect. This stems from that the derivation of the cross entropy in Appendix A computes it by \n$$\n-\\frac{\\int q(x)p(x)^{\\gamma}dx}{\\left(\\int q(x)^{1+\\gamma}dx\\right)^{\\frac{\\gamma}{1+\\gamma}}}\n$$\ninstead of\n$$\n-\\frac{\\int q(x)p(x)^{\\gamma}dx}{\\left(\\int p(x)^{1+\\gamma}dx\\right)^{\\frac{\\gamma}{1+\\gamma}}}\n.\n$$\nSee the definition of the gamma-power divergence in Kim et al. (2024, Eq. 8). The above mistake is likely from the typo in Proposition 3 of Kim et al. (2024): the integral $\\int q(x)^{1+\\gamma}dx$ in the third-to-last line of the proof should be $\\int p(x)^{1+\\gamma}dx$. Nevertheless, it is the correct $\\tau^2$ (\"approximation\" in (9)) that is used in the computation, so the effect of the incorrectness in contained."}, "questions": {"value": "1. Conditional VAE can be understood as maximizing conditional likelihood instead of the full likelihood. t^3VAE minimizes the gamma-power divergence between two statistical manifolds. How can the conditional t^3VAE be interpreted in terms of divergence minimization? If all the classes are equally likely, then minimizing the *single* gamma-power divergence between the two statistical manifolds defined by $p_{data}(x)q_{\\phi}(z|x)$ and $\\sum_{y} p(x, z | y) p(y)$ result in the objective function (10)?\n\n2. I understand that all CelebA experiments are conducted with binary coding for each attribute, without considering multi-attribute conditioning. In that case, to reproduce the results in Table 1, were four separate experiments required? Or is $y$ treated as a 4-dimensional one-hot vector (i.e., considering 16 classes)? If the former is true, how about considering the latter approach? If the latter is true, how about comparing the results for multi-attribute generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XH6f35TLHi", "forum": "QB7w5JuchB", "replyto": "QB7w5JuchB", "signatures": ["ICLR.cc/2026/Conference/Submission17416/Reviewer_ttdG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17416/Reviewer_ttdG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762264745246, "cdate": 1762264745246, "tmdate": 1762927310563, "mdate": 1762927310563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}