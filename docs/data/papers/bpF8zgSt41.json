{"id": "bpF8zgSt41", "number": 23281, "cdate": 1758341618619, "mdate": 1759896823009, "content": {"title": "How Transformers Learn Causal Structures In-Context: Explainable Mechanism Meets Theoretical Guarantee", "abstract": "Transformers have demonstrated remarkable in-context learning abilities, adapting to new tasks from just a few examples without parameter updates. However, theoretical understanding of this phenomenon typically assumes fixed dependency structures, while real-world sequences exhibit flexible, context-dependent relationships. We address this gap by investigating whether transformers can learn causal structures -- the underlying dependencies between sequence elements -- directly from in-context examples.\nWe propose a novel framework using Markov chains with randomly sampled causal dependencies, where transformers must infer which tokens depend on which predecessors to make accurate predictions. Our key contributions are threefold: (1) We prove that a two-layer transformer with relative position embeddings can provably implement Bayesian Model Averaging (BMA), the optimal statistical algorithm for causal structure inference; (2) Through extensive experiments and parameter-level analysis, we demonstrate that transformers trained on this task learn to approximate BMA, with attention patterns directly reflecting the inferred causal structures; (3) We provide information-theoretic guarantees showing how transformers recover causal dependencies and extend our analysis to continuous dynamical systems, revealing fundamental differences in representational requirements.\nOur findings bridge the gap between empirical observations of in-context learning and theoretical understanding, showing that transformers can perform sophisticated statistical inference over structural uncertainty.", "tldr": "", "keywords": ["transformers", "in-context learning", "interpretability", "Markov chain"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/151f8ce0c51fe2d2f0349400dbf396e38bb89a3b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether transformers can learn causal structures underlying sequential data. Using a framework based on Markov chains with randomly sampled causal dependencies, it shows that a two-layer transformer with relative positional embeddings (RPE) can exactly implement Bayesian Model Averaging (BMA). It further provides information-theoretic guarantees explaining how transformers recover causal structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper provides a rigorous theoretical construction showing that a two-layer transformer with RPE can provably implement BMA for causal structure inference. The information-theoretic analysis strengthens the understanding by proving that attention weights converge to the true parent structure under increasing in-context examples $L$, using mutual information arguments. So I think this paper make meaningful contribution."}, "weaknesses": {"value": "While this paper is theoretically solid, some assumptions, such as linearity of the dynamics and  specific design of RPE, limit its generality. However, I think that these limitations are acceptable for the scope of this paper. Out of interest, I would like to ask the questions below."}, "questions": {"value": "1. Extension to nonlinear dynamical system:\nThe current analysis focuses on linear dynamical systems, while transformer is a nonlinear mapping.\nDo you expect that the this framework could be extended to the nonlinear dynamical system ? \n\n2. Essential role of RPE: The theoretical construction appears to rely on the use of RPE. Do you think other PE, such as absolute PE or learned PE, could, in principle, reproduce the same behaviors, or is RPE essential for causal structure inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "o790bITnDg", "forum": "bpF8zgSt41", "replyto": "bpF8zgSt41", "signatures": ["ICLR.cc/2026/Conference/Submission23281/Reviewer_BaHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23281/Reviewer_BaHK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761142822152, "cdate": 1761142822152, "tmdate": 1762942588349, "mdate": 1762942588349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework based on Markov Chains to explain how transformers create a causal dependency graph between tokens using ICL. In the Markov chain, each token depends on exactly one prior parent.\n1. The paper shows that a disentangled transformer can be trained to do Bayesian Model Averaging.\n2. Provides empirical evidence\n3. Extends the framework to continuous case (Linear dynamical systems)\n4. Provides a theoretical guarantee based on information theory for the transformer causal structure selection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes a good connection between BMA and attention (for the disentangled transformer). \n2. Shows there exists a model which, by construction, implements BMA. Also supports the claim empirically. \nThe core strength of the paper is that it helps in formalising how ICL works through a probabilistic framework."}, "weaknesses": {"value": "1. As mentioned in the strengths, the formalisation of ICL that the paper brings is useful; however, the main weakness is that both the proofs and the experimental work are limited to a special form of the transformer. It is unclear how the results can be applied to the standard architecture.\n\n2. Additionally, the paper claims that the proofs and experiments were conducted on a standard transformer. As far as I understand, they work on the disentangled transformer, which is not the same. Claims should be addressed to reflect the paper.\n\n3. It seems that the paper is missing a conclusion."}, "questions": {"value": "Do the authors have any insight into how their work can be extended to transformers with MLPs and multiple layers?\nIf not, maybe an experimental section on standard transformers to back the claims empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z4oCxYCctE", "forum": "bpF8zgSt41", "replyto": "bpF8zgSt41", "signatures": ["ICLR.cc/2026/Conference/Submission23281/Reviewer_9XhA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23281/Reviewer_9XhA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673185003, "cdate": 1761673185003, "tmdate": 1762942588148, "mdate": 1762942588148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends prior work on analyzing how transformers learn causal structure by proposing and analyzing a task which requires estimating and performing inference on an unknown causal graph from a number of in-context examples. The main theoretical result is a construction of a two layer disentangled transformer which can compute the posterior distribution over causal graphs (Theorem 1). It then uses experiments on disentangled transformers to support the claim that transformers learn a similar construction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The toy task is interesting and captures the idea of adapting to an unknown causal structure\n- The paper is generally well written\n- The authors show consistency of the construction (Theorem 2)"}, "weaknesses": {"value": "- Theorems 1 and 3 aren't mathematical theorems due to the use of $\\approx$. For example for Theorem 1 I believe the intended theorem is something like there exists a sequence of weights $\\theta_\\beta$ such that $\\lim_{\\beta \\to \\infty} f_{\\theta_\\beta}(\\ldots) = \\pi(\\ldots)$? Similarly, the mathematical statement of Proposition 2 is also unclear to me.\n- As far as I can tell, the paper doesn't actually demonstrate weight-space agreement between their construction and the one learned by gradient descent (only the attention maps are verified). Is the challenge in matching the behaviors of the different heads? It's not clear at all to me from Figure 16 that the construction matches the one in Theorem 1.\n- It would be good to run some experiments on a standard transformer with MLPs to check whether you learn a similar construction or get similar performance.\n\nMinor points:\n- There is a redundant prove in the abstract: \"We **prove** that a two-layer transformer with relative position embeddings can **provably** implement\"\n- The figures in the appendix are very hard to read. For example, Figure 10 makes it look like every weight is identically 0 since the diagonal entries are barely visible. Perhaps putting the attention weights in log scale or reducing the size would help?\n- Typo on line 268: \"We set transformers has $K$...\"\n- I'm not sure what footnote 3 is saying?\n- Proposition 2 is cited at Lemma 2 in Appendix B.5."}, "questions": {"value": "- It seems that in addition to reducing the number of parameters, factoring the positional embeddings into L and H also simplifies the construction since it gives you parameter sharing between the in-context examples and the test-example for free. How do the experiments change if you use a more standard architecture?\n- Is it possible to interpret the learned attention heads and show that it really does implement the same construction as Theorem 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1CStmE33JR", "forum": "bpF8zgSt41", "replyto": "bpF8zgSt41", "signatures": ["ICLR.cc/2026/Conference/Submission23281/Reviewer_h4T5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23281/Reviewer_h4T5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927731333, "cdate": 1761927731333, "tmdate": 1762942587904, "mdate": 1762942587904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how transformers can learn causal structure in context. It designs a task where for each input sequence a causal graph is sampled and used to generate Markov chains (with fixed kernel). Given $L$ examples in context, the model has to learn to predict the continuation of the $L+1$-th sample. The paper shows that there exist a construction of a 2-layer transformer with $L$ attention heads in the first layer which can implement BMA to solve the task, and this is (approximately) learnt by the trained models. Finally, it studies the extension of this task and model to continuous data, i.e. a dynamical system."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies a novel problem, i.e. how transformers can learn causal structures in-context, which continues an established line of work on understanding the internal functioning of transformers in simplified setups.\n\n- The paper studies the proposed task both theoretically and empirically, showing that transformers implement BMA, and even provides an extension to the continuous case."}, "weaknesses": {"value": "- The paper contains a lot of different parts, which makes it cluttered and none of them is presented too clearly. For examples, the construction of the transformer to implement BMA is not actually given, the figures are too small to be readable, there's no conclusion section. Moreover, the notation is in several places imprecise, e.g. in L292 $x$ is discussed but doesn't appear in the previous equation, in Eq. (6) $k'$ is not defined, etc., and the writing at times unclear. Overall, this makes the paper hard to follow.\n\n- The construction of the transformer which implements BMA needs the number of heads to scale with the length input sequence, which I think it's a limitation and makes the construction even more distant from real-world models.\n\n- While the paper provides several results, it's not clear what the takeaway message is. In fact, the paper shows that transformers can implement BMA to learn a causal structure in context, but this is shown by tailoring the architecture to a specific task (see point above). This construction doesn't seem to reveal some general mechanism applicable to other tasks, as it was for example with induction heads, which may limit the impact of this result on future work."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dVanXRV5fp", "forum": "bpF8zgSt41", "replyto": "bpF8zgSt41", "signatures": ["ICLR.cc/2026/Conference/Submission23281/Reviewer_M4B8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23281/Reviewer_M4B8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010723207, "cdate": 1762010723207, "tmdate": 1762942587585, "mdate": 1762942587585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how transformers can learn and adapt to different causal structures in-context. The authors propose a framework using Markov chains with randomly sampled parent dependencies and prove that a two-layer transformer with some version of Relative Positional Embeddings (RPE) can implement Bayesian Model Averaging (BMA), the optimal algorithm for this inference task. The work provides a detailed theoretical construction, information-theoretic guarantees, and empirical validation showing that trained transformers learn to approximate this mechanism.\n\nThe paper's contribution lies in the connection to BMA and its 2-layer RPE construction. However, the manuscript suffers from a significant weakness in the way in which it acknowledges and contextualizes these contributions with respect to relevant prior works. Additionally, the theoretical construction relies on several specific assumptions that tailor the architecture to the specific task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.  **Principled Connection to Bayesian Inference:** The paper provides a link between the transformer's attention mechanism and Bayesian Model Averaging (BMA). \n2.  **2-Layer RPE Construction:** The explicit construction of a 2-layer transformer using a two-axis RPE to implement BMA reinforces the idea that architectural components can be mapped to algorithmic steps.\n3.  **Information-Theoretic Guarantees:** The paper provides additional theoretical guarantees for causal structure recovery based on mutual information, strengthening the formal understanding of the learning dynamics."}, "weaknesses": {"value": "1.  **Insufficient Acknowledgment of Prior Work:** The paper lacks a proper discussion of the related work  [1]. It does not properly contextualize its contributions, preventing a clear understanding of the paper's own contribution over prior works.\n    - **Framing of Novelty:** The introduction frames the manuscript’s contribution as a departure from prior work limited to \"fixed dependency structures\" yet [1] seems to have already moved beyond this by introducing the same setup considered here based on in-context causal structure selection in Markov chains. The section \"Our Approach\", which conceptually outlines a setup closely related to [1] (each token depends exactly on one of the past tokens, with this dependence inferred in-context), omits any mention of this direct precedent work.\n    - **Appendix Citation:** The citation in the appendix appears to report incorrect information. It claims that the verification in [1] is \"limited to attention visualizations\" however, [1] also provides quantitative validation using KL Divergence. It also criticizes [1] for \"task-specific constructions,\" a trait shared by this work (see points below).\n    - **Unacknowledged Theoretical Precedent:** the work presents in its Lemma 2, the central statistical argument proving the identifiability of the true causal parent. However, the result seems to closely parallel Lemma 2 in [1].  The present version is slightly stronger (the strict inequality) and presents a different proof, but leads to the same conclusion. This fact should be correctly acknowledged. \n    *   **Setups overlap** It would be helpful if the authors could acknowledge the conceptual overlap and similarities with [1]. The key difference seems to me to be that [1] infers a single global structure from within one sequence, while this work infers position-specific structures from across multiple sequences. This makes this work a valuable generalization and a genuine advance in the expressivity of the underlying graphs, but also requires the number of heads as well as the embedding to scale with the length of the examples (H). The two frameworks address the same underlying estimation problem: at a given position, identifying the correct parent accumulating log-likelihoods in a one-parent Markov process,  but differ in how the evidence is presented in the context (within-sequence vs. across-sequences). Acknowledging this would strengthen the paper by highlighting its novelty while giving proper credit.\n    \n2. **Dependence of the BMA result on architectural tailoring:** While the paper presents interesting ideas regarding linking in-context causal selection to Bayesian Model Averaging (BMA) in transformers. It remains unclear whether the demonstrated BMA implementation genuinely reflects what standard transformers learn and can implement or rather a consequence of the architectural choices, aligned with the task structure, made in the construction.\n\n3.  **Different attention domains across layers (T×T then H×H)** Layer 1 attends over the full concatenated sequence (shape (T\\times T) across examples×positions) while Layer 2 attends only across positions inside an example (shape (H\\times H)). This effectively changes the logical index set the model operates on between layers, i.e. the model is built so different layers operate on different axes, tailored to the task they need to solve. It remains unclear whether a standard attention mechanism can discover a similar pattern.\n\n4.  **Two-axis positional encoding (separate lookups for example-index and position-within-example)**\n  The construction uses distinct positional biases for the example axis and the within-example position axis. This is a RPE that is tailored to the task and explicitly factors the task axes; it is not the standard single-axis absolute or relative positional scheme used in many transformer models. It remains unclear if replacing the two-axis RPE with a single standard positional encoding (absolute or standard RPE) the same BMA mechanism emerges or if it can even be represented.\n\n5.  **First layer omits W_QK and W_OV.** The first attention layer is constructed to behave as a direct copier (fetching particular past tokens), which is commonly employed in the literature. However, the matrices acting over the semantics W_QK and W_OV seem to be omitted in the construction. It is acceptable for the purpose of the theoretical construction to fix these components (W_{QK} and W_{OV}​) to zero, but this design choice should be stated explicitly. Moreover, the paper should provide empirical evidence showing that allowing these parameters to be trainable does not substantially alter the main BMA mechanism or invalidate the proposed construction.\n\n6.  **No positional encoding  for the second attention layer**. Similarly, the construction does not handle positional information consistently across layers: the second attention layer either omits or fixes to zero positional encodings compared to the first. This asymmetry effectively hardcodes aspects of the intended computation and departs from the symmetry typical of standard multi-layer transformers. As in the previous point, it would be important to assess whether including full positional biases in the second layer would materially change the mechanisms the model implements.\n\n\n7. **Theorem 3** Theorem 3 leverages the same underlying statistical mechanism used in [2] (as acknowledged by the authors): it expresses the gradient signal in terms of a χ²–mutual information measure between each candidate parent and the child token, and then applies the Data Processing Inequality to show that this measure is maximized for the true parent. The contribution of Theorem 3 is, in my opinion, incremental: it adapts the same statistical idea to the specific transformer construction that exactly implements Bayesian Model Averaging. Moreover, the paper states that its proof “eliminates the stationary assumption of the data distribution and doesn’t require the Markov chain to be mixed,” but  Theorem 3 still relies on expectations such as $(\\mathbb{E}[\\pi(x_h \\mid x_{h'}) / \\mu(x_h)])$, which implicitly assume access to a stationary marginal distribution or i.i.d. sampling across examples. Without some ergodicity or mixing condition, it is unclear to me how these expectations are defined or estimated.\n\n[1] D'Angelo Francesco, Francesco Croce, and Nicolas Flammarion. \"Selective induction heads: How transformers select causal structures in context.\" In The Thirteenth International Conference on Learning Representations. 2025.\n\n[2] Nichani Eshaan, Alex Damian, and Jason D. Lee. \"How Transformers Learn Causal Structure with Gradient Descent.\" In Forty-first International Conference on Machine Learning. 2024."}, "questions": {"value": "1.  **Unused Heads (Lines 294–295):** The authors state that \"some heads... didn’t learn meaningful features.\" According to the construction, each head in the first layer should learn a specific copying mechanism. Does this observation imply the model was trained with more heads than theoretically necessary, or that some heads failed to specialize as expected?\n2.  **Attention Notation (Lines 246–252):** The equations appear to use both a generic activation `σ(⋅)` and `softmax(⋅)` in the context of attention. Could the authors clarify the distinction or correct the notation if they are intended to be the same?\n3.  **BMA Formulation (Lines 138–140):** For clarity, it would be helpful to explicitly write out the analytical expression for the likelihood `P(x | pa(h))` and the resulting posterior-predictive (BMA) distribution. This would make the target algorithm that the transformer is shown to implement immediately clear to the reader.\n4.  **Theorem 3** Can the authors clarify under what distribution the expectations in Theorem 3 are taken, and how the result holds when the underlying Markov chain is non-stationary or non-mixing?\n5. Could the authors clarify whether a transformer with conventional architecture and positional encodings (without separating L position and H position) would still approximate BMA under the same task formulation, or if these design constraints are essential for the claimed behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3EjHjz0j0L", "forum": "bpF8zgSt41", "replyto": "bpF8zgSt41", "signatures": ["ICLR.cc/2026/Conference/Submission23281/Reviewer_EPSS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23281/Reviewer_EPSS"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195891810, "cdate": 1762195891810, "tmdate": 1762942587394, "mdate": 1762942587394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}