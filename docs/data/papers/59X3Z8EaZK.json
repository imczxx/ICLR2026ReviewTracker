{"id": "59X3Z8EaZK", "number": 3986, "cdate": 1757580344912, "mdate": 1763671540899, "content": {"title": "Symmetric Behavior Policy Optimization", "abstract": "Behavior Regularized Policy Optimization (BRPO) leverages asymmetric (divergence) regularization to mitigate the distribution shift in offline Reinforcement Learning. \nThis paper is the first to study the open question of symmetric regularization. \nWe show that symmetric regularization  does not permit an analytic optimal policy $\\pi*$, posing a challenge to practical utility of symmetric BRPO.\nWe approximate $\\pi^*$ by the Taylor series of Pearson-Vajda $\\chi^n$ divergences and show that an analytic policy expression exists only when the series is capped at $n=5$.\nTo compute the solution in a numerically stable manner, we propose to Taylor expand the conditional symmetry term of the symmetric divergence loss, leading to a novel algorithm: Symmetric $f$-Actor Critic (S$f$-AC).\nS$f$-AC achieves consistently strong results across various D4RL MuJoCo tasks. Additionally, S$f$-AC avoids per-environment failures observed in IQL, SQL, XQL and AWAC, \nopening up possibilities for more diverse and effective regularization choices for offline RL.", "tldr": "we study the problem of behavior regularized policy optimiza", "keywords": ["symmetric divergences; behavior regularization; offline reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc0fa917535059b964c1c32ef4b707b032e8c85b.pdf", "supplementary_material": "/attachment/e8c3dbc63dcfef3a582412135be52d1a22694e15.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an interesting perspective on behavior regularized policy optimization: using symmetric divergences, instead of asymmetric divergences, as the implementation of the regularization. The difficulty of using a symmetric divergence is twofold: 1) common symmetric divergences do not admit analytical solutions for the optimal policy; and 2) numerical issues can occur when dealing with a finite support distribution. To tackle the challenges, this paper introduces two techniques that are all based on Taylor expansion. First, in order to derive a closed-form optimal policy, they truncate the divergence in the RL objective to the second order; second, they truncate the divergence used in the policy improvement step to improve numerical stability. Some other techniques are also included in the proposed Sf-AC algorithm. The empirical evaluations are conducted on D4RL, and Sf-AC does demonstrate improvement over previous methods like SQL, IQL, and XQL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation of the overall idea is clear to me. \n\n2. The literature review is comprehensive. I especially appreciate the discussion in the Appendix, which covers both policy regularization and distribution matching, and elucidates why the problem studied in this paper is unique and has not been addressed by previous literature. \n\n3. The proposed algorithm appears robust and does not require per-task hyperparameter tuning (Tables 4, 5, 6)."}, "weaknesses": {"value": "Although the paper briefly discussed the issue with asymmetric divergence in the introduction section, I would say it would be more appropriate to formally define the problems of using asymmetric divergence somewhere between Sections 2 and 3. In lines 51-53, it is unclear to me how symmetric divergence solves the issue of multiple minimum points due to the capacity of the policy function class.\n\nIn short, the current version of the draft doesn’t provide sufficient motivation for me to switch from an asymmetric divergence to a symmetric one. I would consider raising my score if the authors can make this clear in the rebuttal."}, "questions": {"value": "1. In equation (8), the first term uses weights proportional to the exponential of the advantage, while in line 310, the optimal policy is defined by using [1 + A/\\tau]_+. What causes the mismatch? \n\n2. The value of \\epsilon varies from 0.2 to extremely large values like 100. Is it really necessary to include this hyperparameter? \n\n3. Copy-edits: In Algorithm 1, when defining the advantage regression, the subscript of V should be \\phi."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bShCXL1ulY", "forum": "59X3Z8EaZK", "replyto": "59X3Z8EaZK", "signatures": ["ICLR.cc/2026/Conference/Submission3986/Reviewer_LZeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3986/Reviewer_LZeD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848569229, "cdate": 1761848569229, "tmdate": 1762917126458, "mdate": 1762917126458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies symmetric behavior regularization for offline RL within a BRPO-style framework and asks whether symmetric $f$-divergences admit an analytic optimal policy $\\pi*$ suitable for target matching. \nThe authors prove that for common symmetric divergences (Jeffreys, Jensen–Shannon, GAN) no closed-form $\\pi^*$ exists and that naively using symmetric losses can be numerically unstable with finite-support policies. They propose a remedy by Taylor expanding $f$-divergences into a $\\chi^n$ series, truncating at small order ($N<5$), which yields an analytic surrogate policy (closed form for $N=2$) and a practical algorithm Sf-AC: advantage regression plus a truncated conditional-symmetry term with ratio clipping and a truncation-error bound. Experiments on a Mixture-of-Gaussians fit and 9 D4RL MuJoCo tasks show competitive and failure-robust performance; the JS/Jeffreys variants are frequently top-3 by AUC."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear theory: explains why standard symmetric divergences do not yield an analytic $\\pi^*$ and exposes instability.\n\nPrincipled workaround: $\\chi^n$ expansion recovers a usable surrogate for $N\\le 4$; simple closed form for $N=2$.\n\nPractical loss: decomposes into advantage regression + symmetry expansion, with clipped ratios and an error bound.\n\nEmpirical robustness: strong results on D4RL with ablations over series order and clipping; resilient under failure cases."}, "weaknesses": {"value": "Approximation vs exact symmetry: truncation introduces bias; the bias/variance/stability trade-off could be analyzed deeper.\n\nBenchmark scope: limited to MuJoCo; harder OOD domains (AntMaze, Kitchen, Adroit-relocate) would stress-test the method.\n\nBehavior-policy access: guidance is light when $\\pi_D$ (or density ratios) are poorly estimated.\n\nNo end-to-end convergence or error-propagation guarantees under function approximation for the combined objective."}, "questions": {"value": "Sensitivity to series order $N_{\\text{loss}}$ and analytic-policy order $N$ (e.g., 2 vs 3–4)? Any task-dependent guidance?\n\nWall-clock and memory vs $N_{\\text{loss}}$ and clipping $\\epsilon$?\n\nRobustness when behavior coverage is narrow or multimodal; diagnostics for ratio misestimation?\n\nCould uncertainty signals (critic ensembles) adapt $\\epsilon$ or series coefficients to avoid rare failures?\n\nAny caveats for discrete/bounded actions beyond clipping (e.g., projection effects)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c0CqgDXkJu", "forum": "59X3Z8EaZK", "replyto": "59X3Z8EaZK", "signatures": ["ICLR.cc/2026/Conference/Submission3986/Reviewer_Gvf5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3986/Reviewer_Gvf5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898446896, "cdate": 1761898446896, "tmdate": 1762917126261, "mdate": 1762917126261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a clear theoretical and practical gap in behavior-regularized offline reinforcement learning (BRPO).\nWhile existing approaches rely almost exclusively on asymmetric divergences (e.g., KL or *$\\chi^2$*) that induce strong mode-seeking bias, this work systematically investigates the use of symmetric *$f$*-divergences as regularizers — an area that has been largely unexplored in prior literature.\n\nThe authors prove that symmetric BRPO generally lacks analytic optimal policies due to the non-affine form of *$f'(t)$* in *$\\ln t$* and propose a principled approximation based on finite *$\\chi$*-series truncation. This yields the Symmetric f-Actor-Critic (Sf-AC) algorithm, which preserves convexity, maintains bounded approximation error, and provides a balanced compromise between mode-seeking and mode-covering behaviors. Empirical results on D4RL MuJoCo benchmarks show consistent robustness and per-environment stability improvements over strong baselines (IQL, SQL, AWAC, XQL).\n\nHowever, the empirical evaluation lacks sufficient quantitative evidence to clearly demonstrate the effectiveness of the proposed symmetric regularization. While the results suggest improved robustness, the paper does not include concrete numerical comparisons or diagnostic analyses against existing offline RL approaches that would clarify why symmetric regularization helps. In particular, additional metrics illustrating the robustness deficiencies of conventional mode-seeking or mode-covering methods would make the contribution more complete and convincing.\n\nIf such quantitative analyses or diagnostic metrics are provided during the rebuttal period, I would be highly willing to raise my score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Theoretical novelty**\n\n   * The paper fills a clear theoretical gap in behavior-regularized offline RL by systematically exploring the use of symmetric *$f$*-divergences. This is a novel perspective that extends the well-established BRPO framework beyond its asymmetric KL- or χ²-based formulations and highlights an under-examined dimension of regularization geometry.\n\n* **Practical workaround through principled approximation**\n\n   * The authors introduce a mathematically grounded yet practical approximation via finite χ-series truncation and Taylor expansion of the conditional symmetry term. This approach preserves convexity and boundedness while providing a tractable implementation, making the otherwise intractable symmetric regularization feasible without major instability."}, "weaknesses": {"value": "* **Lack of rigorous comparative analysis**\n\nThe paper mainly presents performance plots (e.g., Figure 3) showing only average returns across environments, without quantitative summaries such as mean ± confidence intervals. This makes it difficult to verify whether the improvements are **statistically significant** or whether the baseline implementations are correctly reproduced. In particular, the reported baseline performances on MuJoCo tasks appear unexpectedly low, raising concerns about possible implementation discrepancies or hyperparameter mismatches. Providing detailed numerical tables with variance statistics would make the comparisons more reliable and transparent.\n\n* **Insufficient diagnostic metrics for robustness**\n\nThe claim of “robust performance” remains qualitative. In offline RL, robustness is often better captured by additional statistics such as CVaR or worst-case return, rather than mean performance alone. Including such risk-sensitive or tail-distribution metrics would substantially strengthen the claim that symmetric regularization mitigates the brittleness of conventional mode-seeking methods.\n\n* **Need for deeper analysis contrasting mode-seeking and symmetric behaviors**\n\nWhile prior works have demonstrated the empirical benefits of using symmetric divergences in *$D_{opt}$*, this paper claims that applying symmetry directly to the regularizer *$D_{reg}$* offers additional advantages. Although the paper clearly articulates the motivation for symmetric regularization, it does not sufficiently highlight the limitations of existing mode-seeking regularization that the proposed method intends to address. In offline RL, purely mode-seeking approaches are often regarded as sufficient for ensuring stability and avoiding out-of-distribution (OOD) actions. Introducing more mode-covering behavior through symmetry could, in principle, encourage exploration into unsupported regions of the dataset, potentially harming performance. Therefore, a deeper analysis or empirical evidence clarifying when and why symmetric regularization provides tangible benefits over purely mode-seeking objectives would make the paper more convincing and theoretically grounded."}, "questions": {"value": "* **Quantitative Comparison**\n\nCould the authors provide detailed numerical results (e.g., mean ± confidence intervals) for each baseline, especially on the MuJoCo tasks where baseline performance appears unusually low, to confirm that the reported improvements are statistically significant and reproducible?\n\n* **Robustness Evaluation**\n\nBeyond average returns, can the authors include additional robustness-oriented metrics (e.g., CVaR, percentile or worst-case returns) to substantiate the claim of “robust performance” and demonstrate that symmetric regularization indeed improves stability over mode-seeking methods?\n\n* **Clarification on D_reg vs. D_opt Effectiveness**\n\nSince prior works have already shown benefits of symmetric divergence when used in *$D_{opt}$*, could the authors isolate or ablate the contribution of using symmetry in *$D_{reg}$* to clarify when and why this leads to tangible gains over purely mode-seeking regularization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yrKyE1RLZQ", "forum": "59X3Z8EaZK", "replyto": "59X3Z8EaZK", "signatures": ["ICLR.cc/2026/Conference/Submission3986/Reviewer_RJqh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3986/Reviewer_RJqh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984362660, "cdate": 1761984362660, "tmdate": 1762917126070, "mdate": 1762917126070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}