{"id": "rCNe4N7cP4", "number": 6310, "cdate": 1757966511757, "mdate": 1759897923186, "content": {"title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction", "abstract": "We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker–Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language–speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker’s hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.", "tldr": "an end-to-end multimodal human world modeling framework for constructing an infinitely streamable digital human from one single portrait, capable of generating intelligent, real-time, multi-turn responses across text, speech, and video.", "keywords": ["Portrait Animation; Diffusion Forcing; Generative Models; Large Language Models;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac6ae580125a1151e4df5b8fb0fa0575d04cc2de.pdf", "supplementary_material": "/attachment/a4b5ca3fb5cc950014aabd22ba9360c99129f4aa.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a **dual-transformer architecture** to achieve unified multimodal understanding and generation. Specifically, the **VLM branch** handles comprehension and produces text and audio outputs, while the **diffusion branch** generates video. The two branches are temporally and semantically aligned via **cross-attention**. Additionally, the authors introduce **inter- and intra-chunk attentions** and **time-aligned multimodal positional embeddings** to further strengthen temporal and semantic coherence across modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is **well-structured and clearly written**, with a coherent presentation of ideas.  \n- It makes a **notable contribution** by introducing a unified framework for interactive human agents, which was previously handled by modular or pipeline-based systems.  \n- The proposed approach shows **significant improvements** in **modality alignment**, **temporal consistency**, and **generation efficiency**, representing a meaningful step forward for multimodal generation systems."}, "weaknesses": {"value": "1. According to the description, the full system seems to include **Qwen-Omni’s Talker** for audio decoding, but this component is **not reflected in Figure 2**. The authors should clarify its role and integration.  \n2. As the autoregressive (AR) generation progresses, the **diffusion model’s context length** naturally grows, which could impact decoding speed. Is there **context truncation** during deployment, and if so, **how is it implemented**? Does truncation affect temporal or semantic consistency?  \n3. Could a **non-autoregressive diffusion model** serve as a viable baseline for video decoding? For instance, such a model could take as input a portrait, short text, and audio segments to generate short video clips in parallel. Since the portrait remains a consistent conditioning signal, **identity preservation** might still be maintained. The paper does not seem to include a comparison to this baseline."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ca6r5FH7SQ", "forum": "rCNe4N7cP4", "replyto": "rCNe4N7cP4", "signatures": ["ICLR.cc/2026/Conference/Submission6310/Reviewer_EKMn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6310/Reviewer_EKMn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890427123, "cdate": 1761890427123, "tmdate": 1762918608798, "mdate": 1762918608798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces X-Streamer, an end-to-end multimodal framework for building interactive digital humans that enable real-time text-speech-video interactions from a single input portrait. It adopts a Thinker–Actor dual-transformer architecture, where the frozen Thinker interprets streaming text/audio inputs, and the Actor autoregressively generates interleaved multimodal outputs. The quantitative and qualitative results show that X-Streamer achieves strong and consistent performance in visual quality, temporal coherence, and audio–visual synchronization during real-time talking-head interactions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a unified architecture that seamlessly integrates text, speech, and video generation for real-time digital humans.\n- X-Streamer achieves strong visual quality and maintains real-time performance.\n- The paper shows stable and visually coherent long-form video generation, maintaining identity and temporal consistency over minutes of continuous interaction."}, "weaknesses": {"value": "1. Limited novelty: The approach primarily constitutes an engineering integration of existing components (e.g., GLM‑4‑Voice backbone, diffusion forcing) and introduces few fundamentally new modeling ideas.\n2. Insufficient experiments: The experiments on long-duration videos are limited; more cases and in-depth analyses are needed. In addition, ablation studies should examine factors such as the impact of visual context length on both performance and latency.\n3. Limited scope of contribution: Although the paper frames its contribution as “human world modeling,” the demonstrated capabilities are largely restricted to audiovisual talking-head generation, which slightly overstates the scope of the claimed modeling."}, "questions": {"value": "1. Please refer to points listed in weakness.\n2. How does the visual context length affect performance and latency? Did the authors explore trade-offs between longer visual context for identity preservation and real-time efficiency?\n3. Would jointly fine-tuning the Thinker module with the Actor improve long-term temporal consistency, multimodal alignment, or identity preservation compared to keeping it frozen?\n4. Table 2 claims qualitative ablation in capiton while the content are quantitative results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gtXbGt8qJz", "forum": "rCNe4N7cP4", "replyto": "rCNe4N7cP4", "signatures": ["ICLR.cc/2026/Conference/Submission6310/Reviewer_J6da"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6310/Reviewer_J6da"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914309735, "cdate": 1761914309735, "tmdate": 1762918608307, "mdate": 1762918608307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces X-Streamer, an end-to-end multimodal framework that generates synchronized text, speech, and video streams from a single portrait for digital human interactions. The framework is built upon a Thinker–Actor dual-transformer architecture. A frozen pretrained language–speech model acts as the Thinker for perception and reasoning. An Actor autoregressively generates synchronized multimodal outputs using chunk-wise diffusion forcing and global identity referencing. The framework runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Coherent framework unifying reasoning and generation across text, speech, and video for digital human interactions\n2. Demonstrated cross-modal alignment and visual quality."}, "weaknesses": {"value": "1. The paper's claim of real-time operation lacks substantiation, as it does not provide enough quantitative latency measurements or empirical evaluation to support this assertion.\n2. Sweeping assertions such as \"infinite\" and \"hours-long\" are not backed by quantitative evidence, such as long-term consistency curves, which are necessary to validate such strong performance statements.\n3. The system implementation described in the paper reads more like an engineering effort that stitches together a collection of known, established techniques and models, lacking any particularly impressive innovation."}, "questions": {"value": "1. The so-called \"global identity reference\" is a common paradigm in LLM-based reference image/video generation, where the reference image is simply embedded as part of the multimodal input sequence. Therefore, how does your proclaimed approach substantively differ from these existing methods?\n2. What is the actual end-to-end latency (ms) from user audio input to first video frame, and how are Thinker and Actor distributed or synchronized across GPUs?\n3. Can X-Streamer work normally across different races, ages, genders and nonhuman or stylized avatars?\n4. Can X-Streamer maintain coherence over 1-hour conversations without any degradation?\n5. How to consider using a 2-second multimodal segment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kPFTkYL8wL", "forum": "rCNe4N7cP4", "replyto": "rCNe4N7cP4", "signatures": ["ICLR.cc/2026/Conference/Submission6310/Reviewer_16Le"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6310/Reviewer_16Le"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921836815, "cdate": 1761921836815, "tmdate": 1762918607995, "mdate": 1762918607995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}