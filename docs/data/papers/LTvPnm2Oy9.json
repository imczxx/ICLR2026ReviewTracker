{"id": "LTvPnm2Oy9", "number": 21176, "cdate": 1758314535745, "mdate": 1759896937857, "content": {"title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons", "abstract": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory–accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.", "tldr": "", "keywords": ["large language models", "reasoning", "efficiency", "reinforcement learning", "compression", "kv cache"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5269a920550c6e3f011980a8eb54b6166ee9b5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a breadcrumb compression reasoning approach.  During long-context reasoning generation, a beacon token is inserted every c tokens. This beacon token learns to compress the key-value cache of the previous c tokens into a single entry. Then, the key-value cache of these c tokens is directly removed, retaining only the beacon token. This approach reduces the KV cache usage to 1/c of the original. The authors train this capability into the model using a joint RL+distillation training process. Compared to training-free cache pruning methods, this method delivers superior memory-accuracy Pareto frontiers on three inference tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is interesting and makes sense. Compressing the key-value cache of every 8 tokens in the context into a single beacon token is an intuitive approach.\n\n2. The authors provide very detailed training pipelines in the paper.\n\n3. The proposed method demonstrates strong results on the Countdown, Linsys, and Stargraph tasks."}, "weaknesses": {"value": "1. Although the proposed method demonstrates strong results on Countdown, Linsys, and Stargraph, as a method specifically designed to optimize long-context reasoning, does it also perform well on mainstream mathematical reasoning tasks (such as MATH500 and AIME)?\n\n2. The method demonstrates its effectiveness on the instruction models Qwen-2.5-1.5B-Instruct and Phi-4-Mini-Instruct. However, can it be directly applied to the long-context reasoning model (deepseek-distil-qwen-1.5b)? Or does it require an instruction model or base \nmodel as a starting point for training? \n\n3. Some important kv-cache pruning baselines, such as SnapKV and PyramidKV, are missing from the experiment.\n\n4. The Contributions, despite the interesting methods, are not so significant. \n\n5. The organization and formatting of the papers can indeed be improved."}, "questions": {"value": "Please see the issues raised in the Weaknesses session.\n\nIt is critical to address questions 1, 2, 3, and 4, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pU6w2aCV1W", "forum": "LTvPnm2Oy9", "replyto": "LTvPnm2Oy9", "signatures": ["ICLR.cc/2026/Conference/Submission21176/Reviewer_868j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21176/Reviewer_868j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890562862, "cdate": 1761890562862, "tmdate": 1762941575329, "mdate": 1762941575329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Breadcrumbs Reasoning proposes a memory-efficient framework for long-context reasoning by periodically compressing the Transformer key–value cache with a learned “beacon” token. The model summarizes recent reasoning segments into compact embeddings and evicts redundant cache entries, reducing memory usage during generation. Trained via a joint reinforcement learning and distillation process, it learns to reason and compress simultaneously with minimal overhead. Evaluated on Countdown, LinSys, and StarGraph using Qwen2.5-1.5B and Phi-4-Mini, the method achieves a superior memory–accuracy trade-off, retaining 67-94% of baseline performance with up to 32× less memory and even surpassing the uncompressed teacher under the same cache budget, demonstrating that reasoning traces can be effectively compressed for efficient test-time scaling"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of periodically compressing previous Chain-of-Thought tokens through learned beacon representations is interesting and conceptually appealing, offering a new perspective on memory-efficient reasoning.\n2. The method demonstrates strong flexibility, as it can be applied with different compression ratios, allowing users to balance accuracy and memory efficiency based on deployment constraints."}, "weaknesses": {"value": "1. Some performance degradation is observed across tasks, even at moderate compression ratios (e.g., 2×), suggesting that the compression process may occasionally discard useful reasoning context.\n2. The methodology section could be clarified further, particularly how the beacon token is constructed, integrated, and optimized during training.\n3. It would be valuable to include additional baselines, such as Controlling How Long A Reasoning Model Thinks With Reinforcement Learning [1], to better position the method within the broader literature on reasoning efficiency.\n3. The paper does not report inference speed or latency improvements, which are essential for evaluating the practical benefits of the proposed compression scheme.\n\n[1] L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZWuZDTGH5C", "forum": "LTvPnm2Oy9", "replyto": "LTvPnm2Oy9", "signatures": ["ICLR.cc/2026/Conference/Submission21176/Reviewer_QxJL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21176/Reviewer_QxJL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897314785, "cdate": 1761897314785, "tmdate": 1762941573962, "mdate": 1762941573962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Breadcrumbs Reasoning,\" a method to mitigate the high memory cost of the Transformer KV cache during long-form reasoning. The model is trained to periodically compress its generation KV cache into a single \"beacon\" token and then evict the compressed entries. The core contribution is a joint RL-distillation framework where trajectories from a standard RL-trained \"teacher\" policy are used to simultaneously distill a \"student\" policy that learns both to reason and to compress. This joint training approach minimizes additional overhead. Experiments on Qwen2.5-1.5B and Phi-4-Mini models show this method achieves a superior memory-accuracy Pareto frontier. It can match or exceed the teacher's accuracy within a fixed memory budget by enabling more reasoning steps , and it significantly outperforms training-free compression baselines like TOVA and StreamingLLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Efficient Training Strategy: The joint RL-distillation framework is a key strength. It cleverly piggybacks on the standard RL training process, using its trajectories to simultaneously teach compression. This avoids a complex and data-intensive two-stage training pipeline, and the paper provides a clear validation (Fig. 5) that this joint approach is as effective, if not more so, than a sequential one.\n\nStrong Empirical Results: The method demonstrates a clear and significant advantage over training-free baselines (TOVA, StreamingLLM), which seem incapable of handling complex reasoning tasks. This provides strong evidence that a learned, task-aware compression scheme is necessary for this domain.\n\nEffective Test-Time Scaling: The paper clearly demonstrates the practical benefit of the method. By compressing the cache, the model can afford to generate many more reasoning tokens within the same memory budget, often leading to higher final accuracy than the uncompressed teacher (as seen in Table 1 and Figure 3). This is a valuable practical trade-off."}, "weaknesses": {"value": "Task-Specific Training: The proposed method requires running the entire joint RL-distillation process for each specific reasoning task. This limits the \"out-of-the-box\" generality of a trained model. \n\nLimited Model Scale: The experiments are conducted on relatively small models (1.5B and ~4B). While promising, it remains an open question how the training dynamics and compression effectiveness will scale to much larger.\n\nPerformance on Structured Reasoning: The method's performance improvement was least pronounced on the LinSys task, which the authors note requires more structured, step-by-step deduction. This may suggest that the \"breadcrumb\" compression is too lossy for tasks where all intermediate steps are critical, and that it performs best on tasks with more \"trial-and-error\" components."}, "questions": {"value": "1. Models. The experiments are conducted on 1.5B and ~4B parameter models (Qwen2.5-1.5B and Phi-4-Mini). How is the performance and training stability of the joint RL-distillation framework expected to scale to significantly larger models (e.g., 7B, 14B)? Furthermore, have the authors considered applying this method to reasoning models like DeepSeek-Distill-Models Series? \n\n2. Novelty. The paper positions itself relative to Activation Beacons [1] by noting simplifications (e.g., no chunk/sub-chunk distinction) . However, the core concept of a learned token summarizing a past window is also present in methods like Gist Tokens [2]. Could the authors further clarify the primary novelty? Is the main contribution the joint RL-distillation training strategy that adapts this compression specifically to reasoning tasks, rather than a fundamental difference in the compression mechanism itself?\n\n\n3. Variable Compression. Have the authors experimented with training a single model to handle variable compression ratios? For instance, by stochastically sampling the ratio c during training, or providing the target ratio as a conditioning input to the model?\n\n\nI would like to improve me scores if authors can solve my questions.\n\n\n[1] Zhang, Peitian, et al. \"Long context compression with activation beacon.\" arXiv preprint arXiv:2401.03462 (2024).\n\n[2] Mu, Jesse, et al. \"Learning to compress prompts with gist tokens.\" Advances in Neural Information Processing Systems 36 (2023): 19327-19352."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "usftQM310C", "forum": "LTvPnm2Oy9", "replyto": "LTvPnm2Oy9", "signatures": ["ICLR.cc/2026/Conference/Submission21176/Reviewer_iHta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21176/Reviewer_iHta"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909223950, "cdate": 1761909223950, "tmdate": 1762941572806, "mdate": 1762941572806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Breadcrumbs Reasoning (BR), a learned KV cache compression method for long-context reasoning in LLMs. The approach periodically inserts special \"beacon\" tokens every c tokens that compress information from the preceding window, allowing those KV cache entries to be evicted. The key innovation is a joint RL-distillation training scheme that trains a compression-enabled policy πBR​ by distilling from an uncompressed teacher policy πRL​ during RL training, eliminating the need for separate pretraining on general data. Evaluated on Countdown, LinSys, and StarGraph tasks using Qwen2.5-1.5B and Phi-4-Mini, the method achieves 2-32× memory reduction while retaining 67-94% of teacher performance, and demonstrates superior test-time scaling compared to training-free baselines (TOVA, StreamingLLM)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Practical training methodology: The joint RL-distillation approach is elegant and efficient. By reusing πRL​ rollouts for distillation, it adds minimal overhead (~2-3× vs. ~5-10× for two-stage approaches). The ablation (Figure 5) convincingly shows it matches or outperforms late distillation.\n\n2. Effective test-time scaling: The ability to generate 32,000 tokens (32× beyond training length) while maintaining coherence is impressive. This enables aggressive test-time compute scaling within fixed memory budgets - a practically important capability.\n\n3. Strong empirical results on applicable tasks: On Countdown and StarGraph, the method achieves excellent memory-accuracy tradeoffs. For example, on Phi-Countdown, BR reaches 0.60 accuracy with only ~100 cache entries vs. 800+ for the teacher.\n\n4. Thorough experimental design: The AUAC metric is appropriate for evaluating memory-accuracy tradeoffs. The comparison across multiple compression ratios (2-32×) provides good coverage of the design space.\n\n5. Architectural simplicity: Removing the chunk/sub-chunk hierarchy and specialized attention from prior work (Zhang et al., 2025) is a genuine contribution that improves implementability."}, "weaknesses": {"value": "1. Task-dependent brittleness: \nThe dramatic performance collapse on LinSys (91.8% → 28.9% for Qwen at c=32) is concerning and inadequately explained. The paper speculates this relates to \"structured reasoning\" but provides no analysis. This suggests the method may be fundamentally unsuitable for dense deductive reasoning where all steps are tightly coupled. Request: Provide attention analysis showing what information is lost during LinSys compression vs. Countdown. Is this a failure of the compression mechanism or the training procedure?\n2. Fixed compression rate is a major limitation: \nTraining separate models for each compression ratio is impractical. More critically, uniform compression is suboptimal - some reasoning steps (e.g., discovering key constraints) warrant preservation while others (exploratory dead-ends) can be aggressively compressed. This limitation should be prominently acknowledged as it significantly impacts practical deployment.\n3. Missing ablations:\n- Sensitivity to beacon token initialization\n- Effect of distillation coefficient (if using weighted RL + distillation loss)\n- Performance with non-uniform compression (e.g., adaptive c based on heuristics)\n4. Memory-time tradeoff underemphasized: While framed as enabling \"test-time scaling,\" the method fundamentally trades memory for time. At c=32, you generate 32× more tokens to achieve similar accuracy. Total FLOPs and latency increase proportionally. The paper should more clearly discuss when this tradeoff is favorable (memory-constrained but not latency-sensitive scenarios)."}, "questions": {"value": "1. LinSys failure mode: Can you provide deeper analysis of why LinSys fails? Specifically:\n- What information is being lost during compression? (attention weight analysis, probing classifiers)\n- Does increasing beacon token dimension help?\n- Is the issue that LinSys requires dense bidirectional reasoning that is incompatible with autoregressive compression?\n\n2. Beacon content: What information do beacons actually encode? Can you:\n- Visualize beacon embeddings (t-SNE, PCA)\n- Measure mutual information between beacons and their compressed windows\n- Show how beacon quality degrades with compression ratio\n\n3. Comparison justification: Why were other learned compression methods (AutoCompressor, GIST) not compared? Are they incompatible with the reasoning setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pxP8pqtvqb", "forum": "LTvPnm2Oy9", "replyto": "LTvPnm2Oy9", "signatures": ["ICLR.cc/2026/Conference/Submission21176/Reviewer_yRRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21176/Reviewer_yRRw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762409761199, "cdate": 1762409761199, "tmdate": 1762941571823, "mdate": 1762941571823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}