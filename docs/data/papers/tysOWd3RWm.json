{"id": "tysOWd3RWm", "number": 9704, "cdate": 1758135641883, "mdate": 1763724868259, "content": {"title": "How Effective is Your Rebuttal? Identifying Causal Models from the OpenReview System", "abstract": "The peer review process is central to scientific publishing, with the rebuttal phase offering authors a critical opportunity to address reviewers' concerns. Yet the causal mechanisms underlying rebuttal effectiveness, particularly how author responses influence final review decisions, remain unclear. In this work, we study rebuttal effectiveness through a two-layer causal analysis of ICLR submissions collected from the OpenReview system. At the structured level, we construct both metadata features (e.g., soundness, presentation) and LLM-inferred features (e.g., clarity, directness), and apply a suite of independence tests to uncover systematic associations with post-rebuttal rating changes. At the unstructured level, we model rebuttal text using a weakly supervised Causal Representation Learning (CRL) framework, where review-related features serve as concept-level supervision. Theoretically, we establish identifiability conditions for recovering human-interpretable latent features under mild assumptions. Empirically, our results uncover complementary causal patterns across structured and unstructured features, highlighting how specific rebuttal strategies shape reviewer assessments. These findings provide actionable guidance for authors in crafting more effective rebuttals, while offering broader implications for transparency, fairness, and efficiency in the peer review.", "tldr": "This paper analyzed comprehensive data from the OpenReview system to examine how rebuttal strategy causally influence reviewer rating changes.", "keywords": ["OpenReview system", "peer review", "latent causal model", "causal representation learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f21501cf96fa12f2544b4b7dc6cbf08b2aca70c1.pdf", "supplementary_material": "/attachment/12cc7637e2036f92d2b6b48660005bf9c975a9ef.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates how author rebuttals influence final review decisions by conducting a two-layer causal analysis of ICLR submissions. At the structured level, it combines metadata and LLM-inferred features to test their associations with rating changes. At the unstructured level, it applies a weakly supervised Causal Representation Learning (CRL) framework on rebuttal texts, using concept-level supervision from LLM-inferred features. The study provides theoretical identifiability guarantees and empirically reveals how specific rebuttal strategies shape reviewer assessments, offering actionable guidance for crafting more effective rebuttals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a two-layer causal analysis framework (structured + unstructured) leveraging real-world ICLR OpenReview data, and develop a weakly supervised Causal Representation Learning (CRL) model for rebuttal texts with concept-level supervision.\n\n- The integration of the two levels is well-motivated: the CRL builds directly on the structured analysis, refining noisy concept features while also discovering new latent concepts. This design enables a richer understanding of how rebuttal strategies shape reviewer assessments.\n\n- The paper provides both theoretical identifiability guarantees (Identifiability of Concepts) and empirical insights into effective rebuttal strategies.\n\n- The visualizations, especially the violin plots, effectively and intuitively illustrate the relationships between rating changes and various features. Some conclusions — such as “more rebuttal rounds tend to increase scores,” “rating decreases often correspond to increased confidence,” and “higher clarity/directness tends to yield higher score improvements” — align well with human intuition."}, "weaknesses": {"value": "- The experiments are conducted only on 8,000+ ICLR 2024–2025 papers, which raises concerns about the scale and diversity of the dataset. It remains unclear whether this is sufficient for robust generalization.\n\n- The authors employ 8 review-level, 6 paper-level, and 10 LLM-inferred features, meaning that nearly half of the features (10/24) are LLM-generated. However, the reliability and consistency of these LLM-inferred features — and how they affect downstream analysis — are not clearly justified.\n\n- The study focuses solely on textual characteristics of papers and rebuttals, without considering non-textual (e.g., visual or figure-related) information, which limits the comprehensiveness of the analysis.\n\n- The problem setting is restricted to examining the effect of rebuttal behavior on final rating changes, without accounting for the underlying paper quality. For instance, poor-quality submissions may not benefit from clear and well-written rebuttals. Such latent factors could potentially confound the causal conclusions drawn."}, "questions": {"value": "1. Why are the experiments limited to the 8,000+ papers from ICLR 2024–2025? While ICLR’s open-review transparency is understandable, why not include data from additional years to enhance robustness?\n\n2. For the LLM-inferred features, are the detailed generation settings (e.g., prompts) provided? How is the reliability of these AI-generated features ensured to support subsequent analyses?\n\n3. In Figure 3, why does concern_severity show an increase–then–decrease trend with rating_change? Does the paper provide a reasonable explanation? Could this pattern instead arise from the instability of the LLM-generated metric itself?\n\n4. I noticed the existence of a related dataset, Re2 [1], which includes multi-year, multi-conference peer review and rebuttal data. Could the authors extend their experiments to Re2 to test whether the conclusions hold across broader settings? Such an extension would significantly strengthen the paper’s robustness.\n\n5. The lack of multimodal analysis (e.g., incorporating figures or visual data) should be explicitly discussed as a limitation in the paper.\n\n6. In generating concept scores using the prompt “Don’t hesitate to give a 3 for strong, competent responses; use higher scores only for standout cases,” why did the authors adopt this conservative scoring encouragement? Wouldn’t a wider score distribution (greater discrimination) be preferable?\n\n> [1] Re2: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fVecNOnu3q", "forum": "tysOWd3RWm", "replyto": "tysOWd3RWm", "signatures": ["ICLR.cc/2026/Conference/Submission9704/Reviewer_2xmX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9704/Reviewer_2xmX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537918820, "cdate": 1761537918820, "tmdate": 1762921211914, "mdate": 1762921211914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "VwQQecYpzq", "forum": "tysOWd3RWm", "replyto": "tysOWd3RWm", "signatures": ["ICLR.cc/2026/Conference/Submission9704/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9704/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724866296, "cdate": 1763724866296, "tmdate": 1763724866296, "mdate": 1763724866296, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates what makes rebuttals effective in changing reviewer ratings, analyzing ICLR 2024-2025 submissions from OpenReview. The authors perform a two-layer analysis. First, they apply conditional independence tests to structured metadata features and 10 LLM-inferred concepts derived from rebuttal and review texts (7 and 3 resp). Second, they apply weakly supervised Causal Representation Learning (CRL) directly to review and rebuttal text, using the noisy LLM-inferred concepts as weak supervision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The peer review process is an important real-world setting, and this paper seeks to draw conclusions about the influencing factors based on analysis of a real-world dataset. The authors do perform some original analysis of the dataset and testing of different causal methods---though fail to demonstrate the generality of their findings. The paper is largely clear in its writing, though leaves out key details."}, "weaknesses": {"value": "- In Section 3, the authors make conclusions about the \"drivers of reviewer updates\" that are inappropriate based on independence tests alone -- those factors like \"persuasiveness, clarity, and responsiveness\" may be correlated rather than drivers.\n- Section 3 is also missing contextualization of how their results contribute to current knowledge (related work).\n- The introduction provides a paragraph on prior work, which is insufficient and refers to the appendix rather than elaborating in the body text as is standard.\n- \"We aggregate each paper’s reviews without accounting for the precise timestamps of individual revisions.\" - this seems like a dangerous assumption. It's crucial to ensure that predictions are conditioned only on past events when drawing conclusions about the driving factors, as is the goal of this paper.\n- The CRL analysis does not meaningfully improve the trustworthiness of the conclusions. Please see the questions about the synthetic setup, and conclusions drawn from the brief real-world section (4.5)."}, "questions": {"value": "- Why use Paper Copilot and not the ICLR data directly? How reliable is that website?\n- How are the independence tests appropriate for the features, or what processing was done to make them appropriate? The data appears ordinal or continuous which does not meet the requirements for all the tests (eg chi-square).\n- \"with disagreements resolved through discussion to produce gold-standard scores\" (p. 3): What was the rate of disagreements?\n- \"Interestingly, rating init and rating final appear independent of each other but both are dependent on rating change, suggesting that absolute ratings and their shifts capture complementary aspects of the review process.\" This doesn't seem supported by Fig 1 - please explain.\n- Why is the synthetic data generated in Section 4.4 appropriate?\n- \"the Author Unknown node, connected to openness and influencing reviewer openness, likely reflects hidden aspects of persuasiveness or tone in author responses, while the Reviewer Unknown node, linked with review quality and concern severity and directly affecting rating change, appears to\ncapture latent reviewer dispositions such as strictness or flexibility\" - what additional evidence do the authors have to back these guesses about the meaning of these variables? Have the authors performed any analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fqT9C7C2YH", "forum": "tysOWd3RWm", "replyto": "tysOWd3RWm", "signatures": ["ICLR.cc/2026/Conference/Submission9704/Reviewer_Aec3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9704/Reviewer_Aec3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874910146, "cdate": 1761874910146, "tmdate": 1762921211586, "mdate": 1762921211586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper combines dependence testing on structured/LLM-inferred features with a CRL model to uncover which rebuttal strategies causally relate to rating changes in OpenReview.\n\nThey conduct a two-layer analysis linking discourse quality features (clarity, evidence, rigor, openness, etc.) and interaction volume to rating changes, showing weak effects from static paper metadata.\n\nThey introduce a CRL framework with identifiability guarantees to recover human-aligned latent concepts from rebuttal/review text.\n\nThe learned causal graph highlights how author clarity/openness and reviewer openness/severity connect to rating changes, and it surfaces additional latent factors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Understanding what moves reviewer scores during rebuttals is valuable for authors, reviewers, and program committees. This is especially interesting as an ICLR 26 submission author!\n\nThe authors provide explicit disclosure of the LLM labeling setup, model selection (DeepSeek-R1), and permissions for dataset usage."}, "weaknesses": {"value": "The paper’s overall framing and emphasis (rebuttal effectiveness) make it seem like a meta-study rather than a core ML contribution. The causal layer feels incremental, not groundbreaking. It’s well-motivated and technically solid, but not strong enough on its own to carry an ICLR acceptance. This paper might do better in an applied ML venue or a technical blogpost track.\n\nAggregating the thread loses who responded when and what changed after which message. This could confound cause/effect timing. Even coarse timestamps or a panel model (pre/post rebuttal message windows) would strengthen the narrative."}, "questions": {"value": "Some other questions I think would be interesting to answer :\n\n1. How does reviewer confidence (both reported and gauged confidence) affect score change? Are highly confident reviewers more likely or unlikely to change scores? What confounding variables influence this decision?\n\n2. Can the LLMs predict the time invested by the reviewer in reading a paper and writing the review (i.e shallowness of the review). How does the shallowness of the review affect the chance of a rebuttal score change?\n\n3. What happens where there is a lot of variance in reviewer scores? Are reviewers incentvized to align with each other?\n\n4.  Did you identify any way to game rebuttals? For example, some authors present a summary of all rebuttal discourse for the meta reviewer to review at a glance. Some authors are overly grateful, some overly direct.  Are these influential in the final paper decision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PTX96bXUBZ", "forum": "tysOWd3RWm", "replyto": "tysOWd3RWm", "signatures": ["ICLR.cc/2026/Conference/Submission9704/Reviewer_GsGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9704/Reviewer_GsGe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882374483, "cdate": 1761882374483, "tmdate": 1762921211194, "mdate": 1762921211194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Provide a two-layer causal analysis of peer reviews, studying authors' influence to final decisions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "Did in-depth analysis of peer reviews on metadata, reviewl-level features, and LLM-inferred concepts.\nThe design and formulation of the second-layer causal framework seems well executed and grounded."}, "weaknesses": {"value": "Can't understand how the list of 10 LLM concepts is chosen. Following text, I also checked Appendix (e.g., A4.1), but still don't rationales behind it.\nMost findings from review-level metadata features seems quite obvious, not giving additional excitement, while findings from LLM-inferred features are not reliable unless human verification performed.\nI wish to learn more about the application of the trained model on real-world peer-review data and more in-depth discussion, but only visual diagram of inferred graph is given without details.\nAlso, the causal network is primarily designed by unverified concepts and their labels in synthetic data, and I don't know how much I should trust the output from the CRL model."}, "questions": {"value": "See my comments above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XEr5gbX1Ca", "forum": "tysOWd3RWm", "replyto": "tysOWd3RWm", "signatures": ["ICLR.cc/2026/Conference/Submission9704/Reviewer_6XKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9704/Reviewer_6XKW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941546778, "cdate": 1761941546778, "tmdate": 1762921210547, "mdate": 1762921210547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. Your comments on the methodology, conceptual framing, and evaluation have been invaluable in helping us better understand the limitations and future potential of this work. We will further refine the ideas and experiments based on your suggestions and hope to resubmit an improved version in the future. Thank you again for your time and detailed engagement with our submission!"}}, "id": "J5yzOohFDL", "forum": "tysOWd3RWm", "replyto": "tysOWd3RWm", "signatures": ["ICLR.cc/2026/Conference/Submission9704/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9704/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9704/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724832303, "cdate": 1763724832303, "tmdate": 1763724832303, "mdate": 1763724832303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}