{"id": "Gt4v9WBPzm", "number": 22948, "cdate": 1758337379281, "mdate": 1759896839111, "content": {"title": "HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation", "abstract": "Agentic Retrieval-Augmented Generation (RAG) is a powerful technique for incorporating external information that Large Language Models (LLMs) lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a Reinforcement Learning (RL) framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce $\\textbf{Hi}$erarchical $\\textbf{P}$rocess Rewards for Efficient agentic $\\textbf{RAG}$ (HiPRAG), a novel training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4\\% (3B) and 67.2\\% (7B), outperforming strong agentic RAG baselines. This is accomplished while dramatically improving search efficiency, reducing the over-search rate from over 27\\% in baselines to just 2.3\\% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents. We will release our code upon acceptance.", "tldr": "This work introduces HiPRAG, a RL training method that uses hierarchical process rewards to teach agentic RAG systems to search more efficiently by reducing sub-optimal behaviors like over-search and under-search.", "keywords": ["Retrieval Augmented Generation", "LLM Agent", "Agentic RAG", "Question Answering", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a378c7f5f970374f0d5a8e607e0cfb934a5a71c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The main contribution of this work is HiPRAG, a novel Reinforcement Learning training methodology that incorporates a fine-grained, knowledge-aware \"Hierarchical Process Reward\". This approach is achieved by first decomposing the agent's reasoning trajectory into discrete, parsable <step> blocks to distinguish between search and non-search steps ; it then evaluates the necessity of each search decision \"on-the-fly\" during training, using external LLM judges to identify \"over-search\" and \"under-search\" behaviors ; finally, it applies a hierarchical reward function that provides an additional \"gated process bonus\" based on the proportion of optimal steps, which is added only when both the final answer and the format are correct. Experiments show HiPRAG achieves 67.2% average accuracy (7B), outperforming strong baselines , while dramatically improving efficiency by reducing the over-search rate from over 27% to just 2.3%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.The quality of the experimental methodology is high. The paper's conclusions are based on extensive experiments across seven QA benchmarks, multiple model families (Qwen, Llama), and multiple RL algorithms (PPO, GRPO). The ablation studies in Section 5.3 are exhaustive and provide strong support for HiPRAG's key design choices.\n\n2.A key finding is that the HiPRAG model trained based on Qwen2.5-3B-Instruct + GRPO not only surpasses strong external 7B baselines like R1-Searcher++, but it also outperforms the 7B counterpart trained with the authors' baseline reward . This indicates that the authors' training methodology is a more effective path to performance gains than simply scaling the model size with conventional rewards, which has significant implications for resource-constrained research and deployment.\n\n3.The paper's presentation is very clear and well-organized, making a complex methodology easy to understand. The figures in the paper are of high quality and greatly enhance understanding."}, "weaknesses": {"value": "The Over-search Detection method  requires the policy model to perform a second, complete generation. This \"re-generation step\" effectively doubles the inference cost for each search step during the RL rollout phase. Although the authors mention that \"the re-generation step can be executed separately through batch generation\", this still represents significant training overhead. A discussion on the impact of this training method on the total training time would be valuable."}, "questions": {"value": "1.Why did the authors not provide an ablation study for λf ? How was the value of 0.2 determined? How sensitive are the model's final performance and convergence stability to the choice of λf ?\n\n2.The knowledge source used for the experiments is the 2018 Wikipedia dump , while the judge model for 'under-search' detection is gpt-5-mini, which possesses 2025-era knowledge. If the policy model provides a non-search answer that was factually correct based on the 2018 knowledge base,but is outdated according to the 2025 judge's knowledge, will the judge incorrectly flag this step as an 'under-search'? Could this \"temporal misalignment\" between the retrieval knowledge source and the judge's internal knowledge base introduce significant noisy rewards, thereby contaminating the training process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IePnSj6Xl6", "forum": "Gt4v9WBPzm", "replyto": "Gt4v9WBPzm", "signatures": ["ICLR.cc/2026/Conference/Submission22948/Reviewer_DZXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22948/Reviewer_DZXL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729833757, "cdate": 1761729833757, "tmdate": 1762942448546, "mdate": 1762942448546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the over-searching or under-searching phenomena in Retrieval-Augmented Generation, which are critical for both effectiveness and efficiency. The authors propose HiPRAG, a hierarchical reward function that supervises search or non-search decisions at each step through reinforcement learning. Specifically, this reward function comprises two key components: an over-search detection module and an under-search detection module, both implemented using two external LLMs. Extensive experiments across 7 QA datasets demonstrate the effectiveness of HiPRAG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed HiPRAG is well-motivated, effectively addressing the over-searching or under-searching issues in RAG. This reward function enhances the original outcome reward by incorporating the over-search and the under-search detection modules.\n\n\n2. This paper provides comprehensive ablation studies examining various aspects of the proposed method, including the impact of process bonus coefficients and different RL algorithm, which offer valuable insights into HiPRAG's mechanisms."}, "weaknesses": {"value": "1. The experimental evaluation lacks comparisons with the efficiency-aware RAG methods. While this paper is not the first work to consider the over-searching issue, it fails to discuss or compare with relevant prior works. Consequently, all baselines in the experiments are limited to standard R1-like RAG methods.\n\n\n2. Although the paper aimes to improve efficiency in RAG, it lacks crucial efficiency metrics such as the average number of search steps or inference time per question.\n\n\n3. Both over-search and under-search detection modules rely on the LLM-as-judge method, which may not be reliable, particularly when using the smaller LLMs. Therefore, GPT-4.1-mini and GPT-5-mini are used for the reward quality. It is worth further discussion on the influence of the different reward LLMs."}, "questions": {"value": "1. Could you provide comparisons with existing efficiency-aware RAG methods (e.g., adaptive retrieval, mechanisms)? How does HiPRAG perform relative to these approaches in terms of both effectiveness and efficiency?\n\n2. How sensitive is HiPRAG to the choice of reward LLMs?\n\n3. What is the performance degradation when using smaller/weaker LLMs for reward generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pIV5Bacwq1", "forum": "Gt4v9WBPzm", "replyto": "Gt4v9WBPzm", "signatures": ["ICLR.cc/2026/Conference/Submission22948/Reviewer_5yKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22948/Reviewer_5yKr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833841583, "cdate": 1761833841583, "tmdate": 1762942448285, "mdate": 1762942448285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HiPRAG, a RL framework for agentic RAG that introduces hierarchical process rewards to guide both accuracy and search efficiency. Instead of relying solely on outcome-based rewards, HiPRAG decomposes the trajectory into discrete steps and introduces step-level rewards to penalize over-searching and under-searching. The framework employs external LLMs to detect these inefficiencies on-the-fly and incorporates the results into a hierarchical reward formulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The hierarchical reward structure provides a novel and interpretable mechanism for optimizing process-level efficiency rather than only final-answer correctness.\n2. Unlike previous works on faithful process supervision, this paper specifically targets over-searching and under-searching issues in multi-hop retrieval, and presents a well-motivated solution.\n3. The proposed method achieves consistent improvements across different model sizes (3B, 7B) and RL algorithms (PPO, GRPO), with significantly reduced over- and under-search rates."}, "weaknesses": {"value": "1. The over-/under-search detection relies on commercial LLMs (GPT-4.1-mini, GPT-5-mini) as external judges, which raises concerns about stability, cost, and reproducibility. The impact of judgment errors is not analyzed.\n2. The on-the-fly verification by external LLMs adds training complexity, yet the paper provides no analysis of computational cost, runtime, or GPU-hour consumption.\n3. The role of $/lambda_p$ is unclear. Since the process reward is already gated by answer and format correctness (i.e., a hierarchical design), it should not directly influence final answer quality. Why does increasing $/lambda_p$ to 0.6 “over-prioritize step purity at the expense of answer correctness”? This suggests that λₚ may not be a meaningful parameter. Please clarify its actual effect on training dynamics."}, "questions": {"value": "1. How is the “Training with Over-search or Under-search Only” setting implemented? Are the ignored types treated as optimal steps in the reward computation?\n2. The ablation study section enumerates many results without clear structure; summarizing the main findings would improve readability and help highlight key insights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3FPstKT3tO", "forum": "Gt4v9WBPzm", "replyto": "Gt4v9WBPzm", "signatures": ["ICLR.cc/2026/Conference/Submission22948/Reviewer_5auK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22948/Reviewer_5auK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896390859, "cdate": 1761896390859, "tmdate": 1762942448058, "mdate": 1762942448058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Computational Time Analysis During RL Training"}, "comment": {"value": "We thank the reviewer for their detailed assessment and for raising important questions regarding the training overhead of HiPRAG during the RL training stage. Our proposed method does introduce additional steps into the RL loop, but our empirical data shows that this overhead is highly manageable. Below, we provide a breakdown of the computational cost and runtime analysis.\n\n### 1. Empirical Analysis\n\nTo address the concern about \"significant computational overhead,\" we profiled the runtime of our training pipeline using the Qwen2.5-3B-Instruct model with PPO on 8 NVIDIA A100 80GB GPUs. For the training framework we utilize veRL with vLLM as inference backend and FSDP as training backend.\n\nContrary to the concern that the method might be slow, our profiling data indicates the following breakdown for a single RL training step on average:\n\n* **Total Time per Step:** 324.55 seconds (100%).\n* **Rollout Generation:** 206.24 seconds (63.55%). This involves the several batch generations of reasoning trajectories with searches and is the most time-consuming component.\n* **PPO Update (Forward/Backward Pass):** 46.19 seconds (14.23%).\n* **Reference & Critic Inference:** 30.09 seconds (9.27%).\n* **HiPRAG Reward Assignment (Including Re-generation & API Calls):** 42.03 seconds (12.95%).\n\n**Result:** The specific overhead introduced by HiPRAG (the batched re-generation and external API calls) accounts for only 12.95% of the total training time. This is a small increase, especially when weighed against the significant gains in sample efficiency and final model performance.\n\n### 2. Methods to Reduce Training Time and Cost\n\nThe reviewer notes that the re-generation step \"effectively doubles the inference cost.\" We would like to clarify why this is not the case in practice:\n\n* **Batched Generation:** The \"re-generation\" check for over-search is performed via batch generation. We collect the queries from the rollout, aggregate them into several batches, and process the queries in each batch in parallel.\n* **Targeted Scope:** Re-generation is only triggered for search steps, not every step in the trajectory.\n* **Generation Length:** The re-generation prompts the model for a direct answer or relevant information to the search query, which is significantly shorter than the full reasoning chain generated during rollout even without actively limiting the generation length.\n* **Asynchronous API Calls:** As noted in the paper (Section 3.2), the external detection is executed separately. The batch processing of API calls to the external verifier occurs in parallel with the local model's batched re-generation, therefore effectively reducing the latency. Specifically:\n    * For over-search detection, the verification calls to external LLM judges start once the first batch has finished and continue concurrently in the later batches (for example, for re-generation batch $t$, execute batch $t-1$ verifier api calls).\n    * For under-search detection, these verification calls to external LLM judges can be executed concurrently with the re-generation of over-search detection as they are independent processes.\n* **Cost Reduction for LLM judges:** We utilize cost-effective models for verification (such as gpt-4.1-mini and gpt-5-mini as detailed in the paper). On top of that, we use a non-reasoning model for over-search detection for further cost saving, because the nature of over-search detection is checking the equivalence of the conclusion of the search step and the re-generation result, which does not require complex reasoning abilities. For under-search detection, due to the need of detecting logical error in the non-search step, a reasoning model is required."}}, "id": "0JPpgorlun", "forum": "Gt4v9WBPzm", "replyto": "Gt4v9WBPzm", "signatures": ["ICLR.cc/2026/Conference/Submission22948/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22948/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22948/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763595652133, "cdate": 1763595652133, "tmdate": 1763597143210, "mdate": 1763597143210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, a new training method, named Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), is proposed to address common inefficiencies in agentic Retrieval-Augmented Generation (RAG) systems. The authors identify two key problems: over-search (needlessly retrieving known information) and under-search (failing to retrieve necessary information, leading to errors). Current Reinforcement Learning (RL) methods rely on coarse, outcome-based rewards. The proposed method tackles this by incorporating a fine-grained, hierarchical process reward into the RL loop and achieves good results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is, in general, well written with good structure and is easy to follow.\n\n2. The hierarchical process reward formulation well balances correctness with efficiency through a gated bonus structure, which avoids the classic RL issues of over-penalizing necessary search.\n\n3. In this paper, the authors conducted various design choices with a lot of experiments with multiple models (Qwen, Llama), sizes (3B, 7B), types (base, instruct), and RL algorithms (PPO, GRPO). And the proposed method achieves good results compared to other baselines across several datasets.\n\n4. The proposed method can significantly reduce the over-search rate, which is very useful and impressive."}, "weaknesses": {"value": "1. The proposed reward mechanism relies on \"on-the-fly\" calls to external LLM judges to detect over-search and under-search during the RL training loop, which could potentially add significant computational overhead, cost, and API call latency to every training step. \n\n2. A further question to this reliance is that the LLM models may behave differently over time. Can we still keep the consistency of the proposed method?\n\n3. The method assumes the judge is accurate. If the judge mislabels a step (false positive over-search or false negative under-search), the agent may be clipped incorrectly."}, "questions": {"value": "Please check the \"Weaknesses\" and also provide comments on the following questions:\n\n1. The paper defines under-search as a factual or logical error in a non-search step. This may be a useful proxy for hallucination, but I wonder how the proposed method behaves with the other under-search situations, such as suboptimal or incomplete knowledge. If I understand it correctly, the current detector, which only checks for correctness, would not flag this as an under-search and would incorrectly reward it as an \"optimal\" non-search step.\n\n2. Can you provide more details on when the reward collapse occurs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l4bxv6muhf", "forum": "Gt4v9WBPzm", "replyto": "Gt4v9WBPzm", "signatures": ["ICLR.cc/2026/Conference/Submission22948/Reviewer_1kv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22948/Reviewer_1kv8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116587420, "cdate": 1762116587420, "tmdate": 1762942447869, "mdate": 1762942447869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Analysis of the Sensitivity to External LLM Judges and Their Errors"}, "comment": {"value": "To address concerns regarding judge stability, cost, and sensitivity to model capability, we conducted an experiment replacing our standard proprietary LLMs (GPT-4.1-mini and GPT-5-mini) with open-source models of varying capabilities, specifically the Qwen3-30B-A3B-Instruct/Thinking-2507 and Qwen3-4B-Instruct/Thinking-2507 (Instruct for over-search, Thinking for under-search). We use the greedy decoding with fixed random seed for all 4 judges to ensure stability and reproducibility. For other parameters, we maintained the exact experimental setup as our main Qwen2.5-3B-Instruct + PPO experiment.\n\n**Table 1: Summary of Performance (Avg. across 7 Benchmarks)**\n\n| LLM Judges | Avg. CEM | Avg. OSR | Avg. USR |\n| :--- | :--- | :--- | :--- |\n| Standard (GPT) | 64.1 | 4.9 | 38.1 |\n| Strong Open (30B) | 64.0 | 4.8 | 39.2 |\n| Weaker Open (4B) | 63.4 | 4.9 | 42.4 |\n\n### Analysis\n\n**1. Sensitivity of Over-Search Detection (OSR)**\nThe OSR remained nearly constant across all judge configurations. This suggests that the task of detecting semantic equivalence is robust and can be handled effectively even by smaller, non-reasoning \"Instruct\" models. The definition of redundancy does not require complex reasoning chains, making it less sensitive to model size.\n\n**2. Sensitivity of Under-Search Detection (USR)**\nThe USR showed higher sensitivity. The weaker Qwen3-4B judges resulted in a USR increase from 38.1% to 41.4%. We hypothesize that weaker judges are less rigorous in factual/logical verification, occasionally failing to flag \"hallucinated\" reasoning or premature conclusions. This allows the agent to terminate the search process earlier than optimal, leading to the slight degradation in the final accuracy (CEM).\n\n**3. Performance Degradation**\nDespite the increase in USR with weaker judges, the overall degradation is not catastrophic. This confirms that the hierarchical process reward framework itself provides a stable training signal even with sub-optimal judges.\n\n### Detailed Results by Dataset\n\nBelow we provide the detailed breakdown of performance across all seven QA benchmarks used in the paper.\n\n**Table 2: Cover Exact Match (CEM) by Dataset**\n\n| LLM Judges | NQ | TriviaQA | PopQA | HotpotQA | 2Wiki | MuSiQue | Bamboogle | Avg. |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Standard (GPT) | 65.6 | 73.9 | 62.1 | 55.6 | 69.6 | 26.0 | 32.8 | 64.1 |\n| Strong Open (30B) | 65.3 | 73.6 | 62.9 | 55.2 | 69.2 | 26.4 | 32.4 | 64.0 |\n| Weaker Open (4B) | 64.7 | 73.7 | 60.8 | 56.5 | 69.5 | 24.4 | 31.5 | 63.4 |\n\n**Table 3: Over-Search Rate (OSR) by Dataset**\n\n| LLM Judges | NQ | TriviaQA | PopQA | HotpotQA | 2Wiki | MuSiQue | Bamboogle | Avg. |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Standard (GPT) | 6.0 | 11.0 | 3.9 | 4.5 | 2.5 | 2.8 | 11.5 | 4.9 |\n| Strong Open (30B) | 5.9 | 10.8 | 3.8 | 4.8 | 2.4 | 1.7 | 11.9 | 4.8 |\n| Weaker Open (4B) | 5.7 | 9.5 | 4.2 | 4.6 | 3.9 | 2.6 | 11.3 | 4.9 |\n\n**Table 4: Under-Search Rate (USR) by Dataset**\n\n| LLM Judges | NQ | TriviaQA | PopQA | HotpotQA | 2Wiki | MuSiQue | Bamboogle | Avg. |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Standard (GPT) | 11.1 | 44.4 | 61.9 | 25.0 | 32.0 | 10.1 | 8.7 | 38.1 |\n| Strong Open (30B) | 11.9 | 48.1 | 63.0 | 22.1 | 40.4 | 10.9 | 5.3 | 39.2 |\n| Weaker Open (4B) | 10.7 | 54.0 | 60.3 | 27.2 | 45.5 | 18.5 | 10.3 | 42.4 |\n\n### Analysis of Judgment Errors and Robustness to Judge Selection\n\nTo address the reviewer's concern about the judgment errors, we performed a manual evaluation of judgment accuracy and analyzed the sensitivity of our method to these errors.\n\n**1. Judge Accuracy**\nFollowing the protocol in Appendix F.2, we sampled 200 trajectories from the test set and established human-labeled ground truth for step-level Over-Search (OS) and Under-Search (US) decisions, for all the steps in those trajectories.\n\n**Table 5: Judge Accuracy against Human Ground Truth**\n\n| External LLM Judges | Over-Search (OS) Accuracy | Under-Search (US) Accuracy |\n| :--- | :--- | :--- |\n| Standard (GPT) | 98.3% | 95.6% |\n| Strong Open (30B) | 97.5% | 94.5% |\n| Weaker Open (4B) | 96.5% | 92.5% |\n\nAs shown in Table 5, all models maintain high agreement with human judgment.\n\n**2. Impact of Judgment Errors**\nWe analyzed the correlation between judge accuracy and downstream agent performance to quantify the impact of mislabeling:\n\n* **Under-search:** The Under-search judge is the primary driver of performance variance. The main risk is a False Negative (failing to flag an under-search). This effectively disrupts the reasoning trajectory, causing the agent to stop searching prematurely, or directly leads to incorrect final results.\n* **Over-search:** Over-search mislabels typically result in a step being flagged as redundant (or not) without altering the information available to the agent. Consequently, these errors rarely change the final answer path, resulting in minimal impact on overall accuracy."}}, "id": "O7kJlXANnd", "forum": "Gt4v9WBPzm", "replyto": "Gt4v9WBPzm", "signatures": ["ICLR.cc/2026/Conference/Submission22948/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22948/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22948/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763596956885, "cdate": 1763596956885, "tmdate": 1763596956885, "mdate": 1763596956885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}