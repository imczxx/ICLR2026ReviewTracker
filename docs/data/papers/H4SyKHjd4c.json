{"id": "H4SyKHjd4c", "number": 6825, "cdate": 1757997046349, "mdate": 1759897891381, "content": {"title": "Sim2Real VLA: Zero-Shot Generalization of Synthesized Skills to Realistic Manipulation", "abstract": "Vision-Language-Action (VLA) models represent a critical milestone toward embodied intelligence in robotic manipulation. To support their training, recent research has developed high-performance simulation engines for data synthesis. However, their effectiveness is still significantly limited by the simulation-to-reality (Sim2Real) gap, as policies trained on synthetic data often fail to generalize reliably to the real world. To address this challenge, we present Sim2Real-VLA, a generalist robot control model trained exclusively on synthetic data, yet capable of transferring seamlessly to real-world manipulation tasks. Sim2Real-VLA features a dual-system architecture: a high-level planner that infers object-centered chains-of-affordances, and a low-level actor that executes and validates these plans in real time via a tokenized action space. This design filters out manipulation-irrelevant features and prioritizes motion-critical dynamics, thereby enhancing Sim2Real domain transfer. Besides, a notable advantage of Sim2Real-VLA lies in its tight integration with automated data generation for manipulation skills, eliminating the need for manual fine-tuning and enabling scalable, hands-free training. Empirical evaluations across bimanual, dexterous, and long-horizon tasks show that Sim2Real-VLA consistently outperforms previous VLA baselines under diverse real-world environments and domain shifts.", "tldr": "This paper introduces Sim2Real-VLA, a generalist robotic control model that enables zero-shot transfer from synthetic simulation to real-world manipulation tasks.", "keywords": ["Zero-Shot Sim2Real", "Vision-Language-Action (VLA) Model", "Long-horizon Manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92fe0d5c3c8e69f8d1a19ebbc8f4d5d0c0f64401.pdf", "supplementary_material": "/attachment/30b2a3862c571c3f3533b4445c181afbea86c1c5.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Sim2Real-VLA, a dual-system vision-language-action framework trained entirely in simulation that aims to zero-shot transfer to real-world manipulation. A high-level planning system predicts a chain of object-centric affordances from multi-view observations and language, while a low-level acting system executes each affordance with a tokenized action policy; the two stages are connected through object masks and validation signals. The method also introduces (i) object-oriented observation adaptation with domain randomization and (ii) a tokenized action space via DCT→quantization→BPE to compress trajectories. Experiments on six long-horizon/bimanual tasks report an average 60.8% real-world success, >35% absolute over the best baseline using only synthetic training, plus robustness under several domain gaps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a clean, end-to-end sim-only training pipeline that links an affordance-conditioned planner with a language-conditioned low-level policy via a tokenized action interface inside a single VLA backbone. While hierarchical affordance→policy decompositions are known, the consolidation of these components into a coherent recipe for zero-shot deployment is well engineered and practically useful. The problem setup and system diagram are clear, and the empirical results show sizable real-robot gains across multiple long-horizon tasks and domain-shift settings. Overall, the contribution is incremental in originality but strong in execution quality and clarity."}, "weaknesses": {"value": "- **Lack comparison to π0-FAST**, Since your “tokenized action space (DCT→quantization→BPE)” is closely aligned with π0-FAST, this comparison is essential to demonstrate benefits beyond tokenization alone.\n- **Zero-shot only; lacks a practically meaningful few-shot comparison.**\nWhile zero-shot sim→real is interesting academically, in realistic deployments collecting 5–10 real demonstrations per task is routine and inexpensive. The paper does not evaluate whether the claimed advantages persist when each method is allowed a small, equal real-data budget. Please add a few-shot finetuning study (e.g., 0/5/10 demos per task) comparing your method against π0 and π0-FAST under identical conditions. Report success vs. #demos curves, wall-clock adaptation time, and compute.\n- **Unclear advantage over non-VLA affordance→policy pipelines (e.g., AnyGrasp-style)**.\nThe paper motivates a hierarchical VLA backbone but does not isolate why VLA is needed once an affordance chain is available. A strong control would be a non-VLA baseline: a GraspNet-like affordance estimator feeding a learned policy head (language-conditioned, no VLM/VLA backbone). Compare on the same tasks and sensors, and quantify concrete advantages: (i) robustness to affordance localization errors (error injection tests), (ii) long-horizon credit assignment (success vs. subgoal depth), (iii) compositional generalization to unseen language/object combinations, and (iv) closed-loop latency/stability. Without this, it is difficult to attribute gains to VLA rather than to the affordance decomposition alone.\n- Perception at deployment is under-specified. As we know, the perception gap is one of the most challenges in sim-to-real. Object masks are trained in sim with DR, but how robust are they on real images (sensor placement, calibration, segmentation model architecture, failure modes)?\n- **“Zero-shot” and Real2Sim prior—scope needs clarification.**\nThe Real2Sim step maps descriptive observations, including teleoperation and human video trajectories, into sim. Clarify whether any real sensor frames or trajectories directly supervise the policy (vs. only configuring simulation), and ensure the “zero-shot” claim explicitly excludes fine-tuning on real data.\n\n[1] Robust grasping across diverse sensor qualities: The GraspNet-1Billion dataset. IJRR 2023  \n[2] AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains. TRO 2022"}, "questions": {"value": "- Since your “tokenized action space (DCT→quantization→BPE)” is closely aligned with π0-FAST, report the comparison to π0-FAST. Please clarify how your tokenization differs from π0-FAST.\n- **Few-shot protocol with π0 / π0-FAST.** Will you include 0/5/10 demo finetuning per task for π0, π0-FAST, and your method with identical training schedules and observation stacks? \n- **VLA vs. non-VLA affordance→policy.**\nCould you add a baseline that uses your affordance chain but replaces the VLA with a compact policy head (e.g., transformer or MLP) conditioned on language and affordance parameters, trained end-to-end? Please report: (i) zero-shot and 10-demo performance, (ii) sensitivity to synthetic→real mask noise, and (iii) latency/failure-mode breakdowns when subgoal detection drifts.\n- **Robustness & recovery from affordance errors.**\nHow does the controller behave when the affordance predictor is perturbed (e.g., ±3–5 cm pose error, partial/eroded masks)?\n- Analysis on perception at deployment. See weakness.\n- **“Zero-shot” and Real2Sim prior—scope needs clarification.** See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o3zROenXHA", "forum": "H4SyKHjd4c", "replyto": "H4SyKHjd4c", "signatures": ["ICLR.cc/2026/Conference/Submission6825/Reviewer_84R9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6825/Reviewer_84R9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749077948, "cdate": 1761749077948, "tmdate": 1762919088853, "mdate": 1762919088853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Vision-Language-Action model trained entirely in simulation. The method adopts a hierarchical architecture that decouples high-level planning from low-level control execution, and further decomposes tasks into a sequence of affordance-governed atomic subtasks. By leveraging the simulation environment for large-scale data augmentation and affordance extraction, the model mitigates the Sim2Real gap without real-world demonstrations. In the execution module, the authors introduce an arm-decoupled control design for bimanual manipulation, preventing unnecessary coupling and interference between the two arms during task execution. Real-world experiments across multiple long-horizon tasks demonstrate that Sim2Real-VLA significantly outperforms several robotic policy baselines, indicating improved robustness and generalization under domain shift."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the paper presents meaningful contributions in both Sim2Real-oriented algorithmic design and VLA architectural innovation. And the paper is well organized."}, "weaknesses": {"value": "In 4.2, the authors state that a validation model is used to determine whether the current sub-goal has been successfully achieved and whether the system should proceed to the next affordance. Yet, no further explanation is provided regarding its formulation or training procedure. A brief description of this component should be included in the appendix. \n\nIn the experimental section, most evaluated tasks are compositions of pick-and-place style atomic tasks. While the results clearly highlight the advantage of hierarchical planning in long-horizon settings, the diversity of atomic-level skills remains limited.\n\nAdditionally, further ablation studies would strengthen the work, particularly comparisons isolating the effects of several architecture designs, for example: (i) arm-decoupling versus joint learning, and (ii) the proposed action executing validation mechanism versus predefined goal-based termination conditions."}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PMAXuNCLRh", "forum": "H4SyKHjd4c", "replyto": "H4SyKHjd4c", "signatures": ["ICLR.cc/2026/Conference/Submission6825/Reviewer_6zd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6825/Reviewer_6zd7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838663241, "cdate": 1761838663241, "tmdate": 1762919088334, "mdate": 1762919088334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Sim2Real-VLA is a dual-system VLA model trained entirely based on simulation data. It designs a framework in which the high-level planner parses language instructions into a \"supply chain\", and the low-level executor tracks and verifies segment by segment through tokenized action space. Cooperate with the automated Real2Sim-SceneScaling data generation workflow. This work focuses on innovation on the data side. It first \"back-projects\" real tasks into simulation, and then uses automatic scaling + skill generation to produce large-scale simulation data. That is, although the method details of Real2Sim are adopted, the overall top-level goal is to bridge the gap between Sim2Real. The experimental scale of the paper is large and the tasks are diverse. It has achieved a success rate improvement of over 35% on real robots for six long-cycle control tasks, providing a new paradigm for zero-shot transfer from pure simulation to the real world. It is suggested that the author explicitly state in one sentence that \"Real2Sim is a means and Sim2Real is the end\", so as to better eliminate the potential misunderstanding caused by the title \"Sim2Real\" but the extensive use of Real2Sim in the article."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sim2Real-VLA introduces an affordable chain-based design that decomposes complex tasks into verifiable sub-tasks, effectively mitigating error accumulation through a conceptually novel framework.\n2. We have developed an automated data generation pipeline comprising real-to-sim projection, scene expansion, and skill generation. This pipeline eliminates the need for manual demonstration and demonstrates significant engineering potential in terms of scalability and large-scale deployment.\n3. Extensive real-world experiments were conducted, encompassing single-arm and dual-arm robotic systems, rigid and articulated structures, as well as periodic tasks of varying durations. Furthermore, multiple domain variations—including background environments, object configurations, and tabletop setups—were incorporated, thereby enhancing the robustness and credibility of the evaluation.\n4. The study compares against five recent and representative baseline methods, all of which were fine-tuned using identical simulation data to ensure a fair comparison. Additionally, attention visualization is provided to improve the interpretability of the model’s decision-making process."}, "weaknesses": {"value": "1.At the methodological level, the generation mechanism of the affordable chain length K lacks a detailed explanation, with insufficient algorithmic description and absence of statistical distribution analysis. The Real2Sim projection phase does not include a clear failure detection criterion or a defined fallback strategy. Furthermore, the tokenized action space has not been evaluated for quantization errors; the impact of codebook size and sequence length in DCT+BPE on control accuracy remains unexplored, and no reconstruction error analysis or comparisons under high-precision tasks have been provided.\n2.At the engineering reproducibility level, critical initialization details are missing in real-world experiments, including random seeds, object initial pose distributions, robotic arm joint angle initialization, and camera extrinsic calibration errors. Additionally, there is no reporting on control frequencies and latency across high-level planning, low-level action token execution, and robot joint controllers. Moreover, fault classification statistics and descriptions of the safety monitoring layer are absent.\n3.In the main experimental evaluation, statistical significance analysis is lacking. For the real-world experiment with n=20, it is recommended to report a 95% confidence interval or p-value to assess the reliability of observed performance improvements. Without such metrics, it is unclear whether the reported over 35% improvement stems from systematic gains or random initialization effects. Complementary failure case analyses should also be included to enhance interpretability and robustness assessment."}, "questions": {"value": "1. Is the length K of the affordable chain determined through offline statistical analysis or dynamically predicted in real time? To clarify the generation mechanism and selection strategy for the affordable chain length, could you provide details regarding the decision-making logic, underlying network architecture, and the distribution of K across different tasks?\n2. Have there been any instances of failure in the Real2Sim projection within the current framework? If so, what is the observed failure rate? In cases involving unsolvable inverse kinematics (IK) or semantic mismatches, how is the rollback mechanism triggered? It would be helpful to include representative failure cases along with comparative analyses before and after corrective actions.\n3. For future work, would it be possible to release the initialization configuration files used in real-world experiments—such as random seeds, object pose sampling ranges, initial joint angles, and camera extrinsic parameters? Additionally, could you report the control frequencies and measured latency values for high-level affordance reasoning, low-level action token generation, and each component of the robot joint controller? Such information would significantly enhance the reproducibility and engineering applicability of the proposed method.\n4. To strengthen the statistical validity of the results presented in Table 2 of the main experimental section, please include a 95% confidence interval or an equivalent measure of statistical significance. This would support the claim of a 35% performance improvement and ensure robust interpretation of the findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uHpWWupzRe", "forum": "H4SyKHjd4c", "replyto": "H4SyKHjd4c", "signatures": ["ICLR.cc/2026/Conference/Submission6825/Reviewer_nkJs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6825/Reviewer_nkJs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839201835, "cdate": 1761839201835, "tmdate": 1762919087783, "mdate": 1762919087783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose Sim2Real-VLA, a Vision-Language-Action (VLA) model that is trained on synthetic data but can be applied to real-world manipulation tasks. The method consists of two components: a high-level Affordance Prediction model and a low-level Action system. The proposed approach outperforms existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors tackle the relevant problem of sim2real transfer. They propose a novel approach that uses a Vision-Language-Action (VLA) model to generate affordances and structure the chain of affordances. This is an innovative idea that, in combination with the low-level Acting system, makes the proposed method effective while maintaining architectural simplicity."}, "weaknesses": {"value": "In the main part of the paper, several important aspects are missing or insufficiently explained and should be clarified to improve clarity and reproducibility.\n\nFirst, the simulation engine is only mentioned in Appendix A6 as EmbodiChain. However, there is no reference provided, and no public information about this engine appears to be available. Given the importance of the simulator in the training pipeline, the authors should include a proper citation or at least a brief technical description in the main text.\n\nSecond, while some details of the VLA model are provided in the appendix, the paper would benefit substantially from a more detailed presentation of these components in the main body. This would help readers better understand how the model operates and how it integrates with the affordance prediction and action systems.\n\nOverall, the proposed method is interesting and demonstrates promising results. I am inclined to recommend acceptance at this stage. However, I would like to emphasize that without sufficient information about the simulator and the data generation process, I would be inclined to lower the score due to concerns regarding reproducibility and transparency.\n\n\nMinor Remarks\n\t•\tLine 68: There is an extra blank space (“Sim2Real-VLA . By coupling”).\n\t•\tLine 206: Domain randomization is reintroduced, though it was already mentioned in line 201.\n\t•\tLine 272: The authors mention Byte-Pair Encoding (BPE) but do not provide a citation or further explanation. A reference or an equivalent description would improve clarity."}, "questions": {"value": "Please see Weaknesses.\n\nHave the authors considered evaluating their approach on different robot embodiments to assess generalization across hardware platforms?\n\nHave the authors considered testing the method on deformable object manipulation tasks to further demonstrate robustness and versatility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IQkfPAhDXw", "forum": "H4SyKHjd4c", "replyto": "H4SyKHjd4c", "signatures": ["ICLR.cc/2026/Conference/Submission6825/Reviewer_MEpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6825/Reviewer_MEpH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954095404, "cdate": 1761954095404, "tmdate": 1762919087428, "mdate": 1762919087428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}