{"id": "YcSvMAs6Tx", "number": 22383, "cdate": 1758330342212, "mdate": 1759896869362, "content": {"title": "Blessings of Many Good Arms in Multi-Objective Linear Bandits", "abstract": "Multi-objective decision-making is often deemed overly complex in bandit settings, leading to algorithms that are both complicated and frequently impractical. In this paper, we challenge that notion by showing that, under a novel *goodness of arms* condition, multiple objectives can facilitate learning, enabling simple near-greedy methods to achieve sub-linear Pareto regret. To our knowledge, this is the first work to demonstrate the effectiveness of near-greedy algorithms for multi-objective bandits and also the first to study the regret of such algorithms for parametric bandits in the absence of context distributional assumptions. We further introduce a framework for *objective fairness*, supported by strong theoretical and empirical evidence, illustrating that multi-objective bandit problems can become both simpler and more efficient.", "tldr": "We study unexpected benefits that arise from multiple objectives.", "keywords": ["multi-objective", "exploration-free", "linear bandit"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96898d9afb6897485560a54180998771bb9ab07b.pdf", "supplementary_material": "/attachment/560106e360ef35b6cc95d5e5395a2b1eb9cfc197.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the multi-objective linear bandit (MOLB) problem, challenging the notion that multiple objectives necessarily lead to complex algorithms. The authors introduce a \"goodness of arms\" condition, which assumes that for each objective, there exists at least one arm that performs sufficiently well. Under this core assumption, the paper proposes a simple near-greedy algorithm, MOG, which cycles through objectives and selects the best arm for the current target objective. The authors provide theoretical analysis showing that MOG achieves sub-linear $\\tilde{\\mathcal{O}}(\\sqrt{T})$ Pareto regret, notably without relying on context diversity assumptions. Furthermore, the paper introduces a new \"objective fairness\" metric to ensure that the algorithm does not neglect any single objective, and proves MOG satisfies this criterion."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-motivated, addressing the high computational complexity of many existing multi-objective bandit algorithms that require, for example, repeatedly updating an empirical Pareto front. The goal of finding a simpler, more efficient algorithm is highly relevant."}, "weaknesses": {"value": "- The proposed MOG algorithm, which greedily optimizes one objective at a time in a round-robin fashion, is fundamentally unsuited for multi-objective optimization. The very purpose of MOLB is to find arms on the Pareto front that represent a compromise between conflicting objectives. The MOG algorithm is structurally incapable of this. For instance, in a simple 2-objective problem with optimal arms [1, 0], [0, 1], and [0.8, 0.8], MOG will only pull [1, 0] (when $m=1$) and [0, 1] (when $m=2$). It will never identify or pull the [0.8, 0.8] arm, which is arguably the most satisfactory and practical solution. The algorithm is not \"rational\" from a multi-objective standpoint; it is simply $M$ non-communicating single-objective algorithms.\n- The paper's entire analysis, and the only reason the flawed MOG algorithm \"works\" (i.e., achieves low regret), hinges on the \"Goodness of Arms\" (GoA) assumption (Definition 5). This assumption essentially posits that for each objective $m$, there is an arm $i$ that is already \"good enough\" for it. This is problematic as it bypasses the core challenge of finding trade-offs by assuming \"good\" solutions for each objective already exist. Furthermore, it replaces the standard (and strong) \"context diversity\" assumption with an equally strong, if not stronger, \"problem structure\" assumption. The paper's claim to be the first to work \"without any diversity assumptions\" is misleading, as it has simply swapped one restrictive assumption for another.\n- The adaptation of \"Objective Fairness\" is not a strength but a further weakness. This metric merely verifies that the algorithm pays attention to each objective, which MOG does by design (due to its round-robin nature). However, as the [0.8, 0.8] arm example shows, an algorithm can satisfy this fairness criterion while completely failing to find the most rational multi-objective compromise. The metric is designed to make this irrational algorithm look successful, but it does not measure what actually matters in this problem.\n- The algorithm itself (greedy round-robin with OLS) is not novel. The paper's contribution is its analysis. But since this analysis only holds under the restrictive GoA assumption, which in turn only justifies a conceptually flawed algorithm, the contribution is negligible. The paper does not show that \"a simple algorithm can solve MOLB\"; it shows that \"if the MOLB problem is structured to be trivially easy (per GoA), a simple (and flawed) algorithm works.\" This is not a significant result for ICLR."}, "questions": {"value": "- The entire analysis hinges on the $\\gamma$-goodness assumption. How does the algorithm's Pareto regret degrade as $\\gamma \\rightarrow 0$? The practical utility of this algorithm seems entirely dependent on this problem-specific parameter, which is not explored.\n- The paper states Assumption 2 (that $\\theta_m^*$ span $\\mathbb{R}^d$) is without loss of generality 18, relaxable to spanning the feature space19. What happens if this assumption is not met, which is a very practical scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wfax4mG6X8", "forum": "YcSvMAs6Tx", "replyto": "YcSvMAs6Tx", "signatures": ["ICLR.cc/2026/Conference/Submission22383/Reviewer_JpnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22383/Reviewer_JpnB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562602565, "cdate": 1761562602565, "tmdate": 1762942192708, "mdate": 1762942192708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates multi-objective stochastic linear bandits with finite arms, contributing a novel approach by introducing objective fairness for multi-objective bandits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on a central issue in multi-objective optimization. By enhancing decision diversity, it advances the resolution of multi-objective optimization problems."}, "weaknesses": {"value": "1. How to initialize $\\{\\beta_1, \\dots, \\beta_M\\}$? \n2. The assumption $||\\theta_m^\\*|| = 1$ or  $\\ell\\leq ||\\theta_m^*||$ is not common in single-objective or multi-objective bandit literature.\n3. The assumption that $\\theta_1^\\*, \\ldots, \\theta_M^\\*$ span $\\mathbb{R}^d$ is not common.\n4. Since maximizing a single objective can make the Pareto suboptimality gap vanish, alternating the maximization across objectives appears to be a natural approach under the proposed fairness definition. This raises the impression that the fairness metric might have been designed to align with the proposed algorithm. Could the authors clarify the design order and motivation behind the fairness definition?\n5. The paper claims that multiple objectives can facilitate learning rather than hinder it. However, it is unclear where this viewpoint is theoretically or empirically demonstrated. Please point to the specific results."}, "questions": {"value": "see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqACEpkwEm", "forum": "YcSvMAs6Tx", "replyto": "YcSvMAs6Tx", "signatures": ["ICLR.cc/2026/Conference/Submission22383/Reviewer_y4Tp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22383/Reviewer_y4Tp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882021233, "cdate": 1761882021233, "tmdate": 1762942192522, "mdate": 1762942192522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers the multi-objective goal for the multi-armed bandit problem. There are $k$ arms and $m$ objectives, with each objective having a different mean reward for a given arm. In such settings, there is no clear objective one should aim to maximize; instead there are various metrics that are better fitted for various settings/contexts. This paper proposes a new metric, defined as the objective fairness index. Essentially it looks to ensure that for each objective, the optimal arms for that objective is selected a constant fraction of the time. The work gives algorithms for this objective, analyzes the corresponding regret bounds, and connects it with the literature on pareto front objectives.\n\n**Strengths:** I think the paper is relatively well written and well motivated. I have some mixed opinions on the proposed objective (see below), but I think if one takes this as a given, the suite of results presented is fairly complete. Upper and lower bounds on the regret and fairness objective achieved by the proposed algorithm are presented. The $\\sqrt{T}\\log(T)$ upper bound is essentially enough for algorithm agnostic regret lower bound. While the algorithm is simple, the technical analysis does require some work.\n\n**Weakness:** The proposed objective is essentially an egalitarian objective -- we care about the welfare of the worst off arm. But I'm not note why the authors don't directly use the egalitarian objective as opposed to strange binary metric of whether an arms was sufficiently close the best arms of the objective? This can exacerbate the issues of egalitarian fairness more - namely, the *best* arm for any agent may be extremely bad for all the other agents. It thus makes sense to choose arms that are more reasonable to more participants. Given the nature of objective, as defined, the chosen algorithm is thus not unsurprising. Choose the best arm for agent in round robin fashion. Can you please justify this?\n\nI also think the work is missing some important connections to past literature. Multi-objective bandits have matured beyond standard Pareto front goals. I especially think that a discussion on Nash Welfare as an objective (which balances utilitarian and egalitarian objectives) is richly warranted. This has been studied in multi-objective bandits; see [1,2,3].\n\n[1]: Hossain, Safwan, Evi Micha, and Nisarg Shah. \"Fair algorithms for multi-agent multi-armed bandits.\" Advances in Neural Information Processing Systems 34 (2021): 24005-24017\n\n[2]: Barman, Siddharth, et al. \"Fairness and welfare quantification for regret in multi-armed bandits.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 6. 2023.\n\n[3]: Zhang, Mengxiao, Ramiro Deo-Campo Vuong, and Haipeng Luo. \"No-regret learning for fair multi-agent social welfare optimization.\" Advances in Neural Information Processing Systems 37 (2024): 57671-57700."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "See above"}, "weaknesses": {"value": "See above"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ui14vFRcQy", "forum": "YcSvMAs6Tx", "replyto": "YcSvMAs6Tx", "signatures": ["ICLR.cc/2026/Conference/Submission22383/Reviewer_adWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22383/Reviewer_adWY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929102329, "cdate": 1761929102329, "tmdate": 1762942192222, "mdate": 1762942192222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies multi-objective linear bandits with $M$ objectives and proposes $\\mathrm{MOG}$, a near-greedy algorithm that cycles objectives: before an OLS “estimation” phase it pushes the Gram matrix’s minimum eigenvalue above a threshold $B$ (i.e., until $\\lambda_{\\min}(V_{t-1}) \\ge B$), then plays greedy per objective in round-robin; two variants (randomized $\\mathrm{MOG}$-$\\mathrm{R}$ and weighted-randomized $\\mathrm{MOG}$-$\\mathrm{WR}$) are also introduced. The central structural assumption is the existence of *$\\gamma$-good arms* near each objective direction (Assumption 3), which implies linear growth of $\\lambda_{\\min}$ of the Gram matrix once residuals are small. The paper claims $\\widetilde{O}(\\sqrt{dT})$ Pareto regret for $\\mathrm{MOG}$ (and variants) and an objective-fairness guarantee showing each objective is served at asymptotic frequency $1/M$ (or $p_m$ for $\\mathrm{MOG}$-$\\mathrm{R}$), with matching $\\Omega(\\sqrt{dT})$ lower bound."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **A nice observation**: if each objective has many $\\gamma$-good arms, the per-objective greedy steps jointly grow the design’s $\\lambda_{\\min}$ linearly; Lemma 2 formalizes this and enables $\\widetilde{O}(\\sqrt{dT})$ regret.\n- **A new fairness index**: the authors propose a new fairness index that does not necessitate computing the empirical Pareto front in each iteration; the objective fairness index bound shows $\\lim_{T\\to\\infty}\\mathrm{OFI}_{\\varepsilon,T}=1/M$ (or $\\min_m p_m$ for $\\mathrm{MOG}$-$\\mathrm{R}$)."}, "weaknesses": {"value": "- **“Free exploration’’ vs explicit gate $B$**: the algorithm enforces an initial phase until $\\lambda_{\\min}(V_{t-1}) \\ge B$, which is a horizon-dependent exploration cost; the “no/low exploration” narrative is misleading without a horizon-free tuning rule for $B$--the authors are **forcing** exploration at the beginning!\n- **Assumption 3**: First, a lower bound on the parameters is an unusual assumption to have. Second, in Assumption 3, for intuition, say $\\alpha=0$. This automatically ensures a lower bound on parameter norms. This seems strong.\n- **Underspecified $T_0$**: $T_0$ (rounds to reach $\\lambda_{\\min}\\ge B$) is not algorithmically guaranteed to be sublinear; without a diversity mechanism, the pre-$B$ phase may fail to span $\\mathbb{R}^d$, making $T_0$ large (even $\\Theta(T)$).\n- **Hidden $\\alpha$-dependence**: Theorem 1’s displayed regret omits explicit $\\alpha$-dependence, yet $B=B(T,\\alpha,\\sigma,d)$ and the additive $4T_0$ inherit it; please state this dependence or bound $T_0(\\alpha,\\gamma,\\lambda)$ explicitly.\n- **Lemma 2**: First, the bound on estimation error should be stochastic. Second, the assumption $||\\hat\\theta_m-\\theta_m^\\ast||\\le \\alpha$ after $T_0$ is not ensured by the stated selection rule in the pre-$B$ phase. Third, the statement says that the estimation error should be less than $\\alpha$, but the stated result does not depend on $\\alpha$. The way the result is stated lacks rigorous and clarity.\n- **Corollary 1 premise external to algorithm**: it assumes the initial feature set spans $\\mathbb{R}^d$, but the algorithm does not enforce this; the resulting $O(\\log T)$ claim for $T_0$ is conditional rather than guaranteed.\n- **Ambiguity in “learning’’ / “simpler solutions’’**: early claims (in the introduction) are qualitative and somewhat opaque--necessitating a clearer exposition.\n- **Role of $\\alpha$ in “goodness’’**: unclear whether $\\alpha$ is an exogenous geometric margin (property of the arm set) or an endogenous estimation tolerance.\n- **Motivation for $\\mathrm{MOG}$-$\\mathrm{R}$ unclear**: the concrete drawback addressed (non-uniform shares, stochastic-context stability, or cycling artifacts) is not crisply articulated.\n- **Horizon coupling of $B$**: $B$ is specified in theorems as a function of $T$ but presented in algorithms as a given threshold; include the explicit selection rule (or a doubling scheme) in the algorithmic description."}, "questions": {"value": "- **Pre-$B$ phase diversity**: Can you provide a constructive rule (and guarantee) ensuring that the set $S$ collected while $\\lambda_{\\min}(V_{t-1})<B$ spans $\\mathbb{R}^d$ with a quantitative lower bound on $\\lambda_{\\min}$? This would make Corollary 1 a consequence of the algorithm rather than an external condition.\n- **Horizon-free $B$ (minor)**: How would you tune $B$ without knowing $T$? Is a doubling-trick analysis feasible without worsening the leading $\\widetilde{O}(\\sqrt{dT})$ term?\n- **$\\alpha$ as assumption vs target**: Is $\\alpha$ part of the arm-set geometry (i.e., an exogenous margin in the definition of “goodness’’) or an endogenous estimation tolerance achieved once $\\lambda_{\\min}\\ge B$? Where exactly does $\\psi(\\lambda,\\gamma)$ enter operationally?\n- **Role of $\\mathrm{MOG}$-$\\mathrm{R}$**: What concrete failure mode of $\\mathrm{MOG}$ does $\\mathrm{MOG}$-$\\mathrm{R}$ address (e.g., non-uniform target shares $p_m$, stabilization under stochastic contexts, mitigation of cycling artifacts)? A small illustrative example would help.\n- **“Free exploration’’ claim**: In what formal sense is exploration “free’’ if the algorithm requires an initial gate $\\lambda_{\\min}\\ge B$ with $B=B(T,\\alpha,\\sigma,d)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cvKxfFB9cb", "forum": "YcSvMAs6Tx", "replyto": "YcSvMAs6Tx", "signatures": ["ICLR.cc/2026/Conference/Submission22383/Reviewer_rDEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22383/Reviewer_rDEZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934501064, "cdate": 1761934501064, "tmdate": 1762942191964, "mdate": 1762942191964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}