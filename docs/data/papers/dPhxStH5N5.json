{"id": "dPhxStH5N5", "number": 12907, "cdate": 1758211525578, "mdate": 1759897477443, "content": {"title": "An Efficient Framework for Length Extension via Dynamically Growing Positional Embedding and Correlation-Aware Routing Attention", "abstract": "Modeling long sequences is critical for numerous large-scale models. However, extending existing architectures to handle significantly longer sequences poses substantial technical and computational challenges. One inevitable issue is the overfitting of large models to positional encodings during pretraining, which limits their ability to generalize to unseen positional encoding scales. Additionally, extending sequence lengths requires extensive computational resources and time. Existing positional encoding methods often rely on carefully designed scaling factors but typically yield suboptimal results. To tackle these challenges, we propose \\textbf{Cyclic, Randomly Truncated, and Dynamically Growing NTK Positional Embedding (CRG NTK)}, a data-augmentation-based technique that fully explores the RoPE encoding space, enabling models to adapt to various positional scales and achieve state-of-the-art extrapolation for the extension of lengths dominated by position encoding. Furthermore, we introduce \\textbf{an efficient attention mechanism with a correlation-based routing strategy to enhance the fitting of the augmented positional encoding}, yielding superior performance and more efficient fine-tuning. With our approach, LLaMA-7B and Mistral-7B fine-tuned at 16K context length achieve extrapolation factors of at least 128$\\times$ on simple tasks and maintain stable perplexity over 32$\\times$ sequence length extensions and saves at least 16 times the GPU training resources compared to the existing optimal method. Experiments also show that correlation routing can achieve good performance by further filtering out large amounts of noise in long sequences.", "tldr": "", "keywords": ["Length Extension", "Positional Embedding", "Efficient Attention"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76e08ebb4ab4d254315dac35a5e7bcd87cb3ff65.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes CRG NTK for the length extrapolation. This work additionally designs an efficient attention mechanism with a correlation-based routing strategy to enhance the fitting of the augmented positional encoding. The experiment is conducted with LLaMA-7B and Mistral-7B, achieving extrapolation 128 times the original length."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The length extrapolation problem is important. This work focuses on length extrapolation, which is important for long-text and reduces training cost.\n* The Merge Selection method is relatively interesting. With MS Attention, the model could better process long context.\n* The experiment results support that the method could extrapolate. For example, Table 3 presents that the model trained at 16K could extrapolate to length of 32 K."}, "weaknesses": {"value": "* The major concern is the novelty of CRG NTK. It is not clear whether the method compared with other baselines, such as PoSE. Both this method and PoSE select a large maximum length during training.\n* Without MS Attention, it seems that the CRG-NTK cannot extrapolate well. For example, in Table 1, the CRG-NTK trained on 16K, the ppl increases on length 32K. Similary, the CRG-NTK trained on 64K, the ppl increases on length 128K.\n* For Figure 1, it is better to use pdf or SVG for higher resolution\n* In Table 4, the LLaMA2-7B-MS performance is worse than original LlaMA-7B."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rXb1VwB54n", "forum": "dPhxStH5N5", "replyto": "dPhxStH5N5", "signatures": ["ICLR.cc/2026/Conference/Submission12907/Reviewer_eogd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12907/Reviewer_eogd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761395859663, "cdate": 1761395859663, "tmdate": 1762923683953, "mdate": 1762923683953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework to efficiently extend LLM context windows. It proposes two key components: 1) CRG NTK, a data-augmentation-based positional encoding strategy (using random shifts, cyclic mapping, and a staged scaling curriculum) to improve extrapolation, and 2) MS Attention, an efficient, correlation-based sparse attention mechanism that routes queries to relevant Key/Value segments to reduce noise and compute. The authors report state-of-the-art extrapolation results (32x on perplexity, 128x on passkey) from short 16K fine-tuning, claiming a 16x reduction in GPU resources. The paper also hypothesizes a link between model depth and extrapolation limits."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper has a clear goal to efficiently and effectively extending LLM's context window. The authors propose a new RoPE extrapolation method \"CRG NTK\" and new efficient fine-tuning method \"MS Attention\". \n2. The paper proposes and provides a novel hypothesis that the model's extrapolation capacity is linked to its layer depth."}, "weaknesses": {"value": "1. The paper is very difficult to read. The descriptions of the core methods, especially CRG NTK in Section 3.1, are dense, and the writing is often convoluted. The contribution of position extrapolation and efficient fine-tuning can be considered as two studies instead of one work which will confuse readers. \n2. The experiments are not sufficient to support the claims. 1) In Table 5, looks like Yarn has the best performance but we cannot see the score in Table 1 and 2. 2) Passkey retrieval and perplexity cannot directly reflect extrapolation. 3) It is not clear whether MS Attention will have degradation compare to Full Attention. \n3. The description of MS Attention (Section 3.2) is complex, and its performance seems highly sensitive to several key hyperparameters (segment size, topk, merge factor). Table 10 shows a perplexity swing from 7.17 to 6.09 based on these settings. This sensitivity implies a costly and difficult hyperparameter search is required to achieve the reported results."}, "questions": {"value": "1.  Please provide ablation results (Table 2,3,5) for a model fine-tuned using CRG NTK with Full Attention? This is essential to isolate the contribution of your positional encoding strategy from your efficient fine-tuning method.\n2. Please clarify the exact \"Dynamically Growing\" schedule used in your experiments? Is it a discrete, staged curriculum? If so, how many stages were used, what was the scaling factor $a$ at each stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sEQmoASBrd", "forum": "dPhxStH5N5", "replyto": "dPhxStH5N5", "signatures": ["ICLR.cc/2026/Conference/Submission12907/Reviewer_Taqr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12907/Reviewer_Taqr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794300874, "cdate": 1761794300874, "tmdate": 1762923683631, "mdate": 1762923683631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a unified approach to efficiently extend large language model (LLM) context windows. It proposes a Cyclic, Randomly-truncated, and Dynamically Growing NTK-aware (CRG-NTK) positional encoding method that progressively scales position frequencies during fine-tuning, improving extrapolation up to 32× the training length—bounded by model depth. The framework also includes a Merge-and-Select (MS) routing attention mechanism that filters irrelevant tokens to suppress noise and reduce computation. Together, these techniques achieve up to 128× extrapolation on retrieval tasks while requiring 16× less GPU time than LongRoPE, offering a practical, theory-supported path to scalable long-context adaptation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The method is simple and in general PE recipe. CRG-NTK unifies several effective heuristics, including random shifts, cyclic truncation, and scheduled scale growth, into a single augmentation that is easy to integrate into standard fine-tuning.\n\n(2) The experiments are comprehensive."}, "weaknesses": {"value": "(1) The paper notes using a fixed base scaling at inference after training through multiple scales. Please analyze failure modes when test lengths fall between trained scales, and whether per-layer scale interpolation helps.\n\n(2) The paper hypothesizes an extrapolation limit $\\approx$ number of layers but gives only sketch intuition and empirical suggestion. This is interesting. Could you please either formalize a brief statement (even under simplifying assumptions) or clearly mark it as conjecture?"}, "questions": {"value": "(1) Why power-law growth? Did you compare power-law, exponential, and additive scale growth in sample efficiency and forgetting? Any signs of catastrophic interference at very large steps?\n\n(2) Does CRG-NTK help more in early vs late layers? Any merit to layer-dependent scales or per-head scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BpIB7lyaJ5", "forum": "dPhxStH5N5", "replyto": "dPhxStH5N5", "signatures": ["ICLR.cc/2026/Conference/Submission12907/Reviewer_Fnaw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12907/Reviewer_Fnaw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804187669, "cdate": 1761804187669, "tmdate": 1762923683273, "mdate": 1762923683273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simple, compute-light recipe to extend LLM context windows far beyond training lengths by (1) augmenting positional encodings via Cyclic, Randomly-truncated, and Dynamically-Growing NTK (CRG-NTK) and (2) replacing full attention during fine-tuning with a relevance-routed sparse attention, Merge–Select (MS) Attention. With only 16K-context fine-tuning on a single A100, LLaMA-7B/Mistral-7B extrapolate to millions of tokens on synthetic retrieval and maintain stable perplexity over ≥32× longer sequences, while cutting fine-tuning compute by ≥16× vs. strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the CRG-NTK framework, a creative combination of cyclic shifts, random truncation, and dynamic NTK scaling that transforms positional encoding from a static component into a data-augmentation process.  \n\n- The Merge–Select Attention mechanism combines correlation-based routing with segment merging—an synthesis of sparse attention and dynamic selection ideas (from Routing Transformer and BiFormer) that yields efficient architecture.  \n\n- Proposing that extrapolation limits scale with network depth offers a theoretical perspective."}, "weaknesses": {"value": "- The proposed approach is overly complex, yet lacks detailed ablation studies for each component, making it difficult to determine its effectiveness.\n\n- It is unclear how the theoretical gradient derivation leads to the conclusion that having more layers results in better extrapolation capability.\n\n- Considering that synthetic retrieval tasks and simple perplexity[2] measurements cannot effectively evaluate true extrapolation capability, could you provide results on some reasoning tasks, such as Many-Shot In-Context Learning[1]?\n\n- As far as I know, there also exist some **training-free approaches[3]** that achieve length extrapolation when combined with sparse attention. Could you compare your method against these?\n\n\n\nRef:  \n[1] Many-Shot In-Context Learning.  \n[2] CAN PERPLEXITY REFLECT LARGE LANGUAGE MODEL’S ABILITY IN LONG TEXT UNDERSTANDING ?  \n[3] Parallel Long-Context Compressor for Length Extrapolation"}, "questions": {"value": "- Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9rdkYAuv89", "forum": "dPhxStH5N5", "replyto": "dPhxStH5N5", "signatures": ["ICLR.cc/2026/Conference/Submission12907/Reviewer_ZcCX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12907/Reviewer_ZcCX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804459135, "cdate": 1761804459135, "tmdate": 1762923683004, "mdate": 1762923683004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}