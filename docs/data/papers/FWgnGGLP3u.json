{"id": "FWgnGGLP3u", "number": 10418, "cdate": 1758170847552, "mdate": 1762945636509, "content": {"title": "Nestor: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training", "abstract": "Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on neural operators. To address these challenges, we propose a large-scale PDE pre-trained neural operator based on a nested Mixture-of-Experts (MoE) framework. In particular, the image-level MoE is designed to capture global dependencies, while the token-level Sub-MoE focuses on local dependencies. Our model can selectively activate the most suitable expert networks for a given input, thereby enhancing generalization and transferability. We conduct large-scale pre-training on twelve PDE datasets from diverse sources and successfully transfer the model to downstream tasks. Extensive experiments demonstrate the effectiveness of our approach.", "tldr": "", "keywords": ["Pre-trained Neural Operators", "Mixture-of-Experts (MoE)", "PDE Modeling", "Operator Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8dbc952c117b9d6bffd7ff160d04d80fe1746abc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents NESTOR, a large-scale pre-trained neural operator based on a nested Mixture-of-Experts (MoE) framework. It designs a image-level MoE to capture global dependencies and a token-level Sub-MoE for local dependencies. A large-scale pre-training experiment is conducted on 12 PDE datasets collected by DPOT to evaluate the model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall easy to follow. The idea is easy to understand.\n- The model architecture design is overall well motivated. The MoE design is a reasonable approach for large-scale heterogeneous PDE datasets pre-training.\n- The experimental results demonstrates the architecture's effectiveness over existing methods. The fine-tuning experiments are also interesting and show the model's transfer learning capability."}, "weaknesses": {"value": "- It seems that this paper is finished in a hurry without careful proofreading. In Figure 1 \"PDF's Diversity\" \"PDF's Complexity\", which I think it should be \"PDE\". The citation formatting is also irregular throughout this paper.\n\n- Though designed carefully, the technical contributions regarding the model architecture and the task loss are somewhat limited.\n\n- It seems that the performance gain over the baseline is relatively modest; 6 of 14 metrics of your model does not exceed the baseline. As the MoE design targets better processing of diverse and complex PDE datasets, this results is not promising. \n\n- Nor have you reported the model parameter numbers, as DPOT has provided results of different scales, so comparison of models with different scales of parameters is not fair. A recently published paper \"Unisolver\" [1] also includes DPOT as its baseline and achieves better results. You should consider compare with it as well.\n\n  [1] Hang, Zhou, et al. \"Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE Solvers.\" *ICML 2025*. 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1jg1PdcBDB", "forum": "FWgnGGLP3u", "replyto": "FWgnGGLP3u", "signatures": ["ICLR.cc/2026/Conference/Submission10418/Reviewer_Vrjs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10418/Reviewer_Vrjs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761211803921, "cdate": 1761211803921, "tmdate": 1762921727163, "mdate": 1762921727163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "atoyAvYo1z", "forum": "FWgnGGLP3u", "replyto": "FWgnGGLP3u", "signatures": ["ICLR.cc/2026/Conference/Submission10418/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10418/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762945635753, "cdate": 1762945635753, "tmdate": 1762945635753, "mdate": 1762945635753, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NESTOR, a nested Mixture-of-Experts (MoE) neural operator designed for large-scale PDE pretraining. The model combines an image-level MoE (for global PDE diversity) with a token-level Sub-MoE (for local spatial complexity), integrating AFNO and FlashAttention experts under hierarchical routing. Experiments on twelve PDE datasets show strong in-distribution performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work presents one of the first successfully trained Mixture-of-Experts (MoE) architectures in the context of foundation models for PDEs, marking a valuable step toward scalable, modular operator learning.\n\n- Despite the complexity of the nested MoE design (image-level and token-level routing), the authors demonstrate that the model can be trained stably and efficiently across a large and diverse collection of PDE datasets.\n\n- The paper shows attention to optimization stability, incorporating dedicated load-balancing losses at both MoE levels to ensure fair expert utilization and prevent collapse.\n\n- The authors include targeted ablation studies examining the contribution of key components, namely the Sub-MoE, the load-balancing term, and the fusion strategy between AFNO and FlashAttention, providing at least partial insight into which parts of the architecture matter most."}, "weaknesses": {"value": "- The paper does not reference or compare against current state-of-the-art foundation models for PDEs, such as Poseidon [1], which already explore large-scale pretraining and transfer to unseen downstream tasks.\n\n- In PDE settings, the concept of a temporal “frame” (Eq. 2) is ambiguous. A specific time step Δt must be fixed to define the “next frame,” which makes the model resolution-dependent. Once the temporal resolution is changed or sub-sampled, the model becomes inapplicable. Moreover, different PDEs evolve on very different temporal scales, so mixing them within one model is problematic and not clearly addressed.\n\n- The authors claim that Transformers “struggle to effectively represent kernel integral operators” (lines 204–205), which is inaccurate. Several recent works [1, 2, 3] demonstrate that Transformers can approximate such operators effectively.\n\n- The core motivation of a foundation model is largely missed in this paper. Although the authors perform large-scale pretraining on a mixture of PDE datasets, the model is then evaluated and fine-tuned on exactly the same distributions. This setup does not align with the goal of foundation models, which is to learn broadly transferable representations that can generalize to new, unseen tasks and data domains. In real-world scientific and engineering problems, new data will almost never follow the same distribution as the pretraining datasets, so true foundation models must demonstrate out-of-distribution transfer and robustness to domain shifts. Here, the model is fine-tuned only on datasets that were already part of the pretraining pool, which effectively measures continued training rather than transfer learning. By contrast, prior works such as Poseidon [1] explicitly evaluated pretraining on one set of PDEs and transfer to multiple unseen downstream tasks, thereby revealing both the strengths and limitations of generalization. In the current paper, only a single transfer-learning experiment (the turbulence task) is provided, which is insufficient to demonstrate general-purpose or cross-physics generalization. Much stronger evidence, such as systematic transfer to unseen PDE types, new geometries, or distinct physical regimes, would be needed to substantiate the claim that the proposed MoE architecture contributes meaningfully toward foundation models for PDEs.\n\n- There is no study of scaling behavior with respect to model size or data size at the transfer-learning level. Such experiments are essential for assessing the efficiency and scalability of foundation models. Only one scaling experiment with respect to data size (on the pretraining dataset level) is provided.\n\n- Ablation studies on expert-utilization statistics are missing. It remains unclear whether the nested MoE routing actually utilizes multiple experts or collapses to a single expert. Quantitative utilization metrics would be necessary to understand the model’s internal dynamics and validate the effectiveness of the MoE design.\n\n- Figure 1 is very cluttered and hard to follow. It is unclear where the token-level and image-level experts are applied and how they are fused. The distinction between “shared,” “non-shared,” and “Sub-MoE” experts is also unclear. A clearer architectural figure with labeled modules (perhaps merging Figures 1 and 2) would significantly improve readability. Additionally, in the section 3.3, the notation for H and W is confusing: sometimes these represent original image dimensions, and other times latent dimensions after patchification.\n\n- The authors never report:\n\n(1) total parameter count,\n\n(2) active parameters per token/image\n\n(3) memory/computation cost relative to dense baselines (e.g., FNO, DPOT).\n\nEven though the overall direction of this work is promising and the proposed nested MoE design could be an interesting step toward scalable PDE modeling, the current experimental evidence is far from sufficient to support its usefulness in true foundation model settings\n\n___\n\n[1] Herde, M., Raonic, B., Rohner, T., Käppeli, R., Molinaro, R., de Bézenac, E., & Mishra, S. (2024). Poseidon: Efficient foundation models for pdes. Advances in Neural Information Processing Systems, 37, 72525-72624.\n\n[2] Li, Z., Meidani, K., & Farimani, A. B. (2022). Transformer for partial differential equations' operator learning. arXiv preprint arXiv:2205.13671.\n\n[3] Ovadia, O., Kahana, A., Stinis, P., Turkel, E., Givoli, D., & Karniadakis, G. E. (2024). Vito: Vision transformer-operator. Computer Methods in Applied Mechanics and Engineering, 428, 117109."}, "questions": {"value": "- How does the nested routing mechanism behave when multiple PDE types exhibit overlapping spatial or temporal statistics—does the router consistently assign similar experts, or does it learn more nuanced distinctions?\n\n- To what extent do the image-level and token-level experts specialize in different physical regimes or frequency ranges, and is there any interpretable pattern behind this specialization?\n\n- Since the model is pretrained and fine-tuned on the same datasets and distributions, how can we interpret its claimed generalization ability within the broader context of foundation models?\n\n- Given that no statistics on expert utilization are reported, how can we be confident that the nested MoE architecture does not collapse to a single active expert during training?\n\n- Would the nested MoE design still maintain stable expert utilization if trained on a single-PDE dataset, or does its benefit mainly arise in multi-PDE pretraining scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KzIABEbgBa", "forum": "FWgnGGLP3u", "replyto": "FWgnGGLP3u", "signatures": ["ICLR.cc/2026/Conference/Submission10418/Reviewer_cnh5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10418/Reviewer_cnh5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766513322, "cdate": 1761766513322, "tmdate": 1762921726503, "mdate": 1762921726503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NESTOR, a framework based on a Mixture of Experts for solving Partial Differential Equations. Through hierarchical routing at the image and token levels, the model aims at capturing variation in PDE solution data at different scales. Moreover, the framework benefits from large-scale PDE pre-training, followed by fine-tuning on specific PDEs and sets of parameters;  enjoying very high performance on next-step prediction for transient simulations. Finally, ablation studies showcase the importance of the various architectural blocks introduced in this model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tIntroduction of an innovative Mixture of Experts architecture to deal with image level and token level PDE solution complexity\n-\tUse of a pre-training procedure on a vast array of PDEs and parameters, with fine-tuning on particular instances leading to increased performance.\n-\tState of the art results obtained on multiple PDE benchmarks studied\n-\tAblation studies justify the need for the various components of the framework."}, "weaknesses": {"value": "-\tThe model is evaluated only on next-step prediction accuracy, without consideration for rollout stability over longer horizons. In contrast, DPOT incorporated noise during training to enhance robustness. Without similar analysis, it remains unclear whether this model maintains stability during multi-step predictions.\n-\tThe model lacks interpretability regarding expert behavior. It is not demonstrated whether the individual experts specialize in different physical regimes or scales, leaving uncertain whether the hierarchical routing effectively captures multi-scale interactions.\n-\tThe proposed architecture may face scalability challenges for high-resolution meshes, where the number of spatial tokens N=H×W exceeds 10^6. The paper does not discuss strategies for handling the associated computational and memory bottlenecks.\n-\tIncreasing the number of experts appears to yield only limited improvements in performance. This raises questions about the efficiency of expert utilization and whether the architecture fully exploits the Mixture-of-Experts capacity.\n-\tThe PDE formulations in the appendix omit key details such as initial conditions, and the figures do not specify the time steps at which the visualizations are generated. These omissions hinder reproducibility and interpretation of the results.\n-\tSome claims are vague, particularly those contrasting this work with DPOT. The authors should clearly articulate what specific limitations of DPOT are addressed here, as the described pretraining procedure appears conceptually similar.\n-\t Typo l.369:  “9 out of L2 tasks” -> 9 out of 12 tasks ?"}, "questions": {"value": "-\tHave the authors considered the stability of the model predictions in the context of longer auto-regressive rollouts ?\n-\tIn table 2, when the authors state 2 experts are used, are those experts the shared expert and one unshared expert ?\n-\tCould the authors precise which initial conditions are used and times are represented in the PDEs and solutions plotted in the appendix ?\n-\tHow does the current pre-training overcome the limitations of the DPOT pre-training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rDgjF3WoMV", "forum": "FWgnGGLP3u", "replyto": "FWgnGGLP3u", "signatures": ["ICLR.cc/2026/Conference/Submission10418/Reviewer_s8iH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10418/Reviewer_s8iH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928381467, "cdate": 1761928381467, "tmdate": 1762921725983, "mdate": 1762921725983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes  a nested mixture-of-experts (MoE) neural operator for large-scale PDE pre-training. It nests an image-level MoE with a token-level Sub-MoE (to model local, fine-grained dependencies inside each field). A spatio-temporal patch encoder feeds experts; AFNO serves as a shared global-frequency expert; FlashAttention is used within the transformer blocks; and two load-balancing losses regularize routing at both MoE levels. The model is pre-trained on 12 PDE datasets (mixture of FNO, PDEBench, PDEArena and CFDBench sources) with a next-frame objective, then fine-tuned. Results show SOTA or strong performance on 6/12 tasks in pre-training, and after fine-tuning SOTA on 9 tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea is novel. It uses a two-level MoE for PDE operators, which is a novel architectural twist for scientific operators. Besides, using AFNO as a shared global expert plus FlashAttention inside an MoE operator is a thoughtful combination.\n\n2. The paper is clear and well-organized, and the figures are quite good and easy to read.\n\n3. The overall performance is positive to show the effectiveness of the method."}, "weaknesses": {"value": "1. Computational cost not quantified. MoE + FlashAttention + AFNO is likely compute/memory heavy. The paper lacks FLOPs/throughput, GPU hours, tokens/updates, or cost-vs-quality curves, and no comparison to strong single-backbone operators at equal compute.\n\n2. Most experiments appear on regular-grid, 2D fields with autoregressive next-frame targets. Little is shown for 3D, irregular meshes, varying boundary/geometry conditions."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Mq0R6dMqmV", "forum": "FWgnGGLP3u", "replyto": "FWgnGGLP3u", "signatures": ["ICLR.cc/2026/Conference/Submission10418/Reviewer_ScpY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10418/Reviewer_ScpY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762842934334, "cdate": 1762842934334, "tmdate": 1762921725654, "mdate": 1762921725654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}