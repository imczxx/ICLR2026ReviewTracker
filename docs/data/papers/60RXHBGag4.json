{"id": "60RXHBGag4", "number": 8116, "cdate": 1758064480050, "mdate": 1762941692942, "content": {"title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification", "abstract": "Recent advances in deep neural networks (DNNs) have led to remarkable success across a wide range of tasks. However, their susceptibility to adversarial perturbations remains a critical vulnerability. Existing diffusion-based adversarial purification methods often require intensive iterative denoising, severely limiting their practical deployment. In this paper, we propose Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient diffusion-based framework for adversarial purification. Central to our approach is a new objective, noise bridge distillation, which constructs a principled alignment between the adversarial noise distribution and the clean data distribution within a latent consistency model (LCM). To further enhance semantic fidelity, we introduce adaptive semantic enhancement, which fuses multi-scale pyramid edge maps as conditioning input to guide the purification process. Extensive experiments across multiple datasets demonstrate that DBLP achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and around 0.2s inference time, marking a significant step toward real-time adversarial purification.", "tldr": "", "keywords": ["adversarial defense", "latent consistency model", "purification", "lora"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7dacfaedc2605311088044a470aa04b23ab9bd67.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DBLP, a diffusion-based adversarial purification framework that addresses the limitations of slow inference in existing methods by introducing noise bridge distillation within a latent consistency model, which constructs a principled alignment between adversarial noise and clean data distributions to enable efficient one-step purification, complemented by an adaptive semantic enhancement module that fuses multi-scale pyramid edge maps to preserve fine-grained details and improve visual fidelity. Extensive experiments on multiple datasets demonstrate that DBLP achieves state-of-the-art robust accuracy"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and well-structured.\n2. DBLP achieves significant acceleration in adversarial purification by leveraging its noise bridge distillation technique.\n3. The paper demonstrates a comprehensive literature review and employs cutting-edge baselines for comparison."}, "weaknesses": {"value": "1. The novelty of this work is limited, as the direct modeling of the transition between adversarial noise and the image distribution has been previously explored by ADBM.\n2. While this method demonstrates certain advantages, it requires a training process, which is computationally expensive. Furthermore, compared to other training-free alternatives, it exhibits limited generalizability.\n3. During training, the method requires a classifier to generate adversarial examples. The noise bridge distillation process relies heavily on the specific classifier and the adversarial attack algorithm used, which can further undermine DBLP's generalization performance."}, "questions": {"value": "Could you explain the your choices of the classifier and the adversarial attack algorithm used during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AO4bPPeoBX", "forum": "60RXHBGag4", "replyto": "60RXHBGag4", "signatures": ["ICLR.cc/2026/Conference/Submission8116/Reviewer_8nTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8116/Reviewer_8nTD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760580637639, "cdate": 1760580637639, "tmdate": 1762920096498, "mdate": 1762920096498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "aFcjf8X9vC", "forum": "60RXHBGag4", "replyto": "60RXHBGag4", "signatures": ["ICLR.cc/2026/Conference/Submission8116/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8116/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762940670382, "cdate": 1762940670382, "tmdate": 1762940670382, "mdate": 1762940670382, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DBLP, a fast adversarial purification method using diffusion models. The authors claim two main contributions: (1) Noise Bridge Consistency Distillation that aligns adversarial and clean distributions in Latent Consistency Models, and (2) Adaptive Semantic Enhancement using multi-scale edge maps for conditioning. However, the paper suffers from critical issues: the core methodology is substantially similar to OSCP (Lei et al., CVPR 2025), the theoretical foundation has fundamental gaps regarding inference-time computation, and essential experimental details are missing. The claimed improvements over OSCP are marginal (1.7%) without statistical significance testing. These issues prevent acceptance at ICLR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1. Addresses Important Problem**\n- Inference speed is a critical bottleneck for diffusion-based adversarial purification\n- Achieving 0.2s represents significant speedup over DiffPure (53s)\n- Motivation for real-time defense is well-articulated\n\n**S2. Comprehensive Experiments**\n- Multiple datasets (CIFAR-10, ImageNet, CelebA) evaluated\n- Various threat models tested ($\\ell_\\infty$, $\\ell_1$, $\\ell_2$)\n- Cross-architecture transferability shown (Table 4)\n\n**S3. Competitive Empirical Results**  \n- 75.6% robust accuracy on ImageNet under PGD-100\n- Maintains reasonable clean accuracy (78.2%)\n- Strong performance on face recognition tasks"}, "weaknesses": {"value": "**W1. Substantial Similarity to OSCP (Lei et al., CVPR 2025)**\n\nThe core methodology is nearly identical to OSCP's GAND approach:\n\n**Identical Forward Process:**\n- OSCP Eq. (11): $\\mathbf{z}^*_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{z} + \\sqrt{1-\\bar{\\alpha}_t}(\\boldsymbol{\\epsilon} + \\boldsymbol{\\delta}_{\\text{adv}})$\n- DBLP Eq. (11): $\\tilde{\\mathbf{z}}_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{z}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\boldsymbol{\\epsilon} + \\frac{\\bar{\\alpha}_T(1-\\bar{\\alpha}_t)}{\\sqrt{\\bar{\\alpha}_t(1-\\bar{\\alpha}_T)}}\\boldsymbol{\\epsilon}_a$\n\nBoth add adversarial noise to the diffusion forward process. DBLP's coefficient $k_t$ derivation (Eq. 10) is merely a mathematical reparameterization with no conceptual difference.\n\n**Nearly Identical Training Objective:**\n- OSCP's GAND loss (Eq. 15) and DBLP's LCD loss (Eq. 12) have the same structure\n- DBLP's only addition is reconstruction loss from Kim et al. (2024)\n\n**Similar Inference Pipeline:**\n- Both use Canny edge detection + ControlNet\n- DBLP's \"Adaptive Semantic Enhancement\" adds standard techniques (Otsu thresholding, image pyramids) to OSCP's CAP\n\n**Inadequate Disclosure:**\n- While DBLP cites OSCP (Table 2), it never explicitly states that \"Noise Bridge Distillation\" is essentially the same idea as OSCP's GAND\n- Related Work (Sec. 2.2) mentions OSCP only for speed, not methodology\n- This insufficient disclosure raises academic integrity concerns\n\n**Marginal Improvement:**\n- Claims 1.71% improvement over OSCP (73.89% → 75.6%)\n- No statistical significance testing provided\n- Could be experimental variation rather than true improvement\n\n---\n\n**W2. Fundamental Theoretical Gap**\n\n**Critical contradiction regarding Eq. (11):**\n\nLine 248 states: \"its exact value [$\\epsilon_a$] is unknown at inference time\"  \nLine 258 claims: \"sampling process doesn't require $\\epsilon_a$\"  \nBut Eq. (11) explicitly requires $\\epsilon_a$: $\\tilde{\\mathbf{z}}_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{z}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\boldsymbol{\\epsilon} + \\frac{\\bar{\\alpha}_T(1-\\bar{\\alpha}_t)}{\\sqrt{\\bar{\\alpha}_t(1-\\bar{\\alpha}_T)}}\\boldsymbol{\\epsilon}_a$\n\n**The problem:** Appendix A.2 shows how to eliminate $\\epsilon_a$ from the posterior distribution through choice of $k_t$, but does NOT explain how to construct $\\tilde{\\mathbf{z}}_t$ without knowing $\\epsilon_a$ at inference. This is not addressed in Algorithm 1, which only describes training.\n\n---\n\n**W3. Severe Reproducibility Issues**\n\nMissing critical details:\n- No GPU specifications\n- No total training time  \n- No random seeds\n- No hyperparameter search description\n- No code release commitment\n- Missing PGD step size $\\alpha$\n- Missing temperature $T^\\ast$ in Eq. (17)\n\nThese omissions violate ICLR reproducibility standards.\n\n---\n\n**W4. Experimental Design Flaws**\n\n**Overfitting to seen attacks:**\n- Training: PGD-100, $\\epsilon=4/255$, $\\ell_infty$, ResNet-50 (Sec. 5.1)\n- Main evaluation: PGD-100, $\\epsilon=4/255$, $\\ell_infty$, ResNet-50 (Tables 1-2)\n- This is essentially a \"seen attack\" scenario\n\n**Missing evaluations:**\n- No adaptive attacks aware of the purification method\n- Limited AutoAttack results (only 2 settings in Table 2)\n- No analysis of failure modes\n- No statistical significance testing (no error bars, no multiple runs)\n\n---\n\n**W5. Limited Novel Contribution**\n\nGiven similarity to OSCP, the incremental contributions are:\n1. Mathematical reparameterization of OSCP's forward process (no conceptual advance)\n2. Adding Otsu thresholding + image pyramids to edge detection (standard CV techniques)\n3. Leapfrog solver shows minimal improvement (Table 9: 0.2% gain)\n\nThese incremental modifications do not constitute sufficient novelty for a major venue."}, "questions": {"value": "**Q1. Relationship to OSCP**\n\nCan you explicitly clarify how \"Noise Bridge Distillation\" differs conceptually from OSCP's \"Gaussian Adversarial Noise Distillation\"? Both methods:\n- Add adversarial noise to the forward diffusion process  \n- Use the same training objective structure\n- Employ edge-based conditioning for inference\n\nIs your contribution primarily an alternative mathematical formulation, or is there a fundamental conceptual difference?\n\n---\n\n**Q2. Inference-Time Computation**\n\nHow is $\\tilde{\\mathbf{z}}_t$ in Eq. (11) computed at inference without knowing $\\epsilon_a$? Please provide:\n- Step-by-step inference algorithm (not just training)\n- Explicit method for handling the $\\epsilon_a$ term\n- Explanation of how Appendix A.2's posterior derivation enables this\n\n---\n\n**Q3. Statistical Significance**\n\nYour improvement over OSCP is 1.71% (73.89% → 75.6%). Please provide:\n- How many independent runs were performed?\n- Standard deviations across runs?\n- Statistical significance test (e.g., paired t-test)?\n- Could this be within experimental noise?\n\n---\n\n**Q4. Adaptive Attacks**\n\nHave you evaluated against:\n- Adaptive attacks aware of your purification (e.g., attacking the LCM directly)?\n- C&W attack?\n- Diff-PGD with different loss functions?\n\nTable 4 shows Diff-PGD-10, but what about stronger adaptive threats?\n\n---\n\n**Q5. Reproducibility**\n\nCan you provide:\n- GPU model and training time\n- Random seeds used\n- Complete hyperparameter values (PGD step size, temperature T*)\n- Code release commitment upon acceptance?\n\n---\n\n**Q6. Ablation Studies**\n\nTable 8 compares CD vs NBD, but what about:\n- Vanilla LCM (no distillation) vs your method?\n- Your noise bridge vs just using OSCP's formulation directly?\n- Impact of each component in \"Adaptive Semantic Enhancement\"?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "**Core Concern:** The paper's main contribution (Noise Bridge Consistency Distillation) appears substantially similar to OSCP (Lei et al., CVPR 2025), which the authors cite but inadequately discuss.\n\n**Evidence of Similarity:**\n\n1. **Identical Core Idea:** Both add adversarial noise to diffusion forward process\n   - OSCP Eq. (11): $\\mathbf{z}^*_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{z} + \\sqrt{1-\\bar{\\alpha}_t}(\\boldsymbol{\\epsilon} + \\boldsymbol{\\delta}_{\\text{adv}})$\n   - DBLP Eq. (11): Mathematically equivalent, just reparameterized\n\n2. **Same Training Objective:** OSCP's GAND loss ≈ DBLP's LCD loss\n\n3. **Similar Inference:** Both use Canny edges + ControlNet\n\n**Disclosure Issues:**\n- DBLP cites OSCP only in Table 2 for empirical comparison\n- Section 2.2 (Related Work) mentions OSCP only for inference speed\n- Never explicitly states that their \"Noise Bridge Distillation\" is the same core idea as OSCP's GAND\n- Claims this as their \"novel\" contribution throughout abstract and intro\n\n**Timeline:** OSCP published at CVPR 2025 before DBLP's ICLR 2026 submission."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BcTFVHDQHh", "forum": "60RXHBGag4", "replyto": "60RXHBGag4", "signatures": ["ICLR.cc/2026/Conference/Submission8116/Reviewer_kUDN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8116/Reviewer_kUDN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396970078, "cdate": 1761396970078, "tmdate": 1762920095909, "mdate": 1762920095909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient framework for diffusion-based purification (DBP).\nExisting DBP methods often require a large number of NFEs for both adversarial image generation and denoising, which limits their practicality. DBLP aims to address this issue and improve the deployability of DBP methods in real-world scenarios.\n\n1) The proposed framework consists of two key components:\n\n2) Latent Consistency Model (LCM): trained to purify adversarial inputs by learning the underlying adversarial noise distribution.\n\nSemantic Preservation Module: leverages computer vision techniques to retain the semantic content of the original image during the denoising process.\n\nExperimental results demonstrate that DBLP achieves comparable or improved robustness against existing DBP baselines across multiple benchmarks, while maintaining efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper focuses on a realistic problem within existing DBP framework, which is the inference time feasibility.\n\n2. Leveraging consistency model is a promising direction for future DBP work."}, "weaknesses": {"value": "Before addressing my specific questions, I would like the authors to respond to the following major concern:\n\nThis work exhibits substantial overlap with the previously published OSCP paper [1]. The authors should carefully clarify the differences in motivation and methodology to distinguish their contributions.\n\nThe other weaknesses are the following:\n\n**1. Formatting and citation issues.**\n\nThe current version contains multiple formatting inconsistencies, such as incorrect citation usage (e.g., using \\citet{} for all references) and duplicated equation references (e.g., line 176). These should be revised to meet standard academic formatting conventions.\n\n**2. Disorganized experimental layout.**\n\nThe presentation of experimental results is confusing. The attack settings should be clearly described in Section 5. Table 2 mixes results from ImageNet under various adaptive attacks and classifiers, making it difficult to interpret or compare performance fairly. Each table should isolate comparable settings for clarity. Also, in line 238, the stated objective is consistent with all the DBP methods that leverages diffusion model to map conditional gaussian distribution back to original data distribution. The objective cannot become a contribution as stated in line 073.\n\n**3.Incorrectly highlighted results.**\n\nTable 1 mistakenly bolds the DBLP results for both clean and robust accuracies, even though ADBM achieves higher $\\ell_2$ robustness. This raises concerns about proofreading and result verification prior to submission.\n\n**4. Limited methodological novelty.**\n\nThe proposed Adaptive Semantic Enhancement (ASE) contributes only marginal robustness gains. ASE essentially applies a layer-wise Canny edge detector with Gaussian blur pyramids — a relatively minor modification compared with prior work [1]. Would you like to explain the design principle of choosing a gaussian blur pyramids?\n\n**5. Incorrect comparison in methodology.**\n\nAt line 236, the statement “Unlike DDPM, DBLP includes adversarial perturbations $\\epsilon_a$ at the start of the noising process” is misleading. In diffusion-based purification (DBP) frameworks, inputs are either adversarial ($x_{adv}$) or natural ($x_{nat}$) images. This comparison misrepresents the role of $\\epsilon_a$ and should be corrected.\n\n**6. Potential plagiarism concern.**\n\nFigure 1 appears highly similar to that of OSCP [1], particularly in the design of the LCM-LoRA module. The overall pipeline closely resembles the OSCP architecture, suggesting that DBLP’s improvements may stem more from leveraging consistency models’ one-step denoising rather than introducing a genuinely novel methodology.\n\n\n[1] Lei et al, \"Instant Adversarial Purification with Adversarial Consistency Distillation\", CVPR 2025."}, "questions": {"value": "Please refer to  **weaknesses** and address all my concerns by answering the listed weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "This work exhibits substantial overlap with the previously published OSCP paper [1]. The authors should carefully clarify the differences in motivation and methodology to distinguish their contributions.\n\n[1] Lei et al, \"Instant Adversarial Purification with Adversarial Consistency Distillation\", CVPR 2025."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OH1k11HLpm", "forum": "60RXHBGag4", "replyto": "60RXHBGag4", "signatures": ["ICLR.cc/2026/Conference/Submission8116/Reviewer_c1Vx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8116/Reviewer_c1Vx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559533049, "cdate": 1761559533049, "tmdate": 1762920095530, "mdate": 1762920095530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NoiseBridge, a DBP defense method that uses a consistency regularization to purify AEs in a very low number of diffusion steps. The approach leverages a latent consistency model to bridge adversarial noise and clean data distributions, and introduces an adaptive semantic enhancement module that injects multi-scale edge information to preserve image content during purification."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles an important challenge in DBP methods. By aiming for a one-step (or very few-step) diffusion purification, NoiseBridge targets the key limitation of speed to make DBP methods practical for real-world use.\n2.  The experimental evaluation shows that NoiseBridge achieves strong adversarial robustness on multiple datasets."}, "weaknesses": {"value": "1. The technical novelty of NoiseBridge is questionable (also mentioned by Reviewer kUDN in the public comments). The core approach appears to be heavily based on OSCP [1] with only incremental changes. OSCP introduced the idea of single-step adversarial purification using a distilled diffusion model, employing a consistency distillation objective and an edge-based guidance to preserve content. NoiseBridge essentially follows the same template: a diffusion model is fine-tuned with a noise-to-clean consistency objective (analogous to OSCP’s GAND) and uses edge information during inference (similar to OSCP’s CAP) to maintain structure. The only notable difference is that NoiseBridge uses *multi-scale* edge maps instead of a fixed edge detector. While this is a useful improvement, it is relatively minor in terms of conceptual novelty. \n2. Compounding the above issue, the paper’s positioning with respect to OSCP is problematic. Given the high degree of overlap in methodology, one would expect a clear acknowledgment and discussion of how NoiseBridge differs from and builds upon OSCP. However, the attribution in the current draft is not adequately prominent. OSCP is indeed cited, but the authors seem to downplay its influence. For example, by renaming similar techniques (e.g., consistency distillation as \"noise bridge distillation\", edge-guided purification as \"adaptive semantic enhancement\") without explicitly crediting that these ideas originate in OS. This insufficient attribution can mislead readers into overestimating the originality of NoiseBridge and raises ethical concerns. Currently it gives the impression that the paper is repackaging someone else’s contributions with only superficial changes.\n3. While the paper reports state-of-the-art results, the actual gains over OSCP are quite small. For instance, NoiseBridge improves robust accuracy on ImageNet by roughly 1%-1.5% (e.g., from ~73.9% to ~75.6% under comparable attack settings), and a similar modest gain is seen in clean image preservation metrics.\n4. The paper evaluates robustness using standard attacks (PGD, AutoAttack, etc.), but it does not evaluate against stronger attacks. According to [2], PGD+EOT should be evaluated for DBP methods. This is an important issue to investigate for a complete evaluation of an adversarial defense.\n\n[1]  Instant adversarial purification with adversarial consistency distillation. In CVPR 2025.\n\n[1] Robust evaluation of diffusion-based adversarial purification. ICCV 2023."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The technical novelty of NoiseBridge is questionable (also mentioned by Reviewer kUDN in the public comments). The core approach appears to be heavily based on OSCP [1] with only incremental changes.\n\n[1] Instant adversarial purification with adversarial consistency distillation. In CVPR 2025."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IFyA4PKWKk", "forum": "60RXHBGag4", "replyto": "60RXHBGag4", "signatures": ["ICLR.cc/2026/Conference/Submission8116/Reviewer_gCjU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8116/Reviewer_gCjU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987724221, "cdate": 1761987724221, "tmdate": 1762920094914, "mdate": 1762920094914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}