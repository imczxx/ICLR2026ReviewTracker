{"id": "oIAUP1K5Dq", "number": 15064, "cdate": 1758247359860, "mdate": 1759897332015, "content": {"title": "Aligner, Diagnose Thyself: A Meta-Learning Paradigm for Fusing Intrinsic Feedback in Preference Alignment", "abstract": "The alignment of Large Language Models (LLMs) with human preferences is critically undermined by noisy labels in training datasets.\nExisting robust methods often prove insufficient, as they rely on single, narrow heuristics such as perplexity or loss, failing to address the diverse nature of real-world noise.\nWe challenge this limited-scope approach by introducing a new paradigm where models learn to diagnose thyself, systematically fusing multiple streams of intrinsic feedback for a holistic reliability assessment of each preference pair.\nWe instantiate this paradigm through a meta-learning methodology that learns to adaptively reweight samples based on a rich diagnostic vector.\nThis vector captures three complementary perspectives: preference consistency, learning difficulty, and generation confidence.\nExtensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods across various noise conditions.\nCrucially, our work provides the first quantitative analysis of these intrinsic diagnostics, revealing that their fusion is essential for overcoming the blind spots inherent in any single heuristic.\nThis diagnostic-driven paradigm offers a principled path towards developing more robust and trustworthy LLMs.", "tldr": "", "keywords": ["Large Language Models", "Direct Preference Optimization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/908e983ec4466bbe85d64f858adc4e151c44e76a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents an approach for robust LLM preference alignment. The approach empowers models to perform self-diagnosis by fusing three key factors, including preference consistency, learning difficulty and generation confidence. Experiments on GoldenHH and OASSIT1 preference datasets  are conducted with simulated noises."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n(1)Two more factors are considered to diagnose noise in preference data, which bring more robustness.\n\n(2)A meta-learning based approach is proposed to learn adaptive weights on the three factors.\n\nSystematic analysis of the interplay and relative importance of the three factors are performed."}, "weaknesses": {"value": "Cons:\n\nIn experiments, the performance gains of the proposed method have been validated by the simulated noise data. However, as shown in Figure 2, the performance difference between various robust DPO variants is not significant, when \\epsilon = 0. Does it mean that the qualities of the two datasets (GoldenHH and OASSIT1) have been very high? Can you validate the effectiveness of the proposed method without noise simulation?\n\nMoreover, the simulation operation for injecting noise follows the protocols in (Kong et al 2024) which propose PPLDiff to measure the preference inconsistency. Consequently, the finding of the predominant role of PPLDiff is inherently expected. Can you give more various noise to simulate the uncertainties and the learning difficulties in preference data?  \n\nMoreover, to investigate the efficacy of the proposed method in the presence of annotation errors commonly found in real-world applications, it is advisable to conduct experiments on more extensive datasets, such as the HuggingFace H4 StackExchange Preference Dataset (with a size of 10 million entries) and the GPT4All dataset (comprising 1 million entries).\n\nSome typos: page 8, the text under Figure 4 is abruptly interrupted by a line break."}, "questions": {"value": "Please find the questions in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NCtUiL5ESq", "forum": "oIAUP1K5Dq", "replyto": "oIAUP1K5Dq", "signatures": ["ICLR.cc/2026/Conference/Submission15064/Reviewer_EPXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15064/Reviewer_EPXP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815233628, "cdate": 1761815233628, "tmdate": 1762925383610, "mdate": 1762925383610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to deal with the issue of label noise in preference tuning datasets for aligning LMs. This new method incorporates separate signals that suggest/correlate with noise (consistency, difficulty, and confidence). Specifically, these are combined into a learnable vector representation that is tuned alongside the model parameters themselves during RLHF training. They find that in a variety of noise settings, their method outperforms baselines methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, the paper is very clear and well-written. I found Section 3, the most technically dense part of the paper, to be very easy to follow compared to other papers where similar concepts are used. \n2. To my knowledge, the meta-learning approach introduced by this paper is novel and is an interesting way of combining heuristic signals that have been used in the past as a proxy for label noise. \n3. The authors mention that only about 100 clean meta-samples are required for their approach. This makes the method highly scalable and efficient. Therefore, there is meaningful practical value for using this particular method. \n4. The experiments are sound and the analysis regarding meta-dataset sensitivity will be useful to practitioners using the method."}, "weaknesses": {"value": "The results seem to suggest that PPLDiff is by far the most important and influential diagnostic criterion. While the ablations show that there is a statistically significant difference between using the fusion of all of the criteria and just using PPLDiff, the difference is very small, and that also seems to be the case in the results presented in Figure 2. That being said, I don't feel as if this is necessarily a demerit against the method itself, but rather I think it would be useful to see results with other metrics as well to see if any other proxy diagnostics could be very helpful. \n\nAnother reason I don't think this fact in and of itself is a demerit is the analysis provided in Section 4.4, which was very interesting. However, I do think the paper would be strengthened by varying some of the important design choices in the meta-learning set-up. In addition to the diagnostics themselves, I think it would be interesting to vary the meta-learner itself to see how sensitive the method is to that choice."}, "questions": {"value": "1. What are some other diagnostic methods you think could be used?\n2. Why was a 2-layer MLP chosen? Was this in comparison with other simple models?\n3. Do you have a sense of what would be a practical way of constructing a \"clean\" set for meta-learning?\n4. What assumptions are being made regarding characteristics of the noise (i.e. is it randomly distributed)? Would it work if the noise is adversarial in some way? I suppose in that case noise is not the best descriptor, but I am curious about this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1rBluV9sre", "forum": "oIAUP1K5Dq", "replyto": "oIAUP1K5Dq", "signatures": ["ICLR.cc/2026/Conference/Submission15064/Reviewer_wsUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15064/Reviewer_wsUf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945971217, "cdate": 1761945971217, "tmdate": 1762925383071, "mdate": 1762925383071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a diagnostic-driven meta-learning paradigm for robustly aligning LLMs with human preferences in the presence of noisy preference data. The authors propose to empower models to \"diagnose themselves\" during training by fusing three types of intrinsic feedback—preference consistency (perplexity difference), learning difficulty (training loss), and generation confidence (token-level entropy)—to form a diagnostic vector per sample. A meta-learning approach adaptively reweights training samples based on this vector, optimizing for performance on a small clean meta-dataset. Extensive experiments on 2 benchmarks show superiority over state-of-the-art robust baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper formalizes the idea that preference reliability for LLM alignment is inherently multi-perspective, motivating the use of preference consistency (perplexity difference), learning difficulty (DPO loss), and generation confidence (entropy) as three synergistic diagnostics. Rather than relying on a single heuristic, the method operationalizes a holistic, adaptive approach.\n2.\tThe use of SHAP to quantify feature importance and uncover non-linear interactions provides deep, novel insights into how the meta-learner works. \n3.\tThe method consistently achieves SOTA performance across multiple models, benchmarks, and noise levels."}, "weaknesses": {"value": "1.\tAll noise scenarios are simulated via label-flipping at random rates. There is no direct evaluation or demonstration of real-world occurring noisy preferences.\n2.\tAlthough the chosen baselines are quite relevant, it’s better to take more recent methods into consideration."}, "questions": {"value": "1.\tThe diagnostic vector is a concatenation of three normalized signals. Did the author experiment with more complex fusion architectures for the meta-learner's input beyond a simple MLP on the concatenated vector?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xsmRBpeNH5", "forum": "oIAUP1K5Dq", "replyto": "oIAUP1K5Dq", "signatures": ["ICLR.cc/2026/Conference/Submission15064/Reviewer_rJ2H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15064/Reviewer_rJ2H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994025378, "cdate": 1761994025378, "tmdate": 1762925382603, "mdate": 1762925382603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a meta-learning paradigm to address the noisy annotations in preference data for RLHF. Specifically, it proposes to diagnose the model itself by using intrinsic feedback, and a diagnostic vector composed of preference consistency, learning difficulty, and generation confidence is employed to adaptively reweight training samples, where noisy or unreliable preference data is down-weighted.\nThe paper provides quantitative and qualitative experimental analysis to demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-motivated. Instead of employing a pre-defined criteria, the paper introduces a multi-dimensional diagnostic vector and demonstrates that a meta-learned fusion strategy substantially enhances the robustness and reliability of preference alignment. Moveover, detailed ablations and fine-grained analysis clearly demonstrate that the proposed fusion strategy consistently outperforms any single aspect, with adaptive weighting under varying noise conditions.\n2. This paper is well-organized, the coherent structure and figures clearly demonstrate the core idea."}, "weaknesses": {"value": "1. Lack of analysis of meta-learner: The meta-learner is consistently a 2-layer MLP. Ablation studies varying depth, width, or overall capacity would help assess the method's stability and generality. Without this, it is unclear to what extent the results depend on a hand-tuned meta-learner architecture.\n2. The method builds heavily on well-established meta-learning frameworks; the provided generalization bound is essentially a direct adaptation of standard meta-learning. Deeper theoretical insights or problem-specific analysis could further strengthen the contribution. \n3. Lack of discussion for generalization ability. The experimental noise is synthetically generated via label flips. Although this enables controlled benchmarking, real-world noise (e.g., conflicting annotators, domain shifts) may exhibit different characteristics. While qualitative examples are provided, additional experiments on general benchmarks, naturally noisy or adversarial datasets would better demonstrate generalizability."}, "questions": {"value": "My main concern is twofold: one is the generalization ability, which could the approach compare on some general benchmarks with some representative methods, such as DPO series, and another is the analysis of the meta-learner, more ablation analysis is needed.\n\nIf you can provide convincing evidence or clarification, I would be open to increasing the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gd5ZCDh5bI", "forum": "oIAUP1K5Dq", "replyto": "oIAUP1K5Dq", "signatures": ["ICLR.cc/2026/Conference/Submission15064/Reviewer_hQMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15064/Reviewer_hQMu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995168257, "cdate": 1761995168257, "tmdate": 1762925382197, "mdate": 1762925382197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}