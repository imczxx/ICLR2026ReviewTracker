{"id": "jugY302BAh", "number": 5231, "cdate": 1757870821223, "mdate": 1759897986787, "content": {"title": "Verification and Co-Alignment via Heterogeneous Consistency for Preference-Aligned LLM Annotations", "abstract": "Large Language Models (LLMs) are increasingly expected to be culturally customizable and personally aligned for natural language understanding (NLU). However, existing methods, from supervised fine-tuning (SFT) to personalized RLHF and prompting, either require costly large-scale annotations or remain constrained by pretraining distributions. Moreover, acquiring annotations that reflect subjective, diverse, and evolving user preferences is both expensive and labor-intensive. To address these limitations, we propose \\textit{\\textbf{H}eterogeneous-\\textbf{C}onsistency \\textbf{C}o-Alignment} (HCC) is a training-free annotation paradigm that leverages two heterogeneous models, which consists of an LLM, rich in knowledge yet often prone to overconfidence, is paired with a task-specialised lightweight model guided by a small user-preference set to verify and co-align misaligned outputs over unlabeled corpora. For verification, HCC introduces the reference-free \\textit{\\textbf{C}onsistent}-\\textit{\\textbf{A}nd}-\\textit{\\textbf{I}nconsistent} (\\textbf{CAI}) Ratio, an uncertainty signal derived from inter-model agreements (consistent samples) and disagreements (inconsistent samples) to determine when refinement is needed. For co-alignment, HCC employs a non-parametric, embedding-based preference assignment scheme to recalibrate inconsistent samples according to user preferences. Across eight NLU datasets and both open- and closed-source LLMs, HCC consistently improves annotation quality and, in several tasks, even enables \\textit{Llama-3-8B} to surpass \\textit{GPT-3.5/4o} after co-alignment. Moreover, CAI correlates strongly with accuracy and reliably tracks pre-/post-alignment gains, offering a reference-free signal for scaling preference-aligned annotation.", "tldr": "", "keywords": ["Verification", "Co-Alignment", "Preference-Aligned LLM Annotations", "Reference-Free Metric"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6eec5b5165cdd2e8114af62d9bbd7b65b1b2e718.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* The authors proposed a training-free annotation method using two heterogeneous models (LLM + lightweight preference model) to verify and co-align outputs for personalized NLU tasks.\n\n* This paper introduces a reference-free metric that measures agreement/disagreement to estimate annotation reliability.\n\n* The proposed methods are validated extensively with both open and closed-source LLMs. The results show that the approach improves alignment and enables smaller models (e.g., 8B) to outperform stronger ones post-co-alignment."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* This paper addresses an important problem in personalized LLMs. In many cases, we don't have enough trustworthy labels from users and finding ways to scale with limited labels is critical.\n\n* The evaluation is well done extensively with most of prompting baselines (Table 2-4).\n\n* The results show that the approach can make a smaller model (8B) outperform larger models (4o and 3.5 Turbo). This has important practical value and demonstrates the promise of the approach."}, "weaknesses": {"value": "* The notations of the paper is very difficult to follow. The authors introduce too many different symbols throughout the paper. Many of them are not necessary (e.g. zero-shot vs. single shot). My assessment is biased by the fact that I can't follow the technical details fully.\n\n* It's unclear to me whether the approach is aligning labels or aligning LLMs. The text suggested that it's model alignment through prompt changes but when looking into the details, what the method is really doing is to use a small set of available user labels to propagate to the rest of the (larger-scale) unlabelled dataset. If this is what the authors are doing, then reward-based (e.g., RLHF) approaches are strong baselines that the paper may want to compare to.\n\n* The methods seems incremental improvements. At its core it's essentially nearest neighbor label matching and majority voting (again, due to the presentation issue, it's hard to dig deeper). It's unclear how generalizable this approach will be for real world tasks where the label space might be much larger and noisy."}, "questions": {"value": "It will be really helpful if the authors can improve the presentation of the paper (especially the notations)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tMEklwX87r", "forum": "jugY302BAh", "replyto": "jugY302BAh", "signatures": ["ICLR.cc/2026/Conference/Submission5231/Reviewer_MYtC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5231/Reviewer_MYtC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851218839, "cdate": 1761851218839, "tmdate": 1762917963458, "mdate": 1762917963458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Heterogeneous-Consistency Co-Alignment (HCC), a reference-free, training-free paradigm to improve LLM preference annotations. Specifically, HCC leverages a LLM and a task-specialized lightweight model to identify consistent and inconsistent samples in large unlabeled corpora. Moreover, the reference-free Consistent-and-Inconsistent (CAI) ratio is proposed as an uncertainty signal for evaluation, while an embedding-based majority-voting scheme enables semi-supervised co-alignment to recalibrate and propagate user preferences. \nExperiments across multiple NLU benchmarks demonstrate the effectiveness of HCC in improving annotation quality, often elevating open-source LLMs above closed-source models after co-alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well motivated: this paper emphasizes the need for scalable, robust, and user-personalized annotation pipelines for LLMs that avoid reliance on costly large-scale preference annotations or ground-truth labels. \n2. The introduced Consistent-and-Inconsistent (CAI) ratio provides a reference-free, interpretable metric to quantify annotation reliability and preference alignment, addressing the annotation challenges in low-supervision or dynamically evolving settings.\n3. Extensive experimental results across eight diverse NLU datasets and multiple LLM backbones show that HCC consistently performs on par with or better than both LLM-only and specialized baselines, and even surpasses some closed-source models, which is significant."}, "weaknesses": {"value": "1. Lack of evaluation on more complex NLU benchmarks such as open-ended question answering or generative preference modeling, which makes the claim that CAI generalizes as a “reference-free metric for preference alignment” may be somewhat overstated.\n2. The exclusive use of MiniLM for embedding-based clustering is only briefly justified, though results may depend heavily on this choice. The paper lacks ablations with additional embedding models or clustering variants to assess robustness.\n3. Lack of computational cost, this paper should include the time and computational costs for better efficiency analysis, does it increase the inference time?"}, "questions": {"value": "My concerns are about the embedding and clustering ablations, specifically:\nCan the authors provide an ablation where the embedding model itself is weak or misaligned with user preferences? Does the CAI ratio remain effective for unreliable, out-of-domain, or adversarial specialized models? If not, how can this vulnerability be mitigated?\n\nHow sensitive are results to the choice of embedding model and hyperparameters (e.g., $k$ in majority voting, dimension reduction in t-SNE)? Will more powerful or context-tunable embeddings yield greater gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aFlyB12dsK", "forum": "jugY302BAh", "replyto": "jugY302BAh", "signatures": ["ICLR.cc/2026/Conference/Submission5231/Reviewer_QHKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5231/Reviewer_QHKk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986831403, "cdate": 1761986831403, "tmdate": 1762917963224, "mdate": 1762917963224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Heterogeneous-Consistency Co-Alignment (HCC), a training-free pipeline that lets an off-the-shelf LLM and a light-weight sentence encoder jointly annotate large unlabeled corpora while respecting a handful of user-preference exemplars. Across eight NLU datasets and three LLM families, HCC raises annotation accuracy by +2–16 %, occasionally pushing Llama-3 above GPT-3.5/4o. CAI ratio correlates strongly with true accuracy, offering a reference-free quality lever."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework requires no parameter updates and only performs forward passes through two heterogeneous models so deployment is extremely straightforward.\n\n2. The CAI ratio tracks true accuracy without any labeled references and achieves remarkable Pearson correlations , indicating high statistical significance."}, "weaknesses": {"value": "1. In theory, any sentence encoder (e.g., Ada-002, SimCSE, RoBERTa-base, E5) could be plugged in to replace MiniLM, but the paper provides no experimental validation of this claim."}, "questions": {"value": "1. The 'DIVIDE-AND-CONQUER REFINEMENT' is fixed at two rounds—was this simply chosen as an empirical rule of thumb? More details will be help.\n\nOther questions refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fszmYa1ELe", "forum": "jugY302BAh", "replyto": "jugY302BAh", "signatures": ["ICLR.cc/2026/Conference/Submission5231/Reviewer_VrB7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5231/Reviewer_VrB7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993581345, "cdate": 1761993581345, "tmdate": 1762917962856, "mdate": 1762917962856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Heterogeneous-Consistency Co-Alignment (HCC), a training-free pipeline to generate preference-aligned annotations for unlabeled text by cross-checking an LLM with a lightweight, task-specific embedding model. A Consistent-and-Inconsistent (CAI) ratio—the count of agreement vs. disagreement between the two annotators—serves as a reference-free reliability signal and is claimed to correlate with accuracy. Inconsistent items are then “repaired” via a two-round divide-and-conquer self-alignment (DCSA) using majority voting over top-K nearest neighbors (MV-VTES) seeded by a small user-preference set. Experiments on eight NLU datasets with open- and closed-source LLMs report sizable gains, sometimes enabling Llama-3-8B to surpass GPT-3.5/4o after co-alignment, and the CAI ratio is empirically correlated with accuracy. (Abstract; Fig. 1 sketch on p. 2; method and metric definitions in §4; results in Tables 2–4; correlation analysis.)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tClear, training-free recipe that practitioners can reproduce: count cross-model agreements (CAI), then correct disagreements with k-NN style voting; the workflow is well-delineated.\n\n•\tReference-free metric: CAI is simple, label-set-agnostic, and positioned as more robust than IRR metrics in skewed/open-set settings. \n\n•\tBroad empirical sweep across eight datasets and multiple LLMs with tables showing consistent improvements (Tables 2–4) and a reported CAI–accuracy correlation. \n\n•\tLow compute barrier: avoids fine-tuning reward models and frames co-alignment as post-hoc annotation repair instead of model training."}, "weaknesses": {"value": "•\tPersonalization claim is under-validated: benchmarks are mostly standard intent/topic datasets where “user preference” ≈ small labeled set per class. The paper does not convincingly show alignment to idiosyncratic user preferences.\n\n•\tValidity of CAI as a proxy: while correlations are reported, causality and failure cases are not deeply analyzed (e.g., StackExchange where CAI ↑ but accuracy ↓). A diagnostic analysis of when CAI misleads would strengthen the case. \n\n•\tDependence on the specialized model: results and CAI both hinge on the chosen sentence-embedding model. Sensitivity to the embedding backbone, domain shift, and k/neighbor choices is not fully explored.\n\n•\tReporting gaps: compute cost of group prompting, number of LLM calls, runtime, and cost/benefit vs. stronger baselines (e.g., co-training with confidence calibration) are unclear. Hyperparameter selection (top-K, neighbor weights, cluster sizes) appears ad-hoc across datasets."}, "questions": {"value": "1.\tSpecialized model choice & robustness: How do results and CAI behave with different embedding backbones (e.g., E5, GTE, bge), or with weaker/stronger models? Any cross-domain generalization tests?\n\n2.\tSensitivity: Please report ablations for K, similarity metric, and cluster-seeding size; do you ever weight votes by similarity rather than uniform majority?\n\n3.\tPersonalization: Can HCC handle conflicting preferences across users for the same input? Any results on pairwise preference data (e.g., RLHF-style comparisons) rather than fixed class labels?\n\n4.\tWhen CAI fails: Provide case studies where CAI increases but task accuracy does not (e.g., StackExchange) and analyze why. What thresholds (if any) make CAI actionable?\n\n5.\tCost & latency: What is the end-to-end token/cost budget per 10k examples for zero-shot vs. single-shot prompting? How does DCSA scale with corpus size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9NN34fyTzd", "forum": "jugY302BAh", "replyto": "jugY302BAh", "signatures": ["ICLR.cc/2026/Conference/Submission5231/Reviewer_Q1Gc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5231/Reviewer_Q1Gc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762538652654, "cdate": 1762538652654, "tmdate": 1762917962598, "mdate": 1762917962598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}