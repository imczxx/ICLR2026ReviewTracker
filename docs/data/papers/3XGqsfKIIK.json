{"id": "3XGqsfKIIK", "number": 6499, "cdate": 1757987043844, "mdate": 1763118352235, "content": {"title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition", "abstract": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamentally challenging problem due to the presence of non-rigid motion, occlusion, appearance variation, and the absence of direct depth supervision. While neural radiance fields (NeRFs) have achieved remarkable results in static scene reconstruction, their computational inefficiency and per-scene optimization make them less practical for large-scale or real-time dynamic applications. Recent advances in 3D Gaussian Splatting (3DGS) provide a more efficient alternative, offering real-time rendering and faster convergence. However, existing 3DGS-based methods typically employ a unified deformation field to model both static and dynamic elements, often leading to motion bleeding, geometric artifacts, and temporal instability. In this paper, we propose SplitGaussian, a novel framework for monocular dynamic scene reconstruction that explicitly separates static and dynamic components within the 3DGS paradigm. By decoupling the learning of deformation for moving and non-moving regions, our method mitigates interference between motion modeling and static geometry preservation. We introduce independent deformation networks for each component, enabling precise motion representation while maintaining the integrity of static regions. Furthermore, to improve rendering quality and training stability, we propose a render-frequency-aware pruning strategy that filters out unreliable or redundant Gaussians with minimal visual contribution. Experiments on complex dynamic scenes demonstrate that our method achieves superior visual fidelity, temporal consistency, and training stability compared to recent baselines. Our approach represents a step toward scalable and artifact-free dynamic reconstruction using Gaussian-based representations.", "tldr": "", "keywords": ["Dynamic Reconstruction", "3D Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8e572fb9a18ccc673a8db1605fb601942189ea85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SplitGaussian is a framework for **reconstructing dynamic 3D scenes from monocular videos**, addressing the challenge of jointly inferring **motion, geometry, and appearance** from limited inputs. Unlike previous Gaussian Splatting–based methods that mix static and dynamic elements in a single representation—causing **motion leakage**, **geometric distortion**, and **temporal flickering**—SplitGaussian **explicitly separates** scene representations into **static** and **dynamic** components.\n\nBy allowing only the dynamic branch to deform over time while keeping the background geometry fixed, the method **avoids motion artifacts in static regions** and supports **view- and time-dependent appearance refinement**. This **disentangled design** improves **temporal consistency**, **reconstruction fidelity**, and **training stability**, while also **accelerating convergence**. Extensive experiments show that SplitGaussian achieves **state-of-the-art performance** in **rendering quality**, **geometric stability**, and **motion separation**."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "[S1] Clearly written equations\n\nThe equations are presented in a concise and intuitive manner, making it easy for readers to follow the mathematical formulation.\nThey effectively connect the theoretical design with implementation details.\nThis clarity significantly enhances the overall readability and technical understanding of the paper.\n\n[S2] Clear and informative figures\n\nThe figures visually convey the proposed method and its workflow with high clarity.\nThey effectively complement the textual explanations and highlight key design components.\nThis strong visual presentation helps readers quickly grasp the core ideas of the approach.\n\n[S3] Well-organized ablation studies\n\nThe ablation experiments are systematically designed to isolate the contribution of each component.\nResults are clearly presented and discussed, providing strong empirical justification for the proposed choices.\nThis structured evaluation enhances the credibility and completeness of the paper’s experimental section."}, "weaknesses": {"value": "[W1] Large overlap with RoDyGS [1].\n\nThe overall concept of SplitGaussian shows substantial overlap with RoDyGS, which also separates static and dynamic components of SfM points. Moreover, on the iPhone benchmark—a common evaluation dataset between SplitGaussian and RoDyGS—RoDyGS significantly outperforms SplitGaussian, despite being trained under a pose-free setup. The authors should clarify the key distinctions and contributions of SplitGaussian compared to RoDyGS, which was first published in 2024.\n\n[W2] Lack of technical novelty.\n\nMost of the components presented in this work have already been introduced in prior studies. RoDyGS [1] proposed static–dynamic Gaussian decomposition, while 4DGS [2] introduced a deformation network for Gaussian motion prediction. Furthermore, the concept of APP appears to be a submodule of the time-evolved and view-dependent color modeling already developed in 4DGS [2]. Moreover, using Track Anything to obtain motion mask is already introduced in RoDyGS.\n\n[W3] Missing recent baselines.\n\nFor both the iPhone and HyperNeRF benchmarks, the comparisons are limited to a narrow set of baselines. The authors should include additional recent methods such as [1], [2], [3], [4], [5], and [6] to provide a more comprehensive evaluation.\n\n[W4] Lack of qualitative results on iPhone.\n\nNo qualitative results are provided for the iPhone benchmark, which is a critical omission. Given that their reported PSNR (13.53) is considerably low, the authors should provide visual evidence and discussion to explain their low performance.\n\n[W5] Missing video demonstrations.\n\nStatic image comparisons are insufficient to demonstrate the temporal effectiveness of the proposed approach. The authors should include video results to better highlight the dynamic reconstruction and motion consistency achieved by SplitGaussian.\n\n[W6] Duplicate entries in the bibliography.\nThe following duplicated entries appear in the references and should be corrected:\n- Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis. In 3DV, 2024a.\n- Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis. In 3DV, 2024b.\n\n[1] RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos, Jeong et.al., arXiv2024\n\n[2] Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting, Yang et.al., ICLR24\n\n[3] 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering, Wu et.al., CVPR24\n\n[4] MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds, CVPR2025\n\n[5] Shape-of-Motion: 4D Reconstruction from a Single Video, Wang et.al., ICCV2025\n\n[6] RoDynRF: Robust Dynamic Radiance Fields, Liu et.al., CVPR2023"}, "questions": {"value": "No questions about the paper. \nMy major concern is to clarify the difference with previous work and to put more qualitative results including videos. \nAlthough the authors claimed they achieved state-of-the-art perormance, they have omitted recent baselines for comparison. \nI hope the authors to add more recent baselines suggested in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "f4NbeYxBRy", "forum": "3XGqsfKIIK", "replyto": "3XGqsfKIIK", "signatures": ["ICLR.cc/2026/Conference/Submission6499/Reviewer_bhUc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6499/Reviewer_bhUc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760977388948, "cdate": 1760977388948, "tmdate": 1762918870101, "mdate": 1762918870101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "aX1RwiLE8F", "forum": "3XGqsfKIIK", "replyto": "3XGqsfKIIK", "signatures": ["ICLR.cc/2026/Conference/Submission6499/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6499/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763118351237, "cdate": 1763118351237, "tmdate": 1763118351237, "mdate": 1763118351237, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first identifies that existing dynamic 3D Gaussian Splatting methods often mix static and dynamic regions, causing motion leakage and geometric distortion. To address this, it proposes a framework called SplitGaussian, which explicitly separates static and dynamic components in the Gaussian representation to decouple motion modeling from background geometry. The method aims to improve temporal consistency and reconstruction stability while maintaining efficient optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper identifies a well-defined weakness in prior dynamic 3DGS works (coupled geometry–appearance modeling) and provides a clear, theoretically sound decomposition strategy.\n\n* The paper reports extensive quantitative and qualitative results across multiple datasets, including ablation studies that isolate each module's contribution.\n\n* The paper is easy to follow, with well-organized sections and clear figures that effectively convey the pipeline and improvements."}, "weaknesses": {"value": "While I agree that prior approaches such as Deformable 3DGS, which apply a unified deformation network to both static and dynamic regions, indeed suffer from some problems, my main concern with this paper lies in its insufficient novelty and contribution.\n\n* The proposed solution to the coupling issue primarily relies on using external masks to separate static and dynamic regions and then applying conventional deformation modeling to the dynamic part. This idea has already been explored in many prior works like [1,2]. So I think the design is too simple and does not demonstrate sufficient originality or contribution.\n\n* The *Spatiotemporal Encoding* (Sec 3.2) is quite common in existing dynamic 3DGS implementations, and the setup of the *static and dynamic components* (Sec 3.2) largely follows the standard design used in prior methods. The *VDP* (Sec 3.3) and *DAP* (Sec 3.4) both focus on static regions, while the dynamic part shows little innovation, which does not align well with the paper's claimed core contribution. And the DAP essentially resembles a simple depth loss, which has already been widely used in many prior works and is typically not considered a methodological innovation. It's fine to use existing modules in a reasonable way to solve problems more effectively, but this paper spends too much space describing widely used designs, which weakens its originality.\n\n* For methods with a similar core idea, such as BARD GS, COGS, DeGauss, DynaSplat, and GauFre (mentioned in Remark I), the experiments only compare with GauFre. The paper should also include results against these methods to better show the advantages of the proposed method over similar works.\n\n* The paper should clearly specify the full composition of the final loss function used in training.\n\nIn addition, I have several questions regarding the experimental section:\n\n* For the NeRF-DS dataset, what resolution was used during training? Some metrics reported in Table 1 are lower than those provided in the original papers. For example, the PSNR of Deformable 3DGS is 24.11 originally but 23.64 here, and GauFre reports 23.9 originally but 23.54 here. What factors contribute to these discrepancies?\n\n* For the NeRF-DS dataset, MotionGS[3] reports a PSNR of 24.54 and has been open-sourced. Since MotionGS outperforms this work in quantitative results, I would like to know why it was not included as a comparison. What are the key advantages or contributions of this work compared with MotionGS?\n\n* For the HyperNeRF and iPhone datasets, were all scenes used or only a subset? I suggest providing the detailed per scene results in the appendix if there is not enough space in the main paper.\n\n* Could you provide visual comparison results on the iPhone dataset?\n\n[1] MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds. CVPR 2025.\n\n[2] Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling. AAAI 2025.\n\n[3] MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting. NeurIPS 2024."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P4QveWcToo", "forum": "3XGqsfKIIK", "replyto": "3XGqsfKIIK", "signatures": ["ICLR.cc/2026/Conference/Submission6499/Reviewer_nwW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6499/Reviewer_nwW2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760283183, "cdate": 1761760283183, "tmdate": 1762918869651, "mdate": 1762918869651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed SplitGaussian framework introduces a two-branch design that explicitly separates static and dynamic Gaussians while integrating spatiotemporal encoding, deformation networks, and visibility-driven pruning for interpretable and stable reconstructions. Furthermore, the inclusion of depth-aware pretraining and visibility-based pruning enhances geometric stability, accelerates convergence, and improves static reconstruction reliability, effectively overcoming the limitations of prior single-motion-field-based 4DGS models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a reasonable two-branch framework that explicitly separates static and dynamic Gaussians. By integrating spatiotemporal encoding, deformation networks, and visibility-driven pruning, the approach provides a clear structure for handling motion and contributes to more stable reconstruction results, even if the overall architectural novelty is moderate.\n2. The inclusion of depth-aware pretraining and visibility-based pruning improves geometric stability, accelerates convergence, and enhances static reconstruction reliability. This training design helps mitigate some limitations observed in previous single-motion-field-based 4DGS models, leading to more consistent optimization behavior."}, "weaknesses": {"value": "1. The proposed method heavily relies on the accuracy of the binary mask used to separate static and dynamic regions. Since this mask plays a crucial role in guiding the disentanglement process, the overall reconstruction quality may be highly sensitive to mask quality. Therefore, it is important to demonstrate the robustness of the method across different mask generation models or noise levels. An ablation or sensitivity analysis on mask reliability would strengthen the claims.\n\n2. Lack of comparison with existing static–dynamic decomposition methods. Several prior works (e.g., [1], [2], [3]) also adopt explicit or implicit strategies to separate static and dynamic components during optimization. To fully validate the effectiveness of the proposed disentanglement framework, it is necessary to include both quantitative and qualitative comparisons against these methods, especially in terms of how well each approach preserves static geometry and isolates dynamic motion.\n\n3. The quantitative results reported in the original MoDec-GS [4] paper are higher than those of the proposed method presented in Table 2 of this paper, raising questions about the claimed superiority of the approach. This suggests that the proposed model may not have achieved sufficient generalization capability for monocular 4D scene reconstruction. Additional quantitative comparisons with a broader range of existing methods, particularly on real-world datasets such as iPhone and HyperNeRF, are necessary to more clearly demonstrate the robustness and generalization capability of the proposed framework.\n\n<Minor weakness>\nAlthough this paper addresses the separation between static and dynamic components, it lacks sufficient discussion of existing static–dynamic decomposition methods. In particular, among monocular 4DGS models, prior approaches that separate static and dynamic regions beyond using a single motion field (e.g., [1], [2], [3]) should be clearly described. Including these related works and elaborating on how they motivate the proposed method would help strengthen the overall motivation and positioning of the paper."}, "questions": {"value": "Please refer to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jpgJZ5xZGX", "forum": "3XGqsfKIIK", "replyto": "3XGqsfKIIK", "signatures": ["ICLR.cc/2026/Conference/Submission6499/Reviewer_959i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6499/Reviewer_959i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952894801, "cdate": 1761952894801, "tmdate": 1762918869237, "mdate": 1762918869237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}