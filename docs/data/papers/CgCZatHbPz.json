{"id": "CgCZatHbPz", "number": 8784, "cdate": 1758098068848, "mdate": 1759897764233, "content": {"title": "Inter-Task Learning Dynamics in Deep Linear Multi-Task Networks", "abstract": "Despite significant empirical progress in Multi-Task Learning (MTL), the theoretical understanding of task interactions and their dynamics remains limited. We present a theoretical analysis of how task alignment shapes learning dynamics in linear MTL, providing a theoretical justification for why task importance is inherently dynamic and why loss weighting schemes should adapt during training.\nLeveraging the Riccati formulation of gradient flow, we analytically characterize the evolution and interaction of shared and task-specific components in deep linear neural networks. For a broad class of initializations, we show how task alignment and magnitude differences govern the trajectories of task outputs, losses, and neural representations throughout training, as well as the representations at convergence. Our analysis reveals that task alignment impacts learning speed and modulates the relative importance of tasks throughout training, with magnitude differences further amplifying these effects. We further show that these factors determine how the structural relationships of the tasks are encoded at convergence in deep linear networks.\nOur framework provides a principled comparison between single-task and multi-task settings, grounded solely in data and task alignment. These results establish a theoretical foundation for understanding task interactions and pave the way toward principled approaches to adaptive loss weighting and task grouping.", "tldr": "", "keywords": ["Multi-Task Learning; Deep Learning; Learning Dynamics"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0886179f798a77b3b0fbe440cb3206c1f4e1f7b7.pdf", "supplementary_material": "/attachment/1eb8dded7e9141fc34c67bbaed01c26117bbc98d.zip"}, "replies": [{"content": {"summary": {"value": "This work presents a theoretical analysis of learning dynamics in multi-task learning (MTL), focusing on the common hard parameter sharing setting.\nThe approach of the authors follows a series of works in the single-task setup, where two-layer deep linear networks are analyzed through the use of the Riccati formulation of gradient flow. These results are adapted and employed in the MTL setting.\nA detailed analysis in the cases of aligned, conflicting and orthogonal tasks is presented, somewhat mirroring common intuitions in the MTL literature. \nThe appendix compares these theoretical results with a small dataset commonly employed in the MTL literature."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The MTL setting is still far from being understood, with a series of works in the area drawing conflicting conclusions for instance concerning the utility of multi-task optimizers. \nThis work is definitely a step towards providing a better understanding of MTL learning dynamics, and will hopefully pave the way for practical advances.\nMost of the results also appear to be relatively intuitive, and in line with common intuitions in the area.\nI appreciate the effort by the authors to provide intuition behind their results, and the extensive appendices."}, "weaknesses": {"value": "- While the work is definitely interesting and novel, its methodology heavily builds on previous work on STL. This is however appropriately acknowledged by the authors.\n- The employed assumptions appear to be extremely strict, and of course do not apply to deep MTL.\n- Not a lot of emphasis is placed on real-world data. Multi-MNIST results are delegated to a fairly long appendix, however it would be quite important to more concisely and directly discuss the relationship between the theoretical results and real-world data in the main body of the paper.\n- Some of the authors' conclusions are not immediately apparent from the figures. For instance, in Figure 3, it is honestly quite hard to discern any difference between the orthogonal and the conflicting tasks setups. This is fairly surprising, though. I am even more surprised by the fact, when considering the loss function in Figure 4, orthogonal tasks seem to make for a harder learning problem.\n- As far as I understand, the provided theory will not easily take into account the effect of network capacity, which however appears to be a very important factor in MTL, especially considering whether any task orthogonality will effectively hinder the learning process."}, "questions": {"value": "- How can network capacity be factored in the provided theory?\n- The practice of Multi-MNIST appears to more or less mirror the provided theory. Would this happen on larger-scale MTL datasets too?\n- Could you please share some more intuition as to how your results could influence the practical design of MTL techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WpAkjBSz9p", "forum": "CgCZatHbPz", "replyto": "CgCZatHbPz", "signatures": ["ICLR.cc/2026/Conference/Submission8784/Reviewer_15xH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8784/Reviewer_15xH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669686059, "cdate": 1761669686059, "tmdate": 1762920558909, "mdate": 1762920558909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the Riccati formulation of gradient flow from single-task to multi-task learning in deep linear networks. The authors characterize how task alignment (measured through SVD overlap) and magnitude differences affect learning dynamics, showing that these factors determine whether tasks learn simultaneously, sequentially, or in a conflicting manner. The main theoretical contribution is an analytical solution for the evolution of shared and task-specific components in linear MTL networks, which provides a motivation for dynamic loss weighting schemes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Practical motivation**: The work addresses a real gap - the lack of theoretical understanding of task interactions in MTL. The connection to dynamic loss weighting is a valuable insight, even if not fully developed.\n\n**Clear presentation**: The paper is well-written with effective visualizations. Figure 2 and 3 nicely illustrate the key phenomena of how task alignment affects learning dynamics.\n\n**Systematic analysis**: The categorization of task relationships (aligned/orthogonal/conflicting) and their impact on learning provides a useful framework for thinking about MTL."}, "weaknesses": {"value": "**Limited theoretical novelty**: The core contribution is essentially applying Braun et al. (2022)'s framework to a multi-task setting. The mathematical machinery remains unchanged - just the dimensions and notation are extended. The proofs follow the same structure with minimal technical innovation.\n\n**Restrictive MSE-only assumption**: The paper assumes all tasks use MSE loss, which severely limits its applicability. Real-world MTL typically involves heterogeneous tasks - classification (cross-entropy), regression (MSE), ranking, etc. For example, in computer vision, depth estimation (MSE) often trains jointly with semantic segmentation (CE). The interaction between different loss types could fundamentally change the dynamics, yet the current framework cannot handle this. This makes the theoretical insights less relevant to practical MTL systems.\n\n**Lack of algorithmic contributions**: Despite providing theoretical motivation for dynamic loss weighting, the paper fails to propose any concrete adaptive weighting algorithms based on their analysis. This is a significant missed opportunity, the theoretical insights about task alignment and learning dynamics could directly inform the design of new weighting strategies.  Without demonstrating how their theory translates into practical algorithms, the paper remains purely descriptive rather than prescriptive. \n\n**Shallow insights**: While the paper describes *what* happens under different task alignments, it doesn't explain *why* these phenomena occur at a deeper level. For instance, why exactly do conflicting tasks create non-monotonic learning curves? The analysis stays at the level of observing consequences of the analytical solutions rather than providing fundamental understanding.\n\n**Experimental limitations**: \n- All experiments use toy problems with small dimensions\n- No comparison with existing dynamic weighting methods (GradNorm, PCGrad, etc.)\n\n**Gap between theory and practice**: The results are confined to linear networks with restrictive assumptions (whitened inputs, zero-balanced weights, MSE loss only). The paper doesn't bridge this gap or provide actionable insights for real MTL systems. How do these results guide the design of actual MTL algorithms?\n\n**Missing key analyses**:\n- No discussion of which task alignments are optimal for generalization\n- No theoretical analysis of sample complexity in MTL vs STL\n- The convergence analysis (Theorem 3) is just stating what the solution converges to, without rates or conditions"}, "questions": {"value": "1. **Can you provide learning dynamics for realistic problem sizes?** The current examples are too small to be convincing. What happens with, say, 100-dimensional inputs and 10 tasks?\n\n2. **How do your theoretical predictions compare with actual dynamic weighting algorithms?** It would strengthen the paper to show that your theory explains when methods like GradNorm succeed or fail.\n\n3. **What happens when assumptions are violated?** Real data isn't whitened, and weights aren't zero-balanced. How robust are your conclusions?\n\n4. **Can you derive optimal task groupings from your framework?** Given task statistics, can you predict which tasks should be trained together vs separately?\n\n5. **The \"conflicting\" case ($0 < \\alpha_i< 1$) seems to cover a huge range of scenarios.** Can you provide more granular analysis? When does partial alignment help vs hurt?\n\n6. **How does the number of tasks NT scale in your analysis?** All examples show $N_T=2$. Does the framework become intractable or reveal new phenomena for many tasks?\n\n7. **What's the computational cost of computing these theoretical predictions?** Could they be used in practice to guide training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wH8K4CKhPR", "forum": "CgCZatHbPz", "replyto": "CgCZatHbPz", "signatures": ["ICLR.cc/2026/Conference/Submission8784/Reviewer_HJRd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8784/Reviewer_HJRd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816634551, "cdate": 1761816634551, "tmdate": 1762920558503, "mdate": 1762920558503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work extends a Riccati framework to deep linear multi-task learning and provides an analysis of the training dynamics and task interactions. It analyzes how task alignment and magnitude imbalance shape interaction patterns: orthogonal, conflicting, and aligned."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It categorizes tasks into orthogonal, aligned, and conflicting cases and clarifies, in a principled way, when interference grows.\n2. It visualizes how alignment and magnitude imbalance lead to learning delays and changing task importance."}, "weaknesses": {"value": "1. As the authors already note, the analysis is limited to deep linear models, which weakens confidence about transfer to realistic nonlinear systems. Beyond that, the paper offers little guidance on how to operationalize the insights, making scalability and generalizability hard to judge. For example, the computational cost of estimating alignment spectra and applying Riccati-based diagnostics on deep networks with large datasets is unclear. It is also uncertain whether online alignment estimation during multi-task training or optimization is feasible.\n\n2. In practice, across many multi-task settings, relationships between tasks rarely fit neatly into “orthogonal,” “aligned,” or “conflicting.” Different aspects can appear simultaneously to varying degrees. How should we handle such mixed cases in real systems? These aspects are not adequately discussed in the paper."}, "questions": {"value": "Please refer to the Weaknesses section and provide detailed responses addressing each point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H3XFoqjMyZ", "forum": "CgCZatHbPz", "replyto": "CgCZatHbPz", "signatures": ["ICLR.cc/2026/Conference/Submission8784/Reviewer_6Wtf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8784/Reviewer_6Wtf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963979821, "cdate": 1761963979821, "tmdate": 1762920557890, "mdate": 1762920557890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies theoretically the dynamic of inter-task learning in linear multi-task learning (MTL) networks. To this end, the authors write down the gradient flow equations of the MTL network using the Riccati formulation and derive from it a closed-form solution for the dynamics of the networks at any step. Then, the authors use these equations to simulate the dynamics of the network under different relationships across tasks (aligned, conflicted, and orthogonal) as well as the effect of having tasks with relatively different magnitudes. Interestingly, the authors also use this approach to obtain theoretical results and simulations on the representations obtained by this linear MTL networks and compare them with their single-task counterparts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S1.** The manuscript seem to be theoretically sound and technically involved. I have not looked into the proofs, but I am pleased to see that the authors corroborated that their findings match a linear MTL in the appendix.\n- **S2.** While technical, the paper is well written and can be easily followed with a bit of patience.\n- **S3.** Experiments are rather extensive given the assumptions on the model, and the conclusions drawn while expected (from my view, these conclusions were relatively known from empirical results over the years), it is great to obtain a theoretical validation.\n- **S4.** I find particularly interesting the results regarding the representations learned by the models in the MTL setup."}, "weaknesses": {"value": "- **W1.** My main concern is the extent to which the assumptions made make the setting interesting. In particular, the assumption that concerns me the most is assumption A3: It is relatively well known in the community that if there is no bottleneck the model will learn all tasks one way or another. The more interesting set-up is when there is a bottleneck and the model needs to learn to use a shared representation across tasks. The linearity of the model also concerns me, but to a lesser extent.\n- **W2.** Another concern is the novelty of the theoretical results. In particular, the article constantly references for the results and proofs to Braun et al. (2022). I'd appreciate it if the authors could make clear what is the contribution of this work in relation with the aforementioned work.\n- **W3.** I find the plots particularly difficult to read. In particular, I am unable to give meaning to those as in Figure 3. I understand that each line represents the evolution over time of one entry of the matrix $W_iW_s$. However, I cannot interpret these values myself.\n- **W4.** Maybe due to the previous point, I do not see how the authors can draw statements regarding the temporal component of the dynamics. For example, in line 395 where is says \"Over time, a task that was initially harmless may later interfere with or suppress the learning of others.\"\n- **W5.** It would be nice to have a section on related work on MTL analysis as well as in MTL techniques to alleviate gradient conflict (such as FAMO, PCGrad, GradNorm, RotoGrad, among many others). Moreover, it would be a great plus to see whether any of these techniques helps with the linear/ReLU MTL examples that appear in the appendix."}, "questions": {"value": "- **Q1.** How does the overlap in definition 2.1 with the cosine similarity between task gradients, which is the de-facto measure used in MTL works?\n- **Q2.** Eq. (11) is rather unclear. Are the $i$ and $j$ powers? Is the $0$ supposed to be a $o$?\n\nOther feedback:\n- Regarding the motivation in the second paragraph of the intro, I'd like to share with the authors [this recent study](https://arxiv.org/abs/2505.10347) that points out at the reasons of why _sometimes_ simple heuristics work as well as other MTL optimizers.\n- Similarly, I'd also like to share [this recently accepted paper](https://web3.arxiv.org/abs/2510.18258) where the study also the effect of tasks in MTL through the lens of neural tangent kernel methods.\n- You are sharing the same label across equations, and sometimes point to the appendix instead that to the main content.\n- I assume the \"+\" in line 263 is supposed to be a \"-\".\n- The caption of Fig. 4 needs to be updated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2eMutOnvek", "forum": "CgCZatHbPz", "replyto": "CgCZatHbPz", "signatures": ["ICLR.cc/2026/Conference/Submission8784/Reviewer_RWSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8784/Reviewer_RWSh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116220102, "cdate": 1762116220102, "tmdate": 1762920557424, "mdate": 1762920557424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}