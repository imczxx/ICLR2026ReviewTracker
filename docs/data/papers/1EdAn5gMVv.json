{"id": "1EdAn5gMVv", "number": 20402, "cdate": 1758305579372, "mdate": 1759896979401, "content": {"title": "SpatialBoost: Enhancing Visual Representation through Language-Guided Reasoning", "abstract": "Despite the remarkable success of large-scale pre-trained image representation models (i.e., vision encoders) across various vision tasks, they often fail to learn 3D spatial relationships between objects and backgrounds in the real world, constraining their effectiveness in various downstream applications. We attribute this to the limited availability of large-scale 3D training data, which makes it difficult for current image representation learning approaches to learn spatial relationships. This motivates the need for learning paradigms that rely on strong supervision while requiring less data. To address this, we propose a novel learning framework that enhances the spatial awareness of existing pre-trained vision encoders by injecting dense 3D spatial knowledge expressed in linguistic forms.\nTo be specific, the core idea involves converting dense 3D spatial information from 2D images into linguistic expressions, which is then used to inject such spatial knowledge into vision encoders through a Large Language Model (LLM). To this end, we adopt a multi-turn Chain-of-Thought (CoT) reasoning process that progressively incorporates dense spatial knowledge and builds hierarchical spatial understanding. To validate effectiveness, we adapt SpatialBoost to state-of-the-art vision encoders such as DINOv3, and evaluate its performance gains on a wide range of benchmarks requiring both 3D perception and general vision abilities.", "tldr": "", "keywords": ["image representations", "spatial reasoning", "multi-modal vision representation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5999d7d584ab2b6e35a0f1c876a7825a914759ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SpatialBoost, a novel framework for enhancing the spatial awareness of pre-trained vision encoders. The authors identify that existing encoders, trained predominantly on 2D images, fail to capture 3D spatial relationships. Their core idea is to distill dense 3D spatial knowledge, derived from specialist vision models (e.g., for depth estimation), into linguistic form and then use this language-based supervision to fine-tune the encoder. The framework involves two key stages: 1) generating a multi-turn, hierarchical Chain-of-Thought (CoT) dataset where a Large Language Model (LLM) converts geometric information into a series of structured questions and answers, progressing from pixel-level to scene-level understanding; and 2) fine-tuning the vision encoder using this dataset. To prevent catastrophic forgetting of the encoder's original capabilities, they employ a dual-channel attention mechanism that adds trainable parameters alongside the frozen original ones. The authors demonstrate that applying SpatialBoost to state-of-the-art encoders like DINOv3 and SigLIPv2 leads to consistent performance improvements across an extensive and diverse set of downstream tasks, including dense prediction, 3D scene understanding, robotic control, and even general image classification and retrieval."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel and Effective Framework:** The central idea of using language as an intermediate representation to \"teach\" a vision encoder about 3D space is both clever and highly effective. Instead of relying on complex architectural changes or end-to-end training with limited 3D data, the framework elegantly distills knowledge from powerful specialist models into a format (structured language) that can be easily injected into the encoder via an LLM-based training loop.\n2. **Comprehensive and Rigorous Evaluation:** The experimental validation is exceptionally strong and is a major highlight of the paper. The authors evaluate their method across a very wide range of challenging benchmarks:\n    - **Geometric/Dense Tasks:** Monocular depth estimation (NYUd, KITTI) and semantic segmentation (ADE20K, Pascal VOC).\n    - **Complex 3D Tasks:** The Lexicon3D benchmark, covering 3D VQA, visual grounding, and geometric understanding.\n    - **Downstream Applications:** Vision-based robot learning on CortexBench.\n    - **General Vision Tasks:** ImageNet classification and multiple image retrieval benchmarks.\n    The fact that SpatialBoost provides consistent, and often significant, improvements across all these diverse domains is very convincing.\n3. **Successfully Addresses Catastrophic Forgetting:** A critical challenge when fine-tuning foundation models is preventing the degradation of their general capabilities. The paper's use of a dual-channel attention mechanism is a simple yet powerful solution. The ablation in Figure 6 and the results in Table 5 are particularly compelling, showing that not only does SpatialBoost avoid performance loss on ImageNet classification, it actually *improves* upon the original SoTA encoders. This is a very strong result, suggesting the injected spatial knowledge also acts as a useful regularizer for general vision tasks.\n4. **High-Quality Data Generation Process:** The design of the multi-turn, hierarchical (pixel → object → scene) VQA dataset is well-reasoned. The ablation study in Table 7 confirms that this structured, forward-reasoning approach is more effective than random or reversed orders, validating the CoT-inspired design."}, "weaknesses": {"value": "1.  **Complexity and Cost of Data Pipeline:** The data generation process, while effective, is quite complex and relies on a cascade of pre-existing models (depth estimators, segmentation models, 3D reconstruction models) as well as a powerful proprietary LLM (GPT-4o). This makes the framework potentially expensive and difficult to replicate or extend to new domains without significant engineering effort.\n\n2.  **Dependence on External Models for Ground Truth:** The quality of the supervision signal is fundamentally capped by the performance of the specialist models used to extract 3D information and the LLM used to convert it to text. Any errors or biases from these external models are directly propagated into the training data for the vision encoder.\n\n3.  **Clarity in Presentation:** While generally well-written, some sections could benefit from greater clarity. The initial overview in Figure 1 is a bit high-level, and a more detailed architectural diagram combining the data generation and training loops would be helpful for understanding the full system at a glance."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RsUCdOvGg9", "forum": "1EdAn5gMVv", "replyto": "1EdAn5gMVv", "signatures": ["ICLR.cc/2026/Conference/Submission20402/Reviewer_sYwN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20402/Reviewer_sYwN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881747996, "cdate": 1761881747996, "tmdate": 1762933848883, "mdate": 1762933848883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SpatialBoost, a technique that inject spatial knowledge into the representations of visual foundation models through LLMs. Specifically, it finetunes the representations from visual foundation models by building up a LLaVA-like architecture to project visual foundation model features to the token space of LLMs. Afterwards, the whole pipeline is tuned with spatial-related tasks like multi-view VQA and multi-turn visual reasoning. Experimental results demonstrate that after finetuning the visual foundation models with SpatialBoost, the features achieve better performance for a variety of tasks including dense predictions, 3D understanding, robot learning, and high-level tasks like image classification and retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method of using LLM supervision to enhance the spatial information for visual representations is simple but effective. The motivation is reasonable, and the solution is natural and intuitive.\n\n- After applying the feature finetuning method SpatialBoost, the visual representations have universal improved performance on various types of tasks including dense predictions, 3D understanding, robot learning, and even high-level tasks like image classification and retrieval. It is very challenging from my opinion, and hard to achieve by previous feature finetuning approaches, because many finetuning methods on visual representations could actually result in a performance tradeoff across different types of tasks, which is also demonstrated in Table 6 in the paper. Therefore, I think the proposed SpatialBoost is a well-rounded solution for enhancing the visual representations.\n\n- The proposed SpatialBoost can bring performance gain by injecting spatial information to many kinds of foundation model features, including CLIP, SigLIP, DINOv2, and even the most recent DINOv3, which demonstrates the robustness, generalizability, and broad application of this approach.\n\n- The presentation of this paper is smooth and clear, which makes the proposed solution easy to understand."}, "weaknesses": {"value": "- I think it is mostly a well-written paper with effective solutions and comprehensive experimental evaluations. One thing that I find a bit confusing is the categorization on the multi-turn spatial reasoning task. In Figure 2, there is a pyramid showing three levels of spatial knowledge being scene-level QA, object-level QA, and pixel-level QA. However, I do not quite agree that the corresponding examples on the right (the three QA examples in Figure 2 corresponds to the left using the color match, if I understand correctly) can be categorized into these different levels of QA tasks. For example, the one shown in the blue bounding box is categorized into scene-level QA. However, it needs the model to have pixel-level localization capability to understand the spatial information of the specific pixels in the images. Therefore, I do not think the of these QA samples are quite properly categorized."}, "questions": {"value": "- We have seen that the models finetuned on multi-view VQA and multi-turn visual spatial reasoning tasks can have universal benefit on all types of tasks. It could be interesting to further find out which downstream tasks get more benefits from which finetuning tasks. Having this information can be provide guidance for users that want to tune the visual representations for some specific downstream tasks.\n\n- For multi-turn visual spatial reasoning, is it always following the order of scene-level QA, object-level QA, then pixel-level QA, basically to narrow down the scope of the questions? Or the questions are just sampled from all categories with random order?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F6GtBBuU3s", "forum": "1EdAn5gMVv", "replyto": "1EdAn5gMVv", "signatures": ["ICLR.cc/2026/Conference/Submission20402/Reviewer_cDb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20402/Reviewer_cDb7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932402195, "cdate": 1761932402195, "tmdate": 1762933848252, "mdate": 1762933848252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SpatialBoost, a framework designed to enhance existing vision encoders with 3D spatial knowledge. Specifically, the method converts 3D spatial information into linguistic expressions and injects this knowledge into vision encoders through a language-guided reasoning process, utilizing a multi-turn CoT approach and a dual-channel attention mechanism to preserve pre-trained capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work is motivated by a well-identified gap in 2D vision encoders—their inherent lack of 3D spatial understanding. The proposed approach of converting 3D information into linguistic expressions is innovative."}, "weaknesses": {"value": "- The spatial CoT data is generated using GPT-4o. There lacks of evaluation regarding the quality of this synthetic data. GPT-4o may suffer from significant hallucinations on spatial-related questions, which could compromise data reliability.\n\n- Experiments are conducted only on vision encoders like DINOv2, OpenCLIP, and the Qwen2.0-7B LLM. The method lacks comparison with recent state-of-the-art MLLMs such as Qwen2.5-VL or InternVL3. It remains unclear whether the approach would be effective when applied to these more advanced models.\n\n- The use of CoT data format increases the sequence length during inference. It is important to quantify how many additional tokens this introduces and whether it leads to significantly higher computational costs."}, "questions": {"value": "- How was the quality of the GPT-4o generated spatial CoT data assessed? What measures were taken to identify and mitigate potential hallucinations in the synthetic spatial QA pairs?\n\n- How many additional tokens does the multi-turn CoT reasoning introduce during inference compared to standard VQA? Could you provide an analysis of the increase in computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zmY31IgD4a", "forum": "1EdAn5gMVv", "replyto": "1EdAn5gMVv", "signatures": ["ICLR.cc/2026/Conference/Submission20402/Reviewer_3LpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20402/Reviewer_3LpB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969611046, "cdate": 1761969611046, "tmdate": 1762933847738, "mdate": 1762933847738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper hypothesizes that language supervision can provide a more effective way to inject 3D spatial knowledge into vision encoders than dense pixel-level tasks. To test this, the authors propose a VLM-based post-training framework that uses spatial reasoning Chain-of-Thought (CoT) data to refine pre-trained vision encoders. The idea is to convert dense 3D spatial information into linguistic descriptions and gradually teach the encoder hierarchical spatial understanding. Experiments show consistent improvements across a range of benchmarks involving both 3D perception and general vision tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The main idea is interesting and clear: using language-based supervision to improve spatial understanding in vision encoders.\n* The method is easy to apply on top of existing pre-trained models, which makes it practical and useful for the community.\n* The experiments are extensive, covering many benchmarks involving both 3D perception and general vision tasks. \n* The results suggest that language supervision can transfer structured spatial knowledge into dense prediction tasks, which is an interesting insight."}, "weaknesses": {"value": "* Although the hypothesis is interesting, it is not fully supported by the results. In Table 6, the LLM-based fine-tuning appears roughly on par with some dense prediction baselines, so the advantage of the language-based supervision is not clearly demonstrated.\n* The comparison in Table 6 is also difficult to interpret because the baselines are not trained on the same data. A fair comparison on the exact same subset & same amount of data would make the claim stronger.\n* Many performance gains in Tables 1–4 seem to come from the post-training data itself, rather than specifically from the language-based supervision. This raises the concern that the improvements may not require the proposed linguistic reasoning framework."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JJZo69PwCQ", "forum": "1EdAn5gMVv", "replyto": "1EdAn5gMVv", "signatures": ["ICLR.cc/2026/Conference/Submission20402/Reviewer_kPA7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20402/Reviewer_kPA7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988229047, "cdate": 1761988229047, "tmdate": 1762933847203, "mdate": 1762933847203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}