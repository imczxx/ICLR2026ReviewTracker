{"id": "8tDIzHFOx6", "number": 15779, "cdate": 1758255135275, "mdate": 1759897282690, "content": {"title": "SPR$^2$Q: Static Priority-based Rectifier Routing Quantization for Image Super-Resolution", "abstract": "Low-bit quantization has achieved significant progress in image super-resolution. However, existing quantization methods show evident limitations in handling the heterogeneity of different components. Particularly under extreme low-bit compression, the issue of information loss becomes especially pronounced. In this work, we present a novel low-bit post-training quantization method, namely static priority-based rectifier routing quantization (SPR$^2$Q). The starting point of this work is to attempt to inject rich and comprehensive compensation information into the model before the quantization , thereby enhancing the model's inference performance after quantization. Firstly, we constructed a low-rank rectifier group and embedded it into the model's fine-tuning process. By integrating weight increments learned from each rectifier, the model enhances the backbone network while minimizing information loss during the lightweighting process. Furthermore, we introduce a static rectifier priority routing mechanism that evaluates the offline capability of each rectifier and generates a fixed routing table. During quantisation, it updates weights based on each rectifier's priority, enhancing the model's capacity and representational power without introducing additional overhead during inference. Extensive experiments demonstrate that the proposed SPR$^2$Q significantly outperforms the state-of-the-arts in five benchmark datasets, achieving PSNR improvements of 0.55 and 1.31 dB on the Set5($\\times 2$) dataset under 4-bit and 2-bit settings, respectively.", "tldr": "", "keywords": ["Image Super-Resolution", "model quantization", "adapter routing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd4226d6365b7bf80c6c65ce5ed02e71b299d2cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SPR2Q, a static priority-based rectifier routing quantization method. The overall pipeline can be divided into two parts. The first part leverages a low-rank rectifier group to enhance the backbond network. The second part uses offline static routing calibration to obtain the SPR2Q table to assign the optimal increment for each layer. The proposed method achieves SOTA performance on Mamba with five commonly used benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The design is effective and clear, obtaining SOTA performance with both metric and visual comparison on the SR task.\n- The writing is clear and easy to follow."}, "weaknesses": {"value": "- The proposed method is only tested on MambaIRv2-light. However, the proposed method can be safely tested on more models, including MambaIRv2_SR2, SwinIR. Please provide the results on these models to demonstrate the generalization ability.\n- Please provide the complexity of the SPR2Q model, including model parameters, storage, inference speedup ratio, and GPU memory usage. These metrics are critical to the model lighting.\n- The rank of each recitifier module is important. However, the influence of the rank is not discussed in the ablation study. Please provide the results of various ranks and the results of full-rank."}, "questions": {"value": "- In Figure 1, it should be \"update\" instead of \"updata\". The corresponding \n- What is the limit of the contribution from the Rectifier group size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2DecYtfoDk", "forum": "8tDIzHFOx6", "replyto": "8tDIzHFOx6", "signatures": ["ICLR.cc/2026/Conference/Submission15779/Reviewer_xceu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15779/Reviewer_xceu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551739237, "cdate": 1761551739237, "tmdate": 1762926011893, "mdate": 1762926011893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Static Priority-based Rectifier Routing Quantization (SPR2Q), a novel post-training quantization method for low-bit image super-resolution. Unlike existing approaches that struggle with severe information loss under extreme low-bit settings, SPR2Q injects rich compensation information into the model before quantization by embedding a low-rank rectifier group during fine-tuning. A static rectifier priority routing mechanism then evaluates each rectifier’s capability offline and updates weights accordingly without adding inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of learning compensation information through lightweight rectifiers is very interesting. In addition, the use of dynamic routing during training followed by static rectifier routing is an effective design choice, improving performance without increasing computational cost. Experimental results support the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The method is tested only on a single SR baseline, MambaIRv2-light. To better demonstrate its applicability and generalization capability, it should also be tested against other baselines, such as MaIR (CVPR 2025), EAMamba (ICCV 2025), and First-Order State Space Model for Lightweight Image Super-Resolution (ICASSP 2025).\n\n2. Since the main contribution lies in the use of multiple rectifiers, a more in-depth analysis of their internal behavior is expected. For example, the authors mention that each rectifier handles different types of information, but visual or statistical evidence is needed to support this claim. In addition, the analysis of optimal gate weights should be expanded to better explain their optimality.\n\n3. The performance of PTQ depends heavily on the training dataset. In particular, the proposed offline routing calibration method appears to target different domains; however, the experiments are conducted using only a single training dataset.\n\n4. The paper would benefit from careful linguistic revision and thorough proofreading."}, "questions": {"value": "The authors used rectifiers of the same size. Have the authors considered using rectifiers with different capacities, so that multiple experts can more effectively distribute the workload?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V9CdepAhSO", "forum": "8tDIzHFOx6", "replyto": "8tDIzHFOx6", "signatures": ["ICLR.cc/2026/Conference/Submission15779/Reviewer_xpnk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15779/Reviewer_xpnk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712668822, "cdate": 1761712668822, "tmdate": 1762926011481, "mdate": 1762926011481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPR2Q, a post-training quantization (PTQ) framework specifically designed for image super-resolution (SR) with an emphasis on Mamba-based architectures. The key insight is that current PTQ techniques do not sufficiently prepare the model for quantization, especially under aggressive bit-width reduction (e.g., 2-bit or 1-bit).\n\nTo address this, the authors propose two complementary techniques:\n1.\tPre-Quantization Fine-Tuning with Fused Rectifiers (PQFR):\nLearnable, low-rank rectifiers are fused into model weights before quantization to reduce error and preserve representational fidelity.\n2.\tStatic Priority-Based Rectifier Routing (SPR2):\nA mechanism for statically assigning rectifiers to layers using an offline-calibrated routing table, which introduces diversity without runtime overhead.\n\nExtensive experiments on five standard SR datasets and comparisons against three strong Mamba-specific quantization baselines (PTQ4VM, Quamba, and MambaQuant) show consistent performance improvements, especially in low-bit regimes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper clearly identifies a relevant and under-addressed problem: the difficulty of applying post-training quantization to super-resolution models, especially in architectures like Mamba that are sensitive to numerical errors due to their recurrent components. The need for specialized approaches in SR is well-motivated.\n2.\tThe PQFR module is inspired by LoRA but adapted for PTQ in the SR setting, which is novel. The SPR2 routing table design is simple, efficient, and effective. It offers a practical compromise between dynamic routing (high cost) and naive shared rectifiers (low performance). And the method is well-integrated into existing training pipelines and avoids runtime penalties.\n3.\tThe method requires only modest fine-tuning and has negligible inference overhead. It is applicable in edge scenarios where low-bit inference is essential."}, "weaknesses": {"value": "1.\tThe entire evaluation is performed on the MambaIRv2-light model. While the motivation is grounded in the Mamba architecture, the method itself (particularly PQFR) should generalize. Add at least one experiment on a Transformer-based SR model (e.g., SwinIR) or a CNN-based model (e.g., EDSR) to support claims of generality.\n2.\tThe routing table optimization (Equation 12) is essential to SPR2Q, yet the paper omits important implementation details. It is unclear whether the optimization of gating weights is performed via exhaustive search, gradient descent, or heuristics. I would like to see more information about how the routing weights ĝ are optimized, how long this process takes, and whether it scales with model size.\n3.\tAlthough the paper claims no additional inference cost, no quantitative measurements are presented. This is important to validate the claim that the fused rectifiers do not affect runtime. I would be great if include a table comparing runtime, model size, and memory usage (before and after SPR2Q), even on a single dataset, to support the efficiency claim."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EXxKAUZytO", "forum": "8tDIzHFOx6", "replyto": "8tDIzHFOx6", "signatures": ["ICLR.cc/2026/Conference/Submission15779/Reviewer_bGTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15779/Reviewer_bGTK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814537819, "cdate": 1761814537819, "tmdate": 1762926010876, "mdate": 1762926010876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper proposes a static priority rectifier routing quantization framework (SPR ² Q) for the low bit post training quantization (PTQ) problem of Mamba architecture image super-resolution (SR) model. This framework injects learnable compensation information through a pre quantized fusion rectifier module and combines it with a static rectifier priority routing mechanism to provide diversified compensation strategies. It effectively alleviates quantization information loss under extremely low bit (2-bit, 1-bit) settings and achieves better performance than existing SOTA methods such as PTQ4VM and Quamba on five benchmark datasets including Set5 and Urban100. Especially, it achieves PSNR improvements of 0.55dB and 1.31dB on 4-bit and 2-bit Set5 (× 2) tasks, respectively."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method proposed in the paper appears to be quite general and easy to integrate into existing models.\n\n2. The paper provides experimental results under different model settings and bits, as well as ablation experiments.\n\n3. The paper achieved SOTA performance under different settings."}, "weaknesses": {"value": "1. The LoRA fine-tuning in the paper requires end-to-end training, which brings additional memory and time overhead compared to other PTQ methods. And the paper uses a lightweight MambaIR model. If other versions such as MambaIR_SR are used, can Lora training be efficiently performed?\n\n2. As the paper requires LoRA fine-tuning and weight updates, I am puzzled whether this is a traditional PTQ method and whether it is reasonable to compare it with other PTQ methods that do not require training?\n\n3. The paper does not report the calibration resource comparison, such as GPU time and GPU memory.\n\n4. The paper did not provide ablation experiments on LoRA's rank and router group size for calibration resources and performance.\n\n5. Does 4bit denotes W4A4 or W4A16? And the quantization memory reduction effect and inference acceleration should be reported."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1RmJDO6ly6", "forum": "8tDIzHFOx6", "replyto": "8tDIzHFOx6", "signatures": ["ICLR.cc/2026/Conference/Submission15779/Reviewer_jipM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15779/Reviewer_jipM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925568503, "cdate": 1761925568503, "tmdate": 1762926010371, "mdate": 1762926010371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}