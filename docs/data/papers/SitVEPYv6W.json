{"id": "SitVEPYv6W", "number": 4741, "cdate": 1757755689268, "mdate": 1763109906396, "content": {"title": "Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning", "abstract": "We study distributed reinforcement learning (RL) with policy gradient methods under asynchronous and parallel computations and communications. While non-distributed methods are well understood theoretically and have achieved remarkable empirical success, their distributed counterparts remain less explored, particularly in the presence of heterogeneous asynchronous computations and communication bottlenecks. We introduce two new algorithms, Rennala NIGT and Malenia NIGT, which implement asynchronous policy gradient aggregation and achieve state-of-the-art efficiency. In the homogeneous setting, Rennala NIGT provably improves the total computational and communication complexity while supporting the AllReduce operation. In the heterogeneous setting, Malenia NIGT simultaneously handles asynchronous computations and heterogeneous environments with strictly better theoretical guarantees. Our results are further corroborated by experiments, showing that our methods significantly outperform prior approaches.", "tldr": "", "keywords": ["reinforcement learning", "federated learning", "distributed learning", "asynchronous methods"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f78ec5e8a8fc6da81b71524fe49c9476e61611b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies distributed reinforcement learning (RL) with asynchronous policy gradient methods. The authors propose two new algorithms: Rennala NIGT for homogeneous environments and Malenia NIGT for heterogeneous environments. Both methods extend normalized policy gradient techniques with asynchronous aggregation, achieving improved computational and communication complexity compared to prior work, e.g., AFedPG. The authors also establish a new lower bound and validate the methods on MuJoCo tasks, showing robustness to heterogeneity and communication delays."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**Originality.** Proposes new algorithms that generalize normalized gradient techniques to asynchronous distributed RL, with contributions in both homogeneous and heterogeneous settings.\n\n**Quality.** Strong theoretical analysis, including new upper bounds, support for AllReduce, and a new lower bound. Experimental results align with the theory.\n\n**Clarity.** The paper is well-structured, with clear algorithm pseudocode and comparisons.\n\n**Significance.** Distributed RL is critical for scaling up to large systems. The improved communication and computation guarantees, especially with AllReduce support, are practically valuable."}, "weaknesses": {"value": "**Optimality gap** There remains a gap between the presented upper bounds (κε⁻²) and the lower bound (κε⁻¹²/⁷), leaving open whether the methods are near-optimal."}, "questions": {"value": "In Malenia NIGT, the mean-like dependency on agent speeds is less favorable than the harmonic dependency in Rennala NIGT. How does this affect performance in highly heterogeneous environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jf25HZVLpY", "forum": "SitVEPYv6W", "replyto": "SitVEPYv6W", "signatures": ["ICLR.cc/2026/Conference/Submission4741/Reviewer_KcX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4741/Reviewer_KcX2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760585884660, "cdate": 1760585884660, "tmdate": 1762917548550, "mdate": 1762917548550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates distributed policy gradient (PG) methods in asynchronous and parallel reinforcement learning (RL) settings. The authors introduce two new algorithms for homogeneous environments and heterogeneous environments. Both algorithms are based on normalized policy gradient methods and asynchronous aggregation schemes inspired by recent work (e.g. Lan et al. 2025)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* I briefly checked the proofs and they seem correct. This means the paper improves over the communication complexity of existing methods. \n\n* They also show a lower bound for the homogenous setting in the appendix which seems to show there is room for improvement."}, "weaknesses": {"value": "* There is a lack of novelty in the algorithmic components, while one may argue this particular combination is done for the first time.\n\n* On the theoretical side, the proofs are highly reliant on/similar to existing results in federated RL and stochastic optimization. For instance, Lemma D.2 is a standard result (similar to SGDm with normalization and also a corresponding lemma from Lan et al. 2025). What is a new technical result proposed by the paper?\n\n* I also have a concern about the heterogenous experiments in the Appendix H.3. To test Malenia NIGT in a scenario with environment heterogeneity (different $J_i$), the paper uses $n=2$ agents, where one agent receives state $s$ and the other receives state $-s$. To distinguish these, the paper states, \"we concatenate the value 0 to $s_{t+1}$... In the case of the second worker, we redirect $(-s_{t+1}, 1)$\". In my view, however, this is not a heterogeneous problem. The algorithm is not learning an average policy $\\frac{1}{2}(J_1(\\theta) + J_2(\\theta))$. It is learning a single policy $J'(\\theta)$ for a single environment whose state space is $\\mathcal{S} \\times \\{0, 1\\}$. The policy network can trivially learn to behave differently based on the appended bit."}, "questions": {"value": "Please see the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GteflYeFxm", "forum": "SitVEPYv6W", "replyto": "SitVEPYv6W", "signatures": ["ICLR.cc/2026/Conference/Submission4741/Reviewer_GmQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4741/Reviewer_GmQ8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105599003, "cdate": 1762105599003, "tmdate": 1762917548219, "mdate": 1762917548219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two asynchronous distributed policy-gradient algorithms for reinforcement learning:\n\n* Rennala NIGT for the *homogeneous* setting (all agents share the same environment), and\n* Malenia NIGT for the *heterogeneous* setting (each agent may have a different environment).\n\nBoth algorithms combine the NIGT policy gradient update (Fatkhullin et al., 2023) with asynchronous aggregation mechanisms adapted from recent federated SGD literature (Tyurin & Richtárik, 2023). The paper provides wall-clock convergence guarantees (in terms of computation and communication time) to reach an ε-stationary point under standard policy gradient assumptions. A lower bound for twice-smooth stochastic objectives is also presented, and empirical results on MuJoCo tasks illustrate the wall-clock speedups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The problem setting (efficient RL under asynchronous environments even with heterogeneity) is important and highly relevant.\n* The paper is clearly written and the proofs are easy to follow."}, "weaknesses": {"value": "**Novelty.** Conceptually, I struggle to see what is new here. The algorithm appears to essentially be a direct combination of two known components:\n\n* Rennala/Malenia from federated SGD (Tyurin & Richtárik, 2023), and\n* NIGT-based policy gradient from non-distributed RL (Fatkhullin et al., 2023).\n\nThe analysis likewise seems to be a straightforward combination. In particular, the core convergence argument (Section D, also F) closely follows the standard NIGT analysis, and relies on the fact that Rennala/Malenia provide unbiased policy gradient estimates with bounded variance (∝ 1/M). While it is normal for new work to build on prior components, here it is unclear where the main technical challenge lies (if any). \n\n**Lower bound and Missing Comparisons.** The paper’s main theoretical focus is ε-stationary convergence and also provide a lower bound for this rate (which is assuming access to only stochastic gradient estimates and no other information). The setting for the lower bound feels very artificial especially since there is a substantial body of RL papers that obtain improved ε-stationary rates by exploiting RL-specific structure (see [A],[B],[C] and also Hessian aided PG in (Fatkhullin et al 2023)). It is very odd that these are not even mentioned, let alone discussed or compared against. \n\n\nFinally, although this is less critical, the paper should be better contextualized relative to existing federated PG work. Apart from (Lan et al. 2025), the empirical comparisons focus on non-federated PG methods, which are not the most natural baselines here. There are federated PG methods with ε-stationary guarantees in the synchronous homogeneous case [C], federated PG based on Hessian aided PG in the same regime [D], and heterogeneous PG methods using softmax policies [E]. Including (or at least discussing) such baselines would better position this work within the federated PG literature.\n\n\n\n---\n[A] Xu, Gao, Gu. Sample Efficient Policy Gradient Methods with Recursive Variance Reduction. ICLR 2020.\n\n[B] Xu, Gao, Gu. An Improved Convergence Analysis of Stochastic Variance–Reduced Policy Gradient. UAI 2020.\n\n[C] Fan, Ma, Dai, Jing, Tan. Fault–Tolerant Federated Reinforcement Learning with Theoretical Guarantee. NeurIPS 2021.\n\n[D] Ganesh, Chen, Thoppe, Aggarwal. Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries. TMLR 2024.\n\n[E] Labbi, Mangold, Tiapkin, Moulines. On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment. arXiv 2025."}, "questions": {"value": "Please see weaknesses listed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LjW3ukryhO", "forum": "SitVEPYv6W", "replyto": "SitVEPYv6W", "signatures": ["ICLR.cc/2026/Conference/Submission4741/Reviewer_Codc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4741/Reviewer_Codc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762489690794, "cdate": 1762489690794, "tmdate": 1762917547601, "mdate": 1762917547601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}