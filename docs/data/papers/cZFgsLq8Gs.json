{"id": "cZFgsLq8Gs", "number": 911, "cdate": 1756823224310, "mdate": 1763408892632, "content": {"title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively", "abstract": "While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, using a cumulative Findings Memory to intelligently balance the exploitation of promising avenues with the exploration of novel hypotheses. Consuming over 20,000 GPU hours, the system generated about 5,000 unique ideas and experimentally validated approximately 1100, ultimately surpassing human-designed 2025 state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\\%, 1.9\\%, and 7.9\\%. Crucially, this was achieved by autonomously redesigning core methodologies, not merely recombining existing techniques. In a striking demonstration, the system achieved progress on AI text detection in just two weeks that is comparable to three years of cumulative human research. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier forward. To facilitate further research into this process, we will open-source all experimental logs and system code.", "tldr": "This is the first empirical demonstration of an AI that acts as an autonomous scientist to progressively push research frontiers, successfully discovering novel methods that outperform the human SOTA across multiple domains.", "keywords": ["Automated Scientific Discovery", "Large Language Models (LLMs)", "AI Scientist"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5028df56553c4a4ceb3127c06a8b9c79edc3f555.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces DeepScientist, a system designed to automate goal-oriented scientific discovery. Experimental results suggest that DeepScientist can identify novel methods that outperform state-of-the-art (SOTA) approaches across three frontier AI tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Motivation:** The motivation is strong. Automating scientific discovery is a compelling direction, especially given the recent advancements in LLMs. It is valuable to explore how such systems can accelerate the pace of scientific progress.\n\n**Substance:** The paper presents experiments on three distinct tasks to demonstrate the capabilities of DeepScientist. The generated research papers appear coherent and reasonably well-written, suggesting that the system can meaningfully contribute to scientific exploration."}, "weaknesses": {"value": "**Presentation:** The baselines in Table 2 lacks sufficient explanation and context. Each baseline should be clearly introduced and described in the Experimental Setup section. Additionally, the overall pipeline of DeepScientist appears very similar to existing systems such as AI Scientist and Zochi. A more detailed comparison would help highlight the genuine innovations and unique contributions of DeepScientist. For instance, what key design elements distinguish DeepScientist from these prior works? Moreover, the paper omits a related work, “AlphaGo Moment for Model Architecture Discovery”, which follows a conceptually similar pipeline and should be discussed.\n\n**Evaluation:** The evaluation methodology seems somewhat limited. The results in Table 2 rely on a single automated reviewer system, which raises concerns about robustness. It would strengthen the paper to cross-validate the results using multiple reviewer systems or complementary evaluation approaches. Notably, systems such as Zochi also report high acceptance rates when evaluated by automatic reviewers, yet here its acceptance rate is listed as 0%—this discrepancy deserves further analysis or justification."}, "questions": {"value": "- Line 319: The paper mentions that five research papers generated by DeepScientist were evaluated. How were these papers selected—randomly or based on performance? Given that automatic evaluation is not resource-intensive, why not evaluate all successfully generated papers for a more comprehensive assessment?\n- Line 859: The authors note that all experimental results were manually inspected by human supervisors to ensure authenticity. How extensive was this human involvement? Quantifying the amount of human effort would help assess the true level of automation achieved.\n- System Design: DeepScientist uses Gemini for high-level planning and Claude for implementation and execution. Did the authors observe any performance differences when using stronger or weaker models for these roles? In other words, how does the planner–executor balance affect the overall efficiency and quality of the discovery process? Which component benefits more from having a stronger model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zprMZy4h3x", "forum": "cZFgsLq8Gs", "replyto": "cZFgsLq8Gs", "signatures": ["ICLR.cc/2026/Conference/Submission911/Reviewer_oPiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission911/Reviewer_oPiU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761231585820, "cdate": 1761231585820, "tmdate": 1762915639628, "mdate": 1762915639628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DeepScientist, a system to conduct autonomous scientific discovery. The system formalizes the process of scientific discovery with an iterative Bayesian optimization problem. Utilizing the existing literature, the system iterates through three stages: hypothesize the research gap, implement the new hypothesis, and analyze results, to find novel methods that improve the performance compared to human-designed SOTA methods. The paper uses three tasks, Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection, on which the proposed system identified novel ideas that achieved better performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes a system to enable autonomous scientific discovery and shows its effectiveness by proposing novel ideas with significant performance improvement in three tasks.\n\n2. The paper provides a comprehensive analysis to evaluate multiple aspects of the proposed system and compare it with other systems. It also includes human experts to evaluate both successful and unsuccessful ideas generated by the system.\n\n3. The presentation of this paper is good with well-organized figures to report and visualize results."}, "weaknesses": {"value": "1. It's challenging for the audience without much background in the three AI tasks to evaluate the significance and contribution of the ideas generated by DeepScientist without the context of prior work. Although some details of the generated ideas are included in Sec 4.1, it might be helpful to also include a brief introduction of the human-designed method that the system primarily gets inspired by, and compare it with the method generated by the system.\n\n2. The proposed system uses a few hyperparameters in Eq 1 to select the hypothesis for verification, but the strategy of choosing the hyperparameters remains unclear.\n\n3. This system relies heavily on the LLM that evaluates the estimated utility, quality, and exploration value of generated hypotheses."}, "questions": {"value": "1. The paper mentioned the system identified 21 ideas of scientific innovations but evaluated 5 ideas. How and why were the 5 ideas selected?\n\n2. How are the hyperparameters in Eq 1 selected and does the same set of hyperparameters apply to all three tasks?\n\n3. How is the initial database of the existing work construct and does it require human efforts within the process?\n\n4. Do the 3 human experts only evaluate the papers generated by the system? Were they only evaluating the papers generated by the system or evaluating along with human-written papers in a blind manner? How does the evaluation setup avoid bias in this process?\n\n5. Minor: the work conducted the scaling analysis of the system by comparing the number of ideas as the number of GPUs increases and claims that more than parallel trial-and-error, shared knowledge contributes to the trend. If knowledge sharing is the dominant factor, does scaling with time rather than hardware show similar gains? As the experiment might be costly, a discussion or revision of the analysis would be sufficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RXP04Rgeag", "forum": "cZFgsLq8Gs", "replyto": "cZFgsLq8Gs", "signatures": ["ICLR.cc/2026/Conference/Submission911/Reviewer_DLyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission911/Reviewer_DLyc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449365010, "cdate": 1761449365010, "tmdate": 1762915639493, "mdate": 1762915639493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepScientist, an autonomous scientific discovery system capable of operating over month-long time horizons. The authors conceptualize the entire process of scientific discovery as a goal-driven Bayesian Optimization problem, where the objective is to identify discoveries that yield an improvement in a target performance metric. This formulation enables the system to independently generate hypotheses and engage in continuous exploration and iterative trial-and-error reasoning.\n\nThrough extensive experimentation, amounting to 20,000 GPU hours, the system generated over 5,000 distinct scientific hypotheses, of which 1,100 were experimentally validated. Ultimately, DeepScientist achieved performance surpassing human-designed state-of-the-art methods published in 2025 across three frontier AI tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. In contrast to approaches that rely solely on agent-based frameworks and prompt engineering, this paper introduces DeepScientist, which brings a novel Bayesian Optimization perspective to the formulation of the scientific discovery problem.\n2. Unlike existing methods that are limited to generating single-step ideas to achieve state-of-the-art performance, DeepScientist is capable of performing deeper, long-horizon exploration by building upon previously generated hypotheses. This sustained process of iterative refinement continuously advances performance, yielding substantially greater improvements than shallow, one-off idea-generation strategies."}, "weaknesses": {"value": "1. In my opinion, the evaluation metrics for an autonomous scientific discovery system should not be limited to outperforming humans on a small set of manually selected benchmark tasks, as such an approach risks introducing bias and cherry-picking experimental settings. Instead, evaluation should be conducted across a broader, more diverse range of real-world tasks — on the order of hundreds or even thousands — allowing the system to explore autonomously. The ultimate measure should be *how many of these tasks the system can surpass human performance on*. From this perspective, the evaluation methodology adopted in the paper still needs improvement.\n2. The progress of science and technology is inherently continuous: the emergence of a new state-of-the-art naturally leads to further advancements, whether proposed by humans or by autonomous discovery systems. Consequently, replacing older methods with newer, higher-performing ones is almost inevitable. Therefore, evaluating scientific discovery systems solely based on their ability to surpass human-designed state-of-the-art methods is insufficient. A more comprehensive assessment should also include research efficiency, as partially discussed in this paper, encompassing dimensions such as time and cost-effectiveness (for example, where the system utilizes computational resources, such as GPUs, more efficiently than humans).\n3. A robust scientific discovery system should be universally applicable. In this paper, the authors tested the system in only three AI-related domains: Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection. However, problems in computer science are already highly formalized and standardized, meaning that what appears to be “scientific hypothesis generation” may, in practice, resemble an interpretability-enhanced version of NAS (Neural Architecture Search). Hence, beyond the AI domain, it would be worthwhile for the authors to extend their exploration to other scientific fields, such as chemistry or physics. Moreover, the experimental focus should move beyond quantitative, engineering-oriented optimization toward the generation of genuinely novel theoretical hypotheses that would allow the system to realize its full potential for advancing science."}, "questions": {"value": "1. In line 187, the paper mentions feeding the entire Findings Memory as context into the surrogate model. If this surrogate model is implemented using an LLM, what’s the prerequisite for the context window length?\n2. The authors report experiments on only three real-world tasks: Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection. Although they justify this selection based on frontier relevance, community interest, and human supervisability, these reasons alone do not fully explain why only these three were chosen. There are undoubtedly other tasks that meet the same criteria. Have the authors conducted experiments on **any other tasks** that satisfy these considerations? If so, what were the outcomes?\n3. Scientific Discovery, by definition, requires a system to demonstrate genuine creativity. However, the behavioral patterns observed in most existing autonomous scientific discovery systems suggest that their “innovations” often amount to summarizing and extending ideas retrieved from related literature, rather than generating truly original insights. These general-purpose LLM-based systems tend to struggle with formulating original scientific hypotheses, as they primarily operate by synthesizing and reinterpreting existing knowledge. Given this limitation, an important question arises: without post-tuning the core general-purpose LLM that serves as its surrogate model, can DeepScientist genuinely propose revolutionary, innovative ideas, or does it merely recombine existing components —for instance, integrating modules from PaperA and PaperB —under the guise of novelty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P5O575IqYt", "forum": "cZFgsLq8Gs", "replyto": "cZFgsLq8Gs", "signatures": ["ICLR.cc/2026/Conference/Submission911/Reviewer_pBcW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission911/Reviewer_pBcW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532896632, "cdate": 1761532896632, "tmdate": 1762915639370, "mdate": 1762915639370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a closed-loop scientific discovery framework including hypothesis proposation, experimentation, and report writing. \nThe experimental outcomes are reused for next-round hypothesis generation. For experiments, the proposed system is tested on three tasks, and the paper claims to reach state-of-the-art hypothesis outperforming the previous human state-of-the-art."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall idea of a LLM-based framework covering each part of the full discovery loop makes sense.\n2. It is couragous to work on the real research problem and compete with the existing human state-of-the-art. This paper could be a meaningful reference for future works."}, "weaknesses": {"value": "1. To be frank, it is unclear on what new knowledge I can learn form this paper: what new concept or innovation it has brought? The closed-loop scientific discovery framework has been proposed by several works, such as [1][2][3]. The memory-based experimental results storage proposed in this paper, are included in [1][2][3]. It seems that this paper is more of a good (or maybe excellent) engineering implementation of existing methodology, rather than a research work that brings in new insights. The good results could result from the existing methodology with the more advanced LLMs today.\n\n2. The experiments and claims are not persuading enough. It claims to \"outperforms\" three human sota results. But the first experiment, \"Agents Failure Attribution\", the selected baseline is from [4]. However, it seems that [4] is the first paper which bring about this research question of \"Agents Failure Attribution\" and propose the first benchmark. It does not make [4] a good and valid \"state-of-the-art human results\". For the second experiment, described in line 289~302, this paper does not mention the previous state-of-the-art. Overall I think more discussion of the three tasks experimented would be beneficial to this paper, to make it more clear on what are the existing research development of these three fields, so to understand the discovery from the proposed framework. \n\n3. In line 352, it says \"The generated papers are available in Appendix D.\". But I didn't find the genrated papers in Appendix D. In line 870, there's a latex reference error on \"Figure ??.c.\". \n\n\n[1] FunSearch: Making new discoveries in mathematical sciences using Large Language Models. \n\n[2] AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms. \n\n[3] LLM-SR: Scientific Equation Discovery via Programming with Large Language Models. \n\n[4] AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?"}, "questions": {"value": "Does this paper introduce any new concepts, theories, or innovations beyond its potentially excellent engineering implementation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Uy9eevrqvA", "forum": "cZFgsLq8Gs", "replyto": "cZFgsLq8Gs", "signatures": ["ICLR.cc/2026/Conference/Submission911/Reviewer_rLBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission911/Reviewer_rLBb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762358320208, "cdate": 1762358320208, "tmdate": 1762915639259, "mdate": 1762915639259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response to All Reviewers - 1"}, "comment": {"value": "We sincerely thank the four reviewers (**Reviewer DLyc, Reviewer oPiU, Reviewer pBcW, and Reviewer rLBb**) for investing significant time and effort to provide detailed, thoughtful, and constructive feedback. Your comments have helped us better understand the strengths of our work, clarify possible misunderstandings, and identify areas that required further explanation or experimentation. We have revised the paper and incorporated additional experiments accordingly.\n\n------\n\n## **Highlights Identified by the Reviewers**\n\n**• Framework and Overall Goal**\n\n- *Reviewer rLBb* affirmed the soundness of building an LLM-based framework that covers the full cycle of scientific discovery—from hypothesis generation and experimentation to paper writing—and noted that the system may serve as a useful reference for future research.\n- *Reviewer oPiU* emphasized that automated scientific discovery is an important direction with long-term impact, and considered our attempt to “compete with human SoTA on real frontier tasks” both meaningful and well-motivated.\n\n**• Methodological and Conceptual Innovations**\n\n- *Reviewer pBcW* highlighted that our formulation of the scientific discovery process as a **goal-driven Bayesian Optimization problem** offers a new perspective compared to agent/prompt-only approaches.\n- *Reviewer pBcW* also recognized that DeepScientist enables **multi-round, long-horizon, incremental deep exploration**, whereas prior systems typically focus on single-step idea generation.\n\n**• Experimental Scale and Analysis**\n\n- *Reviewer DLyc* and *Reviewer pBcW* acknowledged that our study—spanning three frontier tasks with ~20,000 GPU hours, over 5,000 candidate hypotheses, and 1,100+ experimental validations, complemented by human expert evaluation—constitutes a **reasonably comprehensive and persuasive empirical investigation**.\n- *Reviewer DLyc* commended the clarity and organization of our system visualizations and analyses.\n\n------\n\n## **A Common Misunderstanding and Our Clarification**\n\nWe noticed a recurring concern mentioned by multiple reviewers, expressed in different forms:\n\n> *“Is DeepScientist essentially an engineering-scaled version of existing AI Scientist or closed-loop systems? Does its novelty mainly stem from stronger LLMs or selecting three tasks where surpassing human SoTA is relatively feasible?”*\n\nWe clarify the following:\n\n**1. Nature of the Tasks: From Low-Cost Trial-and-Error to High-Cost Modern Problems**\n Prior work often investigates algorithmic or symbolic domains where trial-and-error incurs very low cost (seconds or minutes).\n In contrast, we intentionally selected **three modern, high-stakes, high-cost frontier tasks**—Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection—each actively studied by the human research community.\n On these tasks, **infinite trial-and-error is practically impossible**, which directly motivated our design of a system centered on **efficient exploration under extremely limited budgets**.\n\n**2. What Truly Matters Is Not Only Surpassing SoTA but Revealing Empirical Regularities in Modern Scientific Discovery**\n Beyond demonstrating multiple SoTA improvements, a key contribution of our work is the first systematic quantification of:\n\n- the *“funnel-shaped”* success rate of autonomous exploration (only ~1–3% of hypotheses lead to actual progress),\n- the role of failure memory and hierarchical Bayesian optimization under high-cost settings,\n- an *approximate linear scaling law* between discovery throughput and parallel GPU resources,\n- and the strengths and biases of LLMs across evaluation, filtering, and self-review stages.\n\nThese empirical observations are difficult to obtain from prior automatic discovery systems focused on simpler tasks (e.g., symbolic regression, equation discovery, or basic algorithm synthesis). We believe such findings constitute the core novelty of our work.\n\n------"}}, "id": "pJveWiuMX7", "forum": "cZFgsLq8Gs", "replyto": "cZFgsLq8Gs", "signatures": ["ICLR.cc/2026/Conference/Submission911/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission911/Authors"], "number": 21, "invitations": ["ICLR.cc/2026/Conference/Submission911/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763414464915, "cdate": 1763414464915, "tmdate": 1763414464915, "mdate": 1763414464915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}