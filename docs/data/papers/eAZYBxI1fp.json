{"id": "eAZYBxI1fp", "number": 24262, "cdate": 1758354699036, "mdate": 1759896773867, "content": {"title": "Risk-Aware Robust Graph Network Explanation", "abstract": "Post-hoc explanation methods for Graph Neural Networks (GNNs) are increasingly used to reveal which substructures influence a model’s prediction. However, recent studies show that such explanations are often brittle—small changes to the input graph can lead to drastically different explanations. This instability challenges their reliability in critical downstream tasks such as auditing, debugging, or human-in-the-loop decision making.\n\nIn this work, we introduce GrA, a risk-aware explanation trimming method that enhances the robustness of GNN explanations via a post-hoc, model-agnostic process. GrA identifies unstable edges using gradient-based sensitivity analysis and quantifies their volatility via Conditional Value-at-Risk (CVaR), a tail-aware risk measure. By removing high-risk edges, GrA produces a robust surrogate graph that retains explanatory fidelity while significantly reducing sensitivity to structural perturbations.\n\nGrA requires no modification to the underlying GNN or explanation model and can be seamlessly applied to any gradient-accessible explainer. Across both synthetic and real-world graph classification benchmarks, and under various adversarial perturbation settings, GrA consistently improves explanation stability without compromising fidelity or predictive accuracy.", "tldr": "", "keywords": ["Graph-based Machine Learning", "Explainable AI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b8a422a918370ef32de6b412c6e51513983c515.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes GrA (Graph Risk-aware Explanation Trimming), a post-hoc, model-agnostic approach for improving the robustness of Graph Neural Network (GNN) explanations. It defines edge-level “risk” via gradient sensitivity and trims unstable edges based on Conditional Value-at-Risk (CVaR). Experiments across several datasets show increased stability (IoU, consistency) without degrading fidelity. However, the theoretical depth and experimental completeness are limited. The idea mainly reuses gradient sensitivity combined with a percentile-based trimming heuristic. The work feels incremental, and the experimental evaluation is selective and inconsistent across datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an important issue: the instability of GNN explanations under small perturbations.\n2. The method is simple, model-agnostic, and easy to integrate with existing explainers.\n3. Experimental results show consistent numerical improvements in stability metrics (IoU, consistency)."}, "weaknesses": {"value": "1. Limited conceptual novelty.\nThe proposed method is essentially a gradient-norm-based edge pruning procedure. Using CVaR to select top-risk edges is equivalent to a percentile thresholding rule. This is a modest engineering contribution rather than a conceptual innovation.\n\n2. Lack of deep empirical insight.\nThe experiments only report improvements in IoU and consistency but do not analyze why or when the method works. There is no discussion on semantic fidelity—whether the retained subgraphs still correspond to meaningful rationales.\n\n3. Insufficient comparisons.\nThe method is compared only to vanilla explainers and random trimming. There are no comparisons against recent robust explainers such as V-INFOR (NeurIPS 2023) or others, which limits the empirical credibility.\n\n4. Incomplete and inconsistent experimental setup.\nAlthough the paper claims GrA is compatible with any differentiable explainer, only GNNExplainer and PGExplainer are tested, omitting more recent baselines such as SubgraphX, GraphSHAP, or PGMExplainer. Even worse, the choice of explainers varies by dataset: BA-House and Tree-Cycle use GNNExplainer, while BA-Community, MUTAG, and OGBN-Products only use PGExplainer.\n\nThis inconsistency raises serious questions about experimental fairness and completeness. A convincing evaluation should apply both explainers across all datasets under the same perturbation settings. The current results therefore do not convincingly demonstrate generality or robustness.\n\n5. Presentation issues.\ni) Figure 2 has typographical errors (“befor”) and low visual clarity.\nii) The discussion of hyperparameter robustness (α, δ, λₘ) is missing."}, "questions": {"value": "1. Does trimming ever remove semantically important edges? How is this mitigated?\n2. Why were some datasets evaluated only with PGExplainer but not with GNNExplainer?\n3. How does GrA compare to more modern or certified robust explainers (e.g., V-INFOR)?\n4. What prevents the method from being applied to non-differentiable explainers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2JrBCYWIaQ", "forum": "eAZYBxI1fp", "replyto": "eAZYBxI1fp", "signatures": ["ICLR.cc/2026/Conference/Submission24262/Reviewer_gCEu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24262/Reviewer_gCEu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890545025, "cdate": 1761890545025, "tmdate": 1762943022816, "mdate": 1762943022816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the instability of post-hoc explanations for Graph Neural Networks (GNNs), where small graph perturbations can drastically change the identified explanatory substructures, undermining reliability in auditing, debugging, and human-in-the-loop settings. To address this challenge, this paper proposes a risk-aware explanation trimming method, called GrA, which identifies unstable edges via gradient-based sensitivity analysis and quantifies their volatility using Conditional Value-at-Risk (CVaR), a tail-aware risk measure. Moreover, by removing high-risk edges, GrA constructs a robust surrogate graph that preserves explanatory fidelity while substantially reducing sensitivity to structural perturbations. Across synthetic and real-world graph classification benchmarks, and under various adversarial perturbation settings, GrA consistently enhances explanation stability without sacrificing fidelity or predictive accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated, directly addressing the widely observed instability of post-hoc GNN explanations in high-stakes applications.\n\n2. The experimental evaluation is fairly comprehensive, including baseline comparisons and some ablations that help illuminate the design choices."}, "weaknesses": {"value": "1. The paper does not demonstrate clear empirical advantages over existing methods in terms of fidelity or computational overhead, and the ablation studies reveal no consistent benefit in runtime performance.\n\n2. The writing quality and organization need improvement, with readability affected by the frequent use of dashes and inconsistent formatting, which makes following the technical details more difficult.\n\n3. The proposed model is relatively simple and lacks architectural novelty, raising questions about the depth of technical contribution."}, "questions": {"value": "1. The method is evaluated only on GNNExplainer and PGExplainer. Why not include other widely used explainers such as GNN-LRP and SubgraphX, which are also known to be brittle under small perturbations?\n\n2. Could the authors explain why the proposed method fails to improve performances in terms of  fidelity and overhead?  \n\n3. The proposed method is layered on top of existing explainers to improve performance, which inevitably adds computational complexity. On large graphs/datasets, does this overhead scale to the point of offsetting the gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uK3zRKzLDm", "forum": "eAZYBxI1fp", "replyto": "eAZYBxI1fp", "signatures": ["ICLR.cc/2026/Conference/Submission24262/Reviewer_SVTr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24262/Reviewer_SVTr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932069282, "cdate": 1761932069282, "tmdate": 1762943022453, "mdate": 1762943022453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method to enhance the robustness of post-hoc GNN explanations. Different from existing methods, it is training-free and model-agnostic (which can be applied to different GNNs and GNN explainers). To measure the instability of explanations (in the form of edge importance score after perturbations), they define \"edge risk\" as gradient-based sensitivity. They found that the edge risks follow a long-tail distribution (namely most edges have low sensitivity but the rest exhibit large values). To better select \"fragile\" edges to remove from the explanations, they propose to use CVaR, which can consider both freequency and magnitude in the distribution tail. Besides, to prevent from removing informative edges by overly aggressive selection, they also propose to combine with importance score. Finally, they evaluate their method with extensive experiments. However, the presentation of this paper can be largely improved. It looks very technical, not friendly to general readers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. very interesting idea: this paper define \"edge risk\" as gradient-based sensitivity to measure the instability of explanations. And then they employs CVaR to evaluate the this risk. The ideas of \"edge risk\" and \"CVaR\" are from the finance/risk management domain, and using it here to quantify the instability of explanations looks very interesting.\n2. novel direction: GNN explanations have been investigated very widely; In contrast, the robustness/attacks on GNN explanations become receiving research attentions since 2024 (see the work Jiate Li et al [Graph Neural Network Explanations are Fragile]; and Zhong Li et al. [Explainable Graph Neural Networks Under Fire]); These two work however only discussed on how to attack them without proposing a systematic/well-developed way to defend these attacks. This paper makes the first (formal) step in this direction.\n3. Technically sound: many claims in this paper are derived with math formulations or supported by empirical experiments."}, "weaknesses": {"value": "1. The paper is very technical, not easy to follow: for example, section 3 (the method section) are full of math formulations with very short/no explanations of the meaning and/or motivations of the design chocies, making it hard to follow. To get more spaces for explanations, things such as pseudo-code, implementation details can be moved  to appendix. The authors try to cover everything in the main text, at the cost of making many things not clearly described. Overall, the presentation can be largely improved. \n\n2. This method can only be used to differentiabe post-hoc explanations: although the authors have recognised this part, I still would like to see how these can be solved? Is it possible to transfer the defense capability from one GNN explainer to another explainer? (For exmaple, using PGExplainer as the target, test the effectiveness of defense on other explainers, which can be non-differentiabe)\n\n3. They only consider homogenous graphs in experiments; (It would be interesting to see whether these methods are effective on heterogeneous graphs, with associated heterogeneous GNNs and post-hoc explainers?)"}, "questions": {"value": "1. Page 7, why do you only evaluate on \"correctly classified instances\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dU60gaG66t", "forum": "eAZYBxI1fp", "replyto": "eAZYBxI1fp", "signatures": ["ICLR.cc/2026/Conference/Submission24262/Reviewer_iDte"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24262/Reviewer_iDte"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993288680, "cdate": 1761993288680, "tmdate": 1762943021998, "mdate": 1762943021998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new explanation method for graph neural networks which enhances the robustness of explanations, by removing unstable edges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- relevant problem\n- interesting and original idea\n- well written paper\n- good discussion and analysis\n- good results"}, "weaknesses": {"value": "- discussion of limitations could be improved"}, "questions": {"value": "Can you please characterize when the method works well and when it does not suceed in enhancing robustness substantially?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gx89FdXUGj", "forum": "eAZYBxI1fp", "replyto": "eAZYBxI1fp", "signatures": ["ICLR.cc/2026/Conference/Submission24262/Reviewer_LZBa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24262/Reviewer_LZBa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073381414, "cdate": 1762073381414, "tmdate": 1762943021674, "mdate": 1762943021674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}