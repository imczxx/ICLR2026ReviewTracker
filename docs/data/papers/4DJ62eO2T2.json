{"id": "4DJ62eO2T2", "number": 9284, "cdate": 1758117401679, "mdate": 1763469897489, "content": {"title": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning", "abstract": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. This shift in perspective serves as a conceptual bridge, revitalizing foundational principles from classical learning theory to analyze the unique dynamics of LLMs. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents. We open-source our code through an anonymous GitHub repository at https://anonymous.4open.science/r/CoT-Space-Reasoning-via-RL.", "tldr": "This paper introduces CoT-Space, a novel theoretical framework that recasts LLM reasoning as a continuous optimization problem , which provides a coherent explanation for empirical phenomena such as overthinking.", "keywords": ["large language model", "reasoning", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85661e135228b8bf8e2be2af9c8d6bba7aab1cbb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CoT-Space, a theoretical framework for analyzing the reasoning Chain-of-Thought (CoT) of LLMs trained with RL. The authors argue that a \"fundamental misalignment\" exists between traditional token-level RL frameworks and the reasoning-level, multi-step nature of CoT generation, in that:\n* treating the token as the action fails to capture the higher-order planning inherent to CoT, and\n*  analyzing this discrete, exponentially large space with the continuous math of classical learning theory (like gradients and loss landscapes) is difficult.\n\nThe authors then present their own theory and some RL experiments that are used to support their theory, which is that\n* Shorter CoTs risk underfitting by failing to reach the problem's required reasoning depth, and\n* Longer CoTs risk overfitting by increasing the generalization error."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The experiment observations can be used to support the proposed theory."}, "weaknesses": {"value": "1. The paper's entire motivation rests on a fundamental misalignment between traditional MDP RL and LLM CoT with RL and I don't think it is convincing. The authors use an analogy of Bricks Breaker , but this comparison is weak. The difference between an LLM's action space (a large vocabulary) and Bricks Breaker's action space (\"left,\" \"right\") is one of scale and dimensionality (degrees of freedom), not a fundamental categorical difference. I can also argue that some RL environments have continous action spaces, which, in the authors' logic is also not aligned with Bricks Breaker.\n\n2. The paper insists on a sharp divide between \"token-level\" actions and \"reasoning-level\" actions (i.e., \"complete thoughts\") . This distinction is arbitrary and can be applied to almost any RL problem. A Bricks Breaker agent's policy could also be broken down hierarchically (e.g., a \"plan\" to \"hit the ball behind the bricks\" , which is then executed via low-level motor commands). The paper fails to argue why CoT is fundamentally different from existing, well-studied hierarchical RL problems.\n\n3. Combining 1 and 2, the paper claims a \"conceptual gap\" exists between discrete language and the \"continuous mathematics underpinning classical learning theory\". This ignores the vast body of RL literature that already deals with continuous action spaces. The paper attempts to re-invent the wheel and fails to situate its contribution within a standard area of RL research."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V659s0knXK", "forum": "4DJ62eO2T2", "replyto": "4DJ62eO2T2", "signatures": ["ICLR.cc/2026/Conference/Submission9284/Reviewer_so6J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9284/Reviewer_so6J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629362340, "cdate": 1761629362340, "tmdate": 1762920927875, "mdate": 1762920927875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoT-Space, a reasoning-level framework for RL-trained LLMs that reframes CoT generation as optimization over a continuous semantic manifold. Section 2 defines reasoning states, posits exponential “expressive redundancy” (Assump. 2.9), and proves density-driven continuum results. Section 3 analyzes “overthinking” via noise and generalization-risk, predicting an optimal CoT length Lopt. Section 4 empirically explores correlations between Lopt and task difficulty, model capacity, RL algorithm, and KL loss."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "As of now, I tried but cannot sense any strengths, though the intention to provide a scaffold to achieve reasoning-level formalism is clear"}, "weaknesses": {"value": "- authors use $L$ for both token-length and step-depth. In Sec. 3.1 $L$ denotes CoT length; in Sec. 3.2 $L$ denotes the number of reasoning steps; Fig. 5 treats $L_{opt}$ as length, but the related text for fig 5 links the trend to Theorem 3.3 in terms of reasoning depth $L^*$. This notational conflation makes Theorem 3.1, the empirical proxies in Fig. 5, and the bounds in 3.2 difficult to interpret consistently. \n\n- Reasoning-level is treated as a “simple extension” from tokens to steps, but the paper assumes the output is segmented into steps $\\xi_1$,…,$\\xi_L$ (Defs. 2.2–2.6; Secs. 2.2–2.3) without specifying a segmentation rule or identifiability guarantees. This is a core barrier in practice: semantic step decomposition is non-unique and task/model dependent. Section 2 builds on this premise, and Section 3’s analysis and bounds depend on it, but no operational definition, annotator protocol, or robustness analysis is provided.\n\n- Key constructs are under-specified: Decode(·) and semantic equivalence “≡” in Def. 2.7, the “semantic volume” Vsemantic and its metric in Def. 2.8, and the dimensionality D and manifold assumptions in Theorem. 2.10–2.12. Without a metric and regularity conditions (e.g., smoothness/Lipschitz continuity of C, curvature bounds), the continuum arguments are under-anchored.\n\n- Noise–length relation (Theorem 3.1) lacks a precise definition of g in the main text. If g is an effective optimisation-noise scale analogous to SGD noise, how is it operationalized for CoT generation? Further, where is the relation used in the paper? any purpose and why? if KL divergence is the proxy, justify theoretically why KL tracks the relevant noise scale.\n\n- Theorem 3.2 adapts supervised-learning results to RL-style reasoning but omits RL-specific caveats: policy-induced distribution shift, dependence between samples due to on-policy rollouts, and reward-driven selection effects; we may need to clarify the learning setting (on/off-policy, i.i.d. queries vs trajectories)\n\n- Lower bound (Theorem 3.3) formalizes underfitting via $P_{\\pi}$ (L < L*), but L* (minimum required depth) is not observable. How is  L* estimated, and how sensitive are conclusions to estimation error?\n\n- Fig. 5’s empirical “validations” largely echo patterns already reported elsewhere (task difficulty ↔ longer CoT; stronger models ↔ shorter CoT). What is new here relative to prior empirical studies on CoT length vs difficulty/capacity?\n\n\n- Several broad claims need tighter support in main text. For example: “token-level state space is problematic” (Sec. 2.1), and “zooming out allows abstract, stepwise analysis” (Sec. 2.2 figure narrative) are asserted without formal counterexamples or empirical diagnostics; “density arises from the expressive redundancy of language” (Sec. 2.3) is not plausible\n- Redundancy: Sec. 2.1 vs Sec. 2.2 and Fig. 2 repeat the token vs reasoning-level distinction with similar prose. This space could instead be used for a concrete worked example grounding Def. 2.2–2.6.\n- The link of this work to “internal slow-thinking via RL” is underdeveloped in the main. Beyond the analogy (Fig. 4a–c), the paper does not define slow thinking rigorously nor show how a specific RL objective operationalizes it. \n\n- if the novelty is the proposed theoretical justification; however, due to the issues above (notation, assumptions, theorem gaps), the empirical section reads as confirmation of known trends rather than a stress-test of your theory."}, "questions": {"value": "- What does “classical theoretical results” refer to precisely in line 305 and Sec. 2.3 end? Which specific theorems (e.g.,   information-theoretic bounds with mutual information) are you invoking, and under which assumptions do they carry over?\n- What is a “suitable CoT” in Fig. 1(b)?  \n- You aim to understand internal slow-thinking via RL, but where do you define slow thinking rigorously?  \n- What precisely is “noise” in Theorem 3.1 and Fig. 5(d)? If KL loss is the proxy, why is it a valid stand-in for optimization noise in reasoning-space?\n- How do you estimate L* in practice for Theorem 3.3, and how robust are results to misestimation?\n- For Def. 2.7 (semantic equivalence set), how do you operationalize Decode(·) and “≡” beyond intuition? Is it exact logical equivalence, or model-judged equivalence with tolerance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z863T0uTwl", "forum": "4DJ62eO2T2", "replyto": "4DJ62eO2T2", "signatures": ["ICLR.cc/2026/Conference/Submission9284/Reviewer_thdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9284/Reviewer_thdN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990747612, "cdate": 1761990747612, "tmdate": 1762920927624, "mdate": 1762920927624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a theoretical framework, CoT-Space, that optimizes chain-of-thought (CoT) reasoning in a continuous semantic space rather than at the discrete token level. It formalizes reasoning states and minima, proves a continuum approximation under an expressive redundancy assumption, and analyzes overthinking from both optimization-noise and information-theoretic perspectives. Experiments on math tasks indicate that the optimal CoT length Lopt increases with task difficulty and decreases with model size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and conceptual framing that highlight the misalignment between token-level RL and reasoning-level CoT.\n\n- Detailed definitions and derivations (with supporting appendices), which improve readability and rigor."}, "weaknesses": {"value": "- Assumption 2.9 (exponential semantic redundancy) is central to the continuum result but is not empirically validated, especially given the structured nature of LLM-induced semantic spaces.\n\n- Definition 2.6 provides only an abstract ordering of the loss consistent with reachable distances and does not offer a concrete instantiation.\n\n- Empirical details are insufficiently presented."}, "questions": {"value": "- In the experiments, what concretely proxies C(⋅) (e.g., task-level reward, verifier loss)? How sensitive are your conclusions to this choice? How to choose a proper C in general?\n\n- Beyond theory, can you approximate “state density” by sampling semantically equivalent steps and measuring nearest-neighbor distances in a task-conditioned embedding, to empirically test the continuum assumptions and derived conclusions?\n\n- It seems that the takeaways from the framework deduction are limited (to a suitable CoT length regarding task difficulty), can this framework extend to support a direct CoT optimization in a latent space, e.g., assisting in finding this continuous space and construction loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JZocNbHmsS", "forum": "4DJ62eO2T2", "replyto": "4DJ62eO2T2", "signatures": ["ICLR.cc/2026/Conference/Submission9284/Reviewer_ogAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9284/Reviewer_ogAP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991242696, "cdate": 1761991242696, "tmdate": 1762920927271, "mdate": 1762920927271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoT‑Space, a reasoning‑level view of RL for LLMs that replaces token‑level MDPs with states, reachable minima, and a reasoning loss, thereby recasting multi‑step reasoning as optimization in a semantic space rather than discrete token control. It further argues this space becomes approximately continuous as the token budget grows and bounds the discrete–continuous gap, which licenses gradient/SDE analyses. Building on this, the paper gives two lenses for the empirically observed optimal CoT length: a noise view and a risk view. Experiments on GSM8K/MATH and multiple model families/algorithms support four predictions: increases with task difficulty, decreases with model capacity, is largely algorithm‑agnostic, and inversely correlates with a KL‑loss proxy for noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers a clear conceptual bridge from token‑level RL to reasoning‑level optimization with concrete definitions and a continuum justification, enabling learning‑theory tools to address CoT phenomena. The two complementary analyses provide a unified explanation for both under‑thinking and over‑thinking and yield testable predictions that are validated across tasks, model scales, and RL methods."}, "weaknesses": {"value": "1. Idealized assumptions. The existence of a semantics-aware loss and reachable minima for queries with a single \"golden\" answer narrows scope and is non-constructive; the continuum argument relies on unmeasured semantic density/dimension, making it hard to falsify.\n\n2. Simplified noise model. The SDE derivation treats noise as white and state-independent to obtain $g \\propto 1 / L$; mapping KL-loss to \"noise\" is only a proxy and may conflate effects.\n\n3. Loose generalization bounds. PAC-Bayes/information-theoretic bounds are qualitative (constants and mutual-information terms not estimated) and assume i.i.d. data, whereas training is on-policy."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mKv2HYdA4n", "forum": "4DJ62eO2T2", "replyto": "4DJ62eO2T2", "signatures": ["ICLR.cc/2026/Conference/Submission9284/Reviewer_pW19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9284/Reviewer_pW19"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047127295, "cdate": 1762047127295, "tmdate": 1762920926760, "mdate": 1762920926760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}