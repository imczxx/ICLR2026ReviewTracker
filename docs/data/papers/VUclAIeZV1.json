{"id": "VUclAIeZV1", "number": 6882, "cdate": 1757999630827, "mdate": 1763543885075, "content": {"title": "Don't Lose Sight: Visually-Grounded Credit Assignment for Multimodal Reasoning", "abstract": "Reinforcement Learning (RL) has shown promise for large language models, but its direct application to multimodal LLMs (MLLMs) faces unique challenges. Unlike text-only LLMs, MLLMs must jointly optimize for visual grounding and language reasoning. \nOur analysis reveals that RL primarily enhances textual reasoning, while the crucial visual\ngrounding aspect stalls, creating a bottleneck for overall model performance.\nThis observation highlights a critical mismatch: the learning challenge in MLLMs is concentrated in visually-grounded tokens, yet existing RL algorithms apply uniform optimization pressure across all tokens, thereby diluting the learning effort.\nMotivated by this limitation, we propose Visually-grounded Credit Assignment (VICRA), a simple yet effective approach that reallocates optimization pressure toward visually-grounded tokens, explicitly correcting the token-level imbalance overlooked by prior methods. \nExtensive experiments across benchmarks, base models, and training data show that VICRA consistently enhances multimodal reasoning, achieving significant gains over strong RL baselines. Our work establishes a general framework for more balanced and effective reinforcement learning in MLLMs.", "tldr": "", "keywords": ["LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2559154118a54de0b79330417ddc0ebb28e52b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper makes a point: directly applying RL to multimodal models leads to an uneven learning process. Optimization pressure gets spread out uniformly, which ends up improving textual reasoning but leaves visual grounding behind. The authors show that visually-dependent tokens get trapped in a high entropy state, bottlenecking the model's performance.To tackle this, they propose VICRA, a method for refocusing the optimization. It works by assigning a “Visually-grounded Score“ to each token, calculated from the probability difference with and without the visual input, and then uses that score to concentrate the learning on the visually-grounded parts of the output. This approach is designed to be a simple drop-in for RL algorithms like GRPO and DAPO, and its effectiveness is well-supported by results on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work astutely points out an optimization imbalance in MLLMs trained with RL. Specifically, these models have a tendency to bolster textual reasoning at the expense of visual grounding. To resolve this, the authors introduce VICRA, a simple yet highly generalizable method that strategically reallocates optimization pressure. For this approach, the authors report performance gains across diverse models, algorithms, and benchmarks."}, "weaknesses": {"value": "The paper don't discuss the resource costs of VICRA's “Visually-grounded Score“” calculation. At the same time, it lacks a convincing explanation for why significant gains on reasoning benchmarks do not translate to meaningful improvements on perception tasks. Given the small margins, the authors should report evaluation variance and statistical significance to better support their conclusions."}, "questions": {"value": "1. The model shows clear gains on reasoning-oriented benchmarks but only marginal gains on perception tasks, which is somewhat counterintuitive. Beyond the fact that these benchmarks have high baseline scores, why substantial improvements in visual reasoning do not translate into improved perceptual ability?\n2. Building on the first point, the paper would be stronger if it reported evaluation variance and statistical significance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uVkK3GGiS3", "forum": "VUclAIeZV1", "replyto": "VUclAIeZV1", "signatures": ["ICLR.cc/2026/Conference/Submission6882/Reviewer_7ZPd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6882/Reviewer_7ZPd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562058100, "cdate": 1761562058100, "tmdate": 1762919129498, "mdate": 1762919129498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses optimization challenges of reinforcement learning (RL) in multi-modal large language models (MMLLMs).\n\nIt identifies the phenomenon that visually-grounded tokens maintain high entropy during the training process, while text-related tokens follow the exploration-exploitation trajectory where entropy rises and then falls as the training converges.\n\nTo address the problem, the paper simply enforces a larger weight on the reward for visually-grounded tokens during training.\nSpecifically, the weights are proportional to the visually-grounded credits that are the score differences between with and without images as inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The observation is interesting: visually-grounded tokens maintain high entropy during the training process, while text-related tokens follow the exploration-exploitation trajectory where entropy rises and then falls as the training converges.\n\n(2) The paper writes clearly and is easy to follow."}, "weaknesses": {"value": "(1) The paper identifies the phenomenon that visually-grounded tokens keep high entropy during training while text-related tokens follow the exploration-exploitation trajectory, which means that visually-grounded tokens are more difficult to optimize. However, there is no deep analysis of the root causes.               \n\nDoes it come from the misalignment between language and vision?\n\nThere should be more analysis.\n\n(2) It seems that the motivation is not consistent with the empirical analysis. As shown in Figure 4, KL_prop achieves a much smaller entropy while inferior performance than the proposed VICRA, which implies that lower entropy on visiually-grounded tokens does not mean higher performance. \n\nMoreover, the paper argues that KL_prop can suffer from entropy collapse. What does the 'entropy collapse' mean?\n\nWhy doesn't VICRA suffer from it? \n\n(3) The proposed VICRA training objective attaches more importance to visually-grounded tokens and improves model performance on reasoning tasks.\n    It is interesting to show how the objective affects model performance on benchmarks, like VQA, SQA, GQA, and POPE."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rZHPVSkZ0U", "forum": "VUclAIeZV1", "replyto": "VUclAIeZV1", "signatures": ["ICLR.cc/2026/Conference/Submission6882/Reviewer_TsSS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6882/Reviewer_TsSS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710172746, "cdate": 1761710172746, "tmdate": 1762919128927, "mdate": 1762919128927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key challenge in applying reinforcement learning (RL) to multimodal large language models (MLLMs): a performance bottleneck caused by an imbalance in optimization pressure between visually-grounded tokens and other tokens. The authors' analysis suggests that existing RL algorithms enhance textual reasoning but fail to sufficiently improve visual grounding, as evidenced by the consistently high entropy of visually-grounded tokens during training. To address this, the paper proposes Visually-grounded Credit Assignment (VICRA), a method that reallocates optimization pressure by amplifying the advantage function for tokens identified as visually-grounded. The experiments, conducted across several multimodal reasoning benchmarks, demonstrate that VICRA improves the performance of strong RL baselines like GRPO and DAPO on various base models and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper articulates a previously underexplored problem in the RL-based fine-tuning of MLLMs. The analysis identifying the optimization imbalance between visually-grounded and text-related tokens, supported by entropy-tracking experiments, provides a motivation for the proposed solution.\n2. The proposed VICRA method is straightforward to implement and integrate into existing RL frameworks that use an advantage function (e.g., GRPO). It directly targets the identified problem by re-weighting the advantage based on a \"visually-grounded score.\" The method shows consistent performance improvements across multiple benchmarks, base models (Qwen2.5-VL-7B, Qwen2.5-VL-3B, Llama-3.2-11B-Vision-Instruct), and training datasets (ViRL39k, MMK12).\n3. The authors benchmark their method against a range of closed-source, open-source generalist, and open-source reasoning-specialized MLLMs. The consistent gains over strong baselines like GRPO and DAPO (e.g., an average improvement of +2.25 for GRPO w/ VICRA and +2.22 for DAPO w/ VICRA in Table 1) validate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. The core motivation and the formulation of the visually-grounded score (Equation 4), which contrasts token probabilities with and without visual input, bear a strong resemblance to contrastive decoding methods[3] used to mitigate hallucinations in MLLMs. The paper would be significantly strengthened by acknowledging this connection, discussing the relationship between VICRA's training-time credit assignment and these decoding-time strategies, and citing relevant work. Furthermore, demonstrating VICRA's effectiveness on established hallucination benchmarks, such as POPE[1] and R-Bench[2], would provide a more direct and comprehensive evaluation of its ability to improve visual faithfulness.\n2. The experimental validation is heavily focused on mathematical and logical visual reasoning benchmarks. While VICRA shows improvements in these areas, its effectiveness on more general multimodal tasks appears limited. The results on perception benchmarks in Table 3 show only marginal gains over the GRPO baseline. To establish VICRA as a truly general and robust framework, it is crucial to include experiments across a wider array of common vision-language tasks and demonstrate that the method's benefits are not confined to the niche of complex reasoning.\n\n[1] Evaluating Object Hallucination in Large Vision-Language Models\n[2] Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models\n[3] Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j7NE0Timl8", "forum": "VUclAIeZV1", "replyto": "VUclAIeZV1", "signatures": ["ICLR.cc/2026/Conference/Submission6882/Reviewer_pdYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6882/Reviewer_pdYu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729929553, "cdate": 1761729929553, "tmdate": 1762919128466, "mdate": 1762919128466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}