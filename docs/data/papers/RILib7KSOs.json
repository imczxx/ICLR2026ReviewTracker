{"id": "RILib7KSOs", "number": 15736, "cdate": 1758254646540, "mdate": 1759897285502, "content": {"title": "Uncritical tokens are 'critical' in pretraining: the implicit regularization effect of Next token prediction", "abstract": "Next Token Prediction (NTP) is the prevailing pre-training approach for large language models, which have demonstrated remarkable reasoning capabilities. A key characteristic of NTP is its objective to predict every token in a sequence, including tokens that are not directly relevant to the final answer or core logic—often considered training noise. While such \"noise\" from uncritical tokens is traditionally thought to impair learning by introducing irrelevant information, our research reveals a counterintuitive positive effect. To isolate this phenomenon, we contrast NTP with Critical Token Prediction (CTP), a training paradigm that focuses exclusively on specific tokens such as the final answer.\nOur findings show that NTP consistently surpasses CTP in reasoning ability. We hypothesize and substantiate through theoretical analysis that the learning objective on uncritical tokens acts as an implicit regularizer, analogous to explicit $L^2$ regularization. Further empirical analysis across various benchmark reasoning datasets confirms that NTP-trained models exhibit enhanced generalization and robustness, demonstrating greater resilience to perturbations and achieving flatter loss minima. These findings reveal that uncritical tokens are, in fact, 'critical' for developing robust reasoning during pre-training, offering valuable insights into optimizing training strategies for LLM development.", "tldr": "", "keywords": ["Next token prediction", "Reasoning capability of transformers", "Large language model", "Implicit regularization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fabb8a4c4346423748e187fffce1922951a2aed8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper challenges the traditional view that \"uncritical tokens\" in NTP are training noise, revealing their critical role as implicit regularizers for reasoning tasks. By contrasting NTP with CTP which only optimizes for key tokens like answers, the authors theoretically prove that NTP’s uncritical token loss acts as an L²-like implicit regularizer via Fisher information matrix analysis, and empirically validate that NTP-trained models exhibit superior reasoning ability, generalization, and robustness across synthetic and real-world reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Important insights**: The paper's central finding—that predicting seemingly irrelevant tokens is beneficial for learning robust reasoning—is counterintuitive and significant.\n\n* **Theoretical and Empirical Synergy**: The work is distinguished by its tight integration of theoretical analysis and empirical validation.\n\n* **Clear Mechanistic Explanations**: Beyond accuracy metrics, the paper uses information flow analysis (Fig. 3b,c) to provide a clear, interpretable visualization of how NTP and CTP lead to different internal reasoning strategies."}, "weaknesses": {"value": "* **Theoretical scope**: The theoretical claims rely on strong assumptions such as vanishing initialization and uniform Fisher information. While these conditions are definitely useful for creating a tractable analytical model, their applicability to real-world, large-scale training scenarios is not fully established.\n\n* **Scalability**: The experiments on GPT-2 inevitably raise concerns about the robustness and generalizability of the conclusions in the current research context, as works such as RHO-1 have demonstrated effectiveness on models ranging from 1B to 7B parameters at a minimum.\n\n* **Clarity**: The paper uses the term \"pre-training\" in title and main corpus to describe training models from scratch on task-specific datasets. This could be slightly confusing, as \"pre-training\" typically refers to unsupervised learning on massive, general-domain corpora. Sharpening this terminology to distinguish between \"from-scratch task training\" and \"fine-tuning\" would improve clarity.\n\n* **Reasoning gaps**: I understand that the paper’s task setup is carefully designed and helpful for clarification, but I still worry whether there is a genuine need—and whether significant experimental differences would emerge—when applying the NTP and CTP training paradigms to truly challenging mathematical problems, such as real questions from AIME or OlympiadBench."}, "questions": {"value": "* In fig. 3(a), your theoretical analysis compellingly links NTP to L2 regularization under specific initialization conditions. How does this implicit regularization from NTP interact with explicit regularizers like weight decay or dropout?\n\n* Appendix A.3 notes that CTP is more efficient when fine-tuning an existing pre-trained model. Could you expand on the practical implications of this? Does it suggest a hybrid strategy where NTP is ideal for initial pre-training, while CTP (i.e., standard SFT) is optimal for downstream adaptation?\n\n* The robustness experiments demonstrate NTP's resilience to input noise and label errors. Do you believe this is solely due to the flatter minima it finds, or are there other mechanisms at play, such as learning a more distributed or compositional representation of knowledge that is inherently more robust?\n\n* Do NTP’s advantages persist in larger models (1B+)? If not, what is the threshold where implicit regularization becomes redundant?\n\n* Can you quantify the implicit regularization strength of NTP (e.g., equivalent L² value) for different tasks/datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RbRTDoeZtk", "forum": "RILib7KSOs", "replyto": "RILib7KSOs", "signatures": ["ICLR.cc/2026/Conference/Submission15736/Reviewer_aZhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15736/Reviewer_aZhb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760703972880, "cdate": 1760703972880, "tmdate": 1762925976073, "mdate": 1762925976073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the difference between Next Token Prediction (NTP) and Critical Token Prediction (CTP) during pretraining of large language models. The authors argue that NTP’s inclusion of “uncritical” tokens implicitly regularizes the model, similar to L2 weight decay, improving generalization and robustness. They support this claim via synthetic “Anchor Function” experiments, theoretical derivations, and several reasoning benchmarks (e.g., PrOntoQA, CLUTRR, RuleTaker), training models from scratch."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Includes both theoretical reasoning and empirical comparison with CTP.\n\n- Attempts to unify several lines of thought around noise-induced regularization."}, "weaknesses": {"value": "- The theoretical analysis, while intuitively appealing, is not fully rigorous. The core derivation that  $\\[L_{\\text{NTP}} = \\frac{1}{T}L_{\\text{CTP}} + \\frac{1}{2}\\theta^\\top I_0 \\theta + O(\\|\\theta\\|^3)\\] $ relies on overly strong assumptions—such as *uniform Fisher information* and *small-weight initialization*—that are unlikely to hold in deep transformer architectures. The argument only establishes a **local first-order approximation** near initialization, so the claimed equivalence between NTP and L2 regularization does not necessarily persist during full training. Moreover, since $\\(I_0\\)$ is not isotropic, the resulting penalty is **not true L2 regularization** but a direction-dependent quadratic form. Hence, while the result provides a useful intuition, it should be interpreted as an *analogy* rather than a formal equivalence.\n\n\n- The paper's main claim—that \"NTP is mathematically equivalent to CTP plus weight decay\"—is expressed too strongly relative to the presented evidence. The experiments are carefully executed but limited in scale and diversity: all models are small (GPT-2 125M) and trained on synthetic or narrow reasoning datasets. As such, the empirical results do not fully substantiate the universality of the claim. The observed effects may instead be dataset- or scale-specific, and the paper would benefit from more systematic ablations (e.g., across model sizes, initialization scales, or data noise levels) to convincingly support the general conclusion.\n\n\n- The connection between token-level noise and implicit regularization echoes prior work on SGD noise and dropout; thus, the conceptual novelty may be limited without stronger theoretical or empirical differentiation."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OaOn37hseS", "forum": "RILib7KSOs", "replyto": "RILib7KSOs", "signatures": ["ICLR.cc/2026/Conference/Submission15736/Reviewer_hE2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15736/Reviewer_hE2P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690950586, "cdate": 1761690950586, "tmdate": 1762925975541, "mdate": 1762925975541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores why next-token prediction (NTP), which trains on all tokens, outperforms critical-token prediction (CTP), which trains only on the last token. The authors argue that losses on uncritical tokens act as an implicit L2-style regularizer that improves reasoning and generalization. They support this with theory showing NTP equals CTP plus a quadratic regularization term, synthetic experiments where NTP avoids shortcut solutions, and real benchmarks where NTP-trained GPT-2 models show higher reasoning accuracy, better robustness, and flatter minima. Overall, the study concludes that uncritical tokens, though seemingly redundant, play a key role in regularizing models and promoting better reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper claims that uncritical tokens act as implicit regularizers. They help models avoid shortcut learning and improve reasoning. This idea is useful and explains why removing tokens during training can sometimes harm generalization.\n2.\tThe paper presents a formal analysis showing that NTP is linked to a quadratic regularizer under certain assumptions. It then tests this idea on several synthetic and real reasoning datasets. This mix of theory and experiments makes the claim stronger and more convincing.\n3.\tThe information-flow visualizations and ablations, such as the NTP shuffle, help show how NTP avoids shortcut learning. They reveal that NTP encourages step-by-step reasoning instead of early merging. These visual results make the findings easier to understand.\n4.\tThe paper shows that NTP-trained models are more robust to embedding noise and achieve flatter minima. This finding connects NTP’s behavior to generalization and SGD theory, supporting the idea that it acts as a regularizer."}, "weaknesses": {"value": "1.\tThe assumption that the last token is always critical is oversimplified. Datasets like ReCOGS (line 1329-1337) have critical tokens in different position. Using explainability tools (e.g., SHAP or LIME) could better identify important tokens.\n2.\tThe paper mentions token selection methods (RHO-1, Phi-4) but does not evaluate them. It is unclear how partial token dropping affects NTP’s regularization. Experiments across a spectrum—from full NTP to partial selection to pure CTP—would guide practitioners. Intermediate strategies like random token drop or top-k scoring are suggested.\n3.\tThe theory relies on strong assumptions: near-uniform outputs at initialization, uniform Fisher matrix, and quasi-uniform logits at convergence. These may not hold for large-scale pretraining. The authors should discuss this and provide empirical checks (Fisher spectrum, logits uniformity).\n4.\tAll experiments use small GPT-2 models (125M) trained from scratch. This limits realism. The paper misses the opportunity to explore larger and latest models, bigger corpora, or pretrained + fine-tuned setups. Comparing pretrained NTP and CTP on a downstream task would improve practical relevance. Alternatively, test whether the theoretical regularization scales with model or vocabulary size.\n5.\tThe effect of vocabulary size, label frequency, and rare vs. frequent answer tokens on NTP is unexplored. Regularization dynamics are unclear. Tracking parameter norm or Hessian trace across training, with plots for NTP vs CTP, would clarify these effects."}, "questions": {"value": "- Highlight some quantification of the main findings in the abstract and introduction.\n\n- The legend for Figure 6 is confusing and should be clarified.\n\n- Maintain a consistent color scheme for NTP and CTP throughout the paper (check Figures 3, 4, and 6).\n\n- Move a brief explanation of the NTP shuffle experiment (Appendix A.1) into the main text, as it clearly shows structure vs. objective effects.\n\n- Fairness checks showing NTP outperforms CTP under larger token/epoch budgets are only in the appendix. Include key results with actual numbers and a small table for 1×, 2×, and 4× CTP token budgets in the main text.\n\n- Add a short, intuitive sketch in the main text explaining why the second term becomes (1/2) θᵀ I₀θ.\n\n- Appendix D lists compute but omits key hyperparameters: learning rates, schedules, batch sizes, weight decay, and seeds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ISxtW6vgoX", "forum": "RILib7KSOs", "replyto": "RILib7KSOs", "signatures": ["ICLR.cc/2026/Conference/Submission15736/Reviewer_epoH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15736/Reviewer_epoH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838650104, "cdate": 1761838650104, "tmdate": 1762925974969, "mdate": 1762925974969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the differences between Next Token Prediction (NTP) and Critical Token Prediction (CTP) training approaches. Through theoretical analysis and experimental validation, it proposes that NTP training approximates CTP training with added L2 regularization. Experimental results demonstrate that NTP models exhibit stronger generalization and reasoning capabilities than CTP models, consistently outperforming them across various tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper conducts thorough theoretical analysis, trains all models from scratch, and compares NTP and CTP model performance alongside loss curve across multiple datasets."}, "weaknesses": {"value": "In practice, NTP serves as the mainstream pretraining approach, while CTP—constrained by challenges like critical token identification—is primarily used for fine-tuning on specific tasks after pretraining. The theoretical approximation proposed in this paper is only applicable to the early stages of training for models trained from scratch. This limitation prevents the paper from adequately addressing the effectiveness of training approaches like CTP following NTP."}, "questions": {"value": "1, It is recommended to include the CTP+WD training combination for comparison in experiments such as Figure 4. \n2, Provide performance comparisons between NTP and CTP training approaches using pretrained models on some difficult tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BQ00ntcjNJ", "forum": "RILib7KSOs", "replyto": "RILib7KSOs", "signatures": ["ICLR.cc/2026/Conference/Submission15736/Reviewer_SXkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15736/Reviewer_SXkx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995953809, "cdate": 1761995953809, "tmdate": 1762925974437, "mdate": 1762925974437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}