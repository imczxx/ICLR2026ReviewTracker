{"id": "OwWWuKm16A", "number": 17439, "cdate": 1758276064034, "mdate": 1759897174919, "content": {"title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning", "abstract": "Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed\nclipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and\nMMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability.", "tldr": "", "keywords": ["Reinforcement Learning", "Vision-Language Model", "Policy Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8e874d87090246e4055af1f977dc5125b295fae9.pdf", "supplementary_material": "/attachment/9b94f75c4a3704f62d9741c8c3c9064f5080c8ae.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes ACPO, a novel reinforcement learning method for VLM alignment in reasoning tasks.\nSpecifically, ACPO leverages the novel dynamic curriculum for a trade-off the on-policy exploration and off-policy exploitation. Additionally, ACPO introduces a novel advantage-aware clipping mechanism for robust training.\nExperiments on different benchmarks demonstrate that ACPO outperforms existing strong baselines, including DAPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces ACPO with interesting modifications and conducts experiments in different challenging benchmarks."}, "weaknesses": {"value": "1. There are no illustrations of module A in Fig. 1.\n2. It is improper and unclear to use the term \"dynamic curriculum\", which misleads the reviewer into knowing the exact illustration of the proposed method in section 3.1.2. Eq. (3) is basically an early-stop criterion in on-policy RL methods (e.g., PPO sometimes use entropy as the early-stop criterion).\n3. The underlying motivation of AAAC is similar to existing methods, like clip higher. It would be great to compare AAAC and clip higher using the same baseline (e.g., DAPO).\n4. There are no ablation studies to demonstrate the efficiency of the dynamic curriculum. For instance, the performance comparison with fixed optimisation steps. Or can the dynamic curriculum make ACPO have comparable performance while using less computation?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dV43LTEYxA", "forum": "OwWWuKm16A", "replyto": "OwWWuKm16A", "signatures": ["ICLR.cc/2026/Conference/Submission17439/Reviewer_3pG4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17439/Reviewer_3pG4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602626696, "cdate": 1761602626696, "tmdate": 1762927331034, "mdate": 1762927331034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ACPO, a reinforcement learning framework for aligning vision-language models in complex reasoning tasks. ACPO addresses the limitations of static PPO-style optimization by combining two adaptive mechanisms: 1. a dynamic curriculum that transitions from stable on-policy exploration to efficient off-policy exploitation through progressive sample reuse; 2. an Advantage-Aware Adaptive Clipping strategy that adjusts update bounds per token based on normalized advantage, allowing larger updates for strong signals while constraining noisy ones. Experiments on benchmarks such as MathVerse, LogicVista, and MMMU-Pro show that ACPO achieves improved performance, faster convergence and good stability over baselines like DAPO and PAPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and well-structured. The mathematical formulations are easy to follow, and Algorithm 1 concisely summarizes the training pipeline.\n\n2. The algorithmic design is well-motivated and clearly implemented, with thorough ablation studies verifying the contributions of each module."}, "weaknesses": {"value": "1. The title and narrative emphasize multimodal reasoning, but ACPO itself is modality-agnostic. The algorithm operates purely at the policy optimization level without leveraging any vision-specific mechanisms. This creates a mild mismatch between motivation and actual contribution.\n\n2. Both dynamic curriculum learning and adaptive clipping have prior foundations in RL literature. The paper's innovation lies in combining them rather than introducing a new strategies.\n\n3. Since ACPO is not inherently multimodal, additional experiments on pure language tasks would be valuable to verify its generality."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "75G9tTW6cR", "forum": "OwWWuKm16A", "replyto": "OwWWuKm16A", "signatures": ["ICLR.cc/2026/Conference/Submission17439/Reviewer_bcpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17439/Reviewer_bcpp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814669163, "cdate": 1761814669163, "tmdate": 1762927330572, "mdate": 1762927330572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adaptive Curriculum Policy Optimization (ACPO), a reinforcement learning framework designed to better align vision-language models (VLMs) in complex reasoning tasks. ACPO introduces two key mechanisms: a dynamic curriculum that transitions from on-policy to off-policy training, and an Advantage-Aware Adaptive Clipping (AAAC) strategy that adjusts update magnitude based on token-level advantage values. The authors evaluate ACPO on several multimodal reasoning benchmarks (e.g., MathVerse, We-Math, LogicVista, MMMU-Pro) using Qwen2.5-VL models of 3B and 7B sizes, demonstrating consistent performance improvements over prior PPO-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed adaptive curriculum bridges early-stage stability and late-stage efficiency, which is practically relevant for large-scale policy optimization.\n- The AAAC mechanism provides a principled way to handle heterogeneous token-level learning signals, improving both training stability and learning effectiveness."}, "weaknesses": {"value": "- The proposed ACPO framework does not rely on any vision-specific components, yet all experiments are conducted on VLMs. This raises the question of whether ACPO’s claimed benefits generalize to purely LLMs.\n- According to Tables 1 and 2, ACPO offers marginal or even negative gains over existing baselines (e.g., PAPO) on 7B-scale models, particularly in vision-dependent multimodal reasoning tasks. Moreover, the comparisons omit several strong and recent baselines [1, 2, 3].\n- The AAAC mechanism depends heavily on the choice of δ; inappropriate δ values lead to unstable training. Similarly, the data filtering process (e.g., τ and N_max for B_valid) may require task-specific tuning, limiting general applicability.\n- The paper focuses exclusively on quantitative results without showing qualitative examples (e.g., reasoning traces or failure cases) to illustrate how ACPO changes model behavior.\n\n[1]. SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement. NeurIPS 2025.\n\n[2]. VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning. NeurIPS 2025.\n\n[3]. NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation. NeurIPS 2025."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QZVTf4eZov", "forum": "OwWWuKm16A", "replyto": "OwWWuKm16A", "signatures": ["ICLR.cc/2026/Conference/Submission17439/Reviewer_C5H4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17439/Reviewer_C5H4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881863751, "cdate": 1761881863751, "tmdate": 1762927330036, "mdate": 1762927330036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the problem of aligning large vision-language models (VLMs) for complex multimodal reasoning tasks using reinforcement learning. The authors point out that standard policy optimization methods have problems with static training schedules and a strict uniform clipping threshold, which can make it harder for models to learn. \nTo overcome these issues, the paper proposes Adaptive Curriculum Policy Optimization (ACPO), a framework with two key innovations. First, ACPO employs a dynamic curriculum that gradually transitions the training from a near on-policy exploration phase to an off-policy exploitation phase by progressively increasing the sample reuse count during training. This allows the model to start with stable learning on fresh data and later capitalize on valuable past experiences as training progresses. Second, it introduces an Advantage-Aware Adaptive Clipping (AAAC) mechanism, which replaces PPO’s fixed clipping parameter with a dynamic, sample-wise clipping bound modulated by each token’s normalized advantage. In essence, tokens with higher advantage (indicating more beneficial learning signal) are allowed a looser update constraint, enabling larger policy updates, while low-advantage or risky samples get a tighter bound to prevent destructive updates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "# Strengths\n\n1.  The proposed ACPO framework introduces a two-pronged adaptive strategy that is novel and intuitive. The dynamic curriculum (adaptive sample reuse) provides a principled way to balance exploration vs. exploitation over the course of training.\n\n2. ACPO delivers strong empirical results. It outperforms state-of-the-art baselines (DAPO, PAPO) on multiple complex reasoning benchmarks, achieving the highest average accuracy in both vision-dependent and general multimodal reasoning categories."}, "weaknesses": {"value": "# Weaknesses\n\n1. Incremental Innovation: While effective, the contributions of ACPO could be viewed as incremental improvements over existing methods rather than entirely new techniques. The idea of curriculum learning in RL (gradually increasing difficulty or sample reuse) is not brand-new.\n\n2. The proposed method can also be used for textual QA tasks. The motivation for only applying on VLM is not clear. \n\n3. The performance gains (based on Tables 1 and 2) are not significant. This makes me question whether the method will consistently underperform the DAPO baseline on text QA tasks."}, "questions": {"value": "# Questions\n\n1. Could you elaborate on the choice of a linear schedule for K(t) (the sample reuse count)? Did you try other scheduling strategies (e.g., exponential increase, or an adaptively determined schedule based on performance)?\n\n2. Did the authors evaluate the proposed model on standard text QA benchmarks—the same settings used in the DAPO paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gsSPgFgXLU", "forum": "OwWWuKm16A", "replyto": "OwWWuKm16A", "signatures": ["ICLR.cc/2026/Conference/Submission17439/Reviewer_2Sk6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17439/Reviewer_2Sk6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896470653, "cdate": 1761896470653, "tmdate": 1762927329424, "mdate": 1762927329424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ACPO is a novel VLM finetuning method by taking inspiration from curriculum learning. It is difficult to finetune a VLM directly on difficult reasoning tasks. To counteract this, ACPO uses an adaptive sampler to create a curriculum that reflects the difficulties of the training task. This allows the policy to use off-policy RL to finetune on these tasks, which gives faster convergence and more training stability on top of empirical gains. When evaluated across 4 difficult reasoning benchmarks, ACPO achieves strong performance when compared against DAPO (both on and off policy) and PAPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I believe there is sufficient coverage in the experiments conducted as it deals with a variety of different difficult reasoning domains.\n2. The algorithm section helped me to understand the pipeline well, and I generally like the presentation of the paper.\n3. There is good innovation in combining curriculum learning with finetuning for reasoning: I believe that it might be tricky to directly perform finetuning on difficult visual tasks, so it is good to see inclusion of curriculum learning on that end."}, "weaknesses": {"value": "1. There are not enough statistical analyses being performed on the main results and I cannot distinguish whether the results that you have presented are statistically significant. I believe that it will be very helpful for readers if the authors include metrics such as standard error and/or t-tests.\n2. The method’s generalizability is being limited due to how many moving components the method employs. In particular, I find the three stages of finetuning (on policy exploration $\\rightarrow$ off-policy exploitation) might be very cumbersome. I might be wrong here, but are there any previous works that demonstrate the portability of these implementation details w.r.t. finetuning VLMs?\n3. I believe that the writing can be improved. For example, neither $o$ nor $\\tau$ were introduced in the main text before their usage in your formulation. Defining each term before using it can strengthen the paper’s presentation."}, "questions": {"value": "1. Instead of using error functions, did you try using other nonlinearities that have the same range as erf, such as tanh or rescaled sigmoid?\n2. The abstract mentioned that ACPO is able to achieve accelerated convergence, yet I did not see much of this in figure 2. Can you point to me where did your experiments help validate this claim, or if not, can you provide more ablations to this?\n3. What reward models did the authors use?\n4. Following up on (3), if there are particular failure cases of the reward model, would there be concerns about overfitting or fitting the VLM to spurious outputs by the reward model?\n\nMinor remarks:\n1. In figure 1, $i-th$ and $(i+1)-th$ should be reformatted to $i$-th and $(i+1)$-th to reduce ambiguity.\n2. I took a look at the supplementary material and it appears that some of the dataset in visual reasoning are in Chinese. Did the evaluation procedure require outputs from multiple languages? If so, it would be good to include this disclaimer in the main text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kt1wW5Wv6n", "forum": "OwWWuKm16A", "replyto": "OwWWuKm16A", "signatures": ["ICLR.cc/2026/Conference/Submission17439/Reviewer_daoo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17439/Reviewer_daoo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985009766, "cdate": 1761985009766, "tmdate": 1762927328938, "mdate": 1762927328938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}