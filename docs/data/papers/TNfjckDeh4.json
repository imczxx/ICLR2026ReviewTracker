{"id": "TNfjckDeh4", "number": 13142, "cdate": 1758214039215, "mdate": 1759897461234, "content": {"title": "UnLoc: Leveraging Depth Uncertainties for Floorplan Localization", "abstract": "We propose UnLoc, an efficient data-driven solution for sequential camera localization within floorplans. Floorplan data is readily available, long-term persistent, and robust to changes in visual appearance. We address key limitations of recent methods, such as the lack of uncertainty modeling in depth predictions and the necessity for custom depth networks trained for each environment. We introduce a novel probabilistic model that incorporates uncertainty estimation, modeling depth predictions as explicit probability distributions. By leveraging off-the-shelf pre-trained monocular depth models, we eliminate the need to rely on per-environment-trained depth networks, enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale synthetic and real-world datasets, demonstrating significant improvements over existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$ times higher localization recall on long sequences (100 frames) and $42.2$ times higher on short ones (15 frames) than the state of the art on the challenging LaMAR HGE dataset.", "tldr": "", "keywords": ["floorplan localization", "sequential localization", "depth uncertainties", "mono-depth networks"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/678a0a02d419ec6c282c4c653b80b28e7b665d24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes combining pre-trained depth models with uncertainty modeling to improve the accuracy of floor-plan localization. Experiments are conducted on multiple datasets, including both real-world and synthetic scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is generally well written and easy to follow.\n\n2. It makes sense to leverage uncertainty information for localization.\n\n3. The proposed method achieves state-of-the-art performance on two floor-plan localization benchmarks."}, "weaknesses": {"value": "1. The paper mainly integrates existing methods and applies them to a relatively narrow task—floor-plan localization. While the topic is relevant, the contribution appears incremental rather than conceptually novel. This paper does not introduce any new contribution in terms of problem formulation; it focuses on a narrow task with clearly defined inputs and outputs.\n\n2. As shown in Table 6, F3Loc + Depth Anything V2 already achieves higher accuracy than UnLoc on several datasets. This suggests that much of the performance gain may stem from the use of off-the-shelf pre-trained depth foundation models rather than from the new proposed method in this paper. The paper, therefore, reads more as an application of pre-trained depth models to a specific downstream task than as a novel algorithmic/theory contribution. \n\n3. The discussion on uncertainty is underdeveloped. Uncertainty in neural network predictions can generally be categorized into aleatoric and epistemic uncertainty [1], yet this paper lacks a theoretical explanation or analysis of which type is modeled or how it contributes to performance improvements.\n\n4. The data preprocessing and benchmark setup are not sufficiently transparent. The paper appears to introduce newly processed data and custom split strategies from public datasets, which raises concerns about reproducibility. It is unclear whether the test set division used in this work aligns with the official benchmark split and whether the authors may have chosen a split that yields the best performance. Providing clear details on dataset handling and evaluation would significantly improve the credibility of the results. This paper does not provide the recommended Reproducibility statement and the Use of Large Language Models (LLMs) statement. \n\n\n[1] Alex Kendall et al., What uncertainties do we need in Bayesian deep learning for computer vision?, NeurIPS 2017."}, "questions": {"value": "I am particularly curious about how this work is evaluated on the LaMAR dataset. To my knowledge, LaMAR does not include publicly available floor plans. Could the authors clarify where the floor plan data used in this paper originated? The description in the Appendix is quite brief and would benefit from a more detailed explanation. \n\nMoreover, LaMAR does not provide ground-truth poses for the test set, and the paper does not specify the session divisions used. Since the evaluation is based on estimating 2D poses on floor plans, it remains unclear how these poses were submitted or compared within the LaMAR benchmark to produce the results shown in Table 2. Additionally, even if the official floor plans were obtained, how were the 6Dof camera poses from LaMAR aligned with the 2D pose in the floor plans? If you use custom's align algorithm to calculate the ground-truth label of 2D poses, how can you ensure that this ground-truth itself is reliable?\n \nA detailed explanation of the data processing pipeline and evaluation protocol would be very helpful. \n\nI also noted that F3Loc did not include such evaluation details—it only released the link to the Gibson Floorplan Localization Dataset. This raises further concerns about the fairness and transparency of comparing F3Loc and the proposed method on the LaMAR HGE/CAB benchmark. Overall, this area still lacks openly available datasets and standardized baselines, making reproducibility a critical issue at this stage.\n\nIf the authors could provide a more comprehensive description of the evaluation methodology and experimental setup on LaMAR, I would consider improving my overall rating. Furthermore, I strongly encourage the authors to release the processed version of the LaMAR data used in this work. Since the paper involves substantial preprocessing of the raw LaMAR dataset, referring to it as “widely used” without sharing those processed resources seems somewhat inappropriate and limits the community’s ability to validate the claimed improvements in accuracy and reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YAJHmZyQ4d", "forum": "TNfjckDeh4", "replyto": "TNfjckDeh4", "signatures": ["ICLR.cc/2026/Conference/Submission13142/Reviewer_PYiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13142/Reviewer_PYiR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760850680377, "cdate": 1760850680377, "tmdate": 1762923860537, "mdate": 1762923860537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UnLoc (Uncertainty-aware Localization), a method for sequential camera localization within 2D floorplans. The input of the method is: \n1/ a sequence of t RGB images,\n2/ relative poses between these images,\n3/ gravity direction \n4/ camera intrinsics, and\n5/ geometry layout of the floorplan\n\nIt aims to address two key shortcomings of previous state-of-the-art methods: the lack of uncertainty modeling in depth predictions and the need for custom, environment-specific depth networks for each environment (e.g. in $F^3Loc$). The paper reports strong performance compared to previous methods on LaMAR HGE dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(+) Strong performance shown in the experiment section, although the paper mainly focuses on LaMAR HGE dataset\n\n(+) Compared to the previous method, such as $F^3Loc$, the proposed method is more general, in the sense that it does not rely on per-scene depth estimator, which significantly improves the usability of such methods\n\n(+) the writing is the paper is good -- it is relatively easy to the reviewer to follow\n\n(+) Using a depth encoder seems new to me and makes a lot of sense"}, "weaknesses": {"value": "(-) Biggest concern from me is the lack of novelty and suitability in the sense of the ICLR (learning representation) community. The paper is a 3D computer vision problem in pose estimation in a room. The main contributions / problems to improve from previous methods are 1/ the lack of uncertainty modeling in depth predictions and 2/ the need for custom, environment-specific depth networks for each environment. These are very specific novelties for this particular task, instead of any major novelty for the learning representation community\n\n(-) The proposed method feels like using / combining a few existing tools/engineering techniques (DepthAnything, Rotate/Translate the images, Floorplan matching, and heavy post-processing etc) and made it work for a particular task. This feels more suitable for submission to a 3DV conference, CVPR or robotics conference, instead of a machine learning conference\n\n(-) The performance of the method will be limited by DepthAnything's accuracy, so this paper essentially trades accuracy ceiling with generalizability\n\n(-) the experiments focus heavily on a small dataset"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QT1AthdUxO", "forum": "TNfjckDeh4", "replyto": "TNfjckDeh4", "signatures": ["ICLR.cc/2026/Conference/Submission13142/Reviewer_Lj7y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13142/Reviewer_Lj7y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761276336058, "cdate": 1761276336058, "tmdate": 1762923860190, "mdate": 1762923860190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for visual floorplan localization that integrates uncertainty-aware monocular depth estimation. Specifically, the authors leverage large-scale pre-trained monocular depth models (e.g., Depth Anything) to (1) enhance generalization to unseen environments, and further (2) incorporate weighting on depth predictions based on their estimated confidence/reliability. Experimental results show substantial gains in both accuracy and robustness over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple yet sound idea that leads to performance improvements.\n- This presentation is clear and easy to follow, with figures that are mostly intuitive and effectively support the explanations.\n- The evaluation section is thorough – Table 1 and 2 showing sequence localization on Gibson and LaMAR HGE datasets, respectively. The author also reports the runtime which is crucial for localization tasks. Furthermore, the paper also ablates different depth estimation models and validates the effectiveness of the proposed uncertainty estimation."}, "weaknesses": {"value": "- The overall technical contribution is somewhat limited, as the main novelty lies in replacing the F3Loc depth networks with SOTA monocular depth estimation models. Moreover, uncertainty modeling in depth estimation has already been explored in many prior works [1, 2].\n- Lack of discussion about the uncertainty in the related works or preliminary. The author simply throws the equation at Sec3.5 without sufficient explanation of its derivation or references to prior studies that motivated it.\n\n[1] Kendall and Garl, What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?\n\n[2] Poggi et al., On the uncertainty of self-supervised monocular depth estimation."}, "questions": {"value": "- Is this depth encoder being fine-tuned during training? If it is, wouldn’t this make the approach similar to F3Loc, which requires model training for each environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YQMX7GpRLG", "forum": "TNfjckDeh4", "replyto": "TNfjckDeh4", "signatures": ["ICLR.cc/2026/Conference/Submission13142/Reviewer_wgVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13142/Reviewer_wgVS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980775153, "cdate": 1761980775153, "tmdate": 1762923859785, "mdate": 1762923859785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method for floor plan localisation based on a sequence of input images. Its main contribution lies in introducing an uncertainty-aware depth estimation framework and replacing customised depth models with modern monocular depth networks. As a result, the proposed approach outperforms prior methods on two datasets, LaMAR and Gibson.\n\nStrength: The idea of incorporating uncertainty into the depth matching process is novel and technically well-motivated. The experimental results convincingly support the method’s effectiveness.\n\nWeakness: The evaluation lacks results on the Structured3D dataset, which is used by the closely related baseline F3Loc. Moreover, replacing a customised depth network with a modern monocular model, while beneficial, is a relatively straightforward modification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of introducing uncertainty into the depth matching process is novel and technically sound.\n2. The experiments are comprehensive and largely convincing.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The results on the Structured3D dataset are missing, which is used in the closely related baseline F3Loc.\n2. Replacing the customized depth networks with modern monocular depth networks is relatively straightforward."}, "questions": {"value": "1. Could you include results on the Structured3D dataset?\n2. Are there any challenges in replacing the customized depth network with monocular depth networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Wov6BgVh6h", "forum": "TNfjckDeh4", "replyto": "TNfjckDeh4", "signatures": ["ICLR.cc/2026/Conference/Submission13142/Reviewer_wqUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13142/Reviewer_wqUb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113808317, "cdate": 1762113808317, "tmdate": 1762923859164, "mdate": 1762923859164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}