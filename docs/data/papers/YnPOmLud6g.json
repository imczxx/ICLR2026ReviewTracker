{"id": "YnPOmLud6g", "number": 23251, "cdate": 1758341304308, "mdate": 1759896824107, "content": {"title": "Domain-Specialized Tree of Thought through Plug-and-Play Predictors", "abstract": "While Large Language Models (LLMs) have advanced complex reasoning, prominent methods like the Tree of Thoughts (ToT) framework face a critical trade-off between exploration depth and computational efficiency. Existing ToT implementations often rely on heavyweight LLM-based self-evaluation or rigid heuristics for branch pruning, making them prohibitively expensive and inflexible for broad application. To address this, we introduce DST, an adaptable, plug-and-play predictor that serves as a lightweight, supervised heuristic to guide the ToT search process. Our predictor enables dynamic, context-aware pruning, allowing the search to proceed with near-greedy efficiency on simpler reasoning steps while adaptively expanding the search beam only when encountering uncertainty or task complexity. We evaluate our approach on a diverse suite of benchmarks spanning mathematical reasoning, general reasoning, and complex logical reasoning. Experimental results demonstrate that our method achieves accuracy competitive with or superior to strong baselines, including standard ToT, while reducing computational overhead by 26-75\\%. Our work effectively resolves the accuracy-efficiency trade-off in tree-based reasoning, transforming ToT from a resource-intensive technique into a scalable and practical paradigm for complex problem-solving in LLMs.", "tldr": "", "keywords": ["Large Language Models", "Reasoning", "Adaptive Search"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/400b975fef346f56b1420bb64e5031bdfd12fd9f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Domain-Specialized Tree (DST), a lightweight, plug-and-play supervised predictor that scores partial thoughts during reasoning: go nearly greedy when confident and expand a small beam only when uncertain, achieving an adaptive accuracy–efficiency trade-off. For training, small reasoning trees are built via BFS; leaf correctness is verified by domain checks and then discounted upward to supervise internal nodes, while a gradient-boosted model learns from semantic and ancestry-consistency features. At inference time, one candidate is scored first—if above a threshold, proceed greedily; otherwise, complete a small beam and expand by scores. Across math and general reasoning benchmarks with multiple LLMs, DST matches or surpasses standard ToT while cutting token usage by ~26–75%, substantially improving practicality and cost-effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a practical and plug-and-play way to reduce the compute cost of tree-of-thought reasoning while preserving accuracy. Instead of noisy and expensive LLM self-evaluation or ad-hoc pruning rules, it learns a lightweight predictor from verifiable leaves with discounted credit assignment to internal nodes, yielding a clean supervision signal. The adaptive “greedy-when-confident, beam-when-uncertain” policy exposes a single threshold to trade quality for latency/tokens, and works across multiple backbone LLMs. Ablations confirm the contribution of semantic and consistency features. The method is simple to implement, easy to deploy without retraining the base model, and particularly attractive for domains with programmable validators."}, "weaknesses": {"value": "The experimental coverage is limited: key baselines such as Graph of Thoughts (graph-structured reasoning), RAP/MCTS-based planning, and Verifier/PRM-guided search are missing or insufficiently compared under matched compute. \n\nThe method’s novelty is incremental—learning a cheap heuristic to gate beam expansion closely echoes verifier/PRM-guided test-time search—so a stronger positioning and head-to-head baselines are needed. \n\nReproducibility is a concern as no anonymous code is provided; a minimal implementation with the exact threshold/discount/feature extraction is necessary. The approach further relies on domain verifiers to construct supervision, leaving unclear how it performs in tasks without programmatic checks; failure cases and alternatives should be discussed. \n\nFinally, robustness and transfer (across backbones/tasks) and sensitivity to τ/γ/beam size warrant a more systematic analysis.\n\nIf I missed any details, please let me know during the rebuttal period."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BJfduwEdi2", "forum": "YnPOmLud6g", "replyto": "YnPOmLud6g", "signatures": ["ICLR.cc/2026/Conference/Submission23251/Reviewer_n2np"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23251/Reviewer_n2np"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772829734, "cdate": 1761772829734, "tmdate": 1762942576277, "mdate": 1762942576277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, authors introduce Domain-Specialized Tree of Thought (DST), which integrates a lightweight, supervised predictor as a plug-and-play heuristic for guiding the ToT search process. The main motivation behind introducing the predictor is to score and prune potentially bad reasoning steps during the inference, thus reducing the search breadth. The predictor itself is trained offline on small, domain-specific datasets to estimate the quality of intermediate reasoning steps. Authors perform experiments with Qwen3-8B, Llama3.1-8B, and Gemma3-12B on benchmarks such as GSM8K, MATH-500, GPQA, and BIG-Bench, andd show that DST improves both scalability and performance consistency across models and tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Authors combine the Tree of Search, Verifier, and Adaptive hybrid search strategy in one efficient framework\n2. Paper is clear and well-written\n3. Authors perform experiments on a variety of models and tasks, showing strong generalization abilities"}, "weaknesses": {"value": "1. Limited conceptual novelty beyond existing ToT variants\n\nWhile the paper introduces a well-engineered and effective modification to Tree-of-Thought reasoning, its core idea—using an auxiliary model to guide search or prune branches—is conceptually close to prior adaptive ToT or heuristic-based reasoning methods. For example, Dynamic Parallel Tree Search [1] and Adaptive Graph of Thoughts [2] already propose dynamic or confidence-driven expansion strategies.\nThe probabilistic scoring in ProbTree [3] and the validator-based mechanisms in MA-ToT [4] also aim to reduce unnecessary exploration based on intermediate evaluations. DST’s predictor-based pruning can be viewed as a refinement or reimplementation of these ideas using a supervised model instead of confidence heuristics — a valuable engineering step, but incremental rather than conceptually transformative.\n\n2. The approach depend on a domain-specific verifier training, meaning that adapting the framework to a new domain would always require data collection and training steps. I wonder is cross-domain transfer experiments can show if the same model can be effectively reused.\n\nReferences:\n1. Ding, Yifu, et al. \"Dynamic parallel tree search for efficient llm reasoning.\" arXiv preprint arXiv:2502.16235 (2025)\n2. Pandey, Tushar, et al. \"Adaptive graph of thoughts: Test-time adaptive reasoning unifying chain, tree, and graph structures.\" arXiv preprint arXiv:2502.05078 (2025).\n3. Cao, Shulin, et al. \"Probabilistic tree-of-thought reasoning for answering knowledge-intensive complex questions.\" arXiv preprint arXiv:2311.13982 (2023).\n4. Haji, Fatemeh, et al. \"Improving LLM reasoning with multi-agent Tree-of-Thought Validator agent.\" arXiv preprint arXiv:2409.11527 (2024)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A2JX8zxKxp", "forum": "YnPOmLud6g", "replyto": "YnPOmLud6g", "signatures": ["ICLR.cc/2026/Conference/Submission23251/Reviewer_1WkQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23251/Reviewer_1WkQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870964489, "cdate": 1761870964489, "tmdate": 1762942576000, "mdate": 1762942576000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DST (Domain-Specialized Tree of Thought), which is a framework that solves the problem of high evaluation cost on Tree of Thought (ToT). By training a lightweight supervised predictor with a small domain dataset, the predictor scores intermediate reasoning steps to determine if the search should be greedy or expand into full beam. This allows adaptive and efficient reasoning to retrieve higher accuracy responses. The key contribution of DST is in balancing accuracy and efficiency where this framework can achieve 26-75% of reduced token consumption while maintaining or improving reasoning accuracy. From the experiment, DST outperforms or matches ToT and other baselines on various reasoning task benchmarks with less computation cost in training and inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a clear approach to tackle the main ToTs core bottleneck, which is LLM-based evaluation cost.  The lightweight model presented is being trained as a predictor to score intermediate reasoning steps to allow adaptive and efficient reasoning for ToTs. The authors provide the formal algorithm and formula for training the predictor and running inference. Authors provided comprehensive experiments, using different models like Qwen3-8B, Llama3.1-8B, Gemma3-12B. Authors provided consistent and well-visualized graphs to support the strong efficiency-accuracy trade-offs. Additionally, ablation studies show that semantic and consistency features are essential for the best performance."}, "weaknesses": {"value": "One specific weakness is the claim in “small” datasets, but it does not quantify how small or a clear data size. It is not clear on which training dataset and the size is used to train your LightGBM classifier during experiment. There should be a bit more detail on the experiment setup like training epochs, predictor architecture. One concern on the training where it uses LLM to assess answers based on semantic entailment, which could possibly inherit biases or overfit to the specific model’s reasoning."}, "questions": {"value": "What model architecture is used for the predictor? Is it MLP or small transformer? \nHow does the predictor’s performance scale with the number of training samples?\nHave you tried testing the predictor that was trained on one domain to transfer effectively to another domain? \nSince you proposed plug and play, does that mean the predictor trained based on one inference model can be used to any LLM backbone without re-training to the same reason format of the LLM backbone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9eoVWvRx1W", "forum": "YnPOmLud6g", "replyto": "YnPOmLud6g", "signatures": ["ICLR.cc/2026/Conference/Submission23251/Reviewer_ovtp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23251/Reviewer_ovtp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901455881, "cdate": 1761901455881, "tmdate": 1762942575772, "mdate": 1762942575772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Domain-Specialized Tree-of-Thought (DST): a test-time framework that plugs a lightweight, supervised predictor into a Tree-of-Thought (ToT) search to decide when to (i) greedily commit to the first generated thought or (ii) expand to a wider beam when uncertainty is high. The predictor uses two features per node—(a) a semantic embedding extracted from the LLM’s hidden states and (b) a path-consistency score—and is trained offline from trees labeled via leaf verification and discounted score propagation. At inference, a “predict-first-thought” policy prunes siblings if the first thought’s score exceeds a threshold, otherwise falls back to full beam expansion. Across math (GSM8K, SVAMP, Minerva, MATH-500), general reasoning (GPQA), and BIG-Bench Extra Hard subtasks, the paper reports comparable or better accuracy than ToT/DPTS while reducing tokens by 26–75% versus ToT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Quality.\n\n- Clear algorithms: Algorithm 1 (training data collection + discounted score propagation) and Algorithm 2 (predictor-guided pruning) are specified and align with the described workflow. \n- Complexity analysis relates expected effective beam width to the predictor’s confidence threshold, giving an intuitive handle on efficiency. \n- Ablations indicate both features (semantic vector and consistency) contribute; removing either degrades accuracy and increases tokens. \n\nClarity.\n\n- The method overview and worked example (simple arithmetic word problem) effectively convey the early-exit vs. fallback behavior. \n\nSignificance.\n\n- If reliable, the reported 26–75% token savings on top of ToT while maintaining or improving accuracy would be valuable for test-time scaling under tight compute budgets. The paper shows consistent benefits across three backbones and multiple benchmarks; e.g., +14% absolute accuracy on BoardgameQA over CoT with far fewer tokens"}, "weaknesses": {"value": "1. “Plug-and-play” claim conflicts with reliance on hidden states.\n\n- DST’s key feature (v_s) is derived from LLM hidden states (v_s = h(p_\\theta([x_s; Z_s]))). This requires access to internal activations, which many hosted APIs do not expose; it also couples the predictor to a specific backbone and prompt format. The paper calls the predictor “decoupled” and “plug-and-play,” but in practice this dependency limits portability and undermines the claim of easy deployment across models and providers. Please discuss feasibility when only logits/text are available, or provide a text-only feature variant. \n\n2. Training data pipeline lacks concrete scale/cost details and may embed evaluation shortcuts.\n\n- The labeling pipeline uses pattern matching, NLI, and symbolic execution to score leaves, then discounts scores upward. While conceptually solid, the paper does not quantify: (i) how many problems/trees per domain, (ii) tokens/time to generate trees, (iii) verifier accuracy and failure modes, and (iv) robustness to noisy labels. Without these, “lightweight” is hard to assess and risks circularity if pattern-match heuristics overfit answer formats. \n\n3. Empirical comparisons are selective; broader baselines are missing.\n\n- The paper compares against CoT, ToT, and DPTS, but omits other recent test-time controllers/validators (e.g., interactive ToT / validator agents, retrieval-guided thought scoring). Given the premise (“resolve accuracy–efficiency trade-off in ToT”), the absence of stronger, cost-aware evaluators limits claims about the state-of-the-art frontier. (The related work list mentions such methods, but ablations/benchmarks do not include them.) \n\n4. Reporting and fairness of efficiency metrics need tightening.\n\n- Token accounting should specify whether verifier tokens (for leaf checks) and data-generation tokens are included in total deployment cost, and what is counted at training-time vs. test-time. Currently, token savings are reported per inference run, but many applications must amortize predictor training + per-domain verifier overhead. Clarify the measurement protocol and include confidence intervals or variance across seeds. (The paper states identical hardware and temperatures, but statistical significance is not reported.) \n\n5. Limited analysis of failure cases and generalization.\n\n- The model’s behavior when the predictor is miscalibrated (e.g., over-pruning early) is not thoroughly explored. Sensitivity results are helpful, but we lack case studies showing when DST loses compared to ToT or when the discount factor harms problems requiring deliberate long chains. \n\n6. Ambiguity around domain specialization and transfer.\n\n- The paper asserts easy transfer with “small datasets,” but does not specify how much data per domain, how thresholds are tuned, or how well a predictor trained on math transfers to GPQA/BBEH without re-labeling. Quantitative cross-domain transfer experiments would strengthen the “plug-and-play” narrative."}, "questions": {"value": "Please refer to weaknesses for questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LDL7jJXpmy", "forum": "YnPOmLud6g", "replyto": "YnPOmLud6g", "signatures": ["ICLR.cc/2026/Conference/Submission23251/Reviewer_psYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23251/Reviewer_psYR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056327212, "cdate": 1762056327212, "tmdate": 1762942574929, "mdate": 1762942574929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}