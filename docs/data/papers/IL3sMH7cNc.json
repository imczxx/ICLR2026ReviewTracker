{"id": "IL3sMH7cNc", "number": 7926, "cdate": 1758043596447, "mdate": 1759897821781, "content": {"title": "A Descriptive and Normative Theory of Human Beliefs in RLHF", "abstract": "Human preferences in RLHF are typically modeled as a function of the human's reward function or corresponding optimal state-action values. In this work, we propose that human beliefs about the capabilities of the agent being trained also play a key role in preference generation. We examine two questions related to this hypothesis, one descriptive and one normative, respectively: Do human labelers' beliefs about agent capabilities affect the preferences that they provide? And what is the ideal set of beliefs about an agent---and resulting preferences---for humans to have? We propose a new preference model that incorporates human beliefs and provide a normative theory that bounds the error on the final learned policy based on the _mismatch_ between the human's beliefs and an idealized set of beliefs. We then confirm via a human study that beliefs about agent capabilities do, in fact, significantly affect preferences and can be influenced through simple interventions. Additionally, we empirically show through synthetic experiments that it is often suboptimal for human preference labelers to assume agent optimality. Collectively, these results theoretically and empirically demonstrate how reducing the mismatch between human beliefs and agent capabilities can lead to more performant RLHF and point toward new best practices for RLHF practitioners.", "tldr": "In this work, we propose that aligning human labelers’ beliefs with actual agent capabilities—rather than assuming optimality of agent behavior—leads to more reliable RLHF.", "keywords": ["AI alignment", "preference learning", "preferences", "reinforcement learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9581a0e89c2c52363c523d4e98964fbf679fd8fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors explore two key questions: whether human labelers’ beliefs affect the preferences they give, and what beliefs would be ideal for optimal learning. They introduce a new model that incorporates these beliefs and provide theoretical bounds showing how mismatched beliefs can harm learning performance. The study is based on human and synthetic experiments."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ A formalization of RHLF is per se a very interesting idea."}, "weaknesses": {"value": "- A core assumption of this work is that the best possible policy post RLHF is known a priori. But if this is possible, what is the need for RLHF in the first place? At the point, RLHF appears not to be needed? In fact, the authors say that the “normative ideal in regret-based RLHF is for labellers to provide preferences with respect to a quantity that is related to the best possible post-RLHF policy under any labelling”. This appears to be impossible in practice.\n- The reviewer understands that it is always nice to see something proven experimentally, but the reviewer would say that it is expected that if there is “disagreement” between the labeler and the agent’s capabilities, the actual performance would be poor.\n- In general, the reviewer is not completely sure about the actual overall assumption that is made by the authors: essentially, it seems to me that the authors assume that the insufficient or deceptive information is given to the labelers. How is this something that happens in practice? It seems to me a problem of poor engineering practice that can be tackled with thorough design of the RLHF process.\n- Again I understand that there is a potentially theoretical interest in this problem, but it seems to me that in actual fact RLHF is a very practical problem. The solution offered to the practitioners appears not completely viable in my opinion.\n- The case study (and the human study) that is presented in the paper is very specific. The authors make some claims about lesson learnt and generalisation that are not really supported by strong evidence in my opinion.\n- The formal bound that is presented in the paper refers to a very specific situation. The result is correct, but the reviewer wonders about its practical situation. It is difficult to “approximate” this result to practical situations in my opinion.\n- There is an aspect related to the complexity of the task (if you want related to the problem of decision-making in a bounded rationality setting), which is not really taken into consideration by the authors."}, "questions": {"value": "- The authors should clarify the actual assumptions of the model in terms of the actual “knowledge” that should be provided to the labelers.\n- Can the authors provide evidence that this is a real problem in practical situations? Can you provide some examples?\n- How can you claim that the proposed problem could be generalised to other scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZZjCrxqDY8", "forum": "IL3sMH7cNc", "replyto": "IL3sMH7cNc", "signatures": ["ICLR.cc/2026/Conference/Submission7926/Reviewer_CLTJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7926/Reviewer_CLTJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913662682, "cdate": 1761913662682, "tmdate": 1762919950995, "mdate": 1762919950995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that human beliefs about an agent's capabilities impact their preferences used for RLHF. This mismatch between human's belief and the agent's actual performance would hurt the learned policy. The paper introduces a belief-based preference model and gives theoretical bounds on the performance degradation for the learned policy under this mismatch. Human study is also conducted to verify this claim. The paper finally suggests two approaches that can be applied in actual preference collection process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is mostly well-written, clearly organized, and easy to follow. It formalizes the agent-labeler agreement clearly, which, based on my knowledge, hasn't been discussed much in previous literature.\n\n* Human study is conducted as evidence that preferences can be shifted in a statistically significant way, which confirms the motivation of this paper."}, "weaknesses": {"value": "* The recommendations given by the authors in Sec 6.1 doesn't seem to be actionable to me (e.g., inform labelers of the known limitation). It still leaves out the problem of *how* or *what* to disclose. As the authors have demonstrated through their human study experiment, these choices can impact the preference in a significant way.\n\n* Section 4 gives us a theory for the effect of human-labeler disagreement, but the authors haven't shown how this theory can be applied for designing any operational protocol. Parameters like $\\delta$ in Theorem 4.3 seem to be hard to estimate in real-world scenarios."}, "questions": {"value": "1. Are there any real-world RLHF datasets or collection process that suffer significantly from this belief disagreement problem? It seems to be that if the majority beliefs are \"right\", then the disagreement for the minority can be viewed as noise and wouldn't make much impact on the learned policy? The synthetic human study confirms that it is possible to manipulate human's preference, but the question still remains on whether it happens a lot in the real world.\n\n2. How should we interpret Theorem 4.3? Can we use the bound for estimating anything, such as how significant would the return be degraded in a given dataset? Also, Theorem 4.3 gives a lower bound on the performance with the mismatched belief, can the authors give an intuition or proof on the tightness of this bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n7eCponNwZ", "forum": "IL3sMH7cNc", "replyto": "IL3sMH7cNc", "signatures": ["ICLR.cc/2026/Conference/Submission7926/Reviewer_S7ae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7926/Reviewer_S7ae"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971134500, "cdate": 1761971134500, "tmdate": 1762919948636, "mdate": 1762919948636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new theoretical and empirical framework for understanding human beliefs in RLHF. Standard RLHF models assume that human preferences reflect evaluations of trajectories under an optimal agent, but the authors argue that human labelers’ beliefs about agent capabilities also influence their feedback. The paper makes two main contributions. It empirically demonstrates, via synthetic and human studies, that humans’ beliefs about agent competence significantly affect their preferences. Then it formalizes a new “belief-based preference model,” defining an ideal belief alignment between human labelers and agents, and proves theoretical bounds showing how mismatched beliefs can degrade policy performance.\n\nEmpirically, the authors show in a GridWorld experiment that preference mismatches between human beliefs and actual agent capability reduce performance. The authors also conduct a human study in a self-driving car setting, showing that priming participants to believe the agent is more or less capable systematically shifts their preferences. The paper concludes by recommending practical strategies for aligning human beliefs and agent capabilities during preference collection—such as transparent communication about agent limitations or online priming during RLHF."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper investigates an important, under-explored problem. RLHF typically assumes preferences reflect an (implicit) optimality/regret model. Modeling how beliefs about agent capability influence preferences is a realistic and consequential gap—especially for safety-critical settings where an over-confident preference can induce risky policies. This framing is timely and relevant to the community.\n2. The paper provides theoretical guarantee tying disagreement to expected return. Theorem 4.3 and Corollary 4.4 provide an explicit bound showing how the magnitude of agent-labeler disagreement degrades the learned policy’s return. Having a provable dependence is valuable: it provides intuition and a diagnostic for practitioners."}, "weaknesses": {"value": "1. The paper makes strong simplifications in the theory, which limit practical applicability. The theoretical bound (Theorem 4.3) assumes noiseless preferences, deterministic tabular policies, single-transition preference pairs, and that RLHF yields a deterministic policy that exactly respects all preferences. These assumptions are overly idealistic and far from the modern practice of RLHF (noisy annotators, function approximation, and finite data). The practical implication of the bound in realistic RLHF with high-dimensional policies is therefore unclear.\n2. The scope of empirical evaluation is narrow. The synthetic experiments are helpful, but they use a toy GridWorld and a specific noise model ($\\epsilon$-greedy). It’s unclear how results extend to richer environments (continuous control, long horizon, LLM behavior) or to non-$\\epsilon$ noise (e.g., systematic biases, function approximation capacity limits). Besides, the human study is restricted to a single domain (driving) and a particular priming manipulation. While statistically significant effects were found, it’s unclear how general these priming effects are across labeling populations, tasks, and phrasing of instructions.\n3. The paper provides no experiments connecting to modern RLHF for LLMs. The paper motivates with general RLHF but stops short of showing how belief-based preferences affect language models or large policy classes used in practice (e.g., direct preference optimization or reward modeling + RL on LMs). The step from gridworld/CARLA to LLMs is nontrivial; without at least a small LM experiment, the generality of modeling human beliefs could be questionable."}, "questions": {"value": "1. Theorem 4.3 assumes noiseless preferences, tabular deterministic policies, and exact policy consistency with all preferences. How robust is the bound to realistic noise and approximation error in RLHF (e.g., function approximators, stochastic policies, non-deterministic preference datasets)? Could you provide an intuition or empirical simulation showing whether the derived bound approximately holds when these assumptions are relaxed?\n2. How should we interpret $\\pi_{belief}$ in practice? Is it meant to represent the human’s subjective prior over agent behavior or their estimate of the post-RLHF policy? Please clarify how you envision labelers forming this belief during annotation and how it might be approximated or measured in real settings.\n3. In the human study, you infer that priming alters participants’ beliefs about agent capabilities, but you do not measure those beliefs directly. Have you considered collecting explicit quantitative estimates before preference collection? This would provide stronger evidence that the observed preference shifts truly arise from belief changes rather than other framing effects."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SzO8kCBQlr", "forum": "IL3sMH7cNc", "replyto": "IL3sMH7cNc", "signatures": ["ICLR.cc/2026/Conference/Submission7926/Reviewer_LjtE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7926/Reviewer_LjtE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975614383, "cdate": 1761975614383, "tmdate": 1762919947954, "mdate": 1762919947954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a problem : human preferences are guided by beliefs over agent capabilities rather than only by the actions and observed outcomes. First, the authors verify this claim showing that if the annotators make incorrect assumptions about the policies, it leads to sub-optimal performance post RLF. The paper theoretically and experimental models the ideal belief to achieve maximum performance and then shows the impact of deviation from this ideal belief  on the expected return of the policy. Finally, a human study is conducted to provide stronger evidence for the claim that agent capability influences preference label;. Overall, the paper is very interesting to read, is well written and I strongly recommend accepting it."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces  a belief based model where the preferences over trajectories is proportional to the differences between advantages of the trajectories under the belief policy (while prior works assume an optimal policy). The anecdotal example is very intuitive and clearly presents the problem.\n- The authors present a theoretical lower bound on the policy performance as the annotators deviate from the optimal beliefs. This misalignment leads to sub-optimal performance as shown in the grid based experiments.\n- The problem of  annotator beliefs affecting preference labels is verified via a large human study, where the different groups are put under different beliefs about the driving agents capabilities and then are asked to rate behavior, clearly showing that if annotators have belief that the agent is risky, they prefer the conservative and safe behavior."}, "weaknesses": {"value": "- The paper lacks experiments at scale w.r.t to policy learning to demonstrate the effectiveness of the modelling agent belief capabilities. For eg. in the CARLA experiments, it would be interesting to show the return and qualitative performance of the agent when trained using preferences from different priming groups. This would show that the same policy under different beliefs converges to different performances (but a scalable experiment for the same).\n- It would be interesting to hear the authors views on how this could also be incorporated into population based annotations and how the beliefs across a population affect each other? This seems to be a stronger source of misalignment than function approximation and limited data as overparameterized neural networks and LLM alignment being a bandit problem don't suffer from it as much.\n- While the paper focuses on introducing the problem clearly, I request the authors to also include some insight towards how to build potential solutions towards handling the belief gap. This would make the paper even stronger and influential towards a wider audience."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i0PLIEaazz", "forum": "IL3sMH7cNc", "replyto": "IL3sMH7cNc", "signatures": ["ICLR.cc/2026/Conference/Submission7926/Reviewer_PBay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7926/Reviewer_PBay"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153590142, "cdate": 1762153590142, "tmdate": 1762919947383, "mdate": 1762919947383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}