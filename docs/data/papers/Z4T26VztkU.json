{"id": "Z4T26VztkU", "number": 8873, "cdate": 1758100704039, "mdate": 1759897757816, "content": {"title": "Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy", "abstract": "Time series anomaly detection (TSAD) is a critical task, but developing models that generalize to unseen data in a zero-shot manner remains a major challenge. Prevailing foundation models for TSAD predominantly rely on reconstruction-based objectives, which suffer from a fundamental objective mismatch: they struggle to identify subtle anomalies while often misinterpreting complex normal patterns, leading to high rates of false negatives and positives. To overcome these limitations, we introduce TimeRCD, a novel foundation model for TSAD built upon a new pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning to reconstruct inputs, TimeRCD is explicitly trained to identify anomalies by detecting significant discrepancies between adjacent time windows. This relational approach, implemented with a standard Transformer architecture, enables the model to capture contextual shifts indicative of anomalies that reconstruction-based methods often miss. To facilitate this paradigm, we develop a large-scale, diverse synthetic corpus with token-level anomaly labels, providing the rich supervisory signal necessary for effective pre-training. Extensive experiments demonstrate that TimeRCD significantly outperforms existing general-purpose and anomaly-specific foundation models in zero-shot TSAD across diverse datasets. Our results validate the superiority of the RCD paradigm and establish a new, effective path toward building robust and generalizable foundation models for time series anomaly detection. The code is available in https://anonymous.4open.science/r/TimeRCD-5BE1/", "tldr": "", "keywords": ["Anomaly Detection for Time Series", "Time-series Foundational Model", "Zero-shot", "Supervised Learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a3d7dc8e1bf75d7a175af4bf8e3444664a5b813.pdf", "supplementary_material": "/attachment/c642d95e2a59f584fc1706079cb5b45df58069d1.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new foundation model TimeRCD for zero-shot time series anomaly detection (TSAD) that overcomes the limitations of reconstruction-based approaches. Traditional TSAD models often fail to detect subtle or contextual anomalies and frequently misclassify complex normal patterns due to an objective mismatch between reconstruction and anomaly detection. To address this, the authors propose a new pre-training paradigm called Relative Context Discrepancy (RCD), which trains the model to identify anomalies by comparing discrepancies between adjacent time windows rather than reconstructing inputs. Built on a standard Transformer architecture, TimeRCD captures relational dependencies across time and uses a large-scale, fully labeled synthetic corpus that provides diverse anomaly patterns for robust pre-training. Extensive experiments across several datasets demonstrate that TimeRCD achieves state-of-the-art performance in zero-shot settings and remains competitive with fully supervised baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. TimeRCD demonstrates impressive performance in strict zero-shot settings, outperforming or matching specialized and fully supervised models on diverse datasets.\n\nS2. The paper designs a large-scale, fully labeled synthetic corpus that captures diverse and complex anomaly types, including point, contextual, collective, and cross-variate anomalies.\n\nS3. The experiments demonstrate that performance improves predictably with larger synthetic datasets, indicating scalable pre-training benefits similar to those observed in NLP and vision foundation models."}, "weaknesses": {"value": "W1. While the paper includes some ablation analysis (e.g., testing the effect of synthetic data), it lacks fine-grained ablations to isolate the contribution of individual components within the TimeRCD framework. For instance, the effect of auxiliary anomaly head and RCD window  is not examined in detail. Without such targeted ablations, it is difficult to clearly attribute which design choices most contribute to performance gains.\n\nW2. The paper does not provide sufficient efficiency or scalability analyses. Since TimeRCD relies on extremely long context windows (up to 13k timesteps), understanding its computational feasibility is crucial for real-world deployment. The absence of such experiments leaves open questions about whether the model can scale efficiently to resource-constrained or latency-sensitive environments.\n\nW3. Although the paper compares against several established baselines, it omits more recent or stronger full-shot models—such as modern deep contextual detectors (e.g., DCdetector[1] and TFMAE[2]). Including these would provide a clearer picture of where TimeRCD stands relative to the current state of the art in both zero-shot and fully supervised regimes.\n\n[1] DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection\n\n[2] Temporal-Frequency Masked Autoencoders for Time Series Anomaly Detection"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V3eh2PY5wx", "forum": "Z4T26VztkU", "replyto": "Z4T26VztkU", "signatures": ["ICLR.cc/2026/Conference/Submission8873/Reviewer_YVt4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8873/Reviewer_YVt4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558080989, "cdate": 1761558080989, "tmdate": 1762920635088, "mdate": 1762920635088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of zero-shot time series anomaly detection (TSAD). It argues that existing foundation models, which predominantly rely on reconstruction-based objectives, suffer from an \"objective mismatch\". This mismatch causes them to miss subtle anomalies (false negatives) and misinterpret complex normal patterns (false positives). \nThe paper introduces a new paradigm of training a transformer based “foundation model” (TimeRCD) explicitly on an anomaly detection objective, using a synthetic dataset generated through a novel multi-stage generation method.\nThe paper provides empirical validation of their method’s superiority and some analysis on the results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "Originality: Good\n\nQuality: Fair\n\nClarity: Fair \n\nSignificance: Good\n\nAdditional note: The paper shows clever implementation of techniques that are SOTA and well established in other domains. The synthetic data generation process is especially well thought out."}, "weaknesses": {"value": "Training of TimeRCD: TimeRCD is trained using a multi-objective setup (anomaly detection and reconstruction), the motivation for this setup is not provided, and it somewhat clashes with their claim that reconstruction is a poor objective for anomaly detection.\n\nContext window size: The paper states the benefit of having a large context window size, however it does not address the quadratic computation costs of the attention mechanism, or that transformers perform worse on sequences of lengths that it did not see at training time. There is some result analysis on context length provided but a deeper investigation should be done. I feel RQ2 was not answered fully.\n\nUnsubstantiated assertions: In section 1, the paper asserts that reconstruction models aim to capture dominant patterns and miss complex ones. No empirical analysis or previous work was cited to support these assertions.\nMissing appendix: Appendix not provided.\n\nExclusion of results: Table 1 shows results of the paper’s proposed approach against existing methodologies. Some results (which outperform TimeRCD) are excluded due to data leakage. Details of the data leakage is not provided.\nRepetition in text: Section 2 has repetition of what is already detailed in Section 1.\nMissing related work section, although relevant work was cited, it was not discussed in depth."}, "questions": {"value": "Over-all this paper can be a useful contribution to the community if some additional analysis and motivation of design choices are better provided. Appendix and related work section is not provided, the rating can be improved if clarifications are provided.\n\nPlease provide an appendix.\n\nPlease elaborate on the motivation for the reconstruction objective.\n\nFor the endogenous anomaly injection mechanism the generator creates distinct labels for the \"root-cause\" anomaly and its \"propagated effects\". Does the trained TimeRCD model learn to distinguish between these? Or does it simply flag the entire anomalous segment (root and propagation) as anomalous? \n\nPlease provide analysis on the performance of TimeRCD for sequences of lengths it did not see in training data, a more comprehensive description of the synthetic dataset would be good.\n\nProvide motivation to not finetune TimeRCD as another method for comparison. \n\nA clarification on how a multi-variate time series was flattened to a single sequence would be appreciated.\n\nCutting out repetitive text would make it a better read. Especially in section 1 and section 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qhii1TbJQe", "forum": "Z4T26VztkU", "replyto": "Z4T26VztkU", "signatures": ["ICLR.cc/2026/Conference/Submission8873/Reviewer_LPfi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8873/Reviewer_LPfi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723670945, "cdate": 1761723670945, "tmdate": 1762920634629, "mdate": 1762920634629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TimeRCD, a foundation model for zero-shot time series anomaly detection that addresses fundamental limitations of reconstruction-based approaches. The authors propose a novel pre-training paradigm called Relative Context Discrepancy (RCD), which explicitly trains the model to identify anomalies by detecting significant discrepancies between adjacent time windows rather than learning to reconstruct inputs. To enable this approach, they develop a large-scale synthetic data generation engine that creates diverse time series with token-level anomaly labels, including contextual and causal anomalies with cross-variate propagation. Using a standard Transformer architecture, TimeRCD achieves superior zero-shot performance across 14 benchmark datasets compared to existing general-purpose and anomaly-specific foundation models, demonstrating particular strength in detecting subtle contextual anomalies that reconstruction-based methods often miss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Great effort on constructing a huge dataset\n2. A lot of experiments"}, "weaknesses": {"value": "1. Limited novelty because (1) looking at adjacent time windows to detect anomaly is nothing new, (2) model is not special, and (3) equation-based data generation is always data-dependent (i.e. might work on dataset A but not on dataset B).\n2. None of the experiments include statistical significance tests. For example, they do not report the results of 10 runs with mean and standard deviation to test for significance.\n3. Performance not too great especially when compared to DADA\n4. Unclear explanation of how the model is trained using reconstruction error."}, "questions": {"value": "1. In Section 2.2, is Phase equivalent to Stage (in Figure 3)?\n2. How do you perform hyperparameter search, especially for window size, when there are no anomalies in the training set?\n3. Some font sizes in figures are too tiny\n4. Is the input masked or not? In \"Output Projection and Anomaly/Reconstruction Head\", the text mentioned \"masked portions of the input series\" but I don't see this mentioned in any other sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DRZWW1pUYm", "forum": "Z4T26VztkU", "replyto": "Z4T26VztkU", "signatures": ["ICLR.cc/2026/Conference/Submission8873/Reviewer_eExF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8873/Reviewer_eExF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943093608, "cdate": 1761943093608, "tmdate": 1762920634126, "mdate": 1762920634126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a foundation model for zero-shot time series anomaly detection by leveraging large-scale synthetic data pretraining and a proposed Relative Context Discrepancy methodology. The proposed TimeRCD model achieves top zero-shot results on several datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a structural and comprehensive framework for synthetic time series anomaly generation and detection.\n* It offers systematic experiments and well-designed ablation studies, which effectively demonstrate the contributions of the proposed components."}, "weaknesses": {"value": "* Incomplete evaluation on benchmarks and scalability analysis.\n* The methodology section lacks clarity, especially in defining key concepts such as RCD and the computation of anomaly scores.\n\nPlease find the detailed comments in the following section."}, "questions": {"value": "* The computation of the anomaly score from the anomaly head is insufficiently explained. Specifically, the definition and mathematical formulation of Relative Context Discrepancy (RCD) remain unclear. How is the discrepancy quantified and how does it integrate into the final detection objective?\n* In Section 2.2, the paper introduces two types of labels - localization and detection. It would be helpful to clarify the role of localization labels in model pretraining and downstream evaluation.\n* The authors emphasize the importance of capturing causal dependencies among channels, yet it is unclear how the proposed TimeRCD model models these dependencies. How does the framework evaluate multivariate interactions “in light of their causal dependencies”? More discussion would strengthen this claim.\n* The study only utilizes a subset of TSB-AD benchmark datasets rather than the full collection. What is the rationale for this selection? Including the full benchmark would provide a more comprehensive and standardized evaluation.\n* The omission of strong statistical baselines (such as Sub-PCA or KMeansAD) weakens the comparative analysis.\n* What are the context input window sizes used in Section 3.3, and how is the window length $W$ defined and selected as described in Section 2.1? Additionally, an analysis of the sensitivity of this critical hyperparameter would provide insight into the model’s robustness.\n* Including a distributional overview of time series lengths and the number of variates in the pretraining datasets would provide more transparency regarding dataset diversity. Moreover, the work would benefit from a discussion of how flattening multivariate time series affects performance and whether causal modeling across dimensions mitigates this limitation.\n* The paper lacks runtime efficiency results for TimeRCD compared with baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zwEecqX1ye", "forum": "Z4T26VztkU", "replyto": "Z4T26VztkU", "signatures": ["ICLR.cc/2026/Conference/Submission8873/Reviewer_ZQ5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8873/Reviewer_ZQ5M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8873/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970885251, "cdate": 1761970885251, "tmdate": 1762920633499, "mdate": 1762920633499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}