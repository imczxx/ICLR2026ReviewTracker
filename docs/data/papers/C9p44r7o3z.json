{"id": "C9p44r7o3z", "number": 2783, "cdate": 1757248189434, "mdate": 1759898127569, "content": {"title": "Adamas: Hadamard Sparse Attention for Efficient Long-context Inference", "abstract": "Large language models (LLMs) now support context windows of hundreds of thousands to millions of tokens, enabling applications such as long-document summarization, large-scale code synthesis, multi-document question answering and persistent multi-turn dialogue. However, such extended contexts exacerbate the quadratic cost of self-attention, leading to severe latency in autoregressive decoding. Existing sparse attention methods alleviate these costs but rely on heuristic patterns that struggle to recall critical key-value (KV) pairs for each query, resulting in accuracy degradation. We introduce **Adamas**, a lightweight yet highly accurate sparse attention mechanism designed for long-context inference. Adamas applies the Hadamard transform, bucketization and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-$k$ selections. Experiments show that Adamas matches the accuracy of full attention with only a 64-token budget, achieves near-lossless performance at $128$, and supports up to $8\\times$ higher sparsity than prior state-of-the-art (SOTA) methods while delivering up to $4.4\\times$ self-attention and $1.5\\times$ end-to-end speedups on 32K-length sequences. Remarkably, Adamas attains comparable or even lower perplexity than full attention, underscoring its effectiveness in maintaining accuracy under aggressive sparsity. Code is publicly available at https://anonymous.4open.science/r/Adamas-36EA.", "tldr": "We present Adamas, a lightweight yet accurate sparse attention mechanism, achieving up to 4.4× self-attention and 1.5× end-to-end speedups on 32K sequences with near-lossless accuracy.", "keywords": ["Sparse Attention", "Long-context Inference"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c6c17afa537684c3828b2753d5ff2617bf6f5de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces \"ADAMAS\", a lightweight yet highly accurate sparse attention mechanism with training-free. The key innovation is using Hadamard transform, bucketization and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-k selections. Compared with the existing train-free method, this method can match the accuracy of full attention and achieve up to 1.5× end-to-end speedups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Integrating some existing methods into an elegant solution can achieve better results in existing training-free methods.\n\n2. It is a good idea to use Hadamard transform in similarity estimation/sparse selection, make it to achieve better results within efficient calculation.\n\n3. The paper is clearly structured and well-written. The authors provide a thorough explanation of the research motivation, methodology, and experimental results."}, "weaknesses": {"value": "1. In the efficiency analysis, the kernel analysis can be more detailed. Except for full attention, there is no efficiency comparison with other sparse methods.\n\n2. Why it achieves similar results to full attention, in addition to experimental comparison, it would be better if some explanations were given from other aspects, such as case analysis."}, "questions": {"value": "1. I read the original Quest paper, and the PPL metric for PG19 is very close to full attention. Why does your paper show a discrepancy between the PPL and the original paper? Does the Quest method used here use full cache for the first two layers, as in the original paper?\n\n2. Regarding end-to-end decoding latency comparison, can other methods besides full attention be compared?\n\n3. Why use L1 instead of Dot Product to calculate distance? I guess the dot product is faster to compute."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZLrpLLz51n", "forum": "C9p44r7o3z", "replyto": "C9p44r7o3z", "signatures": ["ICLR.cc/2026/Conference/Submission2783/Reviewer_W5MJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2783/Reviewer_W5MJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036783412, "cdate": 1761036783412, "tmdate": 1762916374528, "mdate": 1762916374528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a sparse attention mechanism for long-context LLM inference that combines Hadamard transforms, 2-bit bucketization, and Manhattan distance estimation to select relevant key-value pairs. The authors claim up to 8× higher sparsity than prior methods while maintaining accuracy comparable to full attention. The paper reads more like an application of previously known methods to sparse attention rather than a fundamental innovation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method shows consistent improvements over baselines across multiple benchmarks  with good speedups.\n- The paper includes perplexity, accuracy, and efficiency metrics, plus ablation studies validating each component.\n- Custom CUDA kernels demonstrate real-world feasibility with actual latency measurements."}, "weaknesses": {"value": "- ( Contribution) The core idea of using Hadamard transforms to smooth distributions before quantization is borrowed directly from QuaRot. The novelty claim is therefore quite weak.\n- (Method) \"Bucketization\" looks a lot like standard quantization. \n- (Baselines) The comparison is limited to StreamingLLM, that is a basic sliding window method that performs poorly on retrieval tasks by design and Quest, that performs page-level selection with known coarse-grained limitations. A lot of Sparse Attention and KV Cache compression methods have been ignored. For example SnapKV, L2 Norm, MInference (mentioned but not compared) and DUOAttention (mentioned but not benchmarked).\n- In Equation 6 why does negative Manhattan distance on 2-bit quantized codes approximate dot products in the original space? The paper provides no theoretical or empirical justification for this.\n- There is no explanation concerning the choice for the bucketization thresholds. Also, there is no analysis about how sensitive is performance to different thresholds."}, "questions": {"value": "- In fig.4 it looks like perplexity with Adamas is lower than full attention, how do the authors explain this ? \n- Can you provide theoretical justification for why Manhattan distance on quantized codes approximates attention scores?\n- How were bucketization thresholds chosen? \n- What is the difference between sparse attention and kv cache compression methods ? How do they affect differently memory and latency ? The paper would benefit from this clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dcN6rNIavq", "forum": "C9p44r7o3z", "replyto": "C9p44r7o3z", "signatures": ["ICLR.cc/2026/Conference/Submission2783/Reviewer_jitG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2783/Reviewer_jitG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738610045, "cdate": 1761738610045, "tmdate": 1762916374323, "mdate": 1762916374323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adamas, a training‑free, token‑level sparse attention mechanism for long‑context LLM inference. The core idea is to apply an orthogonal Hadamard transform to queries and keys, then bucketize the transformed vectors into 2‑bit codes that are stored alongside the KV cache. During decoding, Adamas uses a lightweight Manhattan‑distance estimator on these 2‑bit codes to select top‑k candidates, followed by sparse attention on the reduced set (Figure 2, p.3; Algorithm 1, p.4). The method aims to preserve accuracy while enabling higher sparsity and end‑to‑end speedups. Empirically, the paper claims parity with full attention at small token budgets (near‑lossless at 128) and reports up to 4.4× self‑attention speedup and 1.5× end‑to‑end speedup on 32K sequences, sometimes with perplexity even lower than full attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The pipeline couples an orthogonal transform that smooths activations with low‑bit quantization, enabling efficient integer-only L1 screening before exact attention. The identity \\((QH)(KH)^\\top = QK^\\top\\) justifies operating in the Hadamard basis without loss.\n\n- Token‑level dynamic selection: Unlike Quest’s page‑level granularity, Adamas selects at the token level, which plausibly improves recall under tight budgets. The LongBench curves show the smallest gap to full attention at low budgets across GovReport, HotpotQA, MultifieldQA, NarrativeQA, Qasper, TriviaQA.\n\n- Custom CUDA kernels for fused bucketization+compression, Manhattan-distance estimation, top‑k, and sparse attention are provided; kernel breakdowns demonstrate speedups vs. FlashInfer baselines."}, "weaknesses": {"value": "- The paper does not explicitly describe how Adamas operates during the prefill stage or whether the proposed sparse attention strategy is applied there. If the sparse selection is only used during decoding while the prefill phase still relies on full dense attention, then evaluations on datasets like LongBench—whose inputs involve long prefill sequences—may not fully reflect the accuracy implications of the sparse mechanism. Clarifying whether Adamas affects both prefill and decoding phases (or only the latter) is essential for interpreting the reported efficiency and accuracy trade-offs.\n\n- While LongBench, PG19, and passkey retrieval are helpful, the paper omits widely used Needle‑in‑a‑Haystack stress tests that specifically probe long‑range retrieval under adversarial distractors. Adding these would directly support the “better recall under high sparsity” claim.\n\n- Dynamic/adaptive inference baselines are limited mainly to Quest; other competitive methods targeting inference‑time sparsity appear in related work but are not compared head‑to‑head in decoding regimes.\n\n- The efficiency study is on an RTX A6000. Results on A100/H100 would be informative for server‑side deployment and for understanding tensor‑core utilization, especially given the bit‑wise kernels.\n\n- The figure annotates PPL/Acc values but the exact setup is not fully specified in‑figure. A precise caption or footnote would aid interpretability."}, "questions": {"value": "- Figure 1 and complexity: Could you specify the dataset(s) and token budgets behind the PPL/Acc annotations? And in my opinion, both Adamas and Quest are \\(O(L)\\) like vanilla attention.\n\n- The most critical clarification is whether the proposed sparse attention mechanism is applied during both the prefill and decoding stages, or only during decoding. If Adamas is designed specifically for decoding and the reported speedups are measured only in that phase, while the benchmark datasets (e.g., LongBench) are dominated by prefill-heavy workloads, the results may not convincingly demonstrate end-to-end acceleration.\n\n- Additional detailed questions can be found in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VoRsHxYWn1", "forum": "C9p44r7o3z", "replyto": "C9p44r7o3z", "signatures": ["ICLR.cc/2026/Conference/Submission2783/Reviewer_M26R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2783/Reviewer_M26R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909057782, "cdate": 1761909057782, "tmdate": 1762916373987, "mdate": 1762916373987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}