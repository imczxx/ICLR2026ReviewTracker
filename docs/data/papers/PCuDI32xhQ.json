{"id": "PCuDI32xhQ", "number": 515, "cdate": 1756743394893, "mdate": 1759898256209, "content": {"title": "BézierFlow: Learning Bézier Stochastic Interpolant Schedulers for Few-Step Generation", "abstract": "We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2–3× performance improvement for sampling with $\\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.", "tldr": "", "keywords": ["Stochastic interpolants", "Bézier functions", "Diffusion models", "flow models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ba3e6f987a61a13ccb6c4c1dc0a73846c5fdfb0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes BézierFlow (BF), a lightweight, training-efficient way to learn a scheduler for Stochastic-Interpolant (SI) models so that few-step ODE sampling matches a strong many-step teacher. Schedulers $\\left(\\bar{\\alpha},\\bar{\\sigma}\\right)$ are parameterized as low-dimensional Bézier curves and optimized via learnable control points against the teacher trajectory. BézierFlow demonstrate high-quality few-step sampling ($\\approx$3–8 NFEs) with minimal tuning and no model retraining."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "– Writting is clear and easy to follow. \n\n– Using Bézier control points to model SI schedulers is simple and effective. \n\n– Competitive results vs. baselines at very low NFE."}, "weaknesses": {"value": "– Although the target problem differs, VDM [1] and Multi-marginal SI [2] also optimize schedules to improve performance. A brief discussion contrasting these with BézierFlow would clarify the advantages of using Bézier curves versus alternative parameterizations (and the trade-offs without Bézier).\n\n– I’m curious about the method’s limits. Recent distillation works [3,4] reduce sampling to a few or even one step while maintaining quality. I understand BF is lightweight and not the same setting, but probing the few-to-one NFE regime would better reveal the method’s capability.\n\n[1] Kingma et al., “Variational Diffusion Models”, NeurIPS 2021\n\n[2] Albergo et al., “Multimarginal generative modeling with stochastic interpolants”, NeurIPS 2024\n\n[3] Zhou et al., “Inductive moment matching”, ICML 2025\n\n[4] Kim et al., “Consistency trajectory models: Learning probability flow ode trajectory of diffusion”, ICLR 2024"}, "questions": {"value": "– How far can BF push NFE down (e.g., $\\le2$) before quality collapses?\n\n– Does a scheduler learned on dataset A transfer to B (or to different guidance scales/resolutions) without re-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UR1lpUHk3a", "forum": "PCuDI32xhQ", "replyto": "PCuDI32xhQ", "signatures": ["ICLR.cc/2026/Conference/Submission515/Reviewer_2baL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission515/Reviewer_2baL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892767564, "cdate": 1761892767564, "tmdate": 1762915536014, "mdate": 1762915536014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BézierFlow, a lightweight method to improve few-step generation for pretrained diffusion and flow models by learning the sampling trajectory—formulated as a stochastic interpolant (SI) scheduler—instead of only optimizing discrete ODE timesteps. The scheduler’s coefficient functions are parameterized as 1D Bézier curves whose control points enforce boundary conditions, differentiability, and (claimed) monotonic SNR. Training is a teacher-forcing alignment to a high-NFE teacher using a perceptual loss, and takes ~minutes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper moves beyond learned timesteps to continuous path learning via SI schedulers, which conceptually unifies diffusion and flow settings and widens the search space versus LD3.  Besides, it also provides analyses for endpoint-marginal preservation and schedule-invariance of the SI training objective. \n\n2. This method only needs a few parameters and minutes of training, no finetuning of the base model, and it supports plug-and-play at inference."}, "weaknesses": {"value": "1. Could the authors include more qualitative comparisons and broader evaluation metrics on SD3 models? Besides, for large-scale t2i model like SD3, except the FID values, other metrics, like CLIP are also important to evaluate the performance.\n\n2. Notice that training relies solely on LPIPS. Could this induce instability or mode collapse by using only this loss across most models training? \n\n3. The authors analyze briefly in Sec 4.4 the difference between BézierFlow and other schedulers, but this anlysis is not convincing to clarify its advantages over other schedulers. Could the authors provide more solid theoretical analysis?\n\n4. While “15 minutes” training time is appealing, results are only on CIFAR-10. Include runtime and memory analysis on larger datasets and models (e.g., ImageNet, SD3) to support the scalability claim."}, "questions": {"value": "see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6tQk75RbVb", "forum": "PCuDI32xhQ", "replyto": "PCuDI32xhQ", "signatures": ["ICLR.cc/2026/Conference/Submission515/Reviewer_TYFB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission515/Reviewer_TYFB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917655912, "cdate": 1761917655912, "tmdate": 1762915535727, "mdate": 1762915535727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new method for efficient sampling with generative models in the stochastic interpolants framework [Albergo et al. 2023], such as diffusion, score-based, and flow-based models. They optimize stochastic trajectories (i.e., the path from the latent space to the data distribution), parameterized by continuous functions $\\alpha(s)$ and $\\sigma(s)$, so that a scheduler requiring a small number of NFEs can be distilled from a more complex one in a teacher–student manner. The student scheduler parameters $\\bar{\\alpha}(s)$ and $\\bar{\\sigma}(s)$ are modeled using degree-$n$ 1-D Bézier curves with learnable control points. Bézier polynomials are chosen because they satisfy several constraints required by scheduler functions. The authors demonstrate effectiveness on multiple diffusion and flow-based models and datasets, achieving results that are better or comparable to the state of the art for low-NFE samplers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-written and easy to follow.\n* The use of Bézier polynomials to optimize schedulers for low-NFE samplers is interesting and well-motivated, and the authors conduct extensive experiments on multiple models within the stochastic interpolant framework.\n* In low-NFE regimes, the method achieves quality that is often superior to existing approaches while requiring minimal training cost."}, "weaknesses": {"value": "* In the experiments section, both the “Generalizability to Unseen NFEs” and “Training Efficiency” subsections would benefit from more detailed explanations. In particular, I found the first one somewhat unclear, while the comparison in the latter subsection is a bit confusing. I suggest providing additional training-time comparisons at equal NFE settings, including against non-distillation-based methods.\n* All experiments are done on image-based models, so the choice of the LPIPS loss is justified, but it limits the scope of the evaluation.\n* Minor: typo at line 299."}, "questions": {"value": "Beyond the additional details requested above, could the authors comment on potential alternatives to LPIPS when moving beyond vision tasks? Would their method retain the same advantages under different distance metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BRqBiAX5db", "forum": "PCuDI32xhQ", "replyto": "PCuDI32xhQ", "signatures": ["ICLR.cc/2026/Conference/Submission515/Reviewer_wWWP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission515/Reviewer_wWWP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959911717, "cdate": 1761959911717, "tmdate": 1762915535452, "mdate": 1762915535452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}