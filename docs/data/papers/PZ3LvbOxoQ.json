{"id": "PZ3LvbOxoQ", "number": 23321, "cdate": 1758342073980, "mdate": 1759896821146, "content": {"title": "RepQA: Evaluating Readability of Large Language Models in Patient Education Question Answering", "abstract": "Large Language Models (LLMs) have shown great promise in addressing complex medical and clinical problems. However, while most prior studies focus on improving accuracy and reasoning abilities, a significant bottleneck in developing effective healthcare agents lies in the readability of LLM-generated responses, specifically, their ability to answer patient education problems clearly to people without a medical background. In this work, we introduce RepQA, a benchmark designed to systematically evaluate the readability of LLMs in patient education question-answering (QA). RepQA comprises 533 expert-reviewed QA pairs sourced from 27 online resources, reflecting common concerns of lay users across 4 categories. RepQA incorporates a proxy multiple-choice QA task to directly assess the informativeness of generated responses, alongside two readability metrics. We present a comprehensive study of 25 LLMs on RepQA, evaluating both instruction-following (answering at requested reading levels) and readability understanding (recognizing the level of questions and texts). We find that current models fail to achieve target levels and struggle to identify appropriate reading levels, revealing a gap between raw reasoning ability and effective communication. We then compare four approaches for improving readability: standard prompting, chain-of-thought prompting, Group Relative Policy Optimization (GRPO), and a token-adapted GRPO. While readability-aware post-training substantially improves readability metrics, it often reduces QA accuracy due to over-simplification, exposing a significant readability–accuracy trade-off. These findings point toward methods for building more user-centered public health agents.", "tldr": "", "keywords": ["Readability", "Public Health", "Patient Education"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bcef6cba6801c71ae08a445e6affe47d0c8dcf7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces REPQA, a benchmark for measuring how well patient-education QA system adapt to readable language. It includes 533 expert-valudated QA pairs across 4 intent types such as Suggestions, Facts, Definitions and Rationales with a different target reading levels. The benchmark uses Flesch-Kincaid Grade Level and professional-term ratio to assess readability as well as provides about 36k unlabelled questions for post-training. The authors evaluate 25 LLMs and three tasks (grade-level instruction following, reading-level inference and readability-aware optimisation using GRPO and a proposed Token-Adapted GRPO). The results seem to indicate that LLMs overshoot the requested grade level; TA-GRPO improves readability but harms accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* A well focused benchmark for patient health literacy. REPQA address a real deployment need of producing patient-friendly answers at 6th to 8th grade levels (recommended by US health agencies). It seems to fill a gap between professional-exam benchamarks and consumer-question datasets (e.g. HealthSearchQA within MultiMedQA). \n\n* Practical proxy for answer quality. The propose-critic (LLM-as-judge) is a reasonable and low cost approximation to human eval of answer sufficiency, consistent with recent trends in automatic LLM evalution. \n\n* Broad empirical study across 25 models. The paper rather well documents a readability-accuracy trade-off across by testing 25 models and shows TA-GRPO achieves the biggest improvements in grade level and  jargon reductions. This is consistent with patterns in readability-controlled generation. \n\n* Useful insights for practitioners. The study highlights that instruction-following for readability is poorly calibrated but CoT improves answer quality without significantly hurting reading level, which seems to be also supported by evidence from prior work."}, "weaknesses": {"value": "* Lack of comparison to establish readability-control methods. Prior work showed strong readability control (instruction tuning, other RL methods and decoding strategies) and readability-controlled medical generation [1-4]. A direct comparison against the baselines woudl help to clarify further the contribution. \n\n* Limited readability measurement. The paper relies on FKGL, which can be \"gamed\" and may not capture true language difficulty. Patient-education research typically reports SMOG and Dale Chall [5,6], which are not reported in the current work.\n\n* Unvalidated LLM-as-judge setup. LLM judge may suffer from self-preference and positional biases. Without a human-scored calibration subset to estimate agreement and CI, changes attributed to TA-GRPO remain quite uncertain. \n\n* Scope and generalisability. The labelled test set seems somewhat limited (n=533), English-only and US-centric. This limits claims about multilingual or cross-population applicability.\n\n[1] Ribeiro, L. F. R., Bansal, M., & Dreyer, M. (2023). Generating summaries with controllable readability levels. EMNLP. https://aclanthology.org/2023.emnlp-main.714.pdf\n[2] Luo, Z., Xie, Q., & Ananiadou, S. (2022). Readability controllable biomedical document summarization.\nEMNLP. https://aclanthology.org/2022.findings-emnlp.343/\n[3] Wang, P., Chen, L., Zhu, D., Liu, Q., & Liu, L. (2024). Large Language Models are not Fair Evaluators. ACL. https://aclanthology.org/2024.acl-long.511.pdf\n[4] Ji, Y., Li, Z., Meng, R., Sivarajkumar, S., Wang, Y., Yu, Z., … He, D. (2024). RAG-RLRC-LaySum at\nBioLaySumm: Integrating retrieval-augmented generation and readability control for layman summarization of biomedical texts. BioNLP @ ACL. https://aclanthology.org/2024.bionlp-1.75/\n[5] Tanprasert, T., Kauchak, D., et al. (2021). Flesch-Kincaid is not a text simplification evaluation metric. GEM Workshop. https://aclanthology.org/2021.gem-1.1/\n[6] McLaughlin, G. H. (1969). SMOG grading—A new readability formula. Journal of Reading, 12(8), 639–646. Journal of Reading. https://ogg.osu.edu/media/documents/health_lit/WRRSMOG_Readability_Formula_G.Harry_McLaughlin__1969.pdf"}, "questions": {"value": "1. Could you please report results using additional readability metrics (previously mentioned, such as SMOG, Dale–Chall), or explain why these were not included? If this is possible, would you commit to including them in the final version of the paper to strengthen construct validity? \n\n2. LLM-based evaluation can display self-preference and positional bias, did you run any calibration study with human raters to validate the LLM-judge outputs? If not, could you include a small human-judged subset (agreement + CI) to verify accuracy trends under TA-GRPO?\n\n3. The dictionary-based approach may have limited recall, especially for multi-word medical terms. Could you clarify the coverage of your terminology list and whether multi-word medical terms are captured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PShS4gAVSH", "forum": "PZ3LvbOxoQ", "replyto": "PZ3LvbOxoQ", "signatures": ["ICLR.cc/2026/Conference/Submission23321/Reviewer_nYWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23321/Reviewer_nYWC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994955498, "cdate": 1761994955498, "tmdate": 1762942602876, "mdate": 1762942602876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a QA benchmark aimed at investigating, the readability of LLM-generated healthcare related questions. The questions are sourced from different benchmarks, categorised  filtered by what the paper refers to as experts, and evaluated with regards to readability metrics, (proxy) accuracy scores and the density of jargon. Results indicate that across the board, accuracy is high, jargon is ostensibly low, and readability as measured by the FK index is variable. The paper also proposes to explicitly incorporate the FK score as training feedback into the traiing process (unsurprisingly) improves the metric, but reduces the accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written, the experiments seem to be sound, the RQs are clear and the findings in general accessible and in line with the RQs."}, "weaknesses": {"value": "The magnitude of the contribution does not feel appropriate with the target conference: This is a benchmark that does not introduce new data or methods (bar RL post-training on a metric, but this is hardly novel), and the findings are not very profound either. I have doubts whether the selected readability indices indeed reflect human judgement on the readability of the texts. \nWhile the data is curated by \"experts\", the target - i.e. lay readers - are not considered in this paper.\n\nIncorporating human feedback and correlating model performance with human feedback & automated metrics would be more interesting - it would also give more insights whether training on automated, easy to obtain metrics (e.g. FK index) would improve the human perception of readability or vice versa."}, "questions": {"value": "Can you include human experiments into the empirical study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KusF8God06", "forum": "PZ3LvbOxoQ", "replyto": "PZ3LvbOxoQ", "signatures": ["ICLR.cc/2026/Conference/Submission23321/Reviewer_aBjM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23321/Reviewer_aBjM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762222247277, "cdate": 1762222247277, "tmdate": 1762942602618, "mdate": 1762942602618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces REPQA, a benchmark for evaluating the readability of LLM-generated responses in the patient education domain. The dataset consists of 533 expert-reviewed QA pairs drawn from 27 public health sources, covering common topics such as sleep, diet, mental health, and chronic diseases. The models are evaluated on two readability metrics (Flesch–Kincaid grade level and professional term ratio) and a proxy multiple-choice QA task for factual accuracy.\nThe authors also propose a token-adapted GRPO method which  improves readability metrics substantially but often at the cost of factual precision."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Relevant Research Focus: The paper tackles a crucial and under explored problem in healthcare AI. While prior research has primarily emphasized on factual accuracy and reasoning, this study shifts the focus to whether models can communicate complex medical information in ways that are understandable to patients without medical training. This direction is both socially impactful and highly relevant to real-world healthcare applications, where patient comprehension and informed decision-making depend on clear communication.\n2. Robust Evaluation: The paper's evaluation is further bolstered by evaluating LLMs on three different pillars - linguistic simplicity, jargon density, and factual informativeness.  This ensures that the models are not only producing “easier” texts but are also preserving accuracy, which is critical in healthcare, where over-simplification can be dangerous. The inclusion of both open-source and proprietary models provides a balanced landscape view of the field.\n3. The dataset is built and reviewed by professionals in nursing and public health, ensuring questions and answers reflect genuine patient information needs."}, "weaknesses": {"value": "1. The paper repeatedly claims that REPQA is designed for lay users or patients without medical backgrounds, yet the chosen readability targets (Flesch–Kincaid grade levels 6, 9, 12, and 15) are never grounded in actual measures of patient literacy or health-communication standards. There is no justification for why a target score of 6  represents the intended audience. Moreover, the analysis misses an opportunity to relate model behavior at different grade levels to real-world comprehension gaps — for instance, how responses at FK=6 vs. FK=9 affect clarity for lay readers. Without this linkage, the results remain abstract and disconnected from the paper’s stated goal of serving lay audiences.\n2. The paper reports extensive quantitative results (Tables 4–5) but fails to highlight key trends or outliers. Important comparative insights—such as which models best balance readability and accuracy—are buried in numbers without synthesis. A clearer narrative summarizing overall patterns is missing.\n3. In-depth diagnostics are conducted mainly on open-source models, leaving proprietary systems under explored. This limits the generality of insights and obscures whether observed behaviors (e.g., undershooting readability targets) generalize across architectures.\n4. The reinforcement signal optimizes only for readability, not correctness. As a result, the observed readability–accuracy trade-off is somewhat trivial, since accuracy was never part of the objective. This weakens claims about the inherent nature of the trade-off.\n5. Some reference answers include value judgments (e.g., “the Mediterranean diet is healthy”), raising concerns about bias and consistency in the dataset’s gold standards.\n6. Both Flesch–Kincaid and the professional-term ratio capture lexical simplicity but ignore semantic clarity and completeness. These metrics cannot detect whether essential qualifiers, side effects, or caveats were omitted—an especially serious issue in medical communication.\n7. All evaluations rely on automated metrics or LLM-based critics. Without human judgment—either from patients or clinicians—it is impossible to confirm whether lower-grade outputs are truly more comprehensible or trustworthy.\n8. Simplification often shortens answers, but the paper does not examine whether shorter responses omit critical details. Understanding this relationship is key to assessing whether improved readability compromises informativeness.\n9. Moreover, while the paper reports overall QA accuracy, it lacks a content-granular evaluation—for example, distinguishing whether simplified outputs omit minor supporting details or major medical qualifiers. Without such analysis, it is impossible to determine whether readability gains merely remove redundant wording or meaningfully distort information, limiting the benchmark’s utility for assessing safe patient communication."}, "questions": {"value": "1. Could you provide detailed statistics on inter-annotator agreement during the expert review process? What specific metrics were used, and how were disagreements resolved?\n2. How do automated readability metrics (Flesch-Kincaid) correlate with actual patient comprehension and satisfaction? Have you conducted human evaluation studies with lay users rating clarity and trustworthiness?\n3. To what extent does the multiple-choice QA task serve as an effective proxy for evaluating LLM-generated patient education content? Specifically, does the MCQA task cover the major topics present in the generated context, or are minor and related details overlooked? Human evaluation of coverage will be crucial to assess how well the MCQA task reflects the completeness and factual quality of the generated answers.\n4. How sensitive are your findings to the choice of readability metrics? Would results differ significantly with alternative measures like SMOG?\n5. How do you ensure that readability improvements don't compromise patient safety through oversimplification? Do simplified responses maintain critical medical disclaimers and safety warnings?\n6. How might readability needs vary across different demographic groups (age, education, health literacy) and medical contexts (emergency vs. routine care)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gCRaEN7dEa", "forum": "PZ3LvbOxoQ", "replyto": "PZ3LvbOxoQ", "signatures": ["ICLR.cc/2026/Conference/Submission23321/Reviewer_Df4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23321/Reviewer_Df4s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762808289993, "cdate": 1762808289993, "tmdate": 1762942602477, "mdate": 1762942602477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RepQA, a benchmark dataset specifically designed to evaluate the readability of outputs generated by large language models (LLMs) in patient education question-answering scenarios.\nUnlike previous work that focused solely on medical question-answering accuracy, REPQA emphasizes readability, i.e., whether LLMs can explain health issues to the general public using plain language."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Focusing on readability for patients, which addresses an important practical application dimension in medical LLM research.\n2. Employing expert-reviewed QA pairs covering broad health topics, explicitly curated for “non-expert readers”;\n3. Comprehensive evaluation, including many representative LLMs with many results.\n4. Discussion on the trade-off between accuracy and readability, with RL."}, "weaknesses": {"value": "1. It's confusing in Table 1 that the authors claim this benchmark as `Long Answer & Multiple Choice`, while in Table 2, the reader can only understand this as a multiple-choice QA benchmark.\n2. Relying solely on the Flesch-Kincaid readability and the proportion of medical terms fails to capture deeper characteristics such as semantic conciseness, structural coherence, or syntactic complexity. Employing a combination of LLMs and human evaluation may be a more suitable approach.\n3. Regarding RL, authors may not have found an optimal solution to address the trade-off between readability and accuracy. One reason for this is probably the previous point: is a reward model based on such a simple rule not so reliable?\n4. Would the questions be too simple? Since the numbers in Table 4 are quite good."}, "questions": {"value": "1. Consider using `large language model (LLM)`, not `Large Language Model`, like https://en.wikipedia.org/wiki/Large_language_model\n2. Consider mark important numbers in Table 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z71AhMmzCo", "forum": "PZ3LvbOxoQ", "replyto": "PZ3LvbOxoQ", "signatures": ["ICLR.cc/2026/Conference/Submission23321/Reviewer_5RWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23321/Reviewer_5RWL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762809775608, "cdate": 1762809775608, "tmdate": 1762942602200, "mdate": 1762942602200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}