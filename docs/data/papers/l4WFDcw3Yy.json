{"id": "l4WFDcw3Yy", "number": 21647, "cdate": 1758320058119, "mdate": 1759896910808, "content": {"title": "HALT: Hallucination Assessment via Log-probs as Time series", "abstract": "Hallucinations remain a major obstacle for large language models (LLMs), especially in safety-critical domains. We present HALT (Hallucination Assessment via Log-probs as Time series), a lightweight hallucination detector that leverages only the top-20 token log-probabilities from LLM generations as a time series. HALT uses a gated recurrent unit model combined with entropy-based features to learn model-specific calibration bias, providing an extremely efficient alternative to large encoders. Unlike white-box approaches, HALT does not require access to hidden states or attention maps, relying only on output log-probabilities. Unlike black-box approaches, it operates on log-probs rather than surface-form text, which enables stronger domain generalization and compatibility with proprietary LLMs without requiring access to internal weights. To benchmark performance, we introduce HUB (Hallucination detection Unified Benchmark), which consolidates prior datasets into ten capabilities covering both reasoning tasks (Algorithmic, Commonsense, Mathematical, Symbolic, Code Generation) and general-purpose skills (Chat, Data-to-Text, Question Answering, Summarization, World Knowledge). While being 30× smaller, HALT outperforms Lettuce, a fine-tuned modernBERT-base encoder, achieving a 60× speedup gain on HUB. Together, HALT and HUB establish an effective framework for hallucination detection across diverse LLM capabilities.", "tldr": "We use logprobs matrix [Seq_len, Top-k] of llm generation as time series data and pass it to GRU to detect whether the response has hallucinations or not.", "keywords": ["Hallucination Detection", "LLMS", "Time Series Classification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff38f129e29d415d382571f18251f465ad758616.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a hallucination detection method called HALT. The core idea is to treat the top-20 log probabilities of each token generated by an LLM as a time series and use a lightweight bidirectional GRU to learn model-specific calibration biases, thereby determining whether the output contains hallucinations. This method relies solely on token log probabilities and does not require access to the model’s internal states, attention mechanisms, or external retrieval, making it an efficient “black-box” detection approach.\n\nIn addition, the authors construct a unified hallucination detection benchmark called HUB, covering 10 LLM capabilities, including reasoning tasks (algorithmic, commonsense, mathematical, symbolic, and code generation) and general tasks (dialogue, data-to-text, question answering, summarization, and world knowledge), thereby extending the evaluation beyond the traditional focus on factual hallucinations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Modeling log-probability sequences as time series for classification offers a novel perspective.\n\nThe concept of model-specific calibration bias is learnable and effectively captured using a GRU.\n\nThe method relies solely on token log probabilities, making it suitable for black-box APIs."}, "weaknesses": {"value": "1. Strong dependence on log probabilities: The method assumes that the API provides top-20 log probabilities, but some commercial APIs may not provide them or may restrict access. It would be valuable to explore performance under more limited probability information (e.g., only top-1 or top-5).\n\n2. High model specificity and weak cross-model generalization: The paper shows that HALT-L and HALT-Q perform differently across models, indicating that the calibration bias is model-specific. It is worth investigating whether multi-model training or model-agnostic feature extraction can improve cross-model adaptability.\n\n3. Definition and evaluation of “logical hallucinations” can be further refined: Although HUB introduces reasoning tasks, the labeling and evaluation of logical hallucinations still rely on human annotation and GPT-4, which may introduce subjectivity. More analysis on annotation consistency or the incorporation of formal verification methods is needed.\n\n4. Insufficient comparison with the strongest baselines: While comparisons are made with models like Lettuce, direct comparisons with more advanced hallucination detection methods (e.g., SelfCheckGPT, INSIDE) are limited.\n\n5. Limited evaluation on large models:\nFew LLMs are included in the experiments, making it difficult to determine the method’s consistent effectiveness across different large-scale models."}, "questions": {"value": "1. Strong dependence on log probabilities: The method assumes that the API provides top-20 log probabilities, but some commercial APIs may not provide them or may restrict access. It would be valuable to explore performance under more limited probability information (e.g., only top-1 or top-5).\n\n2. High model specificity and weak cross-model generalization: The paper shows that HALT-L and HALT-Q perform differently across models, indicating that the calibration bias is model-specific. It is worth investigating whether multi-model training or model-agnostic feature extraction can improve cross-model adaptability.\n\n3. Definition and evaluation of “logical hallucinations” can be further refined: Although HUB introduces reasoning tasks, the labeling and evaluation of logical hallucinations still rely on human annotation and GPT-4, which may introduce subjectivity. More analysis on annotation consistency or the incorporation of formal verification methods is needed.\n\n4. Insufficient comparison with the strongest baselines: While comparisons are made with models like Lettuce, direct comparisons with more advanced hallucination detection methods (e.g., SelfCheckGPT, INSIDE) are limited.\n\n5. Limited evaluation on large models:\nFew LLMs are included in the experiments, making it difficult to determine the method’s consistent effectiveness across different large-scale models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Szj0ZmXA4H", "forum": "l4WFDcw3Yy", "replyto": "l4WFDcw3Yy", "signatures": ["ICLR.cc/2026/Conference/Submission21647/Reviewer_cYB5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21647/Reviewer_cYB5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760510467353, "cdate": 1760510467353, "tmdate": 1762941870239, "mdate": 1762941870239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HALT, a lightweight hallucination detector that treats token-level top-k log-probabilities as a time series and trains a small bidirectional GRU to predict sentence-level hallucination labels. The authors also present HUB, a comprehensive benchmark covering ten capability clusters. HALT relies solely on the token log-prob sequence instead of surface text, enabling black-box hallucination detection that is computationally efficient and model-agnostic."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach is practical and lightweight, relying only on token log-probabilities that are available in most APIs.\n2. The time-series perspective is conceptually new compared with previous aggregation-based or text-only black-box detectors.\n3. Experiments are comprehensive, covering ten capability clusters and multiple public benchmarks.\n4. HALT models are compact (around 5M parameters) but outperform larger span-based or encoder-based detectors in most clusters."}, "weaknesses": {"value": "1. Notation inconsistency: in lines 285–289, the notation for ($\\ell_t$) is inconsistent. Each term should represent a single log-probability value instead of a vector, and the paper should use one clear format throughout and include a short notation reference to help readers follow the equations.\n2. Insufficient details about HALT-L and HALT-Q training and evaluation: It would help to have one concise subsection that summarizes what data each model uses, how teacher-forcing is implemented, and how evaluation thresholds are chosen.\n3. Limited transparency of the HUB benchmark: providing metadata, annotation guidelines, and inter-annotator agreement scores would greatly improve reproducibility and community adoption."}, "questions": {"value": "1. HALT-L and HALT-Q perform poorly on the code-generation task, whereas H_alts achieves the best performance specifically on this task. Could the authors explain the reason?\n2. Could the authors clarify HALT’s black-box nature? The current description is a bit confusing. The abstract states that HALT is different from a black-box detector, but I believe HALT is still essentially a black-box detector.\n3. In Table 1, are the missing training entries intentional? The authors should clarify this to avoid reader confusion. For example, are these omissions designed to test the model’s generalization ability on other task types?\n4. Will the annotation guidelines and inter-annotator agreement scores for HUB be released to ensure transparency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ck9EWusNxF", "forum": "l4WFDcw3Yy", "replyto": "l4WFDcw3Yy", "signatures": ["ICLR.cc/2026/Conference/Submission21647/Reviewer_aA7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21647/Reviewer_aA7c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752324785, "cdate": 1761752324785, "tmdate": 1762941869862, "mdate": 1762941869862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HALT (Hallucination Assessment via Log-probs as Time series), which uses GRU model to detect hallucinations from a log-probabilities matrix of LLM generations. This paper also introduces HUB (Hallucination detection Unified Benchmark) which combines prior datasets that evaluate different capabilities."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed method is targeting log-probabilities to detect hallucinations, which is an interesting middle ground between the generated text and model internals."}, "weaknesses": {"value": "## Major\n\n- The experimental design is flawed\n    - The definition of hallucinations that the authors attempt to address is too broad. For instance, the authors define that incorrect reasoning traces as hallucinations. In other words, the authors define any incorrect outputs of models as hallucinations. This definition makes it difficult to properly isolate and understand the reason why these hallucinations happen and why the proposed method is the appropriate solution to it.\n    - In turn, the evaluation setup (HUB) becomes too broad and it is also unclear why the proposed consolidated prior datasets are strictly connected to hallucinations as opposed to other form of incorrectness. If the authors claim that any form of incorrectness as hallucinations, then why do they use these datasets instead of other commonly used datasets? Note that I am not suggesting to use common datasets such as MMLU or GPQA, but rather to highlight the lack of motivation in using these broad set of benchmarks.\n    - Because HUB is a consolidation of multiple datasets, the authors should explain the definition of hallucinations from each dataset and why the definitions are compatible with one another. Without this explanation, the readers cannot be certain that the hypotheses are truly evaluated by this benchmarking suite.\n- Design choices are poorly elaborated\n    - Why is the top-k fixed to 20?\n    - What is the T (sequence length)?\n    - How did the authors choose the selected features on top of the top-k log probabilities?\n    - Why GRU instead of other models?\n    - Why did the GRU trained only on either Llama 3.1-8B or Qwen 2.5-7B? Would it transfer to smaller or larger models?\n    - Missing ablation studies on the design choices?\n- Limited contributions\n    - HUB is a combination of existing datasets. The lack of motivating reason in combining these datasets further highlight the limited contribution of HUB.\n- Lack of baselines\n    - The authors only compare HALT against Lettuce: The central claim of the paper is that log-probabilities is correlated with hallucinations better than other properties of LLM generations. Thus, the natural comparison is against previously proposed methods based on surface-form text, external validators, or model internals.\n    - In several cases, the authors omitted the values because they were not reported in prior baselines. However, it is the authors burden of proof to evaluate these baselines under their settings.\n- The presentation quality is very lacking\n    - In general, the paper is not self-contained. It requires readers to read the references to understand what the authors proposed. This massively harms the readability of the paper.\n    - Table 2: Which model did the authors used to generate the top-k probabilities in this table?\n\n## Minor\n\n- Section 2 which talks about HUB is very difficult to read especially if the readers are not familiar with the datasets consolidated by the authors\n    - The authors should try to make the paper more self-contained by describing the dataset. For instance, the authors could give an example per task type.\n- Cite GRU [1]\n- Explain the five summary statistics explicitly.\n- L282: Please cite the “prior works” mentioned.\n- The citation style is incorrect. It should be either parenthetical citation or textual citation depending on the situation (at the moment they are all textual citation).\n\nReferences:\n\n- [1] On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"}, "questions": {"value": "- L51-58: You seem to present contradictory statements. You start with saying that token probability is not a sufficient evidence for truthfulness, however, you proceed with proposing a detection framework that is based on log-probabilities. Why would that assumption regarding the token probability be different in your case? Is it simply because of the evolving patterns? If so, ablation study on the sequence length of the input becomes very crucial.\n- L91-92 & L108-109: What do you mean by “incorporating reasoning-focused capabilities from CriticBench”? Do you somehow create new variants of the datasets to incorporate reasoning from CriticBench? I do not understand this claim.\n- Reasoning vs General-purpose: How do you define reasoning and general-purpose? Seemingly, the models can also reason to do the general-purpose evaluation, particularly for the question answering and world knowledge subsets.\n- L188-195: Why does response length correspond to linguistic diversity? Perhaps this is not intentional, but response length does not constitute linguistic diversity. If the authors would like to claim for linguistic diversity, they should analyze other syntactic and semantic properties (e.g., n-gram overlaps, languages, difference in reasoning structure, etc.)\n- L216: “top-k log probability vectors reflect the model’s confidence landscape”. This sounds quite redundant or not novel. Probability is indeed confidence, or perhaps I am misunderstanding this hypothesis?\n- L220: How do you define $y_t$ as correct? From my understanding, the correctness of an output in the benchmark cannot be trivially determined by a single token.\n- L220: $y_t$ is not further referenced in the text, did you mean $p_t$?\n- Hypothesis 1 and 3 sound exactly the same. Why are they different?\n- L268: How do you calculate the entropy of the distribution? Does this imply that you need the full distribution?\n- L316: “Overall entropy on the truncated” → the name seems to be truncated.\n- L380: Is the decision to use 0.5 as a threshold for Lettuce backed by the original implementation? If not, this may be an unfair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ulvy7CNp3s", "forum": "l4WFDcw3Yy", "replyto": "l4WFDcw3Yy", "signatures": ["ICLR.cc/2026/Conference/Submission21647/Reviewer_9LNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21647/Reviewer_9LNU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781682969, "cdate": 1761781682969, "tmdate": 1762941869525, "mdate": 1762941869525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HALT, a lightweight hallucination detector that treats the sequence of token-level log-probabilities from LLM outputs as a time-series signal. HALT models these dynamics using a bidirectional GRU, learning model-specific calibration biases that correlate with hallucinations. It operates in a strict black-box setting. To evaluate broadly, the authors introduce HUB, a unified benchmark that consolidates datasets such as HaluEval, RAGTruth, FAVA, and CriticBench into LLM capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Recasting log-prob sequences as a time-series classification problem is novel.\n2. It only requires token-level log-probs, making it lightweight. And the GRUs only has 5M parameters.\n3. The HUB unifies diverse datasets and tasks. It integrates logical hallucinations alongside factual ones."}, "weaknesses": {"value": "1. While HALT performs well, it’s unclear which temporal features (e.g., entropy spikes, rank shifts) most influence decisions. The method could benefit from feature attribution or from visualizing log-probability trajectories to explain predictions.\n2. HALT does not transfer well across models (in both Hypothesis 3 and results). This is a practical limitation if detectors must be retrained per LLM.\n3. The paper claims the method is attractive for API-based deployments. However, most APIs do not expose log probs.\n4. Baselines focus mainly on Lettuce and aggregate metrics. The paper misses comparisons to recent confidence-calibrated decoders, SelfCheckGPT variants, or retrieval-enhanced verifiers under similar constraints. Although they're mentioned in Appendix A.2, there are no results included."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0pwVg9hJjG", "forum": "l4WFDcw3Yy", "replyto": "l4WFDcw3Yy", "signatures": ["ICLR.cc/2026/Conference/Submission21647/Reviewer_VWuN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21647/Reviewer_VWuN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950746504, "cdate": 1761950746504, "tmdate": 1762941869186, "mdate": 1762941869186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}