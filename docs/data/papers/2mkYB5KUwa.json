{"id": "2mkYB5KUwa", "number": 19124, "cdate": 1758293739502, "mdate": 1759897058268, "content": {"title": "Rethinking reasoning with Masked Diffusion Models", "abstract": "Masked diffusion language models (MDLMs) are trained to infill positions in randomly masked sequences, in contrast to traditional next-token prediction (NTP) models. Discussions around MDLMs focus on two benefits: (1) multi-token decoding and 2) any-order decoding. However, we observe that for math and coding tasks, any-order algorithms often underperform or behave similarly to left-to-right sampling, and standard multi-token decoding significantly degrades performance. At inference time, MDLMs compute the conditional distribution of all masked positions. A natural question is: How can we justify this additional compute when left-to-right one-token-at-a-time decoding is on par with any-order decoding algorithms? These findings warrant rethinking how MDLMs are utilized. First, we propose multi-token entropy decoding (MED), a simple adaptive sampler that minimizes the error incurred by decoding positions in parallel based on the conditional entropies of those positions. MED preserves performance across benchmarks and leads to 3× fewer steps. Second, we propose a reasoning-as-infilling framework. By using MDLMs to infill a reasoning template, we can structure outputs and distinguish between reasoning and answer tokens. In turn, this enables measuring answer uncertainty during reasoning. This enables early exits when the model converges on an answer. Combined with MED, this leads to a 69% speed-up on GSM8K with a minimal (0.1%) effect on accuracy. Finally, given an answer, our framework enables sampling from the posterior over reasoning traces conditioned on the answer, even when the model is incorrect. On GSM8K, this enables generating correct reasoning traces for 43% of problems originally solved incorrectly. Our work demonstrates that the training objective and compute used by MDLMs unlock many new possibilities for inference and post-training methods.", "tldr": "", "keywords": ["Masked Diffusion Models", "Diffusion model", "Probabilistic Methods"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36da4d93ff29bbfe5ae19c8219ea8119e2fa0f7e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the parallel decoding behavior of MDLMs, revealing that even decoding two tokens per step leads to substantial performance degradation. To address this, the paper proposes an adaptive multi-token decoding algorithm, MED, which selectively decodes positions with low conditional entropy, thereby enabling controlled parallelism. Compared to entropy decoding, MED reduces the NFEs while maintaining model performance. Furthermore, the paper introduces reasoning-as-infilling, a framework that structures generation using a template with explicit reasoning and answer blocks. This approach enables the monitoring of answer uncertainty for early exits and facilitates the sampling of posterior reasoning traces given a known answer, which can be used for data generation and post-training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated by the analysis of MDLM decoding strategies. The authors first examine both any-order and parallel decoding, revealing that left-to-right sampling remains a strong baseline and that naive parallelization substantially degrades performance. These findings provide a clear and empirical justification for developing the proposed adaptive decoding method.\n\n2. The paper introduces the reasoning-as-infilling framework, a novel approach that structures generation by pre-filling a template with distinct reasoning and answer blocks. A key application of this framework is its ability to enable the sampling of posterior reasoning traces, which provides a more direct mechanism for data generation and model analysis compared to the conventional autoregressive models."}, "weaknesses": {"value": "1. The paper lacks a direct empirical comparison with other acceleration techniques for MDLMs [1, 2], making it difficult to assess the competitiveness of the proposed methods. For instance, the reported 69% speed-up on GSM8K appears relatively modest.\n\n2. Incomplete Empirical Evaluation. The paper's conclusions would be substantially strengthened by a more systematic evaluation. It is suggested to include results for MED on MATH and MBPP [3], and for the early-exit strategy on HumanEval and MBPP, to fully demonstrate the general applicability of the proposed methods.\n\n3. The discussion of limitations could be more focused. Instead of addressing the general drawbacks of MDLMs, it would be more helpful to examine the scope of applicability and potential failure cases of the paper’s own contributions: the MED algorithm and the reasoning-as-infilling framework. This would clarify the specific strengths and limitations of the proposed approach.\n\n4. The paper's clarity could be enhanced by rebalancing its structure. The analysis in Section 3 is extensive; condensing it would allow for a more thorough discussion of the core methods and experimental details, strengthening the paper's overall focus.\n\n[1] Wu et al., Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding.\n\n[2] Wei et al., Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles.\n\n[3] Austin et al., Program Synthesis with Large Language Models."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Xddv9vPnT", "forum": "2mkYB5KUwa", "replyto": "2mkYB5KUwa", "signatures": ["ICLR.cc/2026/Conference/Submission19124/Reviewer_s1QC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19124/Reviewer_s1QC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760873502432, "cdate": 1760873502432, "tmdate": 1762931144432, "mdate": 1762931144432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides two techniques to make use of the parallel decoding and any-order generation attribute of MDLMs, accelerating inference: 1. predict as many tokens as possible if the entropy of their predicted distribution is low enough; 2. Set a predetermined think then answer template, the inference can exit early as soon as the entropy on the answer tokens are low enough."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The idea of predetermine a thinking template is reasonable. It bears similarity with <think> template in AR model, but in MDLMs we can control the thinking length.\n2. The proposed method cleverly leverages the unique property of MDLMs—access to all masked token distributions—to enable adaptive parallel decoding. The theoretical grounding in KL divergence bounds adds rigor to the approach.\n3. The paper provides thorough experiments across multiple benchmarks (GSM8K, Math500, HumanEval), demonstrating consistent speedups (up to 3.3×) with minimal accuracy loss. The post-hoc reasoning analysis is particularly insightful for model improvement."}, "weaknesses": {"value": "1. The idea of fast decoding in MDLMs are already discussed in many papers. For example, Fast DLLM also decode multiple tokens according to their confidence. However, the comparisons with existing methods are ignored in this paper.\n2. The template leads to human-define the length of the reasoning and answer part. I believe that for GSM8K and HumanEval, at least the answer part should be of different length. How did the authors set these hyper-parameters?\n3. In table 3, it seems that we did not try accelerating a lot: the most aggressive hyper-parameter we've tried on GSM8K is still more than 0.5x steps, suggesting relatively conservative acceleration. For accelerating method, we would expect greater speedup potential."}, "questions": {"value": "- How is Post-hoc reasoning evolved in the reasoning process actually? Does the post-hot reasoning lead to a second-time answer, and we actually evaluate with this second-time answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fARcmLX9gO", "forum": "2mkYB5KUwa", "replyto": "2mkYB5KUwa", "signatures": ["ICLR.cc/2026/Conference/Submission19124/Reviewer_28Gt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19124/Reviewer_28Gt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872139908, "cdate": 1761872139908, "tmdate": 1762931143899, "mdate": 1762931143899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reexamines MDLMs and focuses on two widely claimed advantages: multi-token decoding and any-order decoding. The authors propose Multi-token Entropy Decoding (MED), an adaptive sampler that achieves 2–3× faster inference without accuracy loss, and a Reasoning-as-Infilling framework that enables structured reasoning, early exits, and posterior sampling of reasoning traces. Extensive experiments validate the effectiveness of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Multi-token Entropy Decoding (MED) is theoretically grounded and empirically validated. Its entropy-based criterion provides a principled way to control decoding errors, achieving 2–3× faster inference without accuracy loss.\n\n\n2. The Reasoning-as-Infilling framework reveals a novel and previously unexplored property of MDLMs: generating correct reasoning traces for 43% of problems originally solved incorrectly."}, "weaknesses": {"value": "The paper lacks sufficient discussion and comparison with closely related works. Specifically, [1] introduces a confidence-threshold-based sampling approach, and [2] proposes an entropy-bounded unmasking procedure. Both methods are conceptually similar to the proposed Multi-token Entropy Decoding (MED) and also provide theoretical analyses. Unfortunately, the authors do not discuss these works in depth or include empirical comparisons with them.\n\n[1] Wu et al. Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding. arXiv 2025.05.\n\n[2] Ben-Hamu et al. Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking. arXiv 2025.05."}, "questions": {"value": "1. Could the proposed Early Exit mechanism terminate decoding prematurely—i.e., when the final answer has been generated but the reasoning chain is still incomplete? \n\n2. The Reasoning-as-Infilling framework shows that MDLMs can generate correct reasoning traces for problems originally solved incorrectly. What insights does this provide for improving MDLM training or architecture design in the future?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "29EKxr8Sjx", "forum": "2mkYB5KUwa", "replyto": "2mkYB5KUwa", "signatures": ["ICLR.cc/2026/Conference/Submission19124/Reviewer_wz9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19124/Reviewer_wz9d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920505897, "cdate": 1761920505897, "tmdate": 1762931143414, "mdate": 1762931143414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates various inference aspects and their benefits for masked diffusion models. The authors first examine two commonly discussed advantages: any-order sampling and multi-token prediction. They find that any-order sampling offers limited gains, while multi-token prediction degrades performance. To address this, the authors propose a Multi-token Entropy Decoding (MED) strategy, which dynamically determines the number of tokens to decode at each inference step, resulting in an inference-time speedup. They further demonstrate additional efficiency improvements through early exiting based on the model’s uncertainty in its answers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method MED with early exist seems to provide good inference efficiency gains on coding and reasoning tasks."}, "weaknesses": {"value": "- Several claims in the paper are not sufficiently supported by experimental evidence and appear somewhat overstated.\n     - For instance, to support the claim that “MDLM reasoning posterior yields high-quality traces,” the authors only provide correctness scores judged by Qwen and GPT. However, correctness alone does not offer a complete view of trace quality. It would also be more informative to include correctness scores for a baseline model, such as LLaMA 3.1 (or another model with comparable capabilities), to better contextualize the benefits of MDLM’s reasoning quality relative to other autoregressive models.\n     - The claim that left-to-right sampling performs on par with any-order sampling also seems somewhat overstated and comes with a few caveats. Except for the Dream 7B model on the HumanEval task, any-order sampling with certain block sizes consistently outperforms left-to-right sampling. It is also worth noting that the Dream 7B model was adapted from an autoregressive model, which naturally introduces a left-to-right inductive bias. Additionally, tasks like GSM8K are known to rely on predominantly left-to-right reasoning patterns (see [1]). The true advantage of any-order sampling is more likely to appear in tasks requiring non-linear reasoning (e.g., Sudoku, though admittedly more synthetic in nature).\n\n- It would also be more informative for readers if all comparisons were presented in terms of Number of Function Evaluations (NFEs) rather than the number of parallel tokens decoded (as in Tables 2 and 3). The number of parallel tokens directly reduces NFEs by a proportional factor, which results in a major change, whereas reducing the number of inference steps in diffusion impacts NFEs without such drastic changes. For instance, it is unclear how the entropy decoding method operates with 96 or 112 NFEs in Table 3.\n\n- Finally, the paper should include a more comprehensive comparison with the EB sampler and generate_until sampler from Ben-Hamu et al. I would also like to note that the Ben-Hamu et al. paper appeared on arXiv at the end of May, and according to ICLR’s submission guidelines, papers released after July 1 are considered concurrent work.\n\n[1] Premise Order Matters in Reasoning with Large Language Models. Chen et al. 2024"}, "questions": {"value": "- It would be interesting to understand the performance left-to-right decoding and any order decoding with different block sizes work on R-GSM8k proposed in [1]\n- Why is HumanEval on Llada with left-to-right accuracy 15.24 in Table 1 and 11.0% in Table 7? Typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "79JQOkFwnI", "forum": "2mkYB5KUwa", "replyto": "2mkYB5KUwa", "signatures": ["ICLR.cc/2026/Conference/Submission19124/Reviewer_Buvt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19124/Reviewer_Buvt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925170397, "cdate": 1761925170397, "tmdate": 1762931142981, "mdate": 1762931142981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}