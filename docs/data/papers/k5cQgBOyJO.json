{"id": "k5cQgBOyJO", "number": 6597, "cdate": 1757990113515, "mdate": 1759897906117, "content": {"title": "GUI-PRA: Process Reward Agent for GUI Tasks", "abstract": "Graphical User Interface (GUI) Agents powered by Multimodal Large Language Models (MLLMs) show significant potential for automating tasks. However, they often struggle with long-horizon tasks, leading to frequent failures. Process Reward Models (PRMs) are a promising solution, as they can guide these agents with crucial process signals during inference. Nevertheless, their application to the GUI domain presents unique challenges. When processing dense artificial inputs with long history data, PRMs suffer from a \"lost in the middle\" phenomenon, where the overwhelming historical context compromises the evaluation of the current step. Furthermore, standard PRMs lacks GUI changing awareness, providing static evaluations that are disconnected from the dynamic consequences of actions, a critical mismatch with the inherently dynamic nature of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process Reward Agent for GUI Tasks), a judge agent designed to better provide process reward than standard PRM by intelligently processing historical context and actively perceiving UI state changes. Specifically, to directly combat the ``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism consisting of two core components: a Relevance-based Retrieval Module to actively fetch pertinent information from long histories and a Progressive Summarization Module to dynamically condense growing interaction data, ensuring the model focuses on relevant context. Moreover, to address the lack of UI changing awareness, we introduce an Aadaptive UI Perception mechanism. This mechanism enables the agent to reason about UI state changes and dynamically select the most appropriate tool to gather grounded visual evidence, ensuring its evaluation is always informed by the current UI context. To validate the practical utility of our approach, we conduct experiments on two online benchmarks for GUI task. Our best results demonstrate an average success rate improvement of 14.53% across the two benchmarks, a significant outperformance of the 8.56% gain achieved by the standard PRM baseline.", "tldr": "We introduce GUI-PRA, a training-free judge agent that resolves key failure modes in GUI task supervision, through the lens of dynamic memory and adaptive UI perception.", "keywords": ["GUI Agent; Judge Agent; MLLM; Task Automation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9391fb890175f5f3d1b327b894b3f5ca43ca99b6.pdf", "supplementary_material": "/attachment/243974fbc05e3c8c63d2a13432ca9066248b2f63.zip"}, "replies": [{"content": {"summary": {"value": "The authors want to improve the GUI agent in a training-free process, thus proposed GUI-PRA, a multi-step process that first uses a judge agent that guides the reward better, and deploys a visual verification module for \"perceive-reson-verify\" on UI actions, and finally create a selection mechanism to enhance the results. The study evaluated primarily on 2 VLM agent models and 2 benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The tasks of interest is valuable and the combinations of techniques and sensible results make this a worthy study."}, "weaknesses": {"value": "1. Citation format is not consistent\n2. Plots are too small almost impossible to read in print. I urge the authors to revise to a legible quality.\n3. Using a 3rd-party judge, or \"dynamic memory mechanism\" that is a relevance-based module, is widely used already. I don't recall the exact reference, but I was aware of those a long time ago. \n4. The so-called \"lost in the middle\" or maybe \"needle in the haystack\" is a long-standing problem. I wonder if the author's methods can tackle LLM too, not just \"agents.\"\n5. I am thinking of an extended literature review that bridges what was done in LLM/VLM in a similar context, and then agents may be needed.\n6. The experiments lack some ablations on model sizes, and if resources allowed, some closed-source models as well, since it does not require training.\n7. Consider citing below as references to the adaptive visual component (sec3.2) since the motivation looks similar\n[1] Hu, Yushi, et al. \"Visual program distillation: Distilling tools and programmatic reasoning into vision-language models.\" CVPR 2024\n[2] Chiu, et al. \"AIDE: Agentically Improve Visual Language Model with Domain Experts\" ICLR 202"}, "questions": {"value": "1. the so called \"lost in the middle\" or maybe \"needle in the haystack\" is a long standing problem, I wonder if the authors method can tackle LLM too, not just \"agents.\"?\n2. Usually those models has multiple sizes, how does the authors' methods work for other sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WXLaXBMvhE", "forum": "k5cQgBOyJO", "replyto": "k5cQgBOyJO", "signatures": ["ICLR.cc/2026/Conference/Submission6597/Reviewer_N483"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6597/Reviewer_N483"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428344112, "cdate": 1761428344112, "tmdate": 1762918923614, "mdate": 1762918923614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GUI‚ÄëPRA (Process Reward Agent for GUI Tasks), a training‚Äëfree framework designed to enhance GUI agents through improved process supervision. It introduces two key components‚ÄîDynamic Memory, which mitigates the ‚Äúlost in the middle‚Äù issue by condensing long interaction histories, and Adaptive UI Perception, which enables awareness of visual state changes. Experiments on AndroidWorld and Mobile‚ÄëMiniWoB++ benchmarks show an average success rate improvement of 14.53%, outperforming the standard PRM baseline (8.56%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a comprehensive GUI agent framework that integrates multiple components, demonstrating a high level of innovation and thoughtful system design.\n- It evaluates the proposed approach using two different model series, which effectively supports the generalization and robustness of the method.\n- The overall presentation is clear and well‚Äëstructured, with logically coherent arguments and fluent writing that make the work easy to follow."}, "weaknesses": {"value": "- The paper lacks quantitative validation or explanation of its core motivation. While it repeatedly mentions the ‚Äúlost in the middle‚Äù problem, there is no clear evidence showing how severe this issue is, how much it affects performance, or to what extent the proposed method actually mitigates it. A more detailed analysis or empirical verification would strengthen the motivation.\n- The experimental evaluation is limited ‚Äî only two benchmarks are used. This limited scope makes it difficult to fully assess the proposed method‚Äôs superiority. Including more baselines or benchmarks would provide a stronger evidence.\n- Some figures such as Figure 3 contain text that is too small to read clearly, which negatively affects the readability and overall understanding of the paper."}, "questions": {"value": "- In Table‚ÄØ1, there are some anomalous results. For example, in AndroidWorld under the Mixture‚Äëmodel setting, applying GUI‚ÄëPRA (W/‚ÄØGUI‚ÄëPRA‚ÄëQ) shows no improvement over PRM (W/‚ÄØPRM‚ÄëQ); in Mobile‚ÄëMiniWoB++, the PRM method (W/‚ÄØPRM‚ÄëI) even performs slightly worse than the baseline. How do the authors explain these inconsistencies?\n- Line‚ÄØ397 states that ‚ÄúGUI‚ÄëPRA‚Äôs most significant advantages emerge on tasks of medium difficulty.‚Äù Why does the proposed approach yield substantial improvement only at the medium‚Äëdifficulty level? What are the essential differences among the difficulty categories?\n- The ablation study is conducted only on the Qwen‚ÄëVL series. Can the same component‚Äëwise conclusions be generalized to the InternVL or Mixture‚Äëmodel settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rdFReumQVo", "forum": "k5cQgBOyJO", "replyto": "k5cQgBOyJO", "signatures": ["ICLR.cc/2026/Conference/Submission6597/Reviewer_p6sm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6597/Reviewer_p6sm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876697700, "cdate": 1761876697700, "tmdate": 1762918923164, "mdate": 1762918923164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper address a key limitation identified in prior work, that is long-horizon reasoning for GUI agents due to context length constraints. The topic is timely and of clear interest to the community, as there are many different cuncurrent work. The authors ground their formulation in the ‚ÄúLost in the Middle‚Äù, and propose GUI-PRA, which extends an existing framework Process Reward Agents (PRAs) for GUI tasks through two main components:\n\n1) Summarizes earlier interaction history within a ReAct-style chain and appends it to later steps as a compact context signal for the reward critique model. \n2) Enables the model to choose between two perception tools: one for coarse and one for fine-grained visual understanding during evaluation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper tackles a well-motivated problem: the long-horizon performance degradation of GUI agents.\n\n2) The discussion of ‚Äúlost in the middle‚Äù and dynamic memory aligns well with known LLM context limitations, making the contribution conceptually coherent.\n\n3) The approach is conceptually clean, adapting the PRM framework to GUI-specific challenges of long context and visual grounding.\n\n4) The method is evaluated on two online benchmarks (AndroidWorld and MobileMiniWoB++)"}, "weaknesses": {"value": "1.\tLimited Novelty in Core Ideas\nThe proposed summarization module overlaps substantially with mechanisms already introduced in recent GUI-agent literature. For example, GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation (Liu et al.) also introduces a dedicated ‚Äúhistory summarization‚Äù subtask. Thus, the novelty lies more in applying such summarization within a PRM context rather than in the idea itself. Outside of GUI there are lot of papers that do the sumarization to handle long context. Summarise earlier interaction chains (ReAct style) to fetch relevant history on PRA for GUI is very incremental in terms of novelty. \n\n2. Allowing the model to select between ‚Äúcoarse‚Äù and ‚Äúfine‚Äù visual tools resembles prior multi-granularity perception designs (e.g., Read Anywhere Pointed, Attention-Driven GUI Grounding). The contribution appears to be a selection mechanism over pre-existing tools, rather than a fundamentally new perception module. Again this means the conceptual jump is small. Unless I missed a key point, I think the proposed pipeline overall lacks novelty.\n\n3. The benchmarking is limited to two datasets (AndroidWorld and MobileMiniWoB++). Several popular and relevant long-horizon benchmarks‚Äîsuch as WebArena, Mind2Web, OS-World, and WorkArena‚Äîare omitted, despite being directly adaptable to the proposed setting.\nThe ablations are also not exhaustive, and many strong baselines and concurrent works are missing from comparison (e.g., Aria-UI, Autonomous Evaluation and Refinement of Digital Agents, GUI-LLM). Even excluding the arxiv ones, there is prior work like 'Autonomous Evaluation and Refinement of Digital Agents' to be adapted to and compared with proposed method. They study different evaluators and improve the performance of existing agents via fine-tuning and inference-time guidance. \nNot a reason for lower rating but the quantitative improvements reported (e.g., Table 2) are relatively small and not always consistent across datasets or settings. The results appear as early evidence rather than conclusive validation.\n\n\n4. The related work section omits several closely related studies, including Lost in the Middle itself, which is central to the paper‚Äôs formulation. Broader citation coverage of process supervision and GUI perception literature would strengthen the work‚Äôs positioning: \"Lost in the Middle: How Language Models Use Long Contexts\"."}, "questions": {"value": "1.\tCan you include more benchmarks such as Mind2Web and WebArena, which explicitly involve long-horizon web or GUI tasks?\n2.\tCan the authors report the distribution of task lengths (e.g., number of actions per episode) and how the proposed method performs as task length increases?\n3.\tA plot of performance gain versus task length would clarify the practical utility of the proposed dynamic memory mechanism.\n4.\tDoes it make sense to include an ablation for each part with related work like discussing GUI-LLM (CVPR 2024), wfor multimodal perception for GUI reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hbdgt9fJvJ", "forum": "k5cQgBOyJO", "replyto": "k5cQgBOyJO", "signatures": ["ICLR.cc/2026/Conference/Submission6597/Reviewer_jDFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6597/Reviewer_jDFA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963899556, "cdate": 1761963899556, "tmdate": 1762918922755, "mdate": 1762918922755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a training-free process reward adjudicator (GUI-PRA) for mobile GUI agents. At each step it builds a compact memory of recent interactions and queries on-screen perception tools, then re-scores multiple candidate thought-action options and executes the best one. On AndroidWorld and Mobile-MiniWoB++, this test-time controller boosts end-to-end task success over both vanilla agents and a standard PRM baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Practical and model-agnostic (drop-in, no finetuning). Combines dynamic memory with explicit UI perception to curb long-horizon drift and brittle text-only judging. Shows consistent gains across two benchmarks and different backbones, with ablations that clarify where improvements come from. Orthogonal to underlying planners/PRMs, so it can stack with stronger agents and search methods."}, "weaknesses": {"value": "In ablations, removing Point (local UI targeting) drops performance to below standard PRM (‚àí0.44%), indicating the overall gains hinge on the reliability of OmniParser/Point and the routing policy‚Äôs correctness.\n\n\nBoth standard PRM and GUI-PRA do an argmax over k candidate thought‚Äìaction pairs per step. If the base agent doesn‚Äôt propose the correct action, re-scoring cannot recover it.\n\n\nError propagation via ‚Äútemporal consistency.‚Äù The scorer feeds the previously selected action and its score into the current step to enforce consistency. An early misjudgment can bias the remainder of the trajectory.\n\n\nThe paper claims to assess both ‚Äúeffectiveness and efficiency,‚Äù but only reports SR/DSR, no latency, token usage, or tool-call counts, making cost‚Äìbenefit hard to judge.\n\nResults are limited to mobile (AndroidWorld and Mobile-MiniWoB++). Desktop OS scenarios, multi-app workflows, cross-app long-horizon tasks, dynamic window management, aren‚Äôt covered, so generalization to OSWorld/desktop remains unproven."}, "questions": {"value": "See Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0IifY0JRKH", "forum": "k5cQgBOyJO", "replyto": "k5cQgBOyJO", "signatures": ["ICLR.cc/2026/Conference/Submission6597/Reviewer_M8uW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6597/Reviewer_M8uW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045355716, "cdate": 1762045355716, "tmdate": 1762918922368, "mdate": 1762918922368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GUI-PRA (Process Reward Agent for GUI Tasks), a novel training-free framework designed to enhance the performance of Process Reward Models (PRMs) in graphical user interface (GUI) automation. The paper highlights that standard PRMs suffer from a ‚Äúlost in the middle‚Äù phenomenon when handling long task histories and lack awareness of UI state changes. \n\nTo address these limitations, GUI-PRA incorporates a Dynamic Memory Mechanism to condense historical trajectories and an Adaptive UI Perception Mechanism to actively gather visual evidence, thereby providing more accurate supervisory signals. Experimental results demonstrate that GUI-PRA achieves substantially higher success rates than both the baseline and standard PRM-guided models across two online benchmarks, validating its effectiveness in improving the reliability and efficiency of complex GUI task execution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Introduces a new training-free supervisory agent paradigm that can generalize to unseen GUI tasks.\n\n(2) Empirical improvement (~14.5%) is meaningful for GUI automation, where success rates are typically low."}, "weaknesses": {"value": "(1)\t Limitations of Cross-Family Generalization and Lack of Explanation. Although the experimental setup includes an evaluation of cross-family generalization‚Äîwhere the InternVL3 Agent is guided by the Qwen2.5-VL Supervisor‚Äîthe results indicate that GUI-PRA fails to deliver any additional improvement over the standard PRM under this mixed-model configuration. However, the authors provide no explanation for this outcome. Moreover, this finding appears to contradict the paper‚Äôs first claim in the introduction that ‚ÄúGUI-PRA, a novel agent that surpasses standard PRMs for GUI tasks.‚Äù\n\n(2)\tOne of the core contributions of the paper is the Dynamic Memory Mechanism. However, its implementation details raise concerns about rigor and transparency. The definition of ‚Äúrelevance‚Äù is unclear: the paper describes the retrieval stage as a ‚ÄúRelevance-based Retrieval‚Äù process intended to actively fetch pertinent information. Yet, in the formal description, the retrieval function ùëì retrieve f retrieve is simply defined as retaining the most recent m steps. How does this ensure that the selected window of m steps is truly relevant to the task goal?"}, "questions": {"value": "Please see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IbYB4SBdcm", "forum": "k5cQgBOyJO", "replyto": "k5cQgBOyJO", "signatures": ["ICLR.cc/2026/Conference/Submission6597/Reviewer_qcKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6597/Reviewer_qcKY"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152259295, "cdate": 1762152259295, "tmdate": 1762918921860, "mdate": 1762918921860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}