{"id": "SPE9gJfhB9", "number": 5725, "cdate": 1757929553763, "mdate": 1759897957983, "content": {"title": "Egocentric Video Understanding through Latent Action Representations", "abstract": "We study action understanding task in egocentric videos, a task crucial for intelligent systems interacting with dynamic environments, such as assistive robots and augmented reality interfaces. This task requires capturing fine-grained, temporally localized interactions, which we call the action dynamics. Existing approaches often struggle to jointly model the interplay between object appearance and motion cues, limiting their ability to anticipate future actions. To address this, we propose LAF (Latent Action Fusion), a multi-modal Transformer-based framework for egocentric action anticipation and recognition. Our method extracts compact and interpretable latent action tokens from sequential video frames using a latent action model, constructed by VQ-VAE  paradigm and action-conditioned frame reconstruction method for action dynamic measuring. Generated latent action tokens then fuse these tokens with embeddings from pretrained vision encoders and object detectors. The resulting multi-modal representation encodes object, interaction, spatial, and temporal information, enabling modeling of complex temporal dynamics and improving verb-level reasoning. Experiments on large-scale egocentric video datasets demonstrate that LAF shows the usefulness in action recognition and significantly enhances action anticipation (Top-5 mAP: N 24.11 → 31.02; N–V 10.62 → 14.34), highlighting the benefits of integrating latent action representations with multi-modal embeddings for precise verb aspect understanding.", "tldr": "A novel framework that integrates latent action represen ations, object detection, and vision embedding fusion to enhance egocentric action tasks, especially in verb aspect.", "keywords": ["latent action representation", "action prediction", "action understanding", "video understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43b264eb803eb1fc8a6cd64d5cc6e7f9cc17843e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses action understanding in egocentric videos, which is challenging due to the need to capture fine-grained, temporally localized interactions, referred to as action dynamics. Existing methods might not effectively combine object appearance and motion cues, limiting their predictive capabilities. To overcome this, the authors propose LAF (Latent Action Fusion), a multi-modal Transformer-based framework for both action anticipation and recognition. LAF extracts latent action tokens from sequential video frames using a VQ-VAE-based latent action model with an action-conditioned frame reconstruction approach. These tokens are then fused with embeddings from pretrained vision encoders and object detectors, producing a multi-modal representation that captures object, interaction, spatial, and temporal information. Experiments show that LAF improves action recognition and boosts action anticipation performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "There exist novel aspects for the task at hand. For example: (a) Using VQ-VAE to discretize temporal dynamics into latent action tokens is not commonly seen in egocentric action recognition or anticipation tasks. Most prior work focuses on continuous feature embeddings rather than discrete latent codes capturing fine-grained frame-to-frame changes. (b) The idea of conditioning future frame prediction on the current frame embedding and its latent action code explicitly enforces that latent codes encode actionable dynamics, which goes beyond vanilla VQ-VAE or reconstruction losses. (c) Using pairwise frame differences processed via attention pooling to capture salient variations is a different way to focus on motion-relevant changes rather than static background features."}, "weaknesses": {"value": "1) The system depends on YOLOv9, CLIP, and a pretrained LAM, which may limit novelty in terms of base features. Performance may heavily rely on the quality of these pretrained models, potentially reducing generalization to other datasets. Such issues are not discussed in detail. For example, there is no explicit mention of occlusions, multiple interacting objects, or noisy detection scenarios, which are common in egocentric videos. The method assumes that YOLO can reliably detect the \"next active object'' which may not hold in cluttered scenes.\n\n2) LAM encodes only 8 sequential frames, so longer temporal dependencies (e.g., extended actions) may not be captured. This may limit anticipation for actions that unfold over longer timescales. Such a choice may be appropriate for the datasets used but might not generalize to real-world scenarios as mentioned in the introduction.\n\n3) As far as I understand, the method requires YOLO fine-tuning, LAM pretraining, and then joint training. This multi-stage training is computationally expensive and may be difficult to reproduce.\n\n4) The final score \n\\[\ns_{i,j} = \\sigma_i \\times p_{\\text{obj},i} \\times p_{\\text{int},i,j}\n\\] \nis hand-crafted and not learned end-to-end. Can you confirm?\nIt could fail if confidence scores are miscalibrated or if object detection is inaccurate. This issue should be discussed in detail.\n\n5) While the latent action model helps capture motion, verbs that require long-range temporal context may still be difficult to predict accurately. How to handle this is not clear from the text.\n\n6) The implementation details are in detail, but providing them is not the same as making the code publicly available, which is not mentioned in the paper. Will the authors make it available? Otherwise, the reproducibility of this work is questionable.\n\n7) Remaining gaps in the ablation study include exploring alternative fusion strategies, temporal window effects, full multi-task evaluation, robustness, and efficiency. Specifically:\n- Only full fusion vs. single-module removal is tested. Alternative ways of combining embeddings (concatenation, weighted sum, attention fusion) and the position of latent action embeddings in the Transformer input are not explored.\n- No ablation of temporal window size or frame sampling in LAM, which could affect the quality of motion cues.\n- The current table only shows Top-5 mAP. Time-to-contact regression performance is unclear, as are error correlations between nouns, verbs, and TTC. Such details are important to understand the limitations and trade-offs; currently, the results are just numbers and difficult to interpret. In this line, more discussions should be included.\n\n8) While the proposed pipeline demonstrates strong performance, it is relatively heavy compared to prior work, requiring multiple pretrained modules and staged training rather than a fully end-to-end approach. This raises concerns regarding \"computational cost (both in train and test)\", efficiency, practical deployment, and even fairness of the comparisons. In other words, it is not entirely clear whether the reported improvements are solely due to the proposed architecture. The method benefits from multiple large pretrained modules, staged training, and task-specific hyperparameter tuning, whereas some baselines do not use similar resources. Could you please explain these?"}, "questions": {"value": "Please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wA0l3BFjaA", "forum": "SPE9gJfhB9", "replyto": "SPE9gJfhB9", "signatures": ["ICLR.cc/2026/Conference/Submission5725/Reviewer_h1rF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5725/Reviewer_h1rF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741079070, "cdate": 1761741079070, "tmdate": 1762918221203, "mdate": 1762918221203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of fine-grained temporal dynamics modeling in egocentric video understanding, particularly for verb-oriented reasoning in action anticipation and recognition tasks. This paper identifies a limitation in existing Vision-Language-Action (VLA) models and other approaches: their struggle to jointly and effectively model the interplay between object appearance and motion cues, which is essential for understanding how actions unfold over time. To bridge this gap, the paper proposes LAF, a novel multi-modal Transformer-based framework. The core innovation is the introduction of a Latent Action Model (LAM) that explicitly models action dynamics by discretizing frame-to-frame changes into interpretable latent codes. The contributions are as follows:\n1. Latent Action Model (LAM): A novel VQ-VAE-based module that generates discrete, interpretable latent action tokens. \n2. Unified Multi-modal Fusion Framework (LAF): The first framework to seamlessly integrate latent action representations with object-level and scene-level visual features for egocentric action understanding. \n3. Comprehensive Empirical Validation: Extensive experiments on major benchmarks (Ego4D for anticipation, EPIC-KITCHENS-100 for recognition) demonstrate state-of-the-art or highly competitive performance, particularly in verb prediction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core originality lies in the novel formulation of using a discrete latent action space to model fine-grained, frame-to-frame dynamics for verb-centric reasoning explicitly. While VQ-VAEs and multi-modal fusion are established techniques, their creative combination here is novel. \nThe technical quality of the work is high. The LAM and LAF frameworks are carefully designed, incorporating robust components such as pre-trained DINOv2 and CLIP. The experimental evaluation is exceptionally thorough. \nThe work is significant for both practical and conceptual reasons. Practically, it delivers a state-of-the-art method for verb-oriented action anticipation on Ego4D and a competitive, more efficient model for recognition on EK100."}, "weaknesses": {"value": "A significant omission is a detailed analysis of the computational cost and efficiency of the proposed LAF framework. The paper correctly notes that large VLMs are computationally expensive and positions LAF as an effective alternative. However, it provides no concrete data on parameters, FLOPs, or inference time for LAF compared to the baselines (e.g., AVT, GANOv2, EgoVideo). Without this, it is impossible to assess the true efficiency gain or the trade-off between performance and cost. This is crucial for evaluating the practical significance of the method.\nOn the EK100 recognition task, LAF's performance, while competitive, is below the state-of-the-art (SOTA). The authors attribute this to having \"substantially fewer parameters,\" which is a plausible explanation. However, this point warrants a deeper discussion. \nThe LAF framework's multi-stage, multi-component architecture, while effective, risks being perceived as a brute-force aggregation of existing tools, potentially diminishing its methodological novelty and elegance despite its empirical strength."}, "questions": {"value": "Question on Computational Cost: As raised in the weaknesses, could you provide the computational metrics (parameters, FLOPs) for the full LAF pipeline and its main components (LAM, fusion transformers)? This information is essential for a complete assessment.\n\nQuestion on the result of action recognition: Regarding the performance on the action recognition task, the noun accuracy is notably lower than the state-of-the-art. Could you please clarify if this is a conscious trade-off resulting from the framework's heightened focus on modeling action dynamics (verbs), potentially at the expense of object-centric (noun) appearance modeling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vf5epQf4Yi", "forum": "SPE9gJfhB9", "replyto": "SPE9gJfhB9", "signatures": ["ICLR.cc/2026/Conference/Submission5725/Reviewer_ocMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5725/Reviewer_ocMf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791691897, "cdate": 1761791691897, "tmdate": 1762918219637, "mdate": 1762918219637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Latent Action Fusion (LAF), a multi-modal Transformer framework for egocentric video action recognition and anticipation. The architecture is based on VQ-VAE to discretize action dynamics into latent action tokens. These tokens are further fused with object-detector features and visual embeddings to model temporal and semantic cues jointly. Results on Ego4D demonstrate high accuracy on both Noun and Noun-Verb metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates that verb prediction in egocentric videos depends on modeling temporal dynamics rather than static content, implementing this by reconstructing motion from frame-feature differences.\n\nThe fusion of latent tokens with CLIP and YOLO embeddings is cleanly formulated through two Transformer encoders, one for noun/object and one for verb/TTC reasoning."}, "weaknesses": {"value": "On EPIC-Kitchens-100, the method underperforms prior works by 15–20% Top-1 overall accuracy. The authors argue for efficiency, but quantitative efficiency metrics (i.e., FLOPs, latency) are missing.\n\nLAM aggregates differences over short windows (e.g., 8 frames), which may fail to capture long-term dependencies. There is no experiment varying the window length or demonstrating the compositionality of latent actions.\n\nThe author put interpretability as one of the major contributions; however, no direct evidence supports this claim, Relying solely on a t-SNE visualization is insufficient to demonstrate that the model is interpretable.\n\nSome claims lack supporting evaluation, for example “a generative basis for reasoning over actions” and “EK100 recognition task  ... underscoring the model’s generalizability”. Moreover, the paper does not explain how the recognition task improves anticipation performance."}, "questions": {"value": "How does performance depend on the size of the codebook?\n\nUsing only 8 frames could be insufficient to capture the action dynamics. How was this 8-frame window determined?\n\nWhy does uniform sampling slightly reduce Noun-Verb mAP (Table 5) but improve Noun-TTC? Could this imply overfitting to last-frame motion bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SUdLoOMTFJ", "forum": "SPE9gJfhB9", "replyto": "SPE9gJfhB9", "signatures": ["ICLR.cc/2026/Conference/Submission5725/Reviewer_k6xw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5725/Reviewer_k6xw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929567818, "cdate": 1761929567818, "tmdate": 1762918218950, "mdate": 1762918218950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multi-modal framework for egocentric action anticipation and recognition. The framework consists of two main parts: a Latent Action Model (LAM) and a fusion module. The LAM is based on a VQ-VAE paradigm and is designed to learn discrete, interpretable latent action tokens. It does this by processing sequential frames with a visual encoder (DINOv2) and then quantizing the difference between consecutive frame embeddings. These latent tokens capture the action dynamics. The fusion module then combines these latent action tokens with features from a YOLO-based object detector and a CLIP-based visual encoder. A dual-stream Transformer architecture is used to process these fused features where one stream predicts the action noun, while the other predicts the action verb and the time-to-contact (TTC). The method is evaluated on the Ego4D short-term object interaction anticipation task and the EK100 action recognition task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of disentangling the prediction of appearance features (nouns) from temporal features (verbs, TTC) is well-motivated for the task of egocentric action understanding.\n2. The ablation study presented in Table 3 shows that the discrete code-book bottleneck is a critical component of the Latent Action Model and without this model there is a severe degradation in performance."}, "weaknesses": {"value": "1. The model's results on the Ego4D anticipation task (Table 1) contradict the idea of using the embedding difference. The latent action tokens are specifically designed to model action dynamics. However, the LAF model underperforms the baseline TransFusion on the Noun-Time-to-contact metric and is only comparable on the Overall (A) metric. The low performance on the temporal metrics seems to indicate that the latent tokens are not capturing better temporal information than the baseline. \n2. Some discussion is required to contrast the design choice from the prior work, Latent Action Pretraining (LAPA)[1], which also trains an action quantizer with VQ-VAE to obtain discrete actions from videos. The authors are encouraged to highlight the novelty and contributions when compared with the prior work LAPA.\n\n[1]. Ye, Seonghyeon, et al. \"Latent action pretraining from videos.\" arXiv preprint arXiv:2410.11758 (2024)."}, "questions": {"value": "The central assumption of the LAM is that $\\Delta {e}_t$​ isolates action from camera motion. What evidence supports this? How does the model differentiate between $\\Delta {e}_t$​ ​ caused by the user turning their head (large camera motion, no action) and $\\Delta {e}_t$​  caused by an object interaction (small camera motion, fine-grained action)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qZ9wLhTNJN", "forum": "SPE9gJfhB9", "replyto": "SPE9gJfhB9", "signatures": ["ICLR.cc/2026/Conference/Submission5725/Reviewer_rCoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5725/Reviewer_rCoU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954066623, "cdate": 1761954066623, "tmdate": 1762918217974, "mdate": 1762918217974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}