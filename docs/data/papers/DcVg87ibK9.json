{"id": "DcVg87ibK9", "number": 3223, "cdate": 1757380767894, "mdate": 1763722810878, "content": {"title": "Does FLUX Already Know How to Perform Physically Plausible Image Composition?", "abstract": "Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Artifact-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.", "tldr": "", "keywords": ["Image Editing", "Image Composition", "Diffusion Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc831d0c53041c0e24831f294b2c13bea31628f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SHINE is a training-free framework designed for seamless, high-fidelity insertion with neutralized errors. It introduces the manifold-steered anchor loss, which leverages pretrained customization adapters to guide latent representations, ensuring accurate subject representation while maintaining the integrity of the background. Additionally, SHINE incorporates degradation-suppression guidance and adaptive background blending to prevent low-quality results and visible seams, enhancing the overall output quality.\nIn response to the lack of rigorous benchmarks in this domain, the authors propose ComplexCompo, a benchmark suite that tests the framework under challenging conditions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The article presents a clear expression of its ideas, with a thorough visualization analysis of the motivation and proposed modules.\n\n2. The experiments are comprehensive, and the results appear highly promising.\n\n3. The work makes notable breakthroughs in addressing issues related to lighting realism and resolution rigidity in image synthesis."}, "weaknesses": {"value": "1. As shown in Table 1, the method performs well in Subject Identity Consistency, but there is room for improvement in Background Retention. Could you provide further insights and possible approaches for enhancing this aspect?\n\n2. In the Image Quality evaluation, the models used for assessment are somewhat limited. Could you consider incorporating more diverse and widely-adopted image quality evaluation models, such as UnifiedReward [1] and HPSv3 [2], to provide a more comprehensive analysis?\n\n[1] Unified Reward Model for Multimodal Understanding and Generation. 2025\n\n[2] HPSv3: Towards Wide-Spectrum Human Preference Score. 2025"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ln7cSZfGGM", "forum": "DcVg87ibK9", "replyto": "DcVg87ibK9", "signatures": ["ICLR.cc/2026/Conference/Submission3223/Reviewer_Yk7s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3223/Reviewer_Yk7s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203384259, "cdate": 1761203384259, "tmdate": 1762916608273, "mdate": 1762916608273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SHINE, a training-free strategy for image composition using FLUX. Since previous inversion-based and attention manipulation–based approaches struggle to achieve robust image composition, the authors introduce Non-Inversion Latent Preparation, Manifold-Steered Anchor (MSA) Loss, Degradation-Suppression Guidance (DSG), and Adaptive Background Blending (ABB). FLUX.1-dev is a CFG-distilled model, which causes high inversion errors. The subject image is captioned by a VLM, and an inpainting model creates an initial image based on the caption. Building on this, the authors perform a one-step forward diffusion to obtain and manipulate the latent. The Manifold-Steered Anchor (MSA) Loss utilizes the velocity predicted by the original noisy latent as anchor noise, along with the adapter-guided velocity prediction using the subject image’s latent. Degradation-Suppression Guidance (DSG) is inspired by negative prompting and constructs a negative velocity prediction by blurring the attention component $Q_{img}$ of FLUX. Adaptive Background Blending (ABB) selects between the attention mask and a user-provided mask depending on the timestep. Experiments demonstrate the effectiveness of the proposed framework for image composition, both quantitatively and qualitatively. The authors also propose ComplexCompo, a benchmark for challenging composition scenarios."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The design of the Manifold-Steered Anchor Loss, which takes into account the flow-matching process between FLUX.1-dev and the adapter model (InstantCharacter), is well-motivated and technically sound.\n2. The in-depth analysis of the attention blocks in FLUX.1-dev (Sec. C & D) and the derivation of the subsequent improvement, Degradation-Suppression Guidance, are particularly interesting.\n3. The Adaptive Background Blending method is simple yet powerful, and it appears broadly applicable to various image editing tasks.\n4. The paper offers numerous insights drawn from a comprehensive set of dense experiments.\n5. The proposed methods are model- and adapter-agnostic, making them broadly applicable across different architectures and integration settings."}, "weaknesses": {"value": "Overall, the paper is still a strong and well-executed, however, it could be further improved if the authors provide additional details and clarifications on certain points regarding the following aspects:\n\n1. In L159-161, the term “inpainted background image” appears somewhat inappropriate in this context. Phrases such as “scene image” or “image to which the subject is attached” would be more accurate and better convey the intended meaning.\n\n2. In L186-187, the authors state that they abandon the inversion process and instead apply one-step forward diffusion. I interpret this as implying that “the effect of inversion can be approximated by directly adding noise corresponding to a one specific timestep”. If this interpretation is correct, a theoretical justification should be provided. Additionally, the authors should clarify how this approach differs from the inversion-skipping technique used in EEdit [1].\n\n3. While numerical improvements are observed in Config A and C of the ablation study, it remains unclear whether DSG truly acts as a negative guidance vector when blurring the attention component $Q_{img}$ derived from the text embedding $c$ extracted by the VLM. Although FLUX is a CFG-distilled model where non-sensical or explicit negative prompts are often ineffective (as shown in Fig. 4(a)), this behavior may differ in other MM-DiT–based models such as SD3.5. It would strengthen the paper to include an analysis comparing simple negative prompting and Degradation-Suppression Guidance in SD3.5 to better quantify their differences.\n\n4. Please specify which inpainting model (e.g., Brushedit, BrushNet, PowerPaint, Flux.1 fill-dev) and which VLM (e.g., BLIP-3, InternVL, LLaVA-1.5) were used in the proposed frame work and evaluation.\n\n5. In Alg. 1, the origin of $z^{bg}$ is unclear. Is it defined as $z^{bg} = (1 - M^{user}) \\odot z^{init}$ and $z^{subject} =  M^{user} \\odot z^{init}$? Please clarify this in the text or Alg. 1.\n\n6. In Tab. 1 & 3, it would be beneficial to include results for Config A (the input image generated by the inpainting model). This addition would help readers better understand the performance gap and the impact of subsequent modules at a glance.\n\n[1] EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient Image Editing, ICCV 2025"}, "questions": {"value": "1. In L260-262, how are the attention components systematically perturbed? Which 2D Gaussian filter $G$ (kernel size) is used?\n2. How much time and GPU memory are required to generate a composited image using FLUX.1-dev + Adapter and + LoRA? Also, for the other baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wQqaPIXlJ2", "forum": "DcVg87ibK9", "replyto": "DcVg87ibK9", "signatures": ["ICLR.cc/2026/Conference/Submission3223/Reviewer_5xsu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3223/Reviewer_5xsu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900295483, "cdate": 1761900295483, "tmdate": 1762916607963, "mdate": 1762916607963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of physically plausible image composition, particularly the failure of existing models to handle complex lighting, shadows, and reflections. It propose SHINE, a \"training-free\" framework that is effectively a test-time optimization process. \nThe framework consists of three main components: \n1) Manifold-Steered Anchor (MSA) Loss, a guidance loss that uses a pretrained adapter (e.g., IP-Adapter) and the base model's predictions as an anchor to steer the latent variable; \n2) Degradation-Suppression Guidance (DSG), a form of negative guidance achieved by blurring specific query features ($Q_{img}$) inside the DiT to avoid artifacts; \n3) Adaptive Background Blending (ABB), a technique that uses cross-attention maps in early sampling steps to create smoother blends. \n\nAdditionally, the authors contribute a new benchmark for evaluating composition in complex lighting conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Good Qualitative Results: The qualitative results presented (e.g., Fig. 1, 6) are impressive, demonstrating a high degree of physical realism in challenging scenarios.\n2.  Benchmark Contribution: The newly proposed ComplexCompo dataset is a solid contribution, offering a valuable tool for future research."}, "weaknesses": {"value": "1.  Impractical Computational Cost: The \"training-free\" claim is misleading, as the method is a test-time optimization with *extreme computational costs*. Algorithm 1 and Table 5 indicate that it requires $k=10$ optimization iterations *per* denoising step $t$. For a 20-step process, this amounts to ~*200* forward and backward passes. On a 12B parameter model like FLUX, this cost is computationally prohibitive and makes any comparison against single-forward-pass baselines impractical.\n2.  Brittle Pre-processing Pipeline: To avoid image inversion, the paper introduces a (Sec 3.1) pre-processing pipeline that depends on *two* external models: a VLM for captioning and an inpainting model for creating $x^{init}$.  If the VLM caption is wrong (e.g., wrong color), the entire composition fails (as indicated in Sec 5-Limitation). It seems that this merely trades one failure point (inversion) for two new, uncontrolled failure points (VLM failure, inpainting failure)?\n3.  Limited Novelty and Model-Specific:\n    * MSA Loss: Re-naming a known technique (SDS) applied between an adapter and a base model does not constitute a significant novel contribution.\n    * DSG: This is an empirical, model-specific hack (via brute-force experimentation) for the FLUX architecture, it seems to lack a theoretical justification and guarantee of generalizability."}, "questions": {"value": "1.  Compute Cost Quantification: Could you provide a fair, wall-clock time comparison on identical hardware, e.g., how much time does SHINE (with $k=10$) take to generate one 1024x1024 image, versus the time required by baselines like AnyDoor or UniCombine?\n2.  DSG Generalizability: You claim in Appendix E that DSG applies to SDXL, SD3.5, and PixArt. Was the \"blur $Q_{img}$\" heuristic applicable *out-of-the-box* to all these architectures, or did each one require a *new* round of brute-force experimentation to find a *different* hack (e.g., \"blur $K_{txt}$\")?\n3.  Pre-processing Failure Rate: I am curious about the actual failure rate of your [VLM + Inpainting pre-processing pipeline], when running the 300-sample ComplexCompo benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4Y6kjwNRTb", "forum": "DcVg87ibK9", "replyto": "DcVg87ibK9", "signatures": ["ICLR.cc/2026/Conference/Submission3223/Reviewer_6v7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3223/Reviewer_6v7A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989491178, "cdate": 1761989491178, "tmdate": 1762916607779, "mdate": 1762916607779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}