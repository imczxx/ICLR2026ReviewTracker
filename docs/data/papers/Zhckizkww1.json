{"id": "Zhckizkww1", "number": 749, "cdate": 1756816676456, "mdate": 1759898243852, "content": {"title": "DragFlow: Unleashing DiT Priors with Region-Based Supervision for Drag Editing", "abstract": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX’s rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.", "tldr": "", "keywords": ["Image Editing", "Drag Editing", "Diffusion Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a0536acba1ff3a7ae3cc27378e87c699d15c04b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a region-based framework for drag-based image editing using Diffusion Transformers (DiTs) instead of UNet-based diffusion models. The authors argue that point-based supervision fails on DiTs due to overly fine-grained features and address this with region-level learning. They also introduce a new benchmark extending DragBench with region annotations, achieving state-of-the-art results as shown in Table 2, with further component analysis in Table 3."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1: The paper provides a meaningful analysis of why DiT-based models fail with point-based drag supervision. This motivation is well grounded and timely, as future generative systems are likely to rely on DiTs rather than UNet backbones.\n\n2: The method achieves clear state-of-the-art performance on both ReD Bench and DragBench-DR, showing consistent improvements over existing drag-based editing approaches."}, "weaknesses": {"value": "1: The paper does not provide any discussion or quantitative analysis of runtime or inference time. Reporting the average optimization time per image or per drag operation would make the comparison with prior work more complete.\n\n2: The paper mainly presents successful examples but does not include qualitative results in more challenging scenarios such as complex backgrounds or strong occlusions. Showing a few representative failure cases or difficult examples would help readers better understand the method’s limitations and potential failure modes.\n\n3: Section 3.4 mainly integrates existing personalization adapters to improve inversion, which is effective but not novel. More discussion on how the adapter interacts with DiT features would clarify the contribution.\n\nMinor suggestion:\n1 The notion of a “stronger generative prior” is repeatedly emphasized in the abstract and introduction, yet its meaning and mechanism of utilization in DiT remain unclear. The underlying reasoning of why and how this prior facilitates more faithful or controllable edits only becomes explicit in Section 3.1–3.2. It would be helpful to briefly clarify in the abstract how this stronger prior is actually leveraged/achieved by your framework, perhaps in one sentence.\n\n2 The analysis in Section 3.1 relies mainly on qualitative visualization to argue that point-based supervision fails on DiT due to finer feature granularity. While reasonable, the claim lacks quantitative evidence, such as metrics of using DiT to support the argument.\n\n3 It is unclear which dataset was used for the ablation in Table 3."}, "questions": {"value": "1: The method assumes that all drag operations can be represented by affine transformations such as translation, deformation, or rotation. How well does this assumption hold for complex non-rigid deformations, for example hair, cloth, or water surfaces?\n\n2: Which layer of the DiT the feature extractor F(⋅) is taken for region-level supervision. Is the performance sensitive to this choice, and has any analysis been conducted on different feature layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B3pvTVbbVO", "forum": "Zhckizkww1", "replyto": "Zhckizkww1", "signatures": ["ICLR.cc/2026/Conference/Submission749/Reviewer_ZsfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission749/Reviewer_ZsfJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561471980, "cdate": 1761561471980, "tmdate": 1762915597580, "mdate": 1762915597580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DragFlow, a drag-based editing method specifically designed for Diffusion Transformers (DiTs). The authors' core insight is that point-based supervision, standard in prior work, is fundamentally mismatched with the fine-grained feature structure of DiTs. They solve this by introducing a region-level supervision scheme that treats the object as a whole, guiding the generation process with progressive affine transformations. This is combined with a clever adapter-based inversion technique to preserve subject identity and a hard-constrained background mask. The method sets a new state-of-the-art, producing more realistic and structurally coherent results on a new, more challenging benchmark (ReD Bench) also introduced by the authors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Clear, Fundamental Insight:** The paper's main strength is its correct diagnosis of *why* prior methods fail on DiTs. The shift from point- to region-level supervision isn't just an incremental tweak; it's a necessary conceptual shift grounded in a solid understanding of the underlying model architectures. This is a valuable contribution.\n\n2.  **Effective and Cohesive System:** The proposed solution is elegant and well-engineered. Region-level supervision provides robust guidance, while the adapter-enhanced inversion is a very practical solution to the known identity drift problem in distilled models like FLUX. The components work together to solve the problem comprehensively.\n\n3.  **Strong Empirical Support:** The experiments are convincing. DragFlow clearly outperforms a wide range of competitors on both quantitative metrics and visual quality, especially in complex non-rigid deformations where other methods falter. The ablation study effectively validates the importance of each design choice, and the new ReD Bench is a welcome contribution for future research in this area."}, "weaknesses": {"value": "1.  **Dependency on External Adapters:** The method's excellent identity preservation relies on a pre-trained IP-Adapter. This is a pragmatic choice, but it means performance is coupled to the quality of this external module. The paper would be stronger with a brief discussion on this dependency.\n\n2.  **Missing Efficiency Analysis:** The method is iterative and involves multiple components. A quantitative comparison of the runtime against faster, non-optimizing methods (like FastDrag) is missing. This would help users understand the quality vs. speed trade-off. Surpassing non-optimizing methods in speed is not necessary, but a discussion section is necessary for user's information.\\\n\n3. Some prior works (like LightningDrag InstantDrag) predict dense motion field from user's sparse input, can their dense motion be used as region supervision in DragFlow?"}, "questions": {"value": "- The MLLM front-end for intent parsing is a nice usability feature. How robust is it in practice? What happens if it misinterprets the user's intended transformation (e.g., deformation vs. rotation), and is there a simple way for the user to correct it?\n\n- The framework currently handles relocation, deformation, and rotation via affine transforms. How would it generalize to more complex, non-affine warps like twisting or bending?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c6QNaZHol6", "forum": "Zhckizkww1", "replyto": "Zhckizkww1", "signatures": ["ICLR.cc/2026/Conference/Submission749/Reviewer_yXpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission749/Reviewer_yXpA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794437176, "cdate": 1761794437176, "tmdate": 1762915597461, "mdate": 1762915597461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DragFlow, a novel framework for region-based drag editing tailored to Diffusion Transformers, addressing the limitations of point-based methods when applied to models like FLUX. The work first provides a detailed analysis of why point-based drag fails on DiTs and motivates the first region-based approach. DragFlow incorporates region-level affine supervision, gradient mask-based background preservation, and adapter-enhanced inversion to improve subject consistency. It also leverages MLLMs for intent inference and introduces the ReD Bench benchmark. Experiments demonstrate outstanding performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper thoroughly investigates the failure of point-based drag editing on DiTs, contrasting feature granularity between UNets and DiTs. This analysis provides valuable insights into model architectures and justifies the shift to region-based supervision. The motivation is strong and clear, as it addresses a critical gap in adapting drag editing to modern DiT-based models like FLUX.\n2. DragFlow introduces a well-designed pipeline that replaces point-based supervision with region-level affine transformations, avoiding the need for explicit tracking and enhancing feature guidance. Together with gradient masks and pre-trained adapters to enhance background preservation and subject consistency.\n3. Valuable ReD Bench benchmark with detailed annotations and demonstrates SOTA performance on both ReD Bench and DragBench-DR across multiple  metrics.\n4. Effectively uses multimodal LLMs to interpret user intents and generate editing prompts, significantly reducing user interaction burden."}, "weaknesses": {"value": "1. The paper lacks detailed statistics on time and memory consumption, which is critical given the optimization-based nature of DragFlow. With multiple iterations, inversion steps, and adapter integrations, the method likely incurs higher computational costs. A comparison of inference time and GPU memory usage would provide practical insights for real-world applications.\n2. While DragFlow is validated on FLUX, its adaptability to other DiT-based models (e.g., SD3) is not explored. The paper does not address potential architectural or training differences that might affect performance, leaving questions about generalization."}, "questions": {"value": "1. How essential are the MLLM-generated editing prompts? Would the method achieve comparable results without editing prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PZC9dXBUBe", "forum": "Zhckizkww1", "replyto": "Zhckizkww1", "signatures": ["ICLR.cc/2026/Conference/Submission749/Reviewer_EW7a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission749/Reviewer_EW7a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901294432, "cdate": 1761901294432, "tmdate": 1762915597333, "mdate": 1762915597333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of applying drag-based image editing, a paradigm popularized by models like DragGAN, to modern generative models based on Diffusion Transformers (DiTs), such as FLUX."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiments are very sufficient, compared with almost all mainstream baselines, and the results are very convincing.\n2. The proposed method sounds novel.  Similar to RegionDrag, this method doesn't need Point Tracking, which is very promising compared with previous drag-edit methods."}, "weaknesses": {"value": "1. Motivation for some proposed components is not clearly explained. For details, see the questions part.\n\n2.  Lack an introduction to the \"drag edit\" method, like the meaning of motion supervision, tracking. It is unrealistic to assume that all readers are familiar with drag edit.\n\n3. The discussion about ReD Bench is insufficient. Since the authors claim ReD Bench as one of the contributions, I think more details should be provided (such as how the editing instructions and corresponding ground truth are generated, the dataset size, or even some examples)."}, "questions": {"value": "1.  Since the input for point-based and region-based editing is different, how can you guarantee the fairness of the comparison?\n\n2.  I am confused about the statement in Lines 79-80. What is inversion drift? And why will CFG-distilled exacerbate it? Then why does inversion drift make KV injection insufficient?\n\n3.  In this paper, only three kinds of operation are defined (relocation, deformation, and rotation).  Can they represent all the drag instructions? What if there is an instruction consists of a rotation and relocation?\n\n4.  Lines 307- Line 309. About the experimental setting, it is quite rare in deep learning with a learning rate of 1000 or 1200. What's the reason?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Akjwrn8Edj", "forum": "Zhckizkww1", "replyto": "Zhckizkww1", "signatures": ["ICLR.cc/2026/Conference/Submission749/Reviewer_mi22"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission749/Reviewer_mi22"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912065337, "cdate": 1761912065337, "tmdate": 1762915597216, "mdate": 1762915597216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}