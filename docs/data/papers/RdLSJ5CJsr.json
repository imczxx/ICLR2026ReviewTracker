{"id": "RdLSJ5CJsr", "number": 5239, "cdate": 1757874507080, "mdate": 1759897986202, "content": {"title": "MGAL: A Multilingual Granularity-Aware Long-context Benchmark", "abstract": "Evaluation of long-context Large Language Models (LLMs) has advanced rapidly. However, most existing benchmarks are limited to the document level and focus mainly on high-resource languages, leaving many fine-grained challenges insufficiently evaluated. To address this gap, we present MGAL, the first multilingual, granularity- and position-aware long-context benchmark. MGAL is constructed from United Nations (UN) reports spanning 8K to 128K tokens across the six official UN languages. It covers four coherent levels of linguistic granularity (word, sentence, paragraph, and document) and further stratifies entries by their position within the document (begin, middle, and end), indexed at both the document and paragraph levels. This design enables systematic diagnosis of multilingual long-context comprehension across different granularities. \n\nThrough extensive experiments and analyses on 12 long-context LLMs, we find that: (1) LLMs perform well at word-level tasks but struggle with coarser-grained ones; and (2) Closed-source models retain a clear performance advantage in low-resource languages, while open-source models, especially smaller ones, lag behind. We further identify two new key challenges: (1) Under local semantic crowding, where neighboring sentences share topics and entities, models tend to follow surface cues (e.g., connectives like `however' or repeated entities) rather than the discourse role of the sentence in the surrounding context (e.g., background, explanation, outcome); and (2) A persistent gap between fluency and consistency in generated outputs, where models produce text that reads smoothly but drifts from the source facts. In addition, we observe several patterns in line with prior studies, including reliance on nearby evidence and reuse of options under uncertainty. Together, these findings highlight specific weaknesses of current LLMs and emphasize the need for multilingual, fine-grained, and position-aware evaluation, offering guidance for developing future long-context LLMs.", "tldr": "A Multilingual Granularity-Aware Long-context Benchmark", "keywords": ["Long-context Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/855c4be2384b61ff1c3e3191a850434fcb0713bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- The paper introduces MGAL, which is a multilingual benchmark for evaluating multilingual long-context LLMs across multiple levels of linguistic granularity and explicit positional control. MGAL is designed to test model ability at 4 granularities, word, sentence, paragraph, document, and at 3 intra-document positions, beginning, middle, end. This specific design choice of the benchmark enables targeted diagnosis of where and how long-context models fail.\n- Models are strong on fine-grained, local retrieval-style tasks (word-level QA), but performance drops sharply at coarser discourse levels (paragraph filling, summarization), suggesting LLMs face difficulties in maintaining faithfulness and discourse role tracking over long context.\n- The authors suggest that the benchmark exposes two failure modes: first is local semantic crowding, meaning that models latch onto shallow lexical overlap and connectors (such as “however”) rather than understanding the discourse function. Another is fluency-consistency gap, where models generate plausible, stylistically coherent paragraphs or summaries but they actually drift factually from the source."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- In terms of originality, the authors propose a new benchmark named MAGL, which explicitly factors granularity (word → sentence → paragraph → document) and position (begin / middle / end) as axes of evaluation. Most prior long-context benchmarks either (i) test retrieval of a single answer needle or (ii) score very long summarization; they rarely control for where in the document the evidence lies or distinguish which discourse unit is being tested.\n- The notion of “local semantic crowding” as a failure mode is interesting and goes beyond the usual “lost in the middle” narrative: the paper claims models overweight shallow lexical similarity and connective cues when several plausible candidate sentences are semantically adjacent. That’s a new error taxonomy for long-context discourse reasoning.\n- The authors also include ablations on instruction placement and option order in Section 4.5, where they show instruction anchoring effects with ordering bias of multiple-choice distractors studied in prior works.\n- MGAL exposes a persistent gap between sounding good and being faithful in multilingual long context settings. This further sheds light to a real deployment risk for summarization or document reporting, etc."}, "weaknesses": {"value": "- All data in the MAGL benchmark stems from the United Nations reports, where they are typically very formal, bureaucratic, and policy-style documents with specific discourse norms. However, the authors seem to be generalizing their findings to “long-context comprehension” in general, but the benchmark may mostly reflect how well models handle bureaucratic prose, not novels, technical manuals, meeting transcripts etc. This is especially relevant for the “local semantic crowding” finding in the main paper where UN documents may often repeat named entities, country mentions, dates, and procedural language in adjacent sentences. It’s unclear whether the same failure mode would hold in narrative long-form text in other domains.\n- As a complement to the automatic metrics used as main evaluation metrics, “LLM-as-a-judge” is used to rate qualities like Topic Fidelity and Entity Consistency, but we are not shown inter-model agreement, calibration, bias controls, or any human sanity checks beyond the raw scores.\n- Given that one of the failure mode in the paper claims is a “fluency vs. factual consistency gap,” I would expect at least a targeted bilingual human factuality audit (e.g., for summarization hallucinations in Arabic and Russian) to establish that LLM-as-a-judge isn’t itself biased or hallucinated."}, "questions": {"value": "- How exactly are the position alignment enforced across languages? In other words, how can we guarantee that a “middle paragraph” in Arabic actually expresses the same information as the “middle paragraph” in Russian?\n- How were the six languages aligned at the paragraph/sentence level? Are these official UN parallel releases with paragraph numbering, or did you rely on automatic alignment for any languages?\n- Which model served as the judge in LLM-as-a-judge experiments? Was it always the same across all systems being evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RPI73E4duj", "forum": "RdLSJ5CJsr", "replyto": "RdLSJ5CJsr", "signatures": ["ICLR.cc/2026/Conference/Submission5239/Reviewer_1YDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5239/Reviewer_1YDx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761429436252, "cdate": 1761429436252, "tmdate": 1762917967490, "mdate": 1762917967490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MGAL, an evaluation benchmark consisting of numerous tasks designed to evaluate LLM's long context capabilities. This benchmark consists of 7 tasks that test the model's comprehensions at different levels of granularity and notably ensures a diversity in the position of the parts of the source texts required to answer the questions. The data source is UN reports and the resulting dataset covers 6 different languages, emphasizing the cross-lingual comparability of the results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a large, useful benchmark that represents a large body of work. This dataset will be quite valuable given its numerous tasks, evaluating long-context capabilities using different methods. The 6-language coverage also addresses a particular gap in long-context benchmarks.\n1. Quality cross-lingual data source; the use of UN reports ensures the texts themselves are parallel in a quality-assured way.\n1. To accompany the presentation of the benchmark, the authors conduct thorough evaluations of a high number of diverse LLMs (12). The paper provides many detailed findings of LLMs evaluated using MGAL, notably including many tables in the appendix.\n1. The paper provides many details of the LLM prompts used for dataset construction in the appendix for reproducibility\n1. Strong paper organization and writing."}, "weaknesses": {"value": "*Major*\n\nLimited discussion of dataset quality:\n- the entire dataset is created by LLMs. The paper fails to address the risks of this method of benchmark construction, either by discussing quality evaluation or assurance. Quality assurance is alluded to in the intro and in Appendix B, but details are never provided in the main body.\n- Notably, details of the manual verification are heavily unspecified (e.g. L192). Who verified ? What guidelines did they use ? One annotator per sample ? What was the rejection rate ? Was the manual verification only for the word-level tasks ?\n- Because of this, analysis findings from Sections 4.3, 4.5, and 5.1/5.2, especially those relating to surface-level features, could be highly influenced by artifacts from LLM generation.\n\n*Other*\n\n1. The paper positions the dataset and findings with high vs. low resource languages. This requires a change in framing, as the languages included are 6 of the highest-resource languages. By almost any definition, these are not low-resource languages.\n1. The selection of related works discussed in S2.1 seems random and not cohesive, given the large amount of literature in long-context modeling\n1. As I understand, there's a good amount of literature in long-document MT evaluation & benchmarks. It feels as though that should be mentioned.\n1. Main Result #1 (Section 4.2) is unsubstantiated. To make this claim, the authors are comparing performance across tasks/benchmarks which have different difficulties and metrics (which the authors acknowledge in Appendix F.1). It cannot be claimed that performance \"degrades at coarser granularities\".\n\n*Suggestions/Minutia*\n\n1. L120 drop the mention of NTK-aware scaled RoPE unless there's a better citation\n1. L136 mention M^4LE and ONERULER's language coverage \"expands/increases coverage\" is insufficient detail when these are (seemingly) the most important related works. Better contextualizing this benchmark relative to those would better emphasize the paper's contribution.\n1. Flagging that Figure 1 did not render for me when opening the PDF in Safari and rendered bizarrely in Mac's Preview app. Was able to see it fine in other apps, but there might be some issue with the file format & LaTeX.\n1. Section 3.2.1 I would mention this is essentially the same task as extractive QA / reading comprehension benchmarks (e.g. {S,X}QuAD)\n1. L190, L201 typo\n1. L268 cite sacreBLEU\n1. Section 4.1.2 I suggest using chrf++, as it has more or less replaced BLEU as the industry standard MT metric.\n1. Section 4.1.1 I would specify if the open-source models were run locally or accessed via API. I assume API for Kimi, DeepSeek, and GLM given their size, but this should be made clear the distinction between API vs. local querying.\n1. It is probably worth mentioning somewhere the potential for confounding factors when generating the benchmark with GPT-4 and then evaluating on GPT-5.\n1. Mistral-Small-3.2-24B's high performance on the cloze task is not discussed, and so the lack of discussion leaves me a bit skeptic."}, "questions": {"value": "1. It is unclear how the questions were created in different languages. Are they the exact same question across languages (fully parallel), or generated individually in each language. Given the emphasis on \"consistent cross-lingual comparison\", I would assume it was generated in English (?) and translated. Appendix D does not offer this detail, either."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e37JbzgbCa", "forum": "RdLSJ5CJsr", "replyto": "RdLSJ5CJsr", "signatures": ["ICLR.cc/2026/Conference/Submission5239/Reviewer_SAgw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5239/Reviewer_SAgw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610325787, "cdate": 1761610325787, "tmdate": 1762917967096, "mdate": 1762917967096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MGAL, a new benchmark for evaluating long-context Large Language Models (LLMs). The authors argue that existing benchmarks are insufficient, as they primarily focus on document-level tasks in high-resource languages and fail to evaluate fine-grained or position-specific understanding. MGAL addresses this gap using United Nations (UN) reports spanning 8K to 128K tokens across the six official UN languages. It also focuses on Granularity-Aware and Position-Aware evaluation. Based on the evaluation results, the paper shows some observations and challenged described below:\n- Performance vs. Granularity: Models perform well on fine-grained, word-level tasks but struggle significantly with coarser-grained tasks like paragraph filling and summarization.\n- Language Disparity: Closed-source models maintain a clear performance advantage in low-resource languages, though large open-source models are competitive in high-resource languages.\n- Local Semantic Crowding: Models over-rely on \"surface cues\" (e.g., connectives like \"however\" or repeated entities) rather than understanding the true \"discourse role\" of a sentence (e.g., background, explanation).\n- Fluency-Consistency Gap: Models generate text that is fluent and stylistically correct but is \"weakly anchored to the input,\" resulting in factual drift and unsupported claims."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The benchmark's quality is supported by its data source, using position-aligned United Nations documents. This design enables semantically matched, consistent cross-lingual comparisons across all six languages.\n\n- The authors emphasized quality control through rigorous human oversight, stating they \"prioritize quality over quantity by manually auditing every query-response pair and removing flawed samples\". This manual verification was applied to all generated data, including QA pairs and sentence-level cloze tasks."}, "weaknesses": {"value": "### Unclear Synergy Between Benchmark Dimensions\n\nThe paper combines multilinguality and granularity. However, the necessity of merging these three distinct characteristics is not well-justified. The analysis section largely treats them as separate topics, lacking insights that demonstrate a meaningful interaction between them. It feels more like three separate evaluation axes bundled into one benchmark rather than a single, cohesive framework. There are already solid lines of research about each characteristic, which i think the authors should provide supportive reasons of the proposed comprehensive view.\n\n### Superficial and Skewed Analysis:\nGeneral vs. Specific Insights: Most analysis points relate to general long-context behaviors (e.g., recency bias , option-order bias ), not novel insights derived specifically from the benchmark's multilingual or multi-granularity structure.\nWeak Multilingual Focus: The multilingual aspect is significantly underdeveloped in the main paper's analysis, primarily represented by a single spider plot (Figure 2)  without a deep dive.\nOversight in Sentence-Level Analysis: The paper's claim of robust \"mid-context comprehension\" in the sentence-cloze task  seems contradictory. Given that instructions are placed at the end of the context, this peak in performance could be an artifact of the task design, where the instruction's proximity acts as a simpler cue rather than evidence of true comprehension. The analysis is heavily skewed towards this single sentence-level task, and the justification for evaluating multilinguality and granularity together is never established.\n\n\n### Confounded Evaluation Metrics and Task Novelty\n\nLack of Novelty: The tasks for finer granularities are not novel; word-level QA and sentence-level cloze (fill-in-the-blank) are standard formats. It seems that organizing tasks by a granularity view does not offer meaningful insights differentiated from existing works.\nConfounded Variables: The coarser-grained tasks (Paragraph Filling, Summarization, Translation)  introduce confounding variables. They mix the evaluation of long-context comprehension with complex, task-specific generation capabilities. A model's poor summarization score, for instance, might reflect its inherent summarization ability (a skill that varies widely between models) rather than its long-context processing ability. This intertwining of comprehension, generation, and task-specific knowledge reduces the reliability of the findings derived from these tasks.\n\n\n### Coarse Analysis and Weak Visualization:\nTask-Specific Observations: The detailed analysis of the cloze task (e.g., reliance on surface cues ) feels overly specific to that task's design. These observations may not generalize as fundamental properties of long-context models, narrowing the paper's overall impact.\nInsufficient Data in Visuals: The plots in Figures 2, 3, and 4 are criticized for being simplistic. They visualize data that could be presented more concisely (perhaps in a table) and lack the information density or analytical depth expected from primary figures in a benchmark paper."}, "questions": {"value": "Regarding Data Curation: The data construction pipeline relies on LLMs for generating questions, distractors, and identifying key paragraphs. You state that these were all “manually verified” and that “flawed samples” were removed. Could you provide more details on this curation process? Specifically, what percentage of LLM-generated samples were typically discarded as “flawed”? And for the sentence-cloze task, what steps were taken during verification to ensure that the GPT-4-generated distractors were plausible but unambiguously incorrect, especially in the context of “local semantic crowding”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UFcy2EWZYb", "forum": "RdLSJ5CJsr", "replyto": "RdLSJ5CJsr", "signatures": ["ICLR.cc/2026/Conference/Submission5239/Reviewer_r75U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5239/Reviewer_r75U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687482888, "cdate": 1761687482888, "tmdate": 1762917966697, "mdate": 1762917966697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MGAL, the first multilingual, granularity-aware, and position-aware benchmark for evaluating long-context LLMs. Built from United Nations reports, the dataset spans six official UN languages (Arabic, Chinese, English, French, Russian, Spanish) with document lengths ranging from 8K to 128K tokens. It includes tasks at four linguistic levels (word, sentence, paragraph, and document) enabling a systematic assessment of LLM comprehension across different granularities and positional contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The benchmark clearly separates comprehension across word, sentence, paragraph, and document levels, and designs distinct tasks tailored to each granularity.\n- It covers all six official UN languages with position-aligned documents, enabling fair and quantitative cross-lingual comparison under identical contextual settings.\n- The authors conduct a large-scale evaluation of 12 long-context LLMs including GPT-5, Claude-Sonnet-4 with context windows up to 128K tokens, offering a broad empirical foundation.\n- Through systematic ablations on context removal (accuracy drop 73% --> 31%), instruction placement, and option order, the study quantifies models’ reliance on contextual distance and positional bias"}, "weaknesses": {"value": "- The Sentence Cloze and Paragraph Filling tasks show extremely low performance, Without human baseline, making it unclear whether the difficulty reflects task design or inherent complexity. (10-14 choices, random guess =  7~10% ) \n- The metric choice is inconsistent across tasks. Paragraph Filling employs LLM-as-a-judge while Summarization relies solely on ROUGE, despite its acknowledged limitation as a coarse proxy.\n- The analysis of Local Semantic Crowding is anecdotal, lacking quantitative validation or controlled ablations.\n- The reported Fluency-Consistency Gap offers limited novelty. It reiterates a well-known issue in abstractive summarization without presenting new, long-context–specific insights"}, "questions": {"value": "- The paper repeatedly mentions “manually verified,” “post-processed by human check,” and “robustness checks by humans,” but provides no details about the verification process. Who the annotators were? What guidelines they followed? and whether IAA was measured remain unspecified. It is also unclear what aspects were verified or how the human checks in Appendix F.4 were conducted.\n- It is unclear why LLM-as-a-judge was applied only to the Paragraph Filling task but not to other generative tasks such as Summarization or Translation.\n- Table 2 reports an average BLEU of 16.9, which gives the misleading impression that all models fail at translation, while Appendix H shows strong models such as GPT-5 and Gemini-2.5 achieving practical-quality scores. This discrepancy should be clarified in the main table to prevent misinterpretation.\n- Could the authors provide a quantitative validation of the “Local Semantic Crowding” effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zzx4npwSEY", "forum": "RdLSJ5CJsr", "replyto": "RdLSJ5CJsr", "signatures": ["ICLR.cc/2026/Conference/Submission5239/Reviewer_gwje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5239/Reviewer_gwje"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872836035, "cdate": 1761872836035, "tmdate": 1762917966449, "mdate": 1762917966449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}