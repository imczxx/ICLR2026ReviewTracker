{"id": "weWUOuLTdj", "number": 25359, "cdate": 1758367139176, "mdate": 1759896723603, "content": {"title": "Generative Model via Quantile Assignment", "abstract": "Deep Generative models (DGMs) play two central roles in modern machine learning: (i) producing new information (e.g., image synthesis, data augmentation, and creative content generation) and (ii) reducing dimensionality (by deriving low-dimensional latent representations).\nYet, DGMs' versatility must confront training difficulty. Both information generation and dimension reduction using DGMs require learning the distribution. While deep neural networks (DNNs) are a natural choice for parameterizing generators, there is no universally reliable method for learning compact latent representations. As a compromise, current approaches rely on introducing an additional DNN: (i) variational autoencoders (VAEs), which map data into latent variables through an encoder, and (ii) generative adversarial networks (GANs), which employ a discriminator in an adversarial framework.  Learning two DNNs simultaneously, however, introduces conceptual and practical difficulties. Conceptually, there is no guarantee that such an encoder/discriminator exists, especially in the form of a DNN. In practice, training encoders/discriminators on high-dimensional inputs can be more data-hungry and unstable than training a generator on low-dimensional latents (whereas generators usually take low-dimensional latent data as input). Moreover, training multiple DNNs jointly is unstable, particularly in GANs, leading to convergence issues such as mode collapse. Here, we introduce NeuroSQL, a DGM that learns low-dimensional latent representations without an encoder.  Specifically, NeuroSQL learns the latent variables implicitly by solving a linear assignment problem, then passes the latent information to a unique generator. To demonstrate NeuroSQL's efficacy, we benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on three independent datasets on faces from the Large-Scale CelebFaces Attributes Dataset (CelebA), animal faces from Animal Faces HQ (AFHQ), and brain images from the Open Access Series of Imaging Studies (OASIS). Compared to VAEs, GANs, and diffusion models within our experimental setup, (1) in terms of image quality, achieves overall lower mean pixel distance between synthetic and true images and stronger perceptual/structural fidelity, under the same computational setting; (2) computationally, NeuroSQL requires the least amount of training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data when there are limited training data (e.g., neuroimaging data with a higher-dimensional feature space than the sample size). Taken together, by embracing quantile assignment instead of an encoder, NeuroSQL presents us a fast, stable, and robust way to generate synthetic data with minimal information loss.", "tldr": "", "keywords": ["generative models", "quantile assignment", "optimal transportation", "latent representation learning", "synthetic data generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03dc9c505e450ba4983984b1c65cf40beda8f828.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes NeuroSQL, a latent-variable generative model that removes the encoder/discriminator and instead assigns latent codes by solving a linear assignment problem between data and a fixed quantile lattice of the latent prior. Training alternates between fitting a single generator to currently assigned codes and re-solving the assignment with a cost based on a perceptual/structural image loss; a momentum update smooths the assigned codes across iterations. For d>1, the quantiles are built via center-outward multivariate ranks from optimal transport; practically, a low-discrepancy grid on the unit ball is used. Experiments under a small-compute regime compare NeuroSQL against VAE, GAN, and a budget-matched diffusion baseline on MNIST, CelebA, AFHQ, and OASIS, showing improved visual quality and quantitative scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a meaningful attempt to resolve the disadvantages of mainstream generative models VAEs and GANs, by removing the encoder and discriminator modules and integrating statistical quantile learning for stable training. The approach may be of interest in certain domains of generative tasks."}, "weaknesses": {"value": "1. The main issue of the proposed method is scalability. The optimization algorithm runs in O(n^3) time with n being the number of samples. While the paper mentioned approximation via mini-batches, no concrete evidence is provided to show if it still works with large datasets (and models). The paper compares the method with VAEs, GANs and diffusion models in the seemingly fair budgeted setting. However, the comparison is not sound in that other models scale easier and perform much better with more budget. The budget, 200 Google Colab compute units and 2000 training images, is too limited for practical generative tasks.\n\n2. Experimental results are not convincing to show the advantage of the proposed method. Images in Figure 2 are in low resolution making it hard to compare the visual quality. Quantitative results in Appendix show high instability across latent dimensions. In particular, in many cases the FIDs for NeuroSQL, VAE and CAN change significantly and non-monotonically as the latent dimension increases."}, "questions": {"value": "1. How does the method perform in mini-batches settings?\n\n2. For quantitative results, how many runs were executed? Is unstable and insufficient training the cause for the varying evaluation scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bD6iG8Ncwl", "forum": "weWUOuLTdj", "replyto": "weWUOuLTdj", "signatures": ["ICLR.cc/2026/Conference/Submission25359/Reviewer_bBez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25359/Reviewer_bBez"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895596931, "cdate": 1761895596931, "tmdate": 1762943414559, "mdate": 1762943414559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper build a new structure for generative models, with less dimension."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. this is a new structure\n2. test with many different dataset and genearive framework"}, "weaknesses": {"value": "1. The expression of Figure 1 is unclear. From the image, it appears that the input data are fed into the decoder. The paper should clarify why this component is referred to as the decoder rather than the encoder, and explicitly describe what the input data are. Moreover, the roles of Momentum Update and Embedding in the framework are not clearly explained. What does “Cost” represent in this figure? Is it equivalent to the loss function?\nAdditionally, regarding the left-hand side of the figure, I speculate that it corresponds to the grey-shaded part on the right-hand side. However, it is not clear how the output on the left is transmitted to the generator. This connection should be explained more explicitly.\n\n2. Section 3 mainly discusses the quantile assignment, but it should also explain how this mechanism is made trainable and why it is considered optimal. These claims should be supported by theoretical justification or experimental evidence.\n\n3. Dataset and Metrics: The introduction of the dataset and evaluation metrics is not the core contribution of the paper and could be moved to the appendix or combined with the related work section to improve focus.\n\n4. Diffusion Model Performance: The diffusion process seems to fail under the proposed method, which may be influenced by the linear assignment mechanism. Diffusion models often struggle with simple linear interpolation in latent space, resulting in abrupt transitions, artifacts, or degenerate (e.g., grey) images. This appears to be a limitation of the current approach. However, it might be mitigated by adopting smoothed diffusion models [1] or related approaches that enforce smoother, more linear latent mappings.\n\n[1] Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models"}, "questions": {"value": "check with weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BZpJYcKY6l", "forum": "weWUOuLTdj", "replyto": "weWUOuLTdj", "signatures": ["ICLR.cc/2026/Conference/Submission25359/Reviewer_cpKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25359/Reviewer_cpKk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922434113, "cdate": 1761922434113, "tmdate": 1762943414076, "mdate": 1762943414076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NeuroSQL, a generative modeling framework that learns latent variables through a quantile-assignment process derived from optimal transport, eliminating the need for an encoder or discriminator. The model alternates between a generator update and an assignment step solved via the Hungarian algorithm. This approach aims to combine stable, deterministic optimization with the expressiveness of deep decoders. Experiments span MNIST, CelebA, AFHQ, and OASIS, across multiple generator backbones (ConvNet, ResNet, U-Net), showing competitive visual quality and efficient convergence under low-data conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The replacement of encoder–decoder mappings with an assignment-based quantile mechanism is conceptually fresh and theoretically grounded. It bridges optimal transport with generative modeling in a unique and elegant way.\n\nThe method is particularly well-suited for data domains like neuroimaging, where dimensionality exceeds sample size, and the assignment cost is independent of feature dimensionality.\n\nAvoiding adversarial losses makes the model stable and lightweight to train. The simplicity of using an L2-based reconstruction objective allows reproducibility even in constrained computing environments.\n\nThe experiments show meaningful improvements in visual quality and diversity under limited data, highlighting NeuroSQL’s advantage."}, "weaknesses": {"value": "While the paper provides an overall complexity estimate, quantitative comparisons to VAEs, GANs, or diffusion models in terms of runtime, memory, and scalability would provide stronger evidence of its efficiency. The Hungarian step’s cubic cost could be limiting for very large batch sizes, although mini-batching is suggested as a practical solution. \n\nThe performance advantage over GANs and VAEs is not uniform—some settings show weaker results, suggesting NeuroSQL's strengths may be inconsistent."}, "questions": {"value": "Why the quantitative diffusion comparisons under similar compute budgets are missing?\n\nDo you see challenges extending this approach to transformer-based or high-resolution settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JNW3zAzEJs", "forum": "weWUOuLTdj", "replyto": "weWUOuLTdj", "signatures": ["ICLR.cc/2026/Conference/Submission25359/Reviewer_zmjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25359/Reviewer_zmjo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977504320, "cdate": 1761977504320, "tmdate": 1762943413703, "mdate": 1762943413703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel deep generative model (DGM) called NeuroSQL, which departs from the common framework of encoder+decoder (as in VAEs) or adversarial training (as in GANs). Instead it directly learns latent codes for each training datum via a quantile-assignment (linear assignment / Hungarian) to a pre-specified lattice of latent quantiles, and trains a generator network to map those codes to data.\n\nThe authors formulate a minimisation over both generator parameters θ and a permutation π that maps quantile codes to data\n\nFor multivariate (latent > 1D) codes they leverage optimal-transport/multivariate quantile theory and solve assignment between training data and a fixed uniform grid in latent space. \nOpenReview\n\nThey propose an alternating algorithm: (i) keep π fixed, update θ via generator training; (ii) fix θ, update π via Hungarian algorithm assignment; optionally use momentum smoothing of assignments. \nOpenReview\n\nEmpirically they evaluate on 4 domains (MNIST, CelebA, AFHQ animal faces, and OASIS brain images) under a “small‐budget / low-data” regime: e.g., training data capped at ~2 k images, resolution up to 128×128, single Google Colab budget. \nOpenReview\n\nThe main claims: (1) NeuroSQL is more stable (no adversarial or encoder collapse issues), (2) it yields better or competitive image quality (measured via proxy FID, LPIPS, SSIM) under matched generator/backbone conditions, (3) it is more resource-friendly in low-data/high-dimension settings."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Interesting idea / novel paradigm — Replacing the encoder or discriminator with an explicit assignment of latent codes (quantile grid) is novel, and links generative modelling with statistical quantile/transport theory.\n\nPragmatic focus on low-data regimes — The paper addresses an important setting: generating synthetic data when the training dataset is small relative to high ambient dimension (e.g., neuroimaging) which is under-studied.\n\nTheoretical underpinning — The use of quantile assignment and the convergence argument in the univariate / multivariate case gives formal support to the latent-code approximation strategy."}, "weaknesses": {"value": "1. Resolution / dataset scale limited — Their experiments are constrained to small image resolutions (64×64-128×128) and relatively small sample sizes (~2 k images) under a limited compute budget. While this is aligned with their motivation (low-budget), it raises the question of how the method performs at larger, contemporary scales (e.g., 256×256, ImageNet scale). The authors acknowledge this in future work.\n\n2. Interpretability of latent codes — Since the quantile grid is fixed and codes are assigned via permutation, the learned latent space may lack the structure/meaningfulness of e.g., disentangled VAE latents or hierarchical GAN latents. The paper doesn’t deeply analyze the semantics of the latent codes—are they smooth, do they support interpolation, manipulation, etc."}, "questions": {"value": "The paper assumes a fixed quantile lattice in latent space. How sensitive is the method to the choice of quantile grid (e.g., Sobol vs uniform vs Gaussian quantiles)?\n\nThe assignment problem is discrete, yet the generator is trained with continuous gradients. How do you ensure smooth convergence given the alternating discrete–continuous optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wA8L4hu8yl", "forum": "weWUOuLTdj", "replyto": "weWUOuLTdj", "signatures": ["ICLR.cc/2026/Conference/Submission25359/Reviewer_Ggn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25359/Reviewer_Ggn5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062672751, "cdate": 1762062672751, "tmdate": 1762943413312, "mdate": 1762943413312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}