{"id": "kVojSLUcvS", "number": 2791, "cdate": 1757251080081, "mdate": 1763576570765, "content": {"title": "GlowQ: Group-Shared LOw-Rank Approximation for Quantized LLMs", "abstract": "Quantization techniques such as BitsAndBytes, AWQ, and GPTQ are widely used as a standard method in deploying large language models but often degrades accuracy when using low-bit representations, e.g., 4 bits. Low-rank correction methods (e.g., LQER, QERA, ASER) has been proposed to mitigate this issue, however, they restore all layers and insert error-correction modules into every decoder block, which increases latency and memory overhead. To address this limitation, we propose GlowQ, a group-shared low-rank approximation for quantized LLMs that caches a single shared right factor per input-sharing group and restores only the groups or layers that yield the highest accuracy benefit. \nGlowQ computes the high-precision projection once per input-sharing group and reuses it across its modules, reducing parameter and memory overhead, and retaining the expressivity of layer-specific corrections. We also propose a selective variant, GlowQ-S, that applies the cached shared module only where it provides the largest benefit. Compared with strong baselines, our approach reduces TTFB by \\(5.6\\%\\) and increases throughput by \\(9.6\\%\\) on average, while reducing perplexity on WikiText-2 by \\(0.17\\%\\) and increasing downstream accuracy by 0.42 percentage points. The selective model GlowQ-S further reduces latency, cutting TTFB by \\(23.4\\%\\) and increasing throughput by \\(37.4\\%\\), while maintaining accuracy within 0.2 percentage points on average.", "tldr": "", "keywords": ["Quantized large language models", "low-rank error correction", "group-shared factorization", "randomized SVD", "selective restoration", "low-latency inference"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13dbb5ba332e3119cf3c65b34a4550eb231786b0.pdf", "supplementary_material": "/attachment/eaa304ccf518d06f90a968d9ef1b627226686437.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes GlowQ, a group-shared low-rank correction method for quantized LLMs that improves both accuracy and efficiency by sharing a single right-factor matrix across input-sharing modules (e.g., Q/K/V), caching the projection to avoid redundant computation, and aligning the correction subspace with data usage via covariance-weighted optimization. Combined with a selective restoration strategy (GlowQ-S), it reduces TTFB by up to 23.4% and increases throughput by 37.4% with minimal accuracy loss, outperforming existing baselines in perplexity and downstream task performance while remaining compatible with standard PTQ pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficient Group-Shared Correction via Caching: GlowQ reduces redundant computation by sharing a single low-rank right factor across input-sharing modules (e.g., Q/K/V projections), enabling one-time computation and reuse of the projection R=BsharedXR=Bshared​X, which significantly lowers computational overhead during inference.\n2. Data-Aware Subspace Alignment: By incorporating input covariance into the low-rank approximation objective, GlowQ aligns the correction subspace with directions that are most frequently activated in practice, enhancing recovery accuracy without increasing rank or parameters.\n3. Scalable and Deployment-Friendly Design: The method combines a QR-reduced randomized SVD solver for efficient training with a selective restoration strategy (GlowQ-S), achieving substantial latency reduction (up to 23.4% lower TTFB) and throughput improvement (up to 37.4%) while maintaining compatibility with existing post-training quantization pipelines."}, "weaknesses": {"value": "1. While the group-sharing idea is technically sound and well-executed, the central concept is an adaptation of well-known collective matrix factorization and SVD-sharing across blocks, now framed in the context of quantized LLMs. The extension to covariance alignment is also a known trick, though its synergy with group correction is effective.\n2. While the paper compares GlowQ extensively to prior PTQ and error-corrected LLM quantization baselines (e.g., AWQ, GPTQ, L2QER, QERA, ZeroQuant), it omits direct comparison or numerical discussion of very recent approaches, such as rotation-based saliency-aware quantization methods (e.g., ROSAQ), vector quantization for KV cache (CommVQ, AnTKV), and loss-guided PTQ (GuidedQuant).\n3. This method requires an additional calibration phase to estimate the compensation matrix, but the paper does not seem to list the time and memory costs for the training/calibration phase. Additionally, storing intermediate activations and error matrices for larger models (e.g., 30B+) could lead to significant memory pressure.\n4. The paper claims the use of \"custom CUDA W4A16 kernels,\" but it does not describe the implementation details or whether key techniques such as operator fusion were used in the low-rank correction modules. Therefore, it is unclear whether the comparisons between different methods are made at the same level of optimization.\n5. Critically Limited and Potentially Misleading Experimental Scope (W4A16 only): The paper's most significant flaw is that all experiments are conducted in a W4A16 setting. This is an unrealistic and overly forgiving scenario that ignores the primary challenge in modern PTQ: activation quantization. By using FP16 activations, the main computation path and the input to the correction module remain noise-free, which dramatically simplifies the problem. The paper's claims of improved accuracy and efficiency are therefore unsubstantiated in any practical low-bit setting (e.g., W8A8, W4A4). It is entirely possible that the proposed group-sharing benefits would vanish or even become detrimental once activation quantization noise is introduced.\n6. format error: line339 -> BitsAndBytes ?, line218,268,290,293 -> Missing formula number."}, "questions": {"value": "1. The performance of GlowQ-S is highly dependent on the importance scoring functions (such as gec and gner). I am curious if there are other metrics that can indirectly reflect the effectiveness of these scoring functions. In other words, can these two metrics be proven to be globally optimal across a range of models?\n2. The method relies on the assumption that modules within an input-sharing group can share a common right factor ( B_{\\text{shared}} ), but it does not systematically verify the validity of this assumption across all layers or architectures. For example, when the functions of matrix projections like Q and V are different, it is unclear whether enforcing this sharing introduces bias.\n3. The confinement to W4A16 is a major limitation. Can you provide any results or analysis for a W4A8 or W4A4 setting? How does your group-sharing approximation hold up when the input X to the correction path is also quantized, introducing another layer of error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OHcQPZf1tk", "forum": "kVojSLUcvS", "replyto": "kVojSLUcvS", "signatures": ["ICLR.cc/2026/Conference/Submission2791/Reviewer_HaNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2791/Reviewer_HaNH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710274820, "cdate": 1761710274820, "tmdate": 1762916379171, "mdate": 1762916379171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GlowQ, a group shared low-rank approximation method, that a single rightside projection is used for a group with same input. A BX caching method is proposed to make it deployable. A selective method is proposed to further reduce the memory and latency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is technically sound.\n- The experimental results show the effectiveness of the proposed method."}, "weaknesses": {"value": "- Writing and presentation need refinement. For instance, the symbol E in Eq. 1 is identical to E_cat in Eq. 2—consistent notation should be used. In Fig. 1 (inference-path sub-figure), the chosen colors are too similar to be distinguished; a more discernible palette is required.\n- For GlowQ-S, the fraction of restored groups are not provided, should be provided for each experiment"}, "questions": {"value": "- In Fig.3, as PPL is lower the better, what is the definition of the percentage of PPL?\n- In Tab.4, No Caching means GlowQ without caching? Or a layerwise method? If it means GlowQ without caching, the authors should provide performance comparisons with layerwise methods.\n- Modern GPUs already natively support other narrow-bit formats such as MXFP4, NVFP4 and MXFP6, whose peak throughput is significantly higher than that of INT4. The paper should therefore clarify:\na) Is GlowQ directly applicable to these FP4/FP6 formats, or does its low-rank projection rely on integer-only operators?\nb) If applicable, how does GlowQ’s accuracy compare with simply running the network in MXFP4/NVFP4/MXFP6 without any additional compression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MKNG52BFMq", "forum": "kVojSLUcvS", "replyto": "kVojSLUcvS", "signatures": ["ICLR.cc/2026/Conference/Submission2791/Reviewer_DXnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2791/Reviewer_DXnt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902582493, "cdate": 1761902582493, "tmdate": 1762916378745, "mdate": 1762916378745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work describes an efficient low-rank compensation technique for weight-quantized LLMs. \nThe method finds an optimal shared down-projection factor in a data-driven manner, with whitening and grouping treatment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The idea is sound, with adequate proof. \n+ The method is relatively well described.  \n+ There is potential practical significance."}, "weaknesses": {"value": "- The empirical, data-driven, low-rank correction of quantization error is dependent on the data type and its precision.  As most of the results presented are with INT4, there is a lack of demonstration on the effectiveness for different data types and precisions--do the data statistics differ qualitatively under those conditions?  \n- As activation quantization is also important in practice, it is not clear how quantization of the activations, in combination with weight quantization, change the story. \n- Quantization errors tend to accumulation over time across tokens as well, which is particularly relevant in long-context reasoning settings.  Reporting accuracy in language modeling perplexity is insufficient in addressing this issue."}, "questions": {"value": "I have listed major questions in the Weaknesses section above.  Here are a few minor questions in addition.  \n\n* The empirical statistical analysis of the quantization error is essential to this paper.  Do you have any results on any empirical scaling laws of the statistics, in addition to simple qualitative descriptions such as long tail?  It might be helpful to apply logarithmic scale in Figure 2 to expose certain power laws.  \n* Is there any results on how and how well this method could be applied to MoE FFN layers?\n* In addition to full-precision error correction, other orthogonal methods use mixed-precision, how is the choice among these methods subject to practical tradeoff?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Lnp68c0nsF", "forum": "kVojSLUcvS", "replyto": "kVojSLUcvS", "signatures": ["ICLR.cc/2026/Conference/Submission2791/Reviewer_CZae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2791/Reviewer_CZae"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953181716, "cdate": 1761953181716, "tmdate": 1762916378398, "mdate": 1762916378398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}