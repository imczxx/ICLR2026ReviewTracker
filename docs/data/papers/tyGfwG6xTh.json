{"id": "tyGfwG6xTh", "number": 11449, "cdate": 1758199350907, "mdate": 1763667767036, "content": {"title": "Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction", "abstract": "Reasoning language models such as DeepSeek-R1 produce long chain-of-thought traces during inference time which make them costly to deploy at scale. We show that using compression techniques such as neural network pruning produces greater performance loss than in typical language modeling tasks, and in some cases can make the model slower since they cause the model to produce more thinking tokens but with worse performance. We show that this is partly due to the fact that standard LLM pruning methods often focus on input reconstruction, whereas reasoning is a decode-dominated task.  We introduce a simple, drop-in fix: during pruning we jointly reconstruct activations from the input and the model’s on-policy chain-of-thought traces. This “Reasoning-Aware Compression” (RAC) integrates seamlessly into existing pruning workflows such as SparseGPT, and boosts their performance significantly. Anonymized code can be found at: https://github.com/Anon-ICLR-RAC/ICLR-2025-Anonymous-Submission", "tldr": "Standard pruning causes large accuracy losses in reasoning LLMs. Reconstructing chain-of-thought traces during calibration preserves performance, maintaining ~95% accuracy even at 50% sparsity.", "keywords": ["LLM Compression", "Pruning", "Reasoning", "Chain-of-Thought"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fbf1525bc8932a08683d362705bd8d609cd9532.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a pruning strategy for reasoning models. Specifically, it uses the fact that there are more decoding tokens than input tokens for reasoning models, so it collects activations for those and applies existing pruning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to read and follow.\n- The solution is simple and straightforward.\n- The results show that it preserves accuracy and improves efficiency."}, "weaknesses": {"value": "- Isn’t the self-calibration a somewhat similar mechanism [1]? Please correct me if I’m wrong.\n- Missing comparison with efficient reasoning on the token side.\n    - What is the pruning cost compared to the fine-tuning cost of training the model to perform concise reasoning?\n    - How is model pruning more effective compared to token reduction? Reducing decoding tokens might reduce the cost more. Also, since both can be applied, I wonder what would happen if we applied both.\n- Runtime minutes appear reversed. It seems like the dense model is the most efficient, and pruning makes inference slower according to the tables.\n- Evaluation sets for math are limited to MATH500, and the models already perform very well on it (90%+ for 7B+ models). It is not surprising that pruning preserves high accuracy. I would like to see results on AIME as well.\n\n[1] Williams et al. “Self-calibration for Language Model Quantization and Pruning”, NACCL 2025"}, "questions": {"value": "- Are the runtime minutes reversed, or am I misunderstanding something?\n- As in the weaknesses, comparing with efficient fine-tuning for short CoT would make the paper more valuable.\n- Why do you think naive pruning leads to more tokens in the output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "W6UIywsUfA", "forum": "tyGfwG6xTh", "replyto": "tyGfwG6xTh", "signatures": ["ICLR.cc/2026/Conference/Submission11449/Reviewer_1Hgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11449/Reviewer_1Hgg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539670233, "cdate": 1761539670233, "tmdate": 1762922561613, "mdate": 1762922561613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank all reviewers for their thoughtful comments that have helped us to improve the quality of our work. During the rebuttal period, we have performed several new experiments to address the comments and questions from the reviews. The major results from our experiments show:\n\n- Robustness to model family: We perform extensive experiments with models from Qwen3 and Llama families, demonstrating that our proposed RAC benefits the pruning quality for these model families as well. These results are given in Tables 2, 3, and 4 of the revised manuscript.\n\n- Robustness to pruning algorithm: We performed experiments with the ALPS [1] and WANDA [2] pruning methods. We observe that RAC also improves the pruning quality with these methods, in addition to SparseGPT. These results are given in Table 9 of the revised manuscript.\n\n- New benchmark: We added AIME25 benchmark results for the Qwen3 family, in Table 4 (for SparseGPT) and Table 10 (for ALPS) in the updated manuscript. We see RAC also leads to better quality when we evaluate the pruned models on the AIME25 dataset, preserving accuracy very well even on this difficult mathematics benchmark.\n\nWe have highlighted the major changes to the paper in blue.\n\nWe also provided a point-by-point response to reviewers’ comments below, with some additional experiments. We thank the reviewers once again, and we look forward to hearing from them.\n\n[1] ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models, Xiang Meng, Kayhan Behdin, Haoyue Wang, Rahul Mazumder\n\n[2] A Simple and Effective Pruning Approach for Large Language Models, Mingjie Sun, Zhuang Liu, Anna Bair, J.Zico Kolter"}, "title": {"value": "Summary of New Experiments and Revisions"}}, "id": "A3Zi9fvnUg", "forum": "tyGfwG6xTh", "replyto": "tyGfwG6xTh", "signatures": ["ICLR.cc/2026/Conference/Submission11449/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11449/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11449/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763668001050, "cdate": 1763668001050, "tmdate": 1763668053680, "mdate": 1763668053680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that pruning reasoning models (e.g., DeepSeek-R1) with standard methods fails, causing accuracy drops and slower inference. The authors blame a calibration mismatch: methods use prompt activations, but reasoning is decode-dominated. The fix, Reasoning-Aware Compression (RAC), adds on-policy Chain-of-Thought (CoT) activations to the calibration set. Experiments show RAC significantly boosts accuracy (e.g., 90.0% vs. 74.4% at 50% sparsity for a 7B model) and reduces the \"rambling\" CoTs that slow down inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Problem & Diagnosis: Clearly identifies and diagnoses a non-obvious problem: pruning reasoning models can slow them down due to a prompt/decode activation mismatch (Fig. 1).\n\n2. Simple & Practical Solution: The proposed solution (RAC) is simple and practical: a \"drop-in\" fix for the calibration data, not a complex new algorithm.\n\n3. Strong Validation (on DeepSeek): Provides strong empirical validation on the DeepSeek-R1 family, showing consistent gains on math/code and a clear reason for success (lower decode error in Fig. 2)."}, "weaknesses": {"value": "1. Generalizability Concerns: I think this is the primary weakness. The claims are broad, but results are confined only to the DeepSeek-R1 (GRPO-trained) family. There is no evidence this works on standard models (Llama, Mistral) using CoT prompting or other reasoning models like Qwen3, QwQ. This severely limits the contribution's impact.\n\n2. Incomplete Method Comparison: The comparison of pruning methods is incomplete (Table 6). To validate RAC as algorithm-agnostic, the baseline (C4) results for ALPS and Wanda are needed to show relative lift, not just absolute performance.\n\n3. Unquantified Overhead: The paper notes RAC adds overhead but never quantifies the wall-clock time for calibration versus the baseline, making its practicality hard to assess."}, "questions": {"value": "1. Will RAC generalize to standard models (Llama, Mistral) using CoT prompting or other reasoning models like Qwen3, QwQ, or is the benefit specific to GRPO-trained models like DeepSeek-R1? And I think I may improve my score if some of the experiments can be done.\n\n2. Please quantify the calibration overhead (wall-clock time) of RAC versus the 1M C4 baseline.\n\n3. Table 4 shows on-policy traces are best. Is matching a model's idiosyncratic activation patterns more important than using 'cleaner' traces from a larger model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R5NvpqvsAt", "forum": "tyGfwG6xTh", "replyto": "tyGfwG6xTh", "signatures": ["ICLR.cc/2026/Conference/Submission11449/Reviewer_LocM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11449/Reviewer_LocM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569053417, "cdate": 1761569053417, "tmdate": 1762922561146, "mdate": 1762922561146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a method to compress models for reasoning. The key idea is that reasoning involves long generated sequences relatively to the prompts, compared to standard tasks / benchmark where the generated part is short or nonexistent for multi-choice. Hence, using the prompts themselves as \"calibrating data\" to prune is misguided and performance can be improved by using generated sequences as calibrating data. Experiments show that it works very well."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method is simple, straightforward and comparisons to other methods from that domain demonstrate superior performance."}, "weaknesses": {"value": "The proposed method is too simple and the contribution too limited. It boils down to saying \"use the data of your task to calibrate pruning for your task\".\n\nNow, model compression is not a domain I know well and it is easy to have an illusion of triviality post-hoc.\n\nAbout the form of the paper, some sections are just loading pages with equations which are in my opinion needless and give an impression of gratuitous filling to look impressive."}, "questions": {"value": "I do not have question, but I am curious to see if other reviewers who know the field better confirm that this method is not already well known and used in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TC9aeb10dl", "forum": "tyGfwG6xTh", "replyto": "tyGfwG6xTh", "signatures": ["ICLR.cc/2026/Conference/Submission11449/Reviewer_hbbN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11449/Reviewer_hbbN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854003575, "cdate": 1761854003575, "tmdate": 1762922560500, "mdate": 1762922560500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reasoning-Aware Compression, a pruning method for reasoning language models that maintains high accuracy while improving efficiency. Unlike standard pruning, which calibrates using only prompt signals, RAC aligns pruning with decoding behavior by incorporating internal signals from both the prompt and the model’s own chain of thought. This alignment ensures pruning reflects how the model actually reasons during generation. Experiments show that RAC-pruned models produce shorter, cleaner reasoning chains and retain accuracy close to the dense baseline- a 7B model at 50% sparsity maintains about 90% accuracy, compared to 74.4% with standard calibration."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Aligns pruning with chain-of-thought activations, effectively preserving reasoning quality. The input column to be considered for pruning considers not just the provided input prompts tokens but aslo the LLM policy's self generated reasoning CoT tokens, which aligns the model with what it would do during inference time.\n\n2. Achieves up to 95% dense-model accuracy at 50% sparsity with no retraining.\n\n3. Simple, plug-and-play method compatible with existing SparseGPT or WanDA approaches."}, "weaknesses": {"value": "1. How do you pre-calculate $\\mathcal{D}_M$ which tells howmany times you would self-generate tokens from the model?\n2. While the paper has good rigor the presentation and notations can be well-defined, especially the core part in page 4 and 5. Eg. what is $N_P$?"}, "questions": {"value": "1. Is there any study on how the pruning varies with the CoT quality. With long reasoning traces, many times we observe the model has a long approach then backtracks and then corrects itself etc to get the output. Did you observe any specific behaviors there?\n2. How would this method generalize to other tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J3hOk0t7bN", "forum": "tyGfwG6xTh", "replyto": "tyGfwG6xTh", "signatures": ["ICLR.cc/2026/Conference/Submission11449/Reviewer_bqFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11449/Reviewer_bqFF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952542458, "cdate": 1761952542458, "tmdate": 1762922559907, "mdate": 1762922559907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}