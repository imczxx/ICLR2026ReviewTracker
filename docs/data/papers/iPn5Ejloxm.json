{"id": "iPn5Ejloxm", "number": 11911, "cdate": 1758204612874, "mdate": 1759897546743, "content": {"title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier", "abstract": "Complex reasoning with Large Language Models (LLMs) demands a careful balance between accuracy and computational cost. Verification, crucial for reliability, exacerbates this challenge. Existing methods often force a stark trade-off: robust process-based verifiers incur prohibitive costs due to iterative recomputation, while fast, efficient verifiers suffer from low precision. We introduce flexive, a unified generative verifier designed to navigate this trade-off. FlexiVe dynamically allocates compute between rapid \"fast thinking\" and deliberative \"slow thinking.\" A key innovation is our training strategy: we use Reinforcement Learning (GRPO) to specifically enhance the reliability of the fast mode. Remarkably, this targeted training generalizes, elevating the slow mode to state-of-the-art performance. To optimally deploy flexive, we propose the solve-detect-verify (SDV) pipeline. SDV moves beyond static Best-of-N ranking, employing an efficient iterative refinement process that detects solution completion to curtail \"overthinking\" and uses flexive’s feedback for targeted correction. Our results demonstrate significant improvements in both accuracy and efficiency. flexive establishes a new open-source state-of-the-art on ProcessBench, outperforming the much larger GenPRM-32B while requiring ~2.3x fewer TFLOPS with 15x less training data. On the challenging AIME 2024 benchmark, the full SDV pipeline achieves 83.3% accuracy, surpassing strong baselines.", "tldr": "", "keywords": ["Large Language Models", "LLM Reasoning", "Inference Time Scaling", "Test Time Scaling", "Token Efficiency", "Reinforcement Finetuning", "Verification", "Thinking Fast and Slow"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/236432358408701ce93737e4dc4753be75b1d696.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work advances the performance of generative reward models.\nTo be more precise, the authors propose FlexiVe, a method that utilizes several fast evaluations of the output to identify the position of the first mistake.\nFollowed by a detailed (full thinking) evaluation in case of a lack of majority agreement.\nThe authors also propose a Solve-Detect-Verify framework that aims to detect and quickly check early solutions provided by the model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ comparison against GENPRM, majority voting, and thinking models without the reward model.\n+ quantification of the important problem with current reasoning models --- overthinking (models search for additional solutions, and potential mistakes for a significant amount of time)\n+ advancing the frontier of generative PRMs\n+ analysis of statistical significance\n+ quantification of RL vs supervised tuning"}, "weaknesses": {"value": "+ minor: as noted by the authors, the current approach to overthinking detection can fail to generalize, as it is mostly based on keyword detection\n+ minor (as I understand it is  not the case for AIME) benchmark problem: f1 score can be noisy"}, "questions": {"value": "1. How are model responses compared with ground truth on AIME?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NV0Kd7KRIk", "forum": "iPn5Ejloxm", "replyto": "iPn5Ejloxm", "signatures": ["ICLR.cc/2026/Conference/Submission11911/Reviewer_fHBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11911/Reviewer_fHBF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761064063035, "cdate": 1761064063035, "tmdate": 1762922919619, "mdate": 1762922919619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a solve-detect-verify paradigm for inference scaling on math problems. It first generates a solution, then employ a verifier to detect the first error and give the feedback to the generator to improve the solution. The proposed pipeline achieves SOTA step correctness prediction and leads to better accuracy on math becnhamarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method shows a certain level of novelty, especially in the engineering side. For example, the fast/slow thinking parts.\n2. Empirical evaluation shows a consistent improvement against existing models. The efficiency analysis is also valuable."}, "weaknesses": {"value": "1. The proposed method mostly relies on the hand-crafted hack, which **lacks a clear justification/ablation** and shows few contributions to the machine learning side. \n- For example, it remains unclear to me why you need to predict the first error step index in an auto-regressive way. As the simplest approach, you can just train a scalar model to output the error probability on the end token of each step.\n- Second, the completion assessment and hesitation words are more like **hacking of DeepSeek-R1 (the solver model)** instead of a generalizable method. Here I quote the way how hesitation words are defined.\n> These keywords were derived empirically by observing common phrases signaling a pause or self-correction in LLM outputs.\nHowever, there lacks any evidence these tricks are generalizable.\n\n2. The empirical setup is highly problematic.\n - **Both the training data and base model are different from the ones used by baselines**, it is very hard to draw any conclusion about the true advantage of the proposed method here.\n- **Only the DeepSeek-R1 model is used as the solver LLM**, what about other (e.g., Llama) models? \n3. The paper presentation should be significantly improved. The Table 1 is not referred in the main text (it seems that the reference to Table 8 should be Table 1). Similarly, in section 3.2, Figure 2 should be Figure 3. Figure 5 and Figure 6 mixes too many sub-figures, making them hard to read."}, "questions": {"value": "- The results in Figure 6 top left is not convincing to me. BoN usually performs better than majority voting, but this results suggests the opposite way. Why don't you use the same training data  and base model to train a process reward model or even just an outcome reward model? \n- Except Best-of-N, how does your method compare to **weighted majority voting**?\n- Have you tried Llama models, both as the solver and the verifier's base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ue9uO8ZAli", "forum": "iPn5Ejloxm", "replyto": "iPn5Ejloxm", "signatures": ["ICLR.cc/2026/Conference/Submission11911/Reviewer_XWVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11911/Reviewer_XWVa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761279199694, "cdate": 1761279199694, "tmdate": 1762922919226, "mdate": 1762922919226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on improving LLM reasoning via scaling test-time compute, specifically via the use of Generative Reward Models (GenRMs). Prior work has shown that GenRMs, while improving performance, can be costly and inefficient, hindering their applicability in practical scenarios. To this end, this paper proposes an approach that significantly improves their efficiency, making it a useful contribution. The paper has two main contributions: (a) FlexiVe, a flexible verifier that can switch between thinking fast and slow, and performs well while using fewer tokens, and (b) a pipeline to iteratively improve model solutions by incorporating feedback from the verifier, which is better than Best-of-N both in terms of performance and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a new verifier that can flexibly switch between thinking fast and slow, and an approach to decide when to think longer based on the difficulty of the problem. This improves both performance and efficiency.\n1. The paper also contributes an approach to incorporate the verifier in the overall pipeline. Prior works typically use the verifier to select the best out of N solutions (known as best-of-N). They propose a new pipeline called Solve-detect-verify, which iteratively refines the solution based on the verifier's feedback. This performs better than Best-of-N while using fewer tokens.\n\nThe first contribution is novel, and both contributions are of practical importance as they improve overall reasoning performance."}, "weaknesses": {"value": "The idea of iterative refinement is not entirely new (for example, [1]), which affects the novelty of the paper.\n\nWriting is confusing at some places:\n1. Line 165, 167 – this could be elaborated. Maybe talk about the architectures considered in the paper, and also explain what a process-based reward model is (maybe contrast with outcome reward model).\n2. The paper states “fast thinking” is the same as “no think”, but in “no think”, there should be no reasoning trace and the model should directly output the answer. That doesn’t seem to be the case. Either the “fast thinking” stage shouldn’t involve a reasoning trace, or it shouldn’t be confused with “no think”?\n3. How does the model switch between fast and slow thinking modes? Is it a different prompt? Or do you apply some kind of budget forcing? It would be good to clarify this in the main paper.\n4. Table 1 is not referenced in the paper anywhere.\n5. Figure 4a and 4b have different y labels (one is performance, the other is score). Is this intentional? If yes, what do you mean by each term? In any case, it would be good to specify exactly which metric is being talked about (F1, success rate, something else?)\n6. Figure 6 (bottom) could use more details. Which dataset is it for? Is it for a single solution or multiple solutions? \n\n[1] RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, Niket Tandon. ACL 2023."}, "questions": {"value": "1. Line 373: isn’t it better to use the think mode@2? It seems to be better both in terms of FLOPs and performance. Why do you recommend using Flex@8?\n2. Figure 6 left and right are not consistent. The performance at N = 2 for AIME2024 should be the same in both plots, unless I’m missing something. Why is that?\n3. In Figure 6 bottom, it looks like the model used 8K tokens in the first iteration and then only ~2K tokens in the second iteration. Why does it use far fewer tokens in the second iteration? Do you control the number of tokens or does it stop after 2K tokens on its own?\n4. Why did you choose to train on the Big bench mistake dataset? Why not other datasets like PRM800K? Maybe this dataset is the reason why your verifier performs better than previous verifiers?\n5. How does your verifier compare to other RL-trained verifiers like ThinkPRM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "djUMDOAQNG", "forum": "iPn5Ejloxm", "replyto": "iPn5Ejloxm", "signatures": ["ICLR.cc/2026/Conference/Submission11911/Reviewer_13p9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11911/Reviewer_13p9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975334958, "cdate": 1761975334958, "tmdate": 1762922918833, "mdate": 1762922918833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the problem of verifying LLMs' reasoning outputs (CoTs). Specifically, the authors seek for to understand to perform such verifications more efficiently by controlling the trade-off between accuracy and efficiency. They first propose FlexiVe, which is a single verifier model that takes the whole reasoning trace as input (a holistic verifier) with different modes. FlexiVe has the original thinking mode that produces the full thoughts for verification as well as the no-thinking (fast) mode that skips the thought generation by simply using a placeholder (\"Okay, I think I have finished thinking.\") as its thought, where FlexiVe is a model additionally fine-tuned for this no-thinking mode. By leveraging these two modes, the authors propose a hybrid mode named Flex. In the Flex mode, it first generates k outputs with the no-thinking mode and then computes the agreement ratio (the ratio of the most frequent answer), which is thresholded to decide between using it as the final answer (a high-consensus case) or generating the final answer with the thinking mode (a low-consensus case). In addition to FlexiVe, the authors suggest Solve-Detect-Verify as a framework to generate and iteratively improve LLMs' reasoning. In its Solve-Detect stage, the \"solver LLM\" generates the solution step-by-step until it reaches the end of the sequence, or there is a hesitation keyword detected and the LLM token probability for \"Yes\" is larger than the one for \"No\" after a prompt that questions the completeness of the current solution. In the Verify-and-Refine stage, in each iteration, FlexiVe checks the correctness of the solution and provides feedback for refining the answer if it is considered incorrect. Empirically, the authors employ ProcessBench for evaluating FlexiVe and other benchmarks such as AIME 2024 and 2025 for evaluating Solve-Detect-Verify and suggest that FlexiVe and Solve-Detect-Verify can be effective verification and iterative refinement approaches with efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Reasonable high-level approach  \nIt can be a reasonable high-level approach to first perform an efficient inference and then conditionally escalate it to a more computationally intensive inference, depending on the difficulty of the problem. With this theme, I think the proposed approach conceptually makes sense.\n\n2. Comprehensive information and reproducibility  \nThe authors provide most of the details of their approach. They present not only various statistics from the experimental results but also the prompts they used and qualitative examples. Especially, they also release the source codes, which can make this work more reproducible. This is especially important, as publishing open-source verifiers could lead to the development of other language models in the same domain."}, "weaknesses": {"value": "1. Scalability of the Flex mode  \nFor FlexiVe, the authors use the agreement ratio given k answers generated with the no-thinking mode as a consensus to determine whether to proceed with the original thinking mode or stick to the answer generated with the no-thinking mode. They describe that a high consensus \"signals a straightforward case\" (L216). While I agree that this could roughly hold, I have a concern that it may not scale well. Specifically, using a high consensus as a condition to not proceed with the thinking mode can impose a non-negligible upper limit to the verification performance, because the weaker mode (no-thinking mode) having a high consensus (or high confidence) doesn't necessarily yield as accurate answers as the stronger mode (thinking mode). If we examine Table 6, varying k for NoThinking@k, the plateau is formed at a noticeably lower performance group compared to Think@k (Table 5). Besides, comparing Tables 5 and 6 again, the performance improvement going from k=2 to k=128 with NoThinking@k is smaller than the improvement with Think@k and the same settings, on every benchmark.  \nMore importantly, from Tables 5 and 7, it is observed that the efficiency-performance benefit with Flex@k disappears after some scaling. For instance, on every benchmark, the F1 score with Flex@128 is lower than the one with Think@k.\n\n2. Inconsistencies in and concerns about the main experimental results  \nMy understanding is that Figure 5 and Table 9 are supposed to represent the same data (the experimental results on the MATH dataset from ProcessBench). However, it appears that the numbers in Table 9 and the plots in Figure 5 don't match well. If we assume that Figure 5 is correct, then Flex@8 uses more compute and results in a lower F1 score than Think@2.  \nBesides, I notice non-negligible inconsistencies (in F1 scores) between Table 9 and Tables 5, 6, and 7.  \nIn addition, I believe DeepSeek-R1-Distill-Qwen-14B's pass@1 performance on AIME 2024 is reported as 69.7, whereas in this work, the same solver model's performance looks much worse (e.g., Figure 6 and Figure 12).\n\n3. Other presentation issues  \n- While L296 says \"For the full Solve-Detect-Verify, we evaluate end-to-end task accuracy and efficiency on challenging mathematical datasets: AIME (2024, 2025) (Aim, 2024; 2025), AMC, CNMO (Liu et al., 2024), and OlympiadBench,\" it looks to me that only AIME 2024 and 2025 are used as the primary benchmarks for evaluating Solve-Detect-Verify. Other than AIME 2024 and 2025, I notice some occurrence of CNMO in Figure 12.\n- In the \"Open Source Models (14-32B) w/ Moderate Compute\" part of Table 1, the bold-facing and underlining are wrong."}, "questions": {"value": "1. Was there a specific reason to use only the MATH dataset for the analyses in Section 4.3?\n2. Can you explain the solver's performance on the evaluation benchmarks, such as AIME 2024, especially regarding my concern above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IFM7Jxqa4g", "forum": "iPn5Ejloxm", "replyto": "iPn5Ejloxm", "signatures": ["ICLR.cc/2026/Conference/Submission11911/Reviewer_QqbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11911/Reviewer_QqbF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143989582, "cdate": 1762143989582, "tmdate": 1762922918400, "mdate": 1762922918400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}