{"id": "W3QN1SERzH", "number": 16480, "cdate": 1758264994736, "mdate": 1759897238156, "content": {"title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models", "abstract": "Vision-language models (VLMs) have shown remarkable progress in offline tasks such as image captioning and video question answering. However, real-time interactive environments impose new demands on VLMs, requiring them to generate utterances that are not only semantically accurate but also precisely timed. We identify two core capabilities necessary for such settings---$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$---and propose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation (TGLG)}$, to evaluate them. TGLG requires models to generate utterances in response to streaming video such that both content and timing align with dynamic visual input. To support this benchmark, we curate evaluation datasets from sports broadcasting and egocentric human interaction domains, and introduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring semantic similarity and temporal alignment. Finally, we present $\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$, a model that interleaves visual and linguistic tokens in a time-synchronized manner, enabling real-time language generation without relying on turn-based assumptions. Experimental results show that VLM-TSI significantly outperforms a strong baseline, yet overall performance remains modest---highlighting the difficulty of TGLG and motivating further research in real-time VLMs.", "tldr": "We introduce TGLG, a benchmark for evaluating vision-language models in real-time video settings that require temporally grounded, context-aware language generation.", "keywords": ["temporally grounded language", "vision-language models", "streaming video", "real-time video"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a19adc862d7429e770a9c37bb3453c29022a8428.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Temporally-Grounded Language Generation (TGLG), a benchmark designed to evaluate the performance of real-time video language models (VLMs) that must produce time-synchronized responses while processing streaming video input. The benchmark builds on two existing datasets, SoccerNet (Cioppa et al., 2022), featuring sports broadcast videos, and HoloAssist (Wang et al., 2023), comprising egocentric human–object interaction videos. TGLG is organized around two complementary tasks: perceptual updating, which measures how proactively a model responds to evolving visual input, and contingency awareness, which assesses how effectively it provides timely, context-appropriate feedback. The authors also propose TRACE, a metric that jointly evaluates semantic similarity and temporal alignment between predicted and ground-truth utterances. The benchmark primarily evaluates VideoLLM-Online (Chen et al., CVPR 2024) and a modified variant that omits the end-of-sentence (EOS) token to avoid silence gaps. Experimental results show modest but consistent improvements with this modification."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper targets an important and timely problem of evaluating real-time multimodal reasoning in VLMs. As such, it contributes to an emerging and rapidly evolving research direction.\n- The use of two complementary datasets with distinct characteristics (third-person sports vs. first-person human interaction) is a thoughtful design choice that enhances the benchmark’s generality. The inclusion of cross-dataset evaluation (HoloAssist is used just for testing) further strengthens its robustness.\n- The benchmark provides fine-grained task/action groupings, allowing analysis of model behavior under varying temporal and semantic demands. This granularity can help identify where current systems struggle with temporal grounding."}, "weaknesses": {"value": "- As a benchmark paper, the experimental evaluation is rather limited. The authors only assess two versions of VideoLLM-Online, omitting several strong recent baselines such as Stream-VLM (Panchal et al., 2024), FlashVStream (Zhang et al., 2024), Dispider (Qian et al., 2025), StreamChat (Xiong et al., 2025), and StreamChat (Liu et al., 2025). Including or at least discussing results from these models would significantly strengthen the empirical validation.\n- The related work section does not sufficiently situate TGLG within the landscape of contemporary benchmarks such as QEVD (Panchal et al., 2024), OmniMMI (Wang et al., CVPR 2025), and OVO-Bench (Niu et al., 2025). A detailed comparison would clarify the unique contribution and scope of TGLG.\n- The proposed model modification (ignoring the EOS token) is minimal and should not be framed as a novel modeling contribution.\n- The TRACE metric introduces several empirically tuned hyperparameters, which raises concerns about reproducibility and interpretability. Furthermore, it is unclear whether a joint score is preferable to reporting separate measures of semantic and temporal alignment (see related question below).\n- The presentation could be improved, especially in sections explaining motivation and task setup (e.g., lines 139–147 and 256–269, and Figure 1). Given the multimodal nature of the work, clearer visual-textual illustrations would aid reader understanding. Moreover, the related work section omit a detailed discussion of most related models and benchmarks.\n\nMinor Issues:\n- Watch, Talk and Guide (Bao et al., 2023) appeared in Findings of EMNLP 2023.\n- StreamBench (Xiong et al., 2025) appeared in ICLR 2025.\n\n\n**Missing References**\n- Liu et al. StreamChat: Chatting with Streaming Video. ArXiv Preprint arXiv:2412.08646v, March 2025.\n- Niu et al. OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding? CVPR 2025\n- Panchal et al. What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction. NeurIPS 2024 Track on Datasets and Benchmarks. \n- Qian et al. Dispider: Enabling video LLMs with active real-time interaction via disentangled perception, decision, and reaction. CVPR 2025.\n- Wang et al. OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts. CVPR 2025."}, "questions": {"value": "1. The benchmark currently evaluates two architecturally identical VLMs. How might differences in inference latency or token generation rate across models affect real-time responsiveness and TRACE scores?\n2. The TRACE metric combines semantic and temporal components via a weighted sum. Why not report these two aspects independently, as done in (Panchal et al., 2024)?\n3. To strengthen the empirical evaluation, please consider including additional recent real-time VLMs or at least discussing their performance relative to TGLG’s objectives. Similarly, a comparative discussion with existing streaming benchmarks (see Weaknesses) would help clarify the distinct contributions of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dS1YnsPDK5", "forum": "W3QN1SERzH", "replyto": "W3QN1SERzH", "signatures": ["ICLR.cc/2026/Conference/Submission16480/Reviewer_f4Ef"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16480/Reviewer_f4Ef"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760955489227, "cdate": 1760955489227, "tmdate": 1762926582782, "mdate": 1762926582782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new task and benchmark for Temporally-Grounded Language Generation (TGLG), which aims to incorporate two core capabilities of perceptual updating and contingency awareness into real-time interactive video understanding settings. Specifically, this work builds two subsets based on real-time soccer game commentary and egocentric human interaction videos, and a new temporally synchronized token interleaving strategy is proposed to tackle the new challenges. Quantitative experiments are conducted to show the effectiveness of the proposed method and the benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The motivation of incorporating the capabilities of perceptual updating and contingency awareness into real-time video-llms is practical and intuitively reasonable.\n\n2.The proposed benchmark, metric and method in this work are shown to be effective under real-time settings."}, "weaknesses": {"value": "1.As the authors mentioned in the manuscript, the existing turn-based video-llms would give response to the environment with overly high latency, which is a major obstacle for them to handle the real-time settings. However, these video-llms are generally at a large size and have huge amout of parameters which naturally make them unsuitable for real-time response. What if the turn-based video-llms are optimized to have fewer parameters and faster response speed, for example, turn-based models could also generate response promptly if they can finish decoding before new frames come in. Are there any discussions or experiments to analyze this aspect?\n\n2.For the proposed temporally synchronized vision and text token interleaving strategy, it is shown to be effective to generate real-time response conditioned on fastly evolving visual environments. But based on my understanding, such highly fragmented token mixing method would inevitably destroy the coherence of the visual input and also the textual context, especially when the full off-line inputs are available. So are there any experiments or analysis on the performance of such strategy under a off-line setting? Will it largely decrease the performance of the model under off-line settings?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SOmi5XYGZN", "forum": "W3QN1SERzH", "replyto": "W3QN1SERzH", "signatures": ["ICLR.cc/2026/Conference/Submission16480/Reviewer_sNsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16480/Reviewer_sNsp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998250926, "cdate": 1761998250926, "tmdate": 1762926582308, "mdate": 1762926582308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new task for testing vision-language models in online real-time settings, which is called as Temporally-Grounded Language Generation (TGLG). The proposed benchmark includes sports broadcasting and egocentric videos. Within this work, TRACE, a novel evaluation metric, is also introduced to evaluate models on the TGLG benchmark. Furthermore, the authors introduce VLM-TSI, which enables processing interleaved vision and language tokens in a time-synchronized way, later tested on the repurposed benchmark using the proposed metric TRACE. The downstream task experiments reveal that the further finetuning the baseline using VLM-TSI approach on SoccerNet and HoloAssist datasets improve real-time spatio-temporal processing based on the proposed metric."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Research on an interesting direction even within the domain of spatio-temporal vision-language learning: time-synchronized processing in real-time scenarios.\n- Substantial improvements over the baseline."}, "weaknesses": {"value": "- No human validation study on how the proposed metric aligns with human preferences.\n- I'm confused about this work's purpose. It seems that the data resources already exist, and VLM-TSI method actually proposes finetuning a suitable model on these resources. According to my current understanding, the task is actually not novel, the methodology is actually to finetune a proper model on time-synchronized interleaved video-language data. The only actual novelty is the proposed metric, which is not evaluated.\n- Dataset-specific finetuning: It appears that this work performs two separate finetuning on two separate downstream datasets. It would be good to expand the experiments considering both data resources simultaneously.\n- Limited evaluation: There is only one baseline in the current evaluation setup. I am aware the fact that the other models would not be good baselines as suggested by authors. However, expanding evaluations to more general zero-shot video-language benchmarks (e.g., Video-MME) could be beneficial. For instance, did the model become better in spatio-temporal processing after learning more real-time dynamics?"}, "questions": {"value": "- L428-L431: Could this be more related to models finding shortcuts through patterns? Penalty position could be detected by using single frame, no spatio-temporal processing is required actually.\n- Fig. 1, and Fig. 4 could be combined to explain the task in a better way. Same goes for Fig 2. and Fig 3., they do not need to be separate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LdEYf4hwwf", "forum": "W3QN1SERzH", "replyto": "W3QN1SERzH", "signatures": ["ICLR.cc/2026/Conference/Submission16480/Reviewer_TqVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16480/Reviewer_TqVq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024043974, "cdate": 1762024043974, "tmdate": 1762926581996, "mdate": 1762926581996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}