{"id": "I69SaLbwqZ", "number": 4159, "cdate": 1757615728017, "mdate": 1763672869753, "content": {"title": "Distribution-informed Online Conformal Prediction", "abstract": "Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are i.i.d. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.", "tldr": "We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule with provable coverage and tighter intervals.", "keywords": ["Conformal Prediction; Uncertainty Quantification; Distribution Shift; Time Series"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4117cf413838db609fa254e5b3580d348ab9ee48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Conformal Optimistic Prediction (COP), an online conformal prediction algorithm designed to address the over-conservatism of existing methods in non-stationary environments like time series. \n\nThe core innovation of COP is a two-step update mechanism that refines the standard gradient-based adjustment for the prediction set size. After a primary update based on the current miscoverage error, COP applies a refinement step using an estimated cumulative distribution function (CDF) of the non-conformity scores. This estimated CDF acts as an \"optimistic hint\" about the data's underlying patterns, enabling the algorithm to produce tighter, more efficient prediction sets when such patterns exist. \n\nCrucially, the method is robust; it maintains provable, distribution-free, finite-sample coverage guarantees even if the CDF estimate is inaccurate. The authors frame COP within the Optimistic Online Gradient Descent (OOGD) paradigm, deriving a novel joint bound on regret and coverage that clarifies its theoretical motivation. \n\nExperiments on synthetic datasets with distribution shifts and real-world time series from finance, energy, and climate show that COP consistently achieves the target coverage while generating significantly narrower prediction intervals than state-of-the-art baselines, demonstrating its superior efficiency and practical utility."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "*   **Originality**\n    *   Presents a new synthesis of online Conformal Prediction (CP) with Optimistic Online Gradient Descent (OOGD).\n    *   Uniquely frames distributional information as a proactive \"optimistic hint\" rather than simply replacing the robust reactive update rule.\n\n*   **Quality**\n    *   **Comprehensive Theory:** Provides a full suite of rigorous guarantees, including a novel joint regret-coverage bound, essential distribution-free finite-sample coverage, and asymptotic consistency, establishing strong reliability.\n    *   **Thorough Experiments:** Validated on a wide range of synthetic and diverse real-world datasets, using multiple base models against a strong set of seven state-of-the-art baselines, convincingly demonstrating its superior performance and general applicability.\n\n*   **Clarity**\n    *   The paper is exceptionally well-written, with a clear logical flow that makes complex ideas accessible.\n    *   **Excellent Background Explanation:** The background section effectively explains all necessary concepts, providing the context needed to fully appreciate the paper's contribution.\n\n*   **Significance**\n    *   Addresses the critical and practical problem of over-conservatism (excessively wide intervals) in online CP, a major barrier to real-world adoption.\n    *   The proposed method (COP) is effective, theoretically sound, and simple to implement, giving it high potential to become a standard practitioner's tool.\n    *   The theoretical framework introduces new analytical tools that can inspire future research."}, "weaknesses": {"value": "1.  **Lack of Statistical Significance in Experimental Claims:** While the experiments are extensive, the tables present only point estimates (mean and median) for the evaluation metrics from a single run. This makes it difficult to ascertain whether the observed improvements of the proposed method over baselines are statistically significant or merely due to experimental randomness. For example, a small difference in average interval width between two methods might not be meaningful without a measure of variance. The empirical claims would be substantially strengthened by including standard errors or confidence intervals for the reported metrics, which could be derived from multiple runs with different random seeds.\n\n2.  **Over-reliance on Tables for Presenting Results:** The paper relies heavily on large, dense tables (Table 1 and 2) to display the primary experimental outcomes. Such a presentation format makes it challenging for readers to quickly parse results and identify performance trends across different datasets and models. The paper's readability and impact would be greatly enhanced by using visualizations. For instance, it is suggested that the authors consider replacing or supplementing the tables in the main text with figures like **boxplots**. Boxplots would more effectively illustrate the distribution, median, and spread of the prediction interval widths, while also providing a natural way to incorporate the standard error or uncertainty estimates mentioned in the first point, thereby increasing the perceived reliability and clarity of the experimental validation. The detailed tables could then be moved to the Appendix for reference."}, "questions": {"value": "**On the Limits of the CDF Estimator's Robustness:**\n\nA major strength of the paper is that COP maintains coverage guarantees even with an inaccurate CDF estimate. However, a poor estimate could still harm efficiency, potentially making the intervals wider than those from a simpler baseline like OGD.\n\nCould you discuss the practical boundaries of this robustness? Is there a point where a consistently misleading CDF estimate (e.g., during a prolonged, adversarial shift) makes COP perform worse than the baseline OGD in terms of interval width?\nAn experiment on a synthetic dataset with a deliberately miscalibrated or noisy CDF estimator could help characterize this failure mode and provide practical guidance on when the optimistic step is most beneficial.\n\n**On Statistical Significance of Experimental Results:**\n\nThe experimental results in the tables are promising, but they report point estimates from what appears to be a single experimental run. To strengthen the claims of superiority, especially when the performance differences with top baselines are small, could you provide measures of uncertainty for the reported metrics (e.g., standard errors or confidence intervals calculated over multiple runs with different random seeds)? This would be crucial for confirming that the observed improvements are statistically significant.\n\n**On the Impact of Noise and Low Predictability in Scores:**\n\nCOP's advantage stems from exploiting predictable patterns in the non-conformity scores. This raises questions about its performance in settings where such patterns are weak or heavily obscured.\n\nHow does COP perform in high-noise or low signal-to-noise ratio settings, where the underlying predictable structure in the scores is minimal? In such a scenario, does the optimistic update, which is based on a noisy and potentially uninformative CDF estimate, risk making spurious adjustments that could degrade efficiency (i.e., widen intervals) compared to a more conservative baseline like decay-OGD?\nIt would be very illuminating to include an experiment on a synthetic dataset where the level of i.i.d. noise added to the non-conformity scores is systematically varied. This would help to characterize the performance trade-offs and define the boundaries of where COP's optimism is a clear advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0uBCDG41iW", "forum": "I69SaLbwqZ", "replyto": "I69SaLbwqZ", "signatures": ["ICLR.cc/2026/Conference/Submission4159/Reviewer_nxNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4159/Reviewer_nxNU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760792618579, "cdate": 1760792618579, "tmdate": 1762917205448, "mdate": 1762917205448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Main Responses"}, "comment": {"value": "We are grateful to the engaged reviewers, who took a clear interest in the paper and suggested ways to improve it. Thank you\\!\n\n**New Content of Revised Manuscript:**\n\nWe have presented a revised version, including discussions on the \"same-sign\" assumption  (Appendix A.3), experimental results under three other simulation datasets (Appendix E), Post-shift coverage recovery time (Appendix F), robustness of inaccurate estimated CDF (Appendix G), sensitivity analysis of the scale factor (Appendix H), statistical significance analysis (Appendix I), computational complexity analysis (Appendix J), visualization of coverage and interval (Appendix K).\nWe hope to respond to the critical comments through the following extensive revisions:\n\n**1\\. Analysis of Computational Overhead：**\n\nThe simplest gradient-based methods (OGD and decay-OGD) are the fastest, requiring only 0.001 ms per step. Methods that consider all past data, such as SF-OGD and PID, incur higher overhead (around 0.012 – 0.013 ms). Methods that consider the past data in a certain window, such as ECI, LQT, and COP fall in a similar range (0.007 – 0.011 ms). Overall, the differences between methods remain small in absolute terms, and all methods run easily in real time for typical streaming applications.\n\nWe also observe that as $w$ increases (from 0 to 500), the per-step runtime remains largely stable. Due to the high efficiency of NumPy's vectorized operations, the incremental computational cost associated with $w$ is negligible compared to the fixed overhead of the Python interpreter. To support this, we have added a table comparing COP’s per-step time with other baselines below. It is shown that the slight overhead is well-justified by COP’s superior performance of tighter prediction sets with valid coverage. The details can be seen in Appendix J in our revised version.\n\n| Method | ACI | OGD | SF-OGD | decay-OGD | PID | ECI | LQT | COP |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Time Cost (ms) | 0.043 | 0.001 | 0.012 | 0.001 | 0.013 | 0.007 | 0.010 | 0.011 |\n\n**2. Robustness of Inaccurate Estimated CDF:**\n\nWe have added experimental results of COP with $\\\\hat{F}\\_t$ deliberately corrupted by noise. Specifically, at each time step, we construct a noisy ECDF by  $  \\\\hat{F}\\_{noisy} \\= \\\\gamma\\\\hat{F} \\+ (1-\\\\gamma)\\\\epsilon,  $\nwhere $\\\\hat{F}$ is the ECDF, $\\\\epsilon \\\\sim U(0,1)$ denotes a uniform random noise, and $\\\\gamma \\\\in \\\\{1, 0.9, 0.5, 0.1, 0\\\\}$ controls the level of the corruption. \n\nThe table below shows that the coverage rate remains stable across all values of $\\\\gamma$, even when the CDF is fully replaced by uniform noise ($\\\\gamma=0$). **It coincides with our theoretical results of theorem 2 that COP achieves long-term coverage regardless of misspecification of $\\\\hat{F}\\_t$.** As for the width, we note that COP consistently performs better than SF-OGD when taking $\\\\gamma=1, 0.9, 0.5$, and exhibits comparable performance when taking $\\\\gamma=0.1$. Only when the CDF is fully replaced by uniform noise ($\\\\gamma=0$) does SF-OGD outperform COP. The details can be seen in Appendix G  in our revised version.\n\n| Dataset | γ | Prophet |  |  | AR |  |  | Theta |  |  |\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n|  |  | Coverage (%) | Average width | Median width | Coverage (%) | Average width | Median width | Coverage (%) | Average width | Median width |\n| Amazon Stock | 1.0 | 89.6 | 39.86 | 27.91 | 89.5 | 17.09 | 12.90 | 89.6 | 17.21 | 12.23 |\n|  | 0.9 | 89.6 | 38.95 | 27.85 | 89.3 | 17.14 | 12.71 | 89.6 | 17.45 | 13.12 |\n|  | 0.5 | 89.7 | 42.24 | 29.56 | 89.4 | 17.24 | 13.08 | 89.6 | 17.20 | 12.76 |\n|  | 0.1 | 90.1 | 46.93 | 30.89 | 89.2 | 17.45 | 12.44 | 89.6 | 17.60 | 12.73 |\n|  | 0.0 | 90.0 | 62.65 | 53.09 | 89.7 | 22.07 | 18.26 | 89.5 | 30.76 | 28.56 |\n| Google Stock | 1.0 | 89.7 | 49.72 | 42.09 | 89.6 | 19.87 | 17.04 | 89.3 | 30.25 | 28.24 |\n|  | 0.9 | 89.8 | 49.99 | 42.06 | 89.6 | 19.81 | 16.92 | 89.4 | 30.43 | 27.79 |\n|  | 0.5 | 90.0 | 54.57 | 46.08 | 89.6 | 19.94 | 17.33 | 89.4 | 29.82 | 27.27 |\n|  | 0.1 | 90.0 | 61.25 | 51.91 | 89.6 | 19.95 | 17.23 | 89.5 | 30.68 | 28.18 |\n|  | 0.0 | 90.8 | 86.26 | 55.43 | 90.0 | 30.04 | 20.32 | 89.9 | 30.24 | 20.76 |\n\n**3\\. Impact of Noise and Low Predictability of Scores:**\n   \nTo better assess the robustness of COP's CDF-based refinement, we added experiments in three other simulation datasets under variance changepoint, heavy-tailed noise, and extreme distribution drift setting, respectively. In the tables below, we reported coverage, width and recovery time across these settings. We note that COP performs comparably to OGD and consistently outperforms other baselines in most cases, indicating the robustness. The details can be seen in Appendix E in our revised version."}}, "id": "DAbzBgl9ch", "forum": "I69SaLbwqZ", "replyto": "I69SaLbwqZ", "signatures": ["ICLR.cc/2026/Conference/Submission4159/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4159/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4159/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763668831781, "cdate": 1763668831781, "tmdate": 1763668831781, "mdate": 1763668831781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an online conformal prediction algorithm that incorporates distributional information of non-conformity scores into the update rule. The proposed method leverages an estimated cdf of scores to anticipate predictable patterns and tighten prediction sets while preserving coverage guarantees. By establishing a connection between the proposed method and OOGD, authorss demonstrate a joint bound on regret and coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1- The idea of incorporating distribution-informed optimistic updates into online conformal prediction is a meaningful advancement in this area.\n\n2- The paper provides solid theoretical foundations for both coverage and regret.\n\n3-The paper is well written and easy to follow, with clear motivation and smooth storytelling that connects prior work to the proposed method."}, "weaknesses": {"value": "Please check the questions."}, "questions": {"value": "1- How were the hyperparameters chosen for COP?\n\n2- Could the authors include a computational complexity analysis of the proposed method and a comparison with previous baselines, such as ACI?\n\n3- Would authors show empirically whether performance degrades significantly with poor CDF estimation or not?\n\n4- Could the authors clarify whether $q_t$ in line 121  is a type or not? It's not consistent with eq(2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iVY3gXrmMW", "forum": "I69SaLbwqZ", "replyto": "I69SaLbwqZ", "signatures": ["ICLR.cc/2026/Conference/Submission4159/Reviewer_LFYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4159/Reviewer_LFYw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546075876, "cdate": 1761546075876, "tmdate": 1762917204981, "mdate": 1762917204981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Conformal Optimistic Prediction (COP), an online conformal prediction (CP) algorithm that integrates distributional information of nonconformity scores into the update rule. COP leverages estimated cumulative distribution functions to refine the prediction radius dynamically. A joint coverage–regret bound and finite-sample distribution-free coverage have been established."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces an elegant refinement step using estimated CDFs. Theoretical contributions include finite-sample and asymptotic coverage guarantees, as well as a joint regret–coverage bound under general learning rates."}, "weaknesses": {"value": "- Although both empirical and kernel-based CDFs are considered, the impact of different estimators or misspecification on performance and validity is not deeply analyzed.\n\n\n- The presentation could be improved for greater readability. For instance, abbreviations should be used consistently once their full forms have been introduced. Moreover, Sections 3.1–3.3 are mathematically dense, which may make it challenging for readers unfamiliar with OOGD or online conformal prediction to grasp the underlying intuition behind each step.\n\n- Computation time of the methods in experiments."}, "questions": {"value": "- How to choose the scale factor in real practice? How sensitive is COP’s performance to the choice of the scale factor?\n\n- In adversarial settings where estimated CDFs are inaccurate, how robust is COP compared with purely adversarial CP methods like SF-OGD?\n\n- How does COP behave under extreme concept drift or abrupt regime changes, where the empirical CDF is no longer representative of recent data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xFpvCcKMZ9", "forum": "I69SaLbwqZ", "replyto": "I69SaLbwqZ", "signatures": ["ICLR.cc/2026/Conference/Submission4159/Reviewer_CEXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4159/Reviewer_CEXt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807710283, "cdate": 1761807710283, "tmdate": 1762917204618, "mdate": 1762917204618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COP, an online conformal method that injects predictable structure via an estimated CDF of nonconformity scores to perform an “optimistic” radius refinement—tightening sets when patterns exist while remaining robust under misspecification. The theory provides a joint coverage–regret bound together with distribution-free, finite-sample coverage under arbitrary learning rates and convergence when scores are i.i.d. Empirically, COP attains target coverage and yields consistently shorter intervals than strong baselines across synthetic shift scenarios and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is original in recasting online conformal prediction as optimistic online gradient descent, blending a standard feedback step with a CDF guided optimistic refinement to reduce conservativeness. The theory is solid, delivering a joint coverage regret bound, distribution free finite sample coverage for arbitrary learning rates, and convergence under i.i.d. scores, while the presentation is clear with precise setup and actionable pseudocode. Experiments across simulated shifts and multiple real world domains show target coverage with consistently tighter intervals, underscoring practical significance and easy deployability, using simple ECDF or KDE plugins."}, "weaknesses": {"value": "The central theory relies on an unverifiable same sign assumption and boundedness that may fail under nonstationary or heavy tailed regimes; an assumption free, non asymptotic refinement bound based on observable CDF error would strengthen the claims. The simulations omit heteroscedasticity, variance changepoints, and heavy tails, lack ablations isolating the optimistic step and ECDF versus KDE, and do not report recovery time or conditional coverage. Practical clarity and scalability are under specified: window and bandwidth choices are sensitive, per step cost appears $(O(w))$ time and $(O(w))$ space without specialized data structures, and the fairness of hyperparameter tuning is unclear; adding adaptive selection rules and explicit complexity with wall clock scaling would improve deployability. Minor exposition gaps remain in mapping the trust region to the step size and in indexing consistency."}, "questions": {"value": "1. Table 2 shows COP attains competitive coverage with narrow intervals, but Proposition 1 hinges on the unverifiable ``same--sign'' assumption.\n\n  (1) Please report how often $\\mathrm{sign}(\\hat F_{t+1}(\\hat q_{t+1}) - (1 - \\alpha)) = \\mathrm{sign}(F_{t+1}(\\hat q_{t+1}) - (1 - \\alpha))$ holds on the real-world datasets (e.g., by sliding windows), and correlate this rate with coverage/width. This would clarify whether COP's gains are explained by the assumption or by broader robustness.\n\n  (2) Provide a refinement-step analysis based solely on the observable CDF error \n$\\varepsilon_t := \\sup_q \\lvert \\hat F_t(q) - F_t(q) \\rvert$ that yields\n$$\n\\text{coverage} \\;\\le\\; \\Phi(\\varepsilon_{1:T}, \\{\\eta_t\\}, \\{\\lambda_t\\})\n$$\nregardless of the sign, that is, a non-asymptotic worst-case bound that does not rely on the same-sign assumption. For i.i.d. windows, a DKW-type inequality (and for time series, its mixing/martingale analogues) controls $\\varepsilon_t$, aligning the strong empirical results with equally robust theory.\n\n\n\n2. The simulation studies assume homoscedastic Gaussian noise. To better assess the robustness of COP's CDF-based refinement, please include experiments with heteroscedasticity, variance changepoints, and heavy-tailed noise (e.g., Student-$t$). Please report coverage and width as well as post-shift recovery time (steps to return to target coverage) across these settings, and summarize observed failure modes.\n\n\n3. For deployability, could you briefly report the time and space complexity of COP’s per-step update (with windowed ECDF/KDE) and a small scaling plot of per-step wall-clock versus window size $w$? If the implementation is $O(w)$ per step (vs. $O(1)$ for OGD/PID), please state this explicitly; if a data structure is used, please indicate the amortized complexity (e.g., $O(\\log w)$) and the memory footprint $O(w)$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MB6I0CtpAI", "forum": "I69SaLbwqZ", "replyto": "I69SaLbwqZ", "signatures": ["ICLR.cc/2026/Conference/Submission4159/Reviewer_JBdV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4159/Reviewer_JBdV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808130266, "cdate": 1761808130266, "tmdate": 1762917204396, "mdate": 1762917204396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}