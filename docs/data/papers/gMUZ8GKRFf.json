{"id": "gMUZ8GKRFf", "number": 17853, "cdate": 1758281264660, "mdate": 1761187535480, "content": {"title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats", "abstract": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly embracing low-precision floating-point (FP) formats to handle the pervasive activation outliers in Large Language Models (LLMs). Despite this industry trend, a unified comparison of FP and integer (INT) quantization across varying granularities has been missing, leaving algorithm and hardware co-design without clear guidance. This paper fills that gap by systematically investigating the trade-offs between FP and INT formats. We reveal a critical performance crossover: while FP excels in coarse-grained quantization, INT consistently surpasses it as the quantization block size shrinks. Our comprehensive comparison demonstrates that for popular fine-grained formats like MX (block size 32), MXINT8 and MXINT4 are superior to their FP counterparts in both algorithmic accuracy and hardware efficiency. We also introduce a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8 training. These findings challenge the current hardware trajectory and advocate for prioritizing fine-grained INT formats in future AI accelerators to achieve a better balance of accuracy, power, and efficiency.", "tldr": "This paper demonstrates that integer (INT) quantization surpasses floating-point (FP) for LLMs in  fine-grained quantization, challenging the current hardware trend.", "keywords": ["Low-Precision Quantization; INT v.s. FP; Microscaling format;"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/64d59a184cab483aec032a2432bdc94e6d87b00e.pdf", "supplementary_material": ""}, "replies": [{"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "There were some errors in this paper, so we withdrawal it to make corrections."}}, "id": "zR6mKuwIst", "forum": "gMUZ8GKRFf", "replyto": "gMUZ8GKRFf", "signatures": ["ICLR.cc/2026/Conference/Submission17853/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17853/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761187534503, "cdate": 1761187534503, "tmdate": 1761187534503, "mdate": 1761187534503, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}