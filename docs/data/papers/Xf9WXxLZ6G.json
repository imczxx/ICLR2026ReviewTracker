{"id": "Xf9WXxLZ6G", "number": 1758, "cdate": 1756915176295, "mdate": 1759898188746, "content": {"title": "FACTS: Table Summarization via Offline Template Generation with Agentic Workflows", "abstract": "Query-focused table summarization requires generating natural language summaries of tabular data conditioned on a user query, enabling users to access insights beyond fact retrieval.\nExisting approaches face key limitations: table-to-text models require costly fine-tuning and struggle with complex reasoning, prompt-based LLM methods suffer from token-limit and efficiency issues while exposing sensitive data, and prior agentic pipelines often rely on decomposition, planning, or manual templates that lack robustness and scalability.\nTo mitigate these issues, we introduce an agentic workflow, FACTS, a Fast, Accurate, and Privacy-Compliant Table Summarization approach via Offline Template Generation.\nFACTS produces offline templates, consisting of SQL queries and Jinja$2$ templates, which can be rendered into natural language summaries and are reusable across multiple tables sharing the same schema.\nIt enables fast summarization through reusable offline templates, accurate outputs with executable SQL queries, and privacy compliance by sending only table schemas to LLMs.\nEvaluations on widely-used benchmarks show that FACTS consistently outperforms baseline methods, establishing it as a practical solution for real-world query-focused table summarization.", "tldr": "", "keywords": ["LLM-based Agentic Workflow", "Query-focused Table Summarization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/565d2373935e1e5b87aee5409b033a43df5efc69.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FACTS, a query-focused table summarization framework that generates reusable offline templates composed of schema-aware SQL queries and a Jinja2 text renderer. The system is designed to be fast, accurate, and privacy-compliant by sending only table schemas—not cell values—to language models. FACTS follows a three-stage agentic workflow: it first elicits schema-guided clarifications and filtering rules, then synthesizes and validates executable SQL, and finally produces a Jinja2 template aligned with the SQL outputs. An LLM Council—an ensemble that votes and provides feedback—audits artifacts at every stage to improve correctness and robustness. According to the workflow diagram on page 6, this process yields a template that can be applied across any table sharing the same schema and query semantics, enabling amortized speedups. The example on page 4 shows how the approach selects top savers via SQL and renders a narrative via Jinja2, while the comparison diagram on page 2 emphasizes gains in reusability and privacy over direct prompting. Experiments on FeTaQA, QTSumm, and QFMTS report best or second-best scores across BLEU, ROUGE-L, and METEOR (page 8), a 100% SQL pass rate versus an 83.2% single-call baseline (page 9), and substantial runtime advantages when reusing templates across many tables or as table size grows (page 9). The appendix provides prompts, pseudocode, and a step-by-step case study that illustrate the system’s mechanics (pages 14–19)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work is original in framing query-focused table summarization as offline template generation that deliberately separates computation (SQL) from surface realization (Jinja2) and binds both to the table schema. This abstraction moves beyond natural-language plans and ad-hoc program synthesis by yielding a concrete, reusable artifact that amortizes LLM cost and avoids re-prompting for recurring queries, which is convincingly demonstrated by the reusability and scalability plots on page 9. \n\nMethodological quality is good: the three-stage agentic pipeline with Council-style validation reduces brittle one-shot failures, achieves a 100% SQL pass rate relative to a single-call baseline (page 9), and grounds all textual claims in executed queries, improving factuality. Clarity is high: the paper defines terms precisely, presents a clear end-to-end workflow (page 6), gives a concrete running example (page 4), documents prompts and pseudocode (pages 14–18), and provides a case study that shows intermediate artifacts and the final rendered summary (pages 19–20)."}, "weaknesses": {"value": "The empirical evaluation relies exclusively on automatic overlap metrics, which only imperfectly capture factual faithfulness and utility; a human evaluation or auditor-based factuality check would strengthen claims about summary quality. \n\nThe privacy story, while thoughtful, could be more rigorous: although values are never sent to LLMs, schemas and query text can still leak sensitive structure or intent; articulating a formal threat model and the conditions under which schemas are safe would clarify limits. The LLM Council improves robustness, but its cost/latency and sensitivity to council composition, temperature, and prompt phrasing are not quantified; an ablation isolating each validation point would disentangle where the gains arise. \n\nGeneralization beyond DuckDB and portability across SQL dialects are not evaluated, yet production environments often involve diverse engines; similarly, the approach presumes stable, clean schemas, while many real datasets require entity resolution and schema drift handling. \n\nThe single-call baseline is a relatively weak foil for the agentic method; comparisons against stronger structured-program baselines configured for reusability would better calibrate the contribution. \n\nwhile the runtime plots on page 9 are compelling, a full accounting that includes initial template-creation time, council iterations, and the overhead of guided-question generation would help practitioners plan deployments, and releasing code only upon acceptance limits current reproducibility."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5lcFbgJD6Z", "forum": "Xf9WXxLZ6G", "replyto": "Xf9WXxLZ6G", "signatures": ["ICLR.cc/2026/Conference/Submission1758/Reviewer_xc11"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1758/Reviewer_xc11"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980668979, "cdate": 1761980668979, "tmdate": 1762915881144, "mdate": 1762915881144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "•\tThe paper presents FACTS, an agent-based workflow for query-focused table summarization. The system produces an offline template that combines SQL queries with Jinja2 rendering. The idea is to let LLMs generate reusable templates once and reuse them across tables with the same schema. The workflow has three stages: schema-guided filtering, SQL generation, and text rendering, each verified by an LLM Council. Experiments on FeTaQA, QTSumm, and QFMTS show consistent gains over several prompt-based and agent baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe separation between template generation and execution is practical, allowing expensive reasoning to occur once and inexpensive SQL execution to repeat many times.\n•\tThe LLM Council improves reliability, as the full FACTS workflow achieves a 100% SQL pass rate, whereas the single-call variant fails on several queries.\n•\tThe method scales efficiently. Once the template is ready, runtime remains nearly constant even as table size increases, while baseline methods become slower."}, "weaknesses": {"value": "•\tW1. Section 3.1 suggests that the generated templates can generalize to semantically similar queries, but the evaluation in Section 4.6 only varies table values while keeping both the query and schema fixed.  The results therefore demonstrate reuse under identical conditions rather than true semantic variation.  In addition, all reuse tests are conducted on tables with the same schema, so it is unclear how the method would behave if column names were changed or new fields introduced.\n\n•\tW2. The improvements over strong baselines are marginal. In Table 2, FACTS scores 46.0 BLEU and 70.8 ROUGE-L on QFMTS, while SPaGe reaches 45.7 and 68.3. On FeTaQA, SPaGe even performs slightly better.\n\n•\tW3. The council design involves several LLMs working together, but the paper only compares the full setup with a single-call variant.  While this shows that iteration helps, it leaves open whether multiple models are actually necessary or if a single model with repeated prompting could achieve similar performance.\n\n•\tW4. The evaluation focuses on BLEU, ROUGE-L, METEOR, and SQL pass rate, which assess fluency and executability but not factual consistency.  A summary could read well yet misrepresent the SQL output.  The paper lacks a measure of factual faithfulness or any human verification of whether the rendered summary matches the actual SQL execution results.\n\n•\tW5. The paper highlights offline scalability but does not quantify the actual generation cost. For example, Algorithm 2 includes iterative loops in Stage 1 and Stage 2, yet the average number of iterations per template is never reported."}, "questions": {"value": "•\tHow broadly can a generated template handle query or schema variations? For example, can it adapt to “top 3” vs “top 5,” renamed columns, or added fields without regeneration?\n•\tSection 3.3 already includes an execute–repair loop. Is there an ablation comparing the three-model council with a single-model version using the same loop?\n•\tHow do the authors ensure that the generated summary is factually consistent with the actual SQL execution? For example, could the Jinja2 template render an incorrect value or variable while still achieving a high BLEU score?\n•\tWhat is the average number of iterations (t) in Algorithm 2 for each benchmark? What are the average runtime and token cost for generating a single template?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ONvWWPwY6q", "forum": "Xf9WXxLZ6G", "replyto": "Xf9WXxLZ6G", "signatures": ["ICLR.cc/2026/Conference/Submission1758/Reviewer_J47W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1758/Reviewer_J47W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081695457, "cdate": 1762081695457, "tmdate": 1762915880271, "mdate": 1762915880271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FACTS, an agentic framework for query-focused table summarization that generates reusable offline templates (SQL + Jinja2) validated by an LLM Council. The approach is evaluated on FeTaQA, QTSumm, and QFMTS, showing strong results compared with recent baselines such as TaPERA (ACL 2024) and SPaGe (2025) under the same model backbone. The method is practical and well-implemented, but its core idea of offline template generation and validation is not fundamentally new, having appeared in other data-to-text and report-generation contexts. Moreover, while the framework is claimed to be efficient, no runtime or cost evaluation is provided to substantiate that claim."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is technically solid, clearly written, and experimentally thorough, but its novelty is incremental and key claims about speed and efficiency are not empirically supported.\n It would still be a useful addition for practitioners and researchers interested in reliable, schema-aware table summarization.\n\n- Comprehensive evaluation across all major query-focused summarization benchmarks with up-to-date baselines.\n- Methodologically coherent framework combining schema-guided SQL generation, Jinja2 templating, and multi-LLM validation.\n- Strong reproducibility — clear prompts, detailed methods, and transparent implementation.\n- The design addresses practical aspects such as reusability and privacy."}, "weaknesses": {"value": "- Limited novelty: offline or template-based summarization has prior art; the main contribution is integration rather than conceptual innovation.\n- No empirical efficiency evidence: runtime, latency, or token-cost comparisons are missing despite efficiency claims.\n- Evaluation metrics (BLEU, ROUGE-L, METEOR) capture surface similarity but not factual consistency or execution accuracy.\n- Narrow domain scope: all datasets are Wikipedia-based, so real-world privacy or deployment benefits are not demonstrated."}, "questions": {"value": "- Add quantitative runtime and efficiency measurements.\n- Include factual-consistency or SQL execution accuracy metrics.\n- Clarify the novelty position as a domain adaptation or integration of existing ideas.\n- Consider testing on non-Wikipedia domains to support practical claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KsmYmNcWty", "forum": "Xf9WXxLZ6G", "replyto": "Xf9WXxLZ6G", "signatures": ["ICLR.cc/2026/Conference/Submission1758/Reviewer_c8fF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1758/Reviewer_c8fF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136241008, "cdate": 1762136241008, "tmdate": 1762915879078, "mdate": 1762915879078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the FACTS framework for Query-Focused Table Summarization (QFTS).\nThe core idea lies in offline template generation: SQL queries are paired with Jinja2 templates to form reusable templates, and an iterative LLM Council process ensures SQL executability and consistency between queries and templates.\nThe system protects privacy by uploading only table schemas, allowing templates to be reused across tables with the same schema, thus improving efficiency.\nExperiments on FeTaQA, QTSumm, and QFMTS show that FACTS achieves superior or near-best BLEU, ROUGE-L, and METEOR scores, and demonstrates strong SQL executability and reasoning stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Template reuse significantly reduces repeated inference costs and achieves high efficiency under fixed schemas.\n\n- Generating text based on SQL execution results reduces hallucinations compared with prompt-based methods."}, "weaknesses": {"value": "- Using SQL pass rate only measures syntactic correctness and does not guarantee semantic consistency with user queries; BLEU and similar scores may also be hacked.\n\n- Fixed templates may degrade under schema drift, column name changes, or complex multi-table logic.\n\n- Only latency in the reuse phase is reported; the multi-round iterative cost of initial template generation is not disclosed.\n\n- In multi-table scenarios, potential degradation and execution inconsistencies are not analyzed."}, "questions": {"value": "- How is the semantic consistency between SQL and user queries verified? Is there a plan to introduce execution-result-to-summary factual alignment metrics?\n\n- When the schema changes slightly, how can template reuse be maintained?\n\n- Can the authors report the full generation cost, including iteration rounds and token usage?\n\n- The Council’s “majority voting + self-correction” logic, under a self-bootstrapping setup without ground truth, may converge to consistent errors — does this occur, and how often?\n\nIf authors can address my concern especially in evaluation, I'm free to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YsLpxZNr2Z", "forum": "Xf9WXxLZ6G", "replyto": "Xf9WXxLZ6G", "signatures": ["ICLR.cc/2026/Conference/Submission1758/Reviewer_CGHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1758/Reviewer_CGHL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206417826, "cdate": 1762206417826, "tmdate": 1762915878633, "mdate": 1762915878633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}