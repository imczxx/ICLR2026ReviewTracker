{"id": "uvc5qrD3R2", "number": 22327, "cdate": 1758329674330, "mdate": 1759896872211, "content": {"title": "Multiple Correlation Encoder-based Naive Bayes", "abstract": "Naive Bayes (NB) continues to be one of the top 10 data mining algorithms. However, due to its assumption of attribute conditional independence, NB encounters significant challenges in addressing attribute-class correlations, attribute-attribute correlations, instance-class correlations, instance-instance correlations, and so on. In the last few decades, a large number of improved algorithms have been proposed, but none of them simultaneously addresses all these correlations. To bridge this gap, this paper proposes a novel algorithm called multiple correlation encoder-based naive Bayes (MCENB). In MCENB, we first design a multiple correlation encoder to generate new attributes, where multiple correlations are simultaneously captured and optimized. Specifically, the newly generated attributes are highly correlated with the class, yet uncorrelated with each other. Instances consisting of new attribute values are highly correlated with those in the same class. Subsequently, we augment original attributes by concatenating them with new attributes. Finally, we weight each augmented attribute to alleviate the attribute redundancy and then build NB on the weighted attributes. The experiments across numerous datasets show that MCENB significantly outperforms its benchmark competitors.", "tldr": "Multiple Correlation Encoder-based Naive Bayes", "keywords": ["Naive Bayes", "attribute conditional independence", "multiple correlation encoder"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0321cb390d4bddb956ea7f503b19746a81d573d6.pdf", "supplementary_material": "/attachment/47ba2a0eade52511c70191b9016684335b0fd6bd.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a two-stage learning framework that aims to address attribute–attribute, attribute–class, instance–instance, and instance–class correlations in naive Bayes.\nThe approach introduces three components:\n(1) an ELBO-style objective for feature reconstruction,\n(2) two regularization terms that minimize correlations between attribute-attribute, attribute–class, instance-instance, and instance–class correlations in naive Bayes,\n(3) an additional linear weighting layer for final prediction.\nExperiments are conducted on small-scale toy datasets to demonstrate the effect of each component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is conceptually straightforward and easy to follow."}, "weaknesses": {"value": "1. The overall motivation and contribution of the paper are unclear.\nThe proposed components resemble well-studied techniques in prior work.\nSpecifically, the ELBO objective is similar to VAE-regularized classification [1,2];\nminimizing correlations among features has been explored in [3,4];\nmaximizing correlations between representations and classes is equivalent to optimizing a classifier via cross-entropy, as discussed in [2,5];\nusing contrastive learning to regularize classification has been explored in [6];\nand the final weighting step is effectively a standard linear classifier trained with cross-entropy.\nThus, similar results could likely be achieved via an end-to-end MLP classifier trained with cross-entropy combined with a series of known regularizations [1,2,3,4,6], which would likely be simpler and computationally more efficient than the proposed two-stage framework.\nAs such, it is difficult to identify a novel technical contribution or conceptual insight.\n\n2. The empirical evaluation is limited.\nThe experiments are performed only on small toy datasets with few features and limited samples, lacking validation on high-dimensional or real-world data such as images or text.\nThis makes it hard to assess whether the approach scales or provides meaningful improvements in realistic settings.\nMoreover, the reported results exhibit large variances across datasets, suggesting unstable performance across runs.\n\n3. There are several presentation issues.\nThe introduction claims to address attribute–class and instance–class correlations, but later sections appear to enhance certain correlations, which creates a conceptual inconsistency.\nThe paper also does not adequately explain why optimizing the ELBO is necessary or beneficial in this setting, and no ablation studies are provided to justify individual design choices.\n\n- [1] Stainvas, Inna, Nathan Intrator, and Amiram Moshaiov. \"Improving classification via reconstruction.\" Available at citeseer. nj. nec. com/article/steinvas00improving. html (2000).\n- [2] Alemi, Alexander A., et al. \"Deep Variational Information Bottleneck.\" International Conference on Learning Representations. 2017.\n- [3] Cogswell, Michael, et al. \"Reducing overfitting in deep networks by decorrelating representations.\" arXiv preprint arXiv:1511.06068 (2015).\n- [4] Zbontar, Jure, et al. \"Barlow twins: Self-supervised learning via redundancy reduction.\" International conference on machine learning. PMLR, 2021.\n- [5] Poole, Ben, et al. \"On variational bounds of mutual information.\" International conference on machine learning. PMLR, 2019.\n- [6] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xml7yIl37A", "forum": "uvc5qrD3R2", "replyto": "uvc5qrD3R2", "signatures": ["ICLR.cc/2026/Conference/Submission22327/Reviewer_doFK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22327/Reviewer_doFK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906025210, "cdate": 1761906025210, "tmdate": 1762942171212, "mdate": 1762942171212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified Naive Bayesian framework to simultaneously model correlations among attributes, instances, and classes. The core of the proposed method is a multiple correlation encoder, which is optimized by two correlation objective functions (attribute and instance). This encoder provides a more comprehensive representation and consequently improves NB. The proposed method is validated through extensive experiments on public real-world and synthetic datasets, and achieves considerable performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method designs a multiple correlation encoder to jointly model correlations among attributes, instances, and classes.\n\n2. The experimental results show that the proposed method obtains competitive performance. \n\n3. This paper is written well and provides an in-depth analysis of effectiveness and efficiency."}, "weaknesses": {"value": "1. The technical contributions of the proposed method are not clarified, especially compared with ICGNB. If I understand correctly, the major difference between the proposed method and ICGNB is how the encoder is learned: the proposed method utilizes the attribute correlation loss and the instance correlation loss, while ICGNB adopts the ELBO loss only. For the parts of attribute generation, augmentation, and weighting, it is hard to tell the difference between the two methods.\n\n2. The selected baselines are out-of-date (mostly published from 2019 to 2023, except for ICGNB), and so are the related studies. Yet compared with ICGNB, the performance gains of the proposed method are marginal."}, "questions": {"value": "1. The results of ICGNB reported in Table 2 are different from those in the original paper, while other methods remain the same. I wonder the reason behind such a gap.\n\n2. The selected datasets are quite small; can the proposed method be evaluated on other large-scale attribute-related tasks, like zero-shot image classification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8NjzfSoX4S", "forum": "uvc5qrD3R2", "replyto": "uvc5qrD3R2", "signatures": ["ICLR.cc/2026/Conference/Submission22327/Reviewer_MDk3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22327/Reviewer_MDk3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998851435, "cdate": 1761998851435, "tmdate": 1762942170993, "mdate": 1762942170993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an autoencoder-augmented variant of the Naive Bayes (NB) classifier, motivated by the idea that not only attribute-class relationships but also attribute-attribute, instance-class, and instance-instance dependencies should be considered in NB learning. The method operates in two main phases: first, an autoencoder is trained to learn latent features from the original inputs, and second, a weighted NB classifier is trained on the concatenation of both original and latent attributes. Class labels are injected into the latent representation to encourage class-dependent feature learning. The proposed loss function extends the standard ELBO objective by adding correlation-based regularization terms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an interesting question, whether integrating multiple forms of dependency into Naive Bayes models can enhance classification capability.\n- The approach is clearly presented, and the implementation appears straightforward enough to reproduce. The structure of the paper, including figures and tables, contributes to readability.\n- The motivation to move beyond the traditional \"conditional independence\" assumption of NB is conceptually sound and aligns with recent interest in hybrid generative-representation learning models."}, "weaknesses": {"value": "- The central hypothesis that explicitly capturing four types of correlations will improve performance is not convincingly substantiated. The paper provides neither theoretical reasoning nor experimental evidence that isolating all four dependencies yields tangible benefits.\n\n- Experimental validation is limited and not well targeted. Since the datasets used do not have known correlation structures, it is impossible to assess whether the proposed mechanism truly models the intended dependencies. A more principled design with synthetic or controlled datasets would have been necessary.\n\n- The method's originality appears modest. Autoencoder-based extensions of Naive Bayes and weighted NB formulations have been explored in prior work; the present approach seems to combine these ideas rather than introduce a fundamentally new direction.\n\n- Empirical results show only marginal or inconsistent gains over existing baselines. Furthermore, several baselines are somewhat outdated, which weakens the experimental claims.\n\n- The analysis section is descriptive rather than analytical, the paper reports accuracy numbers but provides little interpretation of what the model actually learns or how the introduced components affect behavior.\n\n- Overall, while the paper is competently executed, the technical novelty and empirical impact appear insufficient for acceptance at a top-tier venue like ICLR."}, "questions": {"value": "- What is the rationale for concatenating both original and latent attributes? Does this indicate that the latent representation alone cannot fully encode the necessary information?\n\n- Why must the dimensionality of the latent space be identical to that of the original features? Could a smaller or larger latent dimension provide better inductive bias?\n\n- The loss function comprises multiple terms, yet the paper does not analyze their individual effects. Are all components effectively optimized? Reporting each loss term separately would clarify this.\n\n- Were weighting coefficients between loss components considered or tuned? If not, why?\n\n- Do the 24 datasets used in the experiments truly contain all four types of correlations? If not, how can the results be interpreted as evidence for the proposed approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VopQEVPhUM", "forum": "uvc5qrD3R2", "replyto": "uvc5qrD3R2", "signatures": ["ICLR.cc/2026/Conference/Submission22327/Reviewer_S37T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22327/Reviewer_S37T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005289826, "cdate": 1762005289826, "tmdate": 1762942170781, "mdate": 1762942170781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive approach to improving the Naive Bayes classifier by learning the correlations between features and instances ( (attribute-class, attribute-attribute, instance-class, instance-instance)). The core idea of using a learned encoder to generate new attributes that explicitly optimize multiple correlations is innovative and would have a better ability. The experimental design is thorough, using a substantial number of datasets and rigorous statistical tests."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Integrating data from both brain tissue and CSF provides a more holistic view of the AD proteome than most previous studies.\n2. The systematic evaluation of four batch correction methods, using multiple metrics, is valuable.\n3. The strategy of building models both with and without the core AD CSF biomarkers is excellent. This can demonstrate that the proteomic panels provide substantial predictive power independently."}, "weaknesses": {"value": "1. The analysis is constrained by the proteins measured and the cohorts available.\n2. The result section lacks deeper mechanistic insight. A pathway enrichment analysis or protein-protein interaction network analysis of the novel candidate biomarkers would have strengthened the biological narrative and proposed potential functional roles in AD pathogenesis.\n3. This study lacks a true external validation on a completely independent cohort.\n4. The study treats AD as a binary condition (Control vs. AD). It does not address disease subtypes or progression stages."}, "questions": {"value": "1. The related works are too simple, and the authors do not analyze previous works in detail. The definition of the four types of correlation is unclear.\n2. No external validation datasets.\n3. Several symbols in  Fig. 1 are unclear. The details of the framework of MCENB are missing.\n4. Maximizing L_A cannot ensure that each correlation (rho_bc and rho_bb) will be optimized.\n5. Why combine original attributes with newly generated attributes? This could incur many redundancies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jo2kUUo1uv", "forum": "uvc5qrD3R2", "replyto": "uvc5qrD3R2", "signatures": ["ICLR.cc/2026/Conference/Submission22327/Reviewer_yRcn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22327/Reviewer_yRcn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175582853, "cdate": 1762175582853, "tmdate": 1762942170566, "mdate": 1762942170566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}