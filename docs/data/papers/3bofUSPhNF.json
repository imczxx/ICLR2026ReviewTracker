{"id": "3bofUSPhNF", "number": 7526, "cdate": 1758025829934, "mdate": 1763355828791, "content": {"title": "DiffGED: Computing Graph Edit Distance via Diffusion-based Graph Matching", "abstract": "Graph Edit Distance (GED), which aims to find an edit path with minimum number of edit operations to transform one graph into another, is a fundamental NP-hard problem and a widely used graph similarity measure. Recent matching-based hybrid approaches have demonstrated better scalability than A* search-based hybrids by reformulating GED as a graph matching problem. In these methods, a neural network predicts a single deterministic node matching matrix, from which top-$k$ node mappings are extracted iteratively to derive candidate edit paths. However, these methods often suffer from highly correlated candidates that easily lead to suboptimal solutions, while the iterative extraction becomes inefficient for large $k$. In this paper, we propose DiffGED, the first generative approach for GED computation. Specifically, we formulate the graph matching problem as a generative task, and employ a diffusion-based model to generate multiple diverse node matching matrices simultaneously, from which diverse node mappings can be efficiently extracted. The generative diversity introduced by the diffusion process enables DiffGED to avoid suboptimal solutions and achieve superior solution quality close to the exact solution. Experiments on real-world datasets show that DiffGED generates multiple diverse edit paths with accuracy comparable to exact solutions, while running faster than existing hybrid approaches.", "tldr": "", "keywords": ["Graph Edit Distance", "Graph matching", "Generative model"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38ef3fbf1f058c93cd6b9d607b87afd070c55843.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DiffGED, the first generative approach for computing Graph Edit Distance (GED). The key innovation is reformulating GED computation as a generative task using a diffusion-based graph matching model (DiffMatch) that generates multiple diverse node matching matrices simultaneously, from which diverse node mappings can be efficiently extracted in parallel."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Generative Formulation: This is the first paper to apply generative/diffusion models to graph matching and GED - genuinely novel contribution. And it is well-motivated because multimodal distribution of optimal solutions naturally suits generative approach\n\n2. Strong Empirical Results: DiffGED shows exceptional accuracy: ~95-100% across all datasets (Table 1) and significantly outperforms existing methods (e.g., 98% vs 58.7% accuracy on AIDS700) while showing good efficiency in inference.\n\n3. Demonstrated Diversity Benefits: Figure 7 shows DiffGED generates 60-80 distinct edit paths vs ~5 for baselines.\n\n4. Thorough Experimental Validation."}, "weaknesses": {"value": "1. Limited Scalability Analysis: All datasets used in the paper have graphs with average 8-13 nodes, max 89 nodes. The reviewer is concerned about the scalability of DiffGED.\n\n2.Analysis of Diffusion Design Choices: Could continuous diffusion design work for GED problem?"}, "questions": {"value": "1. Could author provide additional experiments on dataset containing larger graphs or more analysis on the scalability of the proposed method?\n\n2. The inference of diffusion model takes S=10 steps, which means that the network is applied ten times for denoising. However, the inference time is much smaller than previous mixture methods. Could the author provide some analysis on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "XHmNsz9XMt", "forum": "3bofUSPhNF", "replyto": "3bofUSPhNF", "signatures": ["ICLR.cc/2026/Conference/Submission7526/Reviewer_MUEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7526/Reviewer_MUEf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539346315, "cdate": 1761539346315, "tmdate": 1762919628643, "mdate": 1762919628643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a diffusion-based approach to the NP-hard problem of finding the edit distance between two graphs. The method comprises of two steps. First, a diffusion network denoises a randomly initialized node matching matrix. Second, a greedy algorithm derives a node mapping from the denoised matching matrix by iteratively selecting the most probable node pair. From this injective node mapping, an edit path and edit distance can be easily acquired. In order to achieve optimality, the described procedure is repeated over a number of sampled initial matrices. In the end, the minimum edit distance among all the candidates is returned. The main advantage over previous hybrid approaches is the diversity of the generated matching matrices thanks to the diffusion process (previous approaches only produce a single deterministic matching matrix)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty: The paper claims to present the first generative and diffusion-based solution to the problem of GED computation.\n- Strong empirical results: The method outperforms the presented baselines on a number of evaluation metrics. \n- With the capability of making $k$ arbitrarily large, the method has the potential of retrieving several optimal edit paths (if there is more than one), which may be relevant in some scenarios."}, "weaknesses": {"value": "- More baselines should be included. First, more non-neural baselines should be shown. The paper only presents two algorithmic approaches, both of which are bipartite matching, but there are several algorithmic and heuristic methods which are not directly based on bipartite matching [1]. Second, the regression-based neural methods (cited in Section 2) should also be compared against. There can be applications in which only the numerical GED value is important. \n- While the idea of using the diffusion process for GED estimation is new, the constituent components are taken directly from previous works (e.g. discrete diffusion, bipartite matching, Anisotropic GNN). This is fine but it'd be valuable to show some theoretical and proven insights on why these previous works are applicable. \n- Overall, the presentation of the methodology can be made more clear. For example, the final loss function should be shown in the main paper. Moreover, Section 4.1 (Overview) could be more concise. \n\nPlease see other comments in the questions.\n\n[1] Blumenthal, David B., et al. \"Comparing heuristics for graph edit distance computation.\" The VLDB journal 29.1 (2020): 419-458."}, "questions": {"value": "- The time reported is for inference. However, a drawback of diffusion-based models is their high training cost, due to the long iterative denoising process [2]. How comparable is your method to other non-generative models in terms of training efficiency?\n- The GED definition is restricted and can be made more general by accounting for edit operations of various costs. How difficult is it to incorporate generalized costs in your proposed framework?\n- The pipeline in Figure 3 is a bit confusing. It may be better to just show the inference procedure for one random initial matrix and note that the procedure is repeated for $k$ times. Is it possible to improve it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IWmOts9MUK", "forum": "3bofUSPhNF", "replyto": "3bofUSPhNF", "signatures": ["ICLR.cc/2026/Conference/Submission7526/Reviewer_4JsK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7526/Reviewer_4JsK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843947176, "cdate": 1761843947176, "tmdate": 1762919628373, "mdate": 1762919628373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiffGED, a novel generative framework for computing Graph Edit Distance (GED). The authors identify a key weakness in prior hybrid methods: they predict a single, deterministic node matching matrix, and iteratively extracting the top-k mappings from it leads to highly correlated, suboptimal candidates.\n\nTo solve this, DiffGED formulates GED as a generative task. It uses a discrete diffusion-based model, DiffMatch, to generate k diverse node matching matrices in parallel, starting from k different random noise inputs. A fast, greedy top-1 mapping is extracted from each of the k matrices, and the minimum GED from these k candidates is chosen. Experiments show that DiffGED achieves SOTA results, claiming near-perfect accuracy on standard benchmarks (AIDS, Linux, IMDB) while being more efficient than competing hybrid approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The work is well-motivated, since existing matching-based hybrids do suffer from correlated candidates.\n(2) The two-phase view (generate k matrices → extract k mappings independently) is neat and practical.\n(3) Strong empirical numbers vs. recent GED hybrids with ablations."}, "weaknesses": {"value": "(1) The \"first generative approach\" claim is a bit overstated – the technical innovation beyond applying known diffusion methods is related limited.\n(2) All experiments involve small graphs (max 89 nodes). No evidence of scalability to real-world large graphs (1000+ nodes) where GED computation is actually challenging."}, "questions": {"value": "(1) Table 1 shows MATA* is ~15x faster than DiffGED on AIDS700. How do you reconcile this with claims of superior efficiency?\n(2) Why no comparison with VAEs, GANs, or other generative models? A brief discussion on these methods would further strength the paper.\n(3) The \"GEDGNN(AGNN)\" experiment, which simply swaps the baseline's GNN for the authors' more powerful AGNN architecture, shows a massive performance boost (e.g., 52.5% to 66.7% accuracy on AIDS). Does this suggest that the GNN architecture (AGNN) is a primary driver of the performance, independent of the diffusion framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DWacJu2li1", "forum": "3bofUSPhNF", "replyto": "3bofUSPhNF", "signatures": ["ICLR.cc/2026/Conference/Submission7526/Reviewer_HVVk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7526/Reviewer_HVVk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870652455, "cdate": 1761870652455, "tmdate": 1762919627819, "mdate": 1762919627819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiffGED, a generative framework for Graph Edit Distance (GED) computation. The key idea is to treat node matching as a sampling problem rather than a single deterministic prediction task. The authors employ a discrete diffusion model to generate multiple diverse node matching matrices, from which candidate node correspondences and corresponding edit paths can be derived."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The use of generative diffusion models to produce multiple node matchings for GED computation is  unexplored. \n\nIf successful, such a formulation could open up a direction in which GED models leverage stochastic proposal generation rather than deterministic alignment."}, "weaknesses": {"value": "-  Several recent neural GED and graph alignment models relevant to neural GED estimation  are not appropriately acknowledged or compared. See examples [1,2]  and baselines therein. In particular [1] is a cross graph early interaction model similar to what this paper employs in the denoising steps. [2] employs a gumbel sinkhorn on top of soft node matching proposals. Furthermore, the method assumes access to ground-truth node matching matrices during training, which are costly to obtain in practice. This  undermines the practical benefit of using a neural model rather than combinatorial or search-based techniques, and has to be appropriately addressed.\n\n- Another issue concerns dataset choice. The evaluation relies on datasets (e.g., AIDS, Linux) that are now known to exhibit structural train–test leakage [3], which makes them unsuitable for evaluating neural generalization performance. \n\n- The paper claims that the diffusion process “breaks down the complex GED task into a sequence of iterative refinements, where each step makes minor adjustments and progressively improves the quality of the matching.” However, no evidence is provided to support this claim. As implemented, the method applies a standard discrete diffusion model to the matching matrix without introducing any task-specific inductive bias that would encourage meaningful step-wise refinement.\n\n-  Given that the method starts from a randomly initialized discrete matching matrix and applies diffusion-based denoising, a much simpler baseline would be to sample random matrices and apply Sinkhorn normalization to obtain valid soft matching proposals. Pooling over multiple such normalized samples would naturally yield diversity without the substantial computational overhead of running a diffusion process. As written, it is unclear whether the diffusion mechanism provides any meaningful benefit beyond this straightforward alternative.\n\n- The paper argues that existing neural architectures would require multiple prediction heads in order to generate diverse matching proposals, which would in turn increase parameter count. This claim is incorrect. Prior work based on the Gumbel–Sinkhorn [4] already allows for sampling multiple diverse matching matrices without increasing the number of parameters. Diversity is obtained simply by injecting Gumbel noise at inference time, while the underlying scoring network is shared across all samples. Qualitatively, this mechanism is very similar to what the proposed diffusion model aims to achieve, but at substantially lower computational cost.\n\n[1] Graph Matching Networks for Learning the Similarity of Graph Structured Objects, ICML 2019\n\n[2] Graph edit distance with general costs using neural set divergence NeurIPS 2024\n\n[3] Position: Graph Matching Systems Deserve Better Benchmarks, ICML 2024\n\n[4] Learning latent permutations with gumbel-sinkhorn networks"}, "questions": {"value": "My questions are implied in the comments in the weaknesses section. \n\nCurrently, the paper does not adequately motivate the need for a neural architecture, and the use of diffusion on randomly initialized matching matrices appears unnecessary without stronger justification. Several relevant prior works and alternative techniques are not cited or compared against, and the evaluation relies on datasets known to exhibit structural train–test leakage. While the direction is interesting, the motivation, experimental rigor, and baseline comparisons require substantial strengthening. The diffusion component adds significant complexity, yet no clear evidence is provided that it offers advantages over simpler, cheaper diversity-inducing methods (e.g., Gumbel–Sinkhorn sampling)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7QjcHtSxDd", "forum": "3bofUSPhNF", "replyto": "3bofUSPhNF", "signatures": ["ICLR.cc/2026/Conference/Submission7526/Reviewer_vj7u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7526/Reviewer_vj7u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004496536, "cdate": 1762004496536, "tmdate": 1762919627375, "mdate": 1762919627375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}