{"id": "moeOrHkDg2", "number": 3923, "cdate": 1757568283569, "mdate": 1759898062471, "content": {"title": "An Open-Ended Benchmark and Formal Framework for Adjuvant Research with MLLM", "abstract": "Adjuvants play a critical role in modulating immune responses and are central to the development of vaccines and immunotherapies. Yet progress in this field is constrained by data scarcity and incomplete understanding of mechanisms of action, which limit the transition from experience-based design to AI-driven approaches. To address these challenges, we present the first benchmark dedicated to adjuvants, constructed in an open-ended Q\\&A format and annotated by domain experts. The benchmark comprises 1,294 Q\\&A pairs and 1,364 formal descriptions, providing a resource for evaluating general-purpose multimodal large language models (MLLMs) and for developing domain-specific systems.  \nWe systematically assess 11 closed-source and 18 open-source MLLMs across dimensions including domain-specific Q\\&A, hallucination rejection, data generation, and instruction following. Results indicate that OpenAI-o1 (STS = 0.7495, LLM Score = 7.7) and DeepSeek-R1 (STS = 0.7415, LLM Score = 7.7) achieved the strongest performance among closed- and open-source models, respectively. In addition, we introduce a formal description framework for representing adjuvant design principles and immune mechanisms as structured abstractions, which can serve as building blocks for future domain-specialized MLLMs. Overall, this work provides a first step toward systematically integrating MLLMs into adjuvant research by offering a dedicated benchmark, comparative evaluation of existing models, and a formal foundation for future development. Data and code will be released at \\href{https://anonymous.4open.science/status/Advancing-Adjuvants-1C2B}{Anonymous}.", "tldr": "We introduce the first benchmark and formal framework for evaluating multimodal LLMs in adjuvant research, comprising 1,294 Q&A and 1,364 formal data entries.", "keywords": ["Adjuvant", "Scientific Benchmarks", "Multimodal Large Language Model"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00c77d428eea003fe4ec25111dba85e5ebdba288.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the first benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in the domain of vaccine adjuvants. Recognizing a critical gap in AI resources for this field—characterized by data scarcity and complex, heterogeneous mechanisms—the authors construct a high-quality dataset comprising 1,294 expert-annotated open-ended Question-Answer (Q&A) pairs and 1,364 formal, structured descriptions of adjuvant design and immune mechanisms. Using this benchmark, they perform a systematic evaluation of 29 MLLMs (11 closed-source, 18 open-source), assessing their capabilities in domain-specific Q&A, hallucination rejection, and instruction following. The results identify top-performing models like OpenAI-o1 and DeepSeek-R1. A key secondary contribution is the proposal of a formal framework to abstract complex biological processes into structured variables and functions, intended to serve as a foundation for future domain-specialized models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The creation of the first dedicated benchmark for adjuvants is a good contribution that can catalyze AI-driven research in vaccinology and immunology.\n2. The evaluation is exceptionally thorough. It goes beyond simple Q&A to include critical aspects like hallucination rejection and instruction following. The comparison of 29 models across closed-source and open-source categories provides a valuable landscape analysis for the community."}, "weaknesses": {"value": "1. The hallucination dataset, with only 69 examples, is relatively small. While its curated nature is valuable, the small size may limit the statistical power and generalizability of the HRR metric. The standard deviations reported in Table 6 highlight this variability.\n2. The paper provides relatively few statistical analyses (e.g., confidence intervals, significance testing) to support claims about model ranking differences.\n3. The benchmark's Q&A data is generated by a subset of the models being evaluated (e.g., GPT-4o, DeepSeek-R1). This could introduce a bias, as a model might perform better on questions that are stylistically or structurally similar to its own generation pattern. The authors acknowledge this and use multiple models to mitigate it, but it remains a minor methodological concern."}, "questions": {"value": "1. Beyond using multiple MLLMs for data generation, what specific steps were taken in the prompt design and expert annotation phase to ensure that the benchmark does not overfit to the stylistic or reasoning patterns of the specific generator models (like GPT-4o and DeepSeek-R1)?\n2. The results show that even top-performing models struggle with hallucination rejection (HRR < 27%). Did your error analysis reveal any common patterns in the types of hallucinations that models fail to reject? For instance, are they more likely to miss factual errors or logical inconsistencies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GHBBeNRmHi", "forum": "moeOrHkDg2", "replyto": "moeOrHkDg2", "signatures": ["ICLR.cc/2026/Conference/Submission3923/Reviewer_zNQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3923/Reviewer_zNQ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381556997, "cdate": 1761381556997, "tmdate": 1762917099348, "mdate": 1762917099348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark for adjuvants, curated by LLMs and montiored by domain experts, which includes an open-ended Q&A, hallucination data for rejection tests, and adjuvant formal data—template-driven formal descriptions produced with expert-designed variables. The benchmark is then used to evaluate frontier open and closed source models via automatic and LLM as a judge based metrics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The dataset curation pipeline is clear and involves human experts in the loop for validation. There is a clear attempt to reduce generator–evaluator bias by using multiple MLLMs.\n2. Samples discarded by experts are used for the hallucination subset.\n3. Evaluation consists of a large set of automatic and LLM as a judge based metrics."}, "weaknesses": {"value": "1. Since some of the LLMs (e.g. GPT-4o, DeepSeek) are used in the data curation pipeline and are among top performers, studying possible advantage of these models on their curated samples is missing.\n2. It is not clear from the text if the formal description of adjuvants is used in the data curation process or in evaluation, despite being framed as a main contribution."}, "questions": {"value": "1. How exactly is the formal description used or intended to be used now? from the title/abstract it sounds central, but did not understand from the text where it is utilized other.\n2. Is it possible to include an ablation study on the possible bias of models on their own curated samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JalJbNm7Me", "forum": "moeOrHkDg2", "replyto": "moeOrHkDg2", "signatures": ["ICLR.cc/2026/Conference/Submission3923/Reviewer_2T68"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3923/Reviewer_2T68"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921815196, "cdate": 1761921815196, "tmdate": 1762917099023, "mdate": 1762917099023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces the first adjuvant-focused benchmark: 1,294 expert-vetted open-ended Q&A pairs, 69 hallucination cases, and 1,364 “formal descriptions” of mechanisms/design.\n\nEvaluates 29 MLLMs (11 closed, 18 open) on domain QA, hallucination rejection, instruction following; reports STS/BERTScore + an LLM-as-judge rubric (Similarity, Rationality, Inclusiveness).\n\nBest reported: OpenAI-o1 (closed) and DeepSeek-R1/V3, Qwen3-x (open). Overall hallucination rejection remains low.\n\nProposes a formal abstraction framework (variables/functions for immune processes) as scaffolding for future neuro-symbolic models.\n\nClaims novelty, releases benchmark + code (anonymous), and describes expert annotation pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear problem framing & real need: adjuvants are underserved by existing bio/chem benchmarks.\n\nOpen-ended evaluation better matches scientific reasoning than MCQ; includes a multimodal slice.\n\nScale + expert pass: sizable curated set with domain-expert filtering; explicit hallucination subset.\n\nMethodical reporting: model families, “think” vs “inference” comparison, and some robustness checks (e.g., HRR with mean±SD)."}, "weaknesses": {"value": "Evaluator circularity / bias: GPT-4o & DeepSeek-R1 both generated data and act as judges; risk of self-preference and contamination despite multi-model generation.\n\nSmall hallucination set (n=69): limits power; HRR estimates are noisy and may not generalize.\n\nMetric fragility: STS/BERTScore conflate surface similarity with correctness; LLM-as-judge can reward verbosity/formatting; limited length controls and no strong human-only adjudication for the main tables.\n\nIncomplete reliability stats: no inter-annotator agreement numbers (κ/α), limited statistical tests (no CIs/bootstraps for main scores), and few ablations."}, "questions": {"value": "IAA & QC: What are inter-annotator agreement stats (Cohen’s κ/Krippendorff’s α) for validity and hallucination labels? How often did adjudication change labels?\n\nLength & bias controls: Did you apply length-controlled judging (e.g., LC-AlpacaEval-style) or verbosity penalties to mitigate LLM-judge bias?\n\nJudge neutrality: Can you re-run main tables with held-out judges (e.g., Claude/Gemini + a smaller open model) and report rank correlations vs human scores?\n\nAblations: Remove items originating from GPT-4o / DeepSeek-R1 and re-evaluate those same models; how do rankings shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r3SrSfrYFb", "forum": "moeOrHkDg2", "replyto": "moeOrHkDg2", "signatures": ["ICLR.cc/2026/Conference/Submission3923/Reviewer_2Z13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3923/Reviewer_2Z13"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049115350, "cdate": 1762049115350, "tmdate": 1762917098838, "mdate": 1762917098838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces the first adjuvant-focused benchmark: 1,294 expert-vetted open-ended Q&A pairs, 69 hallucination cases, and 1,364 “formal descriptions” of mechanisms/design.\n\nEvaluates 29 MLLMs (11 closed, 18 open) on domain QA, hallucination rejection, instruction following; reports STS/BERTScore + an LLM-as-judge rubric (Similarity, Rationality, Inclusiveness).\n\nBest reported: OpenAI-o1 (closed) and DeepSeek-R1/V3, Qwen3-x (open). Overall hallucination rejection remains low.\n\nProposes a formal abstraction framework (variables/functions for immune processes) as scaffolding for future neuro-symbolic models.\n\nClaims novelty, releases benchmark + code (anonymous), and describes expert annotation pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear problem framing & real need: adjuvants are underserved by existing bio/chem benchmarks.\n\nOpen-ended evaluation better matches scientific reasoning than MCQ; includes a multimodal slice.\n\nScale + expert pass: sizable curated set with domain-expert filtering; explicit hallucination subset.\n\nMethodical reporting: model families, “think” vs “inference” comparison, and some robustness checks (e.g., HRR with mean±SD)."}, "weaknesses": {"value": "Evaluator circularity / bias: GPT-4o & DeepSeek-R1 both generated data and act as judges; risk of self-preference and contamination despite multi-model generation.\n\nSmall hallucination set (n=69): limits power; HRR estimates are noisy and may not generalize.\n\nMetric fragility: STS/BERTScore conflate surface similarity with correctness; LLM-as-judge can reward verbosity/formatting; limited length controls and no strong human-only adjudication for the main tables.\n\nIncomplete reliability stats: no inter-annotator agreement numbers (κ/α), limited statistical tests (no CIs/bootstraps for main scores), and few ablations."}, "questions": {"value": "IAA & QC: What are inter-annotator agreement stats (Cohen’s κ/Krippendorff’s α) for validity and hallucination labels? How often did adjudication change labels?\n\nLength & bias controls: Did you apply length-controlled judging (e.g., LC-AlpacaEval-style) or verbosity penalties to mitigate LLM-judge bias?\n\nJudge neutrality: Can you re-run main tables with held-out judges (e.g., Claude/Gemini + a smaller open model) and report rank correlations vs human scores?\n\nAblations: Remove items originating from GPT-4o / DeepSeek-R1 and re-evaluate those same models; how do rankings shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r3SrSfrYFb", "forum": "moeOrHkDg2", "replyto": "moeOrHkDg2", "signatures": ["ICLR.cc/2026/Conference/Submission3923/Reviewer_2Z13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3923/Reviewer_2Z13"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049115350, "cdate": 1762049115350, "tmdate": 1763578899962, "mdate": 1763578899962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a domain-specific benchmark and formalized knowledge resource for evaluating LLMs and MLLMs in vaccine adjuvant research. The benchmark includes 1,294 open-ended QA pairs, 69 hallucination-focused samples, and 1,364 formalized entries describing design principles and immunological mechanisms. The authors evaluate 29 models across STS/BERTScore, LLM-as-a-Judge scoring, and hallucination rejection, finding that models such as OpenAI-o1 and DeepSeek-R1 perform best. The formalized representations aim to support future neural–symbolic reasoning and domain-specialized systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The work introduces the first benchmark specifically targeting the adjuvant domain, covering factual QA, hallucination rejection, and mechanistic reasoning. Existing biomedical benchmarks provide limited coverage of this problem space.\n\n- Data collection and cleaning follow a clear process involving literature extraction, expert annotation, and filtering. The evaluation spans a broad range of closed- and open-source models with consistent prompt settings.\n\n- Metrics, data distributions, annotation pipeline, and prompts are clearly documented, and the appendix provides formulas and templates that increase reproducibility.\n\n- The benchmark offers the community a structured foundation for evaluating scientific reasoning and reliability in immunology-related tasks. The results provide insights into how “reasoning” models and domain-informed knowledge can help mitigate hallucination."}, "weaknesses": {"value": "- Potentially misleading use of “multimodal.”\nThe title and main text repeatedly claim evaluation of 29 “MLLMs,” yet some are purely text-based LLMs. If all models were truly multimodal, OCR would not be required. The paper should distinguish LLM vs. MLLM more precisely, adjust wording accordingly, and expand evaluation to include domain-relevant multimodal biomedical models (e.g., LLaVA-Med).\n- Multimodal evaluation design masks visual differences.\nAll images are transformed into text via OCR, eliminating spatial and structural cues essential for true visio-linguistic reasoning. This turns the task largely into text-only QA and may suppress advantages of genuinely multimodal models. OCR noise also introduces confounding error. The paper should clearly acknowledge this limitation and ideally present an “original image vs. OCR-only” comparison or dual leaderboard.\n- Lack of critical experimental details hurts reproducibility.\nKey inference settings (temperature, top-p, random seeds) are not reported. Important dataset details—such as the list of source papers, annotation protocol, annotator backgrounds, and inter-annotator agreement—are also missing. Some model performances in Table 5 are extremely close, yet no error bars or significance tests are provided.\n- Risk of self-enhancement bias in LLM-as-a-Judge.\nDeepSeek-R1 (and potentially related models) played roles in both data generation and evaluation, raising the possibility of stylistic bias. Although the appendix provides human–LLM correlation analysis, this limitation should be explicitly acknowledged in the main text, and future versions should consider using a disjoint panel of judges.\n- Formalized entries lack empirical validation.\nThe paper introduces 1,364 formalized entries as a resource for neural–symbolic reasoning, but no experiments demonstrate that they improve retrieval, consistency, or hallucination reduction. Even a small controlled study would substantially strengthen this contribution.\n- Data release and licensing are not clearly described.\nIt is unclear whether images, textbook content, or literature excerpts can be redistributed legally. Without explicit licensing clarification, the dataset’s usability and adoption may be limited."}, "questions": {"value": "- Why does the paper refer to all 29 evaluated models as “MLLMs” when many are text-only? Will the terminology be corrected to clearly distinguish LLM vs. MLLM?\n- Since all images were converted to text via OCR, is the benchmark effectively evaluating text-based reasoning rather than multimodal reasoning? Can the authors provide a comparison between OCR-only input and native visual input for top multimodal models?\n- Could the authors report key inference settings (e.g., temperature, top-p, random seed)? The absence of these prevents exact reproduction of the reported results.\n- Is there any empirical evidence that the 1,364 formalized entries improve retrieval, consistency, or hallucination reduction? Even a small-scale downstream experiment would substantiate this contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "61RfCOAOPI", "forum": "moeOrHkDg2", "replyto": "moeOrHkDg2", "signatures": ["ICLR.cc/2026/Conference/Submission3923/Reviewer_Z32D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3923/Reviewer_Z32D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093153451, "cdate": 1762093153451, "tmdate": 1762917098545, "mdate": 1762917098545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}