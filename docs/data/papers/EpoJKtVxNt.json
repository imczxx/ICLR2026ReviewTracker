{"id": "EpoJKtVxNt", "number": 15510, "cdate": 1758252167860, "mdate": 1759897302147, "content": {"title": "Not Errors but Guardians: Understanding Sink Tokens in Multimodal LLMs", "abstract": "Multimodal large language models (MLLMs) achieve remarkable success in vision–language tasks but remain prone to hallucination, often attributed to abnormal attention behaviors. \nA recurring phenomenon is the emergence of attention sinks—tokens that absorb large amounts of attention despite limited semantic content. \nWhile previously regarded as artifacts that exacerbate hallucination, we show that in MLLMs certain tokens within system prompts act as stable, system-level attention sinks. \nThrough causal interventions including masking and content substitution, we find these tokens serve critical functions: anchoring attention to ensure computational stability, influencing outputs, and supporting internal state transitions. \nBuilding on this, we propose the Attention-Budget Hypothesis, which reframes modality bias as a trade-off in attention allocation. \nGuided by this perspective, we design SPEAR (Sink-PrEserving Attention Reallocation), an intervention that boosts visual attention while preserving sink functions, achieving effective hallucination mitigation without degrading reasoning. \nOur work provides the first systematic characterization of system-level attention sinks in MLLMs and highlights their functional role in both model stability and multimodal reasoning.", "tldr": "", "keywords": ["Sink token", "Modality bias", "Hallucination"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97041658d37d09765a3099d9cc1fb1ed23798b1d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper delves into the phenomenon of attention sinks in Multimodal Large Language Models (MLLMs), specifically focusing on sink tokens within system prompts. While these tokens were initially considered problematic for hallucinations, the authors argue that they serve critical functional roles, including stabilizing attention and facilitating task progression. Through causal interventions like attention masking, zeroing, and content substitution, the authors explore how these tokens contribute to computational stability, information flow, and higher-order reasoning. Additionally, the paper introduces the Attention-Budget Hypothesis and proposes SPEAR, a novel intervention that reallocates attention to improve visual processing while preserving the stability provided by sink tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper challenges the conventional wisdom surrounding sink tokens, proposing that they perform critical functions related to stability and reasoning, not just artifacts of hallucinations.\n2.  The Attention-Budget Hypothesis provides a fresh perspective on modality bias and attention allocation, making a strong case for the trade-offs involved in boosting visual attention while maintaining system stability.\n3.  The proposed SPEAR intervention is a novel and effective method for reallocating attention to mitigate hallucinations without sacrificing reasoning capabilities, outperforming the baseline and alternative methods like VAF."}, "weaknesses": {"value": "1. The paper primarily compares the proposed SPEAR method with Visual Amplification Fusion (VAF). However, it lacks a broader comparison with other hallucination mitigation strategies (e.g., [1][2][3]) that could provide a more comprehensive understanding of the method’s performance\n2. The paper could benefit from a more thorough error analysis, including failure modes and situations where SPEAR or other interventions might not perform as expected.\n3. Some minor typos (e.g., “vi￾sion–language”) and inconsistent notation ($T_{\\text{sys}\\backslash\\text{sink}}$ vs. $T_{\\text{sys}} \\setminus T_{\\text{sink}}$). Figures are informative but could use clearer legends.\n4. I think this version of the paper is rough, the authors should improve it before choosing to submit ICLR.\n\n[1] VCD: Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding, CVPR 2024.\n\n[2] OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation, CVPR 2024.\n\n[3] Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models, ICML 2025."}, "questions": {"value": "1. How exactly do sink tokens act as “state-machine” elements? Are their activations correlated with control tokens (e.g., `<s>`, `</s>`, or position encodings)?  \n2. Is this behavior consistent across transformer architectures with rotary vs. absolute position embeddings?\n3. Have you tried tracking divergence or entropy in attention distributions post-intervention?\n4. Equation (6) formalizes $\\sum_T \\Delta \\alpha_i(T) = 0$. Can you empirically validate the *budget conservation* assumption across layers?  \n5. Does it introduce any latency or memory overhead compared to VAF?\n6. Could preserving sink tokens reinforce biases or hallucinations under adversarial prompting?  \n7. The paper identifies sink tokens via high attention occupancy. Is there a dynamic component, e.g., per-layer thresholding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLnDQtIGcG", "forum": "EpoJKtVxNt", "replyto": "EpoJKtVxNt", "signatures": ["ICLR.cc/2026/Conference/Submission15510/Reviewer_fCro"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15510/Reviewer_fCro"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532056267, "cdate": 1761532056267, "tmdate": 1762925796702, "mdate": 1762925796702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank the reviewers for their time and constructive feedback. We are encouraged that the reviewers recognized the novelty and rigor of our work. Specifically, we appreciate the consensus on our core contributions:\n\nConceptual Novelty (\"Guardians\" vs. \"Errors\"): Reviewers e8ts and fCro commended our work for offering a \"compelling and insightful reframing\" of attention sinks, marking a \"significant departure\" from the prevailing view that treats them merely as artifacts. They appreciated moving the discourse beyond bug-fixing toward a \"nuanced functional analysis.\"\n\nRigorous Methodology (Causal Interventions): Reviewers e8ts and ry9f highlighted the strength of our \"rigorous and systematic methodology.\" Reviewer ry9f, in particular, noted that our graduated series of causal interventions (from masking to value substitution) provides a \"robust way to probe\" the token functions, finding the evidence for their role in higher-order processing \"compelling.\"\n\nTheoretical & Practical Value (Hypothesis & SPEAR): Reviewer fCro praised the Attention-Budget Hypothesis as a \"fresh perspective\" on modality bias and recognized SPEAR as a \"novel and effective\" intervention. Reviewer R7Wm also noted the importance of the topic and the clarity of our presentation.\n\nIn this rebuttal, we have carefully addressed the reviewers' concerns, particularly regarding the validity of the VAF baseline (Reviewer ry9f), the distinction from LLM-based findings (Reviewer R7Wm), and comparisons with other baselines (Reviewer fCro). We have included new experiments and analyses, which we believe strongly substantiate our claims and demonstrate the robustness of our approach."}}, "id": "0uUZjqE0QR", "forum": "EpoJKtVxNt", "replyto": "EpoJKtVxNt", "signatures": ["ICLR.cc/2026/Conference/Submission15510/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15510/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15510/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763709509911, "cdate": 1763709509911, "tmdate": 1763709509911, "mdate": 1763709509911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper characterizes attention sinks in MLLMs. Through causal interventions, the authors demonstrate that these tokens are essential for stable inference. They propose an attention reallocation method to mitigate hallucination without decreasing attention on sink tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of attention sinks is interesting. Attention sinks are important tokens that stabilize inference and training.\n2. The paper organization and writing are easy to follow."}, "weaknesses": {"value": "1. The characterization of attention sinks in Section 3 largely recapitulates well-established findings from the LLM literature. They are common knowledge in the field and have been extensively documented in prior work. The paper does not provide novel insights beyond confirming that these patterns extend to MLLMs.\n\n2. The functional analysis in Sections 4.1 and 4.2 is already covered by existing literature:  The finding that masking attention sinks causes collapse has been demonstrated in LLMs (e.g., Xiao et al 2023). \n\nXiao et al. Efficient Streaming Language Models with Attention Sinks.\n\n3. The claim that sink tokens function as part of an internal \"state machine\" in Section 4.3 is made without adequate support.\n\n4. The proposed SPEAR method lacks technical innovation. It is a commonly adopted way widely utilized in previous works (e.g., Yang et al. 2025)\n\nYang et al. Understanding and Mitigating Hallucinations in Large Vision-Language Models via Modular Attribution and Intervention. ICLR 2025\n\n5. The logic is not sound, attention sink is essential for stability does not mean that they are not harmful (not responsible for hallucination). \n\n5. Experiments are not sufficient to validate the effectiveness of this method. Should include other metrcis such as Chair_s and Chair_i. The improvement is also marginal."}, "questions": {"value": "Attention sinks are important tokens that stabilize inference and training, but fundamental questions remain fully understudied: how they are formed, why they are necessary, whether they are sufficient, and what their side effects are."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OQIevagL02", "forum": "EpoJKtVxNt", "replyto": "EpoJKtVxNt", "signatures": ["ICLR.cc/2026/Conference/Submission15510/Reviewer_R7Wm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15510/Reviewer_R7Wm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721464772, "cdate": 1761721464772, "tmdate": 1762925796314, "mdate": 1762925796314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of \"attention sinks\"—tokens that absorb a large amount of attention—within the system prompts of Multimodal Large Language Models (MLLMs). The authors challenge the prevailing view that these sinks are artifacts to be suppressed, arguing instead that they are \"guardians\". Through a series of causal interventions (masking, value zeroing, and value substitution), the paper claims to demonstrate that these sink tokens are critical for computational stability and serve as part of an internal \"state machine\" essential for multi-step reasoning. Based on this finding, the authors propose SPEAR, a plug-and-play intervention method that mitigates hallucinations by boosting visual attention while explicitly preserving these critical sink tokens. The paper claims this method is effective at reducing hallucinations without the degradation in reasoning performance seen in other methods."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper's primary strength is the graduated series of causal interventions in Section 4. The progression from masking attention to zeroing values to mean-value substitution  is a robust way to probe the function of these tokens.\n2.The finding in Section 4.3 and Figure 4 that replacing sink token content selectively breaks multi-step reasoning while sparing simple tasks is a compelling (though qualitative) piece of evidence for their role in higher-order processing."}, "weaknesses": {"value": "1. The paper's main weakness is the invalid experimental comparison in Section 6. The VAF baseline is defined as suppressing all system tokens ($\\mathcal{S}_{VAF}=\\mathcal{T}_{sys}$), which the paper has already proven in Section 4.1 leads to catastrophic model collapse.\n2. The main results (Tables 4 & 5) are a product of this flawed setup. SPEAR outperforms VAF not because it's a better hallucination method, but because VAF (as defined by the authors) is a broken intervention that destabilizes the model. The experiment lacks a valid, strong baseline.\n3. The claim that sink tokens function as an \"internal 'state machine'\"  is a significant exaggeration. The evidence is based on a single, qualitative example in Figure 4. While the tokens are clearly crucial for complex reasoning, \"state machine\" implies a level of procedural, computational function that is not fully substantiated by the provided data."}, "questions": {"value": "1. Can the authors justify their definition of the VAF baseline? Did the original VAF paper (Yin et al., 2025) explicitly recommend suppressing *all* system tokens, including known stability anchors like the `<s>` token? Or is this a \"strawman\" definition created for this paper?\n2. To demonstrate any real value, SPEAR must be compared against a *valid* baseline. How does SPEAR compare to a \"VAF-Fixed\" baseline—i.e., an implementation of VAF that *also* preserves the sink tokens ($\\mathcal{S} = \\mathcal{T}_{sys\\backslash sink} \\cup \\mathcal{T}_{user} \\cup \\mathcal{T}_{out}$)?\n3. Following from Question 2: The definition of SPEAR and a \"VAF-Fixed\" (as defined above) appear to be identical. Is the entire methodological contribution of this paper simply the observation that the VAF baseline must be implemented correctly to avoid model collapse?\n4. Can the authors provide *quantitative* data to support the \"state machine\" claim? Specifically, what are the full benchmark scores (e.g., on SQA, MM-Vet) for the mean-value substitution intervention (Section 4.3), not just the qualitative example in Figure 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vDQe2Ja4WW", "forum": "EpoJKtVxNt", "replyto": "EpoJKtVxNt", "signatures": ["ICLR.cc/2026/Conference/Submission15510/Reviewer_ry9f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15510/Reviewer_ry9f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971208542, "cdate": 1761971208542, "tmdate": 1762925795845, "mdate": 1762925795845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the view that attention sinks in MLLMs are merely errors, arguing instead that specific tokens within the system prompt act as functional \"guardians\" essential for model stability and reasoning. Through a series of causal interventions, the authors demonstrate that these sink tokens are crucial for computational stability and act as part of an internal \"state machine\" for complex tasks. Building on this insight, the paper proposes the \"Attention-Budget Hypothesis\" and introduces SPEAR, a hallucination mitigation method that preserves these critical sink tokens while reallocating attention from non-essential text to visual tokens. Experimental results show SPEAR effectively mitigates hallucination without compromising reasoning abilities, thus validating the functional importance of sink tokens."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work offers a compelling and insightful reframing of attention sinks. The argument that specific tokens act as functional \"guardians\"—essential for stability and reasoning—is a significant departure from the prevailing view that treats them as artifacts to be mitigated. This perspective moves the discourse beyond simple bug-fixing toward a more nuanced functional analysis of the model's internal mechanisms.\n\n2. The claims are substantiated by a rigorous and systematic methodology. The use of well-designed causal intervention experiments—including attention masking and value vector manipulation—goes beyond correlational observations to convincingly dissect the functional roles of these \"guardian\" tokens. This robust experimental design provides strong, direct evidence for their necessity in maintaining computational stability and tracking task states, making the conclusions highly credible."}, "weaknesses": {"value": "1. The conclusions are derived primarily from a specific model architecture (LLaVA) and a set of curated, synthetic tasks. It remains unclear whether the \"guardian\" mechanism is a universal phenomenon across MLLMs or an emergent property specific to certain architectural choices and training paradigms. The claims would be substantially strengthened by validation on a more diverse set of models, including different open-source families and closed-source systems, as well as on more complex, open-ended real-world scenarios.\n\n2. The severity of the causal interventions—such as completely zeroing out attention scores or value vectors—may introduce confounds. This \"hard\" intervention forces the model into a highly unnatural, out-of-distribution state it would never encounter during training. Therefore, the observed performance collapse might not be solely due to the loss of the guardian's function, but could also be a result of the model's general fragility to such drastic internal state perturbations. Softer interventions, such as dampening activations to a low, non-zero level or replacing them with activations from a neutral token, could more precisely isolate the specific functional contribution of these tokens while minimizing the risk of inducing a general model failure."}, "questions": {"value": "1. The emergence of a \"guardian\" token is a fascinating finding. Could you elaborate on the potential architectural or training precursors for this phenomenon? For instance, do you hypothesize that this is tied to the specific vision-language connector design in LLaVA, or perhaps to the instruction-tuning data format? How might this behavior differ in models with more deeply integrated cross-modal attention from early layers?\n\n2. Regarding the functional role of the guardian token, your interventions effectively demonstrate its necessity. But do they reveal the dynamics of its contribution? For example, is its function a binary \"on/off\" switch, where any significant disruption causes catastrophic failure, or is it more of a graded, stabilizing influence? Have you considered the effects of \"softer\" interventions, such as dampening the guardian's value vector magnitude rather than nullifying it, to see if performance degrades more gracefully?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Qq2QQrLORZ", "forum": "EpoJKtVxNt", "replyto": "EpoJKtVxNt", "signatures": ["ICLR.cc/2026/Conference/Submission15510/Reviewer_e8ts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15510/Reviewer_e8ts"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985957991, "cdate": 1761985957991, "tmdate": 1762925795433, "mdate": 1762925795433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}