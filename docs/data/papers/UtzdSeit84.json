{"id": "UtzdSeit84", "number": 15163, "cdate": 1758248475508, "mdate": 1759897324103, "content": {"title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM: a unified, scalable framework for building Process Reward Models (PRMs) in multimodal settings. We first build MM-Policy-8B, a strong multimodal policy model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting MM-PRM-8B is used to rerank candidate reasoning paths and achieves significant improvements across both in-domain and out-of-domain benchmarks. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at https://anonymous.4open.science/r/MM-PRM-F608/.", "tldr": "We present MM-PRM, a unified and scalable framework for training multimodal Process Reward Models with MCTS-generated step-level supervision, significantly improving mathematical multi-step reasoning performance without manual annotations.", "keywords": ["Multimodal Reasoning", "Process Reward Model", "Monte Carlo Tree Search"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f37264e5213f0f5a9e47a7b514ebab1bb0d2500.pdf", "supplementary_material": "/attachment/63c932af6a55be62c9750a8f32762348a208fd0d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MM-PRM, a three-stage framework for training process reward models for multimodal mathematical reasoning: it first trains a multimodal policy model, then uses MCTS on a newly curated dataset MM-K12 to automatically generate ~700k step-level soft labels, and finally trains an 8B PRM to rerank reasoning paths at inference. Experiments show consistent accuracy gains across multiple multimodal math benchmarks and diverse base models, outperforming outcome-based reward models and demonstrating modest modality-agnostic generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Large-scale policy model and prm training data contribution.\n* The paper provides clear ablations on candidates N, learning rate and labeling type that illustrates useful guidance for practical prm."}, "weaknesses": {"value": "* The work tends toward engineering applications, lacking new insights. For example, I found that many similar works at the algorithm level or involving multimodal process supervision—such as OmegaPRM [1], ViLPRM [2],  URSAPRM [3], and VisualPRM [4]—were not included in the comparisons of MM-PRM. What are the differences in terms of data pipelines and final results compared to these approaches?\n\n* As a reward model, why isn't MM-PRM used for online RL evaluation? To my knowledge, works such as EurusPRM[5] and DeepSeekMath [6] have already begun exploring the application of PRMs in online RL. TTS is merely a shallow validation substitute with high inference cost.\n\n* Table 1 lacks comparisons with baselines—for example, MLLM-as-ORM, other PRMs (such as PRM400K and Math-Shepherd), or even self-consistency. I found the self-consistency comparison in Appendix F, but this should be directly included in the main table for clearer visualization.\n\n* The improvement is marginal. For instance, in Appendix F.1, the minimum calculation and average calculation methods improve upon self-consistency by only 1.0 and 2.3 percentage points on average, respectively. Methods such as MLLM-as-ORM or MLLM-as-Generative-PRM may achieve even better TTS performance.\n\n* Many experimental details are missing. For example, what is the inference cost during the MCTS phase? How many GPU hours were spent on rollouts? How were the parameters for rollout data generation determined? Were there empirical considerations for accuracy and diversity?\n\n* Lack of generalization validation: the base models are primarily limited to the InternVL-series MLLMs; experiments on models such as Qwen2.5-VL, InternVL3, and Kimi-VL-MoE are necessary.\n\n[1] Improve Mathematical Reasoning in Language Models by Automated Process Supervision. Arxiv 2406.\n\n[2] ViLBench: A Suite for Vision-Language Process Reward Modeling. EMNLP 2025.\n\n[3] Unlocking Multimodal Mathematical Reasoning via Process Reward Model. NeurIPS 2025.\n\n[4] VisualPRM: An Effective Process Reward Model for Multimodal Reasoning. Arxiv 2503.\n\n[5] Process Reinforcement Through Implicit Rewards. ICML 2025.\n\n[6] Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Arxiv 2402."}, "questions": {"value": "* Typo in Line 291. Accuracy on mathvista seems wrong.\n* Typo in Line 502. Inconsistency with MMK-12 and MM-K12.\n* Cross-domain generalization: Beyond math, can MM-PRM operate on other multimodal reasoning tasks (e.g., chart QA, scientific diagrams)?\n* Have you tried using MM-PRM as a critic in an RL fine-tuning loop for the policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gw5Ye7tihP", "forum": "UtzdSeit84", "replyto": "UtzdSeit84", "signatures": ["ICLR.cc/2026/Conference/Submission15163/Reviewer_gHGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15163/Reviewer_gHGm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604883271, "cdate": 1760604883271, "tmdate": 1762925476009, "mdate": 1762925476009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MM-PRM, a multimodal Process Reward Model that improves the performance of complex multi-step reasoning in Multimodal Large Language Models (MLLMs). By leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, the authors generate over 700,000 step-level annotations without human labeling, and demonstrate substantial performance gains on both in-domain and out-of-domain benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive Data and Model Pipeline: The paper offers a detailed and well-executed approach to dataset curation (MM-K12) and the training of the MM-PRM model. The dataset, containing 10,000 multimodal math problems, is a significant contribution to the field.\n\n2. Open-source Resources: The authors provide both the dataset and code, enabling reproducibility and further research within the community."}, "weaknesses": {"value": "1. Clarification of the Data Cleaning Pipeline: The authors use Qwen2.5-72B-Instruct for data cleaning in the policy model construction stage. Since Qwen2.5 is not a multimodal model, could this introduce biases or incorrect visual inputs? Further discussion is needed.\n\n2. Lack of Baseline Comparisons: The paper could benefit from comparisons against other models and approaches in the multimodal reasoning space, such as GPT or Gemini. This would provide a clearer context for evaluating MM-PRM's effectiveness.\n\n3. Limited Ablation Studies: The paper primarily focuses on multimodal math benchmarks. It would be valuable to include ablation studies on conventional math benchmarks and other reward models to better understand the specific advantages of MM-PRM.\n\n4. Incorporation of More Advanced Inference Techniques: The BoN evaluation method used during inference could potentially be enhanced with more sophisticated techniques. Further exploration of advanced PRM inference strategies would be beneficial."}, "questions": {"value": "Clarify Novelty in the Pipeline: While the paper presents a solid technical approach, it would be useful to elaborate on the novelty of the MM-PRM pipeline compared to existing models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "B7RfB1QAtH", "forum": "UtzdSeit84", "replyto": "UtzdSeit84", "signatures": ["ICLR.cc/2026/Conference/Submission15163/Reviewer_hVgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15163/Reviewer_hVgs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736749148, "cdate": 1761736749148, "tmdate": 1762925475641, "mdate": 1762925475641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MM-PRM, a framework for building Process Reward Models in multimodal tasks. It consists of policy construction, process supervision generation, and PRM training. The authors demonstrate how it works on math, contributing a PRM that is shown to improve test-time scaling performance. The paper also makes a dataset contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper is well-written and overall easy to follow.\n+ The improvement and generalization ability of MM-Policy-8B seems well-supported.\n+ The analysis in Section 5 appears well-done."}, "weaknesses": {"value": "- The central contribution, the MM-PRM framework, does not seem to be as novel as the authors claim it to be. The paper \"VisualPRM: An Effective Process Reward Model for Multimodal Reasoning\" has proposed something very similar.\n- The 10k math problems in MM-K12 are all collected from existing benchmarks. The authors say that human verification is performed to select questions from the existing benchmarks. What are some inclusion criteria? What are characteristics of included/excluded problems? What is the filtering ratio? These details appear to be missing. If the paper wants to claim the dataset as one of its core contributions I would expect to see more originality or proof of improved quality in MM-K12 compared to the original benchmarks.\n- The authors frame MM-PRM as generalizable across MM settings, but have only evaluated it on math. It is unclear how this can generalize to other domains."}, "questions": {"value": "- The paper says \"By applying binary search, the algorithm efficiently pinpoints the earliest step at which the reasoning begins to deviate.\" How does this work exactly? Is there a threshold of MC score below which you consider a node a deviation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "omw90rxI9z", "forum": "UtzdSeit84", "replyto": "UtzdSeit84", "signatures": ["ICLR.cc/2026/Conference/Submission15163/Reviewer_SVXa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15163/Reviewer_SVXa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982933697, "cdate": 1761982933697, "tmdate": 1762925475206, "mdate": 1762925475206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MM-PRM, a three-stage framework that adapts process reward modeling to multimodal math reasoning. The pipeline is straightforward: a vision-language policy model is trained to produce step-by-step traces; an MCTS procedure auto-labels intermediate steps for correctness from a curated K-12 seed set (no human annotators); and a process reward model is then trained on these labels to re-rank Best-of-N solution paths at inference. In my view, the contribution is primarily a careful engineering extension of text-only PRMs to the vision-text setting, coupled with a sensible data recipe (MM-K12) and a set of practical training tips (notably small learning rates and soft step labels). Empirically, MM-PRM acts as a drop-in selector that yields consistent, non-trivial improvements on standard visual-math benchmarks and across smaller and larger backbones, without changing the base generator. The paper positions MM-PRM as a “unified, scalable” framework and suggests applicability beyond math; I find the core pipeline clear and replicable, though the evidence presented is strongest within the math domain, with claims of broader generality reading as promising but still to be demonstrated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Modular, reproducible design. The three-stage flow—policy training → MCTS step-labeling → PRM re-ranking—is easy to reason about, isolates responsibilities, and can be dropped into existing VLM stacks without retraining the generator end-to-end.\n\n- Human-free process supervision at scale. Using MCTS to localize first-error steps produces dense, step-level signals from a small, curated seed—practically valuable when human annotation of chains is infeasible.\n\n- Consistent test-time lifts as a selector. PRM re-ranking yields reliable accuracy bumps across several visual-math benchmarks and backbone sizes; it’s a low-risk, plug-in way to monetize extra sampling (Best-of-N) rather than redesigning the model.\n\n- Actionable training guidance. Small learning rates and soft labels make PRM training more stable and performant—concrete knobs practitioners can reuse without extensive hyper-sweeps.\n\n- Reasonable data curation. A seed set with unique, verifiable answers enables automatic labeling and trustworthy evaluation; that choice aligns well with the process-supervision objective.\n\n- Evidence of portability. The PRM trained in one setup transfers to other model sizes/backbones with positive deltas, suggesting the scorer is not overly tied to a single policy."}, "weaknesses": {"value": "- Incremental novelty. The contribution largely ports known text-PRM + MCTS pipelines to the multimodal setting; there’s limited algorithmic innovation beyond adding an image encoder and adapting prompts.\n\n- Scope overreach. Claims of generality to non-math domains are not empirically supported; the approach leans on tasks with deterministic verifiers, which many target domains lack.\n\n- Opaque compute economics. End-to-end cost (MCTS rollouts, PRM training, Best-of-N inference) is not quantified, making it hard to judge practicality vs. simpler ensembling/self-consistency.\n\n- Selector ceiling and N-dependence. PRM cannot exceed the best sampled candidate; gains saturate with (N) and hinge on the base policy being strong/diverse enough—limits not fully characterized.\n\n- Label quality unvalidated. No human audit or noise analysis of MCTS step labels; if labels conflate “path leads to success” with “local step correctness,” PRM may learn policy-specific shortcuts.\n\n- Aggregator sensitivity. Multiple heuristics are explored to collapse step scores, but the choice appears tuned; without a fixed or learned aggregator, there’s risk of cherry-picking per dataset."}, "questions": {"value": "- Why the GPT-4o MathVista drop with PRM? Please analyze failure cases and state the aggregator used for all closed-source rows.\n\n- Compute budgets. Report wall-clock/GPU costs for (a) MCTS labeling, (b) PRM training, (c) BoN inference at N=16/32.\n\n- Baseline gaps. Add self-consistency/majority vote and a tuned outcome-reward re-ranker as baselines to quantify PRM advantage.\n\n- Label auditing. Provide a small human audit of MCTS step labels (precision/recall on “correct step”) and discuss mitigation if noisy.\n\n- Fix the aggregator. Re-run with a single pre-specified aggregator (or a learned aggregator) across all datasets to remove tuning confound.\n\n- Beyond selection. Any preliminary results on PRM-guided policy training (RL/DPO) to move gains from reranking into the generator?\n\n- Topic imbalance. Report per-topic results on MM-K12 and, if possible, rebalanced training to test sensitivity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g1YKP3cyiR", "forum": "UtzdSeit84", "replyto": "UtzdSeit84", "signatures": ["ICLR.cc/2026/Conference/Submission15163/Reviewer_ADDG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15163/Reviewer_ADDG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762491709046, "cdate": 1762491709046, "tmdate": 1762925474411, "mdate": 1762925474411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}