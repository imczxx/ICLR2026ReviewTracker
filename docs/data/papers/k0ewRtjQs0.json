{"id": "k0ewRtjQs0", "number": 811, "cdate": 1756819373847, "mdate": 1763710161002, "content": {"title": "Towards W2A4 LLM Inference: Hybrid SQ-VQ Framework with Adaptive Error Compensation", "abstract": "Quantization presents a powerful approach for reducing the memory footprint and accelerating the inference of Large Language Models (LLMs). However, it faces a fundamental dilemma: computation-friendly Scalar Quantization (SQ) suffers performance degradation at ultra-low bit-widths, whereas memory-friendly Vector Quantization (VQ) maintains higher accuracy but fails to reduce computational demand. As a result, achieving both computational efficiency and high-fidelity compression in ultra-low-bit regimes (e.g.W2A4) remains a tough challenge. To address this, we propose $\\textbf{AEC-SVQ}$, a hybrid framework that synergistically integrates SQ ,VQ for high-performance, ultra-low-bit LLM inference. The framework is built on three  innonvations. To simultaneously address the disparate distributional challenges presented by weight VQ, activation SQ, and codebook integer quantization, we introduce a $\\textbf{learned rotation-smooth transformation}$ that adaptively promotes quantization-friendly distributions for weights, activations, and codebooks within the hybrid SQ–VQ scheme. To mitigate the compounding errors caused by the independent quantization of weights and activations, we propose the $\\textbf{Cumulative-Error-Aware Vector Quantization (CEAVQ) algorithm}$. CEAVQ adjusts weights to compensate for the cumulative error from upstream quantized layers, thereby proactively aligning with the full-precision output distribution. To ensure robustness against statistical noise from limited calibration data, we introduce a closed-form, data-driven $\\textbf{Adaptive Compensation}$. It modulates the compensation strength for cumulative errors, preventing overfitting to calibration set statistics and guaranteeing stable generalization. AEC-SVQ enables a W2A4 pipeline that achieves the memory footprint of a 2-bit model while exploiting the computational efficiency of 4-bit integer arithmetic. On LLaMA-30B, it delivers a 3.6$\\times$ speedup and 7.1$\\times$ memory saving, establishing a practical frontier for ultra-low-bit LLM deployment.", "tldr": "", "keywords": ["LLM Quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4dbef3fa44af3b2dcd2318a27e72a6a708c6129.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AEC-SVQ, a quantization framework for large language models that employs scalar quantization for activations and vector quantization for weights, utilizing INT4 arithmetic units. It introduces an effective transformation for quantization, a novel quantization algorithm that accounts for cumulative errors across quantized layers, and adaptive correction methods to further enhance accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The proposal to synergistically use scalar quantization for activations and vector quantization for weights, while also quantizing the codebook to leverage low-precision ALUs, is brilliant and makes this work highly relevant to real-world practitioners.\n* AEC-SVQ is extensively compared against several existing solutions and demonstrates promising result."}, "weaknesses": {"value": "* The overall contributions of this work are somewhat difficult to grasp at a high level.\n* The speedup evaluations are incomplete."}, "questions": {"value": "* First, I would like to ask about two of the three main contributions claimed in the paper: learned transformation and CEAVQ.\n\nRegarding the learned transformation, I am curious how the proposed mechanism differs from existing learned rotation or transformation approaches such as SpinQuant or DuQuant, and why it performs better both quantitatively and qualitatively.\n\nAs for CEAVQ, I found it somewhat difficult to understand the core novelty that distinguishes it from prior work, partly due to my lack of mathematical literacy. To me, the objective function in Equation (5) does not appear particularly novel, and it seems that the key contribution lies in Equation (6) and the accompanying discussion. I would greatly appreciate a higher-level explanation of CEAVQ’s conceptual contribution beyond the mathematical formulation.\n\nMinor: Why not just write equation (5) as W^X-WX~? This seems more intuitive to me.\n\n* Second, I am wondering why decode speedup results are not included in the manuscript. I assume that this is because achieving decode speedup would require a specialized kernel supporting vector quantization with quantized codebooks. If that is the case, I understand the challenge. However, including at least a discussion on this limitation or providing partial evidence of end-to-end speedup would make the paper more valuable.\n\n* Third, low-precision floating-point formats (e.g., FP4) are becoming increasingly popular for LLM inference as newer GPUs offer native support. How would the proposed approach extend to support FP4 instead of INT4? What would you expect in terms of performance and accuracy? Would it outperform the INT4 version, or not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "49xen5t9Wz", "forum": "k0ewRtjQs0", "replyto": "k0ewRtjQs0", "signatures": ["ICLR.cc/2026/Conference/Submission811/Reviewer_QyPF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission811/Reviewer_QyPF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565999145, "cdate": 1761565999145, "tmdate": 1762915609973, "mdate": 1762915609973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to combine scalar quantization and vector quantization. Essentially the proposal is to use vector quantization on weights, but then quantize the entries inside the codebooks to INT4 representation. Activations are kept in INT4. This by itself is not novel and susceptible to excess noise. The proposed method is to enhance this quantization using learned transformations and other techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-CEAVQ provides formulas for updating weights to account for activation quantization."}, "weaknesses": {"value": "- The proposed learned transformation in Section 3.1 is given by T=O\\Lambda, i.e, a product of a rotation matrix and a smoothing matrix. An expression for the MSE of the layer's gemm output is given in terms of the weight and activation statistics. Then it is claimed that the transformation is theoretically guaranteed to minimize this MSE with a promised proof in the appendix. But the actual construction of T is not given. Therefore, the claim is essentially void.\n- The paper assumes INT4 tensor cores. These have been discontinued since the Ampere architecture. In Blackwell, there are NVFP4 tensor cores which may be more fit to explore."}, "questions": {"value": "- Please provide a construction for the learned transformation T.\n- Shouldn't CEAVQ also account for weight quantization noise?\n- Please explain why the evaluations of related works are not similar to those in the corresponding papers, e.g., quarot is claimed to have a perplexity in the 5e4 regime - but that paper itself claims good accuracy for extremely low bitwidth quantization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v6AH8muTJH", "forum": "k0ewRtjQs0", "replyto": "k0ewRtjQs0", "signatures": ["ICLR.cc/2026/Conference/Submission811/Reviewer_HrU8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission811/Reviewer_HrU8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875910199, "cdate": 1761875910199, "tmdate": 1762915609831, "mdate": 1762915609831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method that combines scalar quantization (SQ) and vector quantization (VQ) to quantize large language models (LLMs) to W2A4 precision. The approach employs a learned rotation-smooth transformation and a cumulative error aware vector quantization technique, supported by a layer-wise correction factor. Through this design, AEC-SVQ achieves both high compression rates and computational efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This LLM quantization paper presents a hybrid approach that combines SQ and VQ with a learned transformation, leading to an intuitive understanding of its benefits, supported by clear mathematical derivations. It also provides a detailed study that examines various aspects and techniques across multiple models. Additionally, it adopts a layer-wise regularization factor to prevent overfitting in the compensation process."}, "weaknesses": {"value": "Please refer to the questions below."}, "questions": {"value": "1. Beyond the combination of existing SQ and VQ techniques, what are the main novelties of this paper? The use of a learned rotation matrix has already been explored in prior works such as SpinQuant and OSTQuant, and the CEA column-wise compensation based on WX error appears similar to GPTQ, not to mention the use of codebook quantization. Please clarify which components are borrowed or inspired by prior work and which constitute the novel contributions of this paper.\n\n2. How sensitive is your method to the choice of the calibration dataset?\n\n3. Fine-tuning appears to have a significant impact on the results, which raises some concerns about the robustness of the proposed strategy. Could you elaborate on this point and confirm whether the comparisons with other works are conducted fairly (e.g., fine-tuning applied to your method but not to baselines)?\n\n4. In Section A.5.2, is the analysis referring to OSTQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a7TuASBmpX", "forum": "k0ewRtjQs0", "replyto": "k0ewRtjQs0", "signatures": ["ICLR.cc/2026/Conference/Submission811/Reviewer_vGbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission811/Reviewer_vGbd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891089375, "cdate": 1761891089375, "tmdate": 1762915609660, "mdate": 1762915609660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach for W2A4 (2-bit weight and 4-bit activation) quantization of large language models. While the motivation is clear and the experiments are conducted on relevant benchmarks, the contribution lacks sufficient novelty and fails to situate itself properly within the current literature on low-bit quantization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation is clear.\n2. Extensive experiments are done on Llama and Qwen models."}, "weaknesses": {"value": "1. The proposed method combines vector quantization for weights and scalar quantization for activations. THis is a setup that has already become standard practice in recent quantization research.\n2. Without a clear conceptual or technical innovation, the contribution falls short of the standards for publication.\n3. Several recent vector quantization approaches such as AQLM and QuIP# are not included in the comparison. These methods represent the state-of-the-art in efficient LLM quantization with vector quantization and should be considered essential baselines. The absence of such comparisons makes it difficult to assess the actual competitiveness of the proposed method. As it stands, the reported results cannot convincingly demonstrate superiority or even parity with existing solutions."}, "questions": {"value": "1. It would strengthen the work if the authors provided hardware-level latency or energy efficiency evaluations, as quantization benefits are often hardware-dependent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BOxK8Ly95P", "forum": "k0ewRtjQs0", "replyto": "k0ewRtjQs0", "signatures": ["ICLR.cc/2026/Conference/Submission811/Reviewer_uQLA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission811/Reviewer_uQLA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952203060, "cdate": 1761952203060, "tmdate": 1762915609518, "mdate": 1762915609518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}