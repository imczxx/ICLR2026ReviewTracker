{"id": "vhv48Bxpud", "number": 19211, "cdate": 1758294494722, "mdate": 1759897052019, "content": {"title": "StructZip: Compressing Large-Scale Structured Prompts to One Token via Learning Natural Language Descriptions", "abstract": "Tool use has become a central capability in large language model (LLM)-based agents, enabling them to interact with external environments through structured APIs. However, effective tool use typically requires including a large number of tool descriptions, often with complex schemas, in the context for each inference. This static and structured portion of the prompt grows linearly with the number of tools and poses a significant challenge to inference efficiency. Although prior work has explored prompt compression for long contexts, most approaches focus on unstructured text and are not optimized for the compression of structured prompts. To bridge this gap, we introduce \\textbf{StructZip}, a novel framework that transforms large structured prompts into parametric memory, which can be elicited by a single token. Our approach first \"unzips\" the structured prompt into a set of semantically equivalent question-answer (QA) pairs. By fine-tuning the LLM on these QA pairs, StructZip encodes the information into the model's parameters, making it accessible through a designated special token at inference time. We evaluate our method on three representative tasks: table-based question answering, tool-use, and closed-set text classification. Experimental results demonstrate that StructZip can compress prompts of millions of tokens into a single one while maintaining performance nearly on par with using the full, uncompressed prompts, offering a practical solution for efficient structured data handling in LLMs.", "tldr": "", "keywords": ["Prompt Compression", "LLM"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2d5ce230732688178175343270d1b7927201586.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces StructZip, a framework for large structured prompt compression (LSPC), aiming to compress long, rigidly formatted inputs (e.g., JSON schemas, API docs, tables) into a single special token that elicits equivalent semantic knowledge during inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Tackles a genuinely underexplored but highly practical problem — compressing structured prompts rather than unstructured text, which previous works largely ignored.\n- The idea of one-token elicitation for structured memory could have major implications for tool-using LLM agents and long-context efficiency."}, "weaknesses": {"value": "- The framework lacks a formal information-theoretic model linking “QA-pair training” to the capacity of the compressed token.\nFor example, there is no bound on information retention or parametric storage entropy — how much structured data can truly be embedded into a token?\n- GPT-4o is used as an untrainable reference, but StructZip models (Qwen2.5-7B, Llama3.1-8B) are fine-tuned — the comparison thus conflates compression with model specialization.\n- Training involves converting millions of structured entries into QA pairs — the paper does not quantify the data explosion or compute cost of this “unzipping” process.\n- Many implementation details (QA sampling ratios, SFT mixture percentage, optimizer configs) are omitted or buried in appendices."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XP6SoUDX08", "forum": "vhv48Bxpud", "replyto": "vhv48Bxpud", "signatures": ["ICLR.cc/2026/Conference/Submission19211/Reviewer_sQBY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19211/Reviewer_sQBY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412714198, "cdate": 1761412714198, "tmdate": 1762931202076, "mdate": 1762931202076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Defines the Large Structured Prompt Compression (LSPC) problem as using long, dense, and structure inputs (tables, JSON schemas, etc.) without paying inference token costs. Proposes StructZip which is comprised of two stages: (1) unzip the structure into reversible natural-language QA pairs via fixed templates, and (2) fine-tune the model on the QAs in a next token prediction fashion combined with a general SFT corpus while introducing a single new token whose learned embedding serves as a \"key\" to the compressed knowledge. During inference time, this token is used as a replacement for the original content."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper covers an important and interesting open problem in LLM agents."}, "weaknesses": {"value": "## Presentation Issues\n- The paper uses `\\citep{...}` instead of `\\cite{...}` throughout.\n- Line 183: my knowledge → our knowledge. Many parts in the paper require changes of this nature.\n- Reduce informal phrasing. Example, Line 250: “Here are our experimental results across three different tasks” → We report results across three tasks.\n- Improve table captions and column headers for precision and consistency (units, symbols, and ordering).\n\n## Major Concerns\n- I am highly skeptical of the results in Table 1, which reports unusually strong baselines for open models relative to GPT-4o (e.g., Chinese text classification ~90% vs ~72% for GPT-4o; English text classification where Llama-8B surpasses GPT-4o). This is plausible only under specific evaluation and prompting choices, which is not specified by the paper.\n- The approach is a bit too close to recent context compression / learned representations, but many such works are not cited (e.g., [1] is notable). I do not see a big difference in the approach of StructZip to existing methods.\n\n## Other Weaknesses\n- Besides tool calling, text classification and some table QA setups are not compelling test beds for millions → one token compression. Given that the chief motivation is for AI agents and tool calling, the study should include harder settings where structured context is truly necessary and cannot be trivially summarized (e.g., large tool catalogs used by agents across tasks, schema evolution, compositional multi-table reasoning).\n\n## Minor Comments\n- The “w/” vs “w/o” rows mix models and settings and are hard to parse.\n\nOverall, the paper is significantly below the acceptance standards of a typical ICLR paper and requires major revisions for reconsideration.\n\n\n[1] Eyuboglu, S., Ehrlich, R., Arora, S., Guha, N., Zinsley, D., Liu, E., Tennien, W., Rudra, A., Zou, J., Mirhoseini, A., & Ré, C. (2025). *Cartridges: Lightweight and General-Purpose Long Context Representations via Self-Study.*"}, "questions": {"value": "- Why wouldn’t simple next-token prediction on the original structured content (or minimally re-formatted to plain text) work as well as the QA formulation? An ablation study would be necessary. Similarly, ablations on whether including general SFT data is necessary would be nice.\n- The QA template set looks finite and hand-designed. Were templates chosen by trial-and-error to maximize results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UKnmCZlKjQ", "forum": "vhv48Bxpud", "replyto": "vhv48Bxpud", "signatures": ["ICLR.cc/2026/Conference/Submission19211/Reviewer_dqX1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19211/Reviewer_dqX1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628188919, "cdate": 1761628188919, "tmdate": 1762931201524, "mdate": 1762931201524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of inference inefficiency when large language models (LLMs) process large-scale structured prompts, such as API documentation or complex tables. The authors introduce **StructZip**, a two-stage framework that first \"unzips\" a large structured prompt into a comprehensive set of semantically equivalent natural language question-answer (QA) pairs. Second, it fine-tunes an LLM on these QA pairs mixed with general-purpose instruction data, thereby encoding the structured information into the model's parameters. At inference, this embedded knowledge can be elicited by a single special token. Experiments across table question answering, tool-use, and text classification tasks demonstrate that StructZip can compress prompts of millions of tokens into one, achieving performance nearly on par with using the full, uncompressed prompts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper has the following strengths:\n1. Clear Problem Definition: The paper clearly identifies and motivates a significant, practical problem: the failure of existing compression techniques, designed for unstructured text, to handle information-dense, syntactically rigid structured data.\n2. Comprehensive Task Evaluation: The authors validate **StructZip** across three distinct and representative tasks involving structured data: text classification (Sec. 4.1.1 ), table question-answering (Sec. 4.1.2 ), and tool invocation (Sec. 4.1.3 ), demonstrating the method's broad applicability.\n3. Comprehensive discussion and ablations: The paper analyzes compression–performance trade-offs (Sec. 5), token count effects (Fig. 2b), and QA pair coverage impacts (Table 3)."}, "weaknesses": {"value": "This paper also has the following weaknesses:\n1. Limited reproducibility details: Training hyperparameters, fine-tuning epochs, optimizer, and batch sizes are omitted (Sec. 3.2). No code or pseudo-code provided for QA generation (“fully reversible” process claimed without evidence).\n2. Insufficient methodological clarity: The methodological description is overly concise, making it difficult to fully understand how structured data are transformed and compressed into the proposed token-based representation. \n3. Limited technical contribution beyond dataset construction: Based on the description, the framework's primary contribution appears to be a specialized data curation pipeline that is then consumed by a SFT process. This raises a concern about the technical novelty of the work, as the method could be characterized as a clever application of SFT rather than a fundamentally new compression technique. (If this interpretation is incorrect, I would appreciate clarification on the novel technical components beyond the data generation.)\n4. Lack of fidelity of \"Unzipping\": The method relies on the strong claim that the QA generation is \"fully reversible\" and \"preserves the complete semantic and structural information\" (Sec. 3.1 ). This is not empirically validated. It is unclear how fidelity is ensured for extremely complex, nested data. A qualitative analysis of potential information loss during this \"unzipping\" would be beneficial."}, "questions": {"value": "Based on the identified weaknesses and aspects that are not entirely clear to me, I propose the following questions:\n1. Regarding the \"Structured Information Decoding\" stage (Sec. 3.1), could you elaborate on the specific methodology used to generate the \"diverse range of QA pairs\"? For instance, were they created using rule-based templates or by prompting an auxiliary LLM, and what steps were taken to validate that this \"unzipping\" process is \"fully reversible\" and preserves the complete information from the original structured prompt?\n2. The paper demonstrates strong results on static structured prompts. How does the StructZip framework handle dynamically changing data, such as an evolving API documentation? If a minor change is made to the source data, is it necessary to perform the entire SFT process again, or does the method support more efficient, incremental updates to the parametric memory?\n3. Table 1 shows that StructZip without the compressed token (“w/o”) still achieves strong performance, sometimes close to the compressed variant. Could the authors clarify the reason behind this observation? Does this imply partial information retention in the base fine-tuned model or an inherent alignment effect from the SFT corpus?\n4. The analysis on the number of compressed tokens (Sec. 5.2, Figure 2b) shows that performance generally saturates after ~10 tokens. However, for some tasks like TNEWS and Firefly, there appears to be a slight performance degradation when increasing the token count to 20. Do the authors have a hypothesis for this observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EQXXNqemmX", "forum": "vhv48Bxpud", "replyto": "vhv48Bxpud", "signatures": ["ICLR.cc/2026/Conference/Submission19211/Reviewer_eFqv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19211/Reviewer_eFqv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840644260, "cdate": 1761840644260, "tmdate": 1762931201114, "mdate": 1762931201114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}