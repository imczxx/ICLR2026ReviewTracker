{"id": "oEVfNf0w4B", "number": 5464, "cdate": 1757912191254, "mdate": 1763696489116, "content": {"title": "ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents", "abstract": "We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks; however, it remains challenging due to environmental inefficiency and instability during extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and GLM-4.1V-9B-Thinking, and evaluate them on the OSWorld benchmark. The GLM-ComputerRL-9B achieves a new state-of-the-art accuracy of 48.9%, demonstrating significant improvements for general agents in desktop automation. Our code and demos are available at https://computer-rl.vercel.app/.", "tldr": "", "keywords": ["large language model", "computer use agent", "reinforcement learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e9e399c7048f7d812c3edb6c68a2b3c00550236.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ComputerRL, a large-scale framework for training computer use agents that can operate desktop environments via a unified API-GUI interaction paradigm. The framework integrates (1) a distributed RL infrastructure supporting thousands of parallel virtual desktops, (2) an automated pipeline for generating APIs from LLMs to augment GUI control, and (3) a training strategy named Entropulse, which alternates between reinforcement learning (RL) and supervised fine-tuning (SFT) to mitigate entropy collapse during long training runs.\nThe proposed system is evaluated on the OSWorld and OfficeWorld benchmarks, achieving state-of-the-art results (48.9% success rate) on computer automation tasks with open-weight models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The API-GUI unification is an interesting engineering contribution that bridges the gap between human-designed interfaces and agent-level programmatic control.\n\nThe distributed RL infrastructure is impressive in scale and demonstrates strong engineering capability, enabling parallelized desktop environments at large scale.\n\nThe Entropulse idea addresses an important issue in long-horizon RL (entropy collapse), and the empirical results suggest measurable benefits in maintaining exploration and training stability.\n\nThe evaluation across multiple benchmarks (OSWorld, OfficeWorld) provides strong empirical evidence of performance improvements."}, "weaknesses": {"value": "The paper’s novelty lies primarily in implementation and scaling, not in new algorithmic contributions. The API-GUI paradigm is conceptually straightforward—it effectively automates API construction via LLMs rather than introducing a new interaction or reasoning mechanism. Similarly, the Entropulse training alternation between RL and SFT is more of a practical training schedule than a novel learning algorithm.\n\nThe training curves in the figures appear to correspond to single runs, with no error bars or indication of variance across seeds. It is therefore unclear how stable the reported improvements are. The lack of information about the number of runs or random seeds undermines the reliability of the reported trends.\n\nThe paper does not include prior literature on diversity or exploration in RL, which would be relevant given the focus on entropy restoration and SFT alternation. Existing works in exploration-based RL, policy diversity, or ensemble-based approaches could provide valuable context, but they are not mentioned.\n\nGiven the large-scale infrastructure described (thousands of virtual desktops and 9B-parameter models), the paper lacks concrete information on hardware setup, training cost, and total compute used. For a work that emphasizes scalability, this omission is significant. Details such as training duration, GPU/CPU utilization, and cluster size should be clearly reported to assess feasibility and reproducibility."}, "questions": {"value": "Could you clarify what you consider the core algorithmic innovation of ComputerRL beyond the engineering scale-up?\n\nYou mention that Entropulse increases exploration and behavioral diversity — could you provide quantitative evidence (e.g., entropy statistics, trajectory variance, or action coverage) supporting this claim?\n\nCould you provide details on the hardware setup used — such as GPU type/count, CPU cluster size, memory, and network bandwidth?\n\nHow does your approach compare to prior work addressing entropy collapse or exploration in RL (e.g., maximum entropy RL, population-based methods)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uisD79DmCw", "forum": "oEVfNf0w4B", "replyto": "oEVfNf0w4B", "signatures": ["ICLR.cc/2026/Conference/Submission5464/Reviewer_fU96"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5464/Reviewer_fU96"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746360553, "cdate": 1761746360553, "tmdate": 1762918078160, "mdate": 1762918078160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **ComputerRL**, a framework for training computer use agents capable of performing desktop operations through a unified API-GUI interaction paradigm. The framework integrates:\n1. a distributed RL infrastructure that supports training across thousands of virtual desktop instances;\n2. aan LLM-driven module that automatically derives application programming interfaces (APIs) to extend and complement traditional GUI-based controls;\n3. Entropulse, a hybrid training regime that periodically alternates between RL and SFT phases to counteract entropy collapse and maintain exploration over extended training trajectories.\n\nEvaluations on OSWorld and OfficeWorld benchmarks show state-of-the-art results (48.9% success rate) using open-weight models (GLM-4-9B-0414, GLM-4.1V-9B-Thinking), outperforming both proprietary and open baselines such as OpenAI CUA, Claude 4.0, and UI-TARS."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Strong systems and engineering contribution:** The distributed RL infrastructure is technically impressive, enabling large-scale online RL across thousands of virtualized desktop environments. Such scale is rare in open research and represents a substantial engineering achievement.\n\n- **Practical API-GUI paradigm:**  The unified action space combining GUI operations with automatically constructed APIs addresses a key bottleneck in desktop automation. The LLM-driven API construction pipeline is pragmatic and lowers the barrier for generalization.\n\n- **Entropulse training strategy:** Alternating RL and SFT phases effectively combats entropy collapse and stabilizes long-horizon training. While conceptually simple, it appears empirically effective and easy to adopt in practice.\n\n- **Empirical performance:** The results on both OSWorld and OfficeWorld are strong and consistent. The proposed approach achieves superior performance and sample efficiency (fewer steps per task) compared to all evaluated baselines. Ablation studies indicate the importance of multi-stage training and the API-GUI design.\n\n- **Writing and organization:** The paper is very well written, clearly structured, and visually well presented."}, "weaknesses": {"value": "- **Unsubstantiated claims about diversity and exploration:** The paper claims that alternating SFT with RL increases exploration and diversity, yet no quantitative evidence is provided. Metrics such as action entropy, trajectory variance, or coverage are not analyzed. The only evidence is a qualitative entropy curve, which is insufficient.\n\n- **Incomplete empirical rigor and reproducibility:** All training curves appear to represent single runs without confidence intervals or variance estimates. The number of seeds, randomization strategy, or statistical robustness is not discussed. Given the scale (9B-parameter models, thousands of desktops), compute and reproducibility details are critically missing — no information on hardware setup, training duration, or cluster configuration. For a paper emphasizing scalability, this omission is significant.\n\n- **Limited methodological novelty:** The paper’s main innovations are engineering-oriented. The API-GUI paradigm is conceptually straightforward since it. The novelty lies in the automation pipeline, not the interaction paradigm itself. Similarly, Entropulse is a *training schedule* rather than a new RL algorithm; no theoretical or comparative justification is provided beyond empirical observations.\n\n- **Missing broader impact and ethical discussion:** Given that this system trains autonomous agents capable of full computer control, a discussion of potential misuse, safety mechanisms, or privacy implications is notably absent."}, "questions": {"value": "- Please specify the number of seeds, compute hardware, and total training cost (GPU hours, cluster size). How reproducible are the reported results on smaller scales?\n- What are the hardware requirements and cost implications for scaling your distributed RL infrastructure to the reported scale? What performance trade-offs or bottlenecks did you observe in practice?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "I just want to pinpoint that there is a human annotation protocol that is carefully explained by authors in appendix E."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6axCux8Tto", "forum": "oEVfNf0w4B", "replyto": "oEVfNf0w4B", "signatures": ["ICLR.cc/2026/Conference/Submission5464/Reviewer_oLgc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5464/Reviewer_oLgc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929627216, "cdate": 1761929627216, "tmdate": 1762918077813, "mdate": 1762918077813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents ComputerRL, a scalable framework for training autonomous computer-use agents via end-to-end reinforcement learning. ComputerRL introduces an API-GUI paradigm that unifies APIs and GUI actions, enabling higher efficiency and generalization on computer-based tasks. To support large-scale training, the authors develop a distributed RL infrastructure orchestrating thousands of virtual desktops. They also introduce Entropulse, a hybrid training strategy that alternates between RL and supervised fine-tuning, in order to prevent entropy collapse. Applied to GLM-based models, ComputerRL achieves 48.9% success on OSWorld, surpassing prior state-of-the-art agents such as OpenAI CUA o3, Claude 4.0, and gemini 2.5 pro, while demonstrating superior efficiency and stability. This work establishes a robust foundation for scaling RL-based desktop agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is overall well written.\n- Novel API-GUI paradigm for more generality in computer-based tasks.\n- New Entropulse training strategy that mitigates entropy collapse by alternating between supervised learning and RL.\n- Scalable and asynchronous RL training pipeline\n- Strong empirical results"}, "weaknesses": {"value": "Overall, the contribution lies more in engineering execution than in theoretical advancement.\n- Limited algorithmic novelty: primarily builds upon GPRO, and alternating between SFT and RL is similar to exploration-refresh or replay strategies.\n- The paper does not fully disentangle how much gain comes from ComputerRL’s methods compared to having strong pre-trained models.\n- Limited experimental diversity: they mostly evaluate on OSWorld and OfficeWorld, with no long-horizon or multi-user adaptation or interactions.\n- Limited accessibility to reproduction: no analyses on FLOPs or cost of the experiments, and a complex engineering setup."}, "questions": {"value": "1. Could Entropulse be generalized to other settings, such as text-based reasoning or code synthesis tasks?\n2. Why do you think Entropulse's performance plateaued below 50%? What would it take to scale it up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KF1L15njow", "forum": "oEVfNf0w4B", "replyto": "oEVfNf0w4B", "signatures": ["ICLR.cc/2026/Conference/Submission5464/Reviewer_6WJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5464/Reviewer_6WJM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951990834, "cdate": 1761951990834, "tmdate": 1762918077222, "mdate": 1762918077222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their careful review and highly constructive feedback. We greatly appreciate the recognition of our core contributions: the development of a large-scale distributed RL infrastructure capable of operating across thousands of desktop environments; the unified API–GUI interaction paradigm enabling efficient and stable computer control; and the Entropulse training strategy, which alleviates entropy collapse during extended RL training.\n\nThe main concerns raised by reviewers pertain to clarifying the work’s contributions, providing a more comprehensive description of the infrastructure, and including more detailed performance metrics alongside repeated experiments to verify the stability of the proposed improvements. In our response, we have elaborated on the contributions in greater depth and provided, in Appendix D of the revised manuscript, complete hardware specifications, as well as detailed accounts of training cost and time. Furthermore, Appendices E and F now present additional quantitative metrics, and our primary experiments have been expanded to include multiple repetitions with different base models, with averages and confidence intervals reported to substantiate stability claims.\n\nIn addition, we have incorporated several new experiments: generalization studies of Entropulse in domains beyond the primary task; multi-turn user interaction scenarios; and computer control tasks under long-horizon conditions. These efforts further demonstrate the effectiveness and applicability of our approach. We have also enriched the discussion of RL entropy-related work in the manuscript to situate our work within existing research more precisely."}}, "id": "BTOpKvPupF", "forum": "oEVfNf0w4B", "replyto": "oEVfNf0w4B", "signatures": ["ICLR.cc/2026/Conference/Submission5464/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5464/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission5464/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763695202362, "cdate": 1763695202362, "tmdate": 1763695202362, "mdate": 1763695202362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}