{"id": "ZfdnZhOP0k", "number": 13210, "cdate": 1758215107271, "mdate": 1759897456348, "content": {"title": "Hubble: a Model Suite to Advance the Study of LLM Memorization", "abstract": "We present Hubble, a suite of open-source large language models (LLMs) for the scientific study of LLM memorization. Hubble models come as minimal pairs: standard models are pretrained on a large English corpus, and perturbed models are trained in the same way but with controlled insertion of text (e.g., book passages, biographies, and test sets) designed to emulate key memorization risks. Our core release includes 8 models---standard and perturbed, with 1B or 8B parameters, trained on 100B or 500B tokens. Hubble's core experiment establishes that memorization risks are determined by the frequency of sensitive data relative to the training corpus size (i.e., a password appearing once in a smaller corpus is memorized better than the same password in a larger corpus).  Our release includes 6 more models with perturbations inserted at different pretraining phases; we observe perturbations without continued exposure can be forgotten. These findings suggest two best practices: to dilute sensitive data by increasing the training corpus size, and to order them to appear earlier in training. Beyond these general findings, Hubble enables a broad range of memorization research. We show that the randomized perturbations in Hubble make it an ideal testbed for membership inference and machine unlearning methods. We invite the community to explore, benchmark, and build upon our work.", "tldr": "Hubble is a suite of paired LLMs (largest 8B), where the perturbed models are trained in the same way as standard models but with text (e.g. book passages, biographies, and test sets) inserted and designed to emulate key memorization risks.", "keywords": ["memorization", "copyright", "privacy", "test set contamination", "membership inference", "unlearning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1a0aad2404a0d2e57bb6fd477538e180df13ea0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a set of open-source large language models (LLMs) called Hubble, which is designed for the scientific study of LLM memorization. The Hubble models spans 1 billion to 8 billion parameters and are trained on corpora of 100 billion to 500 billion tokens. Importantly, the models come in pairs: standard models trained on a large English corpus and perturbed models that include controlled insertions of sensitive text (like book passages, biographies, and test sets) to simulate memorization risks. During the training process, the authors conduct experiments to understand how the insertion of sensitive data affects memorization. Their analysis reveals that (1) diluting sensitive data by increasing the training corpus size can reduce memorization risks, (2) inserting sensitive data earlier in the training process can help mitigate these risks, (3) larger models memorize with lower duplications and (4) perturbations from different domains minimally interfere with each other. As for the use cases, the authors demonstrate that the Hubble models can be used to evaluate membership inference and machine unlearning methods, providing a controlled environment for further research in these areas."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is a fully open-source contribution to the field of LLM memorization, providing a suite of models that can be used for later research.\n- The training methodology is faithfully described, and the analysis of the results is thorough and valid, without overclaiming as far as I can tell.\n- The amount of experiments and analysis is impressive."}, "weaknesses": {"value": "I am not fully convinced that the published models will add much value to the research community. In general, when people pick a model, they tend to pick commercial ones like LLaMA or Qwen, therefore, it is likely that the Hubble models will be used in special cases as discussed in Section 5. I am not an expert in MIA, so I cannot judge the value of the models in that context. However, I am not sure if what was discussed in Section 5.2 is a desirable benchmark for unlearning. The Huddle benchmark randomly separates the 256-duplicated set into a Unlearn set and a Keep set, meaning that these two sets follows the same distribution. As a result, we see in Figure 3 that unlearning methods fail to reduce the memorization of the Unlearnset without hurting the performance on the Keep set. However, in real unlearning tasks, the forget set and the retain set are usually from different distributions, and unlearning algorithms are usually *expected* to generalize the forget set and remove related but not identical information. From this perspective, the Hubble benchmark might be too strict compared to many unlearning tasks.\n\nBut overall, I think this is a solid paper with a good amount of experiments and analysis, and I recommend acceptance."}, "questions": {"value": "Can you share your comments on the weaknesses above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ClXpw6iRWo", "forum": "ZfdnZhOP0k", "replyto": "ZfdnZhOP0k", "signatures": ["ICLR.cc/2026/Conference/Submission13210/Reviewer_vHGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13210/Reviewer_vHGJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760660948554, "cdate": 1760660948554, "tmdate": 1762923900969, "mdate": 1762923900969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's main contributions are as follows:\n1. Hubble, a suite of open-source pretrained LLMs for memorization research\n2. A memorization study using those models\n3. Benchmarks for membership inference and unlearning using Hubble models\n\n**Hubble model suite**: The core Hubble suite consists of 8 models that span the combinatorial space of 1B vs. 8B parameters, 100B vs. 500B pretraining tokens, and with vs. without added \"perturbations\". Perturbations are documents that are injected into the base corpus (DataComp-LM); they are designed to allow studies of copyright, privacy, and benchmark contamination. Memorization can be studied by comparing pairs of models with and without added perturbations. The Hubble suite contains further \"ablation models\", measuring effects of data ordering, interference between data domains, paraphrasing, and architecture choices. All models use a modified Llama 3.X architecture and training is fully open-source (with known datasets and ordering).\n\n**Memorization study**: The paper performs a broad memorization study on the Hubble models. Most notably, the study confirms that memorization increases with data duplication, that larger models memorize more, and that data appearing earlier in training is memorized less. Furthermore, training on a few samples of a benchmark's test set does not necessarily improve performance on that benchmark. The authors also verify that simultaneously including multiple data domains at once does not lead to interference effects between domains.\n\n**Membership inference and unlearning benchmarks**: Lastly, the paper proposes to use the Hubble models as a benchmark for membership inference and unlearning. The authors perform a small case-study for both types of benchmarks by comparing a few representative methods. They find that none of the evaluated methods manage to unlearn target knowledge and simultaneously preserve capabilities on non-target data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**Highly useful model suite**: The Hubble model suite is highly useful for memorization research, and hence serves as a more contemporary test bed compared to, say, Pythia. For one, the 8 core models allow studying the causal effects of adding certain types of data in a broad range of domains. The authors choose domains and datasets well, and they demonstrate that the models are useful to measure things beyond pure verbatim memorization.\n\nThe model ablations (\"Runs\" on p. 4) are also noteworthy. For one, they allow the authors to verify their setup (e.g., that there is no interference between domains; Figure 20). In addition, due to their design, I can see the model ablations being useful for future research (even beyond memorization).\n\n**Broad memorization study and benchmarks**: The authors not only propose a useful model suite, but also perform a plethora of studies on those models. This includes confirmation of already established phenomena (e.g., data is memorized less if it is seen earlier in training), but also new insights (e.g., test set contamination does not necessarily improve benchmark scores, or that common unlearning methods fall short of their goal). Due to the breadth of the memorization study, and the additional benchmarks, I think those contributions are already significant in isolation.\n\n**Polished and clear presentation**: Overall, the paper seems polished; it is mostly well-organized and easy to understand. The authors provide a lot of detail and transparency on their training and evaluation procedures. In the memorization study, findings that confirm existing observations/beliefs are contextualized with the relevant related work. I also like Figure 4; it intuitively illustrates how the base corpus is modified to include perturbations.\n\n**Insights and tools for model training**: The insights from this paper could be useful beyond memorization research. The authors mention that their research resulted in additional tools that could benefit future research (Footnote 2 on page 4). Additionally, assuming the full training pipeline code is documented well, it might even help other researchers train similar suites of models."}, "weaknesses": {"value": "While this is an overall high-quality paper, there is one potential major issue related to decontamination between the base training corpus and perturbations. I am happy to raise my score if those issues are addressed.\n\n**Potential contamination between base corpus and perturbations**: The decontamination step of Hubble's data pipeline focuses on high precision (i.e., only discarding samples that clearly overlap the base corpus and perturbations). However, false negatives (i.e., contamination that is not detected) are much more harmful for memorization research, as they can taint the results for the use cases in this paper (studying memorization, MIA/unlearning benchmarking).\nFor one, the authors verify matches manually; however, this can only detect false positives and not false negatives. Moreover, decontamination for perturbations (of non-trivial length) only uses exact matches of at least 20 tokens. Hence, it is possible that non-trivial overlaps between perturbations and base corpus exist (e.g., with slightly different formatting).\nI thus argue that, to ensure results of memorization studies are valid, this requires further discussion and a potential empirical study of shorter overlaps between base corpus and perturbation sequences.\n\n**Membership inference benchmark is incomplete**: The membership inference case-study (Section 5.1 and Appendix F) exclusively uses the ROC AUC score as a metric. This is well-known to be misleading and obfuscating important details [(Carlini et al., 2021)](https://arxiv.org/abs/2112.03570). Instead, I recommend reporting the TPR at a low fixed FPR (e.g., 1%), and providing full ROC plots.\nAdditionally, I could not find the size of the member and non-member sets mentioned. However, this information is crucial to assess the resolution of the reported metrics (small set sizes might yield high variance in reported scores).\n\n**Relatively short pretraining compared to \"real-world\" models**: Hubble is only trained on 0.5T tokens, compared to 4T and 9-15T tokens for OLMo and Llama 3.X models of similar sizes. The authors are transparent about this. Yet, the insertion timing results have to be viewed relative, and Hubble might not allow studying memorization due to emergent behaviors from very long pretraining. On the flip side, while Hubble models are obviously not SotA on standard benchmarks, they seem to at least often perform on-par with similarly-sized Llama 3.1/3.2 (but not OLMo 2) models. I understand that this limitation is unavoidable due to the large cost of pretraining.\n\n**Minor feedback on figures**:\n1. Most plots only use a single hue for all lines and distinguish them by brightness (see, e.g., Figures 1 and 2). I think the plots would be easier to read and compare if they used very different colors, especially in plots with many lines.\n2. I found it hard to process Figure 1, particularly since it contains many small plots for many settings. At that point in the paper, I would find it easier to have a high-level figure that highlights an aggregate or a subset of results, and defer the full Figure 1 to the appendix.\n3. In contrast, Section 4.1 provides a lot of interesting insights, but defers most plots to the appendix. The paper could be easier to read if the main matter showed (a subset of) the results discussed there (to avoid jumping between sections) and deferred detailed plots (such as Figure 1) to the appendix."}, "questions": {"value": "1. How are duplication counts sampled? Is it completely uniformly at random, or are the numbers balanced (beyond using 256 duplicates less often)? From Footnote 3 on p. 4, I believe it to be the former, but I could not find an exact statement.\n2. What exactly are the held-out perturbations (e.g., perturbations with duplicate count 0 in Section 5.1)? Will those be published with the results of this paper in an easily-accessible format? I believe that this is an crucial point for benchmarking membership inference; other researchers might theoretically be able to generate such a held-out set themselves, but this requires non-trivial engineering effort that the authors of Hubble already did.\n3. Similarly, what are the sizes of the member and non-member sets in the membership inference benchmarks?\n4. How difficult would it be to extend Hubble to post-training (e.g., instruction tuning)? This might be a highly useful extension, given that there are even fewer benchmarks to measure memorization in post-training.\n5. From my understanding, different repetitions and number of training tokens change the relative _frequency_ of perturbations. This terminology is mentioned consistently throughout the paper, except for the beginning of Section 4. That part of the paper also mentions the _spacing_ between perturbations. Is the spacing something that is effectively controlled? Or does the beginning of Section 4 just assume that spacing and frequency are negatively correlated? In the latter case, I would either only mention frequency or empirically validate that lower frequency indeed yields uniformly larger spacing between perturbations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f2MLurHDwY", "forum": "ZfdnZhOP0k", "replyto": "ZfdnZhOP0k", "signatures": ["ICLR.cc/2026/Conference/Submission13210/Reviewer_XuhG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13210/Reviewer_XuhG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760964706986, "cdate": 1760964706986, "tmdate": 1762923900628, "mdate": 1762923900628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a suite of (not yet) open sourced models specifically for study of memorization in LLMs. HUBBLE consists of standard and perturbed model pairs trained on the same data with the latter containing controlled insertions of synthetic or real-world sensitive data.\n\nOn standard NLP benchmarks, HUBBLE models perform comparably to other open-weight models (e.g., Pythia) at similar scale, confirming training stability. \n\nThe authors train 8 primary models (1B and 8B parameters, 100B and 500B tokens) and 6 auxiliary models with variations in insertion timing, frequency, and architecture. Experiments on these trained models show two primary trends:\n\nDilution effect: memorization risk decreases when sensitive data are diluted within larger corpora.\nOrdering effect: placing sensitive data early in pretraining reduces long-term memorization.\nModel size effect: Larger models (8B) memorize faster and at lower duplication counts, aligning with previous scaling trends.\nMinimal interference: Perturbations from distinct domains (privacy, copyright, contamination) do not interfere significantly.\n\nThis work also opens up the possibility for controlled benchmarks for MIA attacks and unlearning"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this work are listed as follows:\n\n- This work would help form any following work overcome one of the primary drawbacks in this field, where experiments had to be either painfully small scale or had to rely on somewhat unknown data patterns of models trained without a focus on memorization studies\n- The predictable and standard nature of the perturbations added to the training data would help any further study conduct clear A/B tests on this study\n- I really like the detailed nature of the experiments especially the domain specific results"}, "weaknesses": {"value": "- This is an (computationally)expensive study and ideally more patterns could/should have been embedded into the training data to make the most out of it. One interesting direction might've been understanding the relation between the distance between paraphrased samples and the degree to which they're memorized\n\n- Test set insertions focus on benchmarks like MMLU and HellaSwag, whereas contamination in real corpora can be indirect or paraphrastic.\n\n- The work isolates memorization metrics from general model utility. While HUBBLE provides general evaluation benchmarks, it does not deeply analyze how memorization mitigation (via dilution or ordering) affects downstream performance or factual recall."}, "questions": {"value": "- Please detail if there are a set of directions which could be explored using these models apart from the standard memorization and unlearning tasks for which this study is directly intended for"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uzwWS0uQTC", "forum": "ZfdnZhOP0k", "replyto": "ZfdnZhOP0k", "signatures": ["ICLR.cc/2026/Conference/Submission13210/Reviewer_HMNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13210/Reviewer_HMNX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882888687, "cdate": 1761882888687, "tmdate": 1762923900236, "mdate": 1762923900236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HUBBLE, a comprehensive, open-sourced suite of LLMs and data to explore the memorization of pretrained LLMs. More specifically, it consists of 8 pairs of standard models and perturbed models with 1B or 8B and pretrained on 100B or 500B tokens. The authors conducted a series of experiments on the HUBBLE, which show conclusions consistent with the existing studies on LLM memorization and also provide new insights on how to mitigate the memorization of sensitive data. Moreover, the experimental results also show the effectiveness of HUBBLE to build benchmarks for use cases of the membership inference attacks and machine unlearning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the paper are listed as follows.\n1. HUBBLE will be fully open-sourced upon publication, which is important for the researchers to reproduce its results and leverage the suite for their own study in LLM memorization.\n2. The design of HUBBLE is reasonable and useful. The pair of standard models and perturbed models enables a fair and clean comparison when studying the mechanism of LLM memorization.\n3. The open-source data and checkpoints also make the quantitative analysis of LLM memorization feasible. The unavailability of checkpoints or training data is a big bottleneck for the study of LLM memorization.\n4. HUBBLE provides an insightful categorization of the potential risk caused by LLM memorization, which reflects the deep understanding of this research area and the practical safety demands in real-world applications.\n5. The evaluation is comprehensive and solid. The numerical results, observations and conclusions drawn from HUBBLE are reasonable, convincing, and also consistent with the existing works in LLM memorization."}, "weaknesses": {"value": "The major concern of HUBBLE is that it only covers the LLMs with 1B and 8B. Therefore, some observations from HUBBLE may not be extended to large LLMs, which usually have a higher memorization rate and are more widely adopted by commercial applications. However, for the academic research of LLM memorization, the two model sizes are sufficient."}, "questions": {"value": "The questions are listed as follows.\n1. In Figure 1, do the authors have any explanation for why dilution does not work well on the data samples duplicated 256 times?\n2. For the experiments on the impact of the ordering of sensitive data in the pretraining, is the total number of sensitive data and their duplication times kept the same for all settings? For the setting with perturbed data across all four quarters, a specific sensitive data may be used in all four stages. However, for the setting with perturbed data only in the first quarter, will the same sensitive data be repeatedly used four times for fair comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yzpQe3orcM", "forum": "ZfdnZhOP0k", "replyto": "ZfdnZhOP0k", "signatures": ["ICLR.cc/2026/Conference/Submission13210/Reviewer_m7QY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13210/Reviewer_m7QY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969872598, "cdate": 1761969872598, "tmdate": 1762923899812, "mdate": 1762923899812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}