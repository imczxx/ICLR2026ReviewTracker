{"id": "8jSoX4piAf", "number": 17386, "cdate": 1758275304334, "mdate": 1759897178425, "content": {"title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training", "abstract": "Full-duplex dialog models aim to listen and speak simultaneously, delivering rapid responses to dynamic user input. Among different solutions to full-duplexity, a native solution merges multiple channels in each time step, achieving the lowest latency. However, prevailing designs break down the textual monologue sentences for word-level alignment with audio streams, which degrades language modeling abilities. To help address this issue, we introduce “natural monologues”, which are composed by continuous sentences and “waiting” intervals, mimicking humanoid cognitive behavior in dialogs. We find a proper training paradigm to be critical for semantically aligning natural monologues with audio. To this end, we develop a “dual” training paradigm that alternates the position of the monologues, either leading or trailing the audio, across different training stages. A combination of our natural monologue and dual training strategy is applied in developing FLM-Audio, our 7B spoken dialog chatbot with native full-duplexity. As confirmed by experimental results, FLM-Audio achieves superior response qualities and chatting experiences while requiring significantly less training data.", "tldr": "We propose natural monologue and dual training paradigm to improve native full-duplex spoken dialog models", "keywords": ["Audio Language Model", "Full-duplexity", "Spoken Dialog"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c248a2ee145853159c8fd7f7490a3b1f61870650.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FLM-Audio, a 7B full-duplex dialogue model that can listen and speak simultaneously. By using “natural monologues” (continuous sentences with pauses) and a “dual training” strategy to align text and audio, it achieves more natural, low-latency conversations with higher response quality using less training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The presentation of the main idea is generally clear and well motivated."}, "weaknesses": {"value": "(1) Limited core contribution. The main contribution of this paper is rather narrow, focusing primarily on modifying the padding strategy in Moshi. Instead of word-level alignment (i.e., padding to correct bit-rate mismatches between audio and text tokens), the paper proposes preserving sentence integrity by padding at the sentence level. While this is an intuitive enhancement to the Moshi architecture, the contribution remains confined to this single framework. Moreover, with the emergence of other full-duplex models such as dGSLM [1], NTPP [2], and LLaMA-Omni [3], it appears that the proposed approach may not generalize to these architectures, which limits the broader impact of the work.\n\n(2) Lack of systematic empirical evaluation. Several key claims in the paper lack sufficient empirical evidence. For instance, the repeated claim that “FLM-Audio achieves equivalent response latency” is not substantiated by any quantitative benchmark or experiment. Furthermore, the paper does not describe how fairness in comparison is ensured. For example, does Moshi also undergo post-training and supervised fine-tuning under the same setup? Without such clarification, it is unclear whether the reported gains stem from architectural changes or differences in training. Additionally, while Table 1 lists many full-duplex models for comparison, most are not actually included in the experiments, leading to inconsistency between the discussion and the evaluation.\n\n(3) Figure and table presentation. The figures and tables need refinement for clarity. Figure 1 aims to illustrate alignment strategy differences but does not clearly convey the intended comparison. It should be visually polished for better interpretability. Similarly, Table 3 lacks explicit labeling—readers cannot tell which metrics are shown, what the numerical values represent, or whether lower or higher scores are better. Such essential details should not be left only in the main text.\n\nIn conclusion, this paper, in its current form, is not yet ready for publication. The contributions need to be articulated more clearly and supported by systematic, well-documented experiments. Strengthening the empirical analysis and improving the clarity of presentation would greatly enhance the paper’s completeness and credibility. \n\n[1] Generative Spoken Dialogue Language Modeling [TACL 2023]\n[2] NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction [ICML 2025]\n[3] LLaMA-Omni: Seamless Speech Interaction with Large Language Models [ICLR 2025]"}, "questions": {"value": "(1) What is the latency of the proposed approach compared to the Moshi baseline?\n\n(2) Does the Moshi baseline follow the same experimental setup as the proposed approach — including identical training data, fine-tuning, and post-training procedures? This reviewer could not find any related descriptions in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EEVvcaa0Av", "forum": "8jSoX4piAf", "replyto": "8jSoX4piAf", "signatures": ["ICLR.cc/2026/Conference/Submission17386/Reviewer_cLE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17386/Reviewer_cLE2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719432907, "cdate": 1761719432907, "tmdate": 1762927293253, "mdate": 1762927293253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a full-duplex chatbot built on Moshi's codec and a Moshi-like parallel multi-channel framework. It introduces \"natural monologue,\" a sentence-level alignment method to replace word-level alignment in Moshi, reducing degradation in language modeling and lowering data preprocessing costs. Additionally, it proposes a \"dual\" training paradigm that mixes TTS-style and ASR-style training data. Experiments demonstrate comparable human evaluation results with Qwen-2.5-Omni for full-duplex chatting tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Full-duplex speech interaction is an important research topic.\n2. The paper replaces Moshi’s word-level alignment with natural monologue (sentence-level alignment), significantly reducing data preprocessing costs.\n3. Subjective evaluation is conducted for the full-duplex interaction task."}, "weaknesses": {"value": "1. The novelty compared to Moshi is incremental, as it uses the same audio codec, with key changes being the shift from word-level padding to sentence-level padding and the mixed training data (TTS-style and ASR-style).\n2. Table 3 lacks comparisons with common baselines such as Qwen2.5-Omni and Kimi-Audio.\n3. Experimental design is flawed. For instance, DeepSeek-V3 refines responses for instruct-following data and is also used for LLM-score evaluation in full-duplex chatting, creating an unfair comparison with other baselines.\n4. The paper does not report objective full-duplex metrics, such as turn-taking statistics in generated dialogues, including the number of turn-taking events and accumulated durations per minute compared to ground truth.\n5. The natural monologue design is highlighted as the main innovation, but the paper lacks controlled experiments isolating its impact compared to Moshi-like word-level alignment."}, "questions": {"value": "Please address the issues described in the Weaknesses section. Resolving these concerns could improve the paper’s evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j1kNBJF1lO", "forum": "8jSoX4piAf", "replyto": "8jSoX4piAf", "signatures": ["ICLR.cc/2026/Conference/Submission17386/Reviewer_2jGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17386/Reviewer_2jGm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933855455, "cdate": 1761933855455, "tmdate": 1762927292553, "mdate": 1762927292553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FLM-Audio, a 7B bilingual (English–Chinese) full-duplex spoken dialogue model built on a “native full-duplex” architecture that merges listening and speaking channels at each timestep. The authors identify a key limitation in prior work such as Moshi—word-level alignment between text and audio tokens—and propose “natural monologues,” continuous sentence-level text streams interleaved with <wait> tokens to mimic human cognition during speech. They further introduce a dual training paradigm, alternating between ASR-style and TTS-style supervision to better align asynchronous text and audio modalities. Experiments on ASR, TTS, and full-duplex dialog benchmarks show improvements over Moshi and comparable quality to Qwen2.5-Omni with significantly less training data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative reformulation of alignment in full-duplex systems via natural monologues.\n2. Dual-format training effectively integrates both speech recognition and synthesis modes.\n3. Extensive experiments with solid ablations demonstrate the importance of each training stage.\n4. Competitive performance despite limited data (1M hours vs. 8M+ for Moshi)."}, "weaknesses": {"value": "1. Lack of deep analysis: no evidence is provided to explain why the method works beyond intuitive analogies to human cognition.\n2. Weak empirical attribution: ablation results are limited and fail to isolate the effect of natural monologues versus other hyperparameters."}, "questions": {"value": "1. Could you provide quantitative evidence that “natural monologues” improve language modeling capacity (e.g., perplexity, coherence scores)?\n2. What are the exact latency, throughput, and memory metrics compared to TDM-based systems in real deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7kpdm3a4K9", "forum": "8jSoX4piAf", "replyto": "8jSoX4piAf", "signatures": ["ICLR.cc/2026/Conference/Submission17386/Reviewer_k7bv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17386/Reviewer_k7bv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162734354, "cdate": 1762162734354, "tmdate": 1762927292105, "mdate": 1762927292105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes the design and implementation of a transformer-based AI model named FLM-Audio for full-duplex spoken conversation in English and Chinese. Full-duplex means that the user and the system can interrupt each other and speak over each other. The system begins with a textual LLM and does additional training to enable full-duplex speech.\n\nThe input is a temporal stream of tokens. In time-division multiplexing (TDM), output audio, input audio, and output text (called monologue) are interleaved sequentially. In contrast, in native full duplex, each token is the concatenation of one output audio token, one input audio token, and one output text token (see Figure 1). The first main innovation of FLM-Audio is visible in line 173 (Figure 2): the text tokens of one utterance (a sentence or similar) are contiguous. Audio requires many more tokens than text, so after each text utterance. \"wait\" tokens are concatenated with the audio tokens. In contrast, in Moshi, text tokens are individually aligned with audio, so \"pad\" tokens are inserted between text tokens of the same utterance.\n\nThe second main innovation of FLM-Audio, described in Section 3, is in the specifics of the post-training that converts the original text-only LLM into a spoken dialog model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The innovations of this paper are sensible. They have been fully implemented, and the experimental results are good. The innovations are building blocks that other teams will be able to mix and match in the future when creating even better spoken dialog systems."}, "weaknesses": {"value": "The innovations are not highly surprising, and they are not the result of any major new insight or empirical discovery. \n\nThere is hardly any experimental comparison to the closed-source state of the art.\n\nThe terminology \"natural monologues\" makes an implicit claim that is too strong. \"Contiguous text utterances\" would be more precise. Also, \"human-like\" instead of \"humanoid\" would avoid the robot connotations of \"humanoid,\" which are not appropriate here.\n\nA note on LLM usage: It looks very likely that the authors used AI to make the text conform to the conventions of standard English. Line 498 says \"We hope these efforts facilitate the community in implementing our methods and achieve results consistent with our conclusions.\" In this sentence, the non-standard direct object of \"facilitate\" and the unintended literal meaning of \"We hope these efforts achieve results consistent with our conclusions\" reveal what the writing might have been like before improvement by AI.\n\nThe singular-plural mismatch in \"Natural Monologues Improves\" in the title is other evidence of the lower quality of the English before AI improvement."}, "questions": {"value": "Quantify the resources used for training and inference, and the latency achieved.\n\nLine 45: What infrastructure is assumed for the numbers 2 and 45 seconds? What is the cost of this in dollars or other units? What is the % improvement achieved by FLM-Audio?\n\nPlease say more about the state of the art in the entire field of spoken dialog systems:\n- Are there any alternatives to transformers, and to post-training a text-only model?\n- Are current systems better or worse than humans? In what ways, and why?\n- Why should we believe that the innovations here are still valuable when building on top of a text-only model that is much larger and better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q5TPt3NO8y", "forum": "8jSoX4piAf", "replyto": "8jSoX4piAf", "signatures": ["ICLR.cc/2026/Conference/Submission17386/Reviewer_apT6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17386/Reviewer_apT6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762519938542, "cdate": 1762519938542, "tmdate": 1762927291427, "mdate": 1762927291427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}