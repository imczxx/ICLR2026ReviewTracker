{"id": "mHA7Kr1vaO", "number": 4195, "cdate": 1757629675953, "mdate": 1759898048301, "content": {"title": "Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization", "abstract": "Molecular foundation models are rapidly advancing scientific discovery, but their unreliability on out-of-distribution (OOD) samples severely limits their application in high-stakes domains such as drug discovery and protein design. A critical failure mode is chemical hallucination, where models make high-confidence yet entirely incorrect predictions for unknown molecules. To address this challenge, we introduce Molecular Preference Aligned Instance Ranking (Mole-PAIR), a simple, plug-and-play module that can be flexibly integrated with existing foundation models to improve their reliability on OOD data through cost-effective post-training. Specifically, our method formulates the OOD detection problem as a preference optimization over the estimated OOD affinity between in-distribution (ID) and OOD samples, achieving this goal through a pairwise learning objective. We show that this objective essentially optimizes the AUROC, which measures how consistently ID and OOD samples are ranked by the model. Extensive experiments across five real-world molecular datasets demonstrate that our approach significantly improves the OOD detection capabilities of existing molecular foundation models, achieving up to $\\mathbf{45.8%}$, $\\mathbf{43.9%}$, and $\\mathbf{24.3%}$ improvements in AUROC under distribution shifts of size, scaffold, and assay, respectively. Our code is available at: https://anonymous.4open.science/status/Mole-PAIR-61B5.", "tldr": "", "keywords": ["Foundation model", "Preference Optimization", "Out-of-distribution Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bed7f16662e2a4310150151a9cb1a6caa89d79f.pdf", "supplementary_material": "/attachment/5c92fc4a113853b29bffc1243b0f5bf6795efedd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Mole-PAIR, a plug-and-play post-training framework that enhances OOD detection for molecular foundation models. The key idea is to formulate OOD detection as a preference ranking task, optimizing a pairwise objective that aligns directly with AUROC. The method trains only a small scoring head on top of frozen pretrained encoders and can be applied across different models and datasets. Experiments show consistent improvements under scaffold, size, and assay distribution shifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear identification of a practical and important reliability issue in molecular modeling.\n\n2. Method is architecture-agnostic, lightweight, and label-free, making it broadly usable.\n\n3. The pairwise objective aligns well with AUROC, supported by theoretical justification."}, "weaknesses": {"value": "1. Baseline coverage seems insufficient.\nWhile the paper compares against several standard post-hoc OOD detection methods (e.g., MSP, ODIN, Energy, Mahalanobis, LOF, KNN), it does not include more recent and domain-specific OOD methods for molecular or graph data. This makes it difficult to determine whether the improvements stem from the proposed optimization strategy or simply from comparing against weaker baselines.\n\n2. Lack of comparison with uncertainty-estimation models.\nDeep ensembles, Monte Carlo dropout [1], SWAG [2], or conformal prediction–based uncertainty methods are widely used in high-stakes molecular prediction settings. Since the paper frames its contribution as improving reliability and mitigating hallucination, the absence of comparisons with these strong uncertainty baselines weakens the empirical claims.\n\n[1] Representing Model Uncertainty in Deep Learning. ICML 2016.\n\n[2] A simple baseline for bayesian uncertainty in deep learning. NeurIPS 2019.\n\n3. Dependence on encoder embedding quality not fully discussed.\nSince the encoder is frozen, the proposed method implicitly assumes that ID/OOD are separable in the pretrained embedding space. If this separability is weak, the ranking head may not compensate. The paper does not investigate encoder fine-tuning or analyze how the proposed approach performs when embedding quality varies.\n\n4. Limited statistical and computational reporting.\nThe paper reports performance metrics but does not provide statistical significance across multiple seeds nor comparisons of computational cost relative to competing methods. Given the claim of cost-effectiveness, runtime and resource analysis would strengthen the empirical case."}, "questions": {"value": "1. Have the authors evaluated whether partial or full fine-tuning of the encoder affects Mole-PAIR’s performance? If the encoder representations are not ID/OOD separable, can Mole-PAIR still provide improvements?\n\n2. For GOOD-ZINC, what is the rationale behind median threshold binarization, and how sensitive are the results to threshold choice? Would a regression-aware scoring or continuous uncertainty metric result in different conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EU210q6nvZ", "forum": "mHA7Kr1vaO", "replyto": "mHA7Kr1vaO", "signatures": ["ICLR.cc/2026/Conference/Submission4195/Reviewer_usRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4195/Reviewer_usRx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877001946, "cdate": 1761877001946, "tmdate": 1762917224381, "mdate": 1762917224381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to develop a lightweight, plug-and-play method to enhance the OOD detection capability of existing molecular foundation models without retraining the pretrained model. The authors propose Mole-PAIR, a preference optimization-based framework that reframes OOD detection as a pairwise ranking problem. Extensive experiments are conducted on multiple datasets. Results show that Mole-PAIR significantly outperforms existing OOD detection baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a preference learning perspective for OOD detection in molecules, emphasizing ranking consistency between ID and OOD samples rather than per-sample confidence.\n2. Mole-PAIR is model-agnostic, lightweight, and easy to deploy as a post-training module on any pre-trained molecular foundation model.\n3. This paper demonstrates improvements across multiple datasets, distribution shifts, and backbone models."}, "weaknesses": {"value": "1. Although the method does not use OOD labels during training, it still relies on pre-defined ID/OOD splits for supervision. This may limit its applicability in fully unsupervised or real-world deployment scenarios.\n2. The performance is sensitive to hyperparameters β and λ, with no adaptive mechanism provided. This may require extensive tuning in practice.\n3. The theoretical guarantees (e.g., convergence to Bayes-optimal ranking) assume sufficient data and model capacity. There is no analysis of performance under limited data or model misspecification, which are common in practice.\n4. Are the experiments mostly conducted on binary classification tasks? I did not find evaluations of multitasking, regression, or multimodal scenarios, which are common in real-world drug discovery."}, "questions": {"value": "Refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n3tYtXx8iF", "forum": "mHA7Kr1vaO", "replyto": "mHA7Kr1vaO", "signatures": ["ICLR.cc/2026/Conference/Submission4195/Reviewer_LJLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4195/Reviewer_LJLj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985463088, "cdate": 1761985463088, "tmdate": 1762917223598, "mdate": 1762917223598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for ranking in-distribution samples ahead of out-of-distribution samples. An MLP is finetuned on a frozen encoder such as\nMiniMol and a preference learning objective is uesd to maximize the AUROC. Experiments on multiple datasets in different settings demonstrate the\nperformance of the proposed method, showing high performance gains. Some convergence guarantees are also provided for the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is model agnostic allowing it to be applied to any pretrained model of interest. The cost of training itself is low given that a\n  small MLP is trained with the foundation model frozen. This allows for an easy remedy to fixing the OOD issues in foundation models.\n\n- The theoretical guarantees provided makes the model sound.\n\n- Performance wise, the proposed model shows good empirical results over a range of datasets across multiple data shift regimes."}, "weaknesses": {"value": "- In real world scenarios, the assumption of an OOD dataset to use in the loss function may not be realistic. In most cases, one does not have access\n  to the OOD samples that one wishes to generalize on. At any rate, even if such a split exists, in practice it may be continually shifting.\n  Additionally, there are many situations where crafting such an OOD datasets may not even be feasible. Hence it is unclear as to how this method can\n  be adapted to generalize to unseen OOD samples.\n\n- The proposed loss term requires pair-wise comparisons of ID and OOD samples. Practically, this can be a huge space to explore and an analysis of\n  performance compared to how much this pair-wise space is explored can be useful in gauging how cost effective the method actually is as claimed in\n  the paper. \n\n- In the current framework, the encoder is frozen and no comparison is provided on how the proposed method compares on a simple finetuning or even\n  contrastive approach which seems a natural choices here given that the ID and OOD splits are assumed given."}, "questions": {"value": "Together with some of the weaknesses discussed,\n\n- Currently, the proposed method assumes access to ID and OOD data splits in the post training phase. How realistic is this in the drug discovery\n  pipeline? What happens when we do not have access to an OOD dat asplit? Suppose we have access only to the ID split then is there a way to use the\n  pairwise approach via some other pairwise scheme that allows for OOD generalization? And does Mole-Pair only generalize on given OOD data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vt4hm6PIZc", "forum": "mHA7Kr1vaO", "replyto": "mHA7Kr1vaO", "signatures": ["ICLR.cc/2026/Conference/Submission4195/Reviewer_o8ci"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4195/Reviewer_o8ci"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055435573, "cdate": 1762055435573, "tmdate": 1762917223033, "mdate": 1762917223033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Mole-PAIR, a post-training framework to improve the out-of-distribution (OOD) detection ability of molecular foundation models. The key idea is to reformulate OOD detection as a preference optimization problem, where a pairwise ranking objective is optimized instead of traditional pointwise regression or confidence estimation. The method trains a lightweight MLP detector on top of frozen pretrained encoders such as Uni-Mol and MiniMol. Theoretical analyses show that the proposed loss aligns with optimizing AUROC, and extensive experiments on DrugOOD and GOOD benchmarks demonstrate large improvements in OOD detection under multiple distribution shifts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and practical challenge in molecular AI, which is how to make molecular foundation models more reliable under distribution shifts. The formulation is conceptually clean, and the idea of aligning the training objective directly with AUROC is theoretically well motivated. The method is simple, efficient, and compatible with existing pretrained molecular encoders without requiring retraining. The theoretical analysis is complete and clearly written. The experiments are broad and consistent across datasets, showing that the approach can yield strong performance improvements under various types of shifts."}, "weaknesses": {"value": "The proposed framework mainly applies the pairwise ranking principle, long established in AUROC optimization and preference learning, to the molecular OOD detection setting. Theoretical analysis provides clarity but does not introduce new insights beyond known ranking formulations."}, "questions": {"value": "How sensitive is Mole-PAIR to the availability of OOD samples during post-training? Does performance degrade when OOD examples are scarce or unbalanced relative to ID data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NMN2ASU6Xx", "forum": "mHA7Kr1vaO", "replyto": "mHA7Kr1vaO", "signatures": ["ICLR.cc/2026/Conference/Submission4195/Reviewer_LnWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4195/Reviewer_LnWv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142829537, "cdate": 1762142829537, "tmdate": 1762917222337, "mdate": 1762917222337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}