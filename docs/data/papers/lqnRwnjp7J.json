{"id": "lqnRwnjp7J", "number": 21761, "cdate": 1758321398346, "mdate": 1759896904568, "content": {"title": "Benchmarking Deception Probes via Black-to-White Performance Boosts", "abstract": "AI assistants will occasionally respond deceptively to user queries. Recently, linear classifiers (called “deception probes”) have been trained to distinguish the internal activations of a language model during deceptive versus honest responses. However, it’s unclear how effective these probes are at detecting deception in practice, nor whether such probes are resistant to simple counter strategies from a deceptive assistant who wishes to evade detection. In this paper, we compare white-box monitoring (where the monitor has access to token-level probe activations) to black-box monitoring (without such access). We benchmark deception probes by the extent to which the white box monitor outperforms the black-box monitor, i.e. the black-to-white performance boost. We find weak but encouraging black-to-white performance boosts from existing deception probes.", "tldr": "We introduce a methodology for benchmarking deception probes via black-to-white performance boosts, evaluating probe-based monitoring strategies across diverse deception datasets and finding modest but informative gains alongside key limitations.", "keywords": ["probes", "deception", "honesty", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa560d46d2cfa0fbcb2c11d3d457143df5c4da31.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark for “deception probes” by comparing white-box monitoring (access to token-level probe activations) against black-box monitoring on the same tasks, defining value via the black-to-white performance boost. The key concept is that for white-box monitoring to be useful, there should be a performance boost observed from the black-to-white performance.\n\nThe authors evaluate eight monitoring strategies (numeric, follow-up, hybrid, and baselines) using two linear probes trained on LLaMA-3.3-70B-Instruct and test across five deception datasets (AI Liar, Insider Trading, Sandbagging, Backdoors, and a Deception-Dilution variant) plus Alpaca as control. Full-Averaging of probe activations performs strongly and robustly (e.g., AUROCs ~0.84–0.999), hybrid white-box methods yield modest but consistent boosts over black-box reasoning in most settings, and the Pairs probe tends to generalize better than the Role-Playing probe, though white-box methods underperform on Backdoors (which is out of the training distribution of the probes). \nOverall, results suggest current probes provide weak but encouraging added value for deception detection, while highlighting the need for localization, recalibration across domains, and stronger evasion-aware monitors."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\nThe paper is exceptionally well written and easy to follow; it was a pleasure to read. It tackles an important and timely question with a simple, effective, and empirically grounded strategy: does advanced probe-based white-box monitoring on internal activations provide tangible benefits over more accessible black-box monitoring approaches? The authors present a detailed and systematic set of experiments that address this question convincingly—the results clearly show that while white-box monitoring provides a modest “black-to-white” performance boost, the gains are smaller than often assumed.\n\nThis work stands out for its clarity in framing, careful experimental design, and transparent reporting of limitations. The benchmark is well-motivated and replicable, with clean separations between datasets, probe designs, and monitoring strategies. The proposed metrics (especially “black-to-white value”) offer a clear operational definition for probe utility, which could be broadly useful for future evaluation of model interpretability tools.\n\nThe authors also contribute several thoughtful analyses—such as the dilution experiments, probe generalization across tasks, and the introduction of “Critical Averaging”—that demonstrate nuance and awareness of the broader landscape of mechanistic interpretability. The balance of rigor and practical relevance makes this paper a valuable empirical contribution to the monitoring and safety community."}, "weaknesses": {"value": "The selection of sonnet/haiku seems strange to me. I would like to see results with llama3.1 itself, then it would be a pure black-to-white comparison. I appreciate that the authors put a limitation in their paper, but I don't understand why it was chosen in the first place. There doesn't seem to be a reason to select stronger blackbox monitors than the whitebox counterparts.\n\nIt would also be good to include other open-weight models to know if these kinds of results generalize. I am aware that activation and probing techniques do not necessarily generalize equally across models.  I suppose that this was not done because the probes used came from the other paper. Still since this is a benchmarking paper - I think an effort to reproduce the probes (and their black-to-white performance boost) on other models could have been done.\n\nI understand that the dilution strategy was just to check and see \"would this work\" and that the results seem to be \"kinda\", but it's completely unrealistic for an untrusted assistant since the user in this case would immediately just think their LLM is broken.\nI think the the authors should make it more clear in the paper that this is a non-realistic approach.\n\n\nSmall points for improvement:\n\nNit: line 45: ’hidden’  (reverse quote on left side)\n\nPut some links to the appendix in the body - for example when you describe critical averaging I had trouble understanding how this would look, but it's all in the appendix in great detail - a direct link would make navigating it easier."}, "questions": {"value": "I suppose we could wonder how much worse the blackbox performance would be if you didn't use sonnet/haiku.\nIs there a reason you chose to use stronger models?  Why didn't you just use an OSS model or llama3 itself for these tasks?\nSince we are measuring black-to-white, wouldn't it make most sense to use the whitemodel again?\nThe papers claim of \"modest improvements\" seems weakened by the fact that the blackbox models are much stronger than the whitebox model use to capture activations. Had it been the same model, maybe the improvements would have been more than modest?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fe5KSFwVD0", "forum": "lqnRwnjp7J", "replyto": "lqnRwnjp7J", "signatures": ["ICLR.cc/2026/Conference/Submission21761/Reviewer_acMw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21761/Reviewer_acMw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773491715, "cdate": 1761773491715, "tmdate": 1762941922750, "mdate": 1762941922750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to evaluate deception probes with the performance difference observed when comparing a deception monitor with access to token activations (white-box) vs without such access (black-box). Several variations of deception detection methods are evaluated on 4 benchmark datasets (plus Alpaca for control). On 3 out of 4 datasets the white-box methods outperform black-box, while on the backdoors dataset blackbox methods dominate. The paper also tests the robustness of the detection methods against a simple countermeasure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The evaluation of linear probes for deception detection is an interesting and relevant and well motivated problem.\n* Evaluation on 4 reasonable benchmark datasets, the details of which are described clearly.\n* Robustness against countermeasures is a very interesting direction.\n* Figure is plotted with 95% bootstrap confidence intervals.\n\nOverall I think the research question tackled in the paper is promising."}, "weaknesses": {"value": "* The clarity of the paper could be improved, especially the discussion of the methodology would benefit from a more formal and precise formulation. In its current state it was hard to follow and understand clearly how the monitoring methods are implemented.\n* While this is an interesting direction, I think the current evaluation is fairly limited by considering only a specific set of linear probes, and comparing to only a single model for black-box reasoning. I would have been very interested to see a more in-depth evaluation of the generalisability of linear probes, also considering more than 2 already trained probes, compared against more models for BBR, and an evaluation of the robustness of probes against countermeasures beyond the simple dilution. I think these points could significantly strengthen the paper.\n* The choice of the ensemble (line 311) seems arbitrary, why not evaluate other combinations and other weightings?\n* Several ablations could strengthen the paper, such as, variations on the prompts of the LLM judge, which aspects of WBR drive performance, other reasoning models, at some named above.\n* Regarding the backup of critical averaging (line 286), how do we know the LLM failed to identify the critical claim?\n* While the original paper was referenced, the training details of the linear probes used in the evaluation seem highly relevant and would help in understanding the results.\n* A more thorough explanation of the results and their broader implications would greatly benefit the discussion section. For example why are full averaging and critical averaging comparable on the deception dilution dataset? Does this mean identifying the critical part does not help, i.e. the countermeasure is ineffective, or does it mean the critical part is not identified correctly? Why does WBR underperform so drastically on Backdoors?\n* While acknowledged, I do feel that the use of stronger models for WBR, BBR as well as the critical claim identification is a limitation of this work that could be fixed by simply using a smaller model.\n* Overall, I am doubtful of the significance and generalisability of the conclusions of this paper, since for example on average the difference between WWB and BBR seems insignificant (Figure 2). Maybe a more nuanced conclusion connecting to the training data of the probes could resolve this. \n* In some paragraphs of the discussion it is unclear whether the authors are referring to Table 3 or Figure 2. For example, in the paragraph starting on line 408, based on Figure 2 the difference between PR and Pairs on Backdoors seems insignificant considering the large bootstrap CIs.\n\nMinor:\nThere are quite a few typos / inconsistencies, e.g:\n* '..' vs \"...\" lines 40. vs 49.\n* line 50: [A] model's....\n* use \\citep in line 59\n* line 82 \"with a modest\" -> with modest\n* line 245, it is unclear what is meant by \"a black-box\"\n* BBR is used in line 314 but only discussed in 5.4"}, "questions": {"value": "1. Why are full averaging and critical averaging comparable on the deception dilution dataset? \n2. Does this mean identifying the critical part does not help, i.e. the countermeasure is ineffective, or does it mean the critical part is not identified correctly? \n3. Why does WBR underperform so drastically on Backdoors?\n4. Regarding the backup of critical averaging (line 286), how do we know the LLM failed to identify the critical claim?\n5. The choice of the ensemble (line 311) seems arbitrary, why not evaluate other combinations and other weightings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gXKunS6dZY", "forum": "lqnRwnjp7J", "replyto": "lqnRwnjp7J", "signatures": ["ICLR.cc/2026/Conference/Submission21761/Reviewer_kFyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21761/Reviewer_kFyE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864364129, "cdate": 1761864364129, "tmdate": 1762941922502, "mdate": 1762941922502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the evaluation of \"Deception Probes,\" a white-box technique designed to detect deceptive responses from Large Language Models (LLMs) by analyzing their internal activations.\n\nThe paper's central contribution is a novel benchmarking methodology termed the **\"Black-to-White Performance Boost.\"** This approach seeks to quantify the **marginal value** provided by \"White-Box Reasoning\" (WBR), which accesses both text and probe scores, compared to \"Black-Box Reasoning\" (BBR), which only accesses the public text output.\n\nUsing this methodology, the authors evaluate two existing deception probes (Pairs and RP) on multiple strategic deception datasets (e.g. Insider Trading, and Sandbagging). Additionally, they devise two stress tests: a \"Deception Dilution\" attack, which pads deceptive responses with substantial irrelevant, truthful information, and an out-of-distribution (OOD) generalization test involving the detection of backdoors in code.\n\nThe study finds that existing probes provide some performance improvement, termed \"weak but encouraging\" by the authors. A key finding is that on the OOD `Backdoors` dataset, the performance of the \"smarter\" White-Box Reasoning (WBR) method **significantly underperforms** the pure black-box baseline (BBR), attributed to a distributional shift in the probe activations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed core evaluation metric—the \"Black-to-White Performance Boost\"—is a pragmatic and significant contribution. It shifts the evaluative focus from the \"absolute accuracy of probes\" to the more critical question of \"what marginal benefit probes can offer in realistic monitoring scenarios.\"\n- The inclusion of stress tests for out-of-distribution (OOD) generalization (the `Backdoors` dataset) and adversarial robustness (the `Deception Dilution` attack) is crucial for evaluating the practical utility of these probes.\n- The analysis of WBR's failure on the `Backdoors` dataset (attributing it to a distributional shift in activation scores that breaks calibration) is insightful. It offers an important cautionary note for the future deployment of this technique."}, "weaknesses": {"value": "While the paper's premise is strong, its core methodology and experimental design suffer from flaws that undermine the persuasiveness of its conclusions.\n\n- **Fundamental Flaw in Experimental Design:** The authors employ Claude 3.5 Sonnet as the reasoning engine for both BBR and WBR, to monitor a comparatively weaker LLaMA-3.3-70B assistant. In this asymmetric \"strong-monitors-weak\" setting, the BBR baseline (relying on Claude 3.5's black-box scrutiny alone) achieves exceptionally high performance (e.g., AUROC > 0.82 and even 0.94 on the AI Liar and Sandbagging datasets). This inherently compresses the measurable \"marginal value\" the probes can provide. Consequently, the paper's central conclusion—that probes offer only a \"weak performance boost\"—is likely an artifact of this flawed design. This setup severely underestimates the probes' true potential in more realistic scenarios where a BBR baseline would falter.\n\n- **Fragility of the White-Box Reasoning (WBR) Method Misinterpreted as a Limitation of the Probes:** The paper presents the failure of WBR on the `Backdoors` dataset as a key finding. However, this appears to be a methodological flaw rather than a fundamental limitation of the probes. As per Appendix G.2 and Section 8, the WBR monitor's prompt includes hard-coded activation score ranges calibrated on the `Alpaca` control set (e.g., \"honest reply scores are between -3 and 0\"). When faced with the OOD `Backdoors` dataset, the activation score distribution for *honest* samples shifts fundamentally. This causes the WBR method to fail completely, precisely because it relies on this brittle, hard-coded rule. This demonstrates a lack of robustness in the paper's specific WBR implementation, not the ineffectiveness of the white-box signal in OOD scenarios.\n\n- **Conclusions Lack Generalizability:** All conclusions in this paper are highly contingent on a single model: LLaMA-3.3-70B-Instruct. The two evaluated probes were trained specifically on this model's activations (notably, layer 22), and all deceptive responses were also generated by this same model. It is well-established that internal representations, and likely deception mechanisms, differ significantly across LLM architectures and scales. Therefore, it is highly questionable whether these conclusions generalize to other models.\n\n- **Insufficient Analysis of the Probe Signal:** The experiments repeatedly demonstrate that the simplest `Full Averaging` method nearly always outperforms or matches the more complex `Critical Averaging` and `WBR` methods. This suggests that the signal provided by current probes is **global** (i.e., detecting a *macro-state* of deception) rather than **local** (i.e., detecting deception at the token or sentence level). If so, the `WBR` approach, which attempts to use an LLM to \"reason\" about and \"explain\" specific token-level activations, is likely \"over-interpreting noise\" and may even be harmful. The paper fails to sufficiently discuss this possibility: that complex, LLM-based white-box reasoning is counter-productive and less effective than simple statistical aggregation."}, "questions": {"value": "Given the \"strong-monitor, weak-assistant\" setup, how can the authors be sure that the central conclusion of a \"weak performance boost\" is not an artifact of the artificially inflated BBR baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tZb4y08fau", "forum": "lqnRwnjp7J", "replyto": "lqnRwnjp7J", "signatures": ["ICLR.cc/2026/Conference/Submission21761/Reviewer_o3aa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21761/Reviewer_o3aa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979228276, "cdate": 1761979228276, "tmdate": 1762941922226, "mdate": 1762941922226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a methodology for evaluating \"deception probes\" which are linear classifiers trained on an LM's internal activations to detect deceptive responses. The core contribution is framing the evaluation in terms of a \"black-to-white performance boost,\" which measures the additional accuracy gained by a white-box monitor (with access to probe activations) over a purely black-box monitor (which only sees the public dialogue). Authors test 2 deception probes across 6 datasets, and evaluating on 8 monitoring strategies. The key finding is that existing deception probes offer a \"weak but encouraging\" performance boost, with simple methods like averaging probe activations proving surprisingly effective and robust."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The experimental setup is thorough and well-designed. On top of many datasets tested, the inclusion of an adversarial attack (\"Deception Dilution\") and a control dataset (Alpaca) further strengthens the evaluation.\n\n- The paper produces several clear and actionable insights. The finding that a simple \"Full Averaging\" of probe activations (line 373)  is an important result which suggests that complex reasoning over probe scores may not always be necessary or even beneficial. The paper is also transparent about the probes' failures, such as the RP probe's poor performance on the Backdoors dataset (line 413), which provides valuable data for the community."}, "weaknesses": {"value": "1. Regarding results for hybrid reasoning models\nTo me, the most concerning result is the performance of the White-box Reasoning (WBR) monitor on the Backdoors dataset. As shown in Figure 2, the WBR method performs significantly worse than the Black-box Reasoning (BBR) baseline. This is counterintuitive; adding a genuinely informative signal (the Pairs probe has an AUROC of 0.844 on this dataset with Full Averaging) should not make an LLM-based monitor worse. The paper's explanation (lines 417-424) suggests BBR has an advantage, but it doesn't adequately explain why WBR underperforms. This implies a failure in the hybrid reasoning process itself, where the LLM monitor is incorrectly interpreting or being confused by the probe scores, which is a major weakness of that specific method.\n\n2. The deception dilution might be a bit simplistic\nWhile it's a strength that the paper includes an adversarial evasion strategy, the chosen method is quite basic. Appending a long list of irrelevant facts at the end of a response (lines 184-189) is a simple form of obfuscation. The fact that most monitors were still reasonably effective against it (Full Averaging achieved 0.931 AUROC, line 472) might lead to a false sense of security."}, "questions": {"value": "Regarding the WBR monitor's failure on the Backdoors dataset: Could you provide more insight into why the LLM appears to be confused by the probe scores? Did you perform any analysis of the WBR monitor's chain-of-thought for these examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "clXEAd4NZR", "forum": "lqnRwnjp7J", "replyto": "lqnRwnjp7J", "signatures": ["ICLR.cc/2026/Conference/Submission21761/Reviewer_4Koc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21761/Reviewer_4Koc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994672788, "cdate": 1761994672788, "tmdate": 1762941922024, "mdate": 1762941922024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}