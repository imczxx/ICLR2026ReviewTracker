{"id": "OkQtDD2uHM", "number": 13300, "cdate": 1758216200374, "mdate": 1759897447207, "content": {"title": "SAMPQ: Saliency-aware Mixed-Precision Quantization", "abstract": "Although mixed-precision quantization (MPQ) achieves a remarkable accuracy-complexity trade-off, conventional gradient-based MPQ methods are susceptible to input noise, which leads to suboptimal bit-width allocation strategies. Through saliency analysis, we indicate that treating sample feature regions as equally significant exacerbates the quantization error in MPQ. To mitigate this issue, we propose saliency-aware MPQ (SAMPQ), a novel framework designed to dynamically evaluate the sample saliency. In particular, SAMPQ is formulated as a three-stage cascade-optimized training procedure. At the first stage, the neural network (NN) weights are trained on vanilla samples with its bit-width configuration tentatively fixed. At the second stage, saliency maps are generated by one-step optimized weights. At the third stage, the bit-width allocation is optimized on saliency-reweighted samples while freezing NN weights. By iteratively alternating these optimization phases, SAMPQ enables the quantized NN modules to focus on fine-grained features. Experiments conducted on the benchmark demonstrate the effectiveness of our proposed method within existing MPQ frameworks.", "tldr": "A novel sample-aware mixed-precision quantization method.", "keywords": ["Mixed-precision quantization", "Saliency detection", "Fine-grained optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58eca512b4048dcb4b9814dad92d5f32abb7d853.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper claim that treating sample feature regions as equally significant\nexacerbates the quantization error in MPQ. To mitigate this issue, they propose\nsaliency-aware MPQ (SAMPQ), a novel framework designed to dynamically\nevaluate the sample saliency, and use them to identify optimal model bit-width. Experiments demonstrate the effectiveness\nof proposed method compared to baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper propose a novel 3-stage mixed-precision quantization framework that leverages sample saliency to identify model bit-width."}, "weaknesses": {"value": "1. Limited Novelty: Basically, as I understand, the paper simply weight data in a pixel-wise manner, the way they obtain saliency mask are pretty similar to existing adversarial learning method, the rest of the algorithm simply use gradient descent. So technical novelty is fairly limited, and the problem is not new.\n2. Motivation Justification: it is understandable that weighted data can improve performance compared to treating data uniformly, so  the insight in Figure 3 seems kinda weak to me. And that insight does not really related to mixed-precision quantization, but any data-dependent optimisation.  Also, why do you design the algorithm as a 3-stage frameworks? \n3. Lack of experiment results: the baseline are fairly limited, as the author only compare with a 2020 and 2021 method. Also, the improvement seems incremental to me.\n4. Writing clarity: there are some part that need more detail, like the way the model parameterise the bit-width to backpropagate, and $\\mathcal{L}_{BC}$ is not defined"}, "questions": {"value": "Please see the Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QFNlhnDZ7A", "forum": "OkQtDD2uHM", "replyto": "OkQtDD2uHM", "signatures": ["ICLR.cc/2026/Conference/Submission13300/Reviewer_AP4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13300/Reviewer_AP4o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485120223, "cdate": 1761485120223, "tmdate": 1762923967913, "mdate": 1762923967913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes analyzing salient channels by perturbations and obtain per-layer bit-width via gradient descent while focusing on the selected salient channels. Experiments show the proposed method increases top-1 accuracy by 0.3~0.4% and reduces BitOps by 1~3% on ResNet 18/50 and GoogLeNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Simple but effective method."}, "weaknesses": {"value": "- Novelty seems to be lacking.\n\nThe paper adopts the idea of selecting salient channels (or layers) the basic idea of which is widely used in mixed precision and pruning designs.\nBit-width search is also based on existing works of differential search based on probability distribution of bit-width.\n\n- Small gain.\nTop-1 accuracy increase by 0.3~0.4% and BitOps reduction by 1~3% on ResNet 18/50 and GoogLeNet look promising. However, it is not large enough to change existing designs, e.g., adopting 3 and 5 bit compute units.\n\n- Feasibility\nExperiments are mostly based on 2, 3, 4, and 5 bits. It would be practical to use only 2^N bit-width, e.g., 2 and 4 bits."}, "questions": {"value": "If we use 2^N bit-widths, e.g., 2, 4.,8 bits, what would be the experimental results in terms of top-1 accuracy and BitOps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H7CY1ldbif", "forum": "OkQtDD2uHM", "replyto": "OkQtDD2uHM", "signatures": ["ICLR.cc/2026/Conference/Submission13300/Reviewer_XAGa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13300/Reviewer_XAGa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595366504, "cdate": 1761595366504, "tmdate": 1762923967578, "mdate": 1762923967578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Saliency-Aware Mixed Precision Quantization (SAMPQ), which improves mixed-precision quantization by using saliency information to guide bit-width allocation. It trains the model in three stages, initial weight training, saliency map generation, and saliency-based bit-width optimization. SAMPQ allows quantized NN moduels to focus on fine-grained features."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow.\n- The proposed method is simple and effective."}, "weaknesses": {"value": "- The problem of mixed-precision quantization for CNNs is not new.\n\n- The adversarial-based saliency generation in Table 2 introduces significant computational cost, roughly doubling GPU-hours compared to the vanilla baseline. Additionally, the paper does not analyze or discuss how this cost scales with larger models.\n\n- Lack of experiment results: the baselines are fairly limited, as the authors only compare with methods from 2020 and 2021 (e.g., EdMIPS, GMPQ). Additionally, the improvements appear incremental.\n\n- Certain sections could be improved for readability. The insights from Figure 3 are also weak, as the insight that data samples are not equally important is already well-established in the literature."}, "questions": {"value": "1. Why is adversarial saliency chosen instead of simpler, well-established gradient-based saliency methods? What specific advantages does the adversarial approach provide that justify the doubled computational cost?\n2. How does the computational overhead scale with model size? Can the authors provide analysis for larger architectures to assess practical feasibility?\n3. Have the authors tested the framework on more aggressive quantization settings (e.g., 2-4 bit) where benefits should be more pronounced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cZcNGpn6cO", "forum": "OkQtDD2uHM", "replyto": "OkQtDD2uHM", "signatures": ["ICLR.cc/2026/Conference/Submission13300/Reviewer_d8np"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13300/Reviewer_d8np"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891521954, "cdate": 1761891521954, "tmdate": 1762923967265, "mdate": 1762923967265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes that treating sample feature regions as equally significant exacerbates the quantization error in mixed-precision quantization (MPQ), particularly when influenced by noise inputs from non-significant regions. Therefore, the paper proposes SAMPQ, a saliency-aware framework for MPQ. Each epoch alternates three stages: (Stage-1) fix the current bit-width allocation and train weights under CE + BitOps regularization; (Stage-2) using the updated weights, estimate channel-wise saliency and binarize it to obtain per-channel masks; (Stage-3) freeze weights and search bit-width policy on saliency-reweighted samples.\n\nOn ImageNet with ResNet-18/50 and GoogLeNet, plugging SAMPQ into EdMIPS and GMPQ yields small but consistent Top-1 gains with slightly lower BitOps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The insights presented in this paper are reasonable. Combining MPQ with adversarial-based saliency detection effectively addresses the proposed evaluation of feature region importance. While this innovation is incremental, it is logically sound.\n\n2. The three-stage loop (train weights → saliency → bit search) can be embedded into existing gradient-based MPQ (EdMIPS / GMPQ) with minimal surgery.\n\n3. Saliency only affects search, so there’s no inference-time cost; the channel-wise binary mask construction is clearly specified.\n\n4. Across three CNN backbones, SAMPQ gives consistent improvements in Top-1 with BitOps reductions. Figures & tables show search dynamics and give hardware/hyper-parameter details."}, "weaknesses": {"value": "1. Small absolute gains (≤0.4% Top-1; small BitOps drop). With single-seed reporting, it’s hard to judge robustness.\n2. Using saliency increases search time notably (Table 2). The paper calls the cost “extremely low,” which is not supported; needs careful measurement and reporting.\n3. The intro claims “channel components exert more profound influence than pixel differences”,  which supports the use of channel-wise masking in design. However, this remains largely an empirical statement. Could this statement be rephrased as an observation, or supplemented with supporting arguments?\n4. The approach described in the paper can be implemented as an insertable module. Given the modest performance gains, a reasonable strategy would be to refine and compare against additional baselines, while replicating experiments across more datasets for validation. Currently, the paper employs only one dataset and three convolutional model architecture, while comparing against a limited set of baseline methods.\n5. Please migrate the experimental settings appropriately to the main text in subsequent versions."}, "questions": {"value": "1. Stage-1 wording says “freeze weight and activation quantizer parameters” (line 252) but then “the MPQ model trains its network weights”. I don't quite understand the description here. According to the introduction, shouldn't stage 1 be about training the model weights?\n2. The method relies on a fixed bit-width configuration at the start of stage 1. Based on my understanding, subsequent stages are coupled to this predefined configuration. Therefore, the algorithm's ultimate performance may be influenced by this preset. If this is the case, the authors may consider incorporating appropriate comparative experiments.\n3. Top-1 improvement consistently ranges from 0.1–0.4%; at this magnitude, multiple random seeds (≥3) are required for mean ± standard deviation. Current search and fine-tuning phases primarily use seed=3 (Appendix), while Table 1's significance sample analysis employs seed=16, resulting in inconsistency across subsections; overall, multiple replication reports remain insufficient.\n4. Regarding the scope of the experiments, to further validate the effectiveness of the method, could the authors conduct comparative analyses using additional datasets and alternative MPQ baselines?\n5. Could the current method be extended to the Transformer architecture? Or does it require reliance on the convolutional induction bias of CNNs?\n\n\nIf I have misunderstood anything, please point it out to me. If the authors can resolve my questions, I would be happy to increase the rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "E8EjlMF0HO", "forum": "OkQtDD2uHM", "replyto": "OkQtDD2uHM", "signatures": ["ICLR.cc/2026/Conference/Submission13300/Reviewer_yQiA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13300/Reviewer_yQiA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972161938, "cdate": 1761972161938, "tmdate": 1762923966844, "mdate": 1762923966844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SAMPQ (Saliency-Aware Mixed-Precision Quantization), which improves conventional MPQ methods by incorporating sample-level saliency into bit-width allocation. Through a three-stage cascade optimization process, SAMPQ dynamically refines both weight training and quantization according to feature importance. Experimental results demonstrate that this approach achieves a better accuracy–efficiency trade-off than existing MPQ frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a saliency-aware perspective to mixed-precision quantization, addressing an often-overlooked limitation of existing methods that assume uniform feature importance. \n\n- The proposed three-stage cascade optimization framework is well-structured and conceptually clear, enabling an effective decoupling of weight training and bit-width allocation."}, "weaknesses": {"value": "- The experimental section is rather limited to convincingly demonstrate the effectiveness of the proposed method. The paper only compares against EdMIPS (2020) and GMPQ (2021), which are relatively dated. To better position the contribution, the authors are encouraged to include comparisons with more recent and competitive MPQ methods, as well as a broader range of baselines covering both uniform and non-uniform quantization techniques.\n\n- The experiments are conducted exclusively on CNN architectures. While CNNs remain relevant, modern quantization research increasingly focuses on transformer-based models, which dominate current vision and language applications. The authors should either (a) extend the experiments to include transformer backbones, or (b) provide a substantive discussion on the applicability and potential challenges of adapting their method to transformer architectures.\n\n- The proposed method appears to introduce additional computational overhead compared with EdMIPS. It would be helpful for the authors to discuss how the method scales to larger or more complex models. Without such analysis, the practical feasibility of the approach remains uncertain."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "08qBgd2F3K", "forum": "OkQtDD2uHM", "replyto": "OkQtDD2uHM", "signatures": ["ICLR.cc/2026/Conference/Submission13300/Reviewer_zWPn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13300/Reviewer_zWPn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013547875, "cdate": 1762013547875, "tmdate": 1762923966460, "mdate": 1762923966460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}