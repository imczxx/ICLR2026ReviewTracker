{"id": "Fllp8l6Puy", "number": 9940, "cdate": 1758150981661, "mdate": 1759897684184, "content": {"title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "abstract": "Large Language Models (LLMs) have shown promise in accelerating the scientific research pipeline. A key capability for this process is the ability to generate novel research ideas, and prior studies have found settings in which LLM-generated research ideas were judged as more novel than human-expert ideas. However, a good idea should not simply appear to be novel, it should also result in better research after being executed. To test whether AI-generated ideas lead to better research outcomes, we conduct an execution study by recruiting 43 expert researchers to execute randomly-assigned ideas, either written by experts or generated by an LLM. Each expert spent over 100 hours implementing the idea and wrote a 4-page short paper to document the experiments. All the executed projects are then reviewed blindly by expert NLP researchers. Comparing the review scores of the same ideas before and after execution, the scores of the LLM-generated ideas decrease significantly more than expert-written ideas on all evaluation metrics (novelty, excitement, effectiveness, and overall; p < 0.05), closing the gap between LLM and human ideas observed at the ideation stage. When comparing the aggregated review scores from the execution study, we even observe that for many metrics there is a flip in rankings where human ideas score higher than LLM ideas. This ideation-execution gap highlights the limitations of current LLMs in generating truly effective research ideas and the challenge of evaluating research ideas in the absence of execution outcomes.", "tldr": "", "keywords": ["Large Language Models", "Research Automation", "Scientific Discovery"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e83e41cd5997bc31728e799dad2d0b3cb43d55d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents the first large-scale execution study comparing LLM-generated and human-generated NLP research ideas. 43 expert researchers executed ideas (24 by AI, 19 by human) drawn from a prior ideation study. 58 expert reviewers then conducted double-blind reviews (181 in total) rating novelty, excitement, soundness, effectiveness, and overall quality.\n\nResults reveal a clear ideation–execution gap: although LLM ideas scored higher during ideation, their scores dropped substantially after execution (≈1–2 points on a 1–10 scale), while human ideas remained stable. Post-execution, human ideas slightly outperformed AI ideas, reversing the earlier ranking. Analyses show that LLM ideas often suffered from unrealistic scope and weaker empirical design, whereas reviewers of executed work emphasized feasibility and soundness over speculative novelty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work addresses whether LLMs can generate effective research ideas, not just novel ones. The study uses randomized assignment, double blinding, standardized instructions, and FDR-corrected statistical testing.\n\n- The work combines quantitative results with qualitative review comment analysis, explaining why AI ideas underperform in execution.\n\n- It establishes an “execution-aware” evaluation paradigm and commits to open-sourcing all data, code, and reviews."}, "weaknesses": {"value": "- 43 projects across seven NLP topics limit generalization; authors acknowledge this.\n\n- No contamination or recency discussion: Claude 3.5-Sonnet was used, but its training cutoff and overlap with prior research are not analyzed.\n\n- Reviewer consistency is reported only in aggregate; topic-level calibration is missing.\n\n- Overall technical contribution is not significant."}, "questions": {"value": "- How is training-data cutoff handled—any risk of contamination?\n\n- Would results differ with newer models (e.g., GPT-4o)?\n\n- Could a predicted “feasibility” score reduce the gap?\n\n- According to Table 3, AI idea executors are more experienced than those for Human. Would this affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gGHBe5tIZt", "forum": "Fllp8l6Puy", "replyto": "Fllp8l6Puy", "signatures": ["ICLR.cc/2026/Conference/Submission9940/Reviewer_qsa4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9940/Reviewer_qsa4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727263020, "cdate": 1761727263020, "tmdate": 1762921390557, "mdate": 1762921390557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether research ideas generated by large language models (LLMs) can lead to useful research outcomes once they are actually executed. Previous work has shown that LLMs are often able to propose ideas that look more novel or interesting than those written by human researchers. However, these earlier evaluations mostly stopped at the idea stage and did not test whether such ideas can be successfully turned into working research projects.\nTo address this gap, the authors conduct a controlled experiment involving 43 experienced NLP researchers. Each participant was randomly assigned either a human-written idea or an idea generated by the Claude 3.5-Sonnet model. They then spent around 100 hours implementing the idea and wrote a short 4-page report describing the experiments and results. All these completed projects were blindly reviewed by independent experts using consistent criteria, including novelty, excitement, effectiveness, and overall quality.\nThe results show a clear pattern: although LLM-generated ideas initially received higher scores at the ideation stage, their performance dropped significantly after execution. Human ideas, on the other hand, maintained roughly the same level of quality before and after execution. The authors refer to this difference as the ideation–execution gap. Further analysis indicates that the gap is not caused by poor implementation—since participants made only small adjustments to datasets or metrics—but rather by the lower practical feasibility and methodological soundness of the AI-generated ideas."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper tackles an important and timely issue: whether LLM-generated research ideas can actually lead to successful scientific outcomes once executed. This question extends prior work that only evaluated idea quality (Si et al., ICLR 2025), moving a step forward to test execution outcomes directly.\n2.\tThe study involves a large number of highly qualified human experts, including experienced researchers with strong publication records. It also quantifies key aspects such as working hours (around 100 hours per project) and the number of modifications made to each idea, showing that participants made only minor adjustments without altering core methods. These details make the findings convincing.\n3.\tThe paper combines quantitative comparisons of review scores with qualitative analyses of reviewer comments and modification patterns. This mixed approach allows for a balanced understanding of why AI-generated ideas underperform at the execution stage."}, "weaknesses": {"value": "1.\tWhile the research question is important, the paper mainly contributes through experimental design rather than technical or algorithmic advances. It does not propose new modeling approaches or analytical frameworks beyond the comparative study setup.\n2.\tThe paper identifies the existence of the ideation–execution gap but provides limited theoretical or computational explanation for why LLM-generated ideas fail in execution. A more systematic analysis of idea structure or model behavior could have strengthened the conclusions and would be more helpful for guiding models to generate better and more feasible research ideas in the future."}, "questions": {"value": "1.\tEach executed project was limited to around 100 hours of work and a 4-page report. Could this relatively short duration constrain the depth and maturity of the resulting research? For instance, some AI-generated ideas might be more ambitious or technically demanding, and therefore require substantially more time or resources to reach comparable quality.\n2.\tDid the authors consider normalizing for idea complexity or estimated implementation difficulty when comparing execution outcomes? Without such adjustment, simpler human ideas may appear more successful simply because they are easier to realize within the given time and effort constraints.\n3.\tWould it be possible to introduce AI-assisted execution in future experiments, so that the implementation process is more aligned with the characteristics of AI-generated ideas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H39BrprYaQ", "forum": "Fllp8l6Puy", "replyto": "Fllp8l6Puy", "signatures": ["ICLR.cc/2026/Conference/Submission9940/Reviewer_4Uap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9940/Reviewer_4Uap"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996943050, "cdate": 1761996943050, "tmdate": 1762921389775, "mdate": 1762921389775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the \"ideation-execution gap\" by comparing research ideas generated by an LLM (Claude-3.5-Sonnet) with those from human experts through a rigorous Randomized Controlled Trial (RCT). 43 expert researchers executed the assigned NLP ideas (~100 hours each) and produced short papers, which were then blindly reviewed by 58 expert reviewers. The core finding is that despite LLM ideas scoring significantly higher in novelty and excitement at the ideation stage, their scores dropped significantly more than human ideas after execution (p<0.05) across all metrics (novelty, excitement, effectiveness, and overall). This reversal of rankings suggests that current LLMs struggle to generate ideas that are truly effective when subjected to the rigors of practical research execution."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of a large-scale Randomized Controlled Trial (RCT) with blinding for both execution participants and reviewers is highly commendable and provides a strong foundation for the causal conclusions.\n2. The study mandated significant effort, with 43 expert participants spending an average of over 100 hours each to execute the ideas, lending credibility to the post-execution evaluation outcomes.\n3. The direct comparison of pre- and post-execution scores explicitly quantifies the difference in \"drop\" between AI-generated and human-generated ideas, providing a novel and important metric for evaluating AI ideation capabilities."}, "weaknesses": {"value": "1. While the drop in scores is statistically significant, the direct comparison of the final average execution scores between human and AI ideas was not statistically significant when treating each idea as an independent data point ($N=43$)8. This tempers the claim of human ideas being superior post-execution.\n2. Execution participants for human ideas spent significantly more time (mean 112.6 hr) compared to AI ideas (mean 93.7 hr). Although potentially explained by sample size, this difference could be a confounding variable related to the complexity or inherent feasibility of the assigned ideas.\n3. The study used a single, albeit state-of-the-art at the time, LLM (Claude-3.5-Sonnet) and generation method. It is unclear how different LLMs or more advanced agentic idea generation frameworks (e.g., iterative feedback) would impact the \"execution gap\"."}, "questions": {"value": "The analysis points out that execution reviewers focus more on empirical performance and rigor (e.g., missing baselines, insufficient analysis). Can the authors provide a fine-grained analysis of which specific types of weaknesses (e.g., technical flaws, inadequate experiment design, poor empirical results) were disproportionately mentioned in the low-scoring AI idea reviews compared to the low-scoring human idea reviews? This would help precisely characterize the nature of the LLM's ideation failure.\nThe disparity in mean execution time (112.6 hrs for Human vs. 93.7 hrs for AI) is a potential confounder. Did the projects with lower execution time generally receive lower reviewer scores, and was the difference in execution time due to AI ideas being inherently simpler, or were they less fully explored by the executors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GazmO7kPhT", "forum": "Fllp8l6Puy", "replyto": "Fllp8l6Puy", "signatures": ["ICLR.cc/2026/Conference/Submission9940/Reviewer_P5TK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9940/Reviewer_P5TK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255942942, "cdate": 1762255942942, "tmdate": 1762921389239, "mdate": 1762921389239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}