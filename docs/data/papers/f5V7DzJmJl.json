{"id": "f5V7DzJmJl", "number": 11484, "cdate": 1758200226509, "mdate": 1763247619837, "content": {"title": "Alignment from Ranking and Rating Information", "abstract": "The class of direct preference optimization (DPO) algorithms has emerged as a\npromising approach for solving the alignment problem in foundation models. These\nalgorithms work with very limited feedback in the form of pairwise preferences\nand fine-tune models to align with these preferences without explicitly learning a\nreward model. While the form of feedback used by these algorithms makes the\ndata collection process easy, its ambiguity in terms of the quality of responses has\nsignificant negative implications, including incentivizing policies that favor out-of-\ndistribution responses, a phenomenon referred to as likelihood displacement. In this\npaper, we study how DPO-style algorithms can leverage additional information in\nthe form of rating gap, which informs the learner how much the preferred response\nis better than the rejected one. We present new algorithms that can achieve faster\nstatistical rates than DPO in presence of accurate rating gap information. Moreover,\nwe theoretically prove and empirically show that the performance of our algorithms\nis robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance\nof our algorithms in comparison to a number of DPO-style algorithms across a\nwide range of LLMs and evaluation benchmarks.", "tldr": "", "keywords": ["direct preference optimization", "alignment", "sample complexity guarantees"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54aea34d8162ae2d57a59fcfd36321a941622ac0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a key limitation in Direct Preference Optimization (DPO): by using only binary pairwise preferences, DPO ignores the magnitude of the preference. The authors argue this ambiguity—where DPO cannot distinguish between a pair of high-quality responses and a pair with one good and one bad response —can incentivize policies that favor out-of-distribution responses, a phenomenon termed \"likelihood displacement\".\n\nTo address this, the paper proposes to augment the training data with \"rating gap\" information, which specifies *how much* a preferred response is better than a rejected one. The authors present three new algorithms that leverage this information:\n1.  **Rating DPO (RDPO)** \n2.  **Rating IPO (RIPO)** \n3.  **Maximum-Likelihood-based RDPO (ML-RDPO)** \n\nThe paper provides theoretical guarantees, suggesting these methods can achieve faster statistical rates (\"acceleration\") when the rating gap information is accurate and maintain performance even when this information is noisy, provided the hyperparameters are tuned correctly. Empirically, the authors demonstrate that RDPO and ML-RDPO outperform DPO and other DPO-style algorithms on the AlpacaEval and ArenaHard benchmarks across several base models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-Motivated Problem:** The paper's core premise is intuitive and sound. DPO's reliance on simple binary preferences clearly discards a rich source of information about response quality. The idea of incorporating the *magnitude* of this preference via rating gaps is a logical and compelling extension.\n2. **Principled Algorithm Derivation:** The authors provide principled, step-by-step derivations for their proposed algorithms. RDPO and RIPO are derived by modifying the RLHF objective to include rating information, while ML-RDPO is derived from a maximum-likelihood perspective .\n3. **Theoretical Analysis:** The paper includes a theoretical analysis that formalizes the concepts of \"acceleration\"  and \"robustness\", providing a useful (if idealized) framework for understanding *why* these methods should be beneficial.\n4. **Strong Empirical Performance:** The experimental results in Figures 1 and 2 are solid, showing that RDPO and ML-RDPO consistently achieve higher win rates than DPO and other baselines on the chosen benchmarks."}, "weaknesses": {"value": "1. **Motivation-Experiment Mismatch:** The paper's primary motivation is to solve the \"likelihood displacement\" problem, which it claims incentivizes OOD responses. However, the experimental evaluation never measures this phenomenon. The experiments are limited to win-rate comparisons on standard benchmarks. While the method improves win rates, there is no evidence provided to substantiate the claim that it actually mitigates likelihood displacement or improves OOD generalization.\n2. **Impractical Theoretical Assumptions:** The theoretical guarantees rely on assumptions that are unlikely to hold in practice.\n    * Assumption 1  is particularly strong, as it requires the policy class $\\Pi$ (i.e., the LLM) to be expressive enough to contain the optimal closed-form policy $\\pi_{\\beta}^{*}$ for *any* bounded reward function. The paper does not justify why this would be the case for real-world models.\n    * ML-RDPO relies on simplifying assumptions for its derivation, such as the conditional independence between the preference label ($z_i$) and the rating gap ($\\Delta_r^i$).\n3. **Hyperparameter Tuning and Practicality:** The method's practical utility is a major concern. The theoretical analysis itself provides guidance for setting the crucial hyperparameters $\\beta$ and $\\beta_1$ based on the ratio $Err_{DPO}(N,\\delta)/Err_{\\pi_{ref}}(\\hat{r})$ . This ratio is fundamentally unknowable in a practical setting, as it requires access to the (unknown) true reward error. The robustness experiments in Figure 3  confirm that performance is highly sensitive to finding the *correct* $\\beta_1$ (trust in ratings) , but the paper offers no practical heuristic for setting this value, likely requiring expensive, dataset-specific tuning.\n4. **Limited Experimental Scope:** The empirical validation is narrow. All main experiments are conducted using only the `ultrafeedback_binarized` dataset and evaluated on only two benchmarks, `AlpacaEval` and `ArenaHard`. This makes it difficult to know if the observed performance gains are broadly generalizable or an artifact of the specific properties of this dataset."}, "questions": {"value": "See the weaknesses. Most concerns arise from the disconnection between motivation and evaluation, as well as the practical utility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EBAeApvxXk", "forum": "f5V7DzJmJl", "replyto": "f5V7DzJmJl", "signatures": ["ICLR.cc/2026/Conference/Submission11484/Reviewer_zdGC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11484/Reviewer_zdGC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761110097968, "cdate": 1761110097968, "tmdate": 1762922590236, "mdate": 1762922590236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how DPO-style algorithms can leverage rating gap information as an additional signal. The authors propose methods including RDPO and ML-RDPO, which achieve superior statistical rates compared to standard DPO when ratings are sufficiently accurate, while demonstrating robustness to rating noise. Comprehensive experiments across various LLMs and benchmarks show consistent performance gains over existing DPO-based approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed methods demonstrate consistent improvements across diverse models and benchmarks.\n2. RDPO/ML-RDPO provably achieve faster convergence than DPO under the Bradley-Terry model while maintaining robustness to rating noise."}, "weaknesses": {"value": "1. While the proposed approach shows promise, the technical novelty could be further clarified. The method appears to draw heavily from distill-DPO and DPO.\n2. In the 'Experiments to assess robustness' section, how do other baseline methods perform in terms of robustness?\n3. Figure 4 indicates that ML-RDPO still heavily relies on rating information. Furthermore, comparisons with other DPO variants are absent, limiting the assessment of the method's unique advantages."}, "questions": {"value": "1. Why does ML-RDPO consistently underperform RDPO in most scenarios shown in Figure 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HT1npBska0", "forum": "f5V7DzJmJl", "replyto": "f5V7DzJmJl", "signatures": ["ICLR.cc/2026/Conference/Submission11484/Reviewer_2n9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11484/Reviewer_2n9a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912654570, "cdate": 1761912654570, "tmdate": 1762922589742, "mdate": 1762922589742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the usage of rating-gap information (the scalar difference in ratings of two responses) along with pairwise ranking data for LLM alignment. \n\nThe authors derive two main algorithms: Rating DPO (RDPO), which incorporates the rating gap into DPO, and Maximum-Likelihood Rating DPO (ML-RDPO), derived from a joint likelihood perspective. \n\nTheoretical analyses regarding acceleration and robustness to noisy ratings are given, which i quite liked. It adds a theoretical grounding to the work. Experiments are conducted on Zephyr, Mistral, and Llama-3 base models evaluated on AlpacaEval and ArenaHard."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Sound Theoretical backing to Algorithmic Claims\n\nThe paper is theoretically sound. Several DPO extensions add heuristic regularization terms. Instead, the authors derive ML-RDPO from a joint maximum likelihood perspective. The statistical bounds provided in Theorem 4.1 serve as a theoretical sanity check, confirming that including **accurate** rating information theoretically improves convergence rates compared to ranking-only methods under ideal conditions. (Note the noise in ratings typically sourced from reward models (Which are stochastic) is standard, hence how much this assumption of rating accuracy holds is questionable).\n\n---\n\n2. The idea of using ratings alongside rankings is certainly a good one. \n\nThe idea of using ratings alongside the rankings is good. But the idea is not new. It was first proposed in InfoNCA in neurips 2024 [1] and then extended into the DPO loss function [2,3]. These are references in this area that would be useful to include in their literature review for completeness.\n\n\n---\n\n### References\n\n[1] Chen, H., et al. (2024). Noise contrastive alignment of language models with explicit rewards. Advances in Neural Information Processing Systems, 37, 117784–117812.\n\n[2] Sun, S., et al. (2025). Reward-aware preference optimization: A unified mathematical framework for model alignment. arXiv preprint arXiv:2502.00203.\n\n[3] Gupta, T., et al. (2025). Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts. arXiv preprint arXiv:2412.04628."}, "weaknesses": {"value": "1. No improvement on Old Baselines:\n\nThe proposed method fails to improve on SIMPO for Llama, the strongest baseline that they tested on. The method only shows gains on older models (like Zephyr-7B-beta). \n\n---\n\n2. Theoretical Robustness to noise\n\nTheorem 4.1 is contingent on **knowing the noise level** to set the hyperparameter $\\beta_1$ correctly. In practice, this may lead to the brittleness observed in Appendix F.4, where $\\beta_1$ must be tuned by orders of magnitude (e.g., 0.1 vs 0.005) across different models. The theoretical guarantee of robustness hence is contingent on finding an appropriate $\\beta_1$ suitable to any new model or dataset, is it not?\n\n---\n\n3. Please consider recent baselines.\n\nThe paper reports ~29% Win Rates on Llama-3-8B, and shows equal performance with SIMPO [Note Simpo's own paper shows higher WR% and LC-WR % -- see Table 1 of the paper]. But please see the references below which exceed these numbers. For instance, RSPO [1] reports ~35% *Length-Controlled* win rates, while other recent multi-preference and reference-free approaches [2, 3] have reported win rates exceeding 50% on comparable benchmarks. Furthermore Chen et al.,2024 [4] provide a loss which is close to ML-RDPO. Incorporating these variants may better contextualize the method's true competitiveness.\n\n\n---\n\n### References\n\n[1] Tang, X., et al. (2025). Game-Theoretic Regularized Self-Play Alignment of Large Language Models. arXiv preprint arXiv:2503.00030.\n\n[2] Gupta, T., et al. (2025). REFA: Reference Free Alignment with Fine-Grained Length Control. COLM 2025.\n\n[3] Gupta, T., et al. (2025). AMPO: Active Multi Preference Optimization for Self-play Preference Selection. ICML 2025.\n\n[4] Chen, H., et al. (2024). Noise contrastive alignment of language models with explicit rewards. Advances in Neural Information Processing Systems, 37, 117784-117812.\n\n[5] Wu, Y., et al. (2025). Self-play preference optimization for language model alignment., International Conference on Representation Learning (Vol. 2025, pp. 91558–91582)."}, "questions": {"value": "*   **Degradation on Llama-3.1:** Why do RDPO and ML-RDPO not improve upon the simpler SimPO baseline on Llama-3.1-8B even though there is access to more information. ideally, given the rating information as well as ranking leads to performance improvement. Is it because of lack of sufficient tuning of the hyperparameters \n\n*   **Gaussian Assumption:** Theorem 4.2 and the derivation of ML-RDPO rely on the assumption that rating gaps are Gaussian distributed. Can you verify this assumption empirically on your datasets (e.g., UltraFeedback)? I'm wondering if the real-world rating distributions may have some heavy tail-ness that might violate this. For example, a histogram of gaps on UF would be a great contribution to the community using this training setup.\n\n---\n### Technical Question\n\n- Simpo paper's own numbers are higher than those reported in your paper (as per my understanding in the same setting). 40%LC and 37% WR see Table 1 of their paper. Any reason for this discrepancy?\n\n\n---\n\n### Suggestion\n\n*   **Baselines:** Please consider extending your work with more recent algorithmic works in this area to make it more empirically competitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7jr8gEOYd1", "forum": "f5V7DzJmJl", "replyto": "f5V7DzJmJl", "signatures": ["ICLR.cc/2026/Conference/Submission11484/Reviewer_VeC4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11484/Reviewer_VeC4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033050170, "cdate": 1762033050170, "tmdate": 1762922589318, "mdate": 1762922589318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}