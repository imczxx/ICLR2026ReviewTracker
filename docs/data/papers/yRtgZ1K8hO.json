{"id": "yRtgZ1K8hO", "number": 3512, "cdate": 1757453967712, "mdate": 1763686411794, "content": {"title": "The Polar Express: Optimal Matrix Sign Methods and their Application to the Muon Algorithm", "abstract": "Computing the polar decomposition and the related matrix sign function has been a well-studied problem in numerical analysis for decades. Recently, it has emerged as an important subroutine  within the Muon algorithm for training deep neural networks.\nHowever, the requirements of this application differ sharply from classical settings: deep learning demands GPU-friendly algorithms that prioritize high throughput over high precision. We introduce *Polar Express*, a new method for computing the polar decomposition. Like Newton–Schulz and other classical polynomial methods, our approach uses only matrix-matrix multiplications, making it \nvery efficient on GPUs.\nInspired by earlier work of Chen \\& Chow and Nakatsukasa \\& Freund, *Polar Express* adapts the update rule at each iteration by solving a minimax optimization problem.\nWe prove that this strategy minimizes error in a worst-case sense, allowing *Polar Express* to converge as rapidly as possible both in the early iterations and asymptotically.\nWe also address finite-precision issues, making it practical to use in `bfloat16`. When integrated into Muon, our method yields consistent improvements in validation loss for a GPT-2 model on one to ten billion tokens from the FineWeb dataset, outperforming recent alternatives across a range of learning rates.", "tldr": "We introduce a GPU-friendly algorithm for computing the polar decomposition of a matrix to low accuracy that is optimal in its class. This improves Muon.", "keywords": ["polar decomposition", "matrix sign", "numerical linear algebra", "muon", "optimization", "approximation theory"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d5f587a0c6eeca1b25d787f20ffdcee4149eb0c.pdf", "supplementary_material": "/attachment/d90708a9d57b5523aef148e2f7b0e66044200c03.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for orthogonalizing a matrix, with applications in the Muon optimizer. The method, PolarExpress, greedily computes polynomial approximations of the sign function in a residual manner using the Remez algorithm, and then applies these approximations during training to approximately orthogonalize the momentum buffer in Muon. The authors prove results about the convergence of PolarExpress and its optimality with respect to the L_\\infty norm to the sign function, and empirically show that PolarExpress improves upon existing orthogonalization methods used in current Muon implementations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I will preface this review by saying that I am not an expert in numerical methods. However, the presented algorithm seems novel and interesting, and the there is theory showing that PolarExpress is guaranteed to converge at least as fast as the canonical \"NewtonShultz5\"  method. This paper also has some other nice components, including\n- Theoretical justification for using a greedy residual polynomial construction\n- Empirical results on toy matrices showing that the method converges faster than existing baselines (Figure 3)\n- Results showing the effect of varying the lower bound guess $\\ell$ of the smallest singular value in the gradient matrix\n- Empirical GPT training experiments showing that PolarExpress achieves better initial convergence over prior Muon implementations."}, "weaknesses": {"value": "- There seems to be a missing \"key\" baseline, which is running Muon with the actual `polar' function and computing the SVD online. Although this is obviously impractical, it gives a sense of how far PolarExpress and these other methods are from \"true\" Muon. Can you run an experiment showing how much benefit there is from getting the optimal solution in real-world GPT training?\n- Likewise, does minimizing the L_\\infty norm of the sign function maximize the directional similarity to the actual \"true\" Muon update? Can you generate a plot of the cosine distance of PolarExpress's update (and other baselines) to the SVD update to get a sense of how close these approximations are?\n- The empirical experiments only go out to 1B tokens. 125M parameters @ 1B tokens is way below Chinchilla optimal, and my own experiments in the past have suggested that Muon's initial performance gap diminishes in the \"overtrained regime.\" If you have time, can you run an experiment past Chinchilla (125M parameters @ 10B tokens should be easily doable in a day on the 4 H100s used in the paper) to see how much of a gap there is?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nn7ke876m2", "forum": "yRtgZ1K8hO", "replyto": "yRtgZ1K8hO", "signatures": ["ICLR.cc/2026/Conference/Submission3512/Reviewer_WacA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3512/Reviewer_WacA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710590983, "cdate": 1761710590983, "tmdate": 1762916775261, "mdate": 1762916775261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper seeks to improve the Muon optimizer which is being used in several recent results as an alternative to Adam/AdamW for training small-sized language models. In Muon, one subroutine is calculating the polar decomposition. This is the main task handled in this work. The standard solution involves running Newton-Schulz which works well but convergence can be slow. More recent heuristics to speed it up can work but may not converge to the correct solution. Polar Express proposes to fix by adapting the polynomial at each iteration (a minimax problem is solved). This gives good initial speed of the heuristics and guarantees on convergence. On the practical side, GPT-2 models are trained to a better validation loss. The paper is well written. The technical insights are very nice and the empirical results are convincing."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.  The paper replaces recent heuristic strategies which can be fast but plateau out with a provably optimal algorithm. The initial progress is rapid and has similar convergence guarantees like classical methods. I found this result very interesting and new. \n\n2. I was familiar with the Nakatsukasa/Freund paper, but the way in which the minimax paradigm is adapted here is surprising in a good way, giving Thm 3.1 and 3.3. I feel that the idea is quite novel and can be useful beyond the Muon use case. \n\n3. The experimental results back up the main claims. Fig. 3 is sufficient validation that the idea in this paper deserves consideration. \n\n4. What is also nice is that this is not just a algorithmic results paper. The authors discuss numerical instability by adjusting the polynomials, showing the adjustments needed for practical gains on modern GPUs. Same with precomputing coefficients etc. Excellent balance of technical findings with practicality."}, "weaknesses": {"value": "1. Relatively minor. It will be good to show how much slower is Zolo-PD using newer QR implementations. \n2. Also minor. Experiments are only on GPT-2 with Muon. If Muon is effective, why not expand the scope of experiments to other architectures and even finetuning?"}, "questions": {"value": "1. You may consider slightly expanding the scope of experiments. The paper is still a valuable contribution, but even if the idea is not equally effective in other cases, it will be good to point out where the gains are limited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3QgLVF5gAi", "forum": "yRtgZ1K8hO", "replyto": "yRtgZ1K8hO", "signatures": ["ICLR.cc/2026/Conference/Submission3512/Reviewer_kcTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3512/Reviewer_kcTx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926220651, "cdate": 1761926220651, "tmdate": 1762916775018, "mdate": 1762916775018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Polar Express, a GPU‑ and low‑precision‑friendly method for approximating the polar factor/sign—an essential subroutine in the Muon optimizer. The method composes low‑degree odd polynomials chosen via greedy scalar minimax problems over the current spectral interval, yielding an optimal (worst‑case) composition under this formulation. The authors provide theory (including greedy optimality and convergence guarantees), implement finite‑precision tweaks, and release PyTorch code. Empirically, on synthetic matrices and GPT‑2 Small/Large trained on 1B FineWeb tokens, Polar Express improves validation loss over recent heuristic approaches."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Significance: Muon looks genuinely promising with potentially large practical and research impact; having a fast, stable polar/sign subroutine matters. Polar Express appears to push this forward in a way that can plausibly change practice. The LLM evidence (GPT‑2 Small/Large on 1B tokens) is encouraging and consistent across learning rates. A robust, degree‑5, bfloat16‑friendly routine that slots into Muon is directly actionable (basically without overhead) for groups training LLMs; the drop‑in PyTorch implementation is appreciated.\n\n- Quality: Most claims are supported by rigorous analysis. The formulations are clean and seem to be technically sound.\n\n- Literature overview / educational value: The paper helpfully connects classical numerical analysis to ML practice, which will benefit the ML audience not yet steeped in this area.\n\n- Clarity: Visualizations and step‑by‑step algorithmic presentation are clear. A more high‑level narrative of the research journey (what didn’t work, how you arrived at the greedy formulation) would further help orient ML readers."}, "weaknesses": {"value": "1. For GPT‑2 Large, 1B tokens is on the small side (below Chinchilla optimal) for practical LLM training. It would be valuable to include (or comment on) a longer run under the same hyperparameters to confirm the trends at more realistic budgets.\n\n2. The set of practical ML problems can be diversified a bit, e.g., by considering vision domain.\n\n### Minor\n\n- Figure/caption consistency. \nFigure 5 (p. 9) left caption appears to re‑use the Small model best‑loss numbers from Fig. 4; please correct the numbers/wording and standardize the method label (e.g., use muon‑PolarExp consistently).\n\n- Weight decay choice across model sizes. \nFig. 4 (Small) uses WD=0, while Fig. 5 (Large) uses WD=0.1. A brief explanation (or parallel ablations) would make comparisons easier to interpret.\n\n- Attribution/citation clarity.\nThe text refers to You’s method but cites Cesista et al., 2025; consider clarifying attribution to avoid confusion for readers.\n\n- Copy‑editing nits (easy fixes). \n“QWHD” → QDWH; “approximation ofsign(x)” → “of sign(x)”; keep Newton–Schulz dash style consistent; standardize dataset name to FineWeb; unify method label (muon‑PolarExp vs muon‑polarexpress); and normalize small‑caps spacing in the header."}, "questions": {"value": "- The paper states “…even to train a 32‑billion parameter frontier LLM (Kimi Team et al., 2025).” Can you confirm the exact parameter count for the model trained with Muon (many readers associate Kimi K2 with ~1T parameters) and adjust the phrasing if needed?\n\n- The paper says the Jordan/You heuristics “fail to converge.” In what precise sense (e.g., asymptotic non‑convergence in spectral norm, plateau at a particular error)? A small counter‑example or quantitative statement would be helpful.\n\n- Domain breadth: would you consider adding a non‑LM task (e.g., a vision model) to test whether the gains carry over beyond language modeling?\n\n- You emphasize minimizing the number of matrix–matrix products. Why not target total arithmetic cost?\n\n- Prior theory typically analyzes Muon with an exact polar/sign. For the camera‑ready, could you add a short discussion on how inexactness in Polar Express propagates in Muon’s stability/convergence, and relate this to recent variants such as SUMO (https://arxiv.org/pdf/2505.24749) and follow‑ups?\n\n- Why restrict to odd monomials? A brief early explanation (symmetry of sign; efficient evaluation M^{2q+1}=M(M^\\top M)^q) would help; do even terms ever help under the same product budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HWd8vocCkq", "forum": "yRtgZ1K8hO", "replyto": "yRtgZ1K8hO", "signatures": ["ICLR.cc/2026/Conference/Submission3512/Reviewer_Xn5s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3512/Reviewer_Xn5s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931270251, "cdate": 1761931270251, "tmdate": 1762932242075, "mdate": 1762932242075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new algorithm for computing the matrix polar decomposition for the Muon optimizer used in training large neural networks. Unlike classical methods designed for high numerical accuracy, Polar Express is optimized for GPU efficiency and relatively low-precision computation (bfloat16). The authors derive the algorithm by solving a minimax optimization problem that guarantees optimal convergence in the worst case and demonstrate substantial empirical improvements over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a theoretically grounded algorithm.\n2. The method is practical, using GPU-efficient GEMMs and is numerically stable.\n3. Experiments on GPT-2 with the Muon optimizer show consistent improvements in validation loss over existing methods."}, "weaknesses": {"value": "1. It would be interesting to see how well the method works on models other than language models. \n2. Lack of detailed runtime or throughput benchmarks. The experiments show the method works better than baselines under the same number of iterations, but it is unclear whether it is faster."}, "questions": {"value": "see weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZIUFGUg9F3", "forum": "yRtgZ1K8hO", "replyto": "yRtgZ1K8hO", "signatures": ["ICLR.cc/2026/Conference/Submission3512/Reviewer_caWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3512/Reviewer_caWQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014775719, "cdate": 1762014775719, "tmdate": 1762916774401, "mdate": 1762916774401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision on November 20"}, "comment": {"value": "We thank all the reviewers for their helpful feedback. We have uploaded a revised version of the paper that contains the following changes (highlighted in red in the PDF):\n- Adding a sentence in the contributions (Section 1.3) noting that Polar Express has been incorporated into the modded-NanoGPT speedrun code base, which is a community lead, highly optimized implementation of LLM training. In modded-NanoGPT, Polar Express was used to set the current speedrun record, and continues to be the default method for computing the polar factor.\n- Adding an experiment that compares implementing Muon using Polar Express with 5 iterations (our recommended method) to (a) Polar Express with a range of iterations from 2 to 30, and (b) the exact polar factor computed via an SVD. See Section 4.3 and Figure 5. This experiment shows that: less than 5 iterations performs worse, more than 6 iterations performs no better, and SVD performs no better while also being significantly slower.\n- Adding an experiment that trains GPT-2-Large and GPT-2-Small on 10 billion tokens instead of 1 billion.\n- Adding Figure 8 in Appendix H, which supplements Figure 3 by showing convergence in the Frobenius norm and cosine distance.\n- Reorganizing the placement of our main experiments for greater legibility. Figures 1 and 4 present our experiments training GPT-2 without weight decay on 1 billion tokens, which are described in Section 4.2. Appendix H contains more results from those experiments (Figure 9). It also contains Figure 10, showing the same thing but with weight decay, and Figure 11, showing the same thing but with 10 billion training tokens. A new section, 4.3, describes these ablations in the main text.\n- Adding small clarifications and correcting minor typos pointed out by the reviewers.\n\nIn addition to the new experiments described above, we are currently preparing experiments that use Muon to train a ResNet for image classification. We intend to have these online before the end of the discussion period."}}, "id": "nhaTapFtkH", "forum": "yRtgZ1K8hO", "replyto": "yRtgZ1K8hO", "signatures": ["ICLR.cc/2026/Conference/Submission3512/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3512/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission3512/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763689368581, "cdate": 1763689368581, "tmdate": 1763689368581, "mdate": 1763689368581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}