{"id": "qe060gmfm7", "number": 16152, "cdate": 1758260700858, "mdate": 1763772063380, "content": {"title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning", "abstract": "Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (\\textbf{RPG}) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to fully differentiable surrogates; (iii) identifies and corrects an off-policy importance-weighting mismatch in GRPO's KL term; and (iv) introduces RPG-Style Clip, a truncated-importance-sampling step within RPG-REINFORCE that enables stable, off-policy policy-gradient training at scale. On mathematical reasoning benchmarks (AIME24, AIME25), RPG-REINFORCE with RPG-Style Clip improves accuracy by up to $+6$ absolute percentage points over DAPO. Notably, RPG is a \\emph{stable and scalable} RL algorithm for LLM reasoning, realized via (a) a KL-correct objective, (b) truncated importance sampling, and (c) an iterative reference-policy update scheme.", "tldr": "", "keywords": ["LLM reasoning", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f362b50ebf2b978868502e58959adf675aca7291.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RPG (Regularized Policy Gradient), a unified framework for KL-regularized policy-gradient training of LLMs under off-policy sampling. It systematizes design choices across: (i) KL direction (forward vs reverse), (ii) normalized vs unnormalized KL (UKL), and (iii) estimator/implementation (fully-differentiable vs REINFORCE-style with stop-gradient). Key theoretical results (e.g., Propositions for UFKL/URKL gradients and losses) show how to obtain exact gradients of the intended KL-regularized objective when training off-policy with importance weights, and establish gradient-equivalence between differentiable and REINFORCE-style surrogates. Practically, the paper (a) clarifies that the widely used k3 estimator corresponds exactly to the unnormalized KL, (b) identifies an importance-weighting mismatch in GRPO’s KL term under off-policy sampling and provides a corrected estimator, and (c) introduces RPG-Style Clip, a dual-clip truncated-importance step for stability. On AIME24/25 with Qwen3-4B, RPG-REINFORCE + RPG-Style Clip outperforms DAPO by up to +6pp absolute accuracy. The method also employs periodic reference-policy updates to maintain a practical KL trust region."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It provides a compact derivation that covers Forward/Reverse × Normalized/Unnormalized KL, with explicit off-policy corrections and both differentiable and REINFORCE-style surrogates.\n- It pinpoints the missing importance weight in GRPO’s KL when sampling off-policy, which could be an issue that many people overlook.\n- The paper introduces RPG-Style Clip (dual-clip/truncation) and iterative reference updates, which empirically stabilize off-policy PG for LLM reasoning while requiring only a single active model on GPU (log-probs from $\\pi_{\\textrm{old}}$ cached).\n- Empirically, it has strong performance with +6pp over DAPO in the best cases on AIME24/25, with stable entropy and controlled response length."}, "weaknesses": {"value": "- All core results are on Qwen3-4B and math reasoning. Claims about “stable and scalable RL” would be stronger with larger backbones (e.g., 7B–14B) and diverse tasks or datasets.\n- While GRPO/DAPO are relevant, head-to-head comparisons with varying KL directions (FKL/RKL vs UFKL/URKL) would solidify the empirical story. Current ablations on clip hyper-parameters and reference-update schedules are limited in the main text.\n- RPG-Style Clip introduces controlled bias via truncation. The paper acknowledges this and suggests schedules, but there’s no systematic bias study (e.g., varying $\\epsilon_1$, $\\epsilon_2$) to quantify performance/variance trade-offs.\n\nOverall, the theoretical foundations are solid and the proposed algorithm is well-motivated. The strengths outweigh the weaknesses, and I am leaning toward acceptance."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iIeji5lS0E", "forum": "qe060gmfm7", "replyto": "qe060gmfm7", "signatures": ["ICLR.cc/2026/Conference/Submission16152/Reviewer_jhGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16152/Reviewer_jhGz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964544495, "cdate": 1761964544495, "tmdate": 1762926320050, "mdate": 1762926320050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RPG, a unified theoretical and practical framework for KL-regularized policy gradient algorithms. The authors systematically analyze different KL regularization variants and corresponding estimators, clarifying their gradient properties under off-policy sampling. Their derivation shows how to obtain the exact gradients of intended KL-regularized objectives. It identifies a weighting mismatch in GRPO, and introduces a truncated-importance estimator which stabilizes training. Experiments show RPG and RPG-REINFORCE variants outperform other baselines on mathematical reasoning benchmarks and have better stability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a unified framework connecting various KL regularization forms. The derivations offer clarity and formal grounding.\n2. The authors propose RPG-Style Clip, which can effectively balance stability and bias under off-policy training and improve scalability for LLMs\n3. The experiments demonstrate notable improvements in both accuracy and stability."}, "weaknesses": {"value": "1. Some representative baselines are missing, e.g., RLOO, REINFORCE++, GPG. \n2. The experiments lack ablation studies isolating the contributions of RPG-Style Clip, reference-update frequency, and clipping thresholds.\n3. The RPG-Style Clip introduces a bias-variance trade-off but it is not explored quantitatively.\n4. Section 3 and 4 are heavy and hard to follow."}, "questions": {"value": "1. Why the entropy grows up with training in DAPO and RPG while decreases in GRPO? I assume both DAPO and GRPO should both decrease but to different degrees. This is contradicted to the DAPO paper. \n2. More recent works have opted to remove the KL term from the RL objective and achieve good performance. In comparison, does a compromise of periodically updating the reference model offer significant and irreplaceable advantages? \n3. Do the empirical results show a significant performance difference between the corrected Normalized KL and Unnormalized KL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gtVDuK3hyU", "forum": "qe060gmfm7", "replyto": "qe060gmfm7", "signatures": ["ICLR.cc/2026/Conference/Submission16152/Reviewer_w5SC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16152/Reviewer_w5SC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972155783, "cdate": 1761972155783, "tmdate": 1762926319255, "mdate": 1762926319255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author studied the design of KL regularization in reinforcement learning. Specifically, the analysis shows that the commonly used k3 estimator is equivalent to the use of unnormalized KL. The author also shows that GRPO's original KL divergence implementation lacks an importance sampling term due to its off-policy attributes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Nice theoretical analysis between k3 estimator and UKL\n* The tackled KL divergence issue is indeed a pitfall of GRPO"}, "weaknesses": {"value": "The experiments in the paper are quite poorly conducted:\n* The model is tested only on math tasks, and even then, only two AIME benchmarks are used.\n* Figure 2 is presented without any analysis. It seems that in Figure 2(c), DAPO is not converging — what would it look like if we extrapolated the training curve?\n* (c) RL training often exhibits high variance; instead of showing a single curve, the authors should include a curve with a standard deviation region.\n* (d) In Figure 2(d), why is only GRPO’s actor entropy decreasing? Out of the five curves, three increase and plateau, while the REINFORCEMENT curve continues to rise — why?\n* (e) What is “mean@32”? Do you mean “pass@32”?\n\nAlso, there seem to be quite a few unanswered questions to be verified by the experiments:\n* FKL and RKL seem to be comparable in performance. Is there any evidence that such two constraints result in different properties in the post-trained model?\n* Some papers, like DAPO and Dr.GRPO, advise dropping KL divergence for RL tuning. Is there some analysis on how different designs and strengths of KL regularization change the training dynamic and the tuned model's properties?"}, "questions": {"value": "* In Figure 2, is the difference between GRPO and RPG-URKL in whether having the importance weight in the kl term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tSC2a9S7HZ", "forum": "qe060gmfm7", "replyto": "qe060gmfm7", "signatures": ["ICLR.cc/2026/Conference/Submission16152/Reviewer_LL8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16152/Reviewer_LL8H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16152/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762242532379, "cdate": 1762242532379, "tmdate": 1762926318794, "mdate": 1762926318794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers and Area Chairs"}, "comment": {"value": "Dear Reviewers and Area Chairs,\n\nWe thank you for the careful reading of our work and the detailed comments. In this revision, we have expanded the empirical study of RPG and added more discussions on the theoretical side. The main new content concerns long-context training at 8K, stronger baselines for 4K, and extra discussion of the link to natural policy gradient. New material is marked in blue in the paper.\n\n### 1. Scalability to 8K context length\n\nTo test RPG on long CoT reasoning, we extend the training and evaluation setup to an 8K context length. These runs are very costly and each experiment cost around **9,000 H100 GPU hours**.\n\nUnder this setting, **RPG-REINFORCE reaches 52% accuracy on AIME25**. This value is higher than the official **Qwen3-4B-Instruct model (47%)** and higher than strong baselines such as GRPO and DAPO. These results show that RPG gives a competitive training algorithm for strong reasoning models at long context.\n\nResults at 8K context are shown as follows:\n\n| Method | AIME24 (Last) | AIME24 (Best) | AIME25 (Last) | AIME25 (Best) | AMC23 (Last) | AMC23 (Best) |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| GRPO | 0.3750 | 0.4396 | 0.3354 | 0.4063 | 0.9109 | 0.9297 |\n| DAPO | 0.5438 | 0.5740 | 0.4469 | 0.4740 | 0.9375 | 0.9430 |\n| **RPG-UFKL** | **0.5938** | **0.6177** | 0.4698 | 0.4865 | **0.9492** | 0.9517 |\n| **RPG-URKL** | 0.4542 | 0.5260 | 0.5261 | 0.4938 | 0.9406 | **0.9539** |\n| **RPG-REINFORCE-UFKL** | 0.5906 | 0.5958 | 0.4833 | 0.5031 | 0.9453 | 0.9469 |\n| **RPG-REINFORCE-URKL** | 0.5708 | 0.5781 | **0.5073** | **0.5208** | 0.9398 | 0.9469 |\n\n### 2. Expanded baselines and benchmarks for 4K context\n\nFor the 4K context length, we extend both the set of baselines and the evaluation datasets. We add **REINFORCE++** and **REINFORCE++ Baseline** so that the comparison covers stronger policy gradient variants. We include the **AMC23** dataset as one more math benchmark.\n\nAcross these settings, RPG variants achieve the best scores on many tasks. In particular, RPG-URKL reaches the highest score of **0.9531** on AMC23.\n\n| Method | AIME24 (Last) | AIME24 (Best) | AIME25 (Last) | AIME25 (Best) | AMC23 (Last) | AMC23 (Best) |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| REINFORCE++ Baseline | - | 0.4281 | - | 0.3833 | - | 0.9172 |\n| REINFORCE++ | 0.3490 | 0.3885 | 0.2822 | 0.3479 | 0.8977 | 0.9297 |\n| GRPO | 0.3458 | 0.3677 | 0.2896 | 0.3042 | 0.9016 | 0.9109 |\n| DAPO | 0.4063 | 0.4479 | 0.3510 | 0.3938 | 0.9297 | 0.9297 |\n| **RPG-UFKL** | 0.4031 | 0.4396 | 0.3625 | 0.3979 | 0.9477 | 0.9500 |\n| **RPG-URKL** | 0.3990 | 0.4219 | 0.3438 | 0.3792 | **0.9500** | **0.9531** |\n| **RPG-REINFORCE-UFKL** | 0.4281 | 0.4375 | 0.3771 | 0.4042 | 0.9023 | 0.9133 |\n| **RPG-REINFORCE-URKL** | **0.4458** | **0.4531** | **0.4125** | **0.4313** | 0.9313 | 0.9352 |\n\n### 3. Theoretical connections and explanations\n\n*Connection to Natural Policy Gradient (NPG).*  \nRemark 3.8 and Appendix C show the connection between RPG and NPG and its derivation. We show that the NPG update appears as a special case of the RPG update when we use a linear approximation of the expected return and a quadratic approximation of the KL regularization term.\n\n*Off-policy weighting.*  \nSection 3 now gives a more direct derivation of the importance weighting scheme. With the exact weights, the surrogate gradient matches the gradient of the KL regularized objective. This makes clear that RPG is not a heuristic update but an exact gradient estimator for this objective.\n\nWe believe these additional results address the main concerns and give a clearer view of the strengths and limitations of RPG."}}, "id": "zZfMECQ77L", "forum": "qe060gmfm7", "replyto": "qe060gmfm7", "signatures": ["ICLR.cc/2026/Conference/Submission16152/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16152/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16152/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763755694138, "cdate": 1763755694138, "tmdate": 1763772629479, "mdate": 1763772629479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}