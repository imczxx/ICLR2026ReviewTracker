{"id": "2d3j6bt21A", "number": 22746, "cdate": 1758334948859, "mdate": 1759896849149, "content": {"title": "Relational Graph Transformer", "abstract": "Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.", "tldr": "We propose the Relational Graph Transformer (RelGT), the first graph transformer architecture designed for deep learning relational databases.", "keywords": ["graph", "graph transformer", "relational deep learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/027b56c5bac3d2ee927206fa517050fb7be26a0c.pdf", "supplementary_material": "/attachment/1326880f88eb2deeca88bcb5221c8a49c7d9d747.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RELGT (Relational Graph Transformer), a novel Transformer architecture designed specifically for Relational Deep Learning (RDL),  i.e., learning directly from multi-table relational databases represented as heterogeneous temporal graphs. The model tokenizes each node into five elements: features, type, hop distance, time difference, and subgraph-based positional encoding, aiming to capture heterogeneity, temporality, and structural complexity. RELGT combines local attention (over sampled subgraphs) and global attention (to learnable centroids), and is evaluated on 21 tasks from the RelBench benchmark, outperforming heterogeneous GNNs (HeteroGNN) and HGT baselines by up to 18%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Ambitious motivation: Addresses an important gap, the adaptation of Transformers to relational databases, an increasingly relevant setting (RDL).\n- Unified treatment of temporal, heterogeneous, and structural properties within a single architecture.\n- Comprehensive benchmark evaluation on RelBench (21 tasks), with reasonable performance improvements and ablations to justify design choices.\n- The multi-element tokenization idea (features, type, hop, time, subgraph PE) provides a structured approach to encode complex relational contexts.\n- The local–global hybrid design aligns with recent scalable GT approaches and avoids expensive positional encoding precomputation.\nAblation studies are thorough and help identify which token elements contribute most."}, "weaknesses": {"value": "- Novelty is overstated.\n   - Similar approaches already exist, e.g. Heterogeneous Temporal Graph Transformers and other models combining subgraph tokenization with structural encoders. The paper presents RELGT as “the first GT for relational graphs,” but other prior works [1-3] already tackle heterogeneity with a transformer .\n   - The tokenization strategy appears as a straightforward concatenation of known encodings (type, hop, time, PE), rather than a fundamentally new design.\n  - The subgraph GNN PE encoder appears conceptually very similar to the approach proposed in [4], as also acknowledged by the authors.\n\n- Conceptual gaps / unclear definitions.\n   - The notion of “schema-defined structure” in 2.1 is not clearly formalized. Other heterogeneous graphs (DBLP, IMDB, ACM) also have schema-defined relations, so this does not uniquely characterize relational data.\n\n- Overstatements and misleading claims.\n   - The paper states that RDL GNNs outperform LightGBM on all tasks, while the original RDL paper shows a few opposite results.\n   - Although the abstract claims that RELGT “consistently matches or outperforms” GNN baselines, this statement is not supported by the reported results. In Table 1b (entity classification), the proposed model performs worse than the RDL baseline in about half of the tasks, with gains often close to zero and only a few cases showing meaningful improvements. In Table 1a (regression), the trend is more favorable to the proposed model, although it would be more informative to report the gains in absolute rather than percentage terms.\n   - In Section 4.2, the authors state that HGT underperforms even compared to RDL, and they provide a time complexity analysis showing that adding Laplacian Encodings (LE) to HGT improves performance but at the cost of higher runtime. However, what is missing is a temporal (runtime) analysis of the proposed model compared to all competitors. Moreover, the HGT method has been improved in paper [3], specifically addressing the runtime efficiency issue, but this variant was not considered, making the temporal analysis incomplete.\n   - Furthermore, the reported results (of model and competitors) do not include mean and standard deviation across multiple runs, which are crucial to assess statistical significance and reproducibility of the claimed improvements, as explained in [4].\n\n- Technical novelty and clarity.\n   - Most encoders (feature, type, hop, time, PE) are standard components and not surprising.\n   - The model essentially applies a Transformer on subgraph samples with concatenated encodings, which is conceptually simple and not theoretically grounded.\n   - Lack of formal analysis of computational complexity, scalability, or positional encoding stability.\n\n- Missing broader context.\n   - No discussion on relational inductive bias or how RELGT generalizes to unseen tables or relations.\n   - No exploration of few-shot or inductive RDL settings where Transformers could excel.\n \n[1] Wang et al. HTGformer: Heterogeneous Temporal Graph Transformer\n\n[2] Hu et al, Heterogeneous Graph Transformer\n\n[3] Yun et al, Graph transformer networks: Learning meta-path graphs to improve gnns.\n\n[4] Demšar et al., Statistical Comparisons of Classifiers over Multiple Data Sets"}, "questions": {"value": "- How does RELGT differ concretely and conceptually from existing Heterogeneous Temporal Graph Transformers?\n- Can the authors clarify what they mean by “schema-defined structure” and how it is different from the one of other heterogeneous graph datasets like IMDB, DBLP, ACM and why this structure requires a special architecture?\n- How is the subgraph GNN PE trained and does it introduce additional computational cost?\n- Why other multi-relational GNN models [5-8] have not been tested even if they should be able to work in this multi-relational setting?\n- Could the authors quantify the scalability and training time of RELGT compared to RDL and other competitors?\n\n\n[5] Schlichtkrull et al., Modeling relational data with graph convolutional networks\n\n[6] Yuet al.,  Heterogeneous graph representation learning with relation awareness.\n\n[7] Zhu et al.,  Relation structure-aware heterogeneous graph neural network.\n\n[8] Ferrini et al., A Self-Explainable Heterogeneous GNN for Relational Deep Learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l1HjpWelJ1", "forum": "2d3j6bt21A", "replyto": "2d3j6bt21A", "signatures": ["ICLR.cc/2026/Conference/Submission22746/Reviewer_nFvD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22746/Reviewer_nFvD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760631933569, "cdate": 1760631933569, "tmdate": 1762942368350, "mdate": 1762942368350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Relational Graph Transformer (RELGT), a graph transformer architecture for relational tables. \nRELGT employs a multi-element tokenization strategy that includes five components (features, type, hop distance, time, and local structure). The method shows improved performance on a wide range of experiments, tested on the RelBench benchmark"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation, good explanation of problem\n\n2. Architecture is well-explained, the different modules are well motivated and justified, and an ablation study analyses their benefits. Each component is linked to a specific challenge in relational deep learning, which is very good.\n\n3. Extensive experiments: The evaluation is broad and the results are convincing. That said, as I am not an expert in all recent baselines for this specific area, I cannot definitively assess the completeness of the comparisons.\n\n4. Good writing and clarity.\n\n5. The work successfully leverages and extends the principles of Graph Transformers to handle the challenges of relational data."}, "weaknesses": {"value": "### 1. Relation to Temporal Knowledge Graphs (TKG)\n* There is no proper distinction between relational entity graphs and TKG (see e.g. TGB 2.0), especially in section 2.1 challenges: while I agree that relational entity graphs are difficult and challenging, and different to conventional graph data, in this section, the difference to temporal knowledge graphs is not clarified in my opinion.  in my opinion, the difference are mostly in the entity-specific attributes - do you agree? if yes,  it would be good to mention and clarify.\n\n###  2. Integration of relation types:\n* The formal definition of the REG includes relation types (psi), but these are not explicitly part of the five-element token representation. Why were edge relation types excluded from tokenization or the attention mechanism? A relational encoder or relation-aware attention might improve expressivity.\n\n###\t3. Choice of hop distance (2 hops) not motivated\n* The tokenization is restricted to 2-hop neighborhoods, but this hyperparameter is not justified. Figure 4 ablates K (number of neighbors), but not the hop radius. Is RELGT sensitive to this parameter?\n\n### 4. Figure 2 clarity:\n* The left part of Figure 2 (“seed nodes with local neighbors”) is confusing to me. It is unclear how the nodes are sampled from the relational entity graph, particularly given the temporal-aware 2-hop sampling strategy, vs the fact that in each line you see 6 (plus …) nodes, where some of them are > 2 hops from the black node. A more explicit visual or step-by-step example would help readers follow the process.\n\n### 5. Experimental infos missing\n* The paper does not mention the random seed setting (fixed?) and the number of repetitions, and variance across runs.\n\n### 6. Potential idea for improvement: Interpretability analysis\n* Given that the model is relatively complex, some insights on interpretability, e.g. examples visualizing attention or token importance, would be very valuable.\n\n## Minor:\n### 7. Figure formatting\n* Figures 2 and 3 have small font sizes, which makes them very hard to read.\n\n### 8. Reproducibility, LLM usage\n* The reproducibility statement and LLM usage are missing.\n\n## Overall comment\nThe paper presents a valuable contribution to relational deep learning. The authors apply the concept of graph transformers, and modify and extend them to tackle the challenging task. The paper is written well, and the evaluations make sense. The ablation studies are good, the structure as well.\nThe paper could benefit from clarifying above questions, e.g. differentiation from TKG, improving the figure, and explaining the motivation for excluding the relation type."}, "questions": {"value": "1. How do the challenges that you mention in section 2.1 fundamentally differ from the challenges for temporal knowledge graphs? Do you agree with my satement at W1?\n2. Why were relation types excluded from the token representation?\n3. How sensitive is RELGT to the 2-hops sampling assumption?\n4. Were experiments repeated with fixed random seeds , and or multiple runs to estimate variance?\n5. Would it be possible to integrate an interpretability analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aKOS7xO0lw", "forum": "2d3j6bt21A", "replyto": "2d3j6bt21A", "signatures": ["ICLR.cc/2026/Conference/Submission22746/Reviewer_Y8Ba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22746/Reviewer_Y8Ba"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761072965208, "cdate": 1761072965208, "tmdate": 1762942367994, "mdate": 1762942367994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Relational Graph Transformer (RelGT), a graph-transformer architecture tailored for relational deep learning (RDL), where multi-table relational databases are represented as heterogeneous temporal relational entity graphs (REGs). The core idea is a multi-element tokenization that decomposes each node into five components: node features, node type, hop distance from a seed node, relative time, and a subgraph positional encoding obtained by a lightweight GNN run on the sampled local subgraph—so that heterogeneity, temporality, and local topology are captured without heavy global precomputation. These tokens feed a hybrid local-global transformer: local attention over a sampled K-hop neighborhood plus global attention to a small set of learnable centroids updated with EMA K-means during training, yielding both neighborhood-level and database-wide context for prediction heads."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The multi-element tokenization is a clean, compositional alternative to “one-shot” global PEs, explicitly encoding heterogeneity, relative structure, and time, plus a lightweight subgraph PE. This is well-motivated for relational graphs and distinct from standard GTs or HGT variants.\n2. Establishing a transformer baseline that consistently competes with or surpasses RDL’s hetero-GNN baseline on RelBench is meaningful; the design choices are broadly applicable in enterprise REG settings and could inform future pretraining/foundation models over relational data."}, "weaknesses": {"value": "See Questions."}, "questions": {"value": "1. Attribution of gains to token elements vs. architectural bias remains partially confounded. Table 2 indicates that removing subgraph PE or the global module reduces accuracy on average, but it is unclear whether RelGT’s advantage primarily comes from injecting structural bias via the GNN-based PE rather than the transformer’s attention. The fairness of comparing to HGT(+PE) without, e.g., GraphGPS-style structural encodings or stronger learned PEs on sampled subgraphs is uncertain.\n2. Have you tried replacing the linear time difference with learnable spatio-temporal PEs?\n3. The paper states it avoids exhaustive tuning; however, there are no multi-seed confidence intervals, paired tests, or time-to-target curves, despite notable per-task variability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "r4WWfaRJnH", "forum": "2d3j6bt21A", "replyto": "2d3j6bt21A", "signatures": ["ICLR.cc/2026/Conference/Submission22746/Reviewer_ksWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22746/Reviewer_ksWe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994643801, "cdate": 1761994643801, "tmdate": 1762942367600, "mdate": 1762942367600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work propose a new graph transformer architecture, RelGT, for RDB data.  By designing new positional encoding, add temporal information, and using structural feature extractor, RelGT achieves strong performance on RelBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Detailed ablation study verify the effectiveness of model. Tables provides ablation results of all model components on all datasets. These results show consistent performance gain of designs in this work.\n2. Careful architecture design. Table 3 provides architecture illustration in detail."}, "weaknesses": {"value": "1. Recent strong RDB baseline[1] is missing. Including it may make its contribution more clear.\n2. RelGT is trained on each dataset separately, with no pretrain and transfer learning experiments, making the architectural design contribution less significant. \n\n[1] Yanbo Wang, et al. Griffin:Towards a graph-centric relational database foundation model. ICML 2025."}, "questions": {"value": "1. In RelGT, each node is tokenized to 5 tokens. Will it leading to significantly larger computation overhead compared with baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cOpWfBsGEc", "forum": "2d3j6bt21A", "replyto": "2d3j6bt21A", "signatures": ["ICLR.cc/2026/Conference/Submission22746/Reviewer_1WrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22746/Reviewer_1WrR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996010009, "cdate": 1761996010009, "tmdate": 1762942367288, "mdate": 1762942367288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}