{"id": "NgGsjZ1G92", "number": 20187, "cdate": 1758303426181, "mdate": 1759896993224, "content": {"title": "POLAR: Policy-based Layerwise Reinforcement Learning for Stealthy Backdoor Attacks in Federated Learning", "abstract": "Federated Learning (FL) enables decentralized model training across multiple clients without exposing local data, but its distributed feature makes it vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying entire models, recent studies have explored the concept of backdoor-critical (BC) layers, which poison the chosen influential layers to maintain stealthiness while achieving high effectiveness. However, existing BC layers approaches rely on rule-based selection without consideration of the interrelations between layers, making them ineffective and prone to detection by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise Reinforcement learning), the first pipeline to creatively adopt RL to solve the BC layer selection problem in layer-wise backdoor attack. Different from other commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR dynamically learns an attack strategy, optimizing layer selection using policy gradient updates based on backdoor success rate (BSR) improvements. To ensure stealthiness, we introduce a regularization constraint that limits the number of modified layers by penalizing large attack footprints. Extensive experiments demonstrate that POLAR outperforms the latest attack methods by up to 40\\% against six state-of-the-art (SOTA) defenses.", "tldr": "", "keywords": ["Federated Learning", "Adversarial Attack", "Reinforcement Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca70d857684a0af2cf2d80b60e42ce8102ade4e8.pdf", "supplementary_material": "/attachment/a80f3800c5653492a7fe97e8d29f55dcf44dafa4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces POLAR, a RL-based framework for stealthy backdoor attacks in FL. The authors find that current LP attacks makes too much modifications across layers rather than focus on only critical layers. Unlike previous rule-based layer selections, POLAR selects and poisons only the most influential \"backdoor-critical\" layers using a policy-gradient approach, optimizing both ASR and stealthiness at the same time. It incorporates a regularization term to limit the number of modified layers, reducing detectability. Experiments show that POLAR significantly outperforms existing attacks, achieving up to 40% higher success rates while evading six state-of-the-art FL defenses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper observes that existing LP attacks makes extensive modifications across the model instead of focusing on critical neurons only within critical layers.\n\nPOLAR achieves strong attack effectiveness against the compared defense methods.\n\nBernoulli sampling is leveraged to reduce the computational cost of the RL-based layer selection.\n\nFig 2 provides a clear and well-designed diagram that effectively illustrates the overall attack workflow and methodology.\n\nThe algorithmic complexity of POLAR is explicitly analyzed and reported in Sec 4.4."}, "weaknesses": {"value": "The presentation requires further improvement. For instance, the definition of stealthiness is unclear. In fact, stealthiness can be considered from multiple perspectives, such as visual stealthiness and parameter stealthiness. Additionally, some critical background information is missing. For example, in Fig 1, specifying the model and dataset used to collect the layer index, explaining how the layer index is obtained, and clarifying the meaning of the light and solid blue dots would help readers better understand how the results are produced.\n\nThe wall-clock time comparison between POLAR and classic or related LP attacks is not reported. Although POLAR is claimed to be lightweight, it likely incurs substantially higher computational costs than rule-based layer selection methods, as the local RL policy optimization and multiple attack success rate evaluations per malicious client introduce considerable overhead, which may be prohibitive for resource-constrained edge devices.\n\nThe sampling-based strategy exploration is inherently random, particularly in early rounds. This can produce anomalous malicious updates (e.g., select too many critical layers) while the RL policy is still unstable, increasing the risk of early detection by server-side anomaly detectors.\n\nThe instability of the RL likely stems from the high variance inherent in policy gradient methods. Although the authors mitigate this issue by increasing the batch size (as shown in tab 5), the method’s sensitivity to batch size still limits its practical applicability.\n\nThe evaluation of POLAR is limited to classical defenses proposed in/before 2021, leaving its robustness against newer, adaptive defenses unverified. Moreover, comparisons focus mainly on BadNets and LP Attack, lacking comprehensive evaluation against recent, stronger attacks such as DBA, AGR, and SDBA. The experiments are further restricted to CIFAR-10 and Fashion-MNIST, without validation on broader benchmarks like ImageNet, CelebA, or CIFAR-100, which undermines POLAR’s generalizability and practical applicability.\n\nThe “backdoor-critical” layers in different architectures (e.g., transformers) may not follow the same patterns captured by this paper. The paper does not provide sufficient evidence that the RL strategy can effectively generalize across diverse model types, and the method may implicitly depend on CNN-specific structural characteristics."}, "questions": {"value": "What is POLAR’s empirical runtime (per round and end-to-end), and how does it scale with dataset size, number of clients, and model depth/width, in particular for large datasets and modern, compute-heavy architectures?\n\nDoes POLAR preserve stealthiness throughout the federated training process (across rounds and model updates)? \n\nHow effective is POLAR, and how accurate is the RL-based layer-selection mechanism, when applied to a wider range of architectures (e.g., ViTs/transformers, ResNet variants, EfficientNet)?\n\nIs POLAR robust and effective across additional datasets and benchmarks (ImageNet, CIFAR-100, CelebA, etc.), and how does its attack success and stealth compare to state-of-the-art attacks and adaptive defenses published in the last three years?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Although the method introduced in this paper has the potential to be harmful, the authors released the source code and demonstrated the weaknesses of their method. Therefore, I don't see the necessity of an ethics review of this paper."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bXEnytVg8C", "forum": "NgGsjZ1G92", "replyto": "NgGsjZ1G92", "signatures": ["ICLR.cc/2026/Conference/Submission20187/Reviewer_jXLX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20187/Reviewer_jXLX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689504804, "cdate": 1761689504804, "tmdate": 1762933696487, "mdate": 1762933696487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces POLAR (POlicy-based LAyerwise Reinforcement learning), a novel framework for backdoor attacks in Federated Learning (FL). The work aims to solve the long-standing trade-off between attack effectiveness and stealthiness. The authors critically analyze the limitations of existing methods: model-wise attacks (like BadNets) are easily detected due to their large footprint, while rule-based layer-wise attacks (like LP Attack) are unstable and generalize poorly because they ignore inter-layer dependencies.\nTo overcome these challenges, POLAR is the first framework to apply Reinforcement Learning (RL) to the layer selection problem in layer-wise backdoor attacks. It formulates this discrete optimization task as a Markov Decision Process (MDP) and employs a lightweight, policy-gradient-based REINFORCE algorithm. The framework uses Bernoulli sampling to efficiently explore the action space and optimizes its policy using a reward signal based on Backdoor Success Rate (BSR) improvements. To ensure high stealthiness, POLAR introduces a regularization term that penalizes a large attack footprint, forcing the agent to select a minimal set of Backdoor-Critical (BC) layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "What makes this work stand out is its novel application of Reinforcement Learning (RL) to solve the discrete layer-selection problem in FL backdoor attacks. This isn't just an incremental improvement; it's a paradigm shift from static rules to an adaptive, learning-based policy. The result is the introduction of a new class of intelligent threats that significantly raises the bar for defensive measures in the field.\n\nThe authors present a sound methodology, highlighted by an elegant reward function that successfully navigates the trade-off between effectiveness and stealth. Their claims are not just theoretical; they are backed by a rigorous and comprehensive set of experiments. Testing the attack against six different SOTA defenses on multiple models and datasets provides compelling evidence for its superiority."}, "weaknesses": {"value": "The paper notes that POLAR's computational complexity is O(K · T · E), which is higher than the baseline LP Attack's O(N · E). While this is still linear, the additional overhead introduced by the RL batch size (K) and training steps (T) could be a practical concern on resource-constrained clients. A more detailed discussion on the trade-off between this increased computational cost and the significant performance gains would be a valuable addition."}, "questions": {"value": "As the ablation study shows, performance is sensitive to hyperparameters like λ. In a real-world FL setting with limited feedback (e.g., only knowing if an update was accepted), how might an attacker effectively tune these parameters? Could the authors offer any insights or potential strategies for this?\nWhile this line of research is crucial for understanding vulnerabilities and developing more robust defenses, the work itself is inherently dual-use. The methodology is detailed enough that it could potentially be misused by malicious actors. I would encourage the authors to add a brief section discussing the ethical implications of their work and the importance of this research for the defensive security community, in line with responsible research practices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ufVsk0Dhd9", "forum": "NgGsjZ1G92", "replyto": "NgGsjZ1G92", "signatures": ["ICLR.cc/2026/Conference/Submission20187/Reviewer_hAZy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20187/Reviewer_hAZy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875716547, "cdate": 1761875716547, "tmdate": 1762933696140, "mdate": 1762933696140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes POLAR, a reinforcement learning–based framework for selecting which layers of a neural network\nshould be poisoned by a malicious client in federated learning (FL). Rather than uniformly injecting backdoor perturbations\nacross all layers, POLAR parameterizes a Bernoulli distribution over layers and trains this policy via REINFORCE using the\nchange in backdoor success rate (BSR) as reward. A regularization term is meant to discourage trivial “all layers or none”\nselections. Experiments on CIFAR-10 with ResNet-18/VGG-19 and Fashion-MNIST with a small CNN demonstrate that\nPOLAR achieves higher attack success across six popular FL defenses while maintaining competitive clean accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear framing of layer selection as a discrete policy problem, which is modular and interpretable.\n- Strong empirical gains against a wide range of defense baselines (FLAME, FLTrust, FLDetector, etc.) — the defense\ncoverage is appreciated.\n- The batch size and regularizer ablations provide some transparency into training behavior.\n- Writing and figures are clear; motivation is easy to grasp."}, "weaknesses": {"value": "- The novelty of this work seems incremental, where RL-based adaptation for poisoning has appeared previously [1,2,3]. The main distinction here is simply the action space being binary per layer, which is a narrow extension. The author should further explain the\nclear novelty of the paper compared to previous works.\n- No evaluation of RL-based defenses. Some recent defenses have been proposed for poisoning attacks in FL with RL\nmechanism [4,5]. The author should evaluate the attack against these defenses.\n- The regularizer term +λ∑ log p_l  encourages p_l -> 0 (i.e., no layers selected), contradicting the stated intent of “discouraging trivial extremes.” It is unclear if this is a typo, or if training relies solely on reward gradients to avoid collapse.\n- No baseline or normalization is described, which makes it surprising that training is stable across K= 50, T= 10\n﻿ samples.\n- Only CIFAR-10 and Fashion-MNIST, and only CNN-style models. No CIFAR-100, Tiny-ImageNet, transformers, or other modalities. This is insufficient to justify broad claims of generalizability.\n- No runtime or scalability analysis, despite claiming the method is “lightweight.” With exceed baseline cost for many models.\nK × T reward evaluations, it may\n- No sensitivity studies on τ (layer threshold), non-IID severity, or trigger type/position, suggesting the method may\nbe brittle beyond the chosen setup.\n\n[1] A Meta-Reinforcement Learning-Based Poisoning Attack Framework Against Federated Learning (Zhou et al., 2025)\n\n[2] Learning to Backdoor Federated Learning (Li et al., 2023)\n\n[3] Learning to Attack Federated Learning: A Model-based Reinforcement Learning Attack Framework (Li et al., 2022)\n\n[4] Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning (Wang et al.,\n2024)\n\n[5] Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks (Li et al., 2024)"}, "questions": {"value": "- Could you clarify the sign and role of the regularizer (﻿+λ ∑log p_l)? As written, minimizing the loss pushes all ﻿p_l toward zero.\n- How do you prevent policy collapse from selecting zero layers? Is the reward strong enough, or was a baseline/advantage normalization used?\n- How sensitive is performance to the threshold τ used to determine final layer selection?\n- Would POLAR work with semantic/backdoor triggers or randomized positions, rather than a fixed square patch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jxtGXho5v9", "forum": "NgGsjZ1G92", "replyto": "NgGsjZ1G92", "signatures": ["ICLR.cc/2026/Conference/Submission20187/Reviewer_MLoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20187/Reviewer_MLoh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877581875, "cdate": 1761877581875, "tmdate": 1762933695793, "mdate": 1762933695793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors studied backdoor attacks in federated learning. They suggested using reinforcement learning to select which layers should be implanted with a backdoor and leveraged policy gradients to solve this optimization problem. The authors conducted some experiments to verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-written.\n\n* Federated backdoor attacks are an important problem."}, "weaknesses": {"value": "* Unclear motivation. I do not understand the motivation behind claiming that model-wise attacks are not stealthy. Although the authors claim these attacks are easily detected by defenses, recent methods [a-c], in fact, can still achieve high attack success rates even when facing SOTA defenses. Similarly, I did not see any relevant results to support this claim.\n\n* Limited novelty and technical depth. In my opinion, the proposed method is just a straightforward application of reinforcement learning. There are many other discrete optimization algorithms available that could have been used here, such as genetic algorithms, etc.\n\n* Insufficient evaluation. There is a lack of empirical evaluation regarding the overhead of the proposed method. The performance of the proposed method should intuitively be highly sensitive to the number of malicious clients, but this is not evaluated.\n\n[a] Stealthy Backdoor Attack in Federated Learning via Adaptive Layer-wise Gradient Alignment\n\n[b] Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning\n\n[c] Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3YwepGOwjR", "forum": "NgGsjZ1G92", "replyto": "NgGsjZ1G92", "signatures": ["ICLR.cc/2026/Conference/Submission20187/Reviewer_ENaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20187/Reviewer_ENaE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762569597475, "cdate": 1762569597475, "tmdate": 1762933695282, "mdate": 1762933695282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}