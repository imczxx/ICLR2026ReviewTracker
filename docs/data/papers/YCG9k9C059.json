{"id": "YCG9k9C059", "number": 18125, "cdate": 1758284155775, "mdate": 1763725937642, "content": {"title": "BIGFIX: BIDIRECTIONAL IMAGE GENERATION WITH TOKEN FIXING", "abstract": "Recent advances in image and video generation have raised significant interest from both academia and industry. A key challenge in this field is improving inference efficiency, as model size and the number of inference steps directly impact the commercial viability of generative models while also posing fundamental scientific challenges. A promising direction involves combining auto-regressive sequential token modeling with multi-token prediction per step, reducing inference time by up to an order of magnitude. However, predicting multiple tokens in parallel can introduce structural inconsistencies due to token incompatibilities, as capturing complex joint dependencies during training remains challenging. Traditionally, once tokens are sampled, there is no mechanism to backtrack and refine erroneous predictions. We propose a method for self-correcting image generation by iteratively refining sampled tokens. We achieve this with a novel training scheme that injects random tokens in the context, improving robustness and enabling token fixing during sampling. Our method preserves the efficiency benefits of parallel token prediction while significantly enhancing generation quality. We evaluate our approach on image generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video generation with UCF-101 and NuScenes, demonstrating substantial improvements across both modalities.", "tldr": "We propose a novel method to enhance multi-token prediction models by introducing a \"token fixing\" mechanism. This mechanism aims to correct structural inconsistencies that arise from predicting multiple tokens in parallel.", "keywords": ["Image Synthesis", "Multi-token prediction", "Self-correction", "low-discrepancy sequence"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e57dcb5bb06e4fc91d859d81dd2feb8af704cd70.pdf", "supplementary_material": "/attachment/ce0cc22d072916acec4ee9d45bb76a425755cfd5.pdf"}, "replies": [{"content": {"summary": {"value": "This submission proposes a fixing mechanism in multi-token prediction AR frameworks for efficient visual generation. The method is first based on a MaskGIT-like AR model with multi-token prediction. During training, some predicted tokens are randomly replaced with random tokens, as a regularization to encourage the model to \"fix\" these corrupted tokens. By doing so, the proposed method requires fewer inference steps to generate comparable or better image quality. The main results on ImageNet 256x256 generation demonstrates the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clearly stated;\n\n2. The proposed method is generally easy to follow;\n\n3. The writing style of the introduction is novel.\n\n4. The model's ability to correct random tokens is demonstrated clearly."}, "weaknesses": {"value": "1. My biggest concern is that this submission has missed some important references (strong baselines). \n    - For example, ZipAR [1] is a training-free multi-token (parallel) AR decoding method. Instead of proposing a model family as this submission, ZipAR can work as a plug-in decoding strategy on various architectures, such as llamagen and Lumina-mGPT. It also reduces the inference steps by >90% with a minor sacrifice of generation quality.\n    - NAR [2] further improves the trade-off between efficiency and generation quality.\n    - I think these methods should be strong baselines to compare.\n\n2. The advantage and contribution of this work are not stated very clearly. Specifically, if the core advantage is the efficiency (fewer inference steps), the detailed metrics should be reported, such as throughput or latency, GPU memory, etc. Moreover, the trade-off should be compared with the aforementioned baselines. \n    - To me, an interesting advantage of this work is its capability to \"fix\" the corrupted tokens. More analysis could be presented. For example, does the method contribute to better robustness?\n\n3. The evaluated benchmarks are generally small: ImageNet 256x256 class-conditioned generation. Is the method adaptable to text-to-image generation models such as Pixart, SD3 or FLUX? How does it perform when generating higher-resolution images?\n\n4. Some representation might be confusing. \n    - For example, where are the blue tokens from in the inference mode in Fig. 2 (bottom)? I understand that they can come from human interference. Without human interference, will the model itself generate erroneous tokens? What is the relationship between the \"fixing\" capability and the fewer-step inference?\n    - The modifications of the model architectures are not clearly stated.\n\n[1] ZipAR: Parallel Autoregressive Image Generation through Spatial Locality.\n\n[2] Neighboring Autoregressive Modeling for Efficient Visual Generation"}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7KPka2sOsk", "forum": "YCG9k9C059", "replyto": "YCG9k9C059", "signatures": ["ICLR.cc/2026/Conference/Submission18125/Reviewer_QMWq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18125/Reviewer_QMWq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558563962, "cdate": 1761558563962, "tmdate": 1762927889883, "mdate": 1762927889883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their detailed feedback, and acknowledgement for the paper's clear problem framing and effective token-fixing mechanism, showing strong results across image and video benchmarks. During the rebuttal, we have strengthened the submission with new experiments, additional baselines, clarified methodology, new tables, figures, and theoretical explanations. Below we highlight the major additions and clarifications, and we leave individual comments for specific questions.\n\n**(A) New baselines and fairer comparisons**\nWe added three Halton-MaskGIT models (S/B/L) trained for 410k steps, using their official implementation, strengthening the direct comparison with our variants at matched compute. This now appears in **revised Table 2**, showing that BIGFix improves significantly over strong Halton scheduling baselines. We also added comparisons with the two parallel/multitoken prediction methods highlighted by reviewers: ZipAR and NAR, both are now discussed in the Related Work (L174–177) and added as baselines in **Table 3**.\n\n**(B) New text-to-image experiments (CC12M + MidJourneyDB)**\nAlthough large-scale T2I training (PixArt, SD3, FLUX) is not feasible within the rebuttal time window, we performed a 7M-image text-to-image experiment for 500k iterations at 437M parameters. Results (**new Table 12**) show consistent improvements with α > 0, confirming generalizability. Our bigger model is still training and results will be updated before the end of the rebuttal period.\n\n**(C) New inference-speed analysis with full tables**\nWe added a complete inference speed table (**new Table 9**) covering 5 model sizes, 4 decoding-step budgets (8/16/24/32), and two resolutions (16×16 and 24×24 latents). These results address all practical speed questions and demonstrate that our correction mechanism does not add overhead for inference.\n\n**(D) Quantitative analysis of token-correction behavior**\nWe added the full table of average tokens corrected per image (Steps × α) in the **revised Table 4**. We also clarify that α is used only during training, not sampling, where we correct errors naturally arising due to multi-token prediction paradigms.\n\n**(E) Scaling-law evidence**\nFigure 7b and **updated Table 2** demonstrate that improvements scale consistently with model size, supporting the feasibility of models with >1B parameters.\n\n**(F) Theoretical grounding of the Fixing correction**\nWe added a paragraph in **appendix H** to clarify the role of the correction mechanism and explain how the noise-injection and fixing steps lead to a better approximation of the true joint distribution."}}, "id": "DHvhRQBfpr", "forum": "YCG9k9C059", "replyto": "YCG9k9C059", "signatures": ["ICLR.cc/2026/Conference/Submission18125/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18125/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18125/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726184574, "cdate": 1763726184574, "tmdate": 1763726184574, "mdate": 1763726184574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focus on structural inconsistencies that arise in generative vision models, and propose BIGFIX, which adds random tokens from the data distribution in the context and trains the model to correct these, thereby giving the model the ability to fix such errors during denoising. The authors present strong fidelity results on a variety of benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is excellently written, with a very clear problem formulation and easy to follow methodology.\n- The results on standard image and video benchmarks are particularly strong, making this approach a useful contribution to the community. Fig 5 is a particularly great result, as it shows that even artificially introduced artifacts can be effectively mitigated using this approach.\n- The token correction approach, as per my knowledge, is significantly novel in this context."}, "weaknesses": {"value": "- The method uses (Besnier et al., 2025) with Halton scheduling, however do not include it as a baseline in the results. This leads to slightly misleading, potentially stronger impression of the results. Some of the gain that we see in the current Table 2 should also be attributed to the already strong baseline.\n- The rate of token injection $\\alpha$ is kept fixed during training, the authors mention that a value upto 0.2 leads to improvements. However, would it help more if the alpha value starts with 0 at the beginning of the training process and is scheduled to increase as the training progresses?\n- It would be helpful to see the training dynamics of this approach, particularly how are the two losses balanced, what are the scale of these losses, convergence rates, etc. The training curves, if attached to the paper, would be helpful in understanding how token injection modifies the overall training objective and progression empirically."}, "questions": {"value": "Please see the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kpmnRwkHIW", "forum": "YCG9k9C059", "replyto": "YCG9k9C059", "signatures": ["ICLR.cc/2026/Conference/Submission18125/Reviewer_YYtR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18125/Reviewer_YYtR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988590950, "cdate": 1761988590950, "tmdate": 1762927889143, "mdate": 1762927889143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BIGFIX which introduces a token fixing strategy into the Bidirectional Halton ordering Multi-token prediction paradigm for image/video generation tasks. The paper is very easy to follow. However, the reviewer thinks that the method of introducing perturbations into the NTP/MTP paradigm to enhance the robustness of method prediction is very common and not sufficient as the sole core contribution of the paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well structured and focuses on one of the most popular topics in autoregression based image generation."}, "weaknesses": {"value": "1. The major weakness is that the contribution and novelty are very limited.\n- Introducing noise for improving NTP/MTP paradigm is a commonly used trick in practical scenarios.\n- The introduction of the proposed method is as few as the PRELIMINARIES parts (as the Bidirectional Halton ordering is formerly introduced).\n- As far as the reviewer's knowledge, SOTA approaches AR-based image/video generation methods show very few structural errors such as supernumerary or missing elements which are illustrated in Figure 1, meaning that the challenges proposed in this paper are somehow out-of-date.\n- Besides, the introduced noise still can not resolve the mentioned challenges \"On token dependencies.\" as the error distribution at the inference phase is not exactly the same as the formulated one (sampled from the original image).\n\n2. The introduction section discussion is aimless, the advantage of autoregression based method is not clearly claimed and multiple advanced methods such as next scale prediction for solving the generating order and reducing the generation steps are not discussed.\n\n3. Missing ablation of different image tokenizers. Why are different tokenizers used for different benchmarks?\n\n4. Out-of-date comparison baseline.  All compared baselines are out-of-date as far as the reviewer's knowledge, comparison with the current sota generation model is needed to convince the effectiveness of the proposed method. (such as NAR, Infinity, etc.)"}, "questions": {"value": "See weakness parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0nFOHObWwG", "forum": "YCG9k9C059", "replyto": "YCG9k9C059", "signatures": ["ICLR.cc/2026/Conference/Submission18125/Reviewer_g51t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18125/Reviewer_g51t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065497646, "cdate": 1762065497646, "tmdate": 1762927888552, "mdate": 1762927888552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem.** Multi-token prediction for image generation is fast but brittle: sampling many tokens in parallel introduces incompatible tokens and structural errors that can’t be corrected once committed. **Method.** *BIGFix* trains with **random token injection** into the context and, at inference, **allows backtracking**—the model “fixes” previously sampled tokens while predicting new ones; token order follows a **Halton** low-discrepancy sequence with an **arccos** scheduling of revealed tokens. **Key innovations.** (i) Context corruption via Eq. (5) and dual losses \\(L_{\\text{context}}+L_{\\text{next}}\\) (Eqs. 6–7); (ii) bidirectional attention with Halton ordering; (iii) explicit self-correction during sampling. **Main results.** On ImageNet-256, the XL variant reports FID 2.49 in 32 steps versus Halton-MaskGIT 3.74; α≈0.2 improves metrics across ImageNet/CIFAR/UCF-101/NuScenes; A100 latency claim ≈0.25 s/image (2.86× vs LlamaGen-XL). **Significance.** If robust and fairly compared, a “token fixing” mechanism could close quality gaps while retaining the step-efficiency of masked/parallel decoders."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear problem framing** (parallel sampling causes incompatibilities); concrete toy/qualitative cases (Fig. 1, Fig. 5).\n- **Simple, general training recipe** (random token injection) with ablations over α and steps (Table 1, Table 4; Fig. 7).\n- **Competitive ImageNet results** in few steps; XL gets FID 2.49 @32 steps; speed claims vs AR step counts are plausible (Table 3; Appendix D)."}, "weaknesses": {"value": "1) **Correctness of the probabilistic story is underspecified.**  \nYou attribute incompatibilities to “independent sampling” of parallel tokens and approximate \\(P(z_a,z_b)\\approx P(z_a)P(z_b)\\) (Sec. 3.2), but also note the model outputs are “not independent.” Please provide a formal derivation of how the training objective (Eqs. 6–7) induces *error detection* and *targeted correction* under the actual joint decoder, and quantify the *false-positive* rate of overwriting correct tokens during self-correction (see p.xx, Fig. 3).\n\n2) **Self-correction mechanism lacks algorithmic clarity.**  \nAt inference you “allow the model to backtrack,” but the rules are unclear: which tokens are eligible for overwrite, by what criterion (confidence, disagreement, local consistency), and with what schedule? Please add pseudocode describing selection, re-masking, and stopping conditions; report average **edits per step** and an *edit confusion matrix* (edit-precision/recall on corrupted vs clean tokens) (Fig. 1–5).\n\n3) **Fairness/confounds in SOTA comparisons.**  \nTable 3 mixes distinct tokenizers (e.g., LlamaGen codebook, VAR multi-scale), step counts, and training durations; some rows use cfg and others “No cfg.” Please re-run **matched-tokenizer, matched-compute** baselines (same tokenizer, same training steps, same classifier-free guidance, same image count for FID) and add wall-clock throughput and p95/p99 latency for sampling (see p.xx, Table 3).\n\n4) **Speed claims need breakdown and variance.**  \nAppendix D states 0.25 s/image on A100 and 2.86× vs vLLM-optimized LlamaGen-XL, but no per-component breakdown (attention vs correction vs I/O) or variability. Provide images/s, latency histograms, and ablate correction disabled/enabled to isolate overhead (Appendix D).\n\n5) **Halton/arccos design is inherited; novelty positioning is weak.**  \nHalton ordering and convex reveal schedules are known to help MaskGIT-style decoders; here they are adopted from the baseline. Please quantify how much of the gain vs Halton-MaskGIT comes from **random-injection only** (fixing disabled) and how much from **correction** (your contribution). Table 3 hints at +0.87 FID from correction; make this central with controlled runs (Sec. A; Table 6; Table 3).\n\n6) **Distribution shift from “same-image” random injection.**  \nEq. (5) injects tokens sampled from the *same image* distribution \\(P(Z)\\), i.e., realistic but misplaced tokens. Does this bias the model toward spatial relabeling rather than semantic error detection? Please compare against **random-class** and **patch-shuffle** injections and report robustness when corruptions are out-of-distribution (e.g., foreign-object tokens) (Eq. 5; Fig. 5).\n\n7) **Statistics are thin; many gains could be within noise.**  \nNo μ±σ across seeds; Table 2/3 improvements over strong baselines are modest in places. Please add ≥3 seeds (FID50k with CI), paired tests, and per-class precision/recall for ImageNet (Tables 2–3).\n\n8) **Scope is narrow (no text-to-image).**  \nAll image experiments are class-conditional; text-conditional is not evaluated. Since token incompatibilities are severe with text prompts, include at least a small-scale T2I study to establish relevance (Sec. 4).\n\n9) **Video results are not competitive and confounded by tokenizer.**  \nUCF-101 FVD (242) is behind MAGVIT-family; authors attribute this to OmniTokenizer. Please (i) show BIGFix with **the same tokenizer** as MAGVIT/MAGVIT2, and (ii) report edit statistics on videos (how many tokens corrected per frame) (Table 9; §E).\n\n10) **Reproducibility: configs, seeds, and code.**  \nHyper-parameters are summarized (Table 8), but code, exact Halton sequences, α scheduling, and evaluation scripts are needed for verification; release minimal artifacts to reproduce Table 3 on a single GPU (Table 8).\n\n11) **Known limitations affect generality.**  \nYou acknowledge fixed unmasking schedules and <1B-param scale limits. Please quantify quality loss when varying the reveal schedule at test time, and provide a path to larger-scale models (Sec. G)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sIPsEM5h1B", "forum": "YCG9k9C059", "replyto": "YCG9k9C059", "signatures": ["ICLR.cc/2026/Conference/Submission18125/Reviewer_LeAz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18125/Reviewer_LeAz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079147680, "cdate": 1762079147680, "tmdate": 1762927888113, "mdate": 1762927888113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BIGFix, self-correcting image generation by iteratively refining sampled tokens. It addresses a key limitation in masked generative transformers (e.g., MaskGIT) by injecting random tokens during training to teach the model to recognize and fix structural inconsistencies at inference time. The authors demonstrate that this “token fixing” strategy improves both visual fidelity and robustness while maintaining parallel generation efficiency. Experiments on ImageNet-256, CIFAR-10, UCF-101, and NuScenes show lower FID/FVD scores and faster convergence compared with diffusion, flow-matching, and autoregressive baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Introducing self-correction into multi-token image generation is a meaningful contribution.\n- BIGFix could be integrated with existing token-based architectures (e.g., MaskGIT, LlamaGen, OmniTokenizer) without network modification, diffusion steps, or extra fine-tuning stages. This means the proposed method improves quality and robustness without increasing model size or inference cost.\n-Provide extensive experiments spanning both images (ImageNet-256, CIFAR-10) and videos (UCF-101, NuScenes), and a systematic ablation study on α (token injection rate), number of steps, scheduling strategy, and token ordering"}, "weaknesses": {"value": "### Major:\n- It is unclear to me how this work differs from previous works, given that the random token injection resembles existing denoising or noise-robust training schemes. A more formal analysis of why token fixing improves sample consistency would be better.\n\n- The independence assumption Eq1. P(za, zb) \\approx P(za)P(zb) is qualitatively discussed but not rigorously explored. There is no analysis of joint token entropy or mutual information to validate the rationale for the correction.\n\n- Scalability seems limited.  Experiments are mostly run with models that have fewer than 1 billion parameters and often reuse open-weight tokenizers. Claims about generality and large-scale lack evidence from larger models or text-to-image tasks.\n\n### Minor:\n- No failure case analysis. Failure modes (e.g., over-correction, semantic drift) are not analyzed. \n\n- Comparison to recent Rectified Flow and MaskGIT-v2 results is outdated; the absence of open-weight diffusion transformers (e.g., PixArt-Σ, SD3) limits the context of “state-of-the-art” claims.\n\n### Others\n- The writing style of the paper is non-academic, especially in the introduction and conclusion sections. It is recommended to rewrite the introduction in a conventional scientific exposition style to better align with academic requests."}, "questions": {"value": "- The “token fixing” mechanism relies on random token injection during training to encourage self-correction, but the paper does not provide a theoretical justification of why this improves joint token consistency. Please provide an analysis.\n- The paper mentions that the model “corrects 58 tokens per image on average.”(Line377) Could the authors quantify the effect of the correction on global FID? For instance, does the number of corrected tokens correlate with perceptual improvement?\n- While Random Token Injection α (Line317-32e) controls token corruption during training, does α also affect correction aggressiveness during inference?Would dynamically adjusting α (annealing or adaptive) further improve generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HLvkskAWuV", "forum": "YCG9k9C059", "replyto": "YCG9k9C059", "signatures": ["ICLR.cc/2026/Conference/Submission18125/Reviewer_chfx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18125/Reviewer_chfx"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144955613, "cdate": 1762144955613, "tmdate": 1762927887634, "mdate": 1762927887634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}