{"id": "ihlRtPqPDL", "number": 24420, "cdate": 1758356716513, "mdate": 1759896767280, "content": {"title": "CTDG-SSM: Continuous-time Dynamic Graph State Space Models for Long Range Propagation", "abstract": "Continuous-time dynamic graphs (CTDGs) provide a richer framework to capture fine-grained temporal patterns in evolving relational data. Long-range information propagation is a key challenge in learning representations for CTDGs, wherein it is important to retain and update information over long temporal horizons. Existing approaches restrict models to capture one-hop or local temporal neighborhoods and fail to capture multi-hop or global structural patterns. To mitigate limitations of the current approaches, we derive the state-space modelling framework for continuous-time dynamic graphs $\\texttt{(CTDG-SSM)}$ from first principles. We first introduce continuous-time Topology-Aware HiPPO ($\\texttt{CTT-HiPPO}$), a novel memory-based reformulation of HiPPO to jointly encode temporal dynamics and graph structure, where solution for memory representations from $\\texttt{CTT-HiPPO}$ are obtained by projecting the classical HiPPO solution through a polynomial of the Laplacian matrix, yielding topology-aware memory updates that admit an equivalent state-space formulation for CTDGs ($\\texttt{CTDG-SSM}$). This is then discretized (e.g., using the zero-order hold method) for practical implementation. We further provide theoretical guarantees demonstrating the robustness of memory representations under graph structure perturbations. Across benchmarks on dynamic link prediction, dynamic node classification, and sequence classification, $\\texttt{CTDG-SSM}$ achieves state-of-the-art performance. \nNotably, it achieves large performance gains on dynamic link prediction and sequence classification tasks, specifically on datasets that require LRT and spatial reasoning.", "tldr": "", "keywords": ["Continuous time dynamic graphs", "State space models", "Long range propogation", "Higher-order polynomial projection operator"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82c10965f66328c99b59ca7904ebc0380f6983fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work considers the graph (representation) learning on continuous-time dynamic graphs (CTDGs). Aiming to capture both long-range temporal and spatial dependencies, a new CTDG-SSM (continuous-time dynamic graph state space model) method was then proposed with theoretical guarantees and high scalability, based on some designs of HiPPO-based memory mechanism for CTDGs and ZOH discretization of SSM. Evaluation on public benchmarks of 3 tasks (i.e., dynamic link prediction, dynamic node classification, and sequence classification) demonstrate the effectiveness of CTDG-SSM."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**S1**. This work provide a series of rigorous theoretical analysis for the proposed method.\n\n**S2**. The proposed method was evaluated on 3 tasks (i.e., dynamic link prediction, dynamic node classification, and sequence classification) with different purposes.\n\n**S3**. This work anonymously provides its code to ensure reproducibility of experiments."}, "weaknesses": {"value": "**W1**. The overall presentation of this paper is hard to read, which need significant improvement.\n\nSome abbreviations (e.g., HiPPO, LRT, and RMS) were first used without giving their full names.\n\nIn lines 43-44, the possible applications of CTDGs (e.g., finance, e-commerce, and social network analysis) were described without giving any citations.\n\nThe main paper contains many lengthy paragraphs (especially Section 1 and Section 2), which are hard to read and understand. It is suggested to split them into shorter sub-paragraphs and summarize some key conclusions in tables/figures.\n\n\n***\n**W2**. There are several unclear statements with weak motivations, which need further clarification.\n\nFor the problem statement in Section 3, the availability of graph attributes (e.g., node and edge attributes) were not clearly stated. Most related methods have optional inputs for both node and edge attributes. It is also unclear for CTDG-SSM how to incorporate these attributes combined with the induced subgraph adjacency matrix ${\\bf{A}}_{\\tau}$ and Laplacian matrix ${\\bf{L}}_{\\tau}$.\n\nAccording to the problem statement in Section 3, it seems that the subgraph adjacency matrix and the proposed method can only handle the addition of new edges and nodes but cannot tackle the deletion of them. Such a limitation should be clearly stated in the main paper.\n\nIn lines 184-186, the quadratic Laplacian regularizer and classic HiPPO formulation have the same definition (i.e., $p({\\bf{L}}_{\\tau}) = {\\bf{I}}$), which seem to be inconsistent.\n\nIn lines 194-195, it was claimed that $p({\\bf{L})}^{-1}$ is well-defined, but its formal definition (i.e., how to derive $p({\\bf{L})}^{-1}$) was not given.\n\nThe toy example in Fig. 2 cannot fully demonstrate the overall subgraph sampling procedures described in lines 266-293. As a result, it is hard to understand how the proposed method exactly work by just reading the lengthy text descriptions.\n\nIn Section 5, some necessary details about how to train the proposed model (e.g., training loss, training algorithm, optimizer, etc.) are missing.\n\n\n***\n**W3**. There is no pseudo-code to summarize the overall training and inference procedures of CTDG-SSM. As a result, it is hard to check some details about the proposed method.\n\n\n***\n**W4**. While high efficiency and scalability is one of the highlighted advantages of CTDG-SSM, there is no formal analysis about the (space and time) complexity of CTDG-SSM as well as the comparison with complexities of other baselines, which can theoretically validate this advantage.\n\n\n***\n**W5**. Current experiment setups may not fully validate the effectiveness of CTDG-SSM. Some related details are also missing.\n\nThere are no descriptions about the experiment environments.\n\nAs summarized in Table 4, all the datasets cannot be considered as large-scale dynamic graphs in terms of the number of nodes, which cannot fully verify the high efficiency and scalability of CTDG-SSM. It is suggested to include results from some larger public benchmarks (e.g., TGB).\n\nFor details of datasets summarized in Table 4, there is no information about the node classification task (e.g., the number of classes).\n\nIt is still unclear why the 3 evaluation tasks (e.g., dynamic link prediction, dynamic node classification, and sequence classification) can measure the ability to capture LRS and LRT as stated in the main paper, i.e., due to what mechanisms? It is also unclear what does the toy example in Fig. 4 mean by just reading the short description in lines 429-431.\n\nAs efficiency is usually highly related to scalability, it is also suggested to compare CTDG-SSM's training and inference time with other baselines.\n\n\n***\n**W6**. There are no discussions about the limitations of this work and possible solutions as future research directions."}, "questions": {"value": "See **W1**-**W6**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AxmofdlhIA", "forum": "ihlRtPqPDL", "replyto": "ihlRtPqPDL", "signatures": ["ICLR.cc/2026/Conference/Submission24420/Reviewer_2WD4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24420/Reviewer_2WD4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621046405, "cdate": 1761621046405, "tmdate": 1762943077039, "mdate": 1762943077039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work jointly models temporal dynamics and graph structure for dynamic graphs. It integrates the Mamba state-space architecture with HiPPO-based memory to compress historical information, enabling long-term and long-range sequence modeling. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces high-order topological information into dynamic graph representation learning and achieves strong performance on long-sequence classification tasks.\n\n2. This paper extends the SSM framework to dynamic graph modeling with a solid theoretical foundation.\n\n3. This paper conducts extensive experiments, showing strong results on dynamic link prediction, dynamic node classification, and long-sequence classification."}, "weaknesses": {"value": "1. Time complexity is our primary concern. For example, matrix inversion costs O(N^3); each batch requires constructing/updating the Laplacian; and a K-order polynomial filter entails K matrix multiplications. The paper does not analyze time complexity or provide comparative runtime experiments. We believe the computational cost is substantial and may hinder real-world deployment. Moreover, the absence of experiments on large-scale graphs further undermines confidence in practical applicability.\n\n2. The proposed HiPPO matrix appears very close to JinTang Li et al. (NeurIPS 2024), seemingly as a direct extension to dynamic graphs. In addition, the adopted Mamba structure looks like a straightforward application of Mamba, without a detailed comparison to existing Mamba-based frameworks. This weakens the claimed architectural novelty.\n\n3. The paper lacks essential ablations. For instance: How do we know the proposed high-order graph filters are effective? How do we verify that the model truly handles and benefits from long-range dependencies? How is robustness demonstrated?\n\n4. The dynamic link prediction benchmarks are selectively chosen. Common datasets such as Flights, Can. Parl., US Legis., UN Trade, UN Vote, and Contact are missing. Comparisons with the latest baselines are also absent—for example: [1] DyG-Mamba: Continuous State Space Model for Dynamic Graphs; [2] Towards Better Evaluation for Dynamic Link Prediction; [3] FreeDyG: Frequency-Enhanced Continuous-Time Dynamic Graph Model for Link Prediction.\n\n5. There is no clear investigation of input sequence length or of higher-order graph structure. It remains unclear whether gains come from long-sequence modeling or from high-order structural information. More experiments are needed to disentangle these factors. Prior studies (e.g., GraphMixer) have reported that longer sequences may not help, which the authors should address explicitly.\n\n6. The results on MOOC are surprisingly high, and our reproduction raises a potential data-leakage issue. Specifically, the code calls ssm_utlis.py::get_delta_t(...) with the default parameter default=1e+11. This may leak information for negative samples. The authors should clarify this choice and whether it preserves fairness with DyGLib and other baselines."}, "questions": {"value": "Please refer to the above-mentioned weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6nGQcLfLgv", "forum": "ihlRtPqPDL", "replyto": "ihlRtPqPDL", "signatures": ["ICLR.cc/2026/Conference/Submission24420/Reviewer_Ks8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24420/Reviewer_Ks8e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732315352, "cdate": 1761732315352, "tmdate": 1762943076817, "mdate": 1762943076817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new state-space modeling framework for continuous-time dynamic graphs that jointly captures temporal and structural dependencies. It proposes CTT-HiPPO, a topology-aware memory formulation that projects classical HiPPO solutions through polynomials of the graph Laplacian to encode both temporal evolution and multi-hop structural context, leading to the unified CTDG-SSM formulation. The framework is theoretically grounded with robustness and permutation-equivariance guarantees, discretized for efficient implementation, and achieves state-of-the-art performance on link prediction, node classification, and sequence classification tasks that require long-range temporal and spatial reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well written with strong theoretical support.\n2. The proposed method shows strong results and can capture long range temporal and structural dependencies."}, "weaknesses": {"value": "1. The contribution is somewhat limited. Authors start from DyGMamba and propose an advanced version of SSM-based temporal graph reasoning model. The modification of HiPPO is a good point, but the authors didn’t explain why they wanted to develop their method based on SSM and which characteristics drive them build method on top of it. For other modules like residual connection and memory components, these are for me just some combination of common practices in model design.\n2. Lack of important experiments. I really wish to empirically see which part of the model enables long range dependencies being effectively captured. Currently this is not shown clearly. It would be better to have related experiments and detailed analysis."}, "questions": {"value": "1. For kth order filter, is $L_\\tau^k$ the laplacian matrix for k hop neighbor? Or is it the kth order of $L_\\tau$?\n2. Do you think sequence classification is way too artificial? Could you share where can sequence classification, i.e., preserving bode label, be critical in real world applications? \n3. Do you think the long range dependency of your model comes from the memory module rather than the your design of SSM? Lets say, even if you employ multihop and temporal-aware graph filters, it is still not guaranteed that the model would remember LRT and LRS. Could you provide some kind of analysis to show that the contribution actually comes from your SSM design? This is very important in determining the quality of the proposed method.\n4. I saw model size performance comparison. What about inference time and training time/convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JFASL0veRU", "forum": "ihlRtPqPDL", "replyto": "ihlRtPqPDL", "signatures": ["ICLR.cc/2026/Conference/Submission24420/Reviewer_Dwin"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24420/Reviewer_Dwin"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806198144, "cdate": 1761806198144, "tmdate": 1762943076515, "mdate": 1762943076515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CTDG-SSM, a state-space model formulation for continuous-time dynamic graphs (CTDGs). The method builds on HiPPO to derive topology-aware memory representations (CTT-HiPPO), where a polynomial of the graph Laplacian is used to incorporate structural information. A continuous-time formulation is discretized via zero-order hold and evaluated on link prediction, node classification, and a sequence classification benchmark. Results indicate strong performance using a modest number of parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Addresses a relevant problem: maintaining both long-range temporal and long-range spatial dependencies in CTDGs.\n\n* Empirical performance appears strong on benchmarks requiring long-range propagation.\n\n* Architecture is lightweight in parameter size compared to SOTA CTDG methods."}, "weaknesses": {"value": "* **Novelty**. I find novelty relatively limited. Like GraphSSM, CTDG-SSM extends HiPPO to temporal graphs — however the former cannot be directly applied to CTDGs.\n\n* **Imprecise account of prior literature**. Authors provide an imprecise account of prior literature. For instance, the categorization of methods around line 49 is not faithful to CAW — this method does not even have a notion of explicit node embeddings. Regarding CTDGs vs. DTDGs, it is possible to draw equivalences between both — c.f., Prop. 1. of [1].\n\n* **Matrix inverses and numerical stability**. It is not clear to me why $p(L_{\\tau})^{-1}$ should exist. The repeated use of matrix inverses also makes me wonder if there is some numerical stability worth disclosing — and how it affects runtime. A brief discussion of how invertibility is ensured, and whether any numerical stability considerations are necessary in practice, would improve clarity.\n\n* **Permutation equivariance**. It is not clear to me that the proposed architecture really is permutation equivariant. It seems Theorem 6.2 ignores the stochasticity in subgraph sampling, which breaks exact permutation equivariance. Clarifying how this affects the theoretical property would be helpful.\n\n* **Efficiency claims**. Efficiency claims are not convincingly supported: only parameter counts are compared, while no runtime, memory-usage, or throughput experiments are provided. Given the use of Laplacian-polynomial inverses and evolving graph operators, the practical computational cost is unclear. Reporting wall-clock training time, inference latency, or events-per-second versus DyGmamba and DyGFormer would make the efficiency argument more credible.\n\n\n[1] Provably expressive temporal graph networks, NeurIPS 2022"}, "questions": {"value": "My questions are directly aligned with the points raised in the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nwo70KQYpu", "forum": "ihlRtPqPDL", "replyto": "ihlRtPqPDL", "signatures": ["ICLR.cc/2026/Conference/Submission24420/Reviewer_qcoL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24420/Reviewer_qcoL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050520076, "cdate": 1762050520076, "tmdate": 1762943075803, "mdate": 1762943075803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}