{"id": "rF8iG1QW7Y", "number": 3641, "cdate": 1757490436743, "mdate": 1759898077152, "content": {"title": "WatermarkLab: A Comprehensive Framework for Robust Image Watermarks Benchmarking and Development", "abstract": "With the growing demand for multimedia content, image protection has become increasingly important. Robust image watermarking, as a core technology for copyright protection, has attracted extensive attention. To advance research in this field, we propose WatermarkLab, a comprehensive framework for systematic benchmarking of robust image watermarks and the development of new methods. WatermarkLab supports benchmarking of all types of blind robust image watermarks, including in-generation watermarks and post-generation watermarks. Beyond benchmarking, WatermarkLab integrates 10 representative watermarking methods for systematic comparison. It also includes 34 attackers for benchmarking and 28 differentiable attackers for development. Furthermore, we evaluate the robustness of 9 watermarking methods under 34 attackers and give their weaknesses, assisting researchers in enhancing more robust watermarking methods and designing new watermark removal attackers. In addition, the framework provides auxiliary tools such as arithmetic coding and reversible data hiding commonly used in robust reversible watermarking. For result visualization, WatermarkLab offers comprehensive visualization tools and an interactive website, enabling researchers to intuitively analyze and compare benchmarking results. In summary, WatermarkLab is a powerful framework, aiming to establish a comprehensive, fair, open, and extensive platform for blind robust image watermark benchmarking and development. Interactive visualization website code is available at: https://anonymous.4open.science/r/watermarklab-website.", "tldr": "", "keywords": ["robust image watermarking", "benchmark;"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fba218f5fb4c7093cd5d29aff5152db60746828.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents WatermarkLab, a comprehensive framework for benchmarking and developing robust image watermarking methods. The framework supports both in-generation watermarking (IGW) and post-generation watermarking (PGW), integrating 10 representative watermarking methods, 34 test attackers for evaluation, and 28 differentiable attackers for adversarial training. The authors conduct extensive experiments evaluating 9 watermarking schemes under 34 attacks, finding that GaussianShading and StegaStamp remain the most robust methods for IGW and PGW respectively, though both exhibit vulnerabilities to geometric distortions and regeneration attacks. The framework provides visualization tools and an interactive website for result analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The framework integrates diverse watermarking paradigms (zero-bit, multi-bit, IGW, PGW, robust reversible) with extensive attack coverage spanning compression, color transformations, geometric distortions, noise, diffusion-based regeneration, and filtering.\n\n- The introduction of TPR@x%FPR as a metric that applies to both zero-bit and multi-bit watermarks, while the RQ-AUC metric (area under TPR@x%FPR vs. PSNR curve) provides a principled way to quantify robustness-quality tradeoffs across different attack types.\n\n- The inclusion of 28 differentiable attackers for adversarial training, auxiliary tools (arithmetic coding, reversible data hiding), and modular architecture with clear base classes facilitates development of new watermarking methods beyond pure benchmarking."}, "weaknesses": {"value": "- The paper lacks theoretical justification for why IGW methods fundamentally outperform PGW methods (GaussianShading achieves 1013.98 cumulative RQ-AUC vs. StegaStamp's 899.99). While the authors attribute this to \"stronger implicit correlation with image content,\" no formal analysis or information-theoretic treatment is provided to explain the performance gap.\n\n- The framework provides extensive empirical comparisons but limited guidance on architectural choices, e.g., why certain encoder-decoder designs work better, how watermark capacity affects robustness-quality tradeoffs, or optimal strategies for combining differentiable attackers during training.\nAll experiments use MS-COCO 2017 with limited exploration of domain generalization. No evaluation on diverse content types (medical images, satellite imagery, artistic styles) or resolution variations that would stress-test real-world applicability of these methods.\n\n- No reporting of training time, inference latency, memory requirements, or model sizes across the 10 integrated methods. This omission is critical for practitioners choosing methods for deployment, especially for real-time applications or resource-constrained environments.\n\n- Despite claiming comprehensiveness, the framework omits several important attack categories: adversarial perturbations specifically targeting watermark removal, watermark overwriting attacks, deep learning-based inpainting attacks beyond diffusion models, and physical-world attacks beyond print-capture (e.g., projector-camera distortions, varying illumination).\n\n- While the paper identifies vulnerabilities (e.g., geometric distortions for GaussianShading), it provides insufficient investigation into why these failures occur or potential mitigation strategies. The surprising finding that VINE fails on simple crops but resists deep editing deserves deeper mechanistic analysis."}, "questions": {"value": "- Can the authors provide information-theoretic analysis or capacity-distortion bounds explaining why IGW methods fundamentally outperform PGW methods? What is the theoretical limit on robustness achievable by PGW methods that embed watermarks as post-hoc perturbations versus IGW methods that integrate watermarks during generation?\n\n- How do these watermarking methods perform across different image domains (medical, satellite, artistic) and resolutions? Have you evaluated domain shift scenarios where models trained on natural images (MS-COCO) are applied to specialized domains, and what is the degradation in robustness?\n\n- How robust are these methods against adaptive attacks where an adversary has white-box access to the watermark detector? Have you evaluated attacks that specifically optimize to maximize watermark removal while minimizing perceptual distortion, rather than using predefined transformations?\n\n- What are the computational costs (training time, inference latency, memory) for each method? For practitioners deploying watermarking at scale (e.g., protecting millions of generated images), which methods offer the best robustness-efficiency tradeoff?\n\n- Can deeper analysis be provided of why VINE exhibits such different behavior for crop versus cropout attacks (TPR drops to 0.01 with 10% boundary crop but remains robust to 90% cropout)? What does this reveal about watermark embedding strategies, and could this insight inform better method design?\n\n- How about combining multiple watermarking methods (e.g., both IGW and PGW) to achieve complementary robustness properties? Could ensemble approaches mitigate individual method vulnerabilities while maintaining imperceptibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "00wKhWEVu5", "forum": "rF8iG1QW7Y", "replyto": "rF8iG1QW7Y", "signatures": ["ICLR.cc/2026/Conference/Submission3641/Reviewer_cXoi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3641/Reviewer_cXoi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760733227313, "cdate": 1760733227313, "tmdate": 1762916894434, "mdate": 1762916894434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce a new benchmark for evaluating image watermarks. To ensure the benchmark is comprehensive, fair, open, and extensible, it incorporates 10 representative watermarking methods, covering both in-generation and post-generation techniques. To assess and improve watermark robustness, the benchmark further includes 34 attack methods for evaluation and 28 differentiable attacks for training. Finally, it provides a comprehensive visualization toolkit to facilitate clearer and more informative analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-written, and I appreciate the authors’ effort in integrating a wide range of watermarking and attack methods into a single benchmark. The evaluation is notably more comprehensive than in prior work, as it encompasses a greater variety of both watermarking and attack techniques."}, "weaknesses": {"value": "- My main concern lies in the contribution of this paper. The proposed benchmark appears to be highly similar to WAVES, and the work seems more like an extension of WAVES through the inclusion of additional watermarking and attack methods.\n- The paper emphasizes that the proposed benchmark is comprehensive, fair, open, and extensible. While I agree that it is indeed more comprehensive, the meanings of “fair” and “open” are not clearly justified. For instance, aren’t benchmarks like WAVES also fair and open? Additionally, the claim of being extensible could be a valuable contribution, but the paper does not elaborate on the specific design choices that enable extensibility. It would be helpful for the authors to discuss these aspects in more detail and consider sharing pseudocode or the full benchmark code, as the current website link appears to include only the visualization component.\n- The diffusion model used in the paper, Stable Diffusion v2.1-Base, is somewhat outdated compared to more recent models."}, "questions": {"value": "- How flexible is the benchmark in incorporating new watermarking methods? Would it be straightforward to add them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yAnziRkD2E", "forum": "rF8iG1QW7Y", "replyto": "rF8iG1QW7Y", "signatures": ["ICLR.cc/2026/Conference/Submission3641/Reviewer_BR4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3641/Reviewer_BR4i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342004407, "cdate": 1761342004407, "tmdate": 1762916894023, "mdate": 1762916894023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a new framework for watermark benchmarking. It covers 10 watermarking methods, including post-generation watermarking (PGW) and in-generation watermarking (IGW), and 34 attackers. The system integrates evaluation metrics, datasets, and visualization tools, enabling reproducible and fair comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework follows the previous works and includes abundant of watermarking and attacking methods.\n2. The framework is extendable for the new watermarking method benchmarking in the future, which could be a key contribution for the research community of image watermarking."}, "weaknesses": {"value": "1. Although the paper shows abundant of experimental results, it lacks of clear takeaway messages. All observations for different watermarks are stacked in several paragraphs, which is hard for the readers to get the key information. Moreover, high-level takeaway messages will be more inspiring to the researchers compared to the plain description of benchmarking results.\n2. The necessary explanation for RQ-AUC are missed in the main body. Without any explanation, the readers will be confused with this new metric when they start to read the section about evaluation but have not reached the appendix. \n3. The authors should explain why they choose PSNR as the quality metric in RQ-AUC. Using other quality metric such as FID or SSIM, will the overall ranking shown in Figure 8 change?\n4. The authors highlighted the fair comparison as one of the main contribution of the proposed framework. But the analysis and explanation about the fairness of the comparison is missed in the paper. I think the discussion about how to design a fair comparison will be very insightful for the community.\n5. The paper distinguished robust reversible watermarks such as CRMark with other PGWs. But based on my understanding it is still a special type of PGW. I cannot find any experimental results about this special watermarking. Since the paper highlight this special category of watermarking, the relevant experiments and analysis may be necessary to enable readers to recognize its value.\n\nI am glad to adjust my score if the authors can address these questions."}, "questions": {"value": "1. Typo: The index of last section \"conclusion\" is missed.\n2. Why did you regard DctDwt as one of the\"10 representative watermarking methods\" but ignore this method in the evaluation (where only 9 methods are shown)?\n3. The font size in Figure 7 and 8 is too small to read, especially for the watermark names on x-axis.\n4. For each type of attacking, could there be a \"sweet point\" of strength balancing image quality and the attacking performance? For some strengths degrading the image quality a lot or not successfully attacking the watermark, it does not make sense to apply them in the real world application. But theses strengths seems to be also included in RQ-AUC. So maybe RQ-AUC is not able to show the attacking/robust performance at such a \"sweet point\" of attacking strength?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qLlvzqIox0", "forum": "rF8iG1QW7Y", "replyto": "rF8iG1QW7Y", "signatures": ["ICLR.cc/2026/Conference/Submission3641/Reviewer_7B2h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3641/Reviewer_7B2h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751982535, "cdate": 1761751982535, "tmdate": 1762916893757, "mdate": 1762916893757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents watermarklab, a comprehensive benchmarking framework for robust image watermarks. It supports all types of blind image watermarking algorithms. For benchmarking, it incorporates six categories of attack methods. And to demonstrate the results, in total nine watermarking schemes are tested in this paper to demonsrate the challenge in the proposed benchmark. This will be a good tool for helping the community evaluating and developing new image watermarking methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.[comprehensiveness] The proposed method covers a wide range of image watermarking algorithms and attack methods. Lots of state-of-the-art watermarking methods are benchmarked in this paper as well.\n2.As discussed in the paper, the benchmark is challenging, and I agree it can help the community develop better image watermarking methods that are more robust.\n3.[related work] This paper presents a very comprehensive overview of related works. As a benchmark paper, this is necessary."}, "weaknesses": {"value": "1.[typesetting] Please use \"\\citep\" and \"\\citet\" instead of the plain \"\\cite\". The text in this draft is totally messed up wherever there is a citation. This could have been avoided if the authors had carefully read the draft.\n\n2.[design] In real world internet, images are usually lossy-compressed in different extents. For instance, jpeg quality 70% may affect some non-robust watermarking methods. Does the proposed benchmark involve the consideration on how the images in real world are transmitted?\n\n3.[new knowledge] This is a hard work -- I appreciate it. But I'm not seeing enough new knowledge from the huge amount of experimental observations. Namely, what do we learn from the comprehensive benchmark results? The knowledge is not something like \"XXX method works best in our benchmark\", but something like \"XXX design pattern shows consistent robustness across different methods\", or \"XXX design pattern has some potential problem\". Take one step further, this paper could have contributed much more to the community, than a plain observation, tool, and numbers."}, "questions": {"value": "Please see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CoxlelMeNF", "forum": "rF8iG1QW7Y", "replyto": "rF8iG1QW7Y", "signatures": ["ICLR.cc/2026/Conference/Submission3641/Reviewer_ay64"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3641/Reviewer_ay64"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859151819, "cdate": 1761859151819, "tmdate": 1762916893364, "mdate": 1762916893364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}