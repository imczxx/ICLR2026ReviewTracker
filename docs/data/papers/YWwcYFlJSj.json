{"id": "YWwcYFlJSj", "number": 16071, "cdate": 1758259441205, "mdate": 1759897263895, "content": {"title": "Distributed Algorithm for Multi-objective Multi-agent Reinforcement Learning", "abstract": "Multi-objective reinforcement learning (MORL) aims to optimize multiple conflicting objectives for a single agent, where finding Pareto-optimal solutions is NP-hard and existing algorithms are often centralized with high computational complexity, limiting their practical applicability.\nMulti-objective multi-agent reinforcement learning (MOMARL) extends MORL to multiple agents, which not only increases computational complexity exponentially due to the global state-action space, but also introduces communication challenges, as agents cannot continuously communicate with a central coordinator in large-scale scenarios.\nThis necessitates distributed algorithm, where each agent relies only on the information of its neighbors within a limited range rather than depending on the global scale.\nTo address these challenges, we propose a distributed MOMARL algorithm in which each agent leverages only the state of its $\\kappa$-hop neighbors and locally adjusts the weights of multiple objectives through a consensus protocol.\nWe introduce an approximated policy gradient that reduces the dependency on global actions and a linear function approximation that limits the state space to local neighborhoods.\nEach agent $i$'s computational complexity is thus reduced from $\\mathcal{O}(|\\mathbf{\\mathcal{S}}||\\mathbf{\\mathcal{A}}|)$ with global state-action space in centralized algorithms to $\\mathcal{O}(|\\mathcal{S}\\_{\\mathcal{N}^{\\kappa}\\_{i}}||\\mathcal{A}\\_{i}|)$ with $\\kappa$-neighborhood state and local action space. \nWe prove that the algorithm converges to a Pareto-stationary solution at a rate of $\\mathcal{O}(1/T)$ and demonstrate in simulations for robot path planning that our approach achieves higher multi-objective values than state-of-the-art method.", "tldr": "", "keywords": ["Multi-objective multi-agent systems", "fully distributed reinforcement learning", "pareto-stationary convergence."], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5575edb07aecb6cd1726a74f6ea7954f52a1c2e6.pdf", "supplementary_material": "/attachment/2f4a38d07f0cfd4b3b3c0707a34edec5aa9d4513.pdf"}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenges of high computational complexity and limited communication in large-scale distributed multi-objective multi-agent reinforcement learning (MOMARL) by proposing a distributed algorithm. The approach introduces an approximate policy gradient and linear function approximation based on local neighborhood information to effectively reduce the state–action dimensionality, and employs a consensus-based protocol to adaptively adjust multi-objective weights, enabling convergence to an approximate Pareto-equilibrium solution using only local communication. Theoretical analysis proves that the algorithm achieves an O(1/T) convergence rate, and experiments on a multi-robot path planning task validate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The problem addressed in this paper is clearly defined, and the motivation is well articulated.\n2. A distributed and scalable algorithm is proposed under the MOMARL framework, demonstrating originality.\n3. The algorithm design is reasonable, and the overall description is clear."}, "weaknesses": {"value": "1. The literature review is insufficient: the introduction provides an inadequate discussion of existing MOMARL and distributed RL algorithms, lacking direct comparison with representative works; moreover, the references are relatively outdated and the related work section is missing.\n\n2. The experimental setup is not clearly described, with insufficient details regarding the design of the neighborhood network and other implementation aspects; it is recommended to include comparisons with baseline methods from recent MOMARL studies."}, "questions": {"value": "1. The paper does not review relevant studies on fully decentralized multi-agent reinforcement learning (MARL). Excluding the multi-objective component, it remains unclear how the proposed method fundamentally differs from existing fully decentralized MARL approaches. The paper lacks a Related Work section to situate its contribution in the broader literature.\n\n2. From the current description, it appears that each agent’s local state space includes information from all nodes, which contradicts the notion of locality. The experimental section should provide more detailed explanations of the experimental design, particularly clarifying how the k-hop neighborhood is defined and implemented.\n\n3. Although the paper claims to develop a fully distributed algorithm, the definition of the Q-function indicates dependence on neighboring agents’ rewards. The authors should clarify what “fully distributed” means in the framework—whether it allows partial information exchange among neighbors or strictly prohibits it.\n\n4. The description of the feature vector mapping is insufficient. It is unclear whether this mapping is a user-defined linear function and whether it serves to restrict which neighbors are included for agent $i$. A more explicit formulation or example would improve clarity.\n\n5. In the second term of Equation (12), the normalization factor should likely be the size of the neighborhood $|N_i|$ rather than the total number of agents $N$. This correction is important for maintaining consistency and correct scaling in the local TD update."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gVfQDGK0CS", "forum": "YWwcYFlJSj", "replyto": "YWwcYFlJSj", "signatures": ["ICLR.cc/2026/Conference/Submission16071/Reviewer_LYAQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16071/Reviewer_LYAQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893292363, "cdate": 1761893292363, "tmdate": 1762926259582, "mdate": 1762926259582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a fully distributed algorithm for multi-objective multi-agent reinforcement learning, where each agent updates its policy using a localized, action-averaged critic over its κ-hop neighborhood and negotiates objective trade-offs via a consensus–Frank–Wolfe update of weights. The authors derive bounds showing that the policy-gradient approximation error decays with neighborhood radius and prove convergence to an $\\varepsilon$-Pareto-stationary point under standard assumptions. Experiments on multi-robot path planning report higher per-objective returns and faster optimization (smaller $\\|g_t\\|_2$) than a centralized MORL baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a principled localization of policy gradients to κ-hop neighborhoods with a clear approximation bound that decays as $(\\gamma)^{\\kappa+1}$ and a proof of convergence to an $\\varepsilon$-Pareto-stationary point. Its communication-efficient consensus–Frank–Wolfe weighting—using only neighbor information—translates to practical multi-robot gains, showing higher per-objective returns and faster reduction of $\\|g_t\\|_2$ than a centralized baseline."}, "weaknesses": {"value": "- Consensus implementation vs. theory mismatch: Algorithm 2 uses “while-until-exact-consensus,” implying unbounded communication, whereas the analysis assumes a fixed $K_\\lambda$; the bounds omit an explicit consensus residual, so finite-round practice either violates assumptions or undermines scalability.\n- Core approximation issues: the gradient-error bound lacks explicit dependence on graph topology (degree, spectral gap, neighborhood size), and the critic/TD scaling sums local rewards but divides by global $N$; when $|\\mathcal N_i^\\kappa|\\!\\ll\\! N$, this compresses magnitudes, distorts variance, and complicates step-size selection (a normalization by $|\\mathcal N_i^\\kappa|$ would be more coherent).\n- Weak empirical support for key claims: only a single centralized baseline is considered, with no Pareto frontier, significance tests, ablations, or communication/runtime reporting; claims of being “near central optimum” and “faster” lack a verifiable upper bound or comparisons to strong MARL baselines."}, "questions": {"value": "- Could you broaden the empirical study to plot full Pareto frontiers (by sweeping initial weights), compare against strong MARL baselines (e.g., VDN, QMIX, MAPPO, MADDPG), report mean±std over multiple seeds with significance tests, and quantify communication per update and wall-clock time relative to a centralized approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "q3zpYsMHo8", "forum": "YWwcYFlJSj", "replyto": "YWwcYFlJSj", "signatures": ["ICLR.cc/2026/Conference/Submission16071/Reviewer_e4Vc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16071/Reviewer_e4Vc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001236401, "cdate": 1762001236401, "tmdate": 1762926258927, "mdate": 1762926258927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a fully distributed actor-critic algorithm for multi-objective multi-agent reinforcement learning. Each agent uses only $\\kappa$-hop neighborhood states and its local action, together with an actionaveraged $Q$-function and linear function approximation, to estimate policy gradients. A consensus-plus-Frank-Wolfe procedure adjusts objective weights. The authors prove $O(1 / T)$ convergence to an $\\varepsilon$-Pareto-stationary solution under standard assumptions on rewards and network connectivity conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n\n\n1. The paper replaces the global policy gradient with a local, action-averaged gradient $\\nabla_{\\theta_i} J_{\\text {app }, i}^m(\\theta)$ that depends only on the agent's own action and $\\kappa$-hop neighborhood. It then proves a geometrically decaying approximation error\n$\\left\\|\\nabla_{\\theta_i} J_{\\mathrm{app}, i}^m(\\theta)-\\nabla_{\\theta_i} J^m(\\theta)\\right\\|_2 \\leq \\frac{\\sqrt{2 R}}{\\left(1-\\gamma^m\\right)^2}\\left(\\gamma^m\\right)^{\\kappa+1}$\nwhich makes the approximation transparent and parameterized by $\\kappa$.\n\n\n2. The critic uses a linear approximation of the form\n$\\hat{Q}^m_i\\left(s_{N_i^\\kappa}, a_i ; w_i^m\\right)=\\phi_i\\left(s_{N_i^\\kappa}, a_i\\right)^{\\top} w_i^m$\nso that each agent only estimates values over its neighborhood state and its own action. This aligns the value approximation with the locality assumption used in the policy-gradient derivation.\n\n3. Distributed weight selection that preserves the convergence rate.\nThe scalarization/weight-selection step is solved by a consensus plus Frank-Wolfe update over the network. The analysis shows that, despite decentralization, the overall algorithm still achieves an $O(1 / T)$ convergence rate to an $\\varepsilon$-Pareto-stationary point, which is nontrivial for multi-objective, multi-agent settings."}, "weaknesses": {"value": "Weaknesses:\n\n1. The final actor update bundles truncation, sampling, and function-approximation errors into a single term; the paper shows it stays bounded, but does not give a tight characterization of how this bound scales with all inner-loop parameters, so the sharpness of the $O(1 / T)$ claim is partially opaque.\n\n2. The key bound decays as $\\left(\\gamma^m\\right)^{\\kappa+1} /\\left(1-\\gamma^m\\right)^2$. When $\\gamma^m$ is close to 1 and $\\kappa$ must stay small for communication reasons, this term can be large, so the locality that makes the method scalable can simultaneously weaken the approximation guarantee.\n\n3. The distributed weight-update step requires (at least approximate) consensus at every outer iteration. The theory assumes this is done sufficiently well, but the per-iteration communication/synchronization burden is not incorporated into the main convergence complexity statement."}, "questions": {"value": "1. Normalization in critic updates: The TD-style critic uses localized information. Is the global normalization factor (or averaging step) essential for the contraction argument, or could a purely local normalization reduce variance without breaking the proof?\n\n2. The Frank-Wolfe-based weight update is analyzed as if the subproblem is solved well each round. How does the main convergence bound change if only a fixed, small number of FW steps (and consensus rounds) is used per iteration?\n\n3. The policy-gradient derivation is presented for the softmax/discrete case. Can the same local-action, action-averaged construction be extended to deterministic or Gaussian policies in continuous action spaces while still retaining the $O(1 / T)$ rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ofPt26Wi6W", "forum": "YWwcYFlJSj", "replyto": "YWwcYFlJSj", "signatures": ["ICLR.cc/2026/Conference/Submission16071/Reviewer_atGW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16071/Reviewer_atGW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206644572, "cdate": 1762206644572, "tmdate": 1762926258280, "mdate": 1762926258280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}