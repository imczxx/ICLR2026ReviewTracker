{"id": "CaOaBlq6Bv", "number": 22490, "cdate": 1758331808893, "mdate": 1759896862987, "content": {"title": "Towards Multiplier-Free Transformers with Stochastic Attention", "abstract": "In standard attention, a substantial fraction of compute comes from multiplying softmax weights by high-precision value vectors ‚Äî even in ternary models such as BitNet, which remove multipliers elsewhere. We present Stochastic Additive No-mulT Attention (SANTA), a drop-in inference-time replacement that eliminates these value-stage multiplications. For each query, SANTA samples from the post-softmax distribution, gathers and sums selected values, and applies a single bit-shift normalization, with no expensive multipliers on the value path. SANTA‚Äôs compute scales as $O(n_{queries} \\cdot S \\cdot d_k)$: linear in the number of queries during prefill and linear in the sample budget $S$ during decode, while exhibiting sparse, index-based memory access. SANTA is an unbiased Monte Carlo estimator of dense attention and is orthogonal to upstream efficiency techniques (ternary quantization, low-rank kernels, sparsity, pruning). Combined with existing 1-bit/ternary quantizers, SANTA moves Transformers toward fully multiplier-free, energy-efficient inference.", "tldr": "SANTA: an unbiased post-softmax estimator that swaps the value-stage matmul for sampled gather-add + bit-shift, with compute O(n_queries¬∑S¬∑d_k) (linear in prefill, linear in S at decode) and sparse reads‚Äîtoward multiplier-free LLM inference.", "keywords": ["efficient attention", "Monte Carlo attention", "transformer inference", "energy-efficiency", "sampling", "multiplier-free", "memory bandwidth", "KV-cache", "edge devices", "large language models", "hardware-aware ML"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05d373e7a4050caa7e2378cee8b7eeb27261d409.pdf", "supplementary_material": "/attachment/3f9971b336d358355c1023d8fd8ca7506a6e0fcc.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces SANTA, an additive attention variant that serves as a drop-in replacement for the standard attention mechanism. SANTA approximates the attention output by sampling value vectors according to the post-softmax distribution and then computing their mean. The authors prove that this method provides an unbiased estimate and further propose S¬≤ANTA, which leverages stratified sampling to reduce variance. In comparison to top-k attention, the authors demonstrate that SANTA is more power-efficient by completely eliminating multiplications in the value stage. The experiments show that SANTA achieves promising results, with performance comparable or even superior to the top-k method across various benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n2. To the best of my knowledge, the method is novel.\n3. SANTA effectively eliminates a key computational bottleneck while maintaining competitive model accuracy.\n4. The finding that a simple, unbiased averaging of values can achieve performance comparable to the strong top-k baseline is both surprising and compelling."}, "weaknesses": {"value": "1.  While the authors argue SANTA is more power-efficient by eliminating multiplications, they dismiss sampling costs as \"lightweight relative to the V matrix multiply\" (¬ß3.4) without providing empirical measurements. The computational cost of sampling from categorical distributions over long sequences should be benchmarked against top-k's partial sorting overhead to substantiate the efficiency claims.\n2. Although the authors claim the method is suitable for edge devices, the paper provides no actual hardware deployment experiments, nor does it discuss the implementation on edge devices."}, "questions": {"value": "1. How does SANTA perform on actual edge hardware? While top-k can be fused with online softmax via heap-based algorithms with efficient rescaling, can SANTA similarly avoid full softmax I/O? Is sampling truly faster than partial sorting on resource-constrained devices?\n2. What is the fundamental advantage of weighted Monte Carlo sampling over simply averaging the top-k values with uniform weights (also multiplication-free)? This ablation would clarify whether the stochastic framework provides benefits beyond just selecting a sparse subset.\n\nWould raise my score if authors solve my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1cdj6WljDq", "forum": "CaOaBlq6Bv", "replyto": "CaOaBlq6Bv", "signatures": ["ICLR.cc/2026/Conference/Submission22490/Reviewer_2czp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22490/Reviewer_2czp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810207642, "cdate": 1761810207642, "tmdate": 1762942239829, "mdate": 1762942239829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SANTA (Stochastic Additive No-mulT Attention), a novel attention mechanism that aims to eliminate multiplication operations from Transformer inference.\nInstead of computing the standard attention product \nsoftmax(ùëÑùêæ‚ä§)ùëâ, SANTA samples keys from the attention distribution and approximates the output via Monte Carlo averaging. The resulting estimator ùê¥^ùëâA^V is shown to be unbiased, with variance decreasing proportionally to 1/ùëÜ, where ùëÜ is the number of samples.\n\nAn enhanced version, S¬≤ANTA, applies stratified or systematic sampling to further reduce variance. Because the sampling process involves only additions, indexing, and bit shifts, the model achieves a completely multiplier-free inference pipeline when combined with low-bit quantized feed-forward layers (e.g., BitNet)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The stochastic sampling approach provides an unbiased, theoretically grounded estimator of the attention output.\n2. Addresses a key limitation of efficient Transformers, the heavy reliance on multiplications in attention."}, "weaknesses": {"value": "1. Lack of hardware validation: The claimed multiplier-free and energy-efficiency benefits are purely theoretical; no real measurements or FPGA/GPU latency/energy analysis are provided.\n2. Potential implementation inefficiency: Random sampling and irregular memory access could make GPU execution slower than dense attention in practice.\n3. Missing baselines: No comparison against kernelized linear attention or FlashAttention energy-profiling to substantiate the energy-efficiency claim."}, "questions": {"value": "1. Can you provide any empirical runtime or energy measurements on real hardware (e.g., A100/H100 or FPGA) to substantiate the multiplier-free claim?\n2. If the current hardware is not a good candidate for this algorithm, can the authors provide a clearer discussion on what type of hardware architecture would be suitable for such multiplier-free computation ‚Äî for instance, what memory access patterns, parallelism model, or instruction primitives (e.g., stochastic sampling units or bit-level adders) would be required to efficiently support SANTA on future accelerators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nihS9J2D9T", "forum": "CaOaBlq6Bv", "replyto": "CaOaBlq6Bv", "signatures": ["ICLR.cc/2026/Conference/Submission22490/Reviewer_X3US"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22490/Reviewer_X3US"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851568374, "cdate": 1761851568374, "tmdate": 1762942239558, "mdate": 1762942239558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduced Stochastic Additive No-multiplication Attention (SANTA), a drop-in, inference-time, replacement for the standard attention layer. SANTA approximates the attention operator without performing any score-value multiplications, thus offering a multiplier-free replacement. This is achieved through a Monte-Carlo estimation by sampling from the attention softmax distribution, thus only using additions, as well as a bit-shift operation instead of division (given the number of samples is a power of two)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow and clearly introduces the problem and its proposed attention method.\n\n- Figure 1 is clear and is good representation of the algorithm.\n- The mathematical notation is clearly introduced and the propositions and derivations are easy to follow.\n- The authors showcase a good performance of SANTA compared to the sparse top-$k$ attention."}, "weaknesses": {"value": "One of the main motivations of the paper seems to be getting rid of multipliers for transformer inference, complimenting techniques such as BitNet (that removes multipliers in matrix multiplications), and NoMAD-Attention, that removes multiplications in the $QK^T$ part of attention. The impact of this is not fully clear however, especially as, like the authors note, current hardware is indeed optimised for fast matrix multiplications. Unlike BitNet, SANTA is applied at inference time, and its implications to training are not explored. Furthermore, the authors hint at potential synergy of the method with BitNet-like architectures in order to fully get rid of multiplications in the forward calculation, but this is not backed by any experimental data (i.e., it is not clear if the method would work well with these architectures) ‚Äî a demonstration of a fully multiplication-free transformer would benefit this argument.\n\n- The underlying sampling idea is not new, and has been used in previous work to approximate attention (e.g., https://arxiv.org/abs/2410.16179)\n- The paper does not provide any practical measurements on effective speed-ups that could be achieved. Although the algorithm might benefit future hardware to a greater extent, the decrease in FLOPs and memory transfers should still offer a benefit (and indeed top-$k$ techniques are used for this). Having a practical analysis could give a stronger case for using SANTA-like replacement of attention during inference."}, "questions": {"value": "- The authors note that the number of unique keys accessed can be significantly fewer than the number of samples; it would be useful to get a sense of the implication of this to a practical speed-up.\n- Although authors mention that sampling/sorting costs can be ignored, it would be helpful to get some sense of their relative cost vs. the other operations within the module. This could be especially helpful in order to understand the advantage of SANTA vs. top-$k$- as a drop-in replacement of attention during inference.\n- Minor: It would be useful to also have perplexity results on a standard dataset (such as WikiText)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ow8tzdQGT5", "forum": "CaOaBlq6Bv", "replyto": "CaOaBlq6Bv", "signatures": ["ICLR.cc/2026/Conference/Submission22490/Reviewer_r9EV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22490/Reviewer_r9EV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910817928, "cdate": 1761910817928, "tmdate": 1762942239333, "mdate": 1762942239333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper sets out to remove the need for multiplication in attention networks by using what it calls stochastic attention.  The"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper identifies a worthy goal: Reducing computational cost of neural networks that employ attention would be helpful."}, "weaknesses": {"value": "While the introduction and related work sections (1 and 2) are readable, the main contribution section (3) was not in this reviewer's estimation actually human readable.  The paper says \"LLMs were used to polish the presentation and writing of this contribution\" on lines 843-844 but it's not clear what parts of the paper that applies to.  If this paper was written in a language other than English, then passed to an LLM to translate, I think the LLM did a poor job on the main technical part.\n\nThe results section appears to only compare against sparsity (top-k).  It is unclear why no other quantization approaches are compared against.\n\nRegarding Remark 3.2 that if S = $2^m$ then division can be implemented as a bit shift, I am concerned what this paper is proposing in Equation 2 is to replace multiplication of A times B by summing B copies of A.   That hardly seems like the right way to achieve efficiency."}, "questions": {"value": "Are you just proposing to implement multiplication by summing a bunch of copies of the multiplicand by the multiplier?  (That is what Equation 2 seems to be doing to me.)\n\nI'm not sure what it means to \"treat each row $A_q$ as a categorical distribution over keys and sample $S$ values i.i.d. from it\".   What is $S$ and how is it set or selected?  What does it mean to sample a row of a matrix?  What is meant by categorical distribution?  How does any of this approximate a multiplication?\n\nAccording to what distribution is the sampling of one-hot rows governed in the example in Equation 2?\n\nFor the \"illustrative example\" in Equation 2, what is the corresponding value of A?\n\nIt is unclear how $V_i$ is obtained in Equation 3.  What does it mean to stack rows from V in forming $V_i$?\n\nWhat is \"Categorical($A_q$)\" on line 158?  What is $V_i_{q,s}$ in the equation on Lines 159-160?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AfArMr7AjQ", "forum": "CaOaBlq6Bv", "replyto": "CaOaBlq6Bv", "signatures": ["ICLR.cc/2026/Conference/Submission22490/Reviewer_51as"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22490/Reviewer_51as"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22490/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762997011529, "cdate": 1762997011529, "tmdate": 1762997011529, "mdate": 1762997011529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}