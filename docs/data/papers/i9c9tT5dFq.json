{"id": "i9c9tT5dFq", "number": 15424, "cdate": 1758251222440, "mdate": 1763091025275, "content": {"title": "MST-SAM: Bridging Multi-View Gaps in SAM2 with Spatiotemporal Bank", "abstract": "High-quality, instance-level segmentations are crucial for developing multi-view vision-centric systems, such as self-driving vehicles and mobile robots, yet their annotation acquisition is prohibitively expensive. While human-in-loop labelling paradigms like SAM2 show great promise in monocular videos, adapting them to multi-cameras scenarios is hindered by two fundamental flaws: spatially, an ignorance of cross-view geometry leads to severe tracking ambiguity; and temporally, the exponential memory demands preclude real-time performance. To address these challenges, we propose MST-SAM, a novel streaming framework for robust, multi-view instance segmentation and tracking through spatio-temporal bank. Our method introduces two core components: (1) a Spatio-Positional Augmentation (SPA) module that bridges SAM2’s 2D-centric design with 3D scene geometry. It learns a unified positional prior from camera transformations, enabling tokens to reason about their absolute spatial location across different views. (2) a Memory View Selection (MVS) strategy that prunes the temporal memory bank, significantly reducing the computational overhead of the multi-view system while maintaining high algorithm performance. We validate our method on the nuScenes and Waymo datasets using a custom multi-view instance segmentation benchmark we introduce, where MST-SAM sets a new state of the art and demonstrates strong generalization.", "tldr": "", "keywords": ["SAM", "Spatiotemporal Bank", "multi-camera system."], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d81e6cdf1bebef8f1fa164f3d8f8fc7418a39004.pdf", "supplementary_material": "/attachment/ac34127294c06f08634f937af5ff098d81bb8cf4.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MST-SAM, a method for robust instance segmentation in cross-view videos. Built upon the SAM2 model, which tackles instance segmentation for a monocular video, MST-SAM proposes a Spatio-Positional Augmentation (SPA) module to get a positional encoding that combines both 3D geometric encoding and the standard 2D sinusoidal encoding. Besides, it introduceds the Memory View Selection (MVS) strategy, to selectively prune redundant or useless information in the memory bank to avoid the prohibitive memory and computational costs for processing long, multi-view videos. The experiments and ablation study on nuScenes and Waymo show the effectiveness of the whole method and each individual module."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method extends the SAM2 method to tackle instance segmentation for multi-view videos and outperforms the naive baseline of serializing the multi-view video into a single pseudo-sequence on two datasets.\n2. The method proposes two modules, one utilizes a 3D projection in positional encoding and the other defines a set of rules to make the memory bank more compact."}, "weaknesses": {"value": "1. The writing quality could be improved. Just to name a few places (not an exhaustive list): \n\n(1) L193 and L196 are repeating. \n\n(2) L050 has an incomplete sentence \"the original SAM2 framework\".\n\n(3) L182, $Fmem,t$ should be $F_{mem, t}$.\n\n2. The geometric positional encoding requires camera intrinsics and poses, which seems to impose more constraints to the data. \n\n(1) I wonder how robust the method is with respect to inaccurate / noisy camera parameters, or maybe poses gotten from running COLMAP.\n\n3. My biggest concern is that the method only did experiments on two autonomous driving datasets. I am curious if the method also works on other kinds of datasets, e.g. indoor ones.\n\n4. I am also curious to see more analysis on the capacity of the method, e.g. is there any constraints on the video lengtht that the method can handle, is there any constraints on the object size, how well the method handles repeating objects e.g. trees."}, "questions": {"value": "1. For the aggregation in equation (4), why do you use max-pooling? Have you experimented with other aggregation e.g. averaging?\n\n2. How many frames of the videos can the method handle?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "brEwUQu5NF", "forum": "i9c9tT5dFq", "replyto": "i9c9tT5dFq", "signatures": ["ICLR.cc/2026/Conference/Submission15424/Reviewer_zmUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15424/Reviewer_zmUL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526628124, "cdate": 1761526628124, "tmdate": 1762925701391, "mdate": 1762925701391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "LZewtnz55h", "forum": "i9c9tT5dFq", "replyto": "i9c9tT5dFq", "signatures": ["ICLR.cc/2026/Conference/Submission15424/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15424/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763091024432, "cdate": 1763091024432, "tmdate": 1763091024432, "mdate": 1763091024432, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MST-SAM, an online framework designed to extend SAM2 for multi-view instance segmentation and tracking in autonomous driving scenarios. The method introduces a Spatio-Positional Augmentation (SPA) module to incorporate geometric priors and a Memory View Selection (MVS) strategy to improve efficiency without sacrificing accuracy. Experiments on nuScenes and Waymo show significant performance gains over SAM2 baselines with minimal latency overhead. Overall, the work provides a strong step toward scalable multi-camera segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors reasonably design a memory-based framework that effectively integrates the Segment Anything Model (SAM) for multi-view, instance-level segmentation and tracking.\n2. The proposed method demonstrates significant performance improvements over the baseline, establishing state-of-the-art results on the introduced benchmarks.\n3. The paper also reports a competitive latency, with only marginal computational overhead compared to the SAM2-MV baseline, indicating the method’s efficiency and potential applicability in real-time systems."}, "weaknesses": {"value": "1. The comparison set is limited - the proposed method is mainly compared to SAM-based variants. To strengthen the evaluation, it would be beneficial to include comparisons with state-of-the-art video instance segmentation (VIS) methods, which already provide robust temporal object matching.\n2. The multi-view object matching process appears to rely on additional mechanisms, but the robustness of temporal matching from recent VIS methods could serve as a valuable baseline reference.\n3. Since the paper targets autonomous driving scenarios, the authors should also report inference time and memory consumption comparison to the VIS methods as well. \n4. While MST-SAM effectively extends SAM2 with spatio-temporal and geometric modules, the core contributions (the SPA and MVS modules) are incremental rather than conceptually groundbreaking. The method mainly builds upon existing ideas from 3D-aware positional encodings and memory pruning strategies, which may limit its originality.\n5. (Minor) Typographical error: On page 5, line 255, the notation T_ego(t_{src}←cam(t_{src}) should be corrected to T_ego(t_{src})←cam(t_{src}) for clarity and mathematical consistency."}, "questions": {"value": "Could the authors clarify the scalability of MST-SAM to larger multi-camera systems or higher frame rates? For instance, how would performance and latency change if the number of cameras or the sequence length increases significantly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yMkLRE56Ry", "forum": "i9c9tT5dFq", "replyto": "i9c9tT5dFq", "signatures": ["ICLR.cc/2026/Conference/Submission15424/Reviewer_6brn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15424/Reviewer_6brn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891556546, "cdate": 1761891556546, "tmdate": 1762925700864, "mdate": 1762925700864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to augment SAM2 to segment objects in multi-view video streams. This is made difficult by the fact that SAM2 was only trained on single view videos. The authors propose a spatio-positional encoding module, which lifts patches to 3D, and an attention mechanism that performs token selection across views."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on a relatively understudied aspect of 3D video segmentation, which is fusing multiple cameras/views. The proposal seems sensible and draws on the strengths of SAM2, rather than inventing a whole new architecture which needs to be trained from scratch. The experiments, while small, seem fairly well done, and cover generalization across views and objects."}, "weaknesses": {"value": "The main issues of the paper are the presentation, novelty, inadequate related work, and a simple missing baseline that may affect the conclusion.\n\nThe novelty is always hard to quantify, but in this case it mostly amounts to the application, which is to focus on adding multi-view fusion capabilities to an existing strong video segmentation model. The technical innovation to achieve this is not very large, as it consists of combining elements that are very common in 3D segmentation. For example, the following papers are representative examples that do similar types of 3D segmentation lifting:\n\n- Bhalgat et al., \"3D-Aware Instance Segmentation and Tracking in Egocentric Videos\", ACCV 2024\n- Gu et al., \"EgoLifter: Open-World 3D Segmentation for Egocentric Perception\", ECCV 2024\n- Siddiqui et al., \"Panoptic Lifting for 3D Scene Understanding With Neural Fields\", CVPR 2023\n\nThese and more would be needed to complement the related work, which is currently sparse. The memory attention is, of course, based on very common attention operators (indeed part of SAM2). The memory view selection strategy is heuristic, although it could be somewhat novel, it consists of simple rules. Overall this is not to say that there is no novelty -- the combined system seems novel. It is mostly that the new additions are heavily inspired by very common operations and may not clear the bar for publication.\n\nAnother problem is the presentation. As is, the paper is somewhat hard to follow, due to uninformative and distracting phrasings that have clear LLM-generic-text markings (everything is \"critical\", \"elegant\", \"intelligent\", \"inherent\", etc), some errors (e.g. repetition of a phrase in lines 192 and 195), wrong use of bibtex's citet instead of citep, etc. I hope that the heavy use of LLMs is disclosed. Overall the manuscript needs more human proofreading to improve the clarity of presentation.\n\nFinally, there is one missing baseline that is very important: fine tuning SAM2 on the same dataset (e.g. Waymo). Since the original SAM2 was not fine-tuned in the same data distribution, we do not know to what degree the proposals improve performance, vs. just fine-tuning on the same data."}, "questions": {"value": "I would like to ask the authors about the baseline - if it is in the paper, I missed it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zblgLqbrSR", "forum": "i9c9tT5dFq", "replyto": "i9c9tT5dFq", "signatures": ["ICLR.cc/2026/Conference/Submission15424/Reviewer_25pp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15424/Reviewer_25pp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928161916, "cdate": 1761928161916, "tmdate": 1762925699279, "mdate": 1762925699279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work targets multi-view + temporal instance segmentation for multi-camera systems.\n\nThey introduce two ideas to extend SAM2 to this setting:\n(1) Spatio-Positional Augmentation (SPA): Uses camera intrinsics and extrinsics to encode a ray, concatenates the ray with depth information, then max pools to get 3D positional embeddings.\n(2) Memory View Selection (MVS): a heuristic based strategy for managing temporal context.\n\nFor training, they use hybrid sampling (80% cross-view, 20% single-view instances) on nuScenes and Waymo.\nThey introduce a custom cross-view benchmark on these datasets, measuring J (region IoU) and F (boundary F-measure).\nThey show significant improvement over FastPoly-SAM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method section is written clearly.\n\nThe mathematical notation is concise and easy to follow.\nFigures 3 and 4 are illustrative and explain the ideas quite well."}, "weaknesses": {"value": "The paper doesn't discuss/compare against much of the multi-view perception literature that already have established ways to handle 3D positional embeddings.\nThe 3D positional embedding approach very closely resembles the ideas proposed from PETR/PETRV2 (Liu et al. ECCV 2022, ICCV 2023).\nEven though the tasks differ (BEV detection vs multi-view segmentation), these methods must be in the related work and considered for comparison.\n\nThe baselines they compare against are primarily ablations of their own architecture design(MST-SAM-L, MST-SAM-G, MST-SAM-M) rather than published methods.\nOnly FastPoly-SAM is an external baseline, making it difficult to assess performance against the broader literature."}, "questions": {"value": "The SPA module uses max pooling to aggregate the discretized ray's features.\nI don't quite see the intuition behind this - could the authors elaborate on this?\n\nWhy is a new benchmark necessary?\nTo my understanding the datasets already have established instance segmentation / tracking benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m2qjMCOPFA", "forum": "i9c9tT5dFq", "replyto": "i9c9tT5dFq", "signatures": ["ICLR.cc/2026/Conference/Submission15424/Reviewer_xfN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15424/Reviewer_xfN1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014696185, "cdate": 1762014696185, "tmdate": 1762925698665, "mdate": 1762925698665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}