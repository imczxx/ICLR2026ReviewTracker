{"id": "l4noTuvYMP", "number": 7700, "cdate": 1758032735146, "mdate": 1759897838473, "content": {"title": "Unsupervised Reinforcement Learning with Verifiable Rewards via First Repeat Criterion", "abstract": "Recent advances in Large language models (LLMs) proves that Reinforcement Learning with Verifiable Rewards (RLVR) can enhance the reasoning capabilities of LLMs only with automatically verifiable signals. However, it is still a challenging and labor-consuming task to collect the ground truth answers for reasoning model, specially in the resource-constrained scenario. In this paper, we investigates the potential of unsupervised reinforcement learning with verifiable reward, and propose uns-GRPO framework to improve math reasoning of small LLMs. Firstly, we design an unsupervised reward model by generating pseudo answers by first repeat criterion. It treats the first repeated answer in a sequence of generated responses as the ground-truth, which is demonstrated with efficiency and reliability in resource-constrained settings. Secondly, we propose an adaptive KL regularization to mitigate the noise introduced by pseudo answer. A unique consistency is observed between pseudo answer confidence and accuracy rewards.Adjusting by the accuracy rewards, the adaptive KL regularization enforces conservative optimization when the confidence is low, while encourages diverse exploration when the confidence is high. Experimental results demonstrate that our un-supervised approach achieves stable improvement across diverse models, training datasets, and evaluation tasks.", "tldr": "", "keywords": ["Reinforcement Learning; Unsupervised Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e19ccd7d967670a90179f49a74d09eb1a3341081.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes uns-GRPO, an unsupervised reinforcement learning framework for reasoning models. It replaces ground-truth answers with pseudo-answers generated by a first-repeat criterion, where the first repeated answer among multiple samples is treated as correct. To mitigate noise from potentially wrong pseudo labels, the method adds an adaptive KL regularization: when pseudo-answer confidence is low, KL is strengthened to slow learning; when confidence is high, it is relaxed to allow exploration. Experiments on several math reasoning benchmarks (GSM8K, AIME24, AMC23, Minerva, OlympiadBench) show consistent improvements across models from 0.5B to 7B parameters, with solid ablations on KL settings, datasets, and temperatures."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Clarity and structure: The paper is clearly written, with straightforward explanations and well-organized experiments.\n\n2) Thorough experiments: The empirical evaluation is extensive—covering multiple model sizes, datasets, KL variants, and temperature settings. Ablations and failure cases (e.g., reverse adaptive KL) are clearly analyzed.\n\n3) Practical significance: The method offers a simple and efficient way to reduce supervision cost in RLVR, achieving stable improvements even in resource-constrained settings.\n\n4) Empirical rigor: Results are consistent across benchmarks, especially showing stronger gains for smaller models."}, "weaknesses": {"value": "1) Limited conceptual novelty:\nThe core idea is a small modification of prior self-consistency methods. Using the first-repeat heuristic instead of majority vote reduces cost and complexity but does not change the fundamental assumption that the most consistent answer among multiple generations is likely to be correct. The work improves efficiency rather than redefining the underlying learning signal, so the novelty is more engineering-oriented than conceptual.\n\n2) Dependence on model ability:\nThe framework depends on the base model having a reasonable level of reasoning skill. If the model consistently produces wrong but self-consistent answers, the first-repeat criterion will still select those as pseudo labels. In that case, the model may reinforce its own systematic errors. The adaptive KL term only slows the update when confidence is low; it cannot correct wrong pseudo labels or recover the right reasoning path. Thus, the method improves models that already have moderate capability even though they might not need to be big models, but it may not help very weak ones.\n\n3) Narrow task scope:\nAll experiments are on math-style reasoning tasks where the answer is explicitly verifiable. The method naturally fits these domains, since correctness can be judged by string equality. However, it is unclear whether the same approach would work in open-ended tasks such as general QA, code generation, or preference learning, where there is no clear notion of a single correct answer. The contribution would be stronger if some results on less verifiable domains were provided.\n\n4) Heuristic KL design:\nThe adaptive KL is implemented based on a threshold of pseudo-label confidence. This design works in practice but remains a hand-tuned heuristic rather than a principled or continuous uncertainty modeling approach. A smoother or probabilistic control of KL strength might provide more stable or theoretically grounded behavior."}, "questions": {"value": "How sensitive are the results to the specific threshold (τ=0.8) used in adaptive KL? Would a continuous mapping from confidence to KL coefficient perform better?\n\nCould the authors test this framework on non-mathematical reasoning datasets (e.g., code generation or QA) to check generality?\n\nHow does the approach behave when the base model is much weaker? does it converge to wrong answers?\n\nCould the pseudo-answer selection benefit from lightweight external calibration to reduce error propagation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iTVKqf69NO", "forum": "l4noTuvYMP", "replyto": "l4noTuvYMP", "signatures": ["ICLR.cc/2026/Conference/Submission7700/Reviewer_H57W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7700/Reviewer_H57W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550222033, "cdate": 1761550222033, "tmdate": 1762919758612, "mdate": 1762919758612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on improving the LLMs' reasoning ability in resource-constrained scenarios. Specifically, they target the situation of being resource-constrained and without ground truth answers, i.e., an unsupervised learning situation. To tackle this problem, the authors designed a framework that applies a pseudo-answer generation mechanism and noise processing methods. Experiments show that the method achieves competitive performance compared to the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is clear and specific. Rather than generally discussing reasoning ability under resource-constrained scenarios, the authors focus on the specific task of unsupervised learning under resource-constrained scenarios. This focus deepens the discussion depth of this paper.\n2. The authors provide sufficient experimental details and hyperparameters for the experiments. In general, the reproducibility of this paper should not be a problem."}, "weaknesses": {"value": "1. In the related work section, the authors mention multiple unsupervised learning methods. However, in the experimental results, like in Table 1, each comparison is only for the baseline and +uns-GRPO. The authors fail to discuss the comparison between their method and other existing methods. This lack makes it hard for the reader to confirm whether the method is the best practice in this field compared to other algorithms.\n\n2. In the experimental setup, the setting of τ = 0.8 seems based on empirical observation. However, the authors fail to provide a sensitivity analysis or ablation study for this important hyperparameter. This raises the question of whether the effectiveness of the method is based on this hyperparameter setting."}, "questions": {"value": "1. Based on Eq. 6, the method relies on finding the exact match answer. This may be feasible for mathematical reasoning because the answers are often in a standardized format. The open question is whether this method can be applied to other problems that require more fuzzy matching. For example, in code generation or logic questions, answers with the same semantics but different expressions will not be recognized as \"duplicates\".\n\n2. In this paper, the authors focus on models of 7B and below. On a larger scale, like 70b, the self-consistency of LLMs may be high, and the ensemble strategy may not work. Whether the method can be extended to a large scale is also an open question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vaxx38WcFk", "forum": "l4noTuvYMP", "replyto": "l4noTuvYMP", "signatures": ["ICLR.cc/2026/Conference/Submission7700/Reviewer_eArp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7700/Reviewer_eArp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755380951, "cdate": 1761755380951, "tmdate": 1762919757852, "mdate": 1762919757852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approach for unsupervised pos-training which samples pseudo answers based on the first-repeat principle, accompanied by a method for estimating the confidence levels of pseudo answers and an adaptive KL regularization to mitigate noise in pseudo labels. The proposed approach is evaluated on a variety of models and datasets and demonstrate consistent advantages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using the first-repeat principle is well motivated, as well as the method for estimating the confidence levels of pseudo labels.\n2. The proposed approach is comprehensively tested.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The texts in figures are very hard to see.\n2. The proposed method is not compared with other unsupervised post-training method."}, "questions": {"value": "1. Could you provide empirical evidences for the pros and cons of this method with other unsupervised post training techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sOvmuQWrkH", "forum": "l4noTuvYMP", "replyto": "l4noTuvYMP", "signatures": ["ICLR.cc/2026/Conference/Submission7700/Reviewer_xjoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7700/Reviewer_xjoM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987252970, "cdate": 1761987252970, "tmdate": 1762919757530, "mdate": 1762919757530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised variant of RLVR, uns-GRPO, for math reasoning problems. Unlike prior unsupervised works which are based on output self-consistency, the proposed approach utilizes a first-repeat criteria: the first answer which is repeated is taken to be the true answer. The benefit of which being computational complexity; rather than requiring a quadratic number of comparisons this approach only requires a linear number with respect to the number of generated outputs. When combined with an adaptive KL regularization scheme, the paper shows improvements over baseline models across several math benchmarks and models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n* The answer analysis in Fig. 4 shows some nice computational scaling while yielding comparable accuracy estimates in most cases (although it is not clear whether this trend holds at all points during post-training).\n* The proposed method is conceptually simple and should be easy to implement in practice"}, "weaknesses": {"value": "Weaknesses:\n* Empirical evaluation (Table 1) lacks any baselines other than the baseline model. At the very least this should include the baseline model with ground truth rewards, and would benefit from baselines that use other unsupervised techniques such as majority voting.\n* Although the results in Fig. 4 appear impressive on the surface, the dataset seems relatively “easy” since the accuracy plateaus at 5-10 generated samples. This weakens the argument, since the difference in computational time does not significantly differ until higher generation numbers.\n* There is an additional hyperparameter that needs tuning: tau (for the adaptive KL). There is no sensitivity analysis over this.\n* Minor: Figure text is way too small, and needs to be adjusted.\n* Minor: text needs a language/grammar pass."}, "questions": {"value": "Questions:\n1. In Fig. 4, at what point during training are the accuracy values computed? Since the model is non-stationary (it changes with every gradient update of GRPO), does this also affect the accuracy values?\n2. Does the proposed confidence metric require larger GRPO groups than normal?\n3. The language describing the adaptive KL parameter is confusing. Eq. 8 shows it being set to either 0 or 1, but the text describes it as being “enlarged” or “relaxed”, which does not match with the mathematical description. Am I misunderstanding something here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "prCKYDa7E1", "forum": "l4noTuvYMP", "replyto": "l4noTuvYMP", "signatures": ["ICLR.cc/2026/Conference/Submission7700/Reviewer_QgqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7700/Reviewer_QgqW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762384163337, "cdate": 1762384163337, "tmdate": 1762919757134, "mdate": 1762919757134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}