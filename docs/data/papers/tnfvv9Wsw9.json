{"id": "tnfvv9Wsw9", "number": 15756, "cdate": 1758254923585, "mdate": 1759897284254, "content": {"title": "JURY-RL: Votes Propose, Proofs Dispose for Label-Free RLVR", "abstract": "Reinforcement learning with verifiable rewards (RLVR) enhances the reasoning of large language models (LLMs), but its scalability is hampered by the high cost of human-annotated labels. Label-free alternatives, such as majority voting or LLM-as-a-judge, are susceptible to false positives that lead to reward hacking and training collapse. We introduce JURY-RL, a label-free RLVR framework that separates answer proposal from reward disposal: votes from model rollouts propose a consensus answer, while a formal theorem prover disposes the final reward. Specifically, a rollout is rewarded only if the majority-voted answer is formally verified by a Lean prover. When verification is inconclusive, we activate our proposed ResZero (Residual-Zero) reward: it drops the unverifiable majority proposal and assigns a zero-mean, variance-preserving reward to the remaining (residual) answers. This design maintains a stable optimization gradient for RL algorithms without reinforcing spurious consensus. Experiments across mathematical reasoning, code generation, and multi-task benchmarks show that JURY-RL not only achieves more stable training but also consistently outperforms label-free baselines and even matches or surpasses supervised training with ground-truth rewards across pass@1 and pass@k.", "tldr": "We introduce JURY-RL, a 'votes propose, proofs dispose' paradigm for label-free reinforcement learning, designed to robustly align LLM reasoning with verifiable correctness without human labels.", "keywords": ["Reinforcement Learning", "Large Language Models", "Formal Verification", "Mathematical Reasoning", "Verifiable Rewards"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/076b41a4bcd93f2adfebd82c8a68286a8fb1a0d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces JURY-RL, a framework for label-free RLVR. A formal Lean theorem prover is used to dispose the reward by verifying whether the proposed answer is provably correct. If verification succeeds, the model is rewarded for correctness. When verification is inconclusive, the paper proposes a ResZero fallback reward, which maintains a zero-mean, variance-preserving gradient signal to stabilize rewards. Experiments across different benchmarks show that JURY-RL improves stability over self-reward and LLM-judge baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using Lean as a proof gate bridges RLVR with formal methods.\n\n2. The proposed zero-mean fallback reward stabilizes optimization even when verification fails.\n\n3. The experiments are broad, covering multiple backbones and domains. JURY-RL consistently outperforms other label-free baselines."}, "weaknesses": {"value": "1. The proposed pipeline uses a 32B auto-formalizer (8 candidates), a 32B consistency checker, and a 32B prover (16 proofs), making each rollout more expensive than LLM-as-Judge. Yet the performance gain is less than 3 points on average, questioning the cost–benefit balance.\n\n2. Although Lean is rigorous, the upstream translation and proof-generation models may introduce false positives or negatives. The paper does not quantify conversion accuracy, proof validity rates, or error propagation.\n\n3. The impact of each module (formalizer, checker, prover) is not isolated, making it unclear which contributes to the observed improvements."}, "questions": {"value": "1. What is the total verification cost (e.g. GPU hours/ LLM calls per rollout) compared to LLM-as-Judge? Can you provide a more detailed cost-benefit curve or analysis, e.g., the impact of different autoformalization/ proof candidate number on RL performance?\n\n2. Can you report the accuracy of auto-formalization and model generated Lean proofs? \n\n3. Can you justify why your method performance is better than the ground truth answer setting in table 1 and 2?\n\nI will increase the score if all the questions are well-justified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P1auhcPSYx", "forum": "tnfvv9Wsw9", "replyto": "tnfvv9Wsw9", "signatures": ["ICLR.cc/2026/Conference/Submission15756/Reviewer_kMKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15756/Reviewer_kMKV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708260362, "cdate": 1761708260362, "tmdate": 1762925990566, "mdate": 1762925990566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission proposes a hybrid of LLM-voting and label-free RLVR, where a committee proposes a derivation and then a formal theorem prover verifies it. When the proposed derivation cannot be verified, they fall back to a heuristic reward that achieves zero mean by assigning negative reward to the plurality answer and positive reward to other candidate answers. In this way it is possible to learn even from answers that cannot be verified.\n\nExperiments show that this approach improves significantly over LLM-as-judge, and also outperforms RLVR from ground truth rewards in most cases. Ablations suggest that simply assigning zero reward to unverified derivations does not work, so the proposed approach is justified empirically. Experiments also demonstrate the stability of the method (figure 2), the ability to preserve answer diversity (figure 3), and the impact of the main hyperparameter (figure 4)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important problem: relaxing the need for human annotation while preserving the validity of the annotations.\n- The proposed approach is mathematically sound and appears to be practical to implement\n- The motivation of scalability, truth-alignment, and optimization-stability is persuasive\n- The paper includes unusually comprehensive experimental validation, including ablations and robustness checks\n- The worked example in B.2 is helpful for understanding."}, "weaknesses": {"value": "It would be great to have more formal understanding of the ResZero reward. In particular, I don't have an intuition for why it is a good idea to  penalize the majority and amplifying the residuals in proportion to their frequency. It seems like this could even lead to an oscillatory behavior where two hypotheses alternate as the \"majority\" and top \"residual\".\n\nMinor: \"Majority\" typically refers to >50%, but here I think what is meant is \"plurality\""}, "questions": {"value": "- How is $\\overline{u}$ computed?\n- What happens in ResZero when all the samples are different answers? Do you pick a \"majority\" answer at random?\n- Can you please formally express the proposition being proved in A.2? \n- Table 2 shows that assigning zero reward to unverified answers is generally less effective than ResZero. What about assigning a zero-mean random reward?\n- How are the CIs computed for the averages in tables 1 & 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WeRMuQAGPp", "forum": "tnfvv9Wsw9", "replyto": "tnfvv9Wsw9", "signatures": ["ICLR.cc/2026/Conference/Submission15756/Reviewer_a7FE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15756/Reviewer_a7FE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854282757, "cdate": 1761854282757, "tmdate": 1762925989527, "mdate": 1762925989527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a novel label-free RL framework (JURY-RL) that leverage model-based Learn Prover system to assign psudo-label for majority responses. By leverage strong LLMs to parse rollouts and run multiple verification trials, JURY-RL achieves better reward F1 than LLM-as-a-Judge baseline. Moreover, the author propose a simple ResZero reward to assign valid learning signal for prompts which have non-conclusive majority response and found this simple reward improves both pass@1 and pass@k performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Evaluation results shows that JURY-RL gives better results than other label-free RL method baselines.\n2. Using a Lean verifier to assign psudo-label gives good reward quality.\n3. The proposed simple ResZero reward effectively improves pass@k and is easy to use."}, "weaknesses": {"value": "1. Evaluation: Avg@k results are required on benchmarks like AIME24, AIME25, the reliability is questionable. For example, at least 16 trials are required for AIME24.\n2. Lack important baseline like TTRL (NeurIPS 25').\n3. Efficiency: The author adopt a Pass@K verification setting, might causing 400 sec overhead, while the LLM-as-a-Judge baseline might only introduce 10 sec if also only evaluate the majority. Even when the LLM-as-a-Judge method rewards each response, 400 sec overhead budge can afford much more larger models like Qwen3-235B-A22B. The author might have to give more analysis on this efficiency comparison to demonstrate the effectiveness."}, "questions": {"value": "1. In Table 2, it is confusing that the Proof-Gate + MV setting  performs much worse on Qwen models compared with Majority-Voting baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gqcIBZCvE1", "forum": "tnfvv9Wsw9", "replyto": "tnfvv9Wsw9", "signatures": ["ICLR.cc/2026/Conference/Submission15756/Reviewer_tvcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15756/Reviewer_tvcp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014354816, "cdate": 1762014354816, "tmdate": 1762925988650, "mdate": 1762925988650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a type of hybrid reward that combines a verifiable reward with a label-free heuristic reward that is used when the output fails to be verified. Verifiable rewards (from a formal theorem prover) are highly reliable, but sparse. They can also be expensive to compute. Label-free heuristics (e.g., majority voting or LLM-as-a-judge) can give denser rewards, but are more unreliable. The \"Votes Propose, Proofs Dispose\" method proposed here attempts to leverage both of these by (a) only verifying the majority voted response, and then (b) if that fails, adding a fallback mechanism for assigning nonzero reward to the non-majority (unverified) responses based on a label free heuristic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem the paper addresses is significant, and a key practical challenge for LLM post-training. The proposed idea of using a cheap heuristic to filter candidates before applying an expensive verifier is natural. The fallback reward is also simple + intuitive (encouraging exploration of the next-highest-voted outcomes) that is zero-mean with non-zero variance."}, "weaknesses": {"value": "The main idea of hybridizing sparse rewards with denser, heuristic signals is natural and not particularly new. For example, process rewards achieve a similar goal, but are not discussed or compared to in the paper. Hybrid rewards in particular have also been explored before in Huang et al 2025. The empirical results are also not entirely compelling: in Table 1 in particular almost all of the results appear have confidence intervals that substantially overlap with other methods.\n\n[1] Huang et al 2025. Pitfalls of Rule- and Model-based Verifiers – A Case Study on Mathematical Reasoning. https://arxiv.org/pdf/2505.22203v1"}, "questions": {"value": "- Not all of the baselines appear to be defined. What is CoReward referring to?\n- The motivation for only validating the majority-voted answer is computational efficiency, but it's not clear exactly what is given up by this tradeoff. It would be nice to see how this method scales, for example, given a higher density of applied verified rewards (e.g. to top-k)."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "I might be wrong, but it seems like the margins have been significantly altered on this submission?"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B9Ovor3iQo", "forum": "tnfvv9Wsw9", "replyto": "tnfvv9Wsw9", "signatures": ["ICLR.cc/2026/Conference/Submission15756/Reviewer_sByP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15756/Reviewer_sByP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762249195440, "cdate": 1762249195440, "tmdate": 1762925988021, "mdate": 1762925988021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}