{"id": "xfwlgpe6st", "number": 9377, "cdate": 1758120574564, "mdate": 1759897728546, "content": {"title": "Unitary Convolutions for Message-passing and Positional Encodings on Directed Graphs", "abstract": "In many real-world networks, relationships are inherently directional, yet most graph neural networks (GNNs) assume undirected edges, and naïve adaptations of undirected GNNs to directed graphs amplify oversmoothing and gradient pathologies that cap model depth. Unitary graph convolutions (UniConv) provably prevent representational collapse and oversmoothing, but cannot incorporate edge directionality or edge features. In this paper, we introduce a **d**irected **un**itary GNN with **e**dge features (**Dune**), which retains these guarantees while overcoming UniConv’s limitations by incorporating edge directionality and edge features. Dune keeps gradient norms bounded at any number of layers, allowing it to benefit from neural network depth, unlike existing directed GNNs. The same unitary operator can be embedded in hybrid architectures with graph transformers, where its wavelike propagation supplies positional information and reduces the importance of random-walk or Laplacian-based encodings. We prove that Dune avoids exponential oversmoothing that plagues existing directed GNNs and empirically show that it achieves state-of-the-art performance on 12 directed-graph benchmarks while remaining trainable beyond 100 layers, improving performance by up to 18 percentage points over strong baselines. These results establish unitary convolutions as a scalable, geometry-aware foundation for deep learning on directed graphs. We make a preliminary version of our codebase available here.", "tldr": "We introduce a unitary graph neural network that can incorporate edge features and directionality, comes with theoretical guarantees, and works well in hybrid models with graph transformers.", "keywords": ["Graph neural networks", "directed graphs", "positional encodings", "graph transformers"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7d7040f33d6c2506ccd8e18040bf96f769912e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how to construct unitary matrices as aggregation operators on directed graphs within the existing UniConv framework. Inspired by the work of Lezcano-Casado et al., the authors adapt their approach to the field of graph learning and propose the Dune model. The method can be theoretically proven to avoid the over-smoothing problem. The authors provide experimental results on both node classification and graph regression tasks, demonstrating the superior performance of Dune compared with the baselines mentioned in the paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is overall well-written and easy to follow.\n- The authors provide a theoretical analysis to support their claims.\n- The model can achieve competitive performance on both homophilic and heterophilic graphs."}, "weaknesses": {"value": "- Complexity\n    - Due to the use of the exp(A) operation, the aggregation operator is dense, leading to excessively high model complexity and weak scalability on large-scale datasets.\n    - The authors should provide a runtime and memory comparison with simpler models, such as GCN.\n- Claim\n    - The paper claims to address the problem of stable training; however, it does not provide convincing results to substantiate this claim.\n    - The authors claim that their method enables model training with up to 100 layers. However, according to the ablation study, the performance of Dune decreases as the number of layers increases. The current experimental results do not demonstrate any clear advantage of such deep configurations.\n- Experiment\n    - In the over-smoothing comparisons, such as Dirichlet energy, the authors should at least include the performance of the baseline UniConv to demonstrate that the contribution regarding over-smoothing is novel, rather than an inherent property of the original framework.\n    - In the node classification experiments, the authors should consider including comparisons with models such as graph transformers.\n\n- All the figures in the paper are quite blurry. It is recommended that the authors replace them with higher-resolution versions."}, "questions": {"value": "- Given the high complexity of the proposed method, it is unclear how the authors conducted experiments on the SNAP-Patent dataset. Is a mini-batch training strategy employed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oy8V1DMoWy", "forum": "xfwlgpe6st", "replyto": "xfwlgpe6st", "signatures": ["ICLR.cc/2026/Conference/Submission9377/Reviewer_GaaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9377/Reviewer_GaaG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740862982, "cdate": 1761740862982, "tmdate": 1762920993193, "mdate": 1762920993193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Dune, a directed unitary GNN with edge features that extends UniConv, a NeurIPS 2024 paper entitled \"Unitary convolutions for learning on graphs and groups\". Unlike UniConv, Dune incorporates edge directionality and edge features while still preventing representational collapse and oversmoothing. Its unitary operator keeps gradient norms bounded across layers, enabling very deep architectures. Dune can also be combined with graph transformers, where its wave-like propagation provides positional information without relying on random-walk or Laplacian encodings. The authors prove Dune avoids exponential oversmoothing in directed GNNs and demonstrate state-of-the-art results on 12 benchmarks, with performance gains of up to 18 percentage points and stable training beyond 100 layers. This positions unitary convolutions as a scalable, geometry-aware approach for deep learning on directed graphs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Combines edge direction and edge features \n- Study shows that both unitary convolutions and edge directionality help\n- Detailed analysis, ablation studies"}, "weaknesses": {"value": "- Oversmoothing is addressed due to UniConv, so it does not come from the newly introduced approach\n- Edge feature modeling remains shallow\n- The computational overhead is significant and thus cannnot easily scale to large graphs. The authors mention that in the limitations Section. Nevertheless, it is a very important weakness and an approximated solution would definitely help the approach."}, "questions": {"value": "- Is Dune able to capture richer edge semantics (e.g. multi-relational edges, higher-order edge dependencies)?\n- Why does Dune work better for some datasets and not for all?\n\nSuggestions\n- Add a reference of Table 1 within the text, and state what is exactly shown there (e.g. expressivity)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LUHHJJiAqs", "forum": "xfwlgpe6st", "replyto": "xfwlgpe6st", "signatures": ["ICLR.cc/2026/Conference/Submission9377/Reviewer_kVV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9377/Reviewer_kVV9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947323984, "cdate": 1761947323984, "tmdate": 1762920992927, "mdate": 1762920992927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method that solves the key problems of oversmoothing and gradient when applying standard GNNs to directed graphs. It keeps gradient norms bounded at any number of layers, provably avoids oversmoothing and gradient pathologies, allowing it to be trained at depths beyond 100 layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel and elegant method—a Hermitian projection—to generalize unitary convolutions to asymmetric (directed) graphs\n2. It provably guarantees the proposed method avoids the exponential oversmoothing and vanishing/exploding gradients.\n3. This paper discusses the limitations of its computation cost."}, "weaknesses": {"value": "1. The mechanism for incorporating edge features is not as theoretically integrated as the core topological framework\n2. The matrix exponential is approximated with $T \\approx 10$. How sensitive is the model's performance to the order $T$ of the Taylor approximation? Could $T$ be significantly reduced (e.g., $T=2$ or $3$) to achieve a runtime closer to standard GNNs while still retaining the majority of the benefits over non-unitary models?\n3. Lack of experiments on both graphs with strong directionality and rich, meaningful edge features"}, "questions": {"value": "How sensitive is the model's performance to the order $T$ of the Taylor approximation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4ncCaxSLgN", "forum": "xfwlgpe6st", "replyto": "xfwlgpe6st", "signatures": ["ICLR.cc/2026/Conference/Submission9377/Reviewer_1L3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9377/Reviewer_1L3e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962125805, "cdate": 1761962125805, "tmdate": 1762920992603, "mdate": 1762920992603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Thank you to all the reviewers for thoughtful and insightful feedback! We are very pleased that reviewers are excited about the novel contributions of our work. Reviewers remark that our method is **“novel and elegant”** [1L3e], appreciate our **“extensive experiments on 12 directed-graph benchmarks”** [a2Gp] which “positions unitary convolutions as a **scalable, geometry-aware approach** for deep learning on directed graphs” [kVV9]. They further acknowledged that our model “**provably avoids exponential oversmoothing** and vanishing/exploding gradient problems” [78PN] and found the paper “**well-written** and **easy to follow**” [GaaG]. \n\nWe now highlight a few important points raised by reviewers that we address in our rebuttal:\n\n- **More node-classification baseline models** to compare against were requested by reviewers 78Pn and Gaag: in response, we have added GCN and SAGE with aggregation over only incoming edges as asked for by 78Pn, and two transformer-based GNNs [Gaag]. These results further support our claims that Dune outperforms non-unitary GNNs on node classification tasks on directed graphs.\n- Reviewers 78Pn and Gaag asked for **additional results** on datasets where very deep GNNs outperform shallow ones. To address this, we extended our ablations **on two long-range graph benchmark datasets**, which show that Dune performs best at extreme depths, and that its performance increases with the number of layers.\n- A **detailed ablation on the role of the hyperparameter $T$** in the Taylor expansion was requested by reviewer 1L3e. We now include ablations across $T \\in \\{2, 4, 6, 8, 10\\}$, clarifying the accuracy-efficiency trade-off and when lower $T$ is preferable.\n- Finally, reviewers 78Pn and Gaag wanted to see **statistics on the runtime and peak memory consumption** of our proposed Dune model. In response, we provide detailed statistics for various values of T on three node classification datasets, as well as a comparison with two popular directed GNNs.\n\nWe believe that these additions have made the paper significantly better and more accessible and thank the reviewers again for their input. Below, we address each of the reviewers’ comments and questions individually."}}, "id": "B7CV4UpIuG", "forum": "xfwlgpe6st", "replyto": "xfwlgpe6st", "signatures": ["ICLR.cc/2026/Conference/Submission9377/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9377/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9377/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763491831526, "cdate": 1763491831526, "tmdate": 1763491831526, "mdate": 1763491831526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Dune, a directed unitary GNN that incorporates edge directionality and edge features while preserving the stability guarantees of unitary graph convolutions. By leveraging unitary transformations, Dune maintains bounded gradient norms at arbitrary depths, enabling deep training without oversmoothing. It can also serve as a component in hybrid architectures (e.g., graph transformers), where its wavelike propagation implicitly encodes positional information, reducing the need for external positional encodings. Theoretical analysis proves Dune’s resistance to exponential oversmoothing and gradient explosion, while extensive experiments on 12 directed-graph benchmarks demonstrate state-of-the-art performance and scalability beyond 100 layers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1: The asymmetric adjacency matrix of a directed graph is projected into a Hermitian form, making the exponential operator exp unitary. Following the UniConv paradigm, this leads to a new directed graph convolution framework.\n\nS2: A key property of unitary transformations is that they inherently prevent oversmoothing, gradient explosion, and vanishing during message passing. This has been theoretically proven, enabling the effective training of directed GNNs at very deep layer depths.\n\nS3: The experiment examines whether performance gains in directed message passing arise from the convolution mechanism itself or from modeling edge directionality. It also investigates whether the proposed Dune can achieve an effect comparable to positional encoding in capturing geometric structural information."}, "weaknesses": {"value": "W1: Although unitary transformations are theoretically associated with stability, the paper provides little quantitative analysis of computational efficiency (e.g., FLOPs, parameter count) compared with baseline GNNs such as GCN or GraphSAGE, or with other stability-oriented methods like residual connections and normalization. This gap limits the assessment of the model’s practical scalability.\n\nW2: While the paper presents many theoretical analyses, it would benefit from more intuitive explanations or illustrative examples to help readers better understand the proposed method.\n\nW3: The claimed contribution on positional encoding seems only marginally different from existing approaches, and the paper would benefit from a more precise articulation of its advantages."}, "questions": {"value": "Q1: The theoretical proof of oversmoothing avoidance primarily considers static, homophilic graphs. How does this guarantee extend to heterophilic or dynamic graphs? Could the authors provide supplementary analyses or theoretical bounds for these cases?\n\nQ2: Can the proposed Dune model effectively alleviate the oversquashing problem in graph learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hFM4BJcVgl", "forum": "xfwlgpe6st", "replyto": "xfwlgpe6st", "signatures": ["ICLR.cc/2026/Conference/Submission9377/Reviewer_a2Gp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9377/Reviewer_a2Gp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762534137062, "cdate": 1762534137062, "tmdate": 1762920992307, "mdate": 1762920992307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel GNN architecture, namely Dune, designed for directed graphs with edge features through the use of a unitary convolution which provably prevents oversmoothing and exploding/vanishing gradients, allowing for stable training of deep GNNs. Additionally, the authors propose using Dune in hybrid architectures, reducing excessive reliance of graph transformers to positional encodings."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The model provably avoids exponential oversmoothing and vanishing/exploding gradient problems\n2. The paper extends the concept of unitary convolutions to the more complex case of directed graphs"}, "weaknesses": {"value": "1. The paper is difficult to read. Several key concepts are introduced at a very high level:\n    - Section 4.3 is particularly unclear. It states that Dune can be used in a hybrid model but it is not explained how. I think the node embeddings from the previous layer are first passed through a Dune message-passing step and then the model performs attention over those (while the PE is only added at the first layer), but this is not explained.\n    - Overall, the paper is missing intuitive explanations of concepts. For instance, an intuitive explanation of the method, even on a simple 3 node graphs could significantly improve readability.\n2. The model is motivated by its stability at extreme depths (100+ layers in Fig. 3). However, the optimal hyperparameter settings for the main benchmark results (Table 10) use only 6-24 layers. This either means that the chosen datasets do not require such depths, and therefore do not showcase the capabilities of the model, or that the model cannot really work with significantly large depths. Assuming it is the former, you should test on datasets where a very large number of layers is necessary for obtaining good results.\n3. The empirical evaluation is missing a simple baseline, that is, a standard GCN/SAGE that only aggregates incoming edges (e.g., using $D_{in}^{-1}A$). The paper's \"undirected\" baselines are weaker as they are forced to use an undirected graph. \n\n**Additionally, note that there is some font issue as the section titles appear to have a different font than the standard iclr template.**"}, "questions": {"value": "1.  In the introduction's \"common strategies\" why not mention the simple approach of aggregating only incoming edges?  This also impact the results because in the experiments you can use a simple gcn baseline that only aggregates incoming edges, without converting the graph into an undirected one.\n2. Are you assuming multiple convolutions are stacked one after the other with non-linearities in between? Cause equation 1 cannot represent an entire gnn, but it should be seen as a single gnn layer, with multiple layers interleaved by non linearities, This should be made clear.\n3. How does your work compare to Eliasof et al., AAAI 2024. Feature Transportation Improves Graph Neural Networks, which shows that including a direction behavior mitigates oversmoothing?\n4. The expressivity proof (Proposition 3/10) requires injective non-linearities and an injective final readout. Were these conditions met in the experiments? \n5. What is the wall-clock time, memory overhead and complexity analysis of your method, especially when compared to the standard sparse-matrix multiplication used in baseline gnn methods and Dir-GCN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IyXfozoxkc", "forum": "xfwlgpe6st", "replyto": "xfwlgpe6st", "signatures": ["ICLR.cc/2026/Conference/Submission9377/Reviewer_78Pn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9377/Reviewer_78Pn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762561027836, "cdate": 1762561027836, "tmdate": 1762920992016, "mdate": 1762920992016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}