{"id": "AhQFDBBIRZ", "number": 10367, "cdate": 1758168410927, "mdate": 1759897655677, "content": {"title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents", "abstract": "Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools—and more importantly, whether they can integrate them with general-purpose search to solve complex tasks.\nTherefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality.\nExperiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2\\% accuracy and 67.5\\% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4\\% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling.", "tldr": "", "keywords": ["LLM Agents", "Tool-Augmented Agents", "Model Context Protocol"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/206fec2424125df2c5e788b3d725de82c632335e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces InfoMosaic-Bench, a new benchmark for evaluating the ability of AI agents to perform multi-source information seeking. To construct the benchmark, the paper proposes InfoMosaic-Flow, a two-stage (generation and refinement) agentic pipeline that synthesizes 621 tasks across 6 domains (bio, finance, web, maps, video, and multi-domain). The authors conducted extensive experiments on 14 models using a custom agent scaffold."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an innovative attempt to generate multi-source information retrieval tasks using a two-stage agentic pipeline.\n\n2. The work reflects a substantial effort, encompassing 621 tasks, 6 domains, 77 tools, and a comprehensive evaluation of 14 models.\n\n3. The authors have open-sourced the code, promoting reproducibility."}, "weaknesses": {"value": "1. The evaluation is limited to a single, custom agent scaffold. The choice of agent architecture could significantly influence performance, and this variable is not explored.\n\n2. I have concerns about the dataset's quality. While the paper mentions quality assurance through human annotation (on a sampled subset), it lacks a human performance baseline. I am particularly skeptical about answer uniqueness and whether questions might have multiple valid answers.\n\n3. The paper appears rushed, which affects its clarity. The writing is often difficult to follow, and the core methodology—the data synthesis pipeline—is not explained in a way that is easy to comprehend. There are also several writing issues:\n\t- L255: \"serious\" should be \"series\".\n\t- L341 (minor): \"Open-sourced Model\" is better described as \"Open-weight Model\".\n\t- L675: Figure 5 shows t-SNE visualization of query in different domains, but finance domain is missing.\n\t- L810: Table 8's example is clearly not from Bio domain as it states.\n\t- L829: Table 9's example is in Chinese with no English translation, while other examples are translated."}, "questions": {"value": "1. What portion of the dataset is in Chinese? This information is not mentioned in the main paper, only alluded to in the appendix.\n\n2. Did you conduct an ablation study where the agent has no access to any tools (including web search)? It seems some of the questions may not require any tool use to answer.\n\n3. During the web search experiments, did you encounter any issues with rate limiting from the search engines?\n\n4. The choice of domains seems somewhat arbitrary. What was the rationale for selecting domains as disparate as biology and finance, alongside more general ones like web and video?\n\n5. The results for the web domain are surprising: Claude-4-Sonnet achieved only 3% accuracy, while GLM-4.5 reached 11%. Have you analyzed the reasons for this performance discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HrLUcFlxfK", "forum": "AhQFDBBIRZ", "replyto": "AhQFDBBIRZ", "signatures": ["ICLR.cc/2026/Conference/Submission10367/Reviewer_EoM5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10367/Reviewer_EoM5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760934558211, "cdate": 1760934558211, "tmdate": 1762921691283, "mdate": 1762921691283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces InfoMosaicBench – an evaluation benchmark to assess the abilities of LLM agents to solve information-seeking tasks by gathering evidence from multiple domains/sources. The paper proposes InfoMosaic-Flow pipeline to synthetically curate their data. \n- Furthermore, the paper benchmarks 14 LLMs in an agentic scaffold with only web-search and report poor task performance, highlighting the importance of domain-specific tools for multi-source information seeking queries. Even when given domain-specific tools, the performance improvements are inconsistent and achieved only for 3/6 domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The analyses are very insightful and provide more nuanced insights into various failure modes of agentic systems.\n- The experiments are very comprehensive and the hypotheses are thoroughly tested to empirically validate their claims."}, "weaknesses": {"value": "- Limited real-world grounding: Are the tasks in this benchmark actually representative of real-world multi-source information-seeking tasks? The examples given in Appendix A.9.1 seem extremely complex (and somewhat obvious that they were synthetically crafted), and I am not very convinced that any human user would actually craft such complicated queries for this task.\n- Understanding the choice of tools: How do authors decide which tools to include/not include for each domain? The provided list in the appendix shows extremely fine-grained tools which are only likely to be available in extremely specialized agents designed for a particular task/domain. Do these tools have any bias on the nature of tasks you synthetically curate from your InfoMosaicFlow pipeline?\n- Consistency of claims: The results in Table 3 do not support the overall claim about agents with web search being incapable of solving these tasks. If that was indeed true, domain-specific tools would yield consistent improvements across all 6 domains which is not the case. Thus, is the poor task performance arising from the absence of domain-specific tools or is it just because of the overall difficulty and complexity of the task?\n- Lack of insights for future work: While the paper provides an analysis of failure modes, there is limited novel insight into further agent design."}, "questions": {"value": "Please address each of the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hiAHh5ddLt", "forum": "AhQFDBBIRZ", "replyto": "AhQFDBBIRZ", "signatures": ["ICLR.cc/2026/Conference/Submission10367/Reviewer_w89X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10367/Reviewer_w89X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952785808, "cdate": 1761952785808, "tmdate": 1762921689877, "mdate": 1762921689877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents InfoMosaic-Bench, a benchmark of 621 problems across six domains designed to test whether agents can combine general web search with domain-specific tools exposed via MCP. Tasks are synthesized with a novel pipeline called InfoMosaic-Flow, an organizer–workers pipeline that grounds conditions in verified tool outputs and iteratively prunes web-search shortcuts. Evaluating 14 LLM agents, the authors find: (i) web-only agents underperform, (ii) domain tools help selectively (not uniformly), and (iii) 22.4% of failures stem from tool selection/usage errors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality. First benchmark that systematically evaluates multi-source information seeking with real MCP tools, bridging a gap between web-only search benchmarks and API-centric invocation tests. Organizer–workers InfoMosaic-Flow is a thoughtful synthesis design that enforces cross-source dependencies. \n\n2. Quality. Clear two-stage generation with web-evolving verification and automated + manual quality control; ablations show verification is necessary to remove trivial cases. Well-scoped metrics (Accuracy, Pass Rate) and broad model coverage; analyses include scaling with tool calls and error taxonomies. Human study also supports data quality and reliability). \n\n3. Significance. Results quantify a core limitation of current agents: integrating domain tools with web search; even top closed models underperform on multi-source tasks, signaling an impactful research direction."}, "weaknesses": {"value": "1. Per-domain scale is modest. With 621 items across 6 domains (e.g., 83 medical, 100 finance/video), per-domain statistics can be noisy. Ideally should expand to a larger dev/test split per domain. \n\n2. Judge dependence. Accuracy uses an LLM judge, which can introduce bias. Lack inter-judge agreement, including rule-based scorers where feasible (e.g., exact match for structured answers), and publish judge prompts. \n\n3. Tool-budget sensitivity. Tool call limit fixed at K=20; conclusions may change with higher budgets or adaptive stopping. Lack sensitivity curves for K and latency/throughput trade-offs."}, "questions": {"value": "1. How strict is the multi-source requirement post-refinement? Could you quantify the fraction of items where each condition is necessary and show a “single-source solvability” audit on a held-out judge? \n\n2. Human study details. Could you share rating guidelines and per-domain κ? Also, does κ remain high on borderline cases that web-search could partially solve? \n\n3. Latency & cost reporting. Any measurements of wall-clock, tokens, or tool latency per domain—and how those change under domain-tool vs. web-only settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sWC4gwEIJc", "forum": "AhQFDBBIRZ", "replyto": "AhQFDBBIRZ", "signatures": ["ICLR.cc/2026/Conference/Submission10367/Reviewer_hZCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10367/Reviewer_hZCF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970317780, "cdate": 1761970317780, "tmdate": 1762921689343, "mdate": 1762921689343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a benchmark to test multi-source information seeking by tool-augmented LLM agents. It comprises 621 query tasks spanning six domains and provides 77 specialized MCP tools for these domains. Each task requires combining general web search with one or more domain-specific tool calls. Tasks are automatically synthesized via a pipeline, which uses an “organizer-workers” LLM architecture to propose coherent scenarios grounded in verified tool outputs. Evaluation results on 14 LLMs  results show that web search alone yields very low accuracy. Adding domain tools only yields selective gains (improving results in map/video but degrading others) and about 22.4% of failures were traced to incorrect tool selection or usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- InfoMosaic-Bench explicitly requires agents to integrate evidence from both web search and diverse tools. No existing benchmark systematically tests this combination. \n- Evaluations cover 77 MCP tools across 6 different domains.\n- Tasks are explicitly designed so that no single tool or search can solve them in one step."}, "weaknesses": {"value": "- Because all tasks are synthetically generated, the dataset may not accurately reflect the real distribution of human information-seeking queries.\n- Some conclusions may over-generalize. The reported improvement from tool use in map and video tasks, and degradation in medical and financial ones, likely reflects differences in the specific tools provided and the complexity of their parameterization rather than inherent domain effects.\n- The dataset has uneven domain sizes (e.g., 135 map vs. 90 biomedical tasks) and mixes Chinese and English, with varied text lengths and complexity. While this adds realism, it complicates cross-domain comparisons and may introduce language-related bias."}, "questions": {"value": "- The current evaluation is binary at the task level. Would it be feasible to design a metric that captures partial correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9gpYD3X8aw", "forum": "AhQFDBBIRZ", "replyto": "AhQFDBBIRZ", "signatures": ["ICLR.cc/2026/Conference/Submission10367/Reviewer_S6ix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10367/Reviewer_S6ix"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982140697, "cdate": 1761982140697, "tmdate": 1762921688692, "mdate": 1762921688692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}