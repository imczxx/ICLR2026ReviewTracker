{"id": "oeWqDrTb38", "number": 5661, "cdate": 1757926222227, "mdate": 1759897962357, "content": {"title": "Object-Centric Refinement for Enhanced Zero-Shot Segmentation", "abstract": "Zero-shot semantic segmentation aims to recognize, pixel-wise, unseen categories without annotated masks, typically by leveraging vision-language models such as CLIP. However, the patch representations obtained by the CLIP's vision encoder lack object-centric structure, making it difficult to localize coherent semantic regions.\nThis hinders the performance of the segmentation decoder, especially for unseen categories. To mitigate this issue, we propose object-centric zero-shot segmentation (OC-ZSS) that enhances patch representations using object-level information. \nTo extract object features for patch refinement, we introduce self-supervision-guided object prompts into the encoder. These prompts attend to coarse object regions using attention masks derived from unsupervised clustering of features from a pretrained self-supervised~(SSL) model. Although these prompts offer a structured initialization of the object-level context, the extracted features remain coarse due to the unsupervised nature of clustering. To further refine the object features and effectively enrich patch representations, we develop a dual-stage Object Refinement Attention (ORA) module that iteratively updates both object and patch features through cross-attention. Last, to make the refinement more robust and sensitive to objects of varying spatial scales, we incorporate a lightweight granular attention mechanism that operates over multiple receptive fields. OC-ZSS achieves state-of-the-art performance on standard zero-shot segmentation benchmarks across inductive, transductive, and cross-domain settings.", "tldr": "We propose an object-centric framework to improve zero-shot segmentation.", "keywords": ["Zero-Shot Learning", "Vision-Language models", "Semantic Segmentation", "Computer Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2eb4b51547e8acb91868f5bd838ad721166d0131.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes OC-ZSS, a method for zero-shot semantic segmentation that injects object-centric priors into a frozen CLIP visual encoder. It introduces object prompts guided by self-supervised DINO features, which generate attention masks corresponding to distinct objects. These prompts guide CLIP’s attention toward object-centric regions.\nA novel Object Refinement Attention (ORA) module is designed to iteratively refine both object and patch features in a two-stage manner (object → patch). Additionally, granular attention integrates multi-scale receptive fields via dilated depthwise convolutions to improve scale robustness.\nExtensive experiments on PASCAL VOC, PASCAL Context, and COCO-Stuff show state-of-the-art performance across inductive, transductive, and cross-dataset settings while maintaining low parameter and compute overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Originality: The idea of dynamically generated, self-supervised object prompts is novel and effectively bridges self-supervised and multimodal learning.\n2.Empirical quality: SOTA performance on inductive, transductive, and cross-dataset tasks; solid ablations and visualizations (cleaner masks, sharper object boundaries).\n3.Clarity: The modular structure (Object Prompt + ORA + Granular Attention) is well-motivated and easy to follow.\n4.Significance: Achieves high accuracy with only 27M parameters and 64 GFLOPs, showing strong potential for practical deployment and low-supervision adaptation."}, "weaknesses": {"value": "1.Dependence on SSL masks: The approach relies heavily on DINO-generated clusters. Performance robustness across different SSL models and clustering hyperparameters (K, no) could be further analyzed.\n2.Limited theoretical insight: The convergence and dynamics of the iterative ORA process are not deeply analyzed. More discussion on when ORA may fail (e.g., ambiguous or overlapping objects) would strengthen the paper.\n3.Evaluation granularity: Results on small/overlapping objects or fine-grained categories are underexplored.\n4.Comparisons with stronger backbones: Although experiments with DINOv2-Reg and other CLIP variants are provided, additional integration with high-capacity decoders could better reveal the upper bound and generality of OC-ZSS."}, "questions": {"value": "1.Choice of object prompt number (no): How is the number of object prompts determined? Could it be dynamically adapted based on image complexity or feature entropy?\n2.ORA iteration steps (S): Why is S = 4 chosen as default? Could an adaptive stopping criterion (e.g., convergence of feature distance) reduce computation?\n3.Handling overlapping/merged objects: How robust is the method when DINO clusters merge multiple objects into one? Is there any mechanism to re-estimate or split masks during refinement?\n4.Backbone transferability: How does OC-ZSS behave when plugged into stronger backbones (e.g., DINOv2-Large, EVA-CLIP)?\n5.Runtime analysis: Can the authors provide wall-clock inference times (ms / image) and throughput comparison with OTSeg or CLIP-RC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "207ORGE7tK", "forum": "oeWqDrTb38", "replyto": "oeWqDrTb38", "signatures": ["ICLR.cc/2026/Conference/Submission5661/Reviewer_JToE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5661/Reviewer_JToE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654290092, "cdate": 1761654290092, "tmdate": 1762918180799, "mdate": 1762918180799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OC-ZSS, a novel framework that builds on OTSeg’s framework and adds an object-centric front-end: DINO features are clustered to form attention masks that gate object prompts toward coherent patch groups in a frozen CLIP encoder; features are then iteratively refined by ORA before entering the OTSeg-style decoder. Results are solid across protocols, with especially strong numbers in the very-few-seen setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The modular design of the newly proposed components should be straightforward to adapt to existing CLIP-based frameworks. The experiments are thorough, and the qualitative results align with the object-centric motivation. The ablation studies are helpful and generally consistent."}, "weaknesses": {"value": "1. One of the main proposed components is the SSL DINO encoder for attention mask generation. This adds significant memory and computation costs. However, the authors reported this part in a somewhat tricky way: Params/GFLOPs and throughput are reported with DINO features pre-extracted and with GPU-side tensor indexing, so the cost of **online** DINO inference + clustering/masking isn’t in the headline numbers. That makes the method look closer to OTSeg than an end-to-end deployment would. The inference of DINO is likely to be costly in some deployment environments where special GPU acceleration algorithms are not available.\n2. When the attention mask for object prompts is off, performance looks about the same as OTSeg; it is possible that the main performance gain comes from the strong semantics of the DINO encoder rather than other proposed components.\n3. The results in Table 5 are compelling, but they don’t show readers where the few-seen gains come from. A reasonable guess is that DINO’s semantics carry a lot of the weight there, but the paper doesn’t actually test that. Running an ablation study under this setting would be very helpful for more insights."}, "questions": {"value": "1. It would be helpful to also report the computational costs of online end-to-end inference, including the DINO feature extraction and clustering/masking.\n2. The improvements in the very-few-seen setup are impressive, and it would be very helpful to provide more insights into what contributes to this gain. An ablation study under this setup would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jdDWMKqPa6", "forum": "oeWqDrTb38", "replyto": "oeWqDrTb38", "signatures": ["ICLR.cc/2026/Conference/Submission5661/Reviewer_wdgB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5661/Reviewer_wdgB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899377796, "cdate": 1761899377796, "tmdate": 1762918180578, "mdate": 1762918180578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper Object-Centric Zero-Shot Segmentor (OC-ZSS) proposes a framework to enhance zero-shot semantic segmentation by introducing object awareness into CLIP’s patch representations. While CLIP aligns global image and text features well, it lacks fine-grained correspondence between visual regions and textual concepts, limiting segmentation accuracy for unseen categories.\n\nOC-ZSS addresses this by injecting self-supervision-guided object prompts into the CLIP encoder. These prompts, informed by unsupervised clustering from a pretrained self-supervised model such as DINO, attend to distinct object regions through masked attention. This enables the extraction of coarse object-level features without annotations or encoder finetuning.\n\nTo refine these features, the method introduces a dual-stage Object Refinement Attention (ORA) module, which iteratively updates both object and patch representations through cross-attention. This mutual refinement strengthens semantic grouping and improves text alignment while keeping the CLIP backbone frozen. To handle objects of varying scales, OC-ZSS further employs a granular attention mechanism using multi-scale atrous convolutions, allowing more precise modeling of object structures.\n\nThe approach demonstrates competitive performance across standard zero-shot segmentation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and well-structured, effectively highlighting key challenges in zero-shot semantic segmentation and addressing them through an object-centric refinement approach. It maintains clarity and logical flow while presenting a focused and well-motivated solution that enhances region-level semantic alignment and segmentation accuracy for unseen categories.\n\n#\n\n1. The paper takes a principled step toward introducing object-centric reasoning into zero-shot segmentation, addressing a key shortcoming of CLIP-based models that rely only on global or patch-level embeddings without structured object awareness.\n\n#\n\n2. The proposed framework adaptively refines patch features using object-level cues, enabling it to effectively handle objects of diverse shapes, scales, and spatial layouts, a capability that some existing CLIP-based segmentation models lack.\n\n#\n\n3. The use of self-supervised clustering from a frozen encoder to guide object prompts provides a label-efficient way to utilize the inherent structure of visual features for identifying coarse object regions without requiring annotations or fine-tuning.\n\n#\n\n4. The dual-stage Object Refinement Attention (ORA) module enables bidirectional interaction between patch and object features, allowing their representations to be iteratively refined for improved semantic consistency. The inclusion of multi-scale granular attention, implemented through depthwise separable atrous convolutions with different dilation rates, helps capture contextual information across multiple receptive fields.\n\n#\n\n5. The paper includes experiments across multiple benchmarks and settings (inductive, transductive, cross-domain), showing consistent performance improvements."}, "weaknesses": {"value": "1. The degree of novelty is moderate, as the method primarily combines established components such as self-supervised clustering, cross-attention refinement, and multi-scale feature aggregation. Moreover, the paper lacks a deeper analysis of why mutual refinement improves object-centricity or generalization in zero-shot settings, offering empirical validation without corresponding conceptual depth or theoretical analysis.\n\n#\n\n2. The approach relies on features from a pretrained self-supervised model, which may limit its performance if the external model (e.g., DINO) does not provide strong object cues. If the DINO-based clustering fails to correctly identify an object (e.g., over-segments a single large object or mistakenly merges two separate small objects), this error is propagated and refined by ORA. The refinement module can only improve the representation given the clusters it receives; it cannot fundamentally fix a poor initial object grouping. This makes the entire pipeline vulnerable to the inherent biases and failure modes of an un-analysed, unsupervised upstream process.\n\n#\n\n3. The granular attention mechanism (Section 3.4) shows an architectural inconsistency by applying CNN-style atrous convolutions to transformer patch embeddings without proper adaptation. Since atrous convolutions rely on contiguous spatial neighborhoods, their use on non-contiguous transformer patches may limit effectiveness. The paper also omits details on dilation rates and lacks comparisons with transformer-native multi-scale approaches, making this design choice appear misaligned with transformer principles and weakening the claim of a lightweight refinement module.\n\n#\n\n4. The use of entropy-regularized Sinkhorn attention introduces sensitivity to the regularization parameter ε and iteration count.\nWhile smaller ε leads to gradient instability and near-discrete couplings, larger ε causes over-smoothing and loss of discriminative structure in the transport plan (Cuturi, 2013; Genevay et al., 2018; Peyré & Cuturi, 2019).\nMoreover, the method incurs iterative additional computational cost and requires log-domain stabilization to prevent numerical underflow. These factors raise concerns about training stability, computational efficiency, and generalization sensitivity compared to standard softmax attention.\n\n      [1] Cuturi, M. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. NeurIPS 2013.\n\n      [2] Genevay, A., Peyré, G., & Cuturi, M. Learning Generative Models with Sinkhorn Divergences. AISTATS 2018.\n\n      [3] Peyré, G., & Cuturi, M. Computational Optimal Transport. FnT in ML 2019.\n\n#\n\n5. The paper claims to be \"parameter-efficient\" and \"lightweight,\" yet:\n\n\n     a) Requires running an additional frozen DINO encoder for every input image.\n\n\n\n     b) Adds Voronoi clustering operations for mask generation.\n\n\n\n     c) Performs 4 iterative dual-stage refinements.\n\n#\n\n6. No comparison with foundation models (SAM, recent large-scale foundation models).\n\n     [1] SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding, Wang et. al, CVPR 2024, Workshops.\n\n\n#\n\n7. Missing comparisons with closely related open-vocabulary segmentation/ZSS methods (CLIP-DINOiser, ProxyCLIP, OVSegmentor, FOSSIL) that also combine CLIP with self-supervised features. The paper mentions these methods (lines 130 - 154) to distinguish its approach conceptually, but provides no quantitative benchmarking against them. This omission makes it difficult to assess whether the proposed dual-stage object refinement genuinely outperforms simpler CLIP+DINO fusion strategies or end-to-end trained alternatives.\n\n#\n\n8. The performance gains over the baseline (OTSeg) are often modest despite nearly doubling the parameter count (27.2M vs 13.8M).\n\n#\n\n9. It would strengthen the paper to include a numerical or methodological comparison with the following related approaches, and to cite them appropriately:\n\n     a) Delving into Shape-aware Zero-shot Semantic Segmentation, Liu et. al, CVPR 2023.\n\n     b) CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free, Wysoczanska et. al, WACV 2024.\n  \n     c) Refining CLIP's Spatial Awareness: A Visual-Centric Perspective, Qiu et. al, ICLR 2025.\n\n     d) A Simple Framework for Open-Vocabulary Zero-Shot Segmentation, Stegmüller et. al, ICLR 2025.\n\n#\n\nMinor:\n\n1. Sensitivity to hyperparameters, such as the number of clusters and choice of feature layer in the self-supervised model, is not thoroughly analyzed, leaving questions about robustness across configurations.\n\n#\n\n2. The method focuses on segmentation benchmarks but lacks evaluation on broader vision-language tasks where object-centric refinement might also be relevant. Below would strengthen the claim of generalization across varied scene structures.\n\n     a) ADE20K \n\n     b) Cityscapes\n\n#\n\n3. Cross-dataset generalization (Table 3): Gains are minimal (0.5% on Pascal Context, 0.4% on VOC).\n\n#\n\n4. How were the dilation rates {r1, r2, r3, r4} chosen? Whether they are fixed or data-driven is not clear. \n\n#\n\n5. Line 132, FOSSIL reference is missing."}, "questions": {"value": "Please see the comments in the Weaknesses section.\n\nAdditional:\n\n1. Please discuss following design choices:\n\n\n     a) Number of object prompts: 6 for VOC/COCO, 8 for Pascal Context.\n\n\n      b) Refinement iterations: 4 for most datasets, but 5 for Pascal Context.\n\n\n      c) Clustering parameters not thoroughly analyzed.\n\n\n      d) Training is conducted for 20K, 40K, and 80K iterations on VOC 2012, PASCAL Context, and COCO-Stuff, respectively.\n\n#\n\n2. Why dual-stage? No explanation for why object refinement must precede patch refinement.\n\n#\n\n3. Why GRU specifically? Borrowed from slot attention without justification.\n\n#\n\n4. Why these attention patterns? The masked attention prevents object prompts from seeing the full image that can hurt global context.\n\n#\n\n5. No analysis of what makes patches \"object-centric\" beyond qualitative clustering visualizations (Figure 5 and 7).\n\n#\n\n6. Missing ablation on number of object prompts.\n\n# \n7.  The code or implementation has not been released, which limits reproducibility, as several technical details in the paper remain underspecified and cannot be fully verified or replicated without access to the source code.\n\n#\n\n8. Several implementation aspects remain ambiguous, particularly the masking semantics, GRU sharing strategy, and multi-scale convolution ordering. Clarifying these would improve reproducibility and theoretical transparency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MMBB2dASc7", "forum": "oeWqDrTb38", "replyto": "oeWqDrTb38", "signatures": ["ICLR.cc/2026/Conference/Submission5661/Reviewer_3xo5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5661/Reviewer_3xo5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974803150, "cdate": 1761974803150, "tmdate": 1762918180311, "mdate": 1762918180311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}