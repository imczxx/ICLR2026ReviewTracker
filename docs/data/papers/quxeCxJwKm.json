{"id": "quxeCxJwKm", "number": 19840, "cdate": 1758299878197, "mdate": 1759897016644, "content": {"title": "Shrinking Proteins with Diffusion", "abstract": "Many proteins useful in modern medicine or bioengineering are challenging to make in the lab, fuse with other proteins in cells, or deliver to tissues in the body because their sequences are too long.\nShortening these sequences typically involves costly, time-consuming experimental campaigns.\nIdeally, we could instead use modern models of massive databases of sequences from nature to learn how to propose shrunken proteins that resemble sequences found in nature.\nUnfortunately, these models struggle to efficiently search the combinatorial space of all deletions, and are not trained with inductive biases to learn how to delete.\nTo address this gap, we propose SCISOR, a novel discrete diffusion model that deletes letters from sequences to generate protein samples that resemble those found in nature.\nTo do so, SCISOR trains a de-noiser to reverse a forward noising process that adds random insertions to natural sequences.\nAs a generative model, SCISOR fits evolutionary sequence data competitively with previous large models.\nIn evaluation, SCISOR achieves state-of-the-art predictions of the functional effects of deletions on ProteinGym.\nFinally, we use the SCISOR de-noiser to shrink long protein sequences, and show that its suggested deletions result in significantly more realistic proteins and more often preserve functional motifs than previous models of evolutionary sequences.", "tldr": "We propose a discrete diffusion model that can learn to shrink proteins for bioengineering and medicinal applications.", "keywords": ["Proteins", "Generative model", "diffusion", "discrete diffusion"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2a87eacfc4886d687ed317e9f62dca3a3ec4d95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SCISOR, a discrete diffusion model for shortening proteins by learning to delete residues. The forward process is insertion-only; the reverse denoiser learns to plan deletions that yield sequences resembling natural proteins. A dynamic-programming alignment counting algorithm marginalizes over insertion paths and produces a tractable training objective. Empirically, SCISOR is a competitive protein generative model, achieves SOTA deletion effect prediction on ProteinGym, and, when shrinking real proteins, better preserves predicted foldability and functional motifs than strong baselines. Code and additional analyses are provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear, principled formulation of deletion-learning diffusion. The insertion-only forward / deletion reverse construction is elegant and purpose-built for length reduction, and it is distinct from prior substitution-only diffusion models in sequence generation. \n2. Strong deletion-specific evidence. SCISOR reaches SOTA on ProteinGym deletion effect prediction and shows consistent wins in structure/motif preservation during shrinking.  \n4. Useful case study and qualitative analyses. The RalA example and the comparison to deletions observed in natural alignments help interpret what the model is deleting. \n5. Implementation details, ablations, and code release are documented."}, "weaknesses": {"value": "1. **Practical claim (“preserve function while shrinking”) is only partially supported.** \nEvidence for function preservation relies mainly on (i) ProteinGym deletion assays, which typically remove very few sites per sequence, and (ii) structure-centric proxies (pLDDT/TM/RMSD) for larger shrinkage experiments. These are helpful signals but do not fully validate functional retention at scale. The paper does measure functional-site enrichment, which is a step in the right direction, but this is confined to annotated motifs and could be expanded. Overall, the story is stronger on “planning deletions” than on “confirmed function after large deletions.” \n\n2. **Evaluation granularity for shrinking could be deepened.** \nFor the 200-sequence study, instead of global pLDDT/TM/RMSD curves, more  function-proximal evidence could also be considered in the main paper: e.g., family-stratified analysis of active site/binding pocket conservation, ligand-contact residue retention, or catalytic triad/metal-coordination integrity. \n\n3. **Distribution shift between training noise and inference use-case.** \nThe model is trained to reverse insertions into natural sequences, whereas shrinking deletes from a real protein directly. While the authors acknowledge limitations and future directions; some empirical evidence on the robustness would be helpful (e.g., proteins with long disordered tails vs compact cores)\n\n4. **Compute at inference scales with deletion count.** \nThe paper discusses efficient sampling, but more wall-clock comparisons against strong non-diffusion baselines (e.g., ProGen2-based pruning strategies) on long proteins would clarify practicality."}, "questions": {"value": "1. Would it be possible to include residue-level functional analyses for larger deletions—for example, checking whether annotated catalytic, binding, or interface residues are retained after shrinking, and whether their local structural geometry is preserved?\n2. Are there examples where the model maintains good global pLDDT/TM-score but loses key functional residues or pocket geometry? Showing a few such cases would help clarify the limits of the method and when shrinking becomes unsafe.\n3. The number of inference steps scales with the number of planned deletions. When would this becomes an issue in practice, i.e., what's the computational cost comparison of different methods in terms of different protein length and deletion length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Up3AqUw8YO", "forum": "quxeCxJwKm", "replyto": "quxeCxJwKm", "signatures": ["ICLR.cc/2026/Conference/Submission19840/Reviewer_gx5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19840/Reviewer_gx5L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985661198, "cdate": 1761985661198, "tmdate": 1762932014642, "mdate": 1762932014642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SCISOR, a generative diffusion model designed for protein shrinking, learning to remove amino acids from natural proteins while maintaining functional and structural integrity. The proposed model reformulates sequence diffusion where the forward process inserts tokens, and the reverse process learns to delete them. The authors derive an exact training objective with two KL terms and compute analytical expressions for event intensity.The paper also demonstrates competitive performance on protein sequence modeling (UniRef50/90), strong correlation with natural deletion patterns, and superior structural preservation in protein shrinking tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper extends diffusion models to discrete, length-varying domains, a major conceptual leap beyond standard fixed-length sequence models. The application to protein shrinking which is previously dominated by self-supervised or autoregressive models is novel and impactful.\n2. The theoretical foundation is rigorous, derivations are mathematically sound. Experimental validation is multifaceted, spanning sequence likelihoods, structure preservation, and biological realism."}, "weaknesses": {"value": "1. Basically, this work simply apply masked diffusion to protine domain. And the loss function designed is similar to the loss designed in [1]. It seems this work only extend the loss to the protein shrinking setting (as well as Theorem 4.2). This work should further elaborate what new insights does this paper convey.\n2. Effects of major components (e.g., π distribution, alignment weighting) is not sysytematically isolated. For example, if π is selected from other priors with a distribution shift, will there be a negative influence? By comparison with a unifrom weighting, is there evidence that the event intensity weighting improves stability or convergence?\n3. Section 4.2 acknowledges a distribution shift between training on noisy inserted sequences and sampling deletions from realistic proteins. Is there a way to conduct quantitative analysis of this mismatch?\n4. Hyperparameters are briefly mentioned, but full training details (compute hours, dataset splits) are incomplete.\n\n[1] Why masking diffusion works: Condition on the jump schedule for improved discrete diffusion."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T7WF0f630B", "forum": "quxeCxJwKm", "replyto": "quxeCxJwKm", "signatures": ["ICLR.cc/2026/Conference/Submission19840/Reviewer_xiN6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19840/Reviewer_xiN6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011480204, "cdate": 1762011480204, "tmdate": 1762932013693, "mdate": 1762932013693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SCISOR, a discrete diffusion model that learns to shorten protein sequences while preserving their structural and functional integrity. Instead of using additive noise like standard diffusion models, SCISOR defines insertions as the forward process and deletions as the reverse process, allowing it to model sequence shrinking directly.  Theoretically, the author prove that a stationary distribution is unnecessary for defining a valid diffusion process, derive a schedule-conditioned loss for stable training and introduce a Rao-Blackwellized gradient estimator by integrating over all insertion paths via sequence alignment. Empirically, SCISOR achieves competitive perplexity and structure-based scores compared to leading generative baselines, and demonstrates superior preservation of foldability and active sties when applied to protein shortening tasks. Overall, the work establishes a principled and scalable deletion-based diffusion framework for biological sequence generation"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is original in both formulation and problem scope. It extends discrete diffusion to irreversible deletion processes by defining a continuous-time insertion-only forward process and a deletion-based reverse process, effectively removing the symmetry assumption of prior discrete diffusion models. This enables a principled treatment of the protein shortening problem, a biologically relevant task that has not been previously modeled in this way. The theoretical development is coherent and well integrated, combining the continuous-time formulation, schedule-conditioned objective, and alignment-based gradient estimator into a unified framework. Empirically, the model achieves good results on both sequence and structure-based evaluations. The paper is clearly written, effectively linking theoretical innovation with practical biological relevance."}, "weaknesses": {"value": "Despite the paper’s strong theoretical formulation, its empirical evidence for true functional preservation remains incomplete. The following issues address key gaps between the model’s assumptions, its training evaluation setup, and its stated goal of maintaining biological function after large-scale deletions.\n\n1. Validation Gap  \n**Limited experimental validation**: ProteinGym provides experimental measurements only for 1-3 deletions, but the main application involves 20-50% shrinking (50-100+ residues). While computational metrics show SCISOR outperforms baselines at large scales (Fig. 6), there's no experimental validation that these predicted improvements on large-scale deletions correlate with actual functional retention.  \n**Unvalidated surrogate metrics**: For large-scale shrinking, the paper relies entirely on predicted metrics without validating their correlation with actual function. Additionally, structure predictions use suboptimal parameters: OmegaFold with 1 cycle instead of recommended 10 cycles, acknowledged to produce \"lower overall pLDDT scores\" (App. C.4.3). No validation that method rankings remain consistent with proper parameters.\n\n2. Distribution shift underexplored  \n Training uses sequences with random insertions while inference uses natural proteins. Authors acknowledge this (Sec. 5) but provide only one validation example (R4SNK4, App. D) showing modest correlations (Fig. 7b, mostly <0.5). More examples across diverse protein families would strengthen the claim that the model learns meaningful evolutionary patterns despite this mismatch.\n\n3. Fundamental Gap  \n**\"Natural-looking\" Does Not Guarantee Functional Preservation**:   SCISOR optimizes for shrunk sequences that look \"natural\" (high q_θ(X̃)), but this does not guarantee they maintain the original protein's function. A shrunk sequence can achieve high naturalness by resembling a common domain found in nature, while completely losing other functional domains of the original protein. Example limitation: Consider a 500-residue transcription factor with a DNA-binding domain (100 aa) and activation domain (400 aa). SCISOR might produce a 200-residue shrunk version containing only the DNA-binding domain. This would score high on naturalness and structural metrics (pLDDT, TM), but would completely lose transcriptional activation function.  \n**Metrics don't validate functional retention** (1) pLDDT measures foldability, not specific function. (2) TM score measures structural similarity, but high TM doesn't guarantee functional preservation, e.g., deleting an allosteric regulatory domain can maintain high TM while abolishing regulation. (3) \"Functional enrichment\" only checks if annotated sites exist, not whether they remain functional in the altered structural context.  \n**Acknowledged but understated**: The paper admits \"similar sequences are likely to have the same function, but this is not guaranteed\" (Sec. 9), yet frames SCISOR as a protein engineering tool throughout. This gap between \"generating natural-looking subsequences\" and \"preserving specific function\" is fundamental to the method's practical utility but receives insufficient discussion."}, "questions": {"value": "Considering the weaknesses outlined above, I have a few questions and suggestions that could help clarify the empirical evidence and strengthen the paper’s claims if addressed in the rebuttal.\n\n1. Do you have any evidence that large-scale (20–50%) deletions predicted by SCISOR retain actual biochemical function?\n2. Have you checked whether OmegaFold results and model rankings remain consistent when using the optimal settings ( such as 10 cycles instead of 1)?\n3. How robust is SCISOR to the mismatch between insertion-noised training sequences and natural inference inputs?\n4. Can you provide validation on more proteins or families beyond the single R4SNK4 example?\n5. Since SCISOR optimizes for natural-looking sequences, how do you ensure that key functional domains are not deleted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AacEiQXSZF", "forum": "quxeCxJwKm", "replyto": "quxeCxJwKm", "signatures": ["ICLR.cc/2026/Conference/Submission19840/Reviewer_RVrd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19840/Reviewer_RVrd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762323201484, "cdate": 1762323201484, "tmdate": 1762932012729, "mdate": 1762932012729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SCISOR is a discrete diffusion framework for protein miniaturization. The forward process inserts residues into natural sequences; the learned reverse process deletes them, enabling principled exploration of the deletion space. Trained on large protein corpora with an ESM-style backbone, SCISOR attains strong accuracy on deletion-effect prediction (ProteinGym) and, for design, shrinks diverse UniProt proteins while preserving in-silico foldability (pLDDT), structural similarity (TM/RMSD), and functional-site retention better than LM baselines at moderate shrinkage."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Right inductive bias for deletions. By making the forward process insertions, the reverse process naturally learns deletions. That’s a clean, principled way to search the combinatorial deletion space.\n\n- Design-oriented evaluation. The UniProt shrinking study checks multiple in-silico proxies (pLDDT, TM/RMSD, motif enrichment) and shows consistent gains over ProGen2 and Raygun for realistic shrink amounts.\n\n- Scalable implementation details. Uses ESM2 + flash attention; training/data choices are clear enough to be reproducible.\n\n- Clarity about scope. The paper is explicit that SCISOR currently performs deletions only, which makes contributions and limitations crisp."}, "weaknesses": {"value": "- Wet-lab validation is limited. The main evidence for design quality is in-silico (structure confidence/similarity, motif enrichment). Experimental assays beyond ProteinGym measurements would strengthen claims (e.g., stability, activity, expression yields).\n\n- Function vs. foldability tradeoffs. At very aggressive shrinkage (e.g., ~50%), some baselines can retain more annotated sites while losing foldability; SCISOR’s behavior under extreme compression could be characterized more deeply (kinetics, dynamics, allostery).\n\n- Backbone dependence. Performance and generalization lean on the ESM2 backbone and training corpus; robustness across families with sparse data or disordered regions isn’t fully explored.\n\n- Template-free but prior-bound. Although not template-conditioned, SCISOR is still guided by evolutionary priors; novel functions that deviate from natural sequence statistics may remain hard."}, "questions": {"value": "- Generalization tests: How does performance vary across enzymes, membrane proteins, or multi-chain interfaces, where small deletions can disrupt dynamics or binding? Any oligomeric/complex benchmarks planned?\n\n- Experimental readouts: Can you report ΔTm, expression yields, solubility, etc. for a set of shrunk designs to calibrate pLDDT/TM against real biophysics?\n\n- Search strategy: For multi-deletion design, how sensitive are results to sampling vs. greedy selection? Any gains from guidance (classifier-free or property predictors) to target function preservation explicitly?\n\n- Boundaries of shrinkage: Where is the failure frontier (e.g., % deletion where foldability or motif retention collapses) across diverse families?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N23xSWASEx", "forum": "quxeCxJwKm", "replyto": "quxeCxJwKm", "signatures": ["ICLR.cc/2026/Conference/Submission19840/Reviewer_Xm99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19840/Reviewer_Xm99"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762335870129, "cdate": 1762335870129, "tmdate": 1762932012282, "mdate": 1762932012282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SCISOR, a new discrete diffusion framework designed to “shrink” protein sequences by learning to delete amino acids while maintaining functional integrity. The core idea is to train a denoiser to reverse a pure insertion-only forward process (a pure birth process), effectively teaching the model how to delete. SCISOR is trained on evolutionary sequence data, learning to remove randomly inserted amino acids to recover natural sequences. The trained denoiser is then applied to shorten real protein sequences.\n\nThe authors claim that SCISOR achieves state-of-the-art deletion-effect prediction on ProteinGym, and that its suggested deletions preserve functional motifs and structural integrity better than prior generative sequence models such as ProGen2 and Raygun."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Motivation relevance**: The paper tackles a meaningful biotechnological problem—designing shorter, more manufacturable proteins.\n2. **Technical creativity**: The proposed insertion-only diffusion and the use of sequence alignment to marginalize over insertion paths is elegant and mathematically nontrivial.\n3. **Empirical performance**: SCISOR performs well on ProteinGym deletion benchmarks, outperforming established baselines in both single and multi-deletion effect prediction."}, "weaknesses": {"value": "1. **Training–application mismatch**:The model is trained to reverse random insertions ($p(X_0|X_0+noise)$) but applied to delete residues from real sequences ($p(X_{shrunken}|X_0)$).This mismatch creates a serious distribution shift, acknowledged but not resolved. It remains unproven that a denoiser trained to remove random noise can effectively identify biologically nonessential regions for deletion.\n2. **Unfair or weak baselines**: The main “shrinking” baseline (ProGen2) assumes independent single deletions—a simplified and unrealistic setup. Although a more accurate sequential baseline ($O(L⋅M)$) is mentioned in Appendix F.3, it is not the focus of comparison, and key results (e.g., TM, RMSD, Fnc Enrich) for that stronger baseline are missing.\n3. **Computational cost**: Training SCISOR is extremely expensive. Computing $p(prev(X_t) | X_0, X_t, M_t)$ requires dynamic programming over all deletion alignments $(O(|X_0||X_t|)$, repeated for each batch. This forced the authors to apply window approximations, potentially biasing the training.\n4. **Functional preservation remains indirectly validated**: The assumption that “naturalness” (model likelihood) correlates with function retention is reasonable and common in protein LMs, but the current evidence is largely in silico. Including further structural or biochemical proxies — e.g., energy-based metrics, functional-domain conservation, or experimental validation — would significantly enhance the biological credibility of the work."}, "questions": {"value": "1. Could the authors justify—either theoretically or empirically—why reversing random insertions is a good proxy for learning function-preserving deletions?\n2. What would happen if the forward process included deletions (as in TDDM) instead of insertions? Would the model’s generative behavior improve or degrade?\n3. How sensitive are the shrinking results (Fig. 6) to the sampling strategy for ProGen2 and to the hyperparameters of the corrector steps?\n4. Could the authors provide TM, RMSD, and Fnc Enrich metrics for the stronger baseline discussed in Appendix F.3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4HMl6dTHao", "forum": "quxeCxJwKm", "replyto": "quxeCxJwKm", "signatures": ["ICLR.cc/2026/Conference/Submission19840/Reviewer_Ryfk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19840/Reviewer_Ryfk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762773827453, "cdate": 1762773827453, "tmdate": 1762932011916, "mdate": 1762932011916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}