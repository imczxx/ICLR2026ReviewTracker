{"id": "0oxkxG9cCo", "number": 2377, "cdate": 1757067142609, "mdate": 1759898152386, "content": {"title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation", "abstract": "In this study, we present IL3D, a large-scale dataset meticulously designed for large language model (LLM)-driven 3D scene generation, addressing the pressing demand for diverse, high-quality training data in indoor layout design. Comprising 27,816 indoor layouts across 18 prevalent room types and a library of 29,215 high-fidelity 3D object assets, IL3D is enriched with instance-level natural language annotations to support robust multimodal learning for vision-language tasks. We establish rigorous benchmarks to evaluate LLM-driven scene generation. Experimental results show that supervised fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and surpasses the performance of SFT on other datasets. IL3D offers flexible multimodal data export capabilities, including point clouds, 3D bounding boxes, multiview images, depth maps, normal maps, and semantic masks, enabling seamless adaptation to various visual tasks. As a versatile and robust resource, IL3D significantly advances research in 3D scene generation and embodied intelligence, by providing high-fidelity scene data to support environment perception tasks of embodied agents. The dataset and accompanying code will be publicly accessible on GitHub.", "tldr": "IL3D, a large-scale dataset with 27,816 indoor layouts and 29,215 3D assets, supports LLM-driven 3D scene generation. It includes natural language annotations and rigorous benchmarks, enhancing the 3D scene generation task.", "keywords": ["Indoor scene understanding", "Synthetic dataset", "3D Computer Vision"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1aa39a6f9506ce6a0d2c40291c4b67a9cd29396d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a new 3D scene dataset (IL3D) for LLM-driven 3D scene generation. This dataset is constructed by integrating, cleaning, and supplementing existing popular datasets (i.e., 3D-FRONT and HSSD) and then adding their own synthetic data to enhance scene diversity and cover underrepresented categories. A key feature is its detailed and USDZ/USDA-format annotations, which were generated using the Qwen3-VL model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The choice to represent 3D scene data using USDZ-format assets and USDA-format scenes is appropriate. This structure ensures text readability, which is beneficial for both human understanding and direct parsing by LLMs.\n\n- The experiments analyzing the impact of 3D scene dataset scale and quality on LLM scene generation are intriguing. This ablation study provides quantitative data on how training data volume and annotation quality affect key objective metrics (e.g., OOB, OOR, and CLIP-Sim) across different LLM parameter sizes."}, "weaknesses": {"value": "1. **Incomplete Citations and References**. There is a noticeable omission of citations for several works mentioned in the text, such as 3D-FUTURE, InstructScene, and the Qwen3 model family. The current manuscript includes fewer than 30 references, which suggests a failure to fully acknowledge all relevant prior or utilized work. This lack of comprehensive citation undermines the paper's academic rigor and makes it difficult for readers to trace all discussed technologies.\n\n2. **Insufficient Sample Visualization**. Given that the paper's core contribution is the introduction of a new dataset (IL3D), the manuscript provides insufficient illustrative examples of the dataset's samples. While it includes extensive aggregate statistics (e.g., room type distribution, object counts, area analysis ), it lacks rich, individual-sample visualizations that would allow readers to fully grasp the quality, fine-grained nature of the object arrangements, and the detail of the new natural language annotations.\n\n3. **Limited Technical Contribution**. The primary contribution of this work is the dataset itself. However, the dataset's construction mainly involves integrating and cleaning two existing public datasets (3D-FRONT and HSSD) and supplementing them with annotations generated by external models (e.g., Qwen3-VL for instance-level descriptions). As the most crucial bottlenecks in current 3D scene generation datasets are arguably the difficulty of obtaining high-quality 3D assets and realistic object placement schemes, the scope of the novel technical contribution beyond data aggregation and automatic labeling is limited."}, "questions": {"value": "Please refer to the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BHxDhDHFMP", "forum": "0oxkxG9cCo", "replyto": "0oxkxG9cCo", "signatures": ["ICLR.cc/2026/Conference/Submission2377/Reviewer_N3uX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2377/Reviewer_N3uX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571595173, "cdate": 1761571595173, "tmdate": 1762916213726, "mdate": 1762916213726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a dataset paper. It aims to solve the problem of scarcity of indoor layout dataset. The paper focuses on the method of generating large-scale, high-quality datasets driven by the LLM. The problem of diversity, completeness of annotation and the capability to support multi-modal learning is very severe. IL3d dataset integrates and synthesizes existing ones (3D-FRONT, HSSD) selectively 18 common room types and synthesize many 3d objects as well. It also supports the instance level natural language annotation and multi-modal data outputs. It may support more research in physical-plausible generation, semantic consistent room layouts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength\n-\tThe dataset size and annotations are large-scale and dense. \n-\tThe dataset generation methods seem sound."}, "weaknesses": {"value": "-\tThe core contribution and motivation is to improve the physical plausibility and the semantic coherency in the room layout generation via llm. However, the evaluation of the datasets are limited. E.g. only 2 comparison baseline methods are utilized. \n\n-\tThe annotation, especially the instance natural language annotation are generated via the multi-view image generation pipeline via an VLM, such QWen3. If so, do the annotations heavily rely on and may generate bias based on this particular VLM. How to evaluate such biases and generate an unbiased dataset for usage? Although in the evaluation, there are quite many evaluation indexes proposed. But none of them tackle the bias of the model. \n\n-\tThe mechanism and why SFT will enable the reasoning in the 3D space is interesting but confusing. More explanations and visual example comparisons of with and without SFT."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qzQHYSQ1yp", "forum": "0oxkxG9cCo", "replyto": "0oxkxG9cCo", "signatures": ["ICLR.cc/2026/Conference/Submission2377/Reviewer_n89v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2377/Reviewer_n89v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884791679, "cdate": 1761884791679, "tmdate": 1762916213526, "mdate": 1762916213526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IL3D positions itself as a large-scale dataset for LLM-driven 3D indoor scene generation: it claims 27,816 layouts (18 room types) and ~29k high-fidelity assets, with instance-level text annotations and multi-modal exports (point clouds, 3D boxes, multi-view RGB, depth, normals, semantic masks). Data are organized in USD (USDZ assets + USDA scenes), with evaluation using OOB/OOR/GSR/CLIP-Sim plus GPT-based scores (OP/PR/SC/SF/VA). The paper compares against I-Design and Holodeck and argues that SFT on IL3D yields more stable objective metrics, with mixed results on subjective scores.\nThis work does not have novelty, I recommend rejecting it."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Complete format & export stack: USD organization with one-click export to point cloud/depth/normal/mask is practical and integrable across 2D/3D tasks.\n\n2. Metric coverage: Includes both geometric (OOB/OOR/GSR/CLIP) and GPT-based perceptual metrics, aligning with current literature.\n\n3. Comparisons & ablations present: Shows tables against I-Design/Holodeck and scale/annotation ablations for SFT."}, "weaknesses": {"value": "1. The dataset construction follows exactly the same fundamental pipeline as OptiScene, which is based on the Holodeck and merely scales it up. There is no substantive difference.\n\n2. What can the added modalities (e.g., depth) actually enable downstream? The paper provides no experiments to demonstrate their utility.\n\n3. The writing and figures are rough and poorly executed.\n\n4. The work only releases a dataset; the downstream methods and prompts are almost identical to prior art, with no innovation."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iqKUIPdJj4", "forum": "0oxkxG9cCo", "replyto": "0oxkxG9cCo", "signatures": ["ICLR.cc/2026/Conference/Submission2377/Reviewer_EDZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2377/Reviewer_EDZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970267909, "cdate": 1761970267909, "tmdate": 1762916213368, "mdate": 1762916213368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}