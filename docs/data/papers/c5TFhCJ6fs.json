{"id": "c5TFhCJ6fs", "number": 23824, "cdate": 1758348945607, "mdate": 1759896795249, "content": {"title": "Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin", "abstract": "Attention sinks and compression valleys have attracted significant attention as two puzzling phenomena in large language models, but have been studied in isolation. In this work, we present a surprising connection between attention sinks and compression valleys, tracing both to the formation of massive activations in the residual stream. We prove theoretically that massive activations necessarily produce representational compression and establish bounds on the resulting entropy reduction. Through experiments across several models (410M--120B parameters), we confirm that when the beginning-of-sequence token develops extreme activation norms in the middle layers, both compression valleys and attention sinks emerge simultaneously. Targeted ablation validates our theoretical predictions. This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. Specifically, we posit that Transformer-based LLMs process tokens in three distinct phases: (1) broad mixing in the early layers, (2) compressed computation with limited mixing in the middle layers, and (3) selective refinement in the late layers. Our framework helps explain why embedding tasks perform best at intermediate layers, whereas generation tasks benefit from full-depth processing, clarifying differences in task-dependent representations.", "tldr": "", "keywords": ["attention sinks", "compression valleys", "deep trasformer-based LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76c5f9e992a1534198cac929acadd3b3df7d21a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper attempts to understand the mechanisms tying together some phenomena happening within the internal representations and activations of LLMs and interpret them in terms of information flow. Namely, they tie together massive activations (some activations in the residual stream grow extremely large), attention sinks (some attention heads put all attention on the first begin-of-string (BOS) token), and compression valleys (the representation matrix of an input at middle layers of an LLM has highly concentrated singular values, i.e., is approximately very low rank). They prove some linear algebraic bounds to show that massive activations imply compression valleys. Then, they propose a high-level framework for information flow in LLM layers called \"Mix-Compress-Refine\" where the initial layers mix information throughout the sequence to build context, the intermediate layers compress to preserve only essential information through the massive activations, and the final layers refine the representation for task-specific outputs. They present evidence for this through controlled experiments with real language models from 400M to 120B. They finish the paper by presenting implications for LLM-related tasks such as collecting embeddings (where one wants to collect them from the compressed but unspecialized representations in the middle layers)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- While the discussed phenomena --- attention sinks, compression valleys, massive activations --- have all been extensively explored, they have not been unified under a single explanation before. This would be a nice contribution to research trying to understand empirical phenomena in transformers.\n- The theory is clear and illustrative, showing a worst-case relationship between massive activations and singular value compression. Targeted experiments show that this happens in practice, along with some training dynamics.\n- The Mix-Compress-Refine framework is elegant and, if proven, would be a nice starting point for further interpretability or theoretical studies, and may be used to motivate further empirical methodology.\n- The paper is nicely organized, easy to read, and has nice visualizations."}, "weaknesses": {"value": "- The paper is not fully rigorous w.r.t. demonstrating causal mechanisms. For example, ablating MLP layers and showing that this removes massive activations and compression valleys does not show that massive activations are causally responsible for compression valleys. (Indeed, ablating MLP layers is not necessarily equivalent to ablating massive activations, as there could be several ways to do the latter.) Instead, this experiment shows that MLPs are (at least in part) a mechanism which causes massive activations and compressive valleys. Of course the theory says that both have to co-occur but it's not clear which causes the other. See \"Questions\" Q1 below.\n- The paper discusses a lot about information flow and the information contained in some tokens, and indeed one can see this informally via the paper's examples by pointing at small vs. large weights/values in attention, etc. However, to add more rigor it may be necessary to do other tasks to confirm the precise amount of information stored in a certain location, using e.g. linear probing. See \"Questions\" Q2 below.\n- The paper uses a single dataset GSM8K for almost all data, introducing FineWeb for a single experiment. The latter is a more diverse dataset, while the former has heavily templated questions and answers. In order to make claims at the level of generality in the paper, probably some assessments and analysis should be done on general data, and any differences/similarities with the current evaluation can be discussed and examined. See \"Questions\" Q3 below."}, "questions": {"value": "Q1: Are there other mechanisms to ablate massive activations? Do they still also cause compression valleys? Can you do any kind of formal causal analysis to show the claimed causality?\n\nQ2: Can you trace a simple end-to-end example showing the three stages, using linear probing to validate your theory of information flow and the relevant proposed mechanisms (mix, compress, refine)?\n\nQ3: Can you compare results on different datasets? Are the results substantially different for e.g., prose vs. math questions vs code vs scientific papers (each of these is readily available from public datasets)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UDlXrArqGa", "forum": "c5TFhCJ6fs", "replyto": "c5TFhCJ6fs", "signatures": ["ICLR.cc/2026/Conference/Submission23824/Reviewer_qBpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23824/Reviewer_qBpd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423512968, "cdate": 1761423512968, "tmdate": 1762942821610, "mdate": 1762942821610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aligns two seemingly disjoint phenomena within transformers, attention sinks (where an attention head is dominated by attention weights for a semantically uninformative token) and compression valleys (where intermediate representations show low entropy despite high dimensionality). Both empirically and theoretically, the authors tie these phenomena to massive activations in the residual stream for the beginning of sequence token, showing that as the BOS norms increase, there is a corresponding increase in the sink rate and a decrease in entropy. The authors use these insights to present a mix-compress-refine theory of information flow, additionally tying this theory to insights regarding performance on various classes of tasks at different layers within the network."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The breadth of analysis is strong, with the analysis including decoder-only models of the Pythia, Llama, and Qwen families in the main paper. In particular, the analysis of Pythia 410M over training is visually illuminating of how the properties discussed emerge rapidly over training. The ablation experiments in Llama 3 8B showing that removing massive activations causally decreases attention sinks and compression are also convincing, though not general across every model.\n2. The theoretical framework analyzing massive activations and their influence on compression bounds is well founded.\n3. The mix-compress-refine theory of information flow that the paper presents follows reasonably naturally from the insights on experiments with massive activations. This is then made more rigorous with downstream task evaluations, evaluated at various layer representations, that show that refinement requires later layers, while embedding signals are most useful in the middle layers."}, "weaknesses": {"value": "1. Returning to the BOS ablation experiments, looking at Figure 14, do you have any insights as to why sinks persist in Pythia 410M despite the ablation for the BOS token? Is it the same for other Pythia models, such as 6.9B? \n2. Some of the figures, such as Figure 2-6, while illuminating, show only a single model. Despite many of the additional analyses showing up in the appendix, to increase robustness of the figures in the main text, it may be worth considering adding additional data to such figures when possible.  \n3. The ablation of the MLP contribution currently sets it to zero. A gradual decrease of the contribution from its original value to zero would be very insightful. Do you see a gradual, dose-response effect in terms of the changes to entropy and sink rate as a result of a gradual ablation?\n\nOverall, despite these minor weaknesses, the breadth of analysis of the paper combined with the novel results tying together two seemingly disparate phenomena, as well as the mix-compress-refine framework, motivate my score of 8."}, "questions": {"value": "1. See weaknesses.\n2. Additionally for Figure 14, while the general trend of ablations decreasing sink rate and increasing entropy is supported, for the ablated entropy metric for Llama 3 8B, is there any intuition as to why there is a sudden increase in layer 15, and for Qwen 2 7B, is there any intuition for the increased sink rate in layers 4-6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t0lztx3r1Z", "forum": "c5TFhCJ6fs", "replyto": "c5TFhCJ6fs", "signatures": ["ICLR.cc/2026/Conference/Submission23824/Reviewer_Wogq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23824/Reviewer_Wogq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898027329, "cdate": 1761898027329, "tmdate": 1762942821429, "mdate": 1762942821429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that attention sinks and compression valleys in LLMs are caused by the same underlying mechanism—massive activations in the residual stream, often on the BOS token. The authors show empirically that these phenomena always co-occur in the middle layers across models (410M–120B). They prove that when one token’s activation norm dominates, it forces a dominant singular value in the representation matrix, causing low entropy and thus compression. They confirm the causality through ablations. Based on this, they propose a Mix–Compress–Refine theory of information flow: early layers mix information broadly, middle layers compress representations and halt attention mixing, and late layers re-expand and refine outputs. The paper also connects these phases to task performance—embedding tasks perform best during compression, while generation benefits from full refinement. Overall, the work unifies two previously separate observations and provides a clear mechanistic view of how transformers structure computation across depth."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The three-phrase part seems to be interesting. They propose and try to demonstrate a possible three-phase structure. In early layers, attention entropy is high and attention patterns are broadly mixed (Fig. 6 left). In middle layers, mixing decreases and representations become more compressed (Fig. 6 middle). In later layers, attention shifts to localized positional patterns and token norms become more balanced (Fig. 6 right). Task-level results further align with this trend: embedding performance tends to peak around the mid-layer compression phase, while generation tasks continue improving toward the final layers (Figs. 7, 23–27)."}, "weaknesses": {"value": "The paper has two parts—the relationship between attention sinks and compression valleys, and the three-phase structure. However, I do not find either of them satisfying enough.  \n\n1. The connection between attention sinks and compression valleys is quite trivial. It is well known that attention sinks and massive activations occur simultaneously. Then a massive activation leads to a dominant largest singular value in the activation matrix. The causal ablations that remove early-layer MLPs are not new either.  \n\n2. It was observed previously that early layers have more uniform (or mixed) attention patterns, middle layers have attention sinks, and late layers show special functionality. The paper develops these observations into the three-phase structure too strongly, without providing stronger evidence. Figures 6 and 7 simply reproduce previous experiments and cannot support stronger claims."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vau2T6bCVL", "forum": "c5TFhCJ6fs", "replyto": "c5TFhCJ6fs", "signatures": ["ICLR.cc/2026/Conference/Submission23824/Reviewer_8eqz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23824/Reviewer_8eqz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037497235, "cdate": 1762037497235, "tmdate": 1762942821219, "mdate": 1762942821219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the relationship between attention sinks and compression valleys in LLMs, proposing that both phenomena arise from massive activations in the residual stream. \n\n\nThe authors prove that \"massive activations\" necessarily produce representational compression through singular value dominance. \n\n\nThrough experiments across diff. models they validate that when the beginning-of-sequence (BOS) token develops extreme activation norms, both compression valleys and attention sinks emerge simultaneously. \n\nThe paper provides new insights, but its theoretical contributions might not be strong. In my opinion,  most of the technicalities are based on well-known linear algebra."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written with good contextualization relative to prior work,\nand comprehensive appendix with additional validation. \n   \n\n- While the proof technique is straightforward (using Rayleigh quotient characterization), this is the *first* work to formally prove that massive activations $M = ||x_0||^2 $ necessarily induce spectral dominance:  $\\sigma_1^2 \\geq M + \\alpha R$.   \n\n-   The derived bounds on dominance  $\\frac{\\sigma_1^2}{\\sum_{j \\geq 2} \\sigma_j^2} \\geq \\frac{c+\\alpha}{1-\\alpha}$,  anisotropy $p_1 \\geq \\frac{c+\\alpha}{c+1}$,  and entropy are mathematically sound.\n\n-   Figure 3 shows that in middle layers (where massive activations emerge), the theoretical bounds closely match empirical values, validating that massive activations are the dominant mechanism shaping representation geometry. Targeted MLP ablations (Figures 4 and 14) across multiple models provide causal evidence."}, "weaknesses": {"value": "- The theorem relies on well-known linear algebra and, in my opinion, lacks strong theoretical novelty.\n\n- The bound depends on $\\alpha = \\frac{1}{R}\\sum_{i \\neq 0} \\|x_i\\|^2 \\cos^2\\theta_i$, but Figure 3 suggests $\\alpha R \\ll M$ in practice.  \nMore analysis of what determines $\\alpha$ would strengthen the work.\n\n- Pythia 410M ablations show that compression is removed without eliminating sinks, suggesting that sinks may have multiple emergence mechanisms.  \nSince ablations only target MLP contributions, direct manipulations of massive activations (e.g., through regularization or initialization) would provide stronger causal evidence.\n\n- Lines 200–206: The proof sketch is potentially misleading, as it may imply circular reasoning.  \nYou may clarify that the Rayleigh characterization provides a lower bound when choosing $v = x_0 / \\|x_0\\|$.\n\n- Lines 242–247: This contradicts the claim that massive activations jointly cause sinks and compression.  \nYou may either analyze why Pythia 410M behaves differently or restate the claim as a \"common but not universal\" mechanism.\n\n- Lines 185–187: I don’t think $r = 0.58 \\pm 0.25$ indicates a strong correlation; it appears to be only moderate.\n\n- Lines 206, Figure 3: Although $\\alpha R \\ll M$, $\\alpha$ appears crucial, yet there is no analysis of its origin or importance.\n\n- Lines 185–186, 907–909: No p-values are reported, the sample size ($n = 6$) is small, and there is no correction for multiple testing.  \nWhat are the $p$-values and sample size used?\n\n- Lines 421–424: Some parameters might need to be reported, such as the optimizer, learning rate, batch size, number of epochs, and random seed."}, "questions": {"value": "Q0. Please see Weaknesses\n\nQ1.  Why do Pythia 410M ablations remove compression but not sinks, and does this indicate that sinks can emerge through multiple independent mechanisms?\n\nQ2.  What determines $\\alpha = \\frac{1}{R}\\sum_{i \\neq 0} \\|x_i\\|^2 \\cos^2\\theta_i$, and since Figure 3 shows $\\alpha R \\ll M$, is the alignment term theoretically necessary or practically negligible?\n\nQ3. Have you tried inducing massive activations artificially (e.g., via initialization or regularization) to test whether sinks or compression follow?\n\nQ4. Can phase transition points be predicted from architectural parameters such as depth, width, or number of heads rather than observed post hoc?\n\nQ5. Do these phenomena appear in encoder-only models or models with alternative positional encodings, and how do these differences affect the emergence of massive activations and phase structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hNOhTbjH36", "forum": "c5TFhCJ6fs", "replyto": "c5TFhCJ6fs", "signatures": ["ICLR.cc/2026/Conference/Submission23824/Reviewer_FFpb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23824/Reviewer_FFpb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762908374171, "cdate": 1762908374171, "tmdate": 1762942821017, "mdate": 1762942821017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}