{"id": "Vg9AplGjZ5", "number": 18602, "cdate": 1758289407238, "mdate": 1762931533257, "content": {"title": "CTQWFORMER: A CTQW-BASED TRANSFORMER FOR GRAPH CLASSIFICATION", "abstract": "Graph Neural Networks (GNN) and Transformer-based architectures have achieved remarkable progress in graph learning, yet they still struggle to capture both global structural dependencies and model the dynamic information propagation. In this paper, we propose CTQWformer, a hybrid graph learning framework that integrates continuous-time quantum walks (CTQW) with GNN. CTQWformer employs a trainable Hamiltonian that fuses graph topology and node features, enabling physically grounded modeling of quantum walk dynamics that captures rich and intricate graph structure information. The extracted CTQW-based representations are incorporated into two complementary modules:(i) a Graph Transformer module that embeds final-time propagation probabilities as structural biases in the self-attention mechanism, and (ii) a Graph Recurrent Module that captures temporal evolution patterns with bidirectional recurrent networks. Extensive experiments on benchmark graph classification datasets demonstrate that CTQWformer outperforms graph kernel and GNN-based methods, demonstrating the potential of integrating quantum dynamics into trainable deep learning frameworks for graph representation learning. To the best of our knowledge, CTQWformer is the first hybrid CTQW-based Transformer, integrating CTQW-derived structural bias with temporal evolution modeling to advance graph learning", "tldr": "", "keywords": ["GRAPH CLASSIFICATION，Graph Neural Networks，Transformer-based architectures"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/606d1866844a71b847081db653a8f8c69c3002ed.pdf", "supplementary_material": "/attachment/8373a51dc608bc20ddaeea3fab801c1932c385f6.zip"}, "replies": [{"content": {"summary": {"value": "This works introduces an original quantum walk-(QW) based structural encoding for graph transformers (GT). \nQW probabilities are computed using a time-dependent evolution operator, mimicking Schrodinger's equation, and later used in self-attention and recurrent modules, providing rich structural information to the network.\n\nAlthough the idea of using QW as a structural encoding in GT is novel and very interesting, a lack of reflection on the recent GT literature, which comprises closely related works, prevents a fair assessment of the novelty of this work.\nFurthermore, the experimental comparison lacks all modern GNN methods and omits GT altogether, which is a serious obstacle for an ICLR publication."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The introduction of QW to enhance the expressivity of GT is an exciting research idea, and constitutes an interesting alternative to current structural encoding methods.\nQW have the potential to finely encode the structural information carried by graph topology, and their application for graph classification is definitely worth investigating."}, "weaknesses": {"value": "Despite its undeniable originality and potential, the implementation of the main contribution of this work, i.e. the use of QW as a GT structural encoding method, remains very classical and the lack of perspective on very close research raises questions on its actual benefits.\n\nThis work is therefore crucially missing a comparison with \"classical\" random walks (RW) and their use in graph transformers (especially [6] and [7] below), that have already shown state-of-the-art performances on various graph benchmarks. Just like QW, RW richly encodes graph structures at the edge level up to a $T$-hop neighborhood where the maximum walk length $T$ can be adjusted.\nIn the absence of any mention of these works, the reader cannot measure the benefits of QW over RW in terms of expressivity, scalability or interpretability. This is problematic as this work essentially proposes a replacement for a highly efficient and widely adopted method.\n\nRegarding the lack of originality in the implementation, biasing the self-attention matrix of GT with edge-wise structural information is a classical method used in Graphormer, which the authors cite, but also in all the influential papers referenced below ([1-8]) which the authors do not mention. The \"dynamical\" aspect of walks, which is encoded here using bidirectional-GRU, is also tackled in an arguably more efficient manner in both GRIT [6] and CSA [7]. In summary, this work presents few novelties beyond its initial idea, and the lack of discussion about close methods does not allow to accurately evaluate its benefits, which are certainly real.\n\nA second, very serious weakness, is the incomplete experimental evaluation. The authors invoke difficulties in re-implementing GT methods, which is understandable, but several of the works listed below provide their code and weights (and so does GraphGPS which is mentionned in the paper), and comparing with GT methods is mandatory when proposing a new GT architecture. I suggest that the authors implement the GT methods that can be the most conveniently trained on their datasets (including GRIT and GraphGPS) to strenghthen their experimental results.\n\nThe statement made by the authors that GT are orthogonal to their work is factually wrong, given how close many of these works are to the presented model. It is also contradicted by a statement made in section 2.1, which presents the limitations of current GT as a motivation for this work (\"These drawbacks motivate our approach\", l122).\n\nThe comparison with GNN methods is also piecemeal, and lacks all works past 2018 (such as [9]).\n\nThese serious limitations prevent acceptance in the present state of this work.\n\n\n[1] A generalization of transformer networks to graphs, Dwivedi & Bresson, 20\n\n[2] Rethinking graph transformers with spectral attention, Kreuzer et al., NeurIPS 21\n\n[3] Graphit: Encoding graph structure in transformers, Mialon et al., 21\n\n[4] Global self-attention as a replacement for graph convolution, Hussain et al., KDD 22\n\n[5] Graph propagation transformer for graph representation Learning, Chen et al., IJCAI 23\n\n[6] Graph inductive biases in transformers without message passing, Ma et al., ICML 23\n\n[7] Self-attention in colors: Another take on encoding graph structure in transformers, Menegaux et al., TMLR 23\n\n[8] Enhancing Graph Transformers with Hierarchical Distance Structural Encoding, Luo et al., NeurIPS 24\n\n[9] From stars to subgraphs: Uplifting any gnn with local structure awareness, Zhao et al., ICLR 22"}, "questions": {"value": "The clarification of these additional points would be of further help to the reader:\n\n- Section 3.4: \"In contrast to the convergence behavior of classical random walks, the evolution of quantum walks is dynamic and oscillatory.\" This sentence needs additional clarification. The $e^{-t}$ in Equation (5) shows that states reached by QW are on the contrary stationary.\n\n- Equation 13: This part would need further clarification, especially regarding how $O_{QWGR}$ is used in following layers. Encoding the output of this module into a single graph-level representation likely causes severe information loss. Expliciting the dimensions would help the reader comprehend better these operations.\nIt is not clear how $P_i$ is extracted from $P$ without refering to the appendices. Please check the dimensions of $P_i$.\n\nSeveral typos / writing issues can be found in the manuscript. This includes:\n- Section 2.2, l141: \"spectral of density matrix\"\n- l146: \"Motivated by GQWformer, which pioneers the integration of DTQW and graph Transformers in graph learning.\"\n- l250: \"a two-layer perception\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z0yGzmc6wR", "forum": "Vg9AplGjZ5", "replyto": "Vg9AplGjZ5", "signatures": ["ICLR.cc/2026/Conference/Submission18602/Reviewer_guxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18602/Reviewer_guxs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760712363092, "cdate": 1760712363092, "tmdate": 1762928319018, "mdate": 1762928319018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qmHtpbteCo", "forum": "Vg9AplGjZ5", "replyto": "Vg9AplGjZ5", "signatures": ["ICLR.cc/2026/Conference/Submission18602/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18602/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762931531448, "cdate": 1762931531448, "tmdate": 1762931531448, "mdate": 1762931531448, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new type of graph classification method called CTQWformer that leverages techniques from quantum walks to enhance the performance of attention-based graph representation construction. Experimental results demonstrate competitive performance of CTQWformer over small-to-medium scale graph classification benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Utilizing ideas from quantum mechanics is an interesting and promising direction toward better design of graph representation methods------as the authors have also stated in the paper, sometimes physically-oriented design might brought better inductive biases that potentially improves tasks where such types of features are not trivially captured through mathematical initiatives like kernel methods or message passing neural networks."}, "weaknesses": {"value": "- **On physically structural bias**: Despite the authors use methods from quantum mechanics, it does not appear to me what the clear signal of strength is for such methods under the context of graph representation learning: Does it enhances expressivity? Is there any problems which cannot be solved by message passing or kernel methods (which, up to Weisfeiler-Lehman hierarchies are somewhat closely related) that are solvable via quantum walks? I think in addition to technical introduction, the authors shall discuss more on this design choice.\n- **Lack of empirical comparisons**: While the proposed architecture has close relationship with graph transformers, the authors only compared with kernel methods and message passing methods before 2020 in their empirical investigations. I do not think this is a proper way of doing comparisons in 2025 as graph representation learning is a very active field of research. I also noticed that in appendix C the authors mentioned that due to implementation reasons comparisons with graph transformers are not conducted. However, it appears to me that contemporary graph transformer designs are not that difficult to implement, imo they seem to be simpler than quantum methods.\n- **Minor presentation issues**: In equation (9) in line 256, should the presentation be like $W_\\text{sym}$ instead of $A_\\text{sym}$? The notation is somewhat confusing for me."}, "questions": {"value": "Aside from weaknesses, I have one additional question:\n- What is the complexity comparison between CTQWformer and conventional graph transformers? While their is a complexity breakdown in appendix A, which seems not very satisfactory for large graphs. I would like to see a detailed comparison between per-graph computational complexities between CTQWformer and state-of-the-art graph transformer models like GraphGPS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YgzT87MYST", "forum": "Vg9AplGjZ5", "replyto": "Vg9AplGjZ5", "signatures": ["ICLR.cc/2026/Conference/Submission18602/Reviewer_Y43p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18602/Reviewer_Y43p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760761118734, "cdate": 1760761118734, "tmdate": 1762928318539, "mdate": 1762928318539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CTQWformer, a novel hybrid framework for graph classification. It integrates continuous-time quantum walks with a graph transformer and a graph recurrent module. Using a trainable Hamiltonian that fuses topology and node features, the model uses continuous-time quantum walks-derived probabilities as a static structural bias for the transformer's attention while the recurrent module captures the dynamics of the temporal evolution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces CTQWformer, a hybrid framework to integrate continuous-time quantum walks (CTQW) with both graph transformer and recurrent modules. It uniquely uses a trainable Hamiltonian to model both static structural bias and dynamic temporal evolution from the CTQW dynamics. Furthermore, the paper is well-organized, and the methodology is presented clearly, making the complex concepts easy to follow."}, "weaknesses": {"value": "1. The model's scalability is severely limited, and the complexity analysis is critically flawed on two fronts. First, the classical computation of the matrix exponential requires $\\mathcal{O}(T \\cdot n^3)$ time and $\\mathcal{O}(T \\cdot n^2)$ memory, which is computationally infeasible for large graphs. While the authors suggest using Krylov methods for scaling, this contradicts the model's design, as the QWGT module explicitly requires the full $n \\times n$ probability matrix $P^T$ as a structural bias, not just a more efficient matrix-vector product. \n\n2. More importantly, the entire analysis ignores the cost of quantum measurement. To obtain the probability matrix $P$ from the quantum walk, one must perform quantum state tomography, the complexity of which scales exponentially with the number of nodes (qubits). This exponential bottleneck is a well-known barrier in quantum computing, and its omission suggests the proposed \"quantum\" framework might not practically scalable.\n\n3. A significant weakness is the selection of baselines, which are largely outdated, with most models dating from 2016-2018 (e.g., GCN, GAT, GIN). While it is understood that quantum-based methods are challenging to apply to large-scale datasets, the paper's validation is further weakened by its exclusive focus on graph classification. Reviewer would be happy if the authors can conduct numerical experiments on other general tasks, such as node classification, link prediction or graph reconstruction. Alternatively, performing property prediction on specific small-graph domains on which the quantum-based method can apply, like small molecules, would have provided a more convincing validation of the framework's capabilities beyond a single task.\n\n4. The experimental results demonstrate significant instability, which undermines the reliability of the model's reported performance. The standard deviations are exceptionally high across multiple datasets, such as $92.54 \\pm 5.39$ on MUTAG, $69.16 \\pm 5.17$ on PTC, and a very large $47.47 \\pm 7.84$ on IMDB-M. The authors acknowledge this volatility, attributing it to the \"inherent dynamics and fluctuation of CTQW evolution\". Such high variance suggests the model is extremely sensitive to random initialization or data splits and is not robust, making the reported mean accuracy a potentially misleading metric of its true performance."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lcJHuIkIiQ", "forum": "Vg9AplGjZ5", "replyto": "Vg9AplGjZ5", "signatures": ["ICLR.cc/2026/Conference/Submission18602/Reviewer_r2V5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18602/Reviewer_r2V5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760797912461, "cdate": 1760797912461, "tmdate": 1762928317986, "mdate": 1762928317986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel hybrid framework that integrates continuous-time quantum walks (CTQW) with graph neural networks (GNNs) and Transformer architectures to enhance graph classification tasks. The model leverages a trainable Hamiltonian from CTQW to extract and encode both static and structural biases and dynamic temporal evolution patterns. These are incorporated into two modules: a Graph Transformer (QWGT) and a bidirectional recurrent network (QWGR), which are fused to produce rich graph-level representations. Extensive experiments on benchmark datasets demonstrate that CTQWformer outperforms both kernel-based and GNN-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of integrating quantum walk dynamics into a trainable graph transformer is interesting and novel. \n\nThe use of a learnable Hamiltonian allows the model to adaptively capture both structural and feature-based relationships in graphs. \n\nThe empirical results are compelling, showing consistent improvements across multiple datasets. The ablation studies and sensitivity analyses further validate the contributions of each component and provide insights into optimal configurations."}, "weaknesses": {"value": "While the idea is interesting, the motivation is difficult to grasp. In the Introduction, the authors state that Transformers can enhance GNNs due to their \"strong capability in modelling long-range dependencies.\" However, they later claim that \"Transformer-based GNNs often struggle to capture both local and global dependencies in graph data.\" This appears inconsistent and raises questions about the clarity of the motivation.\n\nThe proposed model is tightly coupled with Continuous-Time Quantum Walks (CTQW), and its description heavily relies on CTQW concepts, which makes the model difficult to understand without prior knowledge of CTQW.\n\nThe results in Table 2 show high standard deviations, whereas those in Table 3 do not. Is there a specific reason for this discrepancy?"}, "questions": {"value": "Please see above comments on weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QioafTSseP", "forum": "Vg9AplGjZ5", "replyto": "Vg9AplGjZ5", "signatures": ["ICLR.cc/2026/Conference/Submission18602/Reviewer_ogv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18602/Reviewer_ogv6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976321826, "cdate": 1761976321826, "tmdate": 1762928317599, "mdate": 1762928317599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}