{"id": "uDP3P9TTRA", "number": 23366, "cdate": 1758342696087, "mdate": 1759896818837, "content": {"title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models", "abstract": "Large reasoning models, which are post-trained on long chain-of-thought (long CoT) data with reinforcement learning, achieve state-of-the-art performance on mathematical, coding, and domain-specific reasoning benchmarks. However, their logical reasoning capabilities—fundamental to human cognition and independent of domain knowledge—remain understudied. To address this gap, we introduce \\textbf{LogiEval}, a holistic benchmark for evaluating logical reasoning in large reasoning models. LogiEval spans diverse reasoning types (deductive, inductive, analogical, and abductive) and task formats (e.g., logical sequence, argument analysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our experiments demonstrate that modern reasoning models excel at 4-choice argument analysis problems and analogical reasoning, surpassing human performance, yet exhibit uneven capabilities across reasoning types and formats, highlighting limitations in their generalization. Our analysis reveals that human performance does not mirror model failure distributions. To foster further research, we curate \\textbf{LogiEval-Hard}, a challenging subset identified through a novel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably predict difficulties for larger models. Modern models show striking, consistent failures on LogiEval-Hard. This demonstrates that fundamental reasoning bottlenecks persist across model scales, and establishes LogiEval-Hard as both a diagnostic tool and a rigorous testbed for advancing logical reasoning in LLMs.", "tldr": "We present LogiEval-Hard as a challenging benchmark for evaluating logical reasoning in large reasoning models", "keywords": ["large language model", "logical reasoning", "evaluation", "reasoning model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abfafb4eff2d99a96fd784bbc00c47b6fc4aad4f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LogiEval, a benchmark for evaluating logical reasoning in large reasoning models (LRMs). Unlike existing benchmarks that focus on domain-specific tasks, LogiEval isolates domain-agnostic logical reasoning by curating 6,235 high-quality questions from human-administered exams. It spans four reasoning types—deductive, inductive, abductive, and analogical—and ten task formats, including syllogisms, artificial language, and situational judgment.\nThe authors evaluate seven state-of-the-art reasoning models (e.g., DeepSeek-R1, Claude-3.7 Sonnet, o4-mini) and find that:\n•\tModels excel at 4-choice argument analysis and analogical reasoning (sometimes surpassing humans).\n•\tThey struggle with formal deductive tasks like syllogisms and blood relations.\n•\tHuman and model difficulty patterns do not align: models often solve “hard” human problems but fail on “medium” ones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tUnifying four reasoning types and ten task formats in a single, exam-sourced benchmark.\n2.\tPreserves original language (English/Chinese), maintaining linguistic nuance and avoiding translation bias.\n3.\tEvaluation of 7 top 2025 LLMs with consistent prompting and answer extraction. Includes human performance baselines and statistical significance testing."}, "weaknesses": {"value": "1.\tMost items are multiple-choice; open-ended or proof-based reasoning is underrepresented.\n2.\tHuman accuracy is derived from historical exam pass rates, which may not reflect controlled, per-item performance.\n3.\tDoes not assess reasoning validity or explanation quality—only final answer correctness."}, "questions": {"value": "Please refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6b854CjSGb", "forum": "uDP3P9TTRA", "replyto": "uDP3P9TTRA", "signatures": ["ICLR.cc/2026/Conference/Submission23366/Reviewer_KkTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23366/Reviewer_KkTK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914743142, "cdate": 1761914743142, "tmdate": 1762942629092, "mdate": 1762942629092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LogiEval, a benchmark for evaluating logical reasoning in large language models. The benchmark comprises 6,235 questions sourced from high-stakes examinations (LSAT, GMAT, Civil Service Exams), covering four reasoning types (deductive, inductive, analogical, abductive) and ten task formats. The authors evaluate seven state-of-the-art reasoning models released in 2025, revealing uneven performance across tasks despite strong aggregate scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation addressing the gap between domain-specific benchmarks and fundamental reasoning evaluation\n- The paper is well-organzied and clearly written."}, "weaknesses": {"value": "- Insufficient human performance analysis. \n- Limited case study. It is important to eval the benchmark via case studies. \n- What is the major difference between this work and previous logical evaluation benchmarks, such as GLoRE?"}, "questions": {"value": "Please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AzyYFNJNoN", "forum": "uDP3P9TTRA", "replyto": "uDP3P9TTRA", "signatures": ["ICLR.cc/2026/Conference/Submission23366/Reviewer_EJEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23366/Reviewer_EJEJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987185144, "cdate": 1761987185144, "tmdate": 1762942628787, "mdate": 1762942628787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a benchmark called LogicEval which evaluates the reasoning abilities of LLMs.\nIt is collected from several human examination, including LSAT, GMAT, Civil Service Exams.\nAlso a subset of LogicEval called LogicEval-hard is presented by filtering too easy examples through the results of some weak LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. **Lack of Novelty**. There are too many benchmarks for evaluating logical reasoning, e.g. [1][2], even mutimodal one [3]. \n\n[1] JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models https://arxiv.org/abs/2510.18855\n\n[2] LogicGame: Benchmarking Rule‑Based Reasoning Abilities of Large Language Models https://arxiv.org/abs/2408.15778\n\n[3] MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs https://arxiv.org/abs/2505.21327\n\nI believe it can be easy to find more with tools like DeepResearch and human double checking.\n\n2. **Almost saturated with evaluation on out-of-dated LLMs**. Even LLMs like Deepseek-R1 / Gemini 2.0 / Claude 3.7 / Grok 3 can get 75%+ on this benchmarks as reported. But today we have:\n\nDeepseek-R1 -> Deepseek 3.2\n\nGemini 2.0 -> Gemini 2.5\n\nClaude 3.7 -> Claude 4.5\n\nGrok 3 -> Grok 4\n\nI believe this benchmark is almost saturated.\n\nAs the LogicEval-Hard one, please present the results on the latest LLMs.\n\n3. **Data leakage**. There's NO NEW DATA in this benchmark, just collected existing exams, and it could be suffered from data leakage."}, "questions": {"value": "1. Please clarity why the community still needs this benchmark at this time.\n\n2. Please evaluate on the latest and powerful LLMs, and present the results to the readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "n052MvdDMZ", "forum": "uDP3P9TTRA", "replyto": "uDP3P9TTRA", "signatures": ["ICLR.cc/2026/Conference/Submission23366/Reviewer_pnNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23366/Reviewer_pnNX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995109231, "cdate": 1761995109231, "tmdate": 1762942628472, "mdate": 1762942628472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}