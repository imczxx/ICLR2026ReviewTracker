{"id": "QAOLaVXiLg", "number": 15544, "cdate": 1758252546938, "mdate": 1759897300366, "content": {"title": "SYNQUE: Estimating Synthetic Dataset Quality Without Annotations", "abstract": "We introduce and formalize the Synthetic Dataset Quality Estimation (SYNQUE) problem: ranking synthetic datasets by their expected real-world task performance using only limited unannotated real data.\nThis addresses a critical and open challenge where data is scarce due to collection costs or privacy constraints.\nWe establish the first comprehensive benchmarks for this problem by introducing and evaluating proxy metrics that choose synthetic data for training to maximize task performance on real data.\nWe introduce the first proxy metrics for SYNQUE by adapting distribution and diversity-based distance measures to our context via embedding models.\nTo address the shortcomings of these metrics on complex planning tasks, we propose SYNQUE, a novel proxy that leverages large language model reasoning.\nOur results show that SYNQUE proxies correlate with real task performance across diverse tasks, including sentiment analysis, Text2SQL, web navigation, and image classification, with LENS consistently outperforming others on complex tasks by capturing nuanced characteristics.\nFor instance, on text-to-SQL parsing, training on the top-3 synthetic datasets selected via SYNQUE proxies can raise accuracy from 30.4\\% to 38.4 (+8.1)\\% on average compared to selecting data indiscriminately.\nThis work establishes SYNQUE as a practical framework for synthetic data selection under real-data scarcity and motivates future research on foundation model-based data characterization and fine-grained data selection.", "tldr": "We introduce the SynQuE problem: estimating quality of synthetic datasets anchored on real unannotated data alongside baselines and our novel LENS metric.", "keywords": ["Evaluation", "Synthetic Data"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f22153889987ff27e3166a519ef69826f3257f45.pdf", "supplementary_material": "/attachment/75bbe9f91946deb8f844ebdfdf7c7732cb00f92e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Synthetic Dataset Quality Estimation (SYNQUE) framework, which addresses the problem of ranking synthetic datasets by their expected real-world task performance using only a small set of unlabelled real samples. The authors propose two types of proxy metrics to estimate dataset quality without training task-specific models. The first category includes representation-based proxies that adapt traditional distributional and diversity measures such as MMD, PAD, and MAUVE to an embedding space. The second, called LENS (LLM-Evaluated Normalized Score), is an LLM-based approach that employs rubric-guided reasoning to compare synthetic and real data directly in natural language. Experiments across diverse domains, including sentiment analysis, Text2SQL, image classification, and web navigation, show that these proxies correlate well with downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The main contribution of the paper is the introduction of a novel and practically relevant problem setting, Synthetic Dataset Quality Estimation (SYNQUE), which focuses on evaluating and ranking synthetic datasets using only unlabelled real data. By formulating the problem without using labelled data, the approach substantially reduces computational cost, avoiding repeated model training while still providing informative quality estimates.\n\n* The paper proposes several representation-based proxy metrics that estimate data quality through measures of diversity and distributional alignment, without requiring labelled real samples or downstream model training.\n\n* The authors further present LENS, an LLM-based evaluation framework that introduces principled debiasing strategies to mitigate order bias, label bias, and score bias, leading to more consistent and interpretable LLM-based judgments.\n\n* It addresses a timely and important challenge in understanding and quantifying the quality of synthetic data, which is increasingly critical for large-scale model development.\n\n* The paper provides comprehensive experimental validation across diverse domains, showing that the proposed proxies correlate strongly with downstream task performance, with LENS achieving the most reliable results on complex and long-horizon tasks."}, "weaknesses": {"value": "* **Framing and originality could be clarified.**  \n  While the problem setting is interesting, the distinction between synthetic dataset quality estimation and general dataset quality estimation remains under-specified. The paper does not fully explain what makes evaluating synthetic data uniquely challenging beyond the label-free constraint. A more explicit comparison with recent works such as [1], [2], and [3] would help position SYNQUE within the broader landscape of data selection and importance estimation. In particular, [2] also studies quality estimation for LLM-generated data and highlights the gap between synthetic and real data, which could provide valuable context for the present work.\n\n* **Limited theoretical motivation for proxy metrics.**  \n  The paper provides intuitive explanations for why metrics such as MMD², PAD, and MAUVE may correlate with downstream performance, but lacks theoretical justification or formal conditions under which these proxies should succeed or fail. Similarly, the rationale for why LENS’s rubric-based reasoning captures true data–distribution similarity remains largely empirical.\n\n* **Quality–diversity balance is not explicitly addressed.**  \n  Each proposed proxy primarily captures either data quality (alignment) or diversity (coverage), but the paper does not present a unified formulation that balances the two. Prior work [1, 2] has shown that maintaining both quality and diversity is crucial for effective data selection, especially for synthetic data. Discussing or analysing this trade-off would strengthen the contribution.\n\n* **Label-quality robustness is not evaluated.**  \n  Although the framework avoids using labels for real data, it still depends on synthetic data labels that may be noisy or hallucinatory. The method implicitly assumes that these synthetic labels are of reasonable quality and aligned with the downstream task, since all proposed proxies rely solely on input features rather than label consistency. It would be valuable to discuss the performance gap between the proposed label-free setting and small labelled baselines, as well as the impact of varying levels of label noise in the synthetic data.\n\n* **Inter-dataset dependency is not discussed.**  \n  The framework assumes that each synthetic dataset can be evaluated independently, yet in practice, different synthetic datasets may share overlapping samples or originate from similar generative prompts or models. Such dependencies could bias the proxy correlations and ranking results, especially when datasets are not mutually independent. It would be useful to discuss how SYNQUE handles or mitigates inter-dataset overlap and whether the proposed proxies remain valid under such dependencies.\n\n[1] *Harnessing Diversity for Important Data Selection in Pretraining Large Language Models* (ICLR 2025)\n\n[2] *Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification* (ICLR 2025)\n\n[3] *Most Influential Subset Selection: Challenges, Promises, and Beyond* (NeurIPS 2024)"}, "questions": {"value": "* Could you provide more discussion or analysis on how sensitive the representation-based proxies are to the choice of encoder? For example, have you compared different embedding models (e.g., smaller Qwen variants or open-source encoders) to assess whether the correlations remain stable?\n\n\nOverall, the paper is conceptually interesting, but the justification of key assumptions and the empirical analysis could be strengthened to make the contribution more convincing. I encourage the authors to address the above concerns and clarify these points in their rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "awo2kURzfS", "forum": "QAOLaVXiLg", "replyto": "QAOLaVXiLg", "signatures": ["ICLR.cc/2026/Conference/Submission15544/Reviewer_mUYC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15544/Reviewer_mUYC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490881381, "cdate": 1761490881381, "tmdate": 1762925820988, "mdate": 1762925820988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes SYNQUE, a method for efficiently ranking synthetic datasets using only a small unlabeled real subset, and establishes the first comprehensive benchmark with several proxy scores specifically designed for synthetic data selection, eliminating the need for exhaustive model training and evaluation. It then introduces LENS, an LLM-based rubric scorer with debiasing to compare synthetic and real data in language. It evaluates all proxies on multiple downstream tasks, showing some correlations with real task performance overall."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, making the ideas easy to follow and the methodology straightforward to understand.\n2. The paper runs a decent number of experiments to substantiate its claims.\n3. They designed the experiments carefully, including baselines, ablations, and sensitivity checks, making the findings comprehensive and more credible."}, "weaknesses": {"value": "Many conclusions hinge on specific experimental settings and hyper-parameters. The paper lacks a theoretical framework linking each proxy and LENS to expected real data performance, so it’s unclear how stable the findings are under different encoders, kernels, classifier choices, and LLMs."}, "questions": {"value": "1. How did you determine the k value in k-medoids for MDM, and how sensitive are your results to k?\n2. Have you tried classifiers besides XGBoost for PAD?\n3. You fix qte-Qwen2-7B for text and E5-V for images. Do the rankings persist if you swap the encoders?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3LqGvqFCrS", "forum": "QAOLaVXiLg", "replyto": "QAOLaVXiLg", "signatures": ["ICLR.cc/2026/Conference/Submission15544/Reviewer_XbUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15544/Reviewer_XbUz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724362278, "cdate": 1761724362278, "tmdate": 1762925820262, "mdate": 1762925820262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formalizes the Synthetic Dataset Quality Estimation (SYNQUE) problem, which aims to rank synthetic datasets based on their expected downstream task performance using only a small amount of unannotated real data. The authors adapt several existing representation-based metrics (MDM, MMD², PAD, MAUVE) for this task and propose a novel LLM-based metric, LENS (LLM-Evaluated Normalized Score), which leverages LLM reasoning to create a rubric for scoring data quality. The effectiveness of these proxy metrics is evaluated across a diverse set of tasks, including sentiment analysis, Text2SQL, image classification, and web navigation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Interesting and practical problem: The paper addresses an interesting and practical problem: how to select the most effective synthetic dataset for a real-world task in scenarios where authentic data is scarce or difficult to obtain.\n\n2.Nice tables: The tables are clear and intuitive."}, "weaknesses": {"value": "1.Novelty of representation-based methods: The suite of representation-based proxy metrics (MDM, MMD², PAD, MAUVE) is sourced from existing literature. The current presentation feels more like a straightforward application or \"pastiche\" of prior work rather than a novel contribution. The authors should explicitly clarify what their specific innovations are in adapting these metrics to the SYNQUE problem.\n\n2.Limited overall contribution: The paper's overall contribution feels limited. As noted, the representation-based methods lack novelty. The main proposed method, LENS, is based on LLM prompting, which, while effective, reduces the perceived technical depth of the contribution. Furthermore, its poor performance on vision tasks limits its practical application. Notably, LENS is often outperformed by simpler metrics like MMD² and Mauve on several tasks (e.g., Sentiment and Text2SQL), which further weakens the paper's central claims about its new method's superiority.\nThis combination makes the paper's overall innovative leap seem modest.\n\n3.Lack of clarity in PAD method details: The description of the Proxy-A-Distance (PAD) method is insufficient. It is unclear what the output of the classifier G(x) specifically represents. Is it the probability of x belonging to the synthetic dataset, the real dataset, or something else? \n\n4.Issues with experimental tables: The accuracy of some numerical values in Table 2 is questionable. For example, some improvements do not seem to add up correctly. Could this be due to rounding errors, or is there a miscalculation? The authors are strongly encouraged to double-check and verify all reported results.\n\n5.Inconsistent claims regarding experimental results: The paper asserts on page 8 (Line 430) that \"the 32B debiased LENS is the only proxy that consistently achieves positive correlation and improves top-3 task performance across all tasks and splits\". However, the results in Table 3 show that for the Image task on \"Split 1\", the debiased LENS 32B metric yielded negative correlations (Spearman: -.28 Pearson: -.28). Furthermore, Table 2 shows that for this same split, the top-3 performance (56.4) was actually worse than the test mean (57.2), failing to show improvement. \n\n6.Formatting and presentation issues: \na. The caption for Figure 1 is missing a period at the end of the last sentence.\nb. There appear to be formatting errors with the footer on page 2 and the header on page 3.\nc. The appendix contains unnecessary large blank spaces."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D9QwrHgZbb", "forum": "QAOLaVXiLg", "replyto": "QAOLaVXiLg", "signatures": ["ICLR.cc/2026/Conference/Submission15544/Reviewer_qUCk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15544/Reviewer_qUCk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915017593, "cdate": 1761915017593, "tmdate": 1762925819475, "mdate": 1762925819475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces synthetic dataset quality estimation method to evaluate synthetic datasets without using labeled data. The goal is to predict which synthetic dataset will produce the best performance on real-world tasks, using only a small set of unlabeled real samples. It proposes representation-based metrics and LLM-evaluated normalized score, offering a robust solution for synthetic data selection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work is well-motivated, addressing realistic scenarios where real data are scarce or expensive to obtain.\nThe authors clearly define order bias, label bias, and score bias, and make an interesting attempt to systematically address these issues. Additionally, the paper presents the prompts in a transparent manner, enabling clear understanding."}, "weaknesses": {"value": "The experiments appear to rely on a relatively narrow set of scoring baselines. It would be useful to evaluate a broader range of models to better validate its superiority over existing state-of-the-art methods. \n\nThe method also seems highly sensitive to prompt formulation, which introduces both human and model-dependent biases. \nThe paper would benefit from more qualitative examples. Providing comparisons between selected and rejected synthetic examples would help illustrate its practical utility."}, "questions": {"value": "To further validate the generality of the proposed scoring method, it would be beneficial to evaluate it with recent state-of-the-art LLMs. I am curious about the correlation performance when recent state-of-the-art LLMs are used for scoring instead of the current backbone. How sensitive are the results?\n\nIs the method robust when the real data are extremely limited or noisy?\nHow does performance vary with different amounts of real data?\nWhat is the minimum number of real samples required to achieve a reasonable level of performance, and at what point does the performance saturate? Additionally, how does this behavior vary across different tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dP2brGZeqf", "forum": "QAOLaVXiLg", "replyto": "QAOLaVXiLg", "signatures": ["ICLR.cc/2026/Conference/Submission15544/Reviewer_jWVL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15544/Reviewer_jWVL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998508568, "cdate": 1761998508568, "tmdate": 1762925819000, "mdate": 1762925819000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}