{"id": "OmBwRijFBA", "number": 8786, "cdate": 1758098169280, "mdate": 1759897764065, "content": {"title": "Multivariate Time Series Imputation with Signal-Noise Disentangled Graph Propagation", "abstract": "Missing data are pervasive in real-world multivariate time series, particularly in large-scale, high-frequency systems. Although recent graph-based and transformer-based methods achieve state-of-the-art (SOTA) performance by performing spatial graph propagation or leveraging self-attention mechanisms, they suffer from two key limitations: (1) treating each time series as an indivisible whole, without uncovering its internal temporal dynamics, and (2) relying on linear projections to connect spatial and temporal representations, which insufficiently depicts the complex spatial-temporal interactions. Motivated by the above limitations, we propose GraphTSI, a Graph-based multivariate Time Series Imputation method with signal-noise decomposition, where the signal component captures predictable dynamics and the noise component reflects unpredictable exogenous shocks of time series. To enable robust decomposition, we propose a prediction–subtraction framework where the prediction step progressively estimates predictable signal component, while the subtraction step uses the discrepancy between this estimate and the observed values to extract the exogenous noise component. Furthermore, for effective spatial-temporal interactions, we build an augmented bipartite graph that captures adaptive, non-linear transformation between spatial and temporal dimensions, and propagates signal and noise components through neighboring time series. Extensive experiments across nine datasets from three real-world domains demonstrate the superiority of GraphTSI, with average MAE improvements of 9.557% and 17.215% over graph-based and transformer-based SOTA methods, respectively.", "tldr": "", "keywords": ["Missing Data; Data Imputation; Multivariate Time Series;"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6427830e3b0e392a310871ba26dedab331975219.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GraphTSI, a graph-based model for multivariate time series imputation that explicitly separates each sequence into signal and noise components. To address the limitations of insufficient component separation and weak spatial–temporal modeling in existing models, GraphTSI introduces a prediction–subtraction mechanism and an augmented bipartite graph for adaptive information exchange between spatial and temporal representations.\nAcross nine real-world datasets, GraphTSI achieves the best performance, reducing MAE by 17.2% over Imputeformer and 9.6% over GSLI. Additional experiments confirm its robustness, interpretability, and strong performance even with up to 95% missing data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper makes a good contribution to multivariate time series imputation. The proposed GraphTSI model is  novel and interesting, combining temporal prediction and spatial reasoning in a unified, interpretable architecture. The introduction of signal–noise decomposition is  insightful.\nExperimentally, the paper is comprehensive and convincing. Results across nine diverse real-world datasets demonstrate consistent and significant improvements over strong baselines. The ablation studies, robustness tests, and downstream task evaluations provide thorough evidence supporting the model’s design choices.\nFinally, the paper is well-structured and clear."}, "weaknesses": {"value": "W1: The paper does not clearly explain the backward propagation mechanism in the bi-unidirectional design.\n\nW2: The computational complexity of GraphTSI is not adequately analyzed. \n\nW3: The dataset selection is heavily skewed toward traffic domains. It would strengthen the generality of the model to include datasets from other domains such as healthcare, energy, or finance.\n\nW4: The set of baseline models could be expanded.\n\n[1] Zhang S, Wang S, Miao H, et al. Score-cdm: Score-weighted convolutional diffusion model for multivariate time series imputation. IJCAI 2024\n\n[2] Liang G, Tiwari P, Nowaczyk S, et al. Higher-order spatio-temporal physics-incorporated graph neural network for multivariate time series imputation. CIKM 2024\n\n[3] Gao S, Koker T, Queen O, et al. Units: A unified multi-task time series model. Neurips 2024\n\n[4] Ahmed N, Yalavarthi V K, Schmidt-Thieme L. Motif-aware Graph Neural Networks for Networked Time Series Imputation. AAAI 2025\n\n[5] Liu S, Li X, Chen Y, et al. Disentangling Dynamics: Advanced, Scalable and Explainable Imputation for Multivariate Time Series. TKDE 2025\n\n[6]  Chen X,  Cheng Z, Cai H, Saunier N, Sun L: Laplacian Convolutional Representation for Traffic Time Series Imputation. TKDE 2024\n\nW5: It would be helpful to evaluate the model under diverse missing patterns (disjoint, MCAR, Overlap, Blackout)\n\n[7] Mind the Gap: An Experimental Evaluation of Imputation of Missing Values Techniques in Time Series"}, "questions": {"value": "Q1: Using the reversed time series for imputation (backward) might raise concerns about potential information leakage. Does leveraging future patterns to enhance the semantic representation of current time steps constitute a form of “cheating”?\n\nQ2: In the augmented bipartite graph, the number of spatial and temporal nodes usually differs. How does the model ensure stable message passing in extreme cases, such as very long sequences or highly sparse graphs?\n\nQ3: The signal–noise decomposition assumes independent noise components, but in many real settings, noise can be temporally autocorrelated or cross-variable correlated. Would the proposed prediction–subtraction mechanism remain stable and effective in such cases?\n\nI will reconsider my rating if the questions and weakness are handled properly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NUstfmht3X", "forum": "OmBwRijFBA", "replyto": "OmBwRijFBA", "signatures": ["ICLR.cc/2026/Conference/Submission8786/Reviewer_6ZuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8786/Reviewer_6ZuJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760548345692, "cdate": 1760548345692, "tmdate": 1762920560513, "mdate": 1762920560513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To tackle the multivariate time series imputation problem, this paper introduces GraphTSI which separates the signal and noise in time series data and utilize a bipartite graph constructed between temporal nodes and spatial nodes. By using transformer architecture and graph message passing methods, the proposed GraphTSI can outperform multiple strong baselines on multiple real-world datasets. Extensive experiments such as ablation study, missing rates analysis, as well as downstream tasks have been conducted to prove the effectiveness of the model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written with clear figures, tables, equations. The motivation and idea of the models in the paper is also grounded.\n2. Extensive empirical experiments as well as theoretical analysis have been included in this paper to support the claims and demonstrate GraphTSI’s effectiveness.\n3. Anonymous codes have been provided for better reproducibility of the proposed method."}, "weaknesses": {"value": "1. Although the proposed GraphTSI shows compelling performance compared to other baselines, there is no efficiency / complexity analysis of the algorithms compared to others. It would help strengthen the contributions of GraphTSI if it is relatively efficient at the same time.\n2. Theorem 1 in appendix is valid when the assumption of noise independence holds between different observations. However, it might not always hold true in real world since some noises collected in data might come from weather or human activities, etc.\n\n\n\n**Suggestions**\n1. There is a typo for section 6 name: it should be “conclusion” instead of “consluion”.\n2. To help address weakness 2, it would be great if the authors can conduct some experiments by adding some manually crafted noise such as temporal dependent noise to the data. Analyzing the model performance and the signal/noise separate modeling under such cases would strengthen the contribution of the paper."}, "questions": {"value": "1. Since transformer is able to handle extremely long window sizes, I wonder if the authors have tried much larger windows sizes like 512 / 1024. What’s the performance? Is there any bottleneck from the algorithms for instance the bipartite graph etc.? Will model having larger windows sizes with more temporal receptive field help improve the imputation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Qvh7UMOTL", "forum": "OmBwRijFBA", "replyto": "OmBwRijFBA", "signatures": ["ICLR.cc/2026/Conference/Submission8786/Reviewer_PEkM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8786/Reviewer_PEkM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761437772255, "cdate": 1761437772255, "tmdate": 1762920559879, "mdate": 1762920559879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a graph-based method that incorporates bidirectional attention layers and spatial attention blocks within a two-step propagation scheme using virtual spatial nodes (analogous to several hierarchical attention methods proposed in the literature). Moreover, the architecture includes a mechanism to refine predictions based on the error at observed nodes. The resulting architecture achieves remarkable performance on a solid selection of datasets. Despite this, the technical novelty of the paper is overstated and relatively limited, and there are also some issues in the empirical evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Strong empirical performance.  \n* The architecture design appears sound and well-motivated.  \n* Using reconstruction error as input to refine imputations is an interesting idea, though not novel."}, "weaknesses": {"value": "### Main comments\n\n* **Overstated technical novelty.**  The claimed technical novelty is overstated and relatively limited.  \n  - The paper presents as its main contribution the introduction of a component that propagates spatiotemporal information using a bipartite graph. However, this approach is equivalent to **many** hierarchical attention mechanisms that rely on hub nodes and two-step propagation (see, e.g., [1, 2] and hierarchical attention in [3]). Similar ideas have also been applied in spatiotemporal graph-based models (e.g., [4]).  \n  - The idea of feeding prediction residuals back into the model to refine predictions is reasonable but well-established. It is a known technique in time series analysis — for instance, in ARIMA models and, more recently, in neural architectures such as [5].  \n  - The paper claims that existing architectures struggle to propagate information effectively. However, as mentioned above, the literature has already explored a wide variety of mechanisms for effective spatiotemporal propagation. For example, the abstract states that prior models “suffer from two key limitations: (1) treating each time series as an indivisible whole, without uncovering its internal temporal dynamics, and (2) relying on linear projections to connect spatial and temporal representations.” This is hardly true for many modern architectures (e.g., [6]) that integrate spatial and temporal propagation more effectively.\n\n* **Possible issue in the empirical evaluation.**  In Appendix A.2, the authors mention training on simulated missing data in the training set. However, this setup would be unrealistic in the real-world scenarios that those missing data are simulating (since one cannot train on data that are actually missing). Many of the included baselines (e.g., [7]) assume that simulated missing data are *not* available during training. Could the authors clarify this point? Are all baselines trained on the same ground-truth data?\n\nGiven these issues and limitations, I cannot recommend acceptance at this stage.\n\n### Minor comments / suggestions\n\n* One component of the model is described as modeling the *noise* part of the signal. However, this can be misleading. Claiming that the model “extracts” the noise component implies assuming the predictor is optimal and fully captures all temporal dependencies. It might be more appropriate to describe this as modeling the **error** or **residual** from the first imputation stage, which is then refined. Moreover, stating that correlations in the noise component are purely spatial seems questionable: for example, the “system-wide incidents” mentioned would likely persist for more than one timestep.  \n* In the propagation step (line 320), how are the neighbors of each node defined? Is the graph fully connected? If so, why refer to it as a graph — how does it differ from standard spatial attention?  \n* Many cited papers refer to their arxiv versions despite having published counterparts; please update the references accordingly.\n\n---\n\n**References**\n\n\n[1] Ravula et al., \"ETC: Encoding long and structured inputs in Transformers\", EMNLP 2020\\\\\n\n[2] Dolga et al., \"Latte: Latent Attention for Linear Time Transformers\" arxiv 2024\\\\\n\n[3] Marisca et al., \"Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations\", NeurIPS 2022 \\\\\n\n[4] Satorras et al., \"Multivariate time series forecasting with latent graph inference\", arxiv 2022\\\\\n\n[5] Kim et al., \"Residual Correction in Real-Time Traffic Forecasting\", CIKM 2022\\\\\n\n[6] Wu et al., \"Traversenet: Unifying space and time in message passing for traffic forecasting\" TNNLS 2022\\\\\n\n[7] Cini et al., \"Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks\", ICLR 2022"}, "questions": {"value": "Please comment on the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wdw4zGRYzL", "forum": "OmBwRijFBA", "replyto": "OmBwRijFBA", "signatures": ["ICLR.cc/2026/Conference/Submission8786/Reviewer_F8oe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8786/Reviewer_F8oe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850763324, "cdate": 1761850763324, "tmdate": 1762920559439, "mdate": 1762920559439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GraphTSI, a graph-based framework for multivariate time series imputation that integrates signal–noise decomposition with spatial–temporal graph propagation. The method introduces a prediction–subtraction mechanism to separate predictable signal components from unpredictable noise and employs an augmented bipartite graph to model non-linear spatial–temporal interactions. Experiments on nine real-world datasets reportedly show that GraphTSI outperforms recent graph-based and transformer-based methods in imputation accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of combining signal–noise decomposition with graph-based spatial–temporal modeling is conceptually interesting and may help disentangle predictable and unpredictable components in temporal systems.\n\n- The proposed prediction–subtraction framework and augmented bipartite graph demonstrate an effort to move beyond standard linear projections.\n\n- The experiments cover multiple real-world datasets with ablation and robustness analyses, showing reasonable empirical effort."}, "weaknesses": {"value": "Overall, the presentation requires substantial improvement before the technical contributions can be properly evaluated. Key weaknesses include:\n\n- **Weak motivation**: Each proposed module (signal–noise decomposition, prediction–subtraction, augmented bipartite graph) lacks a clear rationale for its necessity or effectiveness.\n\n- **Limited methodological depth**: The paper describes design choices without strong theoretical or empirical justification, making it difficult to assess novelty or soundness.\n\n- **Poor writing quality**: The exposition is hard to follow, with unclear transitions and dense descriptions that obscure the main ideas.\n\n- **Others**: The text in Figures 1 and 2 is too small to read comfortably, further hindering understanding."}, "questions": {"value": "1. The paper claims that standard MLPs cannot distinguish missing from observed values and introduces a miss-aware embedding. However, Equation (5) does not clearly show how missingness is encoded. Does it explicitly use the binary mask, or rely on separate learnable parameters? A clearer mathematical explanation or ablation is needed.\n2.  In *Augmented Bipartite Graph Transformation,* the definitions of $\\mathcal{V}^{\\mathrm{T}}$ and $\\mathcal{V}^{\\mathrm{S}}$ are unclear. How are these node embeddings initialized or derived—via separate encoders or direct parameterization? Clarification would improve reproducibility and understanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9InCzUWtAz", "forum": "OmBwRijFBA", "replyto": "OmBwRijFBA", "signatures": ["ICLR.cc/2026/Conference/Submission8786/Reviewer_cqdy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8786/Reviewer_cqdy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985373031, "cdate": 1761985373031, "tmdate": 1762920558942, "mdate": 1762920558942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}