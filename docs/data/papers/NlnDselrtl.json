{"id": "NlnDselrtl", "number": 8821, "cdate": 1758099233513, "mdate": 1759897761956, "content": {"title": "Riemannian Variational Flow Matching for Material and Protein Design", "abstract": "We present Riemannian Gaussian Variational Flow Matching (RG-VFM), a geometric extension of Variational Flow Matching (VFM) for generative modeling on manifolds. In Euclidean space, predicting endpoints (VFM), velocities (FM), or noise (diffusion) are largely equivalent due to affine interpolations. On curved manifolds this equivalence breaks down, and we hypothesize that endpoint prediction provides a stronger learning signal by directly minimizing geodesic distances. Building on this insight, we derive a variational flow matching objective based on Riemannian Gaussian distributions, applicable to manifolds with closed-form geodesics. We formally analyze its relationship to Riemannian Flow Matching (RFM), exposing that the RFM objective lacks a curvature-dependent penalty - encoded via Jacobi fields - that is naturally present in RG-VFM. Experiments on synthetic spherical and hyperbolic benchmarks, as well as real-world tasks in material and protein generation, demonstrate that RG-VFM more effectively captures manifold structure and improves downstream performance over Euclidean and velocity-based baselines.", "tldr": "We derive a variational objective for flow matching on manifolds with closed-form geodesics and test it on material and protein backbone generation.", "keywords": ["Flow matching", "variational inference", "riemannian manifolds", "material generation", "metal-organic framework", "protein backbone generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8230566322f7abdb484adaf48e21ea28bc65d06c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Riemannian Variational Flow matching (RG-VFM), a method that is designed for conducting generative modeling on manifolds. RG-VFM extend variational flow matching to general geometric objects. RG-VFM replaces the velocity matching objective in RFM with a distribution matching objective that is analogue to VFM, but with the distribution defined on a Riemannian manifold. The authors then compare the Jacobi fields of RG-VFM and RFM and show theoretically that such a reparameterization of regression objective is capable of capturing the high-order curvature of the manifold. Empirical results suggest the success of RG-VFM in capturing curvature information and modelling geometries."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The formalization and mathematical derivations are lucid and well-presented.\n\n- The manuscript is well-structured with a clear logical progression, making the core arguments easy to follow.\n\n- The novelty of the proposed method and its key distinctions from prior literature are clearly articulated."}, "weaknesses": {"value": "My primary concerns relate to the empirical evaluation and the clarity of the motivation.\n\n- Empirical Results: The significance of the empirical results is not entirely convincing. In the real-world protein backbone generation benchmarks, the high standard deviations suggest that the performance improvement over existing models may not be statistically significant. Furthermore, for the structure prediction results, standard deviations are not reported, which makes it difficult to ascertain the significance of the reported performance gains.\n\n- Motivation: In the abstract, the authors state that the equivalence of endpoint, velocity, and noise prediction breaks down on curved manifolds. It is not immediately clear how RG-VFM alleviates this issue, or how this observation directly motivates the proposed method. I would appreciate clarification if this point was addressed in the manuscript and I overlooked it."}, "questions": {"value": "- Motivations: Could the authors elaborate on how RG-VFM addresses the breakdown of equivalence between endpoint, velocity, and noise prediction on curved manifolds? Furthermore, how does this challenge directly motivate the design of RG-VFM over other approaches?\n\n- More clarification on the incorporation of high-order curvature. Can I get more clarification on what is the practical impact of the higher-order curvature term on the learning problem? For example, does its inclusion confer specific advantages when learning on heterogeneous manifolds against homogeneous ones?\n\n- Computational Complexity: Could the authors comment on the change of computational complexity when learning with RG-VFM? If learning the higher-order curvature information using RG-VFM induces more computations, relative to methods like RFM?\n\n- Empirical Reults (Table 1): Why does the external view of RG-VFM exhibit significantly better performance when learning on the hyperboloid manifold? Is this phenomenon specific to learning on Riemannian manifolds, or could it also occur in Euclidean settings under certain conditions?\n\n- This question is for my own curiosity: The relationship between RG-VFM and RFM appears analogous to that between Transition Matching [1] and Flow Matching in Euclidean space. Could the authors comment on this analogy? Specifically, is the model effectively learning a conditioned transition kernel, approximating a jump from $x_t$ to $x_1$?\n\n\n[1] Transition Matching: Scalable and Flexible Generative Modeling. N. Shaul et al. NeurIPS 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XGnaTq3UO6", "forum": "NlnDselrtl", "replyto": "NlnDselrtl", "signatures": ["ICLR.cc/2026/Conference/Submission8821/Reviewer_423B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8821/Reviewer_423B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948491823, "cdate": 1760948491823, "tmdate": 1762920589778, "mdate": 1762920589778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows theoretically that training (variational) flow matching on end-point prediction in Riemannian geometry yields a more precise loss, that better accounts for the structure of the manifold. They validate empirically their losses on a variety of datasets, including large-scale ones with applications for science."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-presented: the text is clear and simple.\n- The proofs provided in the appendix are well-detailed.\n- Proposition 4.1 (and 4.3) is an interesting contribution, showing a limitation in Riemannian Flow Matching [2] – the baseline method for Riemannian generative modelling – and demonstrating the method’s superiority in accounting for curvature terms, theoretically.\n- The proposed fix is very simple.\n- The empirical validation is rather extensive and convincing for the given examples, including large-scale ones. Overall, the method improves (arguably slightly) over the provided baselines."}, "weaknesses": {"value": "- I am not certain about the initial trichotomy (or the impression that is given when stating is as follows) of “endpoint (VFM), a velocity (FM/RFM), noise (diffusion)”. Diffusion can be trained all three ways, and noise prediction was introduced in DDPM [1], as it empirically produced better performance. Same goes for flow matching. (Indeed, though, because of the linearity in the path, all reparameterisations are *theoretically* equivalent.)\n- It seems to me that the words “intrinsic” and “extrinsic” are not used properly. If I am not mistaken, intrinsic (informally) “live directly on the considered space”; extrinsic coordinates depend on the ambient space. For instance, intrinsic on the 2-sphere would be the angles $\\theta$ and $\\phi$; extrinsic would be the usual 3D coordinates. Here, the authors seem to use it for projected on the manifold or not. (See Figure 2.) Modelling intrinsically is known to be difficult.\n- I would argue that the paper is not particularly novel on the methods front (not that it is without contributions), mostly (variational) flow matching on Riemannian geometries.\n- Sampling from the Riemannian Gaussian is not trivial on certain manifolds. It seems that the authors have omitted this discussion in the paper.\n- Perhaps I have missed out on this, but it seems that there is no discussion on the hypotheses made about the manifold (homogeneity and existence of closed form geodesics).\n- Training for end-point prediction is not new [3]."}, "questions": {"value": "- Did I misunderstand your usage of the words “intrinsic” and “extrinsic”?\n- What is the novelty in your method [3], beyond the theoretical guarantees: isn’t end-point prediction a rather well-known “trick”?\n- Does homogeneity alongside existence of closed form geodesics not trivialise the structure of the manifolds considered? (I am not certain at all, I would like to discuss.)\n- Have you had the opportunity to try out your method on manifolds where the higher order terms you mention appear (non-constant curvature)? I understand that you might not have, as the examples could be rare, but in which case the contribution of the paper is less significant, practically speaking. This question can also be understood as “Why does Riemannian Flow Matching work so well if it misses those important terms?” (or are they not so important therefore?)\n\nOverall, I think the paper is rather sound and a good read, but the contribution feels relatively small.\n\n### References\n\n[1] Jonathan Ho, Ajay Jain, Pieter Abbeel. “Denoising Diffusion Probabilistic Models”\n\n[2] Ricky T.Q. Chen, Yaron Lipman. \"Flow Matching on General Geometries\"\n\n[3] Bowen Jing, Bonnie Berger, Tommi Jaakkola. \"AlphaFold Meets Flow Matching for Generating Protein Ensembles\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yt5qowUbYv", "forum": "NlnDselrtl", "replyto": "NlnDselrtl", "signatures": ["ICLR.cc/2026/Conference/Submission8821/Reviewer_Kr3K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8821/Reviewer_Kr3K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839603543, "cdate": 1761839603543, "tmdate": 1762920589310, "mdate": 1762920589310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **Riemannian Gaussian Variational Flow Matching (RG-VFM)**, a geometric extension of Variational Flow Matching (VFM) for generative modeling on manifolds.  \nThe authors argue that on curved manifolds, endpoint prediction (as in VFM) provides a stronger learning signal than velocity prediction (as in Flow Matching, FM, or Riemannian FM, RFM) because it directly minimizes **geodesic distances**.  \n\nKey contributions:\n1. **A variational flow matching objective on general Riemannian manifolds**, employing Riemannian Gaussian distributions with closed-form geodesics.  \n2. **A formal analysis linking RG-VFM to RFM** via Jacobi fields, showing that curvature-dependent terms naturally emerge in the RG-VFM objective but are absent in RFM.  \n3. **Experiments** on synthetic (spherical/hyperbolic checkerboards), **materials (MOF)**, and **protein backbone generation** tasks. Results indicate improved geometric consistency and generation quality over Euclidean and velocity-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear and well-motivated theoretical bridge between **variational** and **geometric** generative modeling. The idea of introducing curvature-dependent terms via Jacobi fields to analyze flow-matching losses is novel and elegant.  \n2. The derivation is rigorous, with propositions and proofs connecting RG-VFM and RFM. The mathematical treatment of curvature effects is insightful, especially Proposition 4.3, showing curvature-dependent correction terms.  \n3. The paper is well written and technically organized, with clear mathematical notation and well-illustrated figures (e.g., Fig. 1–3 showing geometric intuition)."}, "weaknesses": {"value": "1. The method is limited to manifolds with **closed-form geodesics**. This assumption restricts applicability to simple spaces (e.g., \\(S^n\\), \\(H^n\\)), leaving open how to handle more general manifolds.  \n2. While results are positive, the experiments lack deeper ablations (e.g., sensitivity to curvature magnitude, loss variants, or effect of variance parameter σ in the Riemannian Gaussian).  \n3. The paper briefly mentions that RG-VFM maintains simplicity of linear flows but does not analyze training/sampling cost relative to RFM or diffusion models."}, "questions": {"value": "1. How would the approach generalize to manifolds without analytic exponential/log maps (e.g., learned manifolds from data)?  \n2. Could the curvature-dependent term be approximated or regularized for arbitrary manifolds, enabling broader applicability?  \n3. In Table 3, could the authors provide variance/error bars for the protein design metrics to quantify statistical significance?  \n4. Is there any insight on the trade-off between extrinsic (RG-VFM-R³) and intrinsic (RG-VFM-M) variants beyond the empirical performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NWTfr2AQfe", "forum": "NlnDselrtl", "replyto": "NlnDselrtl", "signatures": ["ICLR.cc/2026/Conference/Submission8821/Reviewer_fzbJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8821/Reviewer_fzbJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879731822, "cdate": 1761879731822, "tmdate": 1762920588617, "mdate": 1762920588617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on extending Variational Flow Matching (VFM) to Riemannian manifolds through the proposed Riemannian Gaussian Variational Flow Matching (RG-VFM). The authors argue that endpoint prediction provides a stronger learning signal by directly minimizing geodesic distances. The method formulates a variational flow matching objective based on Riemannian Gaussian distributions, applicable to manifolds with closed-form geodesics. They further demonstrate with experiments on spherical, hyperbolic, and real-world datasets that RG-VFM can better captures manifold geometry."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Very well-written paper with excellent explanations and visuals.\n- Variational extension of FM is well motivated; and this paper lifts this idea to Riemannian manifolds.\n- The derivations and design choices are well-justified and appear natural.\n- Good empirical results in two different real-world domains and on one synthetic dataset."}, "weaknesses": {"value": "- The work appears somewhat incremental, as extending Riemannian flow matching to a variational formulation is relatively straightforward. Formally establishing a connection between RG-VFM and RFM through Proposition 4.1 is useful, but not very surprising.\n- The discussion of intrinsic versus extrinsic viewpoints is interesting, yet it remains unclear how the extrinsic perspective offers a clear advantage. While it provides additional flexibility, it essentially represents a trade‑off rather than an optimal method. Similar to the difference between VFM and FM, the primary benefit seems to arise mainly from supervision at the endpoints.\n- Although the ability to choose an arbitrary variational distribution is often presented as a main strength of variational flow matching, this flexibility is rarely reflected in practice, since most experimental setups still rely on simple distributional choices. It is not easy for the reader to establish a direct connection between this generic motivation and the experimental setups (esp. for readers less familiar with these benchmarks). It would be much nicer if the authors could elaborate how this type of generality is used in practical setup."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8sjMPXkSnm", "forum": "NlnDselrtl", "replyto": "NlnDselrtl", "signatures": ["ICLR.cc/2026/Conference/Submission8821/Reviewer_CUa3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8821/Reviewer_CUa3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944638657, "cdate": 1761944638657, "tmdate": 1762920588129, "mdate": 1762920588129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}