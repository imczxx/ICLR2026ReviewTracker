{"id": "QQCadccQqU", "number": 14226, "cdate": 1758230646328, "mdate": 1759897382658, "content": {"title": "FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models", "abstract": "Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model (LVLM) works on map visual question-answering (VQA) often simplify maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains (e.g., geology, urban planning, and environmental assessment) and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20\\% and 37.20\\% accuracy, respectively, far below human performance of 84.87\\%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.", "tldr": "We introduce FRIEDA, a benchmark that tests VLMs' multi-step cartographic reasoning and cross-image inference, highlighting critical gaps in current performance.", "keywords": ["large vision-language model", "multi-step reasoning", "multi-image reasoning", "mapvqa"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81f23b892e2df77d5c25c4ca146fa9a88c6de6c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FRIEDA, a new benchmark designed to evaluate multi-step cartographic reasoning in LVLMs. FRIEDA targets the full spectrum of spatial reasoning, topological (border, equal, intersect, within), metric (distance), and directional (orientation), using real-world maps drawn from public documents such as geological surveys, planning reports, and environmental studies. The benchmark includes 500 questions and 17,030 map images, testing models under two settings: a direct mode, where relevant maps are provided, and a contextual mode, where the model must identify the correct maps before reasoning. Results across 11 state-of-the-art LVLMs show that even the strongest systems (Gemini-2.5-Pro and GPT-5-Think) achieve less than 40% accuracy, far below human performance. The study highlights persistent challenges in spatial and multi-map reasoning, offering FRIEDA as a resource to advance geographic intelligence in LVLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-motivated and timely contribution that fills an underexplored gap in multimodal reasoning research. Its originality lies in extending the evaluation of vision-language models to the domain of cartographic reasoning, which requires understanding spatial relations, map symbology, scales, and orientation—skills that differ fundamentally from those assessed in standard visual question answering or chart-understanding benchmarks. The formulation of three spatial-relation categories (topological, metric, and directional) is conceptually strong and well grounded in GIS literature, offering a structured way to test spatial intelligence in LVLMs.\n\nIn terms of quality, the benchmark construction appears rigorous, combining LLM-assisted question generation with extensive human verification by multiple annotators, ensuring reliability of the ground truth. The inclusion of a human performance baseline and evaluation across both direct and contextual settings enhances the depth of analysis and provides a meaningful comparison point for model performance. The paper is also clearly written and logically organized, guiding readers from the motivation through dataset design, evaluation protocol, and results. The argumentation in the introduction—especially the rationale for curating maps from real public documents—is convincing and helps establish the benchmark’s practical relevance.\n\nFinally, the work is significant because it pushes LVLM evaluation toward a domain that mirrors real-world map interpretation tasks relevant to areas like urban planning and environmental monitoring. By revealing a substantial human-model gap in spatial reasoning, FRIEDA sets the stage for a new line of research on geospatial and multi-map understanding in multimodal AI."}, "weaknesses": {"value": "A few minor aspects could be clarified or expanded to further strengthen the work.\n\nFirst, although the authors describe a careful validation process, the groundtruth generation pipeline (Section 3.2) partly relies on LLMs (GPT-4 and GPT-o3) to propose initial questions and reference answers before human verification. While this hybrid approach is efficient, it may introduce subtle mismatches between the automatically generated “gold answers” and human interpretations. Providing more detail on how such discrepancies were resolved or examples of discarded cases would improve confidence in the dataset’s reliability.\n\nSecond, the evaluation analysis (Section 5)—focused mainly on accuracy and error frequency—could be enriched with more nuanced insights. For example, reporting domain-wise breakdowns (e.g., geology vs. urban planning) or qualitative examples of borderline cases could illuminate specific reasoning challenges faced by LVLMs.\n\nFinally, the contextual setting (Appendix E), although well designed, yielded similar results to the direct setting, raising questions about whether the retrieval aspect was sufficiently demanding. Clarifying how the contextual map sets were selected and randomized, or adding a more difficult retrieval scenario, could make this part of the benchmark more informative."}, "questions": {"value": "1. Could the authors clarify in more detail how the ground-truth answers were finalized during dataset construction? Specifically, when GPT-generated “gold answers” disagreed with human annotators, how were such conflicts resolved, and were any systematic patterns of disagreement observed?\n\n2. For the contextual setting, could the authors elaborate on the retrieval challenge? It would be helpful to understand how many candidate maps were typically presented per question and how similar or visually confounding these maps were. This clarification would help assess whether the contextual setup truly captures document-level reasoning complexity.\n\n3. The paper mentions that the benchmark draws from multiple thematic domains (geology, environment, urban planning, etc.). Did the authors observe domain-specific variations in model performance? If not, would they consider including such an analysis to better understand where models struggle the most?\n\n4. Could the authors comment on how they plan to ensure community adoption of the benchmark—e.g., through public leaderboards, evaluation APIs,\n\nor standardized prompts—once anonymity is lifted"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ekNMrgjXuu", "forum": "QQCadccQqU", "replyto": "QQCadccQqU", "signatures": ["ICLR.cc/2026/Conference/Submission14226/Reviewer_Y3WZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14226/Reviewer_Y3WZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857091649, "cdate": 1761857091649, "tmdate": 1762924681859, "mdate": 1762924681859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FRIEDA, a new benchmark designed to test how well large vision-language models (LVLMs) perform multi-step reasoning over real maps. The benchmark spans 500 questions drawn from 17k maps across 210 documents, covering topological, metric, and directional spatial relations. Tasks require understanding legends, scales, and compass orientation, often across multiple maps. The study evaluates 11 LVLMs (Gemini-2.5-Pro, GPT-5-Think, Claude-Sonnet-4, and several open-source models). Even the best systems achieve under 40% accuracy compared to 85% human performance, revealing a major gap in cartographic reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper targets a truly underexplored domain. Most prior map-VQA datasets treat maps as static charts. FRIEDA is the first to combine multi-map reasoning with explicit tests of spatial relations, bridging cognitive geography and LVLM evaluation.\n\nQuality: The benchmark is carefully built with validated questions, human agreement checks, and diverse sources. The experimental setup is solid, and the error taxonomy is informative.\n\nClarity: The writing is clear, structured, and well-motivated. The examples and figures make the task definition easy to grasp.\n\nSignificance: The benchmark fills an important gap between general multimodal reasoning and geospatial intelligence. The authors’ detailed error analysis provides actionable insight into how LVLMs misinterpret legends, scales, and orientation."}, "weaknesses": {"value": "1. The dataset size (500 questions) may limit statistical robustness and fine-grained analysis.\n2. While the benchmark claims multi-step reasoning, most reasoning is still visual-symbolic.\n3. More discussion on relation to existing reasoning datasets like GeoChain or MapIQ would clarify complementarity and citation positioning."}, "questions": {"value": "1. How reproducible are results across different LLM-judge configurations?\n2. Did you evaluate whether models can explain their reasoning steps (not just final answers)?\n3. Could GeoChain-style supervised reasoning traces help improve performance on FRIEDA?\n4. Are some relation types (e.g., metric vs. topological) more prone to hallucination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QE8y2zA5jj", "forum": "QQCadccQqU", "replyto": "QQCadccQqU", "signatures": ["ICLR.cc/2026/Conference/Submission14226/Reviewer_RzLv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14226/Reviewer_RzLv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904479426, "cdate": 1761904479426, "tmdate": 1762924681279, "mdate": 1762924681279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-step cartographic reasoning benchmark dataset for cartographic reasoning in 3 dimensions: topological, metric, and directional spatial relationships. The maps are sourced from publicly available dataset, 500 questions are drafted by propriety models. Evaluation was performed across 11 LVLMs, providing 5 findings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The study does well to highlight the gap in research across all the other Map VQA studies. For example, it highlights that multi-map VQA is an area not evaluated, and if it is evaluated, it’s very limited (Kazemi et al, 2025).\n\nThe study does a good job linking the human cognition aspect of map querying and exploring how it would be interesting to see how LVLMs reason over maps. A strong real world foundation is provided in this study and it drives the need for the benchmark presented. \n\nEvaluation metrics are clear and fitting for each category of questions tested. LLM as a judge is used for textual questions while distance based answers are evaluated on MAPE. Directional answers are mapped to the relevant locations labeled with North, South, East, West metrics and all directions in between. \n\nAlthough the queries are drafted by proprietary models, they are manually reviewed by humans to maintain the quality. Provides logical findings through extensive evaluations and analyses, found a huge gap between humans and models. Also, the overall writing is good and easy to follow."}, "weaknesses": {"value": "The motivation of this paper is weak. I am not convinced that cartographic QA is different from the other MapQAs, even after reading the whole body of Section 2. Further comparison between related works should be highlighted to demonstrate the novelty of this research direction.\n\nThe scale of the benchmark set is moderately small (500 questions), compared to the other LVLM benchmarks. A very limited novelty is there. Doesn't compare with vast literature of prior work on MapQA[1], MapWise[2], MapIQ [3], MapBench[4], CartoMark[5], MapQA (GQA)[6] and many more. A proper comparison should be there with prior works to show how this is different.\n\nThe task definition introduces too many dimensions for each category. In Spatial Reasoning, many dimensions to the category are formalized, giving the impression that the benchmark would extensively test against it. Figure 2 and Table 1 don’t align with the in-depth explanation for the tasks formalized in section 2, so there is a disconnect with how each dimension is tested. \n\nIn the benchmark section, 500 questions are evaluated against 17,030 maps from 210 documents. Because it’s a multi-map task, the map-per-question distribution may be uneven; while the authors note that each question uses “two or more maps,” they don’t state a maximum. That matters: LVLM performance can differ wildly—two-map queries are manageable for state-of-the-art models, but queries involving hundreds of maps will strain context windows (and humans).\n\nLimiting the scope to Latin-script text is concerning, and demonstration in Figure 1 does not look like it is from Latin-script text. Also, does it mean that all components including legends, region descriptions are translated or retrieved from references?\n\nReferences [1] https://arxiv.org/abs/2211.08545 [2] https://arxiv.org/pdf/2409.00255v1 [3] https://arxiv.org/abs/2507.11625 [4] https://arxiv.org/pdf/2503.14607 [5] https://www.nature.com/articles/s41597-024-04057-7 [6] https://arxiv.org/pdf/2503.07871"}, "questions": {"value": "I am skeptical towards accepting adjacent labels. Also, isn’t it ambiguous to include partially-agreed questions in the benchmark dataset? Does the findings change if we do not consider those partial-agree items?\n\nHow did the authors analyze the errors and misinterpretations of Gemini-Pro? Are those manually checked following the reasoning trace?\n\nIn the “Performance by spatial relation” paragraph, are there any reasons why the models are ranked as in Fig 4? Does the amount or ratio of spatial understanding data in each train split matters?\n\nHow do RL-trained models perform on this benchmark? Are there any inductive bias observed from visual encoders? Are there any underlying biases or trends from the training set? These points are revealed that these two points matter [1,2].\n\nWill there be an explanation for the distribution of questions for multi-map querying i.e how many maps does one question have? \n\nReferences\n[1] Wang et al. VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models. arxiv:2406.16338\n[2] Li et al. Vidhalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding. CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "doUjt0h2b6", "forum": "QQCadccQqU", "replyto": "QQCadccQqU", "signatures": ["ICLR.cc/2026/Conference/Submission14226/Reviewer_VsEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14226/Reviewer_VsEU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940676591, "cdate": 1761940676591, "tmdate": 1762924680454, "mdate": 1762924680454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FRIEDA, a benchmark designed to evaluate multi-step cartographic reasoning in vision-language models (VLMs). FRIEDA focuses on reasoning over real-world maps and includes both single-map and multi-map questions.\nThe benchmark contains 500 questions and supports two evaluation modes: (1) the direct setting, where the relevant map(s) are provided, and (2) the contextual setting, where models must first identify the relevant map(s) from a document set before reasoning.\n\nThe authors evaluated 11 VLMs, finding that even the strongest models achieve less than 40% accuracy, far below the human baseline of 84.9%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper addresses the need for a benchmark that evaluates VLMs’ reasoning abilities over diverse maps. In addition to direct map-based question answering, it includes a retrieval-like setting, where the model must first identify useful maps before answering.\n\n2.All questions are verified by human annotators, ensuring high dataset quality. The appendix provides clear documentation of the annotation and curation processes, which helps readers understand how the dataset was built.\n\n3.The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. With only 500 questions, the dataset may be too small for robust model evaluation, especially when analyzing performance across subcategories (e.g., Table 2 and Figure 4). Can you conduct significance tests on the small categories?\n\n2. The paper asserts that all questions require visual reasoning, but no text-only baseline (question without the map) is reported. This makes it unclear whether VLMs rely on internal knowledge to answer some questions.\n\n3. The questions are synthesized by LLMs given maps, which may make them too artificial and not well aligned with how humans naturally seek information from maps. The textual context of the original documents could better reflect real human interest in these maps but is not utilized. Beyond verifying the validity of these questions, human annotators should also analyze whether the questions resemble those naturally encountered in real-world settings.\n\n4. It is unclear whether the questions contain explicit spatial references (e.g., “the red dots in the image”) or rely only on abstract factual references such as legend words. Prior work [1] suggests that VLMs struggle with understanding visual features, which is particularly important in map interpretation.\n\n5. The contextual setting experiment setup is not clearly explained.\n\nIn general, I believe this paper is well motivated and well executed, though it still has a few concerns that need to be addressed. I would be happy to adjust my scores after seeing the authors’ response.\n\n[1] Rahmanzadehgervi, Pooyan, et al. \"Vision language models are blind.\" Proceedings of the Asian Conference on Computer Vision. 2024."}, "questions": {"value": "Besides the points listed under Weaknesses, I have a few clarification questions.\n\n1. For the multi-map questions, how many maps are provided on average in the direct and contextual settings?\n\n2. In the contextual setting, do the VLMs see the entire document or only the map images when answering a question?\n\n3. How does the contextual setting compare in difficulty for humans? Are these questions also easy for human annotators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N4HYzgH9mm", "forum": "QQCadccQqU", "replyto": "QQCadccQqU", "signatures": ["ICLR.cc/2026/Conference/Submission14226/Reviewer_8fK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14226/Reviewer_8fK6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762489762788, "cdate": 1762489762788, "tmdate": 1762924679890, "mdate": 1762924679890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}