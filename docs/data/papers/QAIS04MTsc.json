{"id": "QAIS04MTsc", "number": 13571, "cdate": 1758219320072, "mdate": 1759897427784, "content": {"title": "MindCraft: How Concept Trees Take Shape In Deep Models", "abstract": "Large-scale foundation models demonstrate strong performance across language, vision, and reasoning tasks. However, how they internally structure and stabilize concepts remains elusive. Inspired by causal inference, we introduce the **MindCraft** framework built upon **Concept Trees**. By applying spectral decomposition at each layer and linking principal directions into branching Concept Paths, Concept Trees reconstruct the hierarchical emergence of concepts, revealing exactly when they diverge from shared representations into linearly separable subspaces. Empirical evaluations across diverse scenarios across disciplines, including medical diagnosis, physics reasoning, and political decision-making, show that Concept Trees recover semantic hierarchies, disentangle latent concepts, and can be widely applied across multiple domains. The Concept Tree establishes a widely applicable and powerful framework that enables in-depth analysis of conceptual representations in deep models, marking a significant step forward in the foundation of interpretable AI.", "tldr": "", "keywords": ["Concept trees; Representation; Counterfactual"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c7971adbbcc214fa17c94e97a765ad2febf29af.pdf", "supplementary_material": "/attachment/3f2133cc99dcead81b1ec7ecacd9046ce0629556.zip"}, "replies": [{"content": {"summary": {"value": "The authors investigate how various concepts \"appear\" within machine learning models through the lens of causal inference. In particular, they introduce a framework, called MindCraft, which investigates how a pair of counterfactual statements (e.g., \"You are a powerful leader\" and \"You are a powerless leader\") differ in last-token representation as we change the layer number. This essentially serves as a signature for a given concept. This can then be used to identify when concepts diverge by placing a threshold for when two concepts should differ. This leads to the creation of trees which visualize branching in this system. The paper demonstrates this for various applications, along with an exploratory experiment that explores whether initial embedding distance is predictive of the separation between high-level concepts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Visually Appealing Representation** - The authors present a tree-based diagram visualizing how different concepts emerge in deep learning models. The representation is visually appealing, and makes it clear the order in which these concepts occur. Such a representation could be valuable, as it allows us to better understand the dynamics underlying these deep models. \n2. **Extensive Examples** - The authors present the material in a fairly clear way through the extensive use of examples. For example, page 7 presents numerous examples of concepts emerging in this context, which makes it clear how such trees operate. Moreover, the paper is generally well-presented, with various examples of how concepts emerge, and the meaning of such emergence. \n3. **Interesting Exploratory Experiments** - In Section 5, the authors explore the disentanglement of input embeddings from concepts, essentially looking at whether token-level embeddings can predict which concepts emerge. Such an experiment is interesting because it investigates the relationship between model representation with the data itself, to identify whether such concept emergence is inevitable."}, "weaknesses": {"value": "1. **Unclear Generalization** - While the authors present numerous examples throughout the paper, my biggest worry is that the patterns seen here might not generalize. Specifically, my worry is two-fold a) concepts might not always be suddenly amplified, yielding some of their analysis moot, and b) it is unclear how to interpret or use the presented concept trees in practice. What is the insight useful for/how can we interpret this insight? \n2. **Lack of Justification for Why** - The authors provide little theoretical justification for why certain layers might propagate signals for concepts moreso than other layers. Many of the plots in the experiments section are for individual examples (which helps to get the point across), but the lack of aggregate analysis makes it hard to understand whether such trends hold across settings, and moreover, why such trends should even hold in the first place."}, "questions": {"value": "1. How should MindCraft be used in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sqBojH515I", "forum": "QAIS04MTsc", "replyto": "QAIS04MTsc", "signatures": ["ICLR.cc/2026/Conference/Submission13571/Reviewer_cnAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13571/Reviewer_cnAx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738611548, "cdate": 1761738611548, "tmdate": 1762924167865, "mdate": 1762924167865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address opacity of neural networks, the paper introduces MindCraft, a method to inspect how the internal representation of an abstract concept evolves through the layers of a neural network. To do so, it creates counterfactual pairs and tracks internal representation differences between at various depths of a foundation model, identifying where concepts diverge. MindCraft can also provide interpretable visualizations for model development and debugging. The reported experiments and visualizations suggest generalization across domains."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novelty: I like the idea of studying how the internal representation of an abstract concept evolves across different layers. A tool capable of performing such analysis could be valuable, as it enables a more comprehensive understanding of the model itself."}, "weaknesses": {"value": "Major:\n\n- **W1** - The authors claim that MindCraft explains how large foundation models internally structure abstract concepts (l. 470–472). However, according to the experimental section, the proposed methodology is tested on only one LLM (Qwen2.5-7B-Instruct). Therefore, the statement in the conclusion lacks sufficient empirical support. Without this claim, the overall impact of the paper is considerably reduced.\n    \n- **W2** - The results presented in the experimental section are mostly qualitative (Figures 3-4-5). Although the authors provide several qualitative examples, a formal validation (established metrics and/or statistical tests) supporting the consistency and reliability of the approach appears to be missing. This limitation likely stems from the absence of a theoretical ground behind both the motivation and the methodology.\n    \n\nMinor:\n\n- **W3** - Motivations seem weak. In l.194, the authors claim “Concept formation, therefore, follows a branch-and-stabilize process:” and, later in l.198 “concept-level organization is not static, but unfolds progressively through the network.” At this point of the paper, this is supported only using a single example (Fig.2). Also, I would expect the validity and extent of such a general claim, as well as the particular observed dynamics of the similarity score, to depend strongly on the specific examples, i.e., sentences and context.\n\n- **W4** - The necessity of using the principal directions extracted from the SVD with respect to the raw ( $W_V$ ) is not well justified. Despite showing several examples in the appendix, the rationale for this choice remains primarily qualitative.\n    \n- **W5** - The concept tree is constructed considering only self-attention. However, a generic LLM is far more complex than that, as LLMs typically employ multi-head attention. Consequently, the internal representation of a specific sequence at a given depth is only partially analyzed by the proposed methodology. Shouldn't it also account for the other attention heads at the same depth?"}, "questions": {"value": "- **Q1** (related to W3) - Have you observed any scenario where the dynamics of the counterfactual pair is not “branch-and-stabilize“?\n    \n- **Q2** (related to W5): How do you think your methodology extends to or interacts with multi-head attention or more complex architectures?\n\nWhile I remain open to a constructive discussion, I believe the paper requires substantial improvement, especially towards establishing a clearer/stronger theoretical foundation for both the proposed method and its evaluation. At this stage, the gap between the current submission and a version that would meet the bar for acceptance still feels wide to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2RedIWBLYF", "forum": "QAIS04MTsc", "replyto": "QAIS04MTsc", "signatures": ["ICLR.cc/2026/Conference/Submission13571/Reviewer_ZdDy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13571/Reviewer_ZdDy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830988822, "cdate": 1761830988822, "tmdate": 1762924167579, "mdate": 1762924167579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MindCraft, a framework for analyzing how llm internally structure abstract concepts. The proposed method uses counterfactual input pairs and tracks the difference in the last token's representation across layers. The core of the method which is Concept Path, is computed by projecting the last token's attention value vector onto the principal components of that layer's value transformation matrix. By finding the layer where the Concept Paths of a counterfactual pair first diverge, a hierarchical visualization of when and where different concept split into separable subspaces."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The branch-and-stabilize hypothesis provides a strong, intuitive foundation for the work.\n3. The paper demonstrates the flexibility of the proposed framework by applying to multiple model across three domains."}, "weaknesses": {"value": "1. The same citation appears in two different formats.\n2. The main text of the figure needs to provide guidance on how to interpret the results in the figure (e.g., what the takeaway is).\n3. The paper need to provide a claim that the Concept Tree faithfully represents the model's internal reasoning process.\n4. The paper need to clarify the choice of parameters k and tau, or at least analyze the change.\n5. The experiment should be compared with baselines (e.g., RepE, LRH). The only baseline comparison is \"raw Value\" vectors which isn't benchmarked on any metric."}, "questions": {"value": "Look at the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z3xQ2fCWSl", "forum": "QAIS04MTsc", "replyto": "QAIS04MTsc", "signatures": ["ICLR.cc/2026/Conference/Submission13571/Reviewer_g9oL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13571/Reviewer_g9oL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941218901, "cdate": 1761941218901, "tmdate": 1762924167266, "mdate": 1762924167266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MindCraft, a framework for explaining AI models by constructing concept trees that illustrate the hierarchy of which neural network layers concepts diverge in the internal representations. The tree is constructed by comparing the spectral decompositions of the representations of a counterfactual pair of inputs and splits the tree at the first layer in which the representations begin to differ. Experiments demonstrate clear examples of trees generated for LLM tasks and also highlight some interesting properties of the approach."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Mindcraft presents a novel framework for interpretable AI that pinpoints precisely where concepts form within the layers of the model.\n\n2. Experiments are comprehensive, easy to understand, and show a variety of properties of the pipeline.\n\n3. The writing is very clear."}, "weaknesses": {"value": "1. There is little theoretical justification of any of the proposed methodology. The final tree is dependent on many parameters left up to the user, and the result is open to vague interpretation. The paper claims that MindCraft “systematically traces how abstract concepts emerge”, but it is unclear why the resulting concept tree answers the “how” question.\n\n2. Counterfactual quantities, from the perspective of causal inference, are not easy to infer, especially in cases where one is attempting to simultaneously infer something about the same input in two different interventions. It is not clear what kinds of assumptions are made to allow for this."}, "questions": {"value": "1. Is there a reason that concept trees are defined in tree format? It seems like all concepts are leaf nodes that branch off of a single main line of nodes that represents the undisambiguated concepts. Could concept trees be instead simply represented as a list of concepts sorted by the order of the layers in which they were disambiguated?\n\n2. Do the patterns that arise in the presented experimental results look similar when applied to non-language tasks?\n\n3. Are the concepts in MindCraft simply defined as sections of the input, or could they represent more abstract concepts produced within the internal workings of the neural network?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "akJv8kRqrZ", "forum": "QAIS04MTsc", "replyto": "QAIS04MTsc", "signatures": ["ICLR.cc/2026/Conference/Submission13571/Reviewer_yQ4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13571/Reviewer_yQ4m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762335842388, "cdate": 1762335842388, "tmdate": 1762924166913, "mdate": 1762924166913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}