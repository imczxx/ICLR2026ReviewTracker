{"id": "azJnEkfqzp", "number": 15357, "cdate": 1758250562527, "mdate": 1759897311971, "content": {"title": "Continuous-Time Discrete Markov Bridge", "abstract": "Discrete diffusion has recently emerged as a promising paradigm in discrete data modeling. However, existing methods typically rely on a fixed-rate transition matrix during training, which not only limits the expressiveness of latent representations—a fundamental strength of variational methods—but also constrains the overall design space. To address these limitations, we propose **Discrete Markov Bridge**, a novel framework specifically designed for discrete representation learning. Our approach is built upon two key components: *Matrix*-learning and *Score*-learning. We conduct a rigorous theoretical analysis, establishing formal performance guarantees for *Matrix*-learning and proving the convergence of the overall framework. Furthermore, we analyze the space complexity of our method, addressing practical constraints identified in prior studies. Extensive empirical evaluations validate the effectiveness of the proposed **Discrete Markov Bridge**, which achieves an Evidence Lower Bound (ELBO) of \\textbf{1.38} on the Text8 dataset, outperforming established baselines. Moreover, the proposed model demonstrates competitive performance on the CIFAR-10 dataset, achieving results comparable to those obtained by image-specific generation approaches.", "tldr": "We propose a new kind of discrete diffusion model, named Discrete Markov Bridge, which perform bidirectional optimization with a new rate transition matrix.", "keywords": ["Discrete Diffusion", "Diffusion Model", "Markov Chain", "unsupervised learning", "machine learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a59a0e6146ac7ed53220f761a6078795af9adc1.pdf", "supplementary_material": "/attachment/c8f174ef47ea17ef5110497d60a89b8c1c4f0aea.zip"}, "replies": [{"content": {"summary": {"value": "The paper under consideration studies a class of generative model to sample from a discrete data distribution $p_0$, called Discrete Markov Bridge (DMB). In contrast with most concurring models DMB also optimizes over the choice of prior. This phase, called *Matrix Learning* complements the more classical *score learning* phase, in which score approximation is performed by minimizing the expected KL divergence between the distribution at time $T$ for a given choice of prior (noising process) and conditional distribution at time $T$ for the same choice of prior given the initial state."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper is to propose a generative framework that allows for flexible prior distributions that can in principle adapt to the underlying data distribution and evolve along iterations. The effectiveness of this idea is corroborated by numerical experiments on CIFAR-10 and Text 8. I like this idea and I believe it deserves to be further explored. The numerical experiments are promising."}, "weaknesses": {"value": "- The main problem is that theoretical guarantees are too weak and the numerical experiments alone, though promising, are not in my opinion strong enough to compensate for the lack of theoretical results. To be more precise\n\n    - Proposition 4.1 appears to me as a generic statement about Markov chains that use nothing of the specifics of DMB\n    - Proposition 4.2 is again a very general results saying that in principle the method can be used to sample from a given data    distribution $\\mu$. Moreover, its statement is also incomplete as the most interesting part is the upper-triangular shape of the transition matrix Q, which is not mentioned.\n   - Theorem 4.7 states under vague and imprecise assumptions that $D_{KL}(\\mu|p_0^{(k)})$ is converging.  But this is not a guarantee of convergence. I would rather expect a statement like $\\lim_kD_{KL}(\\mu|p_0^{(k)})=0$, but I could not find this result in the paper, and after reading the proof of Theorem 4.7, I could only infer that the $D_{KL}(\\mu|p_0^{(k)})$ is decreasing along iterations. \n\nIn conclusion, the theoretical results do not provide with any convergence rates, do not take into account the various sources of error (time-discretization, score approximation…) and even under idealized assumptions do not seem to guarantee the convergence of the algorithm to the target distribution. I may have misunderstood something, in which case I am happy to review my assessment.\n\n- Most of the key statements lack rigor or precision, there are many typos and missing details. To give an example, in the pseudo-code for Algorithm 2 on top of page 5, which basically summarizes the contribution of this work I encountered the following issues\n\n   - The authors propose to update the prior transition matrix $Q_\\alpha$ according to (5) and predict $p_T$ according to (4). But (5) is just a loss function. So how how is the update actually done? I don’t feel like equation (14) clarifies this well enough. \n    - In the same spirit, how is the update of the score estimator $s_{\\theta}$ performed according to equation (8)? At first glance, it seems quite costly as for each iteration, the prior evolves. Therefore, one needs to approximate all transition rates $p_{t|0}$ at each iteration and generate forward many trajectories at each iteration. How is this actually done? \n    - The sampling algorithm used is described quite vaguely as some form of Euler scheme for ODEs. It needs more explanations and detail. Also, I expect probability ratios like $ p_t(y)/p_t(x) $ to take both very large and very small values. How are these problems handled?\n    -  $ \\mathcal{J}_{Q} $ is not defined before. I guess it should be $ J_Q $. The quantity \\mathcal{L}_Q does not seem to be defined before, though it probably is again  J_Q  . Similarly,  \\mathcal{J}_{score} is not defined before: I assume it coincides with J_score"}, "questions": {"value": "- Has the idea of using a flexible prior been exploited before in the context of continuous diffusion models based on the Ornstein Uhlenbeck process? If so, with what results and outcomes?\n\n- I understand that the definition of J_score is taken from previous works. What is its interpretation? Does it carry a probabilistic meaning as some averaged relative entropy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "USszk8v8lk", "forum": "azJnEkfqzp", "replyto": "azJnEkfqzp", "signatures": ["ICLR.cc/2026/Conference/Submission15357/Reviewer_hYP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15357/Reviewer_hYP5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761139021267, "cdate": 1761139021267, "tmdate": 1762925643456, "mdate": 1762925643456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the foundation of the diffusion model and integrates the variational inference into the framework. The authors introduce a learnable transition rate matrix for both the forward and reverse processes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers rigorous derivations of the forward–reverse coupling and provides formal proofs of convergence.\n\n2. The proposed method achieves competitive performance, which is on par with or better than previous discrete diffusion baselines.\n\n3. The paper is generally well-organized, includes clear notation, and presentation."}, "weaknesses": {"value": "1. Experiments are constrained to Text8 and CIFAR-10. No large-scale or multimodal datasets are tested.\n\n2. The advance is related to the SEDD model (Lou et al., 2024) and Variational Diffusion Models (Kingma et al., 2023). The method remains structurally similar, with the main novelty being a learnable rate matrix. It would be better for the author to establish the connection and clarify the novel point more clearly.\n\n3. The paper does not provide ablation studies for the effects of the learnable rate matrix, continuous-time parameterization, or transition efficiency. It is unclear how much each contributes to the improvements."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cjDDHnXdT9", "forum": "azJnEkfqzp", "replyto": "azJnEkfqzp", "signatures": ["ICLR.cc/2026/Conference/Submission15357/Reviewer_NDqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15357/Reviewer_NDqu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761406618630, "cdate": 1761406618630, "tmdate": 1762925643032, "mdate": 1762925643032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework for discrete generative modeling that combines diffusion and variational approaches. It claims to address the rigidity of existing discrete diffusion models, which rely on fixed transition matrices, by introducing a **learnable continuous-time transition matrix**. The model consists of two components: a **Matrix-learning stage** that estimates the forward transition dynamics, and a **Score-learning stage** that reconstructs the data distribution via an ELBO-based objective. Formal guarantees of validity, accessibility, and convergence for their algorithm are discussed and it is given an efficient matrix structure that allows tractable exponentiation. Empirically, DMB is reported to outperform previous discrete diffusion models on `Text8` and achieve competitive image generation results on `CIFAR-10`. The contribution is mainly conceptual, positioning DMB as a general bridge between variational inference and discrete diffusion modeling, though the work remains largely theoretical and limited in empirical depth."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel conceptual framework:**\nThe paper proposes a new paradigm that combines variational inference and discrete diffusion through the formulation of a continuous-time discrete Markov process, which is an original direction in discrete generative modeling.\n2. **Learnable transition dynamics:**\nUnlike most prior discrete diffusion models using fixed transition matrices (Absorb, Uniform), DMB introduces a learned transition-rate matrix, increasing model flexibility and expressiveness.\n3. **Clear algorithmic decomposition:**\nThe two-stage structure (Matrix-learning and Score-learning) provides a clean, interpretable separation between the forward and reverse processes, similar to continuous diffusion methods but adapted to discrete spaces.\n4. **Computational insights:**\nThe proposed diagonalizable matrix form allows efficient matrix exponentiation and reduced space complexity, addressing a key bottleneck in discrete diffusion computation.\n6. **Empirical competitiveness:**\nDespite its generality, the model achieves state-of-the-art performance on `Text8` and competitive image generation results on `CIFAR-10`, showing that the method can scale across modalities.\n7. **Potential generality:**\nThe framework can in principle encompass various discrete domains (e.g., text, symbolic, or categorical data) and may serve as a unified foundation for discrete representation learning."}, "weaknesses": {"value": "While the paper introduces an interesting conceptual framework, it currently falls short of the expectations for an ICLR-level contribution in terms of novelty, theoretical depth, and empirical validation. The work reads more as a promising exploratory idea than as a mature and rigorously substantiated contribution supported by solid theoretical or experimental evidence.\n\n---\n\nMain Concerns\n1. **Limited theoretical novelty**\nThe proposed _Discrete Markov Bridge (DMB)_ framework closely resembles existing discrete diffusion approaches (e.g., D3PM, SEDD), extending them into a slightly more general variational formulation. The main theoretical results (_validity, accessibility, convergence_) appear to be formal restatements of well-established properties of continuous-time Markov processes, rather than providing genuinely new insights into discrete generative learning.\n2. **Weak empirical evaluation**\nThe experimental validation remains limited in scope. Only two datasets are considered (`Text8` and `CIFAR-10`), with relatively few baselines and no ablation or sensitivity analysis. The reported improvements (for instance, a gain of 0.1 BPC on `Text8`) are modest and likely fall within the variance of previously reported results. This makes it difficult to assess the claimed advantages of the proposed approach convincingly.\n3. **Insufficient connection to recent literature**\nThe discussion of related work is incomplete and does not engage deeply with recent progress in discrete and score-based generative modeling (e.g., Lou et al., 2024; Meng et al., 2023; or more recent flow-matching approaches).\nIn particular, **estimating ratios in discrete settings is notoriously difficult**, and recent advances have proposed alternative formulations that bypass this issue by defining the score as an $L^2$ approximation rather than a direct ratio (see [1]).\nMoreover, there is relevant ongoing work on **discrete simulation in hypercubes**, which provides mathematically sound convergence guarantees under minimal assumptions, as well as recent insights on potential quantum extensions of discrete score-based models ([2]).\nA more substantial comparison with these directions would be essential for positioning the contribution within the current theoretical landscape.\n4. **Overstated theoretical claims**\nThe theoretical results rest on strong and somewhat idealized assumptions—such as perfect optimization, linearity of the dynamics, and exact reversibility. As presented, it is **not straightforward to see why the convergence results in Theorems 4.7 and D.2 imply convergence to zero** in practice, as the current derivations do not seem to support this claim rigorously. The guarantees therefore appear more formal than actionable.\n5. **Methodological originality and clarity**\nThe methodological distinction of DMB compared to prior frameworks is limited. The idea of coupling a forward and backward process via learned matrices has been explored in several diffusion and flow-based settings. Here, the main innovation—replacing fixed matrices with learnable ones—does not seem to bring a clearly demonstrated advantage, and the motivation for the additional complexity remains somewhat unclear.\n\n---\n\n**References**\n\n[1] Pham, L.T.N., et al. _“Discrete Markov Probabilistic Models: An Improved Discrete Score-Based Framework with Sharp Convergence Bounds under Minimal Assumptions.”_ Forty-second International Conference on Machine Learning (ICML, 2025).\n\n[2] Bach, Francis, and Saeed Saremi. _“Sampling Binary Data by Denoising through Score Functions.”_ arXiv preprint arXiv:2502.00557 (2025)."}, "questions": {"value": "See **Weaknesses** section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AmYNK1dw88", "forum": "azJnEkfqzp", "replyto": "azJnEkfqzp", "signatures": ["ICLR.cc/2026/Conference/Submission15357/Reviewer_QfCp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15357/Reviewer_QfCp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899525748, "cdate": 1761899525748, "tmdate": 1762925642403, "mdate": 1762925642403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Continuous-Time Discrete Diffusion Model (CTDDM) that unifies continuous and discrete generative processes under a single stochastic differential framework.\nWhile standard diffusion models assume continuous-valued states and Gaussian noise, the authors design a hybrid diffusion process operating on discrete state spaces (e.g., categorical or binary variables) parameterized by continuous-time transition kernels.\nA key contribution is deriving the Kolmogorov forward equation for discrete diffusion with a continuous-time clock, enabling tractable training and sampling via reparameterized discrete noise schedules.\nThe paper establishes theoretical guarantees for consistency with continuous diffusion limits, introduces a novel variational training objective, and empirically validates improvements on text and graph generation benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Unifying framework: Derives a continuous-time theory for discrete diffusion, bridging an important conceptual gap.\n- Strong theory: Clear proofs of convergence from discrete to continuous dynamics and vice versa.\n- General applicability: Applicable to discrete domains such as text, graphs, and symbolic reasoning.\n- Empirical evidence: Shows improved stability and sample diversity over prior discrete diffusion baselines (D3PM, MaskGIT).\n- Analytical insight: The generator-based view clarifies how score matching extends to discrete probability fluxes."}, "weaknesses": {"value": "- Notation overload: Sections 3–4 are mathematically dense; many operators $(Q_t, G_t, L_t)$ appear with minimal intuition.\n- Empirical scope: Experiments focus on small or synthetic datasets; large-scale benchmarks (e.g., LM1B or large graph datasets) are missing.\n- Comparative analysis: Comparison with recent hybrid discrete-continuous works (e.g., SEDD, SMC-Diffusion) could be more explicit.\n- Practical guidance: It remains unclear when CTDDM should be preferred over purely discrete models or continuous relaxations.\n- Computational cost: Continuous-time simulation of discrete jumps may introduce inefficiency; wall-clock comparisons are not discussed."}, "questions": {"value": "- Generator specification: Is the continuous generator $G_t$ assumed time-homogeneous or time-dependent? If time-varying, how is it parameterized in practice?\n- Diffusion limit: In Theorem 3.2, what conditions guarantee convergence to a continuous diffusion as the discrete grid refines? Are there counterexamples when ergodicity fails?\n- Training objective: How is the ELBO-like objective derived from the pathwise KL divergence? Could you provide a short derivation for clarity?\n- Score estimation: How is the score (gradient of log-prob) represented in the discrete case—via logit differences, or by interpolation between categorical probabilities?\n- Sampling complexity: Does simulating continuous-time discrete jumps require adaptive step sizes or event-based simulation (e.g., Gillespie-style)?\n- Empirical fairness: Are baselines tuned with equivalent training budgets? Some prior discrete models depend strongly on temperature annealing.\n- Variance reduction: Does the continuous-time formulation mitigate gradient variance compared to purely discrete noise schedules?\n- Hybrid variables: Can CTDDM handle mixed discrete-continuous data (e.g., tabular or multimodal)?\n- Graph generation: For graph tasks, are transitions applied to node/edge labels independently or via structured coupling?\n- Broader implications: Could this framework unify masked-token diffusion and continuous score-based text generation (e.g., Diffusion-LM, MaskGIT)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PE8sSH5Xvo", "forum": "azJnEkfqzp", "replyto": "azJnEkfqzp", "signatures": ["ICLR.cc/2026/Conference/Submission15357/Reviewer_vKru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15357/Reviewer_vKru"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924888485, "cdate": 1761924888485, "tmdate": 1762925642002, "mdate": 1762925642002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}