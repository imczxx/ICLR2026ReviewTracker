{"id": "wdodgzllZw", "number": 13819, "cdate": 1758223153930, "mdate": 1759897410510, "content": {"title": "DistillKac: Few-Step Image Generation via Damped Wave Equations", "abstract": "We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Kac dynamics enforce finite speed transport and yield globally bounded kinetic energy. Building on this structure, we introduce classifier free guidance in velocity space that preserves square integrability under mild conditions. We then propose endpoint only distillation that trains a student to match a frozen teacher over long intervals. We prove a stability result that promotes supervision at the endpoints to closeness along the entire path. Experiments demonstrate DistillKac delivers high quality samples with very few function evaluations while retaining the numerical stability benefits of finite speed probability flows.", "tldr": "", "keywords": ["generative models", "Kac flow", "damped wave equation", "telegrapher equation", "finite-speed probability flow", "classifier-free guidance", "endpoint distillation", "few-step sampling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f32e6f7c22f3ad918552043d303c7fe2a4a5500.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose DistillKac a strategy for distillation for generative models whose densities evolve with the damped wave equation. This parameterization should remove the singularities at the end points which is common in diffusion/flow models. This achieved by fixing the propagation speed to some bounded value. The authors then propose a distillation strategy for this existing family of models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I liked the detailed maths and found the Kac representation to be interesting.\n\n* Theorem 8 and Corollary 9 seem interesting tools for analysis in the literature on (consistency) distillation.\n* Theorem 4 seems novel within the context of CFG\n* Addressing the problems of singularities in diffusion/flow models is important"}, "weaknesses": {"value": "## Primary concerns\n### Theory\n* How does this work clearly differentiate from [1]? It is unclear where [1] ends and where the contributions of this work begin. My rough understanding is that this work is [1] plus Theorems 4, 8 and 9 with distillation?\n* I'm surprised that learning 1-D independent coordinate processes yields anything useful. It seems strange that this works as there is often interactions between the different coordinates. This seems to be an **incredibly strong** assumption to make.\n* How do you initially train the teacher? Flow matching and diffusion models are nice because of simulation-free training, it is not clear to me that the Kac representation yields this for the teacher.\n* Also since in Section 2.2 the process $X(t)$ is defined as a jump process it doesn't seem right to use standard numerical schemes for ODEs (like Euler or Heun) to integrate teacher model as suggested in line 203.\n* Also since the Kac process is stochastic why are the flow maps governed by an ODE, shouldn't the flow maps be random variables *a la* [3]?\n\n### Empirical\n* Both CIFAR-10 and CelebA-64 are quite small and generally considered toy datasets, in particular since one of the contributions is improved CFG I feel like more compelling experiments for guidance should be included than just CIFAR-10 (32 x 32).\n* The empirical results don't look great for Kac Flows. The traditional diffusion models seem to still outperform them despite the singularity issues, so what is the advantage? This seems especially true when compared to consistency models.\n* Overall, the empirical results are wholly **uncompelling** in convincing the reader of the advantages of Kac flows over standard flow/diffusion models.\n\n## Minor comments / suggestions\n* Table 1 should get reworked it is unclear what each row corresponds to. Some labels would be nice, e.g., PDE, SDE, papers.\n* Since there is a mixture of prior results in the theoretical sections, maybe highlight your contributions with a colored box?\n* FID is a notoriously fickle and unwieldy metric to work with. Recent papers looking at text-guided generation [2] have included CLIP-score  [4] and Image Reward [5].\n\n## References\n[1] Richard Duong, Jannis Chemseddine, Peter K Friz, and Gabriele Steidl. Telegrapher’s generative\nmodel via kac flows. arXiv preprint arXiv:2506.20641v3, 2025.\n\n[2] Skreta et al., 2025, Feynman-Kac Correctors in Diffusion:\nAnnealing, Guidance, and Product of Experts, ICML, https://arxiv.org/pdf/2503.02819\n\n[3] Kunita, Hiroshi. Stochastic flows and jump-diffusions. Springer Singapore, 2019.\n\n[4] Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PmLR, 2021.\n\n[5] Xu, Jiazheng, et al. \"Imagereward: Learning and evaluating human preferences for text-to-image generation.\" Advances in Neural Information Processing Systems 36 (2023): 15903-15935."}, "questions": {"value": "1. Why do you refer to the random paths and evolution of the densities as the *Kac process* and *Kac flow* (resp.)? On line 117 you say when $d \\geq 2$ such processes are called *random flights*. Aren't you working with $d \\geq 2$ objects? The terminology is a bit confusing\n2. How does this work relate to [1]?\n3. Do Theorem 8 and Corollary 9 apply to any flow maps or only those admitted by the *Kac process*?\n4. Likewise, how does Theorem 8 compare to [2, Theorem 4.1] outside of W1 vs W2?\n5. How do you justify the coordinate-wise independence?\n6. Please address any questions raised in the weaknesses sections.\n\n\n[1] Skreta et al., 2025, Feynman-Kac Correctors in Diffusion:\nAnnealing, Guidance, and Product of Experts, ICML, https://arxiv.org/pdf/2503.02819\n\n[2] Dou et al., 2024, Provable Statistical Rates for Consistency Diffusion Models, https://arxiv.org/pdf/2406.16213"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CPAUGKw2dk", "forum": "wdodgzllZw", "replyto": "wdodgzllZw", "signatures": ["ICLR.cc/2026/Conference/Submission13819/Reviewer_eWqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13819/Reviewer_eWqj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690805025, "cdate": 1761690805025, "tmdate": 1762924346542, "mdate": 1762924346542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DistillKac presents an image generative model grounded in the damped wave equation, a hyperbolic PDE represented stochastically through the Kac process. Unlike diffusion-based ODEs or SDEs, whose velocity norms can diverge near the terminal time, Kac dynamics maintain globally bounded kinetic energy and Lipschitz continuity in Wasserstein space. The main contributions include adapting classifier-free guidance directly in velocity space while preserving bounded-energy guarantees, and introducing an endpoint-only distillation scheme with a theoretical stability bound linking endpoint consistency to full-trajectory alignment. Empirically, these features enable stable few-step sampling with competitive image quality, and theoretically, the work clarifies why endpoint matching is sufficient under finite-speed Kac dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and presents solid theoretical foundations with rigorous stability proofs. \n- Adopting a hyperbolic PDE instead of the conventional diffusion formulation offers a novel structural perspective in generative modeling, characterized by finite propagation speed and bounded velocity. \n- The introduction of Kac flow–based generative modeling provides an innovative alternative to diffusion approaches, and the inclusion of stability guarantees and asymptotic links to diffusion broadens the methodological toolkit for finite-speed generative flows."}, "weaknesses": {"value": "- While this paper provides solid theoretical formulations and formal stability proofs, it lacks empirical analysis demonstrating how Kac flow concretely improves stability in practice. Theoretical claims of bounded energy and finite propagation speed are not supported by detailed quantitative studies.\n- Although Table 1 attempts to ensure fairness by matching the number of function evaluations (NFEs) across methods, the comparison would be more convincing if it also included training time, FLOPs, or wall-clock cost, since these reflect actual computational efficiency.\n- The paper does not show whether the claimed stability advantages translate into observable robustness improvements, such as reduced divergence, fewer NaNs, or more consistent quality under large classifier-free guidance scales or few-step sampling."}, "questions": {"value": "Overall, the work would benefit from comprehensive runtime and stability analyses to substantiate that the proposed Kac flow offers practical, measurable improvements beyond theoretical guarantees."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pLGHD9mOCL", "forum": "wdodgzllZw", "replyto": "wdodgzllZw", "signatures": ["ICLR.cc/2026/Conference/Submission13819/Reviewer_FCnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13819/Reviewer_FCnC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746320776, "cdate": 1761746320776, "tmdate": 1762924346007, "mdate": 1762924346007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a fast image generator, DistillKac,  that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. Based on that, it introduces classifier-free guidance in velocity space that preserves square integrability and proposes endpoint-only distillation that trains a student to match a frozen teacher over long intervals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Explore a new equation form (damped wave equation) for the diffusion process.\n2. Provide a solid and detailed proof of the formulation, including the error bounds."}, "weaknesses": {"value": "1. If I’m getting this right, the comparison in Figure 2 is made against normal sampling, specifically the non-distilled version of the model at the same number of steps. But doesn’t that seem a bit unfair? Plus, similar distillation methods can also achieve just a slight increase in FID. How does that show that your distillation method is better? It might be helpful to include comparisons with other distillation methods.\n\n2. In the related work section, you only mention a few papers that focus on CIFAR-10 and CelebA-64, but there’s actually a lot of literature on distillation in latent space that has algorithms similar to yours. For instance, the skipping steps in LCM[1] have the same meaning as N in your Algorithm 1. It would be great to see some mention and analysis of those similarities.\n\n3. While the proof goes into detail about the error and stability bounds, the experiments don’t seem to show better error and stability compared to other distillation methods. This leaves us unsure about the advantages of your proposed distillation method in practice.\n\n[1]Luo, Simian, et al. \"Latent consistency models: Synthesizing high-resolution images with few-step inference.\" arXiv preprint arXiv:2310.04378 (2023)."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ldowsb76yV", "forum": "wdodgzllZw", "replyto": "wdodgzllZw", "signatures": ["ICLR.cc/2026/Conference/Submission13819/Reviewer_H4Mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13819/Reviewer_H4Mg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240789273, "cdate": 1762240789273, "tmdate": 1762924345541, "mdate": 1762924345541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DistillKac, a new framework for fast image generation based on damped wave equations and their stochastic Kac representation. Unlike diffusion models that rely on parabolic Fokker–Planck dynamics, DistillKac models generative flows as finite-speed hyperbolic PDEs, ensuring bounded kinetic energy and improved numerical stability. The authors extend this formulation with (1) velocity-space classifier-free guidance, which allows controlled generation without destabilizing the dynamics, and (2) endpoint distillation, a novel few-step training strategy where a student model learns from the teacher’s terminal distribution rather than stepwise trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical formulation is elegant and original, bridging kinetic transport theory (via the Kac process) with modern generative modeling. Modeling probability flow through damped wave equations offers a compelling alternative to diffusion-based parabolic PDEs.\n2. The proposed endpoint distillation is conceptually simple yet powerful, backed by a provable endpoint-to-trajectory stability guarantee. This provides a rare theoretical justification for few-step model distillation.\n3. The velocity-space classifier-free guidance is well-motivated and resolves instability issues common in diffusion guidance, maintaining bounded energy across time."}, "weaknesses": {"value": "1. While mathematically grounded, the physical intuition behind using wave dynamics for generative flow could be developed further — especially how finite-speed propagation impacts sample diversity or convergence in practice.\n2. The evaluation scope is modest, focusing mainly on CIFAR-10 and CelebA-64. Demonstrations on higher-resolution or more complex datasets (e.g., LSUN, ImageNet) would strengthen the empirical case.\n3. The architecture choice (UNet) and the fixed set of integration schemes limit understanding of how well DistillKac generalizes to larger backbones (e.g., DiT) or different ODE solvers."}, "questions": {"value": "1. Can the authors provide more empirical evidence for the endpoint-to-trajectory stability theorem, e.g., quantitative correlation between endpoint and full-trajectory discrepancies?\n2. How does finite propagation speed affect perceptual smoothness or texture diversity compared to diffusion’s infinite-speed propagation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "onX17pOfng", "forum": "wdodgzllZw", "replyto": "wdodgzllZw", "signatures": ["ICLR.cc/2026/Conference/Submission13819/Reviewer_wRPC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13819/Reviewer_wRPC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762529024324, "cdate": 1762529024324, "tmdate": 1762924344880, "mdate": 1762924344880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}