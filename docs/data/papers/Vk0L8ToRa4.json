{"id": "Vk0L8ToRa4", "number": 10669, "cdate": 1758179168950, "mdate": 1759897636512, "content": {"title": "GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes", "abstract": "Quantitative structure–activity relationship assumes a smooth relationship between molecular structure and biological activity. However, activity cliffs defined as pairs of structurally similar compounds with large potency differences break this continuity. Recent benchmarks targeting activity cliffs have revealed that classical machine learning models with extended connectivity fingerprints outperform graph neural networks. Our analysis shows that graph embeddings fail to adequately separate structurally similar molecules in the embedding space, making it difficult to distinguish between structurally similar but functionally different molecules. Despite this limitation, molecular graph structures are inherently expressive and attractive, as they preserve molecular topology. To preserve the structural representation of molecules as graphs, we propose a new model, GraphCliff, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate that GraphCliff consistently improves performance on both non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses reveal reduced over-smoothing and enhanced discriminative power relative to strong baseline graph models.", "tldr": "GraphCliff is a graph neural architecture that integrates short- and long-range information through gating, yielding more discriminative molecular representations and improving performance on both non-cliff and activity-cliff compounds.", "keywords": ["Graph-based model", "Activity Cliff"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad04e67f2bfbb834176d1fc62bfb69e1e46eddc5.pdf", "supplementary_material": "/attachment/857cc659ea77394bad0e49b330ff277c0b870294.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes GraphCliff, a graph neural model for QSAR tasks with activity cliffs. The core idea is to explicitly combine short-range message passing (i.e. GINE) with long-range propagation (i.e. Chebnet) via a learnable sigmoid gate, followed by attention-based pooling for graph-level prediction. The authors motivate the work by showing that standard GNN embeddings under-emphasize local substructure differences compared to ECFP dissimilarities; GraphCliff is intended to preserve local sensitivity while providing global context. Experiments show lower RMSE overall and ablations indicate both filters and the gate contribute to performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The paper targets activity cliffs, a well-known failure mode for deep models and matter for medicinal chemistry that generic molecular GNNs miss.\n\nS2: The empirical study is extensive, with results on 30 MoleculeACE targets, along with targeted ablations such as removing short/long paths and gating, hop-wise sensitivity and Dirichlet energy estimation, which examine not just accuracy but also how information propagates through the network.\n\nS3: The paper is well organized and readable. The modular architecture (short-range path, long-range path, and gating) is explained clearly, and the figures/equations make the design easy to follow."}, "weaknesses": {"value": "W1: Section 5.2’s hop-wise sensitivity (perturbation of u affecting v at k hops) and Jacobian spectrum are closer to an over-squashing analysis than smoothing. Although Dirichlet energy is included for smoothing, the text and framing mix these notions, and it’s unclear whether the proposed gains primarily combat squashing (limited receptive field / information bottlenecks) or smoothing (Laplacian averaging). \n\nW2: Necessity of the mechanism vs. simpler baselines. If over-smoothing were the key issue motivating deep propagation, residual/skip connections, PairNorm/BatchNorm, JK-Net and APPNP/PPNP are standard, lightweight fixes. The paper does not compare GraphCliff against these strong anti over-smoothing baselines or against simple residual GNNs. This weakens the claim that the proposed mechanism is required.\n\nW3: Chebyshev long-range propagation, gated/highway mechanisms, and attention pooling have all been in literature for other application; GINE is standard for molecular graphs. The specific combination is practical but feels like engineering glue more than a new principle. \n\nW4: Formal problem setup & notations are missing. There is no concise notation paragraph formally defining the input graph G=(V,E), node/edge features, and the prediction problem; the method section jumps directly into components/equations, which hurts clarity."}, "questions": {"value": "Q1: Could you clarify why do deep GNNss are needed for this application ?\n\nQ2: What are the training/inference runtime and memory costs of GraphCliff relative to strong baselines? How do these scale with graph size and Chebyshev order?\n\nQ3: Did you try residual connections to the input or JK aggregation in lieu of gated fusion? Even a small study showing why they underperform would clarify the design choice.\n\nQ4: Could you provide a few negative cases where GraphCliff fails on known cliff pairs and analyze whether the failure is due to insufficient local sensitivity, long-range diffusion, or data sparsity? This would sharpen the pros and cons of GraphCliff"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Uif7051hIX", "forum": "Vk0L8ToRa4", "replyto": "Vk0L8ToRa4", "signatures": ["ICLR.cc/2026/Conference/Submission10669/Reviewer_mPRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10669/Reviewer_mPRp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760640815812, "cdate": 1760640815812, "tmdate": 1762921921490, "mdate": 1762921921490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GraphCliff, a gated graph neural network that integrates short-range (GINE) and long-range (Chebyshev) filters to capture subtle structural variations, known as activity cliffs, in molecular property prediction. The model explicitly balances local substructural sensitivity and global molecular context, addressing over-smoothing issues in conventional GNNs. Extensive experiments across 30 MoleculeACE datasets and small-scale LSSNS benchmarks demonstrate consistent improvements and enhanced discriminative node embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and challenging domain-specific issue—*activity cliffs*—that conventional GNNs struggle with.\n- The proposed gating design effectively balances local and global information, reducing over-smoothing while preserving local sensitivity.\n- The experiments are thorough, including 30 datasets, multiple baselines, ablation and interpretability analyses (e.g., Hop-wise sensitivity, Dirichlet energy, …)."}, "weaknesses": {"value": "- Although the overall idea of integrating short- and long-range information is reasonable, the novelty of the approach is somewhat limited, as similar hybrid architectures (e.g., GROVER, GraphTrans) have already been proposed. The paper should more clearly articulate how GraphCliff’s gating design provides advantages specific to molecular *activity cliff* prediction.\n- Minor issues in figure and notation:\n    - Figure 1 (Overall architecture of GraphCliff) is visually unrefined and lacks clear correspondence between visual components and the mathematical formulation, such as X, h, z.\n    - Some notations and dimensional definitions are missing or ambiguous\n\nGROVER : Rong, Yu, et al. \"Self-supervised graph transformer on large-scale molecular data.\" *Advances in neural information processing systems* 33 (2020): 12559-12571.\n\nGraphtrans : Wu, Zhanghao, et al. \"Representing long-range context for graph neural networks with global attention.\" *Advances in neural information processing systems* 34 (2021): 13266-13279."}, "questions": {"value": "The paper emphasizes integrating short- and long-range information. However, prior works such as GROVER and GraphTrans have already explored combining local message-passing GNNs with Transformer-based long-range modeling.\n- Does it achieve superior results even when compared to such hybrid models? \n- What are the expected advantages of such hybrid models in terms of sensitivity, interpretability, and over-smoothing analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TwkVptdOyv", "forum": "Vk0L8ToRa4", "replyto": "Vk0L8ToRa4", "signatures": ["ICLR.cc/2026/Conference/Submission10669/Reviewer_m4Bd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10669/Reviewer_m4Bd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632452501, "cdate": 1761632452501, "tmdate": 1762921921029, "mdate": 1762921921029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper prosposes GraphClif,  a short–long range gating mechanism to explicitly integrate local substructural sensitivity and global molecular context, mitigating over-smoothing while preserving expressive molecular graph representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The empirical results are strong and were done on 30 benchmarks, nonetheless all of them stem from the same dataset.\n2. The visualization obtained from the gating mechanism seems to provide interesting insights that align with domain priors\n3. The approach to combine high and low frequency signals with gating is intuitive and simple.\n4. The authors provide ablation studies."}, "weaknesses": {"value": "1. The approach is tailored and demonstrated to molecules, and it is not clear whether other domain can benefit from it.\n2. The empirical evaluation although very extensive, focuses only on ChEMBL, and it remains unknown if this method is also beneficial to other domains or datasets. Evaluating it on other diverse benchmarks from other domains and other tasks may be more convincing on the merits of this work.\n3. Based on the two above comments, it is possible the contribution is incremental as it is beneficial only for very specific tasks and types of data. Nonetheless it is possible that this problem of its own is important enough to justify a tailored architecture. As I am not from the molecular field, I lack the ability to judge on the importance of this problem of its own, but rather commenting on the broad contribution of the method to the graph community. \n4. One thing that can strengthen the method is some theoretical example where it is provable that without this combination of high and low rank, the task is unrealizable e.g. with 1-WL GNN, but it is realizable with your method. This would motivate the generality of tis approach to solve general cases as shown here empirically, from the theoretical side."}, "questions": {"value": "1. Could you provide other examples rather than molecular predictions where this approach is critical ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8Y0rRGNWIM", "forum": "Vk0L8ToRa4", "replyto": "Vk0L8ToRa4", "signatures": ["ICLR.cc/2026/Conference/Submission10669/Reviewer_b6d2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10669/Reviewer_b6d2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850115722, "cdate": 1761850115722, "tmdate": 1762921920524, "mdate": 1762921920524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve the discontinuity problem known as the activity cliff, where small structural differences significantly impact activity in QSAR. This paper proposes a new graph neural network called GraphCliff. The proposed method is inspired by StripedHyena2 and integrates local interactions with global interactions using gating mechanisms. The proposed method is applied to 39 QSAR tasks, including those addressing the activity cliff problem, and its predictive performance is evaluated. Additionally, ablation studies are conducted to analyze the impact of local structure on activity cliffs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. (Originality) As noted in this paper, methods for modeling long-range dependencies have been studied in several fields, such as genome language models. Approaches integrating local and global interactions have been studied, including the Hyena Hierarchy. However, to my knowledge, this paper is the first to apply this idea to the long-range dependency problem in graph learning.\n2. (Quality) The paper deals with the important and specific problem of the activity cliff. Furthermore, the numerical experiments cover a wide range of experimental settings, including 16 baseline methods and 39 datasets, which strengthens the credibility of its claims.\n3. (Clarity) The writing is clear. The paper's structure is appropriate, and the explanations in each section are straightforward. I had no significant difficulty in understanding the paper's main points."}, "weaknesses": {"value": "1. The discussion of the numerical experiment results has room for improvement. Specifically, I question whether the presentation of Table 1 is appropriate. This table only highlights the datasets where the proposed method achieves the highest accuracy. Results for the remaining datasets are provided in the appendix. However, if I do not miss any information, no validation or discussion regarding them is presented.\n2. While the paper claims the proposed method achieves the best overall performance (L.309), the basis for this claim is unclear. This claim should be substantiated through a quantitative evaluation using all 39 datasets.\n3. Section 4 provides a detailed analysis of existing methods other than the proposed one (L.310--329). While this analysis is valuable, it deviates from the main focus of this paper, that is, validating the accuracy of the proposed method, and is therefore less important.\n4. If I have not missed any information, the method for determining hyperparameters is not described."}, "questions": {"value": "1. I would like to clarify the basis for claiming that the proposed method achieved the best overall performance.\n2. I would like the authors to deepen the analysis of the causes for the poor prediction accuracy of the proposed method on certain datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WqY1juJCOr", "forum": "Vk0L8ToRa4", "replyto": "Vk0L8ToRa4", "signatures": ["ICLR.cc/2026/Conference/Submission10669/Reviewer_f4qn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10669/Reviewer_f4qn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894576443, "cdate": 1761894576443, "tmdate": 1762921919909, "mdate": 1762921919909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}