{"id": "VgVeQpagf7", "number": 22721, "cdate": 1758334791185, "mdate": 1759896850700, "content": {"title": "High Performance Differentially Private Fine-Tuning using Dataset Distillation", "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD),  which iteratively perturbs clipped per-sample gradients and tracks the cumulative privacy risk using composition accounting, has become a cornerstone in private deep learning. Despite its versatility, DP-SGD in practice faces several limitations. It is constrained by the number of gradient iterations permissible under a limited privacy budget, and is restricted by incompatibilities with common deep learning techniques like ensembling and BatchNorm, and typically produces only a single trained model. In this work, we propose an algorithm for generating a differentially-private (DP) synthetic version of a sensitive dataset. This allows the synthetic dataset to be distributed and postprocessed freely without additional privacy loss, giving more flexibility than DP-SGD. Building on dataset distillation—by producing compact synthetic datasets that preserve downstream performance— we introduce SPS (Summarize–Privatize–Synthesize) and its enhanced variant SPS+. In contrast to prior works, SPS is, to our knowledge, the first alternative to DP-SGD that attains higher accuracy on image-classification tasks. Concretely, on CIFAR 10 / CIFAR 100 with privacy budget $\\epsilon=1$, SPS+ achieves 96.2/76.6% top-1 accuracy, outperforming state-of-the-art (SOTA) DP-SGD results 94.8/70.3%.", "tldr": "We show that differentially private dataset distillation can outperform with DP-SGD for private image classification", "keywords": ["Differential Privacy", "Dataset Distillation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c925c3c8da9e15a6936e815da265e712cea35611.pdf", "supplementary_material": "/attachment/d16940f505d5e036345cdc80eea85865f8f2d748.zip"}, "replies": [{"content": {"summary": {"value": "This paper integrates differential privacy into the D3S data synthesis framework and evaluates the utility of the resulting private synthetic data on downstream training tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper effectively demonstrates that models trained on the private synthetic data can achieve better accuracy compared to those trained with DP-SGD on the real data."}, "weaknesses": {"value": "The comparison is primarily against DP-SGD. To properly situate the contribution, it is crucial to also benchmark against other state-of-the-art private data synthesis methods on both (1) statistical fidelity of the data and (2) utility on downstream tasks."}, "questions": {"value": "1. Figure 3 are difficult to interpret. A quantitative evaluation would provide a more clear measure of image quality across different privacy budgets.\n\n2. In Figure 4, the utility increase as the compression ratio increases. What happens when the ratio is >1? Does performance also increase, or does it simply plateau? This is important for understanding the method's limits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dyP4CQXdj3", "forum": "VgVeQpagf7", "replyto": "VgVeQpagf7", "signatures": ["ICLR.cc/2026/Conference/Submission22721/Reviewer_UvM8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22721/Reviewer_UvM8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760629927929, "cdate": 1760629927929, "tmdate": 1762942356209, "mdate": 1762942356209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new DP synthetic data generation algorithm inspired from the data distillation literature.\n\nTheir method is named **SPS (Summarize-Privatize-Synthesize)**. It leverages a public, pre-trained model to:\n1.  **Summarize** the private dataset by extracting intermediate activation statistics (class-conditional and global means and covariances).\n2.  **Privatize** these statistics in a single step using the Gaussian Mechanism (by clipping and noising the sum of per-sample statistics).\n3.  **Synthesize** a new dataset by optimizing it from random noise to match these privatized statistics.\n\nThe authors also introduce **SPS+**, an enhanced version that incorporates **Multistage Clipping (MC)** to iteratively refine the statistics and **Grouped Pseudo-classes (GPC)** to mitigate the high noise in many-class settings (like CIFAR-100).\n\nThe authors claim SPS+ is the first generation-based method to achieve state-of-the-art accuracy, matching or outperforming DP-SGD on CIFAR-10 and CIFAR-100 benchmarks. They also demonstrate the method's flexibility in practical applications like private federated and continual learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of applying data distillation techniques is a nice conceptual contribution. Their key insight is to adapt a specific family of DD—intermediate activation statistic matching (like D3S)—which is uniquely compatible with DP, as the privacy cost is incurred in a single measurement step (privatizing the statistics).\n\nExperimental results seem strong."}, "weaknesses": {"value": "- The presentation can be difficult to follow for a wider audience that is not already familiar with the data distillation literature. For example, I believe a more informal presentation of the pseudocode would avoid bogging down the reader with notation.\n- Although it is acknowledged by the authors, the computational cost is significant and comparable to DP-SGD, which nullifies one of the advantages of DP synthetic data\n- The released data statistic is very high-dimensional, as the algorithm releases both first and full second-moment (covariance) statistics. For example, SPS+ achieves SOTA classification accuracy on CIFAR10 even at $\\varepsilon=1$. At this privacy level, the reported noise level $b_0 \\approx 4$, while the reported dimension of the release statistic is at least $2\\cdot192^2 = 73,728$, and the clipping radius is at most 3072. In other words, the magnitude of the Gaussian noise added to ensure privacy is at least $4\\cdot 3072\\cdot \\sqrt{2*192^2} = 3,336,548$ . Since there are only 50,000 datapoints in CIFAR10, it's not clear to me if even the mean can be released with low error."}, "questions": {"value": "- Could the authors explain how they were able to overcome the curse of dimensionality in DP?\n\nI look forward to reading the authors' response and would be more than happy to raise my score upon clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Pw2qrpt20u", "forum": "VgVeQpagf7", "replyto": "VgVeQpagf7", "signatures": ["ICLR.cc/2026/Conference/Submission22721/Reviewer_fbnc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22721/Reviewer_fbnc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846022169, "cdate": 1761846022169, "tmdate": 1762942355819, "mdate": 1762942355819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new algorithm for private data synthesis based on the line of dataset distillation, with a focus on image classification tasks. The key idea is to adapt dataset distillation to the privacy setting by summarizing the private dataset via intermediate activation statistics from a publicly pretrained model, privatizing these statistics, and then synthesizing images that match the privatized statistics. On CIFAR-10/100, SPS+ achieves state-of-the-art accuracy across existing DP data synthesis algorithms as well as DPSGD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written. Technical writings are very clean and easy to follow. \n\nThe experiment results are very strong, SPS+ achieves accuracy competitive with, and sometimes better than, DP-SGD on CIFAR-10/100"}, "weaknesses": {"value": "The proposed SPS and SPS+ always generate a class-balanced synthetic dataset by design, without estimating or preserving the true class distribution of the private data. This is harmless for balanced benchmarks like CIFAR-10/100, but in most real-world datasets it can lead to misrepresentation of class frequencies. \n\nThe framework exposes a large number of hyperparameters, including layer selection, which has a huge search space. This will require non-trivial search and engineering effort. Furthermore, HP tuning will also incur privacy costs. The sensitivity with respect to hyperparameters needs to be discussed. \n\nThe current algorithm is restricted to image classification tasks, with all core experiments conducted on 32×32 images (CIFAR-10 and CIFAR-100). While these results are valuable and demonstrate strong performance under controlled conditions, the approach’s scalability to higher-resolution or higher-dimensional data remains unclear."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l16B3Z8joe", "forum": "VgVeQpagf7", "replyto": "VgVeQpagf7", "signatures": ["ICLR.cc/2026/Conference/Submission22721/Reviewer_ziZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22721/Reviewer_ziZt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762399909632, "cdate": 1762399909632, "tmdate": 1762942355525, "mdate": 1762942355525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}