{"id": "FrwqxlUqdL", "number": 18777, "cdate": 1758290799289, "mdate": 1759897081775, "content": {"title": "Enhancing Graph Transformers with Spectral Guidance in Attention", "abstract": "Existing Graph Transformers often overlook the limitations of self-attention mechanism without inductive bias. The pure self-attention tends to aggregate features from unrelated nodes and misalign attention with graph structures, leading to suboptimal modeling of relational dependencies. Moreover, operating solely in the spatial domain, self-attention underutilizes graph spectral components that correspond to more detailed and comprehensive relational patterns. To address the above issues, we propose the Spectral-Guided Attention Graph Transformer (SGA-Former), which introduces rich structural priors from the graph spectral domain to guide attention learning. Specifically, we design two Spectral Relation Metrics as attention bias, which capture complementary low and high-frequency structural patterns. To leverage these priors, we develop the Spectral-Guided Attention Enhancer (SGA-Enhancer), which filters redundant attention scores and emphasizes important node relationships based on the spectral metrics. Incorporating SGA-Enhancer, SGA-Former builds dual-branch Spectral Attention Layers that jointly utilize both spectral views, enabling more balanced and structure-aware attention learning. Extensive experiments show that SGA-Former consistently achieves superior performance across a wide range of graph learning tasks.", "tldr": "We propose SGA-Former, a novel graph Transformer that first leverages spectral priors as inductive bias to explicitly facilitate selective attention learning and and achieve superior performance across diverse graph tasks.", "keywords": ["Graph Learning", "Graph Transformer", "Graph Spectral Theory"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d4fb836ee6fa93d20df0b333ec7d761e926ea39.pdf", "supplementary_material": "/attachment/a36d69e7d0af6be54dba91788a06bd2285dff9ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a spectral-guided Graph Transformer model, SGA-Former, which introduces spectral domain information as an inductive bias. By incorporating Laplacian-powered high- and low-frequency spectral relation matrices into the attention mechanism, it achieves structure-aware attention distributions that efficiently capture both global and local graph structural features. The paper theoretically demonstrates that its expressive power surpasses Graph Transformers with shortest-path biases. Extensive experiments validate its significant performance improvements across multiple benchmark tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel Graph Transformer architecture that leverages spectral domain information as an inductive bias, effectively addressing limitations in existing Graph Transformers, demonstrating clear innovation.\n\n2. It provides an in-depth analysis of the relationship between spectral information and graph structure, designs two spectral relation metrics to guide attention learning, and theoretically validates the effectiveness of the proposed method.\n\n3. Extensive experiments are conducted on multiple graph learning tasks, comparing with various baseline models, and the results convincingly demonstrate SGA-Former’s superior performance and generalization capability."}, "weaknesses": {"value": "1. Although the authors claim that the spectral relation matrices can be computed via simple matrix operations on the existing graph structure, the per-layer/per-graph time complexity, memory consumption, and actual runtime on large-scale graphs are not provided.\n\n2. The definitions of (M_{\\text{low}}) and (M_{\\text{high}}) involve multiple powers of (L_{\\text{sym}}) and linear combinations of (A) and (D). While avoiding eigen-decomposition, in practice these matrices may incur high storage and transmission costs for batched graphs, and the generality across different graph sizes is unclear.\n\n3. The SA-Pruner adopts a hard top-α strategy, which prevents direct gradient propagation and may affect the stability of end-to-end training."}, "questions": {"value": "1. Please provide an analysis of the time and space complexity of computing the spectral relation matrices per layer and per graph, and report runtime performance on large-scale graphs.\n\n2. Explain the storage, transmission, and sparsity optimization strategies for spectral matrices in batched graph processing, and validate the method’s scalability across different graph sizes.\n\n3. Discuss the impact of the non-differentiable pruning strategy in SA-Pruner on end-to-end training, and consider providing differentiable alternatives or stability verification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sbVVcRmPy1", "forum": "FrwqxlUqdL", "replyto": "FrwqxlUqdL", "signatures": ["ICLR.cc/2026/Conference/Submission18777/Reviewer_AR7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18777/Reviewer_AR7x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760954404210, "cdate": 1760954404210, "tmdate": 1763000001398, "mdate": 1763000001398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a new graph Transformer called SGA-Former. The authors argue that existing attention-based GNNs are built on spatial domain which naturally ignores the graph spectral information. To this end, the authors develop a new attention module based on low- and high- frequency filters to preserve various frequency information between nodes. Experimental results on various datasets show the effectiveness of the proposed method on graph data mining tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper is well-organized and easy to follow.\n\n2.The authors provide the theoretical analysis of the proposed method.\n\n3.The proposed SGA-Former provides new insights for GTs."}, "weaknesses": {"value": "1.The research gap is somewhat overclaimed.\n\n2.The complexity analysis is missing\n\n3.Mainstream baselines are missing.\n\n4.The proposed method seems to be sensitive to the hyper-parameters."}, "questions": {"value": "1.The authors claim the limitation in existing GTs which lack objectivity. There are also several works, such as Specformer and GrokFormer, which are built on spectral information-guided attention modules.\n\n2.Matrix eigendecomposition and node sampling are time-consuming operations. Given the marginal performance gains according to the experimental results, I suggest the authors provide the corresponding complexity analysis of the proposed method.\n\n3.Moreover, the efficiency study is also required to determining the training cost of SGA-Former as well as baselines.\n\n4.Some recent GTs are suggested to be added as baselines.\n\n5.According to Table 8, I have noticed that the proposed method seems to be sensitive to some hyper-paramerters, such as “hidden dim”, “k” and “PE dim”. Based on the results of performance comparison, it is questionable whether the proposed method can bring meaningful performance gain in graph mining tasks.\n\n6.In addition, according to Table 7, the average number of nodes within a graph is quite small. Does this situation imply that SGA-Former can only handle small graphs and hard to be conducted on large-sacle graphs with thousands or millions of nodes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UyAat5gQEP", "forum": "FrwqxlUqdL", "replyto": "FrwqxlUqdL", "signatures": ["ICLR.cc/2026/Conference/Submission18777/Reviewer_G9bB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18777/Reviewer_G9bB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620749272, "cdate": 1761620749272, "tmdate": 1763000001253, "mdate": 1763000001253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces low-frequency and high-frequency matrices to inject additional inductive biases into the attention mechanism of Graph Transformers, with theoretical proofs demonstrating the model's strong expressive power. Extensive comparisons against numerous baselines on standard graph datasets highlight the model's superior empirical performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and easy to follow, with well-chosen figures that enable readers to quickly grasp the proposed method.\n\n2. The theoretical analysis is rigorous, providing solid proofs of the model's strong expressive power.\n\n3. The experiments are comprehensive, demonstrating the method's effectiveness through comparisons with a wide range of baselines on multiple standard graph datasets. Ablation studies further validate the necessity of each component."}, "weaknesses": {"value": "1. Minor errors:\n - Page 1, line 085: \"with both both\" contains a redundant \"both\".\n - Page 1, line 090: \"demenstrate\" should be \"demonstrate\".\n - Table 2, Peptides-func: the second-best model is GRIT, not MSA-GT.\n\n\n2. The hyperparameter α is only analyzed on the MNIST and CIFAR datasets. It would be insightful to evaluate its impact on long-range benchmark datasets to better understand how it influences long-range dependencies.\n\n3. It would be beneficial to include visualizations comparing the learned attention coefficients $ A_{\\text{enhancer}} $ with those from the baseline Graph Transformer to highlight their differences."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QMaFQ5f4tw", "forum": "FrwqxlUqdL", "replyto": "FrwqxlUqdL", "signatures": ["ICLR.cc/2026/Conference/Submission18777/Reviewer_x24W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18777/Reviewer_x24W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720923417, "cdate": 1761720923417, "tmdate": 1763000001541, "mdate": 1763000001541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SGA-Former, a Graph Transformer that injects spectral inductive bias into attention via two polynomial spectral relation metrics (M_low and M_high) and a two-stage Spectral-Guided Attention Enhancer (prune + scale). The design yields a dual-branch attention layer combining low- and high-frequency structure. Experiments across Benchmarking-GNNs, LRGB peptides, and ZINC-full show strong results, with ablations on pruning/scaling, α keeping rate, and polynomial order k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, principled mechanism to inject spectral bias directly into attention via prune-and-scale—easy to implement on top of existing GTs.\n2. No eigendecomposition; clean polynomial construction with clear spatial interpretation.\n3. Consistent empirical gains across diverse tasks; strong ZINC-full/LRGB performance. \n4. Useful expressivity analysis relative to SPD biases."}, "weaknesses": {"value": "1. Theoretical-practical gap: Proposition 1 assumes access to each term $(\\tilde{A})^t$ and $k>\\text{diam}(G)$, while practice uses the summed metric with moderate k. Please reconcile and provide conditions under which the practical SGA-Former inherits the stated advantage.\n2. Efficiency: Despite pruning, runtime stays essentially quadratic unless attention computation itself is sparsified. Compare wall-time/VRAM vs. Exphormer/GPS at similar accuracy; report FLOPs/throughput.\n3. Scope of datasets: Add OGB molecular + heterophily/social graphs and very large graphs to validate claims on long-range and boundary modeling beyond peptides and ZINC."}, "questions": {"value": "1. Complexity: What is the end-to-end time/memory delta vs. GRIT and Exphormer on LRGB (same batch sizes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UypmXpc5WD", "forum": "FrwqxlUqdL", "replyto": "FrwqxlUqdL", "signatures": ["ICLR.cc/2026/Conference/Submission18777/Reviewer_QrVu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18777/Reviewer_QrVu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089521251, "cdate": 1762089521251, "tmdate": 1763000001464, "mdate": 1763000001464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}