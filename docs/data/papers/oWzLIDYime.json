{"id": "oWzLIDYime", "number": 12485, "cdate": 1758208121273, "mdate": 1763559375764, "content": {"title": "Who Matters Matters: Agent-Specific Conservative Offline MARL", "abstract": "Offline Multi-Agent Reinforcement Learning (MARL) enables policy learning from static datasets in multi-agent systems, eliminating the need for risky or costly environment interactions during training. A central challenge in offline MARL lies in achieving effective collaboration among heterogeneous agents under the constraints of fixed datasets, where \\textbf{conservatism} is introduced to restrict behaviors to data-supported distributions. Agents with distinct roles and capabilities require individualized conservatism - yet must maintain cohesive team performance. However, existing approaches often apply uniform conservatism across all agents, leading to over-constraining critical agents and under-constraining others, which hampers effective collaboration.\nTo address this issue, a novel framework, \\textbf{OMCDA}, is proposed, where the degree of conservatism is dynamically adjusted for individual agents based on their impact on overall system performance. The framework is characterized by two key innovations: (1) A decomposed Q-function architecture is introduced to disentangle return computation from policy deviation assessment, allowing precise evaluations of each agent's contribution; and (2) An adaptive conservatism mechanism is developed to scale constraint strength according to both behavior policy divergence and the estimated importance of agents to the system.\nExperiments on MuJoCo and SMAC show OMCDA outperforms existing offline MARL methods, effectively balancing the flexibility and conservatism across agents while ensuring fair credit assignment and better collaboration.", "tldr": "", "keywords": ["Offline reinforcement learning", "Reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7864d5f110f0cc533069edb83ac44e9b885949ec.pdf", "supplementary_material": "/attachment/73e37f5cdbdf97a2b6000917a4377994a9f18cf3.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses challenges in Offline Multi-Agent Reinforcement Learning (MARL), particularly the need for effective collaboration among heterogeneous agents using fixed datasets. It introduces a novel framework called OMCDA (Offline MARL with Conservative Degree Allocation) that dynamically adjusts the conservatism level for each agent based on their impact on overall system performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of dynamically adjusting conservatism levels based on agent impact is a significant advancement in the field, addressing limitations of uniform approaches\n2. The paper provides extensive experimental results, showcasing the effectiveness of OMCDA across various scenarios.\n3. The paper is well-organized, with a logical flow from introduction to methodology, experiments, and conclusions."}, "weaknesses": {"value": "The design of the method is overly complicated, and the rationale behind some aspects of the design has not been justified."}, "questions": {"value": "1. For each agent's Q and conservative term, can a similar effect be achieved by mixing them through the same mixer network？\n  2. Is $\\omega_i^r(o)$ in Eq. 13, $\\omega_j^{c,i}(o)$ in Eq. 14 strictly greater than 0?\n  3. Is it feasible to use another mixer network to fit the computation of $m$ in Eq. 20?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TbHOutfUiA", "forum": "oWzLIDYime", "replyto": "oWzLIDYime", "signatures": ["ICLR.cc/2026/Conference/Submission12485/Reviewer_PtX1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12485/Reviewer_PtX1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811520270, "cdate": 1761811520270, "tmdate": 1762923361452, "mdate": 1762923361452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OMCDA, an offline multi-agent reinforcement learning (MARL) framework that dynamically allocates conservative degrees to individual agents based on their estimated influence on system returns. The key innovation lies in disentangling the Q-function into return and deviation components, enabling agent-specific regularization. The proposed method is evaluated on Multi-Agent MuJoCo and SMAC benchmarks, showing consistent performance gains over existing offline MARL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Problem Formulation: Identifies and addresses the underexplored issue of heterogeneous conservatism in offline MARL — a real and important problem.\n\nTechnical Innovation: The Q-function decomposition and influence-based conservative allocation are technically sound and well-motivated.\nStrong Empirical Results: Outperforms 7 strong baselines across diverse tasks and dataset qualities, with thorough ablations validating each component.\n\nTheoretical Justification: Provides derivations for global-to-local policy consistency under agent-specific conservatism levels, enhancing credibility."}, "weaknesses": {"value": "Scalability Concerns: The influence term requires computing partial derivatives w.r.t. per-agent KL divergences, which may become prohibitive in large-N systems or high-dimensional action spaces.\n\nSensitivity to Total Budget: Performance is sensitive to the choice of total conservative degree $d_{tot}$, yet no principled method is provided for setting it across tasks."}, "questions": {"value": "1. What is the computational overhead of estimating influence terms compared to standard offline MARL methods, especially for N > 10 agents?\n\n2. At present the total conservative budget $d_{tot}$ is treated as a hyper-parameter and tuned manually for each task. Have the authors explored any data-driven or adaptive procedure to automatically infer $d_{tot}$ so that the algorithm can adjust the global deviation allowance without human intervention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FYzWi2unZH", "forum": "oWzLIDYime", "replyto": "oWzLIDYime", "signatures": ["ICLR.cc/2026/Conference/Submission12485/Reviewer_U5BW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12485/Reviewer_U5BW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974434077, "cdate": 1761974434077, "tmdate": 1762923360904, "mdate": 1762923360904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Offline MARL with Conservative Degree Allocation (OMCDA) which aims to control the degree of conservatism for each agent by decomposing the Q-values to return maximizing and conservatism terms. They further use value decomposition akin to QMIX to compute the degree of conservatism for each agent based on how the KL regularization term impacts the global $V$. Experimental results are provided for SMACv1 and MA-MuJoCo."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The  illustrative matrix game is somewhat intuitive. Under some fixed total conservative degree, agent A has a higher influence on total reward, so it should follow more risky behavior, and deviate from the dataset distribution (assuming the higher return action is also in the dataset support). \n\nAlso, updating $\\alpha$ in Eq.55 is interesting since we often want less hyperparameter tuning in offline RL."}, "weaknesses": {"value": "1. The observation function $O$ is missing from the Markov Game formulation. \n1. The reward function $r$ is defined in terms of joint observations, but it should be based on state. \n1. The example used to motivate the work is slightly confusing. For football, the risk-taking vs risk-averse behavior between positions is about taking actions that can lead to states with a high return (i.e. score a goal) but may have low probability of reaching that goal. This is somewhat orthogonal to the concept of conservatism in offline MARL, where risk-averse vs risk-taking is about whether to follow the dataset distribution or not.\n1. $Q_i^r, Q_i^c,  w_i^r, w_i^c$ from Eq. 13 and 14 is not well motivated. It seems that additional decomposition assumptions are required on both the reward component and constraint component. There is no detail provided in appendix E.5 as the authors mention. \n1. Related to above, the validity of Eq.23 is questionable. For instance $Q^{c, i, *}$ is not well-defined. If value decomposition is used, the authors should be more clear about how the IGM assumption is extended to the offline setting and how it is affected by the conservative regularization term.\n1. Even with a correct characterization of the decomposed terms, value decomposition is already quite restrictive. For instance, [1]  showed that it cannot solve simple matrix games, and [2] analyzed the limitations of value decomposition in the offline case. \n1. Eq. 18 computes the influence term depending on how the KL term influences return. However, there is still a mismatch because the denominator is in terms of individual actions while the numerator is defined for joint observations/actions. Thus, there is an implicit dependency with the other agents observations. This kind of influence term can be biased in more complex environments if there is no 1-1 mapping between individual and joint observations. \n1. SMAC results are only provided for v1, but this has already been show to be an outdated benchmark due to the deterministic transitions, and the fact that it can be solved by some simple algorithms that don’t even take observations into consideration [3]. Recent offline MARL algorithms such as ComaDICE already use smacv2 as a benchmark. This significantly weakens the experimental results of this paper. Also, smacv2 results would help with addressing my previous point since it is inherently more partially observable. \n\n\n[1] Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning (Fu et, al. 2021)\n\n[2] AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation (Matsunaga \n\n\n[3] SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning (Ellis et, al. NeurIPS 2023 Datasets and Benchmarks)"}, "questions": {"value": "1. While the illustrative example in Table 1 makes some sense, what happens if the payoffs for taking action A for agent 1 are [-100, 3 ] instead of [3, 3]? \n2. CFCQL and OMIGA seem to be important baselines since they both consider a similar problem of applying individual conservatism. How did you tune their hyperparameters? Is it similarly tuned fairly for OMCDA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q7ikOWUXKF", "forum": "oWzLIDYime", "replyto": "oWzLIDYime", "signatures": ["ICLR.cc/2026/Conference/Submission12485/Reviewer_mivS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12485/Reviewer_mivS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071698876, "cdate": 1762071698876, "tmdate": 1762923360357, "mdate": 1762923360357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}