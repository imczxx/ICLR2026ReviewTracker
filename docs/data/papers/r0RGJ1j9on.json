{"id": "r0RGJ1j9on", "number": 9366, "cdate": 1758120176008, "mdate": 1759897729125, "content": {"title": "Real-Time Robot Execution with Masked Action Chunking", "abstract": "Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. \nPrevious methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot’s executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity.\nOverall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.", "tldr": "", "keywords": ["Robot Manipulation", "Real-time Execution"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c1806d36abbc60b3d0e20ca19c423cabd93d654.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a method for finetuning a pretrained VLA policy toward residual modifications i.e., adjusting an action chunk from the pretrained policy by taking into account delay due to latency. To do so, the paper introduces a curriculum learning method in which the policy first learns to imitate the expert and then learns to do residual corrections conditioned on a wide range of delay values. For sampling at test-time, the model conditions on the actions being executed from the previous chunk (due to delay) and the sampling procedure enables the model to adjust the trajectory towards more optimal actions."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method has been tested on a wide range of tasks in both simulation and in the real world. The paper offers interesting analysis at a low-level such as the kinematics of the robot under various methods (see figure 5). \n2. The paper compares the proposed method with all the appropriate baselines. \n3. The paper carries out several important ablations and offers interesting insights into how baseline methods can further uplift the performance of REMAC."}, "weaknesses": {"value": "1. Lack of clarity in the problem formulation, the definition of inter-chunk discontinuity is very unclear (line 153). It is not clear why two trajectories that diverge must not have actions that are different i.e. if the trajectories are similar only up to time $t$ and then the two trajectories diverge to different observations, their action chunks at time $t + h$ should be different? Furthermore, it is not clear what this discontinuity means and what the expected behavior is if there is no discontinuity. Similarly, intra-chunk inconsistency is unclear. While the first $d$ actions are taken from the previous action chunk $\\textnormal{A}_{t-h}$, I do not understand what the perception-action mismatch is. \n2. Lack of clarity in the method discussion. Generally, since there are various choices of implementing flow matching, it would be helpful to either have a preliminaries or an appendix section where you define both the predicted flow and the ground truth flow and, in your notation, specify which policy each is being sampled from. I understand that abstracting these modeling choices helps put the emphasis more on the generality of your method but, at least, some concrete examples will help the reader develop a more concrete understanding of your method. Similarly, the switch to using $\\textnormal{x}_\\mathrm{p}$ to denote the action seems strange. I also think section 4.2 is extremely dense and unclear, from notation to definitions (for e.g., the definition of $f$ in equation 6). \n3. While the method discusses handling temporal consistency, it is unclear whether this method handles the latency issues as discussed in the RTC paper (Black et. al. 2025). It seems like the delay-aware policy would also have high latency, so is it that your method relies on the conditioning on the delay to take care of that? \n\nAs is clear, most of my complaints are with respect to the clarity of the discussion of the problem formulation and the method. I would be happy to raise my score if these issues with clarity are adequately answered. Apart from them, I have some questions about the scalability of the method. \n\n4. The method seems quite data expensive in fine-tuning. For example, for the real world tasks, you collected 200 trajectories. It would be useful to see how the performance changes as we change the size of the dataset. \n5. I am not sure that this method would generalize and I suspect that this might harm the robustness of the pretrained policy. Since you are finetuning offline, it is clear that the REMAC would do well on tasks/environments seen during finetuning – the model knows what the target expert actions are and the model knows what the pretrained policy outputs at these states in the offline dataset. This raises the question of whether or not this fine-tuned model would reliably adapt the pretrained policy’s behavior in unseen states. For example, one issue that might arise is that, at unseen states, the policy might be more uncertain leading to higher entropy of actions from the pretrained policy, and as such your finetuned policy needs to learn how to modify a large number of actions to match the optimal one (which the model does not know yet since this is at test time). Since you are already testing this on $\\pi_0$ which demonstrates some generalization, it would be good to evaluate on some tasks/environments not seen during fine-tuning.\n\n[1] Kevin Black, Manuel Y. Galliker, Sergey Levine. Real-Time Execution of Action Chunking Flow Policies. \n[2] Yuejiang Liu, Jubayer Ibn Hamid, Annie Xie, Yoonho Lee, Maximilian Du, Chelsea Finn. Bidirectional Decoding: Improving Action Chunking via Guided Test-Time Sampling"}, "questions": {"value": "See weaknesses. For me, the most important questions are with regards to clarity and handling latency (see points 1-3 in Weaknesses section)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CFlivIzgi0", "forum": "r0RGJ1j9on", "replyto": "r0RGJ1j9on", "signatures": ["ICLR.cc/2026/Conference/Submission9366/Reviewer_LpbR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9366/Reviewer_LpbR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506392152, "cdate": 1761506392152, "tmdate": 1762920984496, "mdate": 1762920984496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes REMAC for asynchronous inference in real-world robotics, handling chunk discontinuities by finetuning learned policies via flow matching. The key is to condition the policy on a ground-truth action prefix during training and use flow matching to predict the remaining actions. By randomly sampling delay d and adjusting the flow matching curriculum, REMAC trains the policy to adapt to varying inference delays. Experiments on both simulation and real-world benchmarks show that REMAC outperforms baselines under high inference delay."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. REMAC can adapt to various inference delays d with a single training process, without needing to retrain for each delay. The use of LoRA modules maintains model performance while reducing training overhead.  \n2. Flow matching enables the model to learn fine-grained continuity between action prefixes and optimal future actions, capturing the dependency between earlier and later actions.  \n3. REMAC achieves strong results across both simulated and real-world benchmarks."}, "weaknesses": {"value": "1. REMAC has structural requirements on the dataset, which needs to contain diverse future action sequences from the same observation. With this requirement, the model can learn to correct deviating action prefixes resulting from inference delay. Without local adjustment samples in the dataset, REMAC may perform no better than behavioral cloning.  \n2. During inference, the agent must know how long inference takes in order to select an appropriate prefix length d from the previous action chunk. In real-world settings with variable communication delays, accurately estimating this delay may be challenging.  \n3. REMAC improves policies' robustness to inference delays only for flow-matching-based policies. If the bottleneck policy is not trained with flow matching, REMAC requires training from scratch."}, "questions": {"value": "1. I'm confused why REMAC outperforms the pretrained policy (Naive) even when d = 0, according to Figure 2. Is it because REMAC uses additional expert data compared with Naive? If the baselines (Naive, RTC, BID) also access the additional expert data, will they outperform REMAC?  \n2. Can we apply the same dataset used in the pretrained policy (i.e., Naive) to REMAC, so that REMAC does not use additional expert data?  \n3. Can you further explain the Residual Alignment loss Eq. 4? What's the difference between Eq. 4 and the Prefix Masking loss Eq. 2?  \n4. In the simualtion tasks, how is the expert dataset D used for REMAC constructed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fcom9XWE6S", "forum": "r0RGJ1j9on", "replyto": "r0RGJ1j9on", "signatures": ["ICLR.cc/2026/Conference/Submission9366/Reviewer_NdJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9366/Reviewer_NdJX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598497136, "cdate": 1761598497136, "tmdate": 1762920984171, "mdate": 1762920984171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies real-time robot control with chunk-based VLA policies under asynchronous inference, where inference latency causes the robot to execute stale actions while new ones are being generated. The authors highlight a neglected failure mode—intra-chunk inconsistency—in addition to the known inter-chunk discontinuity issue. They propose REMAC, a training-time strategy that applies prefix masking to simulate partial stale-action execution, a self-conditioned curriculum to mitigate exposure bias, and residual LoRA finetuning to correct the base policy. The method increases robustness under varying latencies without adding inference overhead. Experiments on Kinetix and a real Franka arm show improved success rates and smoother execution across delay conditions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Identifies and formalizes intra-chunk inconsistency as a distinct source of degradation in asynchronous execution.\n- Simple and computationally efficient method that is compatible with existing VLA architectures.\n- Strong empirical evidence across simulation and real-robot settings, including latency sweeps and ablation studies.\n- No additional inference latency, in contrast to recent test-time smoothing approaches (e.g., RTC).\n- Parameter-efficient finetuning strategy that preserves the backbone’s capability.\n- Compatible with BID/RTC test-time methods, demonstrating good composability with existing techniques."}, "weaknesses": {"value": "- The method assumes access to accurate and bounded delay estimates. It is unclear how robust the approach is when latency measurements are noisy, rapidly fluctuating, or adversarially spiky.\n- Training samples delays uniformly, but real-world latency tends to be bursty and temporally correlated. Additional evaluation under realistic network- and compute-induced delay profiles would strengthen the claims.\n- Finetuning with masked actions may shift the behavior of the underlying VLA model. The paper does not report out-of-distribution or language generalization results, making it difficult to assess whether broader generalization and grounding capabilities are preserved.\n- The method is demonstrated only on flow-matching VLA architectures. Since REMAC integrates with the sampling process and residual flow fields, it is unclear whether the approach directly applies to non-flow controllers (e.g., autoregressive VLA policies or transformer-based continuous action models). A discussion or preliminary evidence on generality across policy parameterizations would strengthen the paper."}, "questions": {"value": "1. How robust is REMAC to inaccurate delay estimates or rapidly varying latency?\n2. Does delay-aware finetuning impact generalization to unseen tasks/objects or language inputs?\n3. Would a continuous or learned delay embedding outperform discrete integer conditioning?\n4. How does the method behave when real latency exceeds the maximum trained delay?\n5. Can the approach be extended to handle coupled observation latency, not just action delay?\n6. The method uses a fixed chunk length, but in practice the optimal horizon may vary with latency and task demands. Can the approach be extended to dynamically adjust chunk length based on system delay or task complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jglFDXtg2m", "forum": "r0RGJ1j9on", "replyto": "r0RGJ1j9on", "signatures": ["ICLR.cc/2026/Conference/Submission9366/Reviewer_n9Cq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9366/Reviewer_n9Cq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982116039, "cdate": 1761982116039, "tmdate": 1762920983814, "mdate": 1762920983814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses real-time robot execution with action chunks. The authors identify _intra-chunk inconsistency_ as a previously overlooked issue. They propose a method for learning a delay-aware policy by masking the portion of the prefix that restricts supervision to only the executable slice of each chunk. The proposed method incurs no additional latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper makes solid contributions to an important practical problem. Asynchronous inference is a natural system-level solution for real-time execution. Identifying intra-chunk inconsistency as a distinct failure mode adds value.\n- The paper does a thorough experimental analysis, comparing with the state-of-the-art method RTC, and doing ablation studies on each component of the proposed method.\n- Figure 1(c) provides a great concrete example of the problem the paper is aiming to solve."}, "weaknesses": {"value": "- The paper cites intra-chunk inconsistency as a core motivation for the proposed method, but doesn't provide any direct evidence that this is a major issue.\n- Is there a way to apply this idea to policy classes other than flow-matching?\n- I don't understand the residual alignment term. Don't the two $\\tilde{u}$ terms in eq (4) cancel out, making it equivalent to (2)? Is there a typo somewhere, or am I missing something?\n- The method requires specifying d_max as a hyperparameter during training. Do you have a sense of what happens when the model encounters delays longer than d_max? Does performance degrade gracefully or fail catastrophically?"}, "questions": {"value": "Please see the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bWnB7eryNx", "forum": "r0RGJ1j9on", "replyto": "r0RGJ1j9on", "signatures": ["ICLR.cc/2026/Conference/Submission9366/Reviewer_szGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9366/Reviewer_szGQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762229184838, "cdate": 1762229184838, "tmdate": 1762920983374, "mdate": 1762920983374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}