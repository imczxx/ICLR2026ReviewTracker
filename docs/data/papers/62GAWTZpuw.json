{"id": "62GAWTZpuw", "number": 14737, "cdate": 1758242753360, "mdate": 1759897352044, "content": {"title": "Knowledge Debugger: Diagnosis of Knowledge Inconsistency with Multimodal Graph", "abstract": "Human knowledge can naturally be organized as multimodal graphs, with prime examples including research papers or Wikipedia pages. However, identifying information inconsistencies within such knowledge-intensive documents remains challenging. These inconsistencies can be explicit, such as numerical discrepancies between tables and their textual descriptions, or implicit, like differing conclusions presented at the beginning and end of an article. Large Language Models (LLMs) have shown great potential in detecting these types of inconsistencies. Nevertheless, their practical deployment is often hindered by limitations such as restricted context windows and high inference costs. Additionally, standard Retrieval-Augmented Generation (RAG) approaches struggle to effectively capture intricate reference relationships within multimodal graphs. To address these challenges, we propose Knowledge Debugger, an efficient Graph Neural Network (GNN)-based framework that can identify diverse types of knowledge inconsistencies in multimodal data. To evaluate the effectiveness of our method, we built a Multimodal Knowledge Debugging Benchmark (MKDB) including 3 modalities, 699 Wikipedia pages, more than 10000 research papers, and more than 10000 knowledge-debugging tasks with answers. With our approach, we leverage LLMs to generate high-quality labels for training multimodal GNNs. The trained GNNs demonstrate strong performance in consistency checking tasks on multimodal graphs. Specifically, we beat the best RAG methods by 11\\% on node-level bug detection tasks. By employing GNNs, we significantly enhance system efficiency and scalability, enabling effective and practical inconsistency detection in complex multimodal knowledge structures.", "tldr": "", "keywords": ["graph neural network", "knowledge inconsistency"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67a244fb0f3f9d7f266beb56857a2495644d9279.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents Knowledge Debugger, a graph based framework for multimodal knowledge inconsistency detection and correction over long documents. Nodes represent paragraphs, tables, and images. Edges encode follow, reference, and citation relations. Tasks cover node level detection and correction, as well as edge level detection and correction. The authors also introduce the MKDB benchmark that spans Wikipedia and research papers with three modalities and a large number of debugging tasks. Results show consistent gains over similarity based and RAG style baselines with a compact GNN that targets efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is relevant and well defined for real long document pipelines.\n\n2. Document as graph is a natural fit, combining semantics and structure for finer-grained inconsistencies.\n\n3. Benchmark covers multiple modalities and separates detection and correction, which supports fair comparison."}, "weaknesses": {"value": "1. Current baselines focus on generic retrieval. Missing comparison likely to raise the non-GNN ceiling includes table-aware retrieval with header and cell structure, citation-aware reranking, and multi-hop evidence aggregation with a consistency scorer.\n\n2. Relation-level ablations are limited. The method defines several edge types. Please include drop one relation and noise injection ablations to quantify which relations matter most, and add scaling curves with respect to graph density and cross-document edges.\n\n3. Report multiple seeds, mean with ninety-five percent confidence intervals, and significance tests for headline results. Clarify test set sizes per subtask to support stability claims.\n\n4. Correction is mainly measured by retrieval. For node fixes, include human-judged factual correctness and numeric accuracy. For edge fixes, report graph consistency after edits, and if new errors were added."}, "questions": {"value": "1. How do citation-aware and table-aware retrievers and multi-hop aggregation compare to your GNN retriever?\n\n2. Which edge types contribute most, based on drop one relation or noise tests.\n\n3. Can you provide a human evaluation of final rewrites and numeric corrections and a graph-level consistency score after edge edits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gjnknt7D9K", "forum": "62GAWTZpuw", "replyto": "62GAWTZpuw", "signatures": ["ICLR.cc/2026/Conference/Submission14737/Reviewer_uV6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14737/Reviewer_uV6s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761086051520, "cdate": 1761086051520, "tmdate": 1762925097242, "mdate": 1762925097242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a work Knowledge Debugger, which is target of diagnoses of knowledge in consistency with multimodal knowledge graph. First, a multimodal knowledge debugging benchmark named MKDB is proposed for the knowledge debugger task. The benchmark, including four types of debugging tasks, about node- and edge- level  bug detection and correction. Authors also propose GNN – based algorithm for the knowledge, debugger task. Experiments on the MKDB dataset show that the GNN-based method is effective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work targets to identifying information consistencies with seeing knowledge intensive documents, which is important. \n2. This work not only introduces knowledge debugger method, but also a benchmark for this task. \n3. The paper is clearly written and easy to understand."}, "weaknesses": {"value": "1. The benchmark proposed in this paper is interesting, but some details of the benchmark construction is not clearly presented. For example, (1) during the construction of node-debugging tasks, the LLM is used to generate conflicting information, but the accuracy of the generated conflict information is not reported, which is important for evaluating the quality of the benchmark. (2) the evaluation metrics for correction  is based on ranking, it is unclear, what are the candidates for ranking. \n2.  In table 1 and table 2, the results of proposed GNN-based method are missing for the Research dataset. \n3. The training detail of the GNN-based method is missing, for example, the initialisation of the node embedding."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sflbmX4P5T", "forum": "62GAWTZpuw", "replyto": "62GAWTZpuw", "signatures": ["ICLR.cc/2026/Conference/Submission14737/Reviewer_9r8A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14737/Reviewer_9r8A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970108074, "cdate": 1761970108074, "tmdate": 1762925096691, "mdate": 1762925096691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Graph Neural Network (GNN)-LLM hybrid framework for detecting and correcting knowledge inconsistencies in multimodal documents like Wikipedia pages and research papers. The approach represents multimodal data, e.g., text, tables, and figures as nodes while the relations between nodes in a reading flow (paragraphs) as edges, and formulates inconsistency detection as node and edge classification tasks. For evaluation, the authors also introduce a moderate-sized Multimodal Knowledge Debugging Benchmark (MKDB)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of automated detection of factual and logical inconsistencies in multimodal, structured documents is underexplored, yet impactful. \n2. The benchmark dataset is quite useful for the research community. \n3. The core insight of modeling multimodal knowledge as graph-based representations and framing inconsistency detection as node or edge classification tasks is novel and powerful."}, "weaknesses": {"value": "1. **Inconsistencies and missing table data**: There are critical inconsistencies and missing data in the experimental results presented in Table 2 and Table 3. For example, the results for 'Ours (GNN-based)' are missing for the 'Research' dataset in both tables. This undermines the paper's core claims of superiority. \n2. **Issues with Oracle baseline**: The definition and role of the 'RAG (oracle)' baseline are unclear and methodologically flawed. The proposed GNN method outperforms this 'oracle' on the Wikipedia node-level detection task (87.0 vs 80.2 F1 score in Table 2, which contradicts the notion of an 'oracle' as a performance upper bound.\n3. **Issues with prompts**: I do not see any differences between RAG baseline and  ORACLE baseline prompts (c.f. Table 4 in the Appendix)\n4. **Evaluation and benchmark are not Realistic**: The benchmark is largely based on synthetically generated inconsistencies using LLMs, which raises concerns about evaluation realism and bias. Since the model is also trained and tested on LLM-generated noise, it may not generalize to naturally occurring inconsistencies in real-world documents. \n5. **Practicality of edge-level bug correction**:  The bug correction task is not practical. Why do the authors assume that there are some other nodes connecting to which the bug can be corrected, rather than correcting it explicitly? For instance, one might need to retrieve\nthe correct citation by understanding the citing text and the right citation may not even be in the reference list. \n\n6. **Other issues**: \n\n    a. The results for Ours (GNN-based) on the Research paper dataset are missing in Table 2.\n\n    b.  \"Table 2 also shows that hybrid retrieval—integrating both text-based and structure-based signals—consistently outperforms methods that rely solely on one or the other.\"- This is not true. On the Research dataset, the hybrid model performs worse than the structure-based method."}, "questions": {"value": "4. Why is there no RAG (Oracle) for the edge-level task in Table 2?\n5. Why should the Oracle prompt be considered an upper-bound?  Your method performs better than the  Oracle in some cases, so it could hardly be considered an upper bound."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YFkZ6oFArG", "forum": "62GAWTZpuw", "replyto": "62GAWTZpuw", "signatures": ["ICLR.cc/2026/Conference/Submission14737/Reviewer_LJUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14737/Reviewer_LJUb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088970886, "cdate": 1762088970886, "tmdate": 1762925096230, "mdate": 1762925096230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}