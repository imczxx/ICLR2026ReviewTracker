{"id": "3aBWTYGcaT", "number": 2392, "cdate": 1757071136665, "mdate": 1759898151766, "content": {"title": "Watermarking Diffusion Language Models", "abstract": "We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99\\% true positive rate with minimal quality impact and achieves similar robustness to existing ARLM watermarks, enabling for the first time reliable DLM watermarking.", "tldr": "We design the first watermarking scheme tailored for diffusion language models", "keywords": ["Watermarks", "Diffusion Language Models", "LLM"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25325579c656b96c0434af86e8f29b07611b657d.pdf", "supplementary_material": "/attachment/78141d19154748ba4ebda89e3db810bc3d16f568.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the first watermarking framework tailored for Diffusion Language Models (DLMs), demonstrating its effectiveness through theoretical analysis and comparative experiments. The method achieves high robustness while maintaining text quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments in this paper are comprehensive, covering multiple dimensions of DLMs and comparing them with ARLMs.\n2. The paper provides a formal definition and derivation of watermarking for DLMs, which has significant theoretical implications."}, "weaknesses": {"value": "1. The efficiency of the watermarking method proposed for DLMs is relatively low.\n2. The method in this paper is limited to the red-green vocabulary watermark and does not consider other watermarking schemes."}, "questions": {"value": "1. The baseline in this paper adds watermarks only when the context is fully known, which seems unreasonable, as many tokens in the DLM generation process are masked. Why not use the Unigram or PatternMark methods from the appendix as the baseline?\n2. Since the entropy-based remask strategy performs better, why does the main experiment in the paper use a random-based remask strategy?\n3. The elegant decomposition of \"Expectation Boost\" and \"Predictive Bias\" heavily relies on SumHash and the special case $C ={-1}$. Does this intuitive explanation still hold for more general hash functions (e.g., MinHash) and larger context windows?\n4. The hash operation in this paper uses a global binary green list matrix, which seems very similar to the Unigram approach. Does this imply that without a global vocabulary, it is impossible to accurately recover the context of tokens during detection?\n5. In the ε-parametrization, the KL constraint does not effectively control the quality loss. Are there other more suitable constraints or objective functions that could be considered?\n6. Is there theoretical support for the convergence of the fixed-point iteration? Could it fail to converge under certain conditions? The observation that a single iteration is sufficiently good seems more like an empirical finding rather than a result derived from theory. Does this suggest that the optimization problem is not inherently complex?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GNY4XpgYLE", "forum": "3aBWTYGcaT", "replyto": "3aBWTYGcaT", "signatures": ["ICLR.cc/2026/Conference/Submission2392/Reviewer_3nzz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2392/Reviewer_3nzz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760601339421, "cdate": 1760601339421, "tmdate": 1762916217388, "mdate": 1762916217388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of instantiating watermarks for diffusion language models as opposed to autoregressive models.  A watermark is a statistical signal hidden inside text that can be detected by anyone with access to a secret key but is intended to not distort the quality of text, thus remaining undetectable to observers without access to a secret key.  Many schemes have been instantiated for autoregressive models by modifying the sampling procedure of models, with a prominent such scheme being the green list approach, where a hash function looks at the recent context and returns a pseudorandom subset of the vocabulary to upweight in generation.  While this has proven effective in autoregressive models, it is not immediate how to instantiate this scheme with masked diffusion models.  This paper suggests casting watermarking as optimizing a detectability function subject to a KL constraint to ensure maintenance of text quality.  They then instantiate the watermarking scheme with two different hash functions and evaluate their approach empirically, demonstrating success relative to the naive baseline of using the autoregressive approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper identifies a clear problem with the well known greenlist approach to watermarking autoregressive language models when applied to diffusion models and proposes a solution that is empirically effective relative to the naive baseline.  The scale of the experiments is reasonable and the results appear promising and relatively robust."}, "weaknesses": {"value": "One major weakness is the much slower sampling time.  While the authors remark in the figure 17 caption that the watermarking overhead is negligible, their empirical results suggest that there is an almost 30% increase in sampling time for their fastest instantiation relative to no watermark, which does not seem negligible to me.  \n\nI also think that the paper could benefit from further comparison with existing baselines.  There exist watermarking approaches that embed the watermark directly in the weights of a model, with *GaussMark: A Practical Approach for Structural Watermarking of Language Models* being an example; it seems like such an approach could directly translate into diffusion models and potentially be competitive as it does not rely on the autoregressive nature of distribution.  Can the authors comment on this?\n\nFinally, I think a number of earlier works, including Synth-ID and the Gaussmark paper cited above suggest that perplexity alone is insufficient to measure text quality.  I wonder if the authors have experimented with more useful proxies for the extent of distortion of their approach?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6b7Jb6SRg4", "forum": "3aBWTYGcaT", "replyto": "3aBWTYGcaT", "signatures": ["ICLR.cc/2026/Conference/Submission2392/Reviewer_mAXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2392/Reviewer_mAXZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283129765, "cdate": 1761283129765, "tmdate": 1762916216440, "mdate": 1762916216440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the first watermarking method for diffusion language models (DLMs) built upon the green–red watermark framework. It argues that watermarking schemes designed for autoregressive models are not directly applicable to DLMs because these methods require computing a hash value from the context tokens (e.g., the previous $k$ tokens). In diffusion language models, however, parts of the context are often masked—that is, the corresponding tokens have not yet been sampled—making such hash-based watermarks infeasible.\n\nTo address this limitation, the paper proposes injecting watermark signals at every diffusion step, operating directly on the distribution of context tokens rather than on specific sampled tokens. By leveraging the hash distribution derived from the model’s probabilistic context, the method adjusts the token probability distribution to embed the watermark. Experimental results show that this approach achieves higher detectability and greater robustness compared to baseline methods that apply watermarks only when all context tokens are fully sampled."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first work to investigate watermarking for diffusion language models.\n2. The paper formulates watermark injection as a constrained optimization problem, offering a principled framework for embedding watermarks during the diffusion process. \n3. The theoretical analysis is well-grounded, providing clear mathematical justification for the proposed method."}, "weaknesses": {"value": "1. The empirical comparison is not fair. The paper argues that the primary limitation of applying autoregressive watermarking methods to diffusion language models lies in the presence of masked context tokens, which prevent computing context-dependent hashes. Therefore, the most appropriate baselines for comparison should be Unigram and PatternMark, where the green–red list is determined independently of the context, and thus do not suffer from the limitation identified by the authors. For a fair evaluation and to substantiate the claimed limitation of autoregressive watermarking, the authors should compare their method against Unigram and PatternMark as the main baselines. Both methods can be straightforwardly adapted to diffusion language models.\n\n2. The additional benefit of the proposed algorithm appears marginal compared to the correct baselines. In Figure 5 of the Appendix, the authors compare their method with Unigram and PatternMark, claiming that “Our Watermark is More Performant than Prior Order-Agnostic Watermarks.” However, from the figure, when the TPR reaches $1$, the log perplexity of the proposed method is around $1.75$, while Unigram achieves approximately $1.85$. Considering that Unigram is also faster ($8.19$ s vs. $11.52$ s, as shown in Figure 17), the roughly 0.1 improvement in log perplexity may not be significant enough to justify the added complexity. In other words, the proposed approach does not resolve any limitation that Unigram-based methods do not already handle.\n\n3. The proposed method incurs significantly higher computational cost compared to the baselines. In each diffusion step and for every token position, the method requires solving a constrained optimization problem. Even with the introduced approximations, the time complexity per token per diffusion step remains high—on the order of ($O(|\\Sigma|\\log|\\Sigma|)$).\n\n4. In Equation (2), I think the derivation appears to hold only under the assumption that:\n$q(H_t(\\omega) = h, \\omega_t = v) \\approx q_t[v], h_t(q)[h].$ Under this assumption, the hash value at position is independent of the current token (\\omega_t). This implies that one cannot arbitrarily choose the hash function; for instance, self-hash, where the hash depends directly on the current token, would violate this assumption. The paper should clearly state this assumption and discuss under what conditions it holds and how it affects the validity of the proposed framework.\n\n5. Some parts of the paper are difficult to follow. For example:\n- In Section 2, ($\\omega$) is defined as the text sequence. However, in Equation (5), the hash value is computed as the sum of ($\\omega_{t+i}$). It is unclear what this summation means until Equation (32), where it becomes evident that ($\\omega_{t+i}$) refers to the token IDs.\n- In Equation (6), the notation SumHash is ambiguous. It appears to correspond to ($H^{\\text{SumHash}}$), but this should be stated explicitly. In addition, the symbol ($s$) (where $(s \\in \\mathcal{H})$) is introduced without explanation, leaving its role in the equation unclear.\n- The explanation of the hash function in Appendix G is much clearer and should be integrated or summarized in the main text for readability.\n- After Equation (32), it is unclear what ($\\mathcal{N}$) represents. I am also confused about the cardinality of the hash space—is ($|\\mathcal{N}|$ or $|\\mathcal{H}|$) intended to be equal to ($|\\Sigma|$)? Please clarify the definition and size of ($\\mathcal{H}$)."}, "questions": {"value": "Please refer to my earlier questions in the weaknesses.\n \nMy main concern is the practical significance of the proposed approach compared to Unigram when applied to diffusion language models (see Weakness 2). \n\nI also noticed that Appendix~B discusses the limitation of Unigram vulnerability to scrubbing attacks. However, it remains unclear to me why the proposed method would be more robust to scrubbing. This is especially questionable given that the proposed algorithm uses a small context size ($|\\mathcal{C}| = 1$ or $2$). Increasing the $|\\mathcal{C}|$  may increase the robustness against scrubbing for the proposed method, but it would further raise computational complexity (which is already high). \n\nIn addition, there is an inherent trade-off between robustness to scrubbing and spoofing, so criticism of the Unigram watermark in its scrubbing vulnerability is not fully convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R28kCq0w4t", "forum": "3aBWTYGcaT", "replyto": "3aBWTYGcaT", "signatures": ["ICLR.cc/2026/Conference/Submission2392/Reviewer_omL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2392/Reviewer_omL7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849949677, "cdate": 1761849949677, "tmdate": 1762916216156, "mdate": 1762916216156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a watermarking framework for Diffusion Language Models. The paper reframes watermarking as a constrained optimization problem: maximizing the expected green token ratio, subject to a KL divergence constraint to preserve text quality. The solution to this problem yields a novel logits-biasing algorithm with two key components: An \"expectation boost,\" which applies the standard Red-Green boost in expectation over the distribution of the unknown (masked) context hashes. A \"predictive bias,\" which promotes sampling tokens that, in turn, are likely to make other tokens green when they are used as context. A significant advantage of this method is that it requires no changes to the standard Red-Green binomial test detector."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength is its novelty. It correctly identifies a fundamental incompatibility between context-dependent hashing and the arbitrary-order generation of DLMs. The reformulation as an optimization problem to maximize the expected green ratio is highly principled. The resulting two-component (expectation boost and predictive bias) algorithm is also clever and intuitive."}, "weaknesses": {"value": "The main paper's comparison relies heavily on a \"naive\" baseline, which adapts KGW by only watermarking tokens where the full context is already unmasked . The most appropriate and direct baseline for a DLM is Unigram, which can be applied directly without any modification. All key comparisons in the main paper (especially for quality vs. TPR in Figure 2 and robustness in Figure 3) should be benchmarked against Unigram."}, "questions": {"value": "- Algorithm 1 is theoretically complex. The current analysis of O(nL | C || Σ | log | Σ | ) seems to ignore gradient computation. \n- The runtime analysis (Figure 17) is critical to understanding this trade-off between cost and performance, but is relegated to the appendix. It should be in the main paper. Furthermore, the reported results in Figure 17 are questionable and lack error bars. For instance, the KGW ($|\\mathcal{C}|=1$) is slower than KGW($|\\mathcal{C}|=2$), which is counter-intuitive.\n- The paper is unclear on how the $\\epsilon$-parameterization (solving Eq. 4) is practically implemented.  Theorem 3.1 guarantees a unique $\\delta_t$ , and Section 3.2 briefly suggests using bisection to find it, but this seems computationally intensive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KxRrdax8yZ", "forum": "3aBWTYGcaT", "replyto": "3aBWTYGcaT", "signatures": ["ICLR.cc/2026/Conference/Submission2392/Reviewer_d3D1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2392/Reviewer_d3D1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927658311, "cdate": 1761927658311, "tmdate": 1762916216028, "mdate": 1762916216028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}