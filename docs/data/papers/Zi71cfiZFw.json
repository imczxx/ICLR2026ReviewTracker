{"id": "Zi71cfiZFw", "number": 9743, "cdate": 1758137368900, "mdate": 1763764663976, "content": {"title": "Less is More: Contrastive Retrieval Heads Improve Attention-Based Re-Ranking", "abstract": "The strong zero-shot and long-context capabilities of recent Large Language Models (LLMs) have paved the way for highly effective re-ranking systems. Attention-based re-rankers leverage attention weights from transformer heads to produce relevance scores, but not all heads are created equally: many contribute noise and redundancy, thus limiting performance. To address this, we introduce $\\textit{CoRe heads}$, a small set of retrieval heads identified via a contrastive scoring metric that explicitly rewards high attention heads that correlate with relevant documents, while downplaying nodes with higher attention that correlate with irrelevant documents. This relative ranking criterion isolates the most discriminative heads for re-ranking and yields a state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs show that aggregated signals from CoRe heads, constituting less than 1% of all heads, substantially improve re-ranking accuracy over strong baselines. We further find that CoRe heads are concentrated in middle layers, and pruning the computation of final 50% of model layers preserves accuracy while significantly reducing inference time and memory usage.", "tldr": "We introduce contrastive retrieval heads that improves list-wise re-ranker and allows layer pruning to reduce inference latency and memory usage without sacrificing accuracy.", "keywords": ["Large Language Models", "Information Retrieval", "Reranking"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff127f7d3e3eaf6ba4ad36f7b03b22e2d07372bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of building effective attention-based re-ranking systems. The paper proposes CoRe (Contrastive Retrieval Heads), which extends previous attention-based re-rankers by 1) using hard negative data for identifying important heads. introducing a contrastive head selection criterion and 2) CoRe explicitly re-normalizing attention to relevant versus irrelevant documents. The paper evaluates the method on the BEIR benchmark and the multilingual MLDR dataset across several open-weight models, showing some improvements over ICR and QRHead baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper also considers  a simple yet effective layer-pruning strategy, showing that re-ranking accuracy can be preserved while cutting memory usage by ~40% and latency by ~20%.\n\nThe proposed extension is straightforward and seems to be effective.\n\nEvaluation covers two re-ranking datasets (BEIR, MLDR) and four open-source LLMs.\n\nThe paper is mostly well presented as easy to understand."}, "weaknesses": {"value": "I have some concerns regarding the framing of the core idea of CoRE. The paper suggests that QRHead’s absolute scoring fails to capture relative ranking among documents (Ln 146). As attention scores already result from a softmax normalization, they implicitly encode relative weighting. It seems the CoRe is renormalizing at the document level to ignore other parts like instructions, rather than introducing new contrastiveness.\n\nThe current evaluation focuses primarily on BEIR-style single-hop retrieval. Past work (e.g., ICR, QRHead) also explored multi-hop reasoning tasks such as MuSiQue and CLIPPER.\n\nThe paper could include more discussion regarding context length. One main advantage of attention-based re-rankers over generative methods is the potential to scale to long context settings. The experiment mainly considers reranking 40 documents. It would be good to include or discuss performance and scaling behavior under a larger number of documents. \n\nThe paper needs more analysis on intrinsic differences of CoRE heads and QRHeads. Like reporting overlapping between top-32, 64 heads."}, "questions": {"value": "How does CoRe generalize to multi-hop reasoning?\n\nHow does CoRE generalize to longer contexts?\n\nWhat’s the overlap between CoRE heads and QRHeads among top-32, 64 heads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BNkswpA0oM", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Reviewer_3Gtp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Reviewer_3Gtp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506055690, "cdate": 1761506055690, "tmdate": 1762921239369, "mdate": 1762921239369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for taking the time to review our paper and providing constructive comments and suggestions. Below we provide clarification to address common concerns from the reviewers.\n\n## Clarification on the methodology.\nReviewers ivKj and 3Gtp raised similar concerns regarding (1) whether CoRe normalizes the document-side attention scores while ignoring the instruction, and (2) whether CoRe’s contrastive scoring is effectively equivalent to the softmax, which already encodes relative weighting. We believe these concerns stem from our original phrasing, specifically the statement on line 183 in the original manuscript that certain tokens were \"ignored in attention score computation.'' We have revised the wording and updated the mathematical equations (still equivalent to previous version) to avoid this misunderstanding. We sincerely apologize for the lack of clarity and address both reviewers’ concerns below.\n\n* **Clarification on how instruction tokens are treated.** All methods (ICR, QR, and CoRe) rely on the *same* token-level attention scores produced by the model’s native softmax, which already normalizes over the *entire* context, including the instruction. The instruction tokens are excluded *only* during the aggregation step that constructs *document-level* scores, since the instruction does not belong to any document. As a result, QR and CoRe begin from identical document-level attention scores.\n\n* **Clarification on the role of softmax vs. CoRe’s contrastive scoring.**  The model’s internal softmax normalizes attention on a *token-level*, whereas CoRe introduces a contrastive scoring metric that operates on a *document-level*. Because instruction tokens are removed during document-level aggregation, the aggregated document scores are not necessarily normalized. This means a head may appear “good’’ under QR’s absolute scoring simply because it assigns a large score to the positive document, even if it assigns an *even larger* score to semantically similar hard negatives.\n    \n    CoRe explicitly incorporates hard negatives and applies a contrastive objective inspired by InfoNCE to penalize heads that attend strongly to misleading negatives. This yields a genuinely *relative* and discriminative assessment of a head’s selectivity, fundamentally different from QR’s absolute scoring, and explains CoRe’s improved robustness."}}, "id": "E6a4gHlQcO", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684180635, "cdate": 1763684180635, "tmdate": 1763684180635, "mdate": 1763684180635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoRe heads (Contrastive Retrieval heads), which is a subset of attention heads for document re-ranking. The key difference from previous work, such as QRHead, is the contrastive objective to find the retrieval heads. The CoRe heads are then used for document re-ranking, where the authors show improvement over existing approaches on the BEIR benchmark. The experiments include models from Llama, Phi, Mistral, and Granite model families. CoRe also uses fewer heads than other methods to achieve comparable or better performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes adjustments to existing approaches to detect existing approaches and achieve better performance on document re-ranking tasks. The experiments also covers multi-lingual settings that were not previously studied. The analysis also covers important system considerations such as the percentage of layers that can be pruned without affecting performance."}, "weaknesses": {"value": "The contrastive scoring is mathematically similar to the objectives used in previous work. Specifically, the main baseline QR-Head uses post-softmax attention scores over the positive document. As a result, it’s achieves the same mathematical form as the paper’s retrieval score in equation 5 because the QRScore is normalized with respect to the attention mass assigned to the rest of the context as well. The only difference is that (1) CoRe scoring excludes the instruction from the overall attention mass, which is not carefully studied (would QR-Head perform the same if the instruction mass were excluded) and (2) CoRe does an additional softmax and tunes the temperature hyper parameters (the temperature in the QR-Head should also be tuned in a similar fashion to achieve fair comparison).\nThe paper could benefit from such theoretical analysis on how the mathematical form differ from previous works and where the improvements stem from. \n\nFurthermore, while the results and analysis are strong, I find the novelty lacking as it only makes minor changes from previous works."}, "questions": {"value": "How do the results scale with more heads beyond 8 heads?\n\nHow do the final results different with different temperature hyper parameters? \n\nWhy do CoRe-R work particularly well for Mistral but the results on the other models are much closer with QR-R?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MYnY9YqcOH", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Reviewer_ivKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Reviewer_ivKj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953481437, "cdate": 1761953481437, "tmdate": 1762921239050, "mdate": 1762921239050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## Summary of additional experiments.\nWe thank the reviewers for their constructive suggestions. In response, we conducted an extensive set of new experiments, and the resulting findings further strengthen and reinforce the contributions of the work.\n* **Performance at long context (Reviewer 3Gtp).** We extended the reranking experiments to top-80 and top-100 on BEIR using Mistral 7B and Llama-3.1 8B (results reported in Figure 4 and Appendix C.6). Relative to the top-40 evaluations, CoRe-R continues to achieve the highest nDCG@10 scores and shows *even larger* gains over the baselines across all datasets, further widening the performance gap. The intuition is that with long context, the candidate set inevitably contains more semantically similar hard negatives. CoRe’s contrastive scoring explicitly penalizes non-selective heads by comparing positives against their associated negatives, making it robust to noise and dilution effects that arise when attention mass is spread across long contexts. Therefore, CoRe is particularly effective in long context settings where hard negatives become more prevalent and the ranking task is inherently more challenging.\n*  **Generalizability to multi-hop tasks (Reviewer 3Gtp).** In addition to the multi-hop dataset HotpotQA, we ran experiments on two new multi-hop datasets MuSiQue and CLIPPER (results reported in Figure 3 and Appendix C.5). On the new multi-hop tasks, CoRe-R still delivers the best reranking performance compared to the baselines, indicating the applicability of CoRe in both single-hop and multi-hop retrieval tasks.\n* **Sensitivity to the detection dataset (Reviewer fEwK).** To evaluate the robustness of CoRe head selection, we repeated the detection procedure using a different dataset, MSMARCO. The resulting CoRe heads are nearly identical to those detected from NQ: for both Mistral and Llama, 7 out of the 8 selected heads overlap, and the one different head comes close within the top10 heads. We then performed the reranking experiments using these MSMARCO-derived heads (results reported in Appendix C.7). With 7 out of 8 heads overlapping, the reranking performance closely matches the results obtained using NQ, demonstrating that CoRe head selection is highly stable and does not depend heavily on the detection dataset.\n* **Optimal number of heads (Reviewers fEwK and ivKj).** We include additional analysis to verify that the optimal number of CoRe heads is not dataset-dependent. We replicated the “number of heads’’ experiment from Figure 2 on three additional datasets using Mistral and Llama (results reported in Appendix C.9). Across all new datasets, we observe the same consistent trend: reranking performance peaks within the first 10 CoRe heads. This stability across diverse datasets shows that CoRe requires no dataset-specific tuning and that a small, fixed number of heads suffices in practice.\n* **Effect of the temperature (Reviewer ivKj).** We performed ablation study on the temperature $t$ (results reported in Appendix C.8). Since $t$ directly controls the sharpness of the contrastive scoring metric, different temperatures can change the *shape* of the score distribution and may affect which heads appear in the top-ranked set. However, despite these shifts in absolute values, we observe that the *most informative CoRe heads remain consistent* across temperatures, and their layer distribution is highly stable. While a small amount of temperature tuning is required, we emphasize that head detection is an extremely efficient, one-time procedure per model: detecting high-quality CoRe heads for a 7B–8B model takes less than an hour on H100 96GB GPU, and this process can be done in parallel for multiple LLMs. Once selected, the same CoRe heads generalize reliably across cross-domain tasks, multi-hop retrieval, and long-context settings. This demonstrates that CoRe’s performance is not sensitive to temperature in practice, and the detected heads remain robust across applications.\n* **Accuracy-efficiency tradeoff via layer pruning (Reviewer ipmp).** We added pruning experiments for the baselines NIAH-R and QR-R (results in Appendix C.4). Although the specific heads selected by CoRe, NIAH, and QR have very little overlap (see Appendix B.1), both NIAH and QR rely on a substantial number of mid-layer heads. Consequently, their performance remains stable under light pruning (30–40\\%).  However, both baselines begin to degrade significantly once pruning exceeds 40\\%, whereas CoRe-R maintains near-identical accuracy up to 50\\% pruning. This difference is intuitive: all CoRe heads lie in early– to mid–layers, while NIAH and QR additionally rely on several late-layer heads (layer 20 and beyond), which are removed earlier in the pruning schedule. Overall, CoRe-R not only achieves the highest reranking accuracy but also demonstrates the strongest accuracy-efficiency tradeoff, benefiting directly from the structurally favorable distribution of its selected heads."}}, "id": "R5KUdlFDoG", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684262678, "cdate": 1763684262678, "tmdate": 1763684262678, "mdate": 1763684262678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## Summary of additional experiments.\nWe thank the reviewers for their constructive suggestions. In response, we conducted an extensive set of new experiments, and the resulting findings further strengthen and reinforce the contributions of the work.\n* **Performance at long context (Reviewer 3Gtp).** We extended the reranking experiments to top-80 and top-100 on BEIR using Mistral 7B and Llama-3.1 8B (results reported in Figure 4 and Appendix C.6). Relative to the top-40 evaluations, CoRe-R continues to achieve the highest nDCG@10 scores and shows *even larger* gains over the baselines across all datasets, further widening the performance gap. The intuition is that with long context, the candidate set inevitably contains more semantically similar hard negatives. CoRe’s contrastive scoring explicitly penalizes non-selective heads by comparing positives against their associated negatives, making it robust to noise and dilution effects that arise when attention mass is spread across long contexts. Therefore, CoRe is particularly effective in long context settings where hard negatives become more prevalent and the ranking task is inherently more challenging.\n*  **Generalizability to multi-hop tasks (Reviewer 3Gtp).** In addition to the multi-hop dataset HotpotQA, we ran experiments on two new multi-hop datasets MuSiQue and CLIPPER (results reported in Figure 3 and Appendix C.5). On the new multi-hop tasks, CoRe-R still delivers the best reranking performance compared to the baselines, indicating the applicability of CoRe in both single-hop and multi-hop retrieval tasks.\n* **Sensitivity to the detection dataset (Reviewer fEwK).** To evaluate the robustness of CoRe head selection, we repeated the detection procedure using a different dataset, MSMARCO. The resulting CoRe heads are nearly identical to those detected from NQ: for both Mistral and Llama, 7 out of the 8 selected heads overlap, and the one different head comes close within the top10 heads. We then performed the reranking experiments using these MSMARCO-derived heads (results reported in Appendix C.7). With 7 out of 8 heads overlapping, the reranking performance closely matches the results obtained using NQ, demonstrating that CoRe head selection is highly stable and does not depend heavily on the detection dataset.\n* **Optimal number of heads (Reviewers fEwK and ivKj).** We provide additional analysis to examine how the optimal number of CoRe heads varies across datasets. We replicated the “number of heads’’ experiment from Figure 2 on three additional datasets using Mistral and Llama (results reported in Appendix C.9). Across all new datasets, we observe the same consistent trend: reranking performance peaks within the first 10 CoRe heads. This demonstrates that CoRe does not require dataset-specific tuning of the head count and that a small, fixed number of heads suffices in practice.\n* **Effect of the temperature (Reviewer ivKj).** We performed ablation study on the temperature $t$ (results reported in Appendix C.8). Since $t$ directly controls the sharpness of the contrastive scoring metric, different temperatures can change the *shape* of the score distribution and may affect which heads appear in the top-ranked set. However, despite these shifts in absolute values, we observe that the *most informative CoRe heads remain consistent* across temperatures, and their layer distribution is highly stable. While a small amount of temperature tuning is required, we emphasize that head detection is an extremely efficient, one-time procedure per model: detecting high-quality CoRe heads for a 7B–8B model takes less than an hour on H100 96GB GPU, and this process can be done in parallel for multiple LLMs. Once selected, the same CoRe heads generalize reliably across cross-domain tasks, multi-hop retrieval, and long-context settings. This demonstrates that CoRe’s performance is not sensitive to temperature in practice, and the detected heads remain robust across applications.\n* **Accuracy-efficiency tradeoff via layer pruning (Reviewer ipmp).** We added pruning experiments for the baselines NIAH-R and QR-R (results in Appendix C.4). We observe that both NIAH and QR rely on a substantial number of mid-layer heads. Consequently, their performance remains stable under light pruning (30–40\\%).  However, both baselines begin to degrade significantly once pruning exceeds 40\\%, whereas CoRe-R maintains near-identical accuracy up to 50\\% pruning. This difference is intuitive: all CoRe heads lie in early– to mid–layers, while NIAH and QR additionally rely on several late-layer heads (layer 20 and beyond), which are removed earlier in the pruning schedule. Overall, CoRe-R not only achieves the highest reranking accuracy but also demonstrates the strongest accuracy-efficiency tradeoff, benefiting directly from the structurally favorable distribution of its selected heads."}}, "id": "R5KUdlFDoG", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684262678, "cdate": 1763684262678, "tmdate": 1763691463377, "mdate": 1763691463377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## Summary of additional experiments.\nWe thank the reviewers for their constructive suggestions. In response, we conducted an extensive set of new experiments, and the resulting findings further strengthen and reinforce the contributions of the work.\n* **Performance at long context (Reviewer 3Gtp).** We extended the reranking experiments to top-80 and top-100 on BEIR using Mistral 7B and Llama-3.1 8B (results reported in Figure 4 and Appendix C.6). Relative to the top-40 evaluations, CoRe-R continues to achieve the highest nDCG@10 scores and shows *even larger* gains over the baselines across all datasets, further widening the performance gap. The intuition is that with long context, the candidate set inevitably contains more semantically similar hard negatives. CoRe’s contrastive scoring explicitly penalizes non-selective heads by comparing positives against their associated negatives, making it robust to noise and dilution effects that arise when attention mass is spread across long contexts. Therefore, CoRe is particularly effective in long context settings where hard negatives become more prevalent and the ranking task is inherently more challenging.\n*  **Generalizability to multi-hop tasks (Reviewer 3Gtp).** In addition to the multi-hop dataset HotpotQA, we ran experiments on two new multi-hop datasets MuSiQue and CLIPPER (results reported in Figure 3 and Appendix C.5). On the new multi-hop tasks, CoRe-R still delivers the best reranking performance compared to the baselines, indicating the applicability of CoRe in both single-hop and multi-hop retrieval tasks.\n* **Sensitivity to the detection dataset (Reviewer fEwK).** To evaluate the robustness of CoRe head selection, we repeated the detection procedure using a different dataset, MSMARCO. The resulting CoRe heads are nearly identical to those detected from NQ: for both Mistral and Llama, 7 out of the 8 selected heads overlap, and the one different head comes close within the top10 heads. We then performed the reranking experiments using these MSMARCO-derived heads (results reported in Appendix C.7). With 7 out of 8 heads overlapping, the reranking performance closely matches the results obtained using NQ, demonstrating that CoRe head selection is highly stable and does not depend heavily on the detection dataset.\n* **Optimal number of heads (Reviewers fEwK and ivKj).** We provide additional analysis to examine how the optimal number of CoRe heads varies across datasets. We replicated the “number of heads’’ experiment from Figure 2 on three additional datasets using Mistral and Llama (results reported in Appendix C.8). Across all new datasets, we observe the same consistent trend: reranking performance peaks within the first 10 CoRe heads. This demonstrates that CoRe does not require dataset-specific tuning of the head count and that a small, fixed number of heads suffices in practice.\n* **Effect of the temperature (Reviewer ivKj).** We performed ablation study on the temperature $t$ (results reported in Section 5.5). Since $t$ directly controls the sharpness of the contrastive scoring metric, different temperatures can change the *shape* of the score distribution and may affect which heads appear in the top-ranked set. However, despite these shifts in absolute values, we observe that the *most informative CoRe heads remain consistent* across temperatures, and their layer distribution is highly stable. While a small amount of temperature tuning is required, we emphasize that head detection is an extremely efficient, one-time procedure per model: detecting high-quality CoRe heads for a 7B–8B model takes less than an hour on H100 96GB GPU, and this process can be done in parallel for multiple LLMs. Once selected, the same CoRe heads generalize reliably across cross-domain tasks, multi-hop retrieval, and long-context settings. This demonstrates that CoRe’s performance is not sensitive to temperature in practice, and the detected heads remain robust across applications.\n* **Accuracy-efficiency tradeoff via layer pruning (Reviewer ipmp).** We added pruning experiments for the baselines NIAH-R and QR-R (results in Appendix C.4). We observe that both NIAH and QR rely on a substantial number of mid-layer heads. Consequently, their performance remains stable under light pruning (30–40\\%).  However, both baselines begin to degrade significantly once pruning exceeds 40\\%, whereas CoRe-R maintains near-identical accuracy up to 50\\% pruning. This difference is intuitive: all CoRe heads lie in early– to mid–layers, while NIAH and QR additionally rely on several late-layer heads (layer 20 and beyond), which are removed earlier in the pruning schedule. Overall, CoRe-R not only achieves the highest reranking accuracy but also demonstrates the strongest accuracy-efficiency tradeoff, benefiting directly from the structurally favorable distribution of its selected heads."}}, "id": "R5KUdlFDoG", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684262678, "cdate": 1763684262678, "tmdate": 1763764181178, "mdate": 1763764181178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose to leverage a small subset of attention heads in LLMs to specialize in discriminating relevant from irrelevant documents for re-ranking tasks. Building on prior attention-based re-rankers and query-focused retrieval heads, they introduce a contrastive scoring metric inspired by InfoNCE loss. The experiments are conducted on the BEIR and MLDR benchmarks when the experimental results show improvements against the ICR baseline. Analysis also shows the approach can enable pruning of 50% of final layers with negligible accuracy loss, reducing inference latency by 20% and GPU memory by 40%, for the middle layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Consistent improvements against the baseline method over diverse benchmarks \n* By using only a tiny fraction of heads and enabling middle-layer focus for pruning, it significantly reduces latency, memory usage, and overhead compared to full-model aggregation, making it practical for real-world systems.\n* The proposed approach reveals that discriminative retrieval signals are concentrated in middle layers, advancing interpretability of LLM attention and opening avenues for optimized architectures."}, "weaknesses": {"value": "* The proposed approach highly relies on hard negatives from NQ, so the data quality could be sensitive. \n* Hyperparameter tuning is required, and it adds the overhead and reduce the ease of use across new LLMs.\n* The proposed method fails to beat baselines on certain datasets."}, "questions": {"value": "* Could the optimal number of CoRe heads be different models and dataset? How sensitive is it?\n* What specific criteria or thresholds are used to discard passages as false negatives during hard negative data construction?\n* Can CoRe heads be dynamically updated or adapted for new datasets without re-running the entire detection process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IHzxBOUtxf", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Reviewer_fEwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Reviewer_fEwK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993161604, "cdate": 1761993161604, "tmdate": 1762921238554, "mdate": 1762921238554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new attention-based re-ranker method that utilizes the attention weights of LLMs to perform re-ranking of documents. Built upon prior work on attention-based re-ranker methods, the paper introduces a novel approach to detect attention heads in LLMs that are important for re-ranking, termed as contrastive retrieval heads, by computing contrastive retrieval scores over positive and negative documents in the input: such heads attend to the irrelevant documents and are not distracted by the irrelevant documents. Empirical results show that re-ranking with contrastive heads outperforms other attention-based re-ranking baselines. Furthermore, such a method can be combined with layer pruning to improve the run-time efficiency as the detected contrastive heads concentrate in the middle layers of transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new re-ranking method that requires no training and demonstrates empirical results across multiple benchmarks with multiple LLMs.\n\n2. The paper compares with several attention-based re-ranking baselines and shows strength over such baselines using the proposed method to detect important attention heads for re-ranking.\n\n3. The paper further shows the strength of the method in terms of efficiency compared with baselines."}, "weaknesses": {"value": "1. The performance of the contrastive retrieval head re-ranker is not particularly strong compared with the attention-based re-ranker baselines. In Table 1, the improvements of CoRe-R over ICR and QR-R are within around 1% for most subsets of the BEIR benchmark, and only for a single subset, Quora, can a relatively large improvement be observed. Similar results can be observed in Table 2 for the MLDR benchmark as well.\n\n2. Related to question 1, there lacks an in-depth analysis of why CoRe-R outperforms the baselines only on certain tasks. Are there any insights or hypotheses on why CoRe-R performs particularly better than the baselines on the Quora subset of BEIR? What are some implications of such findings?\n\n3. There lacks analysis of the similarities and differences between the heads detected by CoRe and by previous methods, especially NIAH and QR. Given the similar performance as mentioned in question 1, do they detect very similar attention heads? Does CoRe detect some very important missing heads that the other methods fail to detect?\n\n4. There are missing baselines for the layer pruning experiments. Since the retrieval heads (i.e., NIAH-R) in Wu et al., 2024 also concentrate in the middle layers, I do not clearly see how layer pruning can only be applied to CoRe-R to improve efficiency, but not other attention-based re-rankers that use a sparse set of attention heads. Thus, the following baselines should be added for fairer evaluation of the efficiency benefit of the proposed method: QR-R (prune) and NIAH-R (prune)."}, "questions": {"value": "1. Why is contextual calibration only used for the re-ranking process, but not the head detection process of CoRe?\n\n2. Figure 4: The head distribution in Mistral 7B is very different from those in the other models. Do you have any insights into this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BPQzWuUPO2", "forum": "Zi71cfiZFw", "replyto": "Zi71cfiZFw", "signatures": ["ICLR.cc/2026/Conference/Submission9743/Reviewer_ipmp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9743/Reviewer_ipmp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036535501, "cdate": 1762036535501, "tmdate": 1762921238037, "mdate": 1762921238037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}