{"id": "QWYNKkg8mN", "number": 11704, "cdate": 1758203226132, "mdate": 1759897559772, "content": {"title": "GRAFTa: A Graph Attentional Few-Shot Audio Tagging Network", "abstract": "Audio tagging models based on CNNs and Transformers achieve strong results, but they rely on large labeled datasets and heavy computation, and struggle with the irregular structure of spectrograms. We propose GRAFTa, a Graph Attentional Few-shot Audio Tagging framework that represents spectrogram regions as nodes in a dynamically constructed graph, enabling attention to capture both local and long-range acoustic dependencies. To support multi-label generalization from few examples, GRAFTa constructs compositional subset prototypes: embeddings for both individual labels and frequently co-occurring label subsets mined from the support set. We evaluate GRAFTa in episodic few-shot settings on FSD50K and AudioSet-Balanced, achieving 0.2296 and 0.2221 mean average precision (mAP), respectively. To our knowledge, this is the first work to establish benchmarks for few-shot multi-label audio tagging on these datasets, highlighting both the promise of graph-based compositional prototypes and the unique challenges of this setting.", "tldr": "We introduce GRAFTa, a graph-based few-shot model for multi-label audio tagging, benchmark it on FSD50K and AudioSet, and show insights into label co-occurrence and shot scaling.", "keywords": ["few-shot learning", "multi-label classification", "audio tagging", "meta learning", "episodic training", "prototype learning", "graph neural networks", "graph attention networks", "compositional representations", "benchmark datasets", "subset prototypes"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9e9f72fdfeade1f1d4787f16949a49afb389115.pdf", "supplementary_material": "/attachment/8425746cf05439c3c02c71d13515674a77f838d7.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes GRAFTa, a few-shot multi-label audio tagging framework that (i) builds a graph over spectrogram time–frequency regions, processes it with a GAT block, and (ii) performs prototype matching not only for single labels but also for compositional subset prototypes (frequently co-occurring label sets mined from the episode’s support set, up to size 3). The model uses a CNN14/PANNs backbone for per-frame features, constructs a sparse k-NN graph (top-k=5) with multi-head GAT (H=8), and aggregates node embeddings to compare against class/subset prototypes via cosine similarity with temperature. Class scores are aggregated from subset scores with size-weighted averaging and trained using Focal Loss. Evaluation follows episodic 5-way 5-shot protocols on FSD50K and AudioSet-Balanced, reporting mAP; the authors also present ablations on singleton vs. subset prototypes and shot count, and a small compute analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Originality: Introduces compositional subset prototypes in a multi-label few-shot setting; integrates GAT over spectrogram nodes to address non-local dependencies. \n* Quality (technical): The pipeline looks coherent; subset-to-class aggregation and temperature-scaled cosine scoring are well-specified; focal loss is a reasonable choice for imbalance.  \n* Significance (potential): Opens a few-shot multi-label benchmark track for audio; subset prototypes could inspire follow-ups on label-dependency modeling and calibration."}, "weaknesses": {"value": "1. Method section is not perfectly written. Many notation used are not explained or defined. For example, what $F$ in line 108. This makes readabilty poor.   In general this reviewer feel that there is a large room for presentation improvement. \n2. Lack of statistical rigor: No CIs, standard deviations, or multi-seed runs; difficult to judge stability or significance. \n3. Evaluation design & metrics: Over-reliance on averaged mAP; limited per-class analysis, no label-cardinality breakdowns, and calibration metrics are missing (e.g., AUPRC per class, macro vs. micro, expected calibration error). Shot-count table suggests metric artifacts that are not investigated. \n4. Baselines limited: No adapted strong audio encoders (e.g., AST/HTS-AT/CLAP-like features with ProtoNet or matching networks) under identical episodic protocols; graph-free but label-dependency models (e.g., sigmoid-CRF, classifier chains, bipartite attention over labels) are not compared."}, "questions": {"value": "1. You state pruning “first 500 subsets” after sorting, but later “retain top K=80 most frequent subsets.” Which is used in the reported results? How sensitive is mAP to K? Please provide a sweep (e.g., 32/64/128/256/512).\n2. Did you tune per-class thresholds or use any calibration (e.g., Platt/temperature scaling) across episodes? Over-prediction grows with shot count; could a class-wise threshold or subset-wise sparsity penalty address this?\n3. Have you compared subset prototypes to label-graph CRF or attention over the label set (without exponential subset growth)? This could control the combinatorial explosion while retaining dependency structure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1aIKLhitWl", "forum": "QWYNKkg8mN", "replyto": "QWYNKkg8mN", "signatures": ["ICLR.cc/2026/Conference/Submission11704/Reviewer_Ko7d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11704/Reviewer_Ko7d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761430489439, "cdate": 1761430489439, "tmdate": 1762922751863, "mdate": 1762922751863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a model for few-shot audio tagging based on a graph attention network. Experiments are carried out on the FSD50k and AudioSet-balanced datasets, and comparisons are made with a CNN baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is overall clear, well written and well structured.\n* The motivation behind the proposed method is timely, and the overall methodology including data preparation and model architecture is technically correct."}, "weaknesses": {"value": "* There are relevant works on using graph neural networks for audio classification which have not been cited nor compared against. Similarly there are works on few-shot audio classification/tagging which have also not been cited nor compared against.\n* Citations in the main paper are not appropriately presented and formatted - please take care of using \\cite vs. \\citep.\n* Before presenting section 4, a section on the data used would have been useful to have (which is subsection 5.1)."}, "questions": {"value": "* Figure 1 is quite generic, does not include axes labels, does not include any information on the sounds included in the spectrograms, and does not contribute to the main contribution of this work. It can be removed entirely without any loss of information, or it could be revised perhaps to indicate how GNNs could contribute for audio tagging.\n* Citations in the main paper are not appropriately presented and formatted - please take care of using \\cite vs. \\citep.\n* From section 2 there is one relevant paper being cited (Lu et al 2022) however there is no discussion on how is the proposed model different from that previous one (despite the lack of a few-shot setting which is not about the model itself). There are also other relevant works on using GNNs for audio tagging that would need to be cited and compared against, e.g. [i].\n* Section 4 which is about ablation studies mentions some of the specific datasets used in this work, however the datasets have not been presented as of yet - please consider cross-referencing with section 5.1 on data.\n* Section 5.4 mentions that few-shot multilabel audio tagging has not been systematically benchmarked. From a search I was able to find at least two relevant works [ii], [iii], [iv]. I would suggest that any relevant works on multi-label few-shot learning for audio would need to be cited and discussed, and when possible compared against the proposed model.\n\n\n[i] S. Singh, C. J. Steinmetz, E. Benetos, H. Phan, and D. Stowell, “ATGNN: audio tagging graph neural network”, IEEE Signal Processing Letters, vol. 31, pp. 825-829, 2024. \n[ii] K. -H. Cheng, S. -Y. Chou and Y. -H. Yang, \"Multi-label Few-shot Learning for Sound Event Recognition,\" 2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP), Kuala Lumpur, Malaysia, 2019.\n[iii] J. Liang, H. Phan and E. Benetos, \"Learning from Taxonomy: Multi-Label Few-Shot Classification for Everyday Sound Recognition,\" ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 771-775.\n[iv] Y. Wang, N. J. Bryan, M. Cartwright, J. Pablo Bello and J. Salamon, \"Few-Shot Continual Learning for Audio Classification,\" ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Toronto, ON, Canada, 2021, pp. 321-325."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XSv4lzNOAO", "forum": "QWYNKkg8mN", "replyto": "QWYNKkg8mN", "signatures": ["ICLR.cc/2026/Conference/Submission11704/Reviewer_x997"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11704/Reviewer_x997"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754104022, "cdate": 1761754104022, "tmdate": 1762922751552, "mdate": 1762922751552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRAFTa, a Graph Attentional Few-shot Audio Tagging framework that models spectrogram regions as graph nodes to better capture both local and global acoustic dependencies. It further proposes compositional subset prototypes to enhance multi-label generalization from limited examples by representing frequent co-occurring label subsets. Experiments on FSD50K and AudioSet-Balanced demonstrate competitive performance and establish the first benchmarks for few-shot multi-label audio tagging."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Effectively models non-local and irregular spectrogram structures using graph attention."}, "weaknesses": {"value": "1. Its performance can be highly sensitive to the quality of subset mining from limited support examples.\n2. The evaluation is restricted to few-shot settings without comparison to large-scale pretrained or stronger baseline models.\n3. The performance evaluation only reports mAP, and the compared methods are not representative, providing insufficient evidence of superiority.\n4. The overall formatting of the paper is problematic (e.g., layout, figures, and references) and requires careful revision.\n5. The number of cited works is very limited, and many references are outdated, failing to reflect recent progress in few-shot learning and audio tagging.\n\nOverall, I believe the paper’s quality falls significantly below the standards of ICLR."}, "questions": {"value": "Figure 2 has very poor structure and conveys little useful information about the model design or data flow, making it hard to understand or reproduce the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bTFwSEgCUG", "forum": "QWYNKkg8mN", "replyto": "QWYNKkg8mN", "signatures": ["ICLR.cc/2026/Conference/Submission11704/Reviewer_p2Af"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11704/Reviewer_p2Af"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904100456, "cdate": 1761904100456, "tmdate": 1762922751185, "mdate": 1762922751185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present GRAFTa, a graph-based framework for few-shot multilabel\naudio tagging. The model uses a CNN14 backbone to extract per-frame embeddings\nfrom the input spectrograms. Then a graph attention network is used and both\nsingle-class and compositional subset prototypes are utilized."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method achieves decent results on the FSD50K and Audioset datasets\n(Section 6.1). Please also see the paragraph under Weaknesses."}, "weaknesses": {"value": "The proposed method achieves decent results on the FSD50K and Audioset datasets\n(Section 6.1). However, the CNN + ProtoNet comparison method achieves better\nmAP performance. The authors point out that the per node dimension of GRAFTa is\n64, versus 512 for the CMM + ProtoNet method. However, it seems that in both\ncases the resource usage (e.g., GPU memory and training time) is quite limited.\nThe authors should discuss more what the benefits of GRAFTa are for practical\npurposes. Maybe a comparison on mobile devises could be done?"}, "questions": {"value": "CNN + ProtoNet achieve a higher mAP score. What are the benefits of GRAFTa\nbeyond the lower per node dimension?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xxmHvZ4Q0h", "forum": "QWYNKkg8mN", "replyto": "QWYNKkg8mN", "signatures": ["ICLR.cc/2026/Conference/Submission11704/Reviewer_nHBQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11704/Reviewer_nHBQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071140792, "cdate": 1762071140792, "tmdate": 1762922750545, "mdate": 1762922750545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}