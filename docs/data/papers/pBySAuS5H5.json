{"id": "pBySAuS5H5", "number": 15912, "cdate": 1758257038417, "mdate": 1759897273465, "content": {"title": "SpSCO: A Speculative Sampling Approach to Neural Combinatorial Optimization", "abstract": "An open challenge in neural combinatorial optimization (CO), such as using reinforcement learning (RL) and diffusion models (DMs), is the speed–quality trade-off: sequential RL decoders generalize well but tend to settle for suboptimal tours, while DMs generate high-quality full solutions at the cost of long training and slow iterative sampling. We present **SpSCO**, a new framework inspired by speculative sampling (SpS) for large language models (LLMs) inference. Resembling SpS in LLMs, a light-weight draft model (analogous to the sequential RL decoder in SpSCO) collaborates with a high-capacity target model (analogous to a DM in SpSCO) to achieve fast, robust, and high-quality inference -- the target model is triggered only when there is a \"cognitive divergence\" between the draft and target models or internal uncertainty of the draft model. This SpS strategy allows SpSCO to achieve high solution quality while reducing the computational overhead from DMs. Notably, SpSCO is model-agnostic and can be plug-and-play across various RL and DM backbones. It also shows strong robustness: even with under-trained, suboptimal RL and diffusion backbones, SpSCO achieves state-of-the-art performance on diverse CO instances across various scales while attaining faster inference time on large-scale instances.", "tldr": "", "keywords": ["inference", "reinforcement learning", "diffusion model", "combinatorial optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23ff00f0a226e034449f9e57bf66a037acbfc4d6.pdf", "supplementary_material": "/attachment/c41bed399bf4374fc5c52dd798bf8fd5d480ded8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SpSCO, a speculative sampling framework for neural combinatorial optimization (CO). Drawing inspiration from speculative decoding in large language models (LLMs), the authors design an inference pipeline that combines a fast sequential RL “draft model” with a conditional diffusion “target model”.\nAt each decoding step, SpSCO uses an adaptive trigger mechanism based on policy entropy (internal uncertainty) and KL divergence (cognitive disagreement) to decide whether to invoke the diffusion model. When triggered, a dual-track generation strategy corrects the immediate step of the draft model and generates full alternative tours via the target model, from which the best solution is chosen.\n\nThe method is benchmarked on multiple TSP datasets and shows strong performance: near-optimal results on small instances (TSP-100) and improved speed–quality trade-offs on large instances (TSP-500/1000). Ablations highlight the value of the dual-trigger mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel and Well-Motivated Framework:**    \n    The paper presents a creative and conceptually well-grounded adaptation of speculative sampling—originally designed for large language model inference—to the domain of neural combinatorial optimization. The analogy between draft and target models in LLMs and the RL and diffusion components here is intuitive and intellectually appealing. Even though the implementation details need clarification, the underlying idea of using uncertainty-guided coordination between a fast heuristic and a strong generative solver is novel and worth exploring further.\n\n- **Compelling Use of Cognitive Divergence:**   \n    The introduction of cognitive divergence (a KL-based measure between the RL and DM policies) as a lightweight coordination signal is conceptually elegant. Leveraging the diffusion model’s energy-derived prior to quantify disagreement provides a principled and computationally efficient way to trigger corrections, which could inspire broader applications in hybrid model inference.\n\n- **Comprehensive Empirical Evaluation:**  \n    The paper evaluates its framework on a wide range of TSP instances (50, 100, 500, and 1000 nodes) and supplements these with results on the Orienteering Problem (OP-100), demonstrating promising scalability. The reported results achieve near-optimal performance on smaller instances (e.g., a 0.02% optimality gap on TSP-100) and remain competitive on larger problems, indicating practical potential once the methodology is clarified.\n\n- **Model-Agnostic and Extensible Design:**  \n    The proposed framework is compatible with different RL backbones (Attention Model, POMO), and the design principles appear general enough to extend to other combinatorial optimization settings. This plug-and-play property—if validated—would make SpSCO a flexible inference layer for future hybrid neural solvers.\n\n- **Extensive Ablation and Sensitivity Analyses:**  \n    The appendix includes detailed ablations on the entropy/KL trigger mechanism, energy probe timesteps, candidate pool sizes, and cumulative threshold settings. These analyses suggest the approach is reasonably robust to hyperparameter choices and provide valuable insight into how each component contributes to final performance.\n\n- **Potential for Broad Impact if Clarified:**  \n    Despite its current inconsistencies, the paper’s central concept—combining fast RL guidance with diffusion-based refinement under a speculative coordination policy—is innovative and relevant to both the combinatorial optimization and generative modeling communities. With a clearer, internally consistent description and fairer benchmarking, this line of work could represent a meaningful step toward more efficient neural solvers."}, "weaknesses": {"value": "- The paper contains irreconcilable contradictions in describing its core method, making it impossible to understand or implement SpSCO from the paper alone:\n    1. Figure 2 vs. Single-Trigger Design: Figure 2 depicts multiple DM interventions throughout tour construction (corrections at steps 3, 5, and 7), directly contradicting the text's explicit statement of a \"single-trigger design\" where \"once the DM is invoked at step $k^*$, a flag is set, and the RL policy completes the remainder of the hybrid path without further checks\" (lines 139-141). This makes the comparison to LLM speculative sampling misleading, as speculative decoding verifies multiple tokens iteratively, not just once.\n\n    2. Algorithm 1 Logical Errors: The outer ```if```-statement (line 4: ```if tour not complete AND not dm_triggered```) lacks a corresponding ```else``` clause. After ```dm_triggered``` is set to true (line 19), this condition becomes ```false``` for all remaining iterations, yet line 26 attempts to append ```a_next``` and update the state. Without an else clause showing how RL continues after DM invocation, the algorithm appears to terminate prematurely.\n\n    3. Text Contradictions: Section 4.4 describes selecting \"the action from the candidate pool that is most preferred by the DM's prior\" (lines 312-314), suggesting the correction uses the energy-based prior $p_\\phi$. However, Algorithm 1 line 20 suggests extracting the action from complete DM proposals. These are fundamentally different approaches.\n\n    4. Conceptual Framework Mismatch: The paper frames SpSCO as involving a \"propose-verify-correct cycle\" at \"each step\" (lines 128-129), but the single-trigger design means only **one** step receives DM intervention. The entire \"speculative sampling\" analogy from LLMs, where draft tokens are continuously verified, does not actually apply to SpSCO's one-shot intervention strategy.  \n    \n    Required corrections / improvements the authors must provide:  \n        - Consistency in describing the method in the main text, algorithm and figure.  \n        - A corrected Algorithm 1 with complete logic flow after ```dm_triggered = true```.  \n        - A revised Figure 2 showing only one DM intervention   \n        - Consistent description of how the corrected action is selected (prior-based vs. proposal-based).  \n        - Clarification of whether this should even be called \"speculative sampling\" given the single intervention. \n\n\n    These are not minor presentation issues but fundamental communication failures that prevent readers from understanding, evaluating, or reproducing the method. The empirical results may be valid, but without algorithmic clarity, the contribution cannot be properly assessed.\n\n\n\n- The single-trigger design appears ad hoc and conceptually inconsistent with the goal of speculative coordination: it assumes only one “crucial” step in the entire RL rollout requires correction. Table 11 further shows that the diffusion model is almost always triggered extremely early ($k^∗=0.5$), meaning the DM intervenes after the first or second step and effectively constructs the rest of the tour. This raises the question of what purpose the RL component serves if its output is overridden almost immediately. To demonstrate that the hybrid rollout is meaningful, the authors should report winning rates — i.e., how often the final chosen tour comes from the RL-completed hybrid path versus a DM proposal. Without such evidence, the “speculative” RL-DM collaboration appears superficial, and the framework behaves more like a slightly modified diffusion solver than a true hybrid system.\n\n- Benchmark comparisons seem rather unfair. As far as I understand are all models except SpSCO evaluated based on a single, greedy rollout, while SpSCO performs multiple rollouts due to its \"dual track generation\", from which the best solution is selected. To account for that, other models should be allowed to sample multiple rollouts as well. \n\n- Following on from that, Table 10 reveals that the optimality gap decreases monotonically as the cumulative probability threshold, and thus the number of diffusion rollouts, increases, accompanied by higher runtime. This is the standard sample-quality trade-off already observed in probabilistic NCO solvers: more samples yield better tours but slower inference. The purported benefit of SpSCO’s “speculative” design therefore appears to derive mainly from performing additional diffusion rollouts from multiple prefixes, rather than from the proposed entropy/KL trigger or the hybrid inference logic itself. For instance, the DIFUSCO paper reports a 6-percentage-point reduction in optimality gap simply by enabling sampling-based decoding. Therfore, the paper should disentangle improvements purely due to increased sampling from those due to the speculative mechanism and report compute-matched comparisons.\n\n\n- Weak Support for Generality: Experiments are limited to simple routing problems (TSP and Orienteering Problem). Evaluation on more complex routing and/or scheduling problems would better undermine the models generality."}, "questions": {"value": "- please clarify and comment on the issues raised under weaknesses.\n- Can you provide ablation on the single-trigger design vs. allowing multiple triggers?\n- why is the Attention Model slower than SpSCO? In my understanding, this cannot be the case, since the AM only invokes a single, greedy rollout with a similar architecture as the draft model of SpSCO, but without the dual trigger mechanism and DM generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dveAZtonWB", "forum": "pBySAuS5H5", "replyto": "pBySAuS5H5", "signatures": ["ICLR.cc/2026/Conference/Submission15912/Reviewer_sc5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15912/Reviewer_sc5a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298851302, "cdate": 1761298851302, "tmdate": 1762926130882, "mdate": 1762926130882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SpSCO, a speculative sampling framework inspired by large language models (LLMs). The framework utilizes a non-autoregressive reinforcement learning (RL) model, such as AM or POMO, as the draft model to generate initial solutions. When there is a significant discrepancy between the RL policy and the diffusion model (DM) policy, or when the RL policy behaves in a highly stochastic manner, the DM (serving as the target model) is activated to generate an alternative solution. The final output is selected as the better of the two. The proposed method is empirically evaluated on TSP instances of sizes 50, 100, 500, and 1000."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using a fast RL model as a draft generator and a high-quality but slower diffusion model (DM) as compensation is interesting. I agree with the underlying motivation: single-paradigm methods often suffer from inherent limitations. For instance, autoregressive RL models are typically efficient but yield only moderate solution quality, while non-autoregressive diffusion models can achieve superior performance at the cost of prolonged denoising procedures. Combining these two paradigms in a complementary manner is a promising direction for improving the trade-off between solution quality and inference efficiency in neural combinatorial optimization."}, "weaknesses": {"value": "- The influence of different threshold selections should be discussed.\n\n- Due to the reliance on a diffusion model, the proposed method also faces limitations in its applicability to more complex problems. \n\n- The pseudo code is unnecessarily lengthy. Beyond this, the overall presentation requires further improvement for clarity and conciseness.\n\n- The criteria for hyperparameter selection should be explained in Appendix A.4, rather than merely listing their values."}, "questions": {"value": "- How frequently will the DM inference be triggered?\n\n- How does the performance of LKH-3 in Table 2 compare to SpSCO under a shorter time limit?\n\n- Why is 2-opt not used in large-scale TSP in Table 2?\n\n- Why does AM trained on TSP-50/100 use rollout as the baseline, rather than the critic baseline used in the original AM?\n\n- How is the performance on TSPLIB?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ErpzgOXVeo", "forum": "pBySAuS5H5", "replyto": "pBySAuS5H5", "signatures": ["ICLR.cc/2026/Conference/Submission15912/Reviewer_nQiK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15912/Reviewer_nQiK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980298432, "cdate": 1761980298432, "tmdate": 1762926129460, "mdate": 1762926129460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SpSCO, a hybrid framework that combines reinforcement learning (RL) and diffusion models (DMs) using a speculative sampling (SpS) mechanism inspired by recent work in LLM inference acceleration. The RL solver serves as a lightweight draft model, while the DM acts as a high-capacity target model. A dual-signal adaptive trigger, based on (1) the RL policy entropy and (2) the KL divergence between the RL and DM-derived priors, determines when to invoke the DM. When triggered, the DM performs a one-step correction and generates a few complete solutions. A final selection step chooses the best overall tour.\n\nExperiments on standard TSP benchmarks (50–1000) demonstrate strong results: SpSCO achieves near-optimal quality with substantially reduced inference time compared to pure diffusion-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of transferring speculative sampling from LLMs to combinatorial optimization is quite novel and timely.\n\n- The dual-signal trigger (entropy + cognitive divergence) is intuitive and empirically effective.\n\n- Experiments are extensive, and the results on large-scale TSPs show a promising trade-off between speed and quality.\n\n- The method is described as model-agnostic, and the implementation details (especially for Prefix-Difusco) are very thorough, which supports reproducibility."}, "weaknesses": {"value": "- The notion of “cognitive divergence” between RL and DM distributions is attractive, but it feels more metaphorical than grounded. The paper frames it as if the DM provides a “global understanding” of the solution space, but this is not rigorously demonstrated. In practice, the DM prior derived from a single-step denoising probe is only a local statistical snapshot, not an actual global consistency check.\n\n- While TSP results are solid, there’s limited exploration of other CO problems—the single Orienteering example in the appendix is not enough to claim broad generality."}, "questions": {"value": "- How expensive is the energy probing step compared to full DM sampling? what’s the overhead in practice?\n- How does SpSCO perform if both RL and DM are fully trained, rather than under-trained?\n- Could this idea extend to constrained problems like VRP or CVRP, where feasibility must be enforced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hbpsfpCKmH", "forum": "pBySAuS5H5", "replyto": "pBySAuS5H5", "signatures": ["ICLR.cc/2026/Conference/Submission15912/Reviewer_rLv2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15912/Reviewer_rLv2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998292686, "cdate": 1761998292686, "tmdate": 1762926128698, "mdate": 1762926128698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpSCO, a hybrid framework for neural combinatorial optimization that addresses the speed-quality trade-off between fast autoregressive models (trained with RL) and slow, high-quality diffusion models (trained with SL). Inspired by speculative sampling in language generation, SpSCO uses the RL model to draft a solution and adaptively trigger the diffusion model for correction only at a \"critical\" step. This critical step is identified by the entropy of the RL policy or its KL divergence from the diffusion model prior. The proposed method is tested on TSP with various scales."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Adopting the speculative decoding idea to neural combinatorial optimisation is an interesting idea.\n2. Combining autoregressive models and a diffusion-based heatmap generation model is a promising research direction to explore, since they have different pros and cons, as mentioned in this work."}, "weaknesses": {"value": "1. (Clarity) This work has serious concerns in terms of clarity. Most importantly, \"RL model\" is not in the same category as \"diffusion models\" since RL is a learning algorithm while diffusion models are a form of generative model. It should be fixed to \"autoregressive models\", which they used in their experiments. Equation (5) and (6) are hard to understand, and need more explanation, e.g., how $c_i$ and $t_{\\text{probe}}$ are defined or obtained. $p_\\phi$ was defined as a diffusion kernel in equation (2), but used differently in (6). Algorithm 1 seems to have various errors, e.g., once $dm_triggered$, the $a_{next}$ is never sampled again, and hybrid path correction is missing. See my questions for other points regarding the clarity.\n2. (Method) To me, some of the design seems heuristic. For example, the trigger is activated only once, i.e., the proposed speculative decoding is applied only once. Moreover, the diffusion models seem to be assumed to be trained via supervised learning, which limits the \"data-free\" nature of RL-based methods.\n3. (Experiments) The experiments have a limited scope, in that the method is validated only on TSP. While it is claimed to achieve the state-of-the-art performance, it is actually far from it as far as I know; see the following works [1, 2, 3, 4]. No standard deviation or confidence interval is provided. The runtime for Table 1 is also not provided.\n\n---\nReferences \n[1] Luo, Fu, et al. \"Neural combinatorial optimization with heavy decoder: Toward large scale generalization.\" Advances in Neural Information Processing Systems 36 (2023)\n[2] Drakulic, Darko, et al. \"Bq-nco: Bisimulation quotienting for efficient neural combinatorial optimization.\" Advances in Neural Information Processing Systems 36 (2023)\n[3] Kim, Minsu, et al. \"Ant Colony Sampling with GFlowNets for Combinatorial Optimization.\" The 28th International Conference on Artificial Intelligence and Statistics. (2025)\n[4] Kim, Hyeonah, et al. \"Neural Genetic Search in Discrete Spaces.\" Forty-second International Conference on Machine Learning. (2025)"}, "questions": {"value": "1. Line 144: Why are diffusion models better than other models (like autoregressive models) in capturing complex global dependencies?\n2. Line 235: \"the RL policy completes the remainder...\": If I understand correctly, the remainder is completed via both the RL policy and the diffusion model, isn't it?\n3. Line 443: Why is AM slower than SpSCO?\n4. Table 2: Is 2OPT used for the large-scale experiment?\n\n---\n\n### LLM usage disclosure\nI used LLM only to check grammar."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6dnKfG5EN3", "forum": "pBySAuS5H5", "replyto": "pBySAuS5H5", "signatures": ["ICLR.cc/2026/Conference/Submission15912/Reviewer_CJH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15912/Reviewer_CJH2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999489107, "cdate": 1761999489107, "tmdate": 1762926128197, "mdate": 1762926128197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}