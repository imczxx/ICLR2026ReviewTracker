{"id": "OpuPBNcQwe", "number": 24533, "cdate": 1758357710070, "mdate": 1763688671013, "content": {"title": "Enhancing Instruction Following of LLMs via Activation Steering with Dynamic Rejection", "abstract": "Large Language Models (LLMs), despite advances in instruction tuning, often fail to follow complex user instructions. Activation steering techniques aim to mitigate this by manipulating model internals, but have a potential risk of oversteering, where excessive emphasis on the instruction degrades task accuracy and overall text quality. To address this, we introduce DIRECTER (Dynamic rejection steering), a novel steering method that dynamically modulates steering strength by scaling the KV cache without extra dataset. DIRECTER couples steering with a plausibility-guided decoding loop, which adaptively adjusts steering strength at each step by comparing the steered output distribution to the original. If the steered output is deemed implausible, steering strength is progressively weakened. This strength modulation is guided by a lightweight, one-time attention sensitivity analysis that ranks layers by their influence on model representations. Extensive evaluations show that DIRECTER significantly enhances instruction-following capabilities across diverse benchmarks, improving accuracy by up to 6.5% over baselines without the common trade-offs in generation quality or task fidelity. The proposed dynamic, plausibility-guided control during activation steering further demonstrates its potential as a general mechanism for mitigating oversteering that is compatible with existing baselines.", "tldr": "We propose DIRECTER, a dynamic steering method that mitigates oversteering in LLMs by plausibility-guided decoding loop that rejects implausibile outputs and adaptively modulates steering strength.", "keywords": ["Large Language Models", "LLM Steering", "Instruction following", "Activation engineering"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00fec23b5a9cedb12f79eb9889ce5056dfb6b6c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces an attention-based steering mechanism for improved instruction following that maintains output quality and improves performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work is well-written and scoped, and provides solid empirical evidence for their proposed control methodology. DIRECTER avoids the problem of \"over-steering,\" which is present in other steering methods applied broadly to the input. The authors show that their method is successful across different benchmarks, model scales, and can be used successfully in conjunction with many different architectures. Importantly, they show that DIRECTER is efficient and performs well against simpler interventions such as plain prompting."}, "weaknesses": {"value": "* Fixed plausibility threshold, while effective, is applied globally based on a hyperparameter sweep on a subset of the data, but it's unclear in the paper whether this generalizes across models or data domain types. A per-task adaptive value or assessment could help validate the applicability of this parameter.\n\n* The LLM-based evaluation without human validation is a limitation, given the claims of text quality. Reliability of this as a metric should have a subset assessed manually to validate whether the text quality is maintained."}, "questions": {"value": "1. How sensitive is the threshold to different domains or different models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v1kRQXK3qw", "forum": "OpuPBNcQwe", "replyto": "OpuPBNcQwe", "signatures": ["ICLR.cc/2026/Conference/Submission24533/Reviewer_Sn4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24533/Reviewer_Sn4M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489428621, "cdate": 1761489428621, "tmdate": 1762943113903, "mdate": 1762943113903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DIRECTER, a dynamic activation-steering method that improves instruction-following of LLMs by adaptively adjusting during decoding. The main idea is a plausibility-guided decoding loop that compares steered and raw token distributions at each step. If the steered token is deemed implausible, steering strength is reduced by halving the set of ‚Äústeered layers,‚Äù ranked by a precomputed attention sensitivity score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The plausibility-guided decoding loop is conceptually simple yet effective. Its dynamic halving mechanism mimics reinforcement learning‚Äìstyle adaptive control but without extra training.\n2. The method introduces minimal memory overhead and modest throughput reduction (‚âà 16%)\n3. The extensive ablation and robustness analyses (Fig. 2‚Äì3) convincingly demonstrate the benefits of dynamic steering and the effectiveness of layer ranking."}, "weaknesses": {"value": "1. The attention sensitivity metric (Eq. 3‚Äì4) is ad-hoc and lacks theoretical justification. The ‚Äúdirect‚Äù and ‚Äúpropagated‚Äù effects are computed via cosine distance differences, but no intuition or derivation is provided. Why does summing cosine-distance deviations across layers accurately capture ‚Äúinfluence‚Äù?\n2. The method section is hard to follow due to poor narrative flow, cross references and undefined notation. For example, Readers are told what each symbol means after it‚Äôs used ‚Äî e.g., $ùêø_{cand,ùë°}$ appears in the plausibility rule before explaining how it‚Äôs initialized or updated. The paper didn't provides a clear figure or algorithm before diving into formulas. It is better to summarize their method in a pseudocode algorithm. \n3. Eq. (2) assumes top-1 plausibility comparison, but this is sensitive to token-level noise. It could be better if they evaluate with multiple thresholds $\\beta \\in [0.3, 0.5, 0.7]$ to show sensitivity (only $\\beta = 0.5$ is used)."}, "questions": {"value": "* Have you tried applying DIRECTER to other forms of controlled generation (e.g., factuality correction, refusal control)? A small-scale results would demonstrate the method‚Äôs broader applicability and strengthen the paper‚Äôs impact claim that DIRECTER is a ‚Äúgeneral mechanism‚Äù for mitigating oversteering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mNd9ZLSNoy", "forum": "OpuPBNcQwe", "replyto": "OpuPBNcQwe", "signatures": ["ICLR.cc/2026/Conference/Submission24533/Reviewer_CXSA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24533/Reviewer_CXSA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522047268, "cdate": 1761522047268, "tmdate": 1762943113715, "mdate": 1762943113715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DIRECTER , a novel inference-time activation steering method that enhances the instruction-following ability of large language models . Unlike existing static steering methods which risk oversteeringÔºåDIRECTER adaptively adjusts the steering strength at each decoding step through a plausibility-guided decoding loop. It scales the key-value  cache layers dynamically based on attention sensitivity analysis, ensuring plausible outputs while mitigating instruction misalignment. Experiments on IFEval, LIFBench, and GSM8K-Format show consistent improvements (up to +6.5%) in instruction accuracy without sacrificing fluency or reasoning performance. The method is model-agnostic and incurs modest computational overhead (~16% latency)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper accurately identifies the \"oversteering\" problem in existing activation steering techniques. The proposed DIRECTER provides a dynamic, adaptive control mechanism rather than relying on static, manually-tuned hyperparameters.\n- The results strongly demonstrate the method's key advantage: it significantly improves instruction following without sacrificing core task accuracy or text quality.\n- The evaluation is thorough, using diverse benchmarks including IFEval (strict instructions), LIFBench (long context), and the newly-created GSM8K-Format."}, "weaknesses": {"value": "- RQ2 requires  the generalization of DIRECTER across different architectures and model sizes. To make the comparison more complete, Table 2 should also include results of other steering-based methods such as PASTA and SpotLight on more LLM architecture (e.g., Qwen) .\n- The choice of the probability-ratio‚Äìbased plausibility criterion (Eq. 2) is currently mainly empirical. However, commonly used measures for comparing probability distributions include KL divergence or JS divergence. The authors should compare these alternatives to justify Eq. (2). \n- The method proposed by the authors shows some improvements in text quality and task performance. However, from an efficiency perspective, the overhead is prohibitive. In particular, the time required to generate the first token is roughly five times of SpotLight."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QkZr0UvjZ2", "forum": "OpuPBNcQwe", "replyto": "OpuPBNcQwe", "signatures": ["ICLR.cc/2026/Conference/Submission24533/Reviewer_tbf4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24533/Reviewer_tbf4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819953631, "cdate": 1761819953631, "tmdate": 1762943113508, "mdate": 1762943113508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}