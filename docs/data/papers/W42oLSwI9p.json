{"id": "W42oLSwI9p", "number": 19659, "cdate": 1758298056044, "mdate": 1759897027610, "content": {"title": "One-Step Diffusion Solver for Non-binary Integer Linear Programming", "abstract": "Integer linear programming, a fundamental NP-hard problem with broad applications in science and engineering, has gained growing attention in the machine learning community. Yet, progress on effective end-to-end solvers remains limited, largely due to difficulties in enforcing constraints and integrality. Most existing work focuses on binary integer linear programming problems, while generalizing to bounded, non-binary cases often requires transformations that significantly increase problem size and computational costs. Even for purely binary problems, inference time is often prohibitively long, restricting applicability to real-world scenarios. To tackle the aforementioned problems, we propose three one-step diffusion-based approaches, i.e., CMILP, SCMILP and MFILP, inspired by the popular consistency, shortcut and meanflow training techniques. Our methods can further handle non-binary integer problems using a newly proposed iterative integer projection (IIP) layer, eliminating the need for the costly problem transformation. To further improve the solution quality, an objective-guided sampling with momentum scheme is proposed. Experiments demonstrate that our approach outperforms existing learning-based methods on both binary and non-binary instances and shows strong scalability compared to traditional solvers. Source code and detailed protocols will be made publicly available.", "tldr": "", "keywords": ["one-step diffusion model", "integer linear programming"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec29706ce925cd9d50495f7a5da3447829a9c7cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents three one-step diffusion-based solvers—CMILP, SCMILP, and MFILP—for solving mixed-integer linear programming (MILP) problems. Inspired by consistency, shortcut, and meanflow training techniques, these solvers incorporate a novel Iterative Integer Projection (IIP) layer to directly handle non-binary integer constraints, eliminating the need for costly problem transformations. Experimental results show that the proposed approach outperforms existing learning-based methods on both binary and non-binary ILP instances, while demonstrating stronger scalability compared to traditional solvers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  Leveraging diffusion models for MILP solution prediction is a timely and effective choice. Diffusion models offer greater expressive power than graph neural networks (GNNs), enabling more accurate capture of complex solution distributions for ILP problems.\n\n2. The proposed Iterative Integer Projection (IIP) layer addresses the gap in existing literature, which predominantly focuses on binary integer programming. By enabling direct handling of non-binary integer constraints, the method avoids exponential problem size growth associated with binary transformation, enhancing efficiency for general ILP scenarios."}, "weaknesses": {"value": "1. Incomplete baseline comparisons: Key solution prediction baselines are omitted, including Contrastive Predict-and-Search and FMILP. This omission limits the comprehensiveness of the performance evaluation, as these methods are widely recognized in the field of learning-based ILP solvers. The authors may want to refer to the following related works in the paper.\n[1] Apollo-MILP: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming. ICLR, 2025\n[2] FMIP: Joint Continuous-Integer Flow For Mixed-Integer Linear Programming.\n[3] Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion, KDD, 2024.\n\n2. Limited benchmark: The paper does not clearly specify the full scope of the dataset sizes. Additionally, the instances appear relatively easy to solve—evidenced by Gurobi’s ability to find optimal solutions within 100 seconds in Tables 1, 2, and 3. Evaluations on larger-scale instances or real-world benchmarks are needed to validate the method’s scalability and practical utility fully.\n\n3. The proposed solvers exhibit relatively large optimality gaps compared to traditional solvers and some advanced learning-based methods. While inference time is short, the balance between speed and solution quality (including feasibility) is not fully optimized, which may restrict applicability in scenarios requiring high-precision solutions.\n\n4. Insufficient implementation details: Critical implementation specifics are unclear, such as whether PS/Neural Diving were used for neighborhood search and the key hyperparameters adopted for PS and DiffILO. Notably, the paper reports results where PS performs worse than Gurobi, which contradicts the findings of PS’s original study—even on datasets presumably consistent with the original work. This discrepancy raises questions about the experimental setup and requires clarification."}, "questions": {"value": "1. Why do the authors not use the same problem sizes used in the Predict-and-Search paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yUThqY6g4M", "forum": "W42oLSwI9p", "replyto": "W42oLSwI9p", "signatures": ["ICLR.cc/2026/Conference/Submission19659/Reviewer_qDWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19659/Reviewer_qDWz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827777884, "cdate": 1761827777884, "tmdate": 1762931508435, "mdate": 1762931508435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method to design a ML-based heuristics for MILPs, cast as\ndrawing samples from a distribution over solution candidates. The underlying\nprobabilistic generative models used in this paper are variants of flows\n(consistency models, short-cut models and mean-flow models) that are designed to\ndecode (relaxations of) solutions in very few denoising steps. They operate in\nfeature (dense) space (ie embedding space) as opposed to most ML methods fo COs\nbased on generative models which operate in solution (ie discrete) space. The\npaper focus on MILPs with non-binary integer values.\n\n\nIn order to be able to work in high-dimensional dense spaces, this method\nrequires encoding and decoding program and candidate solutions.\n\nEncoding is learned by a contrastive divergence model inspired by CLIP. This method is described\nbriefly and lacks explanation to properly understandhow it is actually performed.\n\nFor decoding, a simple classifier is trained to recover the discrete values\ncorresponding to the dense values predicted. This is complemented by IIP  in order to\nencourage the model to predict real values as closed as possible to integer\nvalues.\n\n\nModels based on variants of flows (CMILP/SCMILP/MFILP) are presented in a very terse and lack definitions that can help understanding what they are about.\nI am wondering what is the use if these paragraphs. Either they should only refer papers introducing flow variants, either they should be written in such a way as readers can understand (at least) the definition.\nFor instance, $\\epsilon$ in CMILP is not defined, so the consistency constraint is simply impossible to understand. especially when models differ in time limits (for some t=0 is data for others it is noise).\nThe same can be said about SCMILP (eg /d/ is not defined) and MFILP ($\\eta$ ?)\nI suggest that only one variant be detailed in the main body of the article, and delegate the other two to the appendix.\nThis would leave room to understand the models and how they are adapted for the task at hand.\n\nThe MGD method is a nice addition the paper. I would recommend to use different letters than $\\beta,\\eta$ which are already used in the paper.\n\n\n\ntypos:\n- l. 143  improper font for n/m \n- l. 148 bt -> by\n- l. 171 MES not defined\n- l. 228  parentheses are imbalanced in Equation 4. Time scheduler is not defined.\n- l.342 Is (Nair et al., 2021) the correct citation for PaS?"}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a complete pipeline that can handle non-binary integer variables.\n2. the global architecture relies on an encoder and a decoder to go from solution space to feature space. Once they are trained, the architecture is agnostic to the generative model (diffusion, consistency, short-cut, mean-flow)"}, "weaknesses": {"value": "- The contribution is essentially the pipeline encoder/generative mode/decoder but the paper has 3 sections (3.2,3.3,3.4) on fast flow models that are either very difficult to understand if the reader is not familiar with these formalism, or uninformative otherwise: notations are not introduced and there is no explanation.\n- The training of the architecture is not performed end-to-end\n- Result tables could be presented in a friendlier way, for instance by using bold face to help identifying the best systems."}, "questions": {"value": "- Could training be performed end-to-end (encoder, generative model, decoder)?\n- Could at least the encoder and the decoder be trained jointly (VAEs come to mind)?\n- About IIP at inference time, how many times is *multiple* times (l. 206)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jrw7HfMvtg", "forum": "W42oLSwI9p", "replyto": "W42oLSwI9p", "signatures": ["ICLR.cc/2026/Conference/Submission19659/Reviewer_GgcN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19659/Reviewer_GgcN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910377150, "cdate": 1761910377150, "tmdate": 1762931507848, "mdate": 1762931507848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel class of one-step diffusion solvers for ILP, particularly for general integer variables. Building upon the observation that existing diffusion-based approaches (e.g., IP-guided DDPM/DDIM) can be viewed as gradient descent with only a single optimization step, the authors introduce three variants (CMILP, SCMILP, and MFILP). In addition, the paper proposes an objective-guided sampling strategy, integrating the optimization objective and feasibility penalties directly into the sampling stage via MGD. Extensive experiments on binary and general ILP benchmarks show that the proposed methods achieve higher sample feasibility and faster inference than both traditional solvers and IP-guided diffusion baselines, while maintaining acceptable optimality gaps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper reinterprets diffusion-based optimization through a unified framework that connects consistency modeling, flow matching, and mean flow formulations within ILP. This conceptual reframing is novel, intellectually stimulating, and provides a coherent perspective on how generative diffusion dynamics can be viewed as gradient-based optimization.\n\nPrior diffusion-based optimization methods were primarily restricted to binary MILPs. The paper’s treatment of general integer variables through the Iterative Integer Projection (IIP) mechanism and smooth surrogate gradients meaningfully broadens the applicability of diffusion models to a wider class of combinatorial problems.\n\nExperimental results demonstrate consistent improvements in sample feasibility and substantial reductions in inference time compared to IP-guided diffusion baselines. The ability to produce feasible solutions in seconds rather than minutes constitutes a clear practical advantage, particularly for time-sensitive applications."}, "weaknesses": {"value": "1. **Overstated novelty claim:** The claim of being the first to extend neural solver to the non-binary ILP case is overstated. Prior work [1] already introduced differentiable integer feasibility layers applicable to general MINLPs, including ILPs as a subset, and provided experiments on integer linear programs. The current contribution is better characterized as integrating such continuous-to-integer mappings within diffusion-based guidance.\n2. **Limited optimality quality:** While the proposed methods achieve high feasibility rates, the optimality gaps remain significantly larger than those of traditional solvers such as Gurobi or SCIP. The models tend to produce feasible but suboptimal solutions, positioning them closer to heuristic generators rather than competitive optimization solvers.\n3. **Unfair runtime comparison:** The runtime advantage reported against traditional solvers is partly a result of comparing approximate solutions to exact ones. A fairer comparison would fix a target optimality gap or time limit for all methods, as state-of-the-art solvers can also produce approximate feasible solutions within seconds under relaxed stopping criteria.\n4. **Training data cost:** The paper mentions using approximately 500 optimal and near-optimal solutions per instance for training but does not discuss the computational cost of generating this data. If the time spent obtaining these labeled solutions is comparable to or exceeds that of running traditional solvers, the overall efficiency advantage of the proposed framework becomes questionable. A discussion of the data acquisition cost relative to inference savings would provide a more balanced assessment.\n5. **Limited sensitivity analysis:**\nWhile the experimental results are convincing, the paper would benefit from additional sensitivity studies on a few key hyperparameters, such as the number of IIP iterations or the momentum coefficient in MGD, to better understand their impact on feasibility and optimality. Nevertheless, the absence of such analyses does not substantially undermine the main empirical findings.\n\n[1] Tang, B., Khalil, E. B., & Drgoňa, J. (2024). Learning to Optimize for Mixed-Integer Non-linear Programming with Feasibility Guarantees. arXiv preprint arXiv:2410.11061."}, "questions": {"value": "1. The feasibility of the proposed methods appears highly dependent on the feasibility penalty term. Could the authors report sensitivity analyses over this coefficient (e.g., feasibility vs. penalty weight curves) to clarify how robust the model is to hyperparameter tuning?\n2. The Iterative Integer Projection (IIP) function is differentiable but non-convex. Have the authors observed any convergence issues or gradient instability when applying IIP in practice? \n3. The Iterative Integer Projection (IIP) layer is applied only once during training for efficiency, whereas multiple iterations are used during inference. This introduces a potential train–test discrepancy, since the training loss is computed in the continuous domain without enforcing full convergence to integers. Have the authors observed any degradation in performance or feasibility due to this mismatch?\n4. For a fair runtime comparison, did the authors allow solvers like Gurobi or SCIP to terminate early under approximate optimality gaps (e.g., 1% or 5%)? If not, could the authors provide runtime comparisons under equivalent solution quality thresholds?\n5. The paper claims that the proposed diffusion framework is more parameter-efficient than L2O-MIMLP. However, this comparison appears inconsistent: L2O-MIMLP [1] employs a simple MLP-based architecture, whereas the proposed model uses a Transformer backbone with contrastive pretraining and diffusion-based components, which likely involve significantly more parameters.\n\n[1] Tang, B., Khalil, E. B., & Drgoňa, J. (2024). Learning to Optimize for Mixed-Integer Non-linear Programming with Feasibility Guarantees. arXiv preprint arXiv:2410.11061."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CzrWkKc5xx", "forum": "W42oLSwI9p", "replyto": "W42oLSwI9p", "signatures": ["ICLR.cc/2026/Conference/Submission19659/Reviewer_3HsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19659/Reviewer_3HsG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971103207, "cdate": 1761971103207, "tmdate": 1762931507356, "mdate": 1762931507356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on solving integer linear programming (ILP) problems with neural networks. To address long inference times and the difficulty of extending to non-binary integer problems, the authors propose three one-step diffusion-based ILP solvers, which are called CMILP, SCMILP, and MFILP, and propose to handle non-binary cases using an iterative integer projection (IIP) layer. To improve the sampling process, they further introduce a gradient-descent-based sampling method and a momentum mechanism into the objective-guided sampling of diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Extending neural solvers from binary to non-binary ILPs is novel and useful.\n2. The experimental comparisons—across different types of solvers, multiple metrics, and both binary and non-binary ILPs—are sufficient."}, "weaknesses": {"value": "1. There are some typos—for example, “n variables and m constraints” in line 143.\n2. Some descriptions could be clearer. For instance, not all symbols in Figure 1 are defined, which reduces readability. Also, the colors in Figure 3 should be made darker for better visibility."}, "questions": {"value": "1. Why is the coefficient $\\lambda$ applied only to the feasibility penalty term in Equation (2)? Why aren’t coefficients $λ_{recon}$ and $λ_{XXILP}$ used for the reconstruction error and the diffusion loss?\n2. Can the method be applied to mixed ILPs? Are there any challenges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZnEhcOWB8o", "forum": "W42oLSwI9p", "replyto": "W42oLSwI9p", "signatures": ["ICLR.cc/2026/Conference/Submission19659/Reviewer_Eyya"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19659/Reviewer_Eyya"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762613212024, "cdate": 1762613212024, "tmdate": 1762931507005, "mdate": 1762931507005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}