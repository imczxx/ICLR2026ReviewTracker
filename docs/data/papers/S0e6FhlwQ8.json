{"id": "S0e6FhlwQ8", "number": 10648, "cdate": 1758178447084, "mdate": 1763379581399, "content": {"title": "Medical Decision Tree-Enhanced LLMs for Interpretable Reasoning", "abstract": "Large Language Models have made significant strides in medical reasoning. However, challenges remain due to their limited medical knowledge and the risk of hallucinations. While RAG methods can mitigate these issues by retrieving relevant medical information, they typically supply verbose text fragments, which challenges the model's comprehension. Inspired by the widespread use and inherent interpretability of medical decision trees in clinical practice, we propose Medical Decision Tree RAG (MDT-RAG), a novel RAG framework specifically designed for medical reasoning. In this approach, clinical guidelines containing diagnostic and therapeutic information are first converted into decision trees, which are then used to augment LLMs in place of raw text. Experiments demonstrate that our method not only enhances the performance of medical LLMs in reasoning tasks but also exhibits strong interpretability. All related resources have been made publicly available.", "tldr": "Using medical decision trees as the retrieval source for RAG systems in medical reasoning scenarios.", "keywords": ["retrieval-augmented generation", "medical decision tree", "medical reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d35d0b11805a4203f8c70e0b482af6433e70ea3a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a Retrieval-Augmented Generation (RAG) framework, MDT-RAG, which uses Medical Decision Trees (MDTs) as retrieval and reasoning carriers. The core workflow includes automatically extracting multi-branch MDTs from clinical guidelines (validated through structural and factual checks), summarizing MDTs into retrievable text using TreeIndex, filtering retrieved results for relevance with LLMFilter, and converting the structured MDT logic into readable IF-ELSE natural language guidance via TreeNLGen to assist LLMs in answering questions.\n\nA large-scale MDT retrieval database was constructed (66,535 trees from ~16k guidelines), along with a medical reasoning evaluation dataset based on MIMIC-IV (2,046 entries, covering both medication recommendation and treatment planning tasks).\nIn comparisons across multiple models (small general-purpose models, medically specialized models, large open-source models, and commercial closed-source systems) and various RAG baselines (OQ-RAG, QR-RAG, Q2D-RAG), MDT-RAG consistently outperformed in terms of average score (AVG) and high-quality response rate (HQR), with particularly significant improvements for smaller models. Ablation studies demonstrate that TreeIndex, TreeNLGen, and LLMFilter are essential components, while sensitivity experiments on the number of trees reveal an \"optimal middle ground.\"\nThe paper highlights enhanced interpretability and a step-by-step reasoning path more aligned with clinical practices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The paper is well-structured, with clear figures illustrating comparisons and workflows. Formalization and implementation details are adequately provided.\n-  Medical reasoning demands interpretability. MDT-RAG's structured guidance and transparent reasoning paths offer practical value for deployment. It shows significant improvements for smaller models, reducing reliance on large proprietary models.\n- The experiments are solid and comprehensive."}, "weaknesses": {"value": "- The two implementations of LLMFilter (large model judgment vs. lightweight scoring threshold) are described as \"having similar effectiveness,\" but a quantitative comparison is lacking.\n-  Additionally, the input token budget for text-based RAG and MDT-RAG is not fully equivalent (with MDT being shorter on average). Although this is disclosed in the paper and addressed through sensitivity experiments, it is recommended to provide a strict comparison under \"equal token budget/equal retrieval units\" conditions."}, "questions": {"value": "Bias mitigation in LLM-as-Judge scoring: Has cross-reviewer model consistency (e.g., across different vendors/architectures) and expert human sampling validation been conducted?\n\nEqual budget comparison: Can a strict comparison between text-based RAG and MDT-RAG under equal token budget/equal retrieval units be provided?\n\nQuantitative comparison of the two LLMFilter implementations: What are the performance, latency, and cost statistics for each approach?\nExternal validity: Is there an expert evaluation of case-to-path alignment or an assessment of path accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BDStRh3kxz", "forum": "S0e6FhlwQ8", "replyto": "S0e6FhlwQ8", "signatures": ["ICLR.cc/2026/Conference/Submission10648/Reviewer_eN9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10648/Reviewer_eN9X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720761125, "cdate": 1761720761125, "tmdate": 1762921902267, "mdate": 1762921902267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "hJBx7n7d9D", "forum": "S0e6FhlwQ8", "replyto": "S0e6FhlwQ8", "signatures": ["ICLR.cc/2026/Conference/Submission10648/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10648/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763379580599, "cdate": 1763379580599, "tmdate": 1763379580599, "mdate": 1763379580599, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MDT-RAG, a retrieval-augmented generation framework that replaces verbose text snippets with medical decision trees (MDTs) extracted from clinical guidelines. The retrieval source comprises 15960 guideline documents, resulting in 66535 decision trees. For evaluation, the authors curate a reasoning dataset from MIMIC-IV, with around 2k patient entries and two tasks per entry (medication recommendation and treatment plan design). Experiments across five LLMs report consistent gains for MDT-RAG over text-based RAG on the proposed metrics."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of replacing unstructured text chunks with structured decision trees sounds interesting and innovative to me.\n- The authors present a practical and reproducible pipeline, converting tree logic into DNF and IF–ELSE rules, and have open-sourced their implementation for transparency.\n-  Across five LLMs, MDT-RAG outperforms text-RAG baselines for both medication and treatment planning tasks.\n- Ablation studies are provided to justify the importance of different components"}, "weaknesses": {"value": "- The dataset construction process relies heavily on LLMs for annotation and correction. It remains unclear whether model-generated content is fully faithful to source knowledge or subject to hallucination. Although a Fact Verification step is mentioned, it is unclear which LLM is used and how its judgments align with human evaluators. This might raise concerns about the reliability of the retrieval source for MDT-RAG.\n- As shown in the ablation study, MDT Filtering is an important step to ensure the high performance of MDT-RAG. However, a similar filtering mechanism can also be easily applied to text-based RAG. A fairer comparison incorporating equivalent filtering for text-RAG would clarify whether the observed gains truly stem from the MDT structure itself.\n- While the authors claim DDXPlus is narrow in its evaluation scope. I don't see why it does not apply to the methods compared here. The current paper only contains evaluations on the self-constructed dataset. Evaluating on additional public datasets would verify if the performance gain is generalizable.\n- All three evaluation metrics rely on LLM-based scoring, which could be subjective and biased. Analyzing the ranking quality or adding more objective metrics would strengthen the credibility of the reported results.\n\nMinor:\nLine 395 should have \"Table 1 and Table 2\" instead of \"Table 2 and Table 2\"."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pHQqdNJ3FC", "forum": "S0e6FhlwQ8", "replyto": "S0e6FhlwQ8", "signatures": ["ICLR.cc/2026/Conference/Submission10648/Reviewer_bRLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10648/Reviewer_bRLi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795978270, "cdate": 1761795978270, "tmdate": 1762921901818, "mdate": 1762921901818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a medical decision tree dataset and a method for medical reasoning using the medical decision trees. The medical decision tree dataset is automatically extracted from medical guidelines using LLMs. The method for medical reasoning first converts (relevant nodes in) the medical decision trees into natural language and makes diagnostic recommendations. The experimental results show some performance improvement."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. I basically think that the dataset potentially contributes to the community (if it is publicly available)."}, "weaknesses": {"value": "1. This is just a minor point, but in Section 3.1, the paper says “we opted for multi-branch MDTs instead of conventional binary trees,” which seems weird to me. The structure of the tree seems to be determined solely based on the structure of diagnostic procedures, and it’s not our choice. \n2. I cannot see how the structural correctness of the medical decision trees is evaluated. \n3. As for the content correctness of medical decision trees, I’m not sure if inherent knowledge in LLMs is sufficient to evaluate it. How is the correctness guaranteed? I would like to see some discussions about this point. \n4. Related to 2 and 3, it is not straightforward to see the correctness of the medical decision trees. This may depend on the paper’s position, i.e., if its focus is to construct faithful medical decision trees for potential deployment or is solely to serve as a benchmark for models. For the former, I believe expert evaluation is mandatory even for a subset of trees. For the latter, at least consistency of the trees (e.g., reachability to the final decision) should be evaluated so that the dataset can provide some ideas about the upper-bound performance. \n5. It’s hard for me to see if the nodes in the proposed medical decision trees really form a tree. First of all, a node $n$ seems to contain multiple triplets, so I guess a single node $n$ forms a tree. However, then $C$ does not make much sense. Also, the logical operators in $\\phi$ are associated with a single triplet, which means binary operators “AND” and “OR” are applied to a single operand? I could not understand this. This medical decision tree structure definition should be fully revised."}, "questions": {"value": "1.\tI don’t see why errors in the generated graph grow significantly when the tree is deeper. Can this be more specific?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J4u80yduMg", "forum": "S0e6FhlwQ8", "replyto": "S0e6FhlwQ8", "signatures": ["ICLR.cc/2026/Conference/Submission10648/Reviewer_WJrA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10648/Reviewer_WJrA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984813633, "cdate": 1761984813633, "tmdate": 1762921901399, "mdate": 1762921901399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method that integrates medical decision trees (MDTs) into large language models to enhance clinical reasoning. By converting guideline knowledge into structured condition–conclusion trees and rewriting them into natural language, the approach aims to make medical reasoning more interpretable and reliable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The in-depth exploration of how to improve medical reasoning is meaningful and interesing, more than just finding the correct answer.\n2. Using MDTs to represent clinical pathways and embedding them into the model improves interpretability and potentially enhances real-world applicability in medical contexts."}, "weaknesses": {"value": "1. The main results rely on o4-mini’s 1–5-scale judgments on only 200 samples. This limited scope and the exclusive use of an LLM judge make the evaluation less convincing, especially without any discussion on inter-judge consistency or possible bias.\n\n2. The entire pipeline depends heavily on large models for data construction, which could introduce systematic bias. There is also little discussion on the medical rigor of the MDT definitions and generation templates.\n\n3. The dataset construction lacks details and transparency. It is unclear how drug and treatment recommendations were extracted and how the reference labels were linked to each case.\n\n4. Lack of more powerful baseline. While the related work section acknowledges that structured RAG methods, such as knowledge-graph or rule-based retrieval can improve precision, the experiments only compare against text-based RAG variants (OQ/QR/Q2D) and CoT. This makes it difficult to isolate the specific advantage of using tree structures."}, "questions": {"value": "1. Are there any objective or quantitative evaluation metrics besides LLM-based scoring?\n\n2. How exactly was the dataset built, and how were the label references matched with the source data?\n\n3. Beyond qualitative case studies, was there any user study or physician evaluation to measure the improvement in interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zi7GIgUYFi", "forum": "S0e6FhlwQ8", "replyto": "S0e6FhlwQ8", "signatures": ["ICLR.cc/2026/Conference/Submission10648/Reviewer_S8tq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10648/Reviewer_S8tq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008382185, "cdate": 1762008382185, "tmdate": 1762921900308, "mdate": 1762921900308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}