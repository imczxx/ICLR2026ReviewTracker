{"id": "tmIRNo66Rg", "number": 4646, "cdate": 1757733750784, "mdate": 1763635221000, "content": {"title": "TriVLA: A Triple-System-Based Unified Vision-Language-Action Model with Episodic World Modeling for General Robot Control", "abstract": "Recent advances in vision–language models (VLMs) have enabled robots to follow open-ended instructions and demonstrate impressive commonsense reasoning. However, current vision–language–action (VLA) frameworks primarily rely on static representations and limited temporal context, restricting agents to short-horizon, reactive behaviors and hindering robust generalization in dynamic embodied environments. Inspired by cognitive neuroscience theories of episodic memory, we are, to our knowledge, among the first to introduce a formalized episodic world model in VLA, enabling embodied robots to accumulate, recall, and predict sequential experiences. As an instantiation of this concept, our unified \\textbf{TriVLA} realizes the episodic world model through a triple-system architecture: integrating multimodal grounding from a pretrained VLM (System 2) and temporally rich dynamics perception from a video diffusion model (System 3). This enables the agent to accumulate and recall sequential experiences, interpret current contexts, and predict future environmental evolution. Guided by episodic representations that span both the past and anticipated future, the downstream policy (System 1) generates coherent, context-aware action sequences through flow-matching and cross-modal attention mechanisms. Experimental results show that TriVLA operates efficiently at ~36 Hz and consistently outperforms baseline models on standard benchmarks and challenging real-world manipulation tasks. It demonstrates strong long-horizon planning and open-ended intent understanding, showcasing the advantages of episodic world model-inspired reasoning for robust, generalizable robot intelligence.", "tldr": "", "keywords": ["Vision-Language-Action Model (VLA)", "Episodic World Model", "General Robot Control", "Imitation Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc4e5f0238df728ae3c11b41331a0d1a983cf221.pdf", "supplementary_material": "/attachment/349857cd5416500de4e3ed53dd32d70da5419881.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces TriVLA, a unified Vision–Language–Action (VLA) model that integrates an episodic world model through a triple-system architecture, inspired by cognitive neuroscience theories of episodic memory. The main difference to previous work is that the low-level policy is conditioned on both VLM features and video model features."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The high-level idea sounds reasonable, triple level system can benefit from both the semantic tokens from VLM and also the visual predictive tokens from video model. Although it makes the system a little bit complicated.\n\n2. The experiments have a wide range, ranging from Calvin, LIBERO, Metaworld and real world.\n\n3. The visualization and ablation of the experiments are good."}, "weaknesses": {"value": "Major:\n1. The main weakness I concerned is that the method looks very similar to VPP baseline used in the paper. I feel the architecture is very similar to video prediction policy (VPP) work after I carefully compare the these two papers. It seems that the system 3 and system 1 is same to the VPP and the author add a system 2 over the VPP framework. While adding a VLM is reasonable, the overall architectural novelty feels incremental and mostly an extension of VPP rather than a substantive departure.\n\n2. (continued to last one) Moreover, on the Calvin benchmark, the improvement over VPP seems minor. This further doubt the effectiveness of the newly added system2.\n\n3. Missing unified-architecture baselines. Since the approach combines a VLM with a video model for policy learning, it should be compared against unified architectures that follow a similar design philosophy, such as F1 and UPVLA [1–2]. Matching training budgets, parameter counts, and inference costs would make these comparisons fair and informative.\n\nMinor:\nThe image order is wrong in the Figure 5. (the last two Calvin image)\n\n[1] F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions\n[2] Up-vla: A unified understanding and prediction model for embodied agent"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dTKSsuwb24", "forum": "tmIRNo66Rg", "replyto": "tmIRNo66Rg", "signatures": ["ICLR.cc/2026/Conference/Submission4646/Reviewer_1yXu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4646/Reviewer_1yXu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433824588, "cdate": 1761433824588, "tmdate": 1762917488061, "mdate": 1762917488061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "is TriVLA a world model or an episodic world model?"}, "comment": {"value": "The paper's introduction seems to be inspired by the cognitive science and neuroscience literature on episodic memory. Two primary features of episodic memory in the brain is its long temporal window and that it is content addressable (i.e. specific memories are recalled given cues about them). The capacity to \"anticipate future dynamics\" is taken as a critical feature to motivate the need for episodic world models. However, future predictions of short horizons is not a unique feature of episodic memory/world models. Any world model is typically expected to predict future outcomes. Predicting object motion trajectories in the brain has also been shown to depend primarily on the prefrontal cortex which is a distinct brain structure from the medial temporal cortex (most critically involved in episodic memory). Given that the TriVLA model doesn't explicitly have any memory modules and its temporal horizon seems to be short, I think use of the term \"episodic\" is somewhat confusing and unjustified here."}}, "id": "8jVJXq1alr", "forum": "tmIRNo66Rg", "replyto": "tmIRNo66Rg", "signatures": ["~Pouya_Bashivan1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Pouya_Bashivan1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4646/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763650921452, "cdate": 1763650921452, "tmdate": 1763651164760, "mdate": 1763651164760, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to train VLAs more effectively. The proposed method leverages a pretrained and fine-tuned diffusion model to generate predicted future frames that hopefully can help  the prediction of current actions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s presentation of its method is overall clear.\n\nThe paper obtains exception results when compared with baselines that incorporate future prediction (Table 1&2)."}, "weaknesses": {"value": "1. The paper lacks a problem statement/setup or an evaluation protocol. Section 3 introduces only the VLA mode; but what is the problem? Is it supervised learning/imitation learning? If so, how are we evaluating it (train-and-test?) What is exactly “Zero-shot long-horizon evaluation” in Table 1’s caption?\n\n2. System 3 is the major technical novelty, yet the paper lacks transparency on what data was used to fine-tune it. Section 4.2 mentions using “self-collected data” for fine-tuning but is unclear about the nature of the data. If the self-collected data is well-correlated with the tasks for evaluation, good performance is not that surprising. Did authors try the pretrained 1.5B SVD model without fine-tuning?\n\n3. Given that the proposed method of producing and incorporating future frames (System 3) is fairly straightforward, it’s a bit unclear in what aspects it is significantly different from prior methods. For example:\n    - If prior methods use “single-step future prediction\" (line 184), how hard is it to extend these prior methods to multi-step  future prediction? And did authors do experiments to study the number of predicted steps to predict as a hyperparameter of their method?\n    - Seer (Tian et al. 2024) “predicts actions by applying inverse dynamics models conditioned on forecasted visual states”, which seems qualitatively similar to the proposed method. The difference is not well explained.\n\n4. Can you clarify what “action flow-matching” is in System 1 (e.g., Equation (5))? Is there a qualitative difference between (5) and the diffusion loss defined in (1)?"}, "questions": {"value": "All my concerns are in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "021Iwg61Ox", "forum": "tmIRNo66Rg", "replyto": "tmIRNo66Rg", "signatures": ["ICLR.cc/2026/Conference/Submission4646/Reviewer_FoJF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4646/Reviewer_FoJF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764651361, "cdate": 1761764651361, "tmdate": 1762917487644, "mdate": 1762917487644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TriVLA, a unified vision-language-action (VLA) framework for general-purpose robot control. The authors' central thesis is that current VLA models are overly \"static\" and \"reactive,\" limiting their ability to perform long-horizon tasks. To address this, they draw inspiration from cognitive neuroscience, specifically the concept of \"episodic memory,\" which involves both recalling the past and simulating the future.\n\nThe paper instantiates this idea as an \"episodic world model\" implemented via a \"triple-system architecture\":\n\nSystem 2 (Episodic Multimodal Perception): A pretrained Vision-Language Model (VLM) (Eagle-2) that processes the current visual observation and language instruction to provide semantic grounding.\nSystem 3 (Episodic Dynamics Perception): A fine-tuned Video Diffusion Model (VDM) (Stable Video Diffusion) that predicts future scene dynamics, providing a temporal-predictive context.\nSystem 1 (Policy Learning): A diffusion-based policy (Diffusion Transformer) that serves as the low-level controller. It integrates the outputs from System 2 and System 3, along with the robot's proprioceptive state, to generate chunks of actions.\n\nThe authors claim that this architecture enables robust, long-horizon reasoning. They support this claim with experiments on the CALVIN, LIBERO, and MetaWorld benchmarks , as well as real-world qualitative demonstrations , showing that TriVLA achieves state-of-the-art performance.\n\nWhile the paper presents a compelling idea and strong results, I have significant concerns about the empirical validation of its core architectural claims. Specifically, the ablation studies are incomplete and fail to provide a clear comparison against the very \"dual-system\" baseline the paper aims to improve upon. My current tendency is to recommend rejection due to the experimental evidence. Given the clarifications in an author response, I would be willing to increase the score."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a critical and widely recognized limitation of current robotic policies: their struggle with long-horizon reasoning due to a reliance on \"static\" representations . The analogy to cognitive neuroscience and \"episodic memory\" provides an intuitive motivation for an architecture that doesn't just perceive the present but also predicts the future.\n\nWhile the individual components (VLMs, VDMs, diffusion policies) are existing technologies, their explicit synthesis in this parallel \"triple-system\" architecture is novel and well-justified. The design, where a semantic module (Sys 2) and a dynamics module (Sys 3) independently process the input and provide complementary information to the policy (Sys 1), is clean and directly addresses the motivated problem ."}, "weaknesses": {"value": "The paper's most significant weakness is the ablation study in Table 4. The paper's entire argument is that its \"Triple-System\" (VLM+VDM+Policy) is superior to the \"Dual-System\" (VLM+Policy) it critiques in Figure 2. To prove this, the most crucial ablation baseline would be EMP + L-Policy (i.e., TriVLA without System 3 / EDP). This baseline is missing. Without it, the authors have not empirically demonstrated that adding the VDM (System 3) is superior to the \"static\" VLA model they aim to improve upon.\n\nThe paper's motivation leans heavily on \"episodic memory,\" which is defined by \"mental time travel\" into both the future and the past. The VDM (System 3) clearly and effectively addresses the \"future simulation\" aspect. However, the \"past recall\" aspect is not clearly instantiated in the architecture at inference time. The paper states the model can \"accumulate, recall... sequential experiences\", but the diagrams (Fig. 1, 3) only show the current observation and instruction as inputs. The paper should be more precise about whether \"recall\" simply refers to the knowledge implicitly stored in the VDM's weights or if there is an active, online mechanism for recalling the current episode's past states."}, "questions": {"value": "1. The paper's entire motivation rests on improving \"dual-system\" (VLM+Policy) models . To validate this claim, a direct ablation comparing the full TriVLA (System 1+2+3) against a \"dual-system\" (System 1+2, i.e., without the EDP/VDM module) is essential. Why was this baseline, which is the most direct test of your core hypothesis, omitted?\n2. Please explain how the baseline model in Table 4 (\"L-Policy + EDP\", Avg. Length 4.06) functions. This model appears to lack the VLM (System 2, EMP). How does it process the language instructions required for the CALVIN benchmark without this VLM?\n3. The paper uses \"episodic memory\" (implying past recall + future simulation) as its guiding analogy . Please clarify how TriVLA performs recall of past sequential experiences from the current episode at inference time. Is this \"recall\" mechanism simply the \"action history\" fed to System 1, or is there a more explicit component, as the cognitive science framing suggests?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r4hYqvh85w", "forum": "tmIRNo66Rg", "replyto": "tmIRNo66Rg", "signatures": ["ICLR.cc/2026/Conference/Submission4646/Reviewer_5iS3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4646/Reviewer_5iS3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917909285, "cdate": 1761917909285, "tmdate": 1762917487257, "mdate": 1762917487257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TriVLA, a triple-system unified vision–language–action (VLA) model that introduces an episodic world model inspired by cognitive neuroscience. The framework integrates three components: (1) a low-level policy module (System 1), (2) an Episodic Multimodal Perception module based on a pretrained vision–language model (System 2), and (3) an Episodic Dynamics Perception module realized via a fine-tuned video diffusion model (System 3). Together, they allow robots to recall, reason, and predict over temporal sequences for improved long-horizon manipulation. TriVLA achieves strong quantitative results on major simulation benchmarks (CALVIN, LIBERO, MetaWorld) and demonstrates qualitative real-world performance, operating at 36 Hz inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a coherent and well-motivated triple-system framework combining vision–language understanding and dynamic modeling, extending the traditional dual-system VLA paradigm.\n\n- Results on multiple benchmarks (CALVIN, LIBERO, MetaWorld) are systematically compared against recent SOTA methods, showing consistent improvements.\n\n- The inference design of System 3 (single forward pass instead of full denoising) enables real-time operation at 36 Hz, demonstrating good computational efficiency for deployment."}, "weaknesses": {"value": "- The introduction of the world model is the paper’s central claim, yet Table 4 does not include results for EMP + L-Policy without EDP. Without this comparison, the contribution of System 3 remains unconvincing. It would also be helpful to report CALVIN per-task scores (1–5) rather than only the average length, and to conduct similar ablations on LIBERO.\n\n- The real-world experiments lack any quantitative metrics—only qualitative demonstrations are shown, which weakens the empirical evidence for real deployment.\n\n- From a model-design perspective, the implementation of System 3 (Episodic Dynamics Perception) is relatively straightforward, a fine-tuned video diffusion module without clear architectural novelty, making this part less conceptually innovative despite its empirical value."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IFMCblPH7w", "forum": "tmIRNo66Rg", "replyto": "tmIRNo66Rg", "signatures": ["ICLR.cc/2026/Conference/Submission4646/Reviewer_ZGUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4646/Reviewer_ZGUY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061504576, "cdate": 1762061504576, "tmdate": 1762917486989, "mdate": 1762917486989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}