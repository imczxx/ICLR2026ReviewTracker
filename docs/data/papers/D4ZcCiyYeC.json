{"id": "D4ZcCiyYeC", "number": 23404, "cdate": 1758343280186, "mdate": 1759896816643, "content": {"title": "V2P: Visual Attention Calibration for GUI Grounding via Background Suppression and Center Peaking", "abstract": "Precise localization of GUI elements is crucial for the development of GUI agents. Traditional methods rely on bounding box or center-point regression, neglecting spatial interaction uncertainty and visual-semantic hierarchies. Recent methods incorporate attention mechanisms but still face two key issues: (1) ignoring processing background regions causes attention drift from the desired area, and (2) uniform modeling the target UI element fails to distinguish between its center and edges, leading to click imprecision. Inspired by how humans visually process and interact with GUI elements, we propose the Valley-to-Peak (V2P) method to address these issues. To mitigate background distractions, V2P introduces a suppression attention mechanism that minimizes the model's focus on irrelevant regions to highlight the intended region. For the issue of center-edge distinction, V2P applies a Fitts' Law-inspired approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight gradually decreases from the center towards the edges. The weight distribution follows a Gaussian function, with the variance determined by the target's size. Consequently, V2P effectively isolates the target area and teaches the model to concentrate on the most essential point of the UI element. The model trained by V2P achieves the performance with 92.3% and 50.5% on two benchmarks ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's contribution, underscoring V2P's generalizability in precise GUI grounding tasks and its potential for real-world deployment in future GUI agents.", "tldr": "", "keywords": ["GUI Grounding", "GUI Agent", "GUI Navigation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d01842331e671ecfbd49faefdd31c1bdb1d566d.pdf", "supplementary_material": "/attachment/e6f53bd2c59f8eeffa5220d0f0e540c33571ffe6.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces V2P, a training strategy for GUI grounding that suppresses attention on background regions and encourages a center-peaked focus on the target element, aiming to reduce attention drift and imprecise clicks. Evaluated on ScreenSpot-v2 and ScreenSpot-Pro, it reports 92.3% and 50.5% accuracy, improving over several reported baselines, and ablations show both components matter."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The proposed V2P method is precisely specified, with both the suppression loss over non-target patches and the Fitts-Gaussian label construction given.\n\nS2: On the two evaluated benchmarks, V2P-7B achieves 92.3% on ScreenSpot-v2 and 50.5% on ScreenSpot-Pro, outperforming several GUI baselines.\n\nS3: The idea of introducing Background Distraction and Centre-edge Confusion to the GUI grounding task is quite interesting."}, "weaknesses": {"value": "W1: The empirical evaluation is restricted to two benchmarks, ScreenSpot-v2 and ScreenSpot-Pro, with no additional benchmarks reported in the main results. Especially, ScreenSpot-v2 is saturated with baselines already achieving >90%. Consequently, the authors might need to include evaluations on diverse benchmarks, such as UI-Vision[1] and OSWorld-G[2], to support the paper’s claims about its applicability to future GUI agents.\n\nW2: All experiments are conducted on a single VLM backbone, Qwen2.5-VL-7B-Instruct. The paper does not report results on either a smaller model or an alternative backbone, so it is difficult to assess whether the proposed V2P mechanism is backbone-agnostic or relies on this specific model configuration.\n\nW3: While the paper introduces background distraction and center–edge confusion as two key failure modes, the latter is not illustrated as clearly as the former (e.g., in Fig. 1) and the paper does not provide a direct quantitative link showing that reducing these two specific phenomena is what drives the gains in grounding accuracy, as opposed to general attention sharpening. A more explicit analysis tying “before/after” attention maps to changes in accuracy would strengthen the motivation for the center-focused supervision.\n\nW4: The manuscript states that training V2P-7B requires 32× NVIDIA H200 GPUs for about 35 hours ( about 1,120 GPU-hours), which raises some concerns about efficiency. For models of a similar parameter scale, the reviewer is aware that GTA1-7B[3] (16× H100 for 2 days), GUI-ARP-7B[4] (8× H20), and GUI-Spotlight[5] (8× H200) have better performance on ScreenSpot-Pro but require much less computational resources in the training.\n\n[1] Nayak et al. UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction.\n\n[2] Xie et al. Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis.\n\n[3] Yang et al. GTA1: GUI Test-time Scaling Agent.\n\n[4] Ye et al. GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents.\n\n[5] Lei et al. GUI-Spotlight: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding"}, "questions": {"value": "Please see Weaknesses. The reviewer is willing to raise the score if the authors address most, if not all, of the questions above in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bk5OExWiPM", "forum": "D4ZcCiyYeC", "replyto": "D4ZcCiyYeC", "signatures": ["ICLR.cc/2026/Conference/Submission23404/Reviewer_BMnm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23404/Reviewer_BMnm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877823595, "cdate": 1761877823595, "tmdate": 1762942647034, "mdate": 1762942647034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle the limitations of existing attention-based approaches for visual GUI agents. To address background distractions and center–edge confusion, this work introduces an Attention Suppression Mechanism and Fitts-Gaussian Peak Modeling. Experiments are conducted on the ScreenSpot benchmark series and demonstrate clear performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is interesting and intuitive. Based on the qualitative analysis, the localization results appear accurate.\n\n- The improvements on GUI grounding benchmarks are impressive.\n\n- The draft is well-organized, and the authors provide both successful and failure cases of their method in the Appendix."}, "weaknesses": {"value": "- Experiments are conducted only on GUI grounding benchmarks. It remains unclear whether the proposed method also performs well on GUI agent task benchmarks. Evaluating the approach on such tasks would be important, as user instructions in agent scenarios often do not exactly match the textual labels of GUI elements. It would also help clarify how the attention mechanism behaves when dealing with semantically ambiguous or partially mismatched instructions.\n\n- Furthermore, after reviewing Appendix D.1.2 and Table 2, I am not fully convinced by the quality of the proposed attention maps. The authors note that “this scattered attention pattern typically occurs in scenarios with numerous distracting elements or cluttered interfaces, suggesting that the model’s decision-making process becomes uncertain when faced with complex visual layouts.” This observation is concerning, as GUI environments naturally and frequently exhibit such cluttered and visually complex conditions. Hence, additional analysis or mitigation strategies would strengthen the claim of robustness."}, "questions": {"value": "- This method may potentially offer a favorable Pareto trade-off compared to SFT and RL approaches, but this is not clearly demonstrated in its current form. Providing such an analysis would significantly strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vi8tiMhpOt", "forum": "D4ZcCiyYeC", "replyto": "D4ZcCiyYeC", "signatures": ["ICLR.cc/2026/Conference/Submission23404/Reviewer_THew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23404/Reviewer_THew"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996330856, "cdate": 1761996330856, "tmdate": 1762942646828, "mdate": 1762942646828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes V2P (Valley-to-Peak), a visual attention calibration method for GUI grounding. The authors identify two major issues in current attention-based GUI grounding approaches: background distraction and center-edge confusion. To address them, they introduce two components — an inverse-attention penalty to suppress attention on irrelevant regions and a Fitts-Gaussian peak modeling (FGPM) to model human-like clicking behavior following Fitts’ Law. The method is integrated into GUI-Actor and evaluated on ScreenSpot-v2 and ScreenSpot-Pro, achieving 92.3% and 50.5% accuracy respectively. The approach is conceptually simple, human-inspired, and empirically validated."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Using a Gaussian distribution to model GUI grounding clicks is quite reasonable and aligns well with human interaction patterns. The inverse-attention penalty also shows a certain degree of innovation.\n- The writing is clear and easy to follow, and the figures are visually well-designed and polished."}, "weaknesses": {"value": "- The explanation for why there is no improvement at all on ScreenSpot-v2 is not convincing. Figure 3(b) shows that the proposed method improves localization for small and medium elements, and ScreenSpot-v2s contains many such elements. The authors should further analyze why the method does not show improvement on ScreenSpot-v2s.\n- The proposed method is built upon GUI-Actor, but it is unclear why the V2P-baseline performs almost the same as GUI-Actor. The authors should provide results for the w/o FGPM & SA setting using their own dataset.\n- The method suppresses background regions through the Attention Suppression Mechanism, which assumes that all background areas are irrelevant. However, if multiple valid buttons exist in the background, this mechanism cannot handle such cases.\n- The paper lacks evaluation on other benchmarks, such as MMBench-GUI and UI-I2E-Bench."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LkSmvTVqHa", "forum": "D4ZcCiyYeC", "replyto": "D4ZcCiyYeC", "signatures": ["ICLR.cc/2026/Conference/Submission23404/Reviewer_tRLC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23404/Reviewer_tRLC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762504750867, "cdate": 1762504750867, "tmdate": 1762942646404, "mdate": 1762942646404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle the problem of precisely localizing UI elements in GUI agents. They highlight two problems in current methods that incorporate attention mechanisms in their method: 1) the background region can affect attention distribution, causing drifts; 2) failure to distinguish between center and edges, leading to mistakes in click actions. They introduce two methods to mitigate this: 1) attention suppression in irrelevant regions; 2) Fitts-Gaussian Peak Modeling. The authors evaluate on ScreenSpot-Pro and Screenspot v2 to demonstrate the effectiveness of their method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The author's motivation is sound. When using models that use attention mechanisms as inductive biases, the two problems highlighted by the authors can be expected, and the authors have come up with an intuitive and innovative method to solve the problem. The authors conduct a detailed analysis/ablation of where their method excels, like what size, shape of the UI elements the methods provide the most gains, and how different design choices influence their method's performance. These help understand the effectiveness of the proposed approaches."}, "weaknesses": {"value": "1. Lack of comparable baselines: The paper is mainly a method paper, and hence, I believe a fair and controlled comparison across different methods is needed to justify the claims that the authors make. To justify the claims, two key baselines are necessary:\n\n- SFT-Only Baseline: The base model trained with conventional SFT on the authors' new, filtered dataset. This would establish a proper baseline and clarify how much improvement the proposed method contributes beyond a standard approach on the same high-quality data.\n- Methods like GUI-Actor should be retrained on the authors' new dataset. As the dataset underwent significant filtering, it is crucial to demonstrate that the performance gains come from the proposed method and not just from a superior dataset compared to what the baselines originally used.\n\n2. The proposed method appears to be a modification of GUI-Actor with additional loss functions, which may limit the paper's perceived novelty. Hence, the contribution score.\n\n3. The paper compares methods trained on vastly different scales of data. The proposed approach uses 700k images, while key baselines like SE-GUI and GUI-G2 use significantly less (e.g., 3k images and 100k instances). This makes the comparison unclear. It is not evident how the proposed method performs in the lower-data regimes where the baselines operate. For a fair comparison, the authors should either re-train the baselines on their 700k dataset or, if that is not feasible, evaluate their own method in a reduced-data setting (e.g., 100k instances).\n\n4. The efficacy of the method is demonstrated on only two benchmarks. To provide a more comprehensive and robust validation, the authors should consider evaluating on a wider array of benchmarks, such as OSWorld-G [1] and UI-Vision [2], which test different aspects of GUI grounding and generalization.\n\n5. The authors could expand the related works section with other relevant grounding works that do not use attention-based methods.\n\n[1] Xie et al. Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis\n\n[2] Nayak et al. UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction"}, "questions": {"value": "1. Could you clarify the total number of individual annotated elements in the dataset used to train V2P, as opposed to just the total number of screenshots?\n2. I am not sure comparing the attention distribution of normal models like UI-Tars to that proposed by the authors is a fair comparison. Attention-based methods like GUI-Actor introduce an inductive bias, but approaches trained using SFT or RL don't need to learn similar inductive biases. Hence, the results for UI-TARS for attention map quality in Table 2 does not make a lot of sense to me. What do the authors think about this?\n3. Can suppressing attention on background elements have unintended consequences? For example, for platforms that are not in distribution to those trained by the authors, could it be possible that suppressing the attention can lead to narrow and overconfident predictions of wrong elements that do not take into account the entire screen and the new elements?\n\nSee questions and suggestions in weakness also."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hVetvW2yKE", "forum": "D4ZcCiyYeC", "replyto": "D4ZcCiyYeC", "signatures": ["ICLR.cc/2026/Conference/Submission23404/Reviewer_grdY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23404/Reviewer_grdY"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762895613009, "cdate": 1762895613009, "tmdate": 1762942646080, "mdate": 1762942646080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}