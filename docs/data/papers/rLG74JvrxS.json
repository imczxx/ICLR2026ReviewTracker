{"id": "rLG74JvrxS", "number": 16296, "cdate": 1758262858614, "mdate": 1763568473515, "content": {"title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning", "abstract": "Large language model (LLM) agents often generate causally invalid plans in collaborative tasks due to their reliance on surface-level correlations rather than grounded causal reasoning. This limitation undermines their performance in terms of coordination and planning in dynamic environments. We address this challenge with CausalPlan, a framework that integrates explicit structural causal reasoning into the LLM planning process. At the core of CausalPlan is the Structural Causal Action (SCA) model, which learns a causal graph from agent trajectories to capture how prior actions and current environment states influence future decisions. This model is then used to inform the planning process, shaping proposed LLM-generated plans through causal scoring, reweighting, and fallback to grounded alternatives when needed. By embedding this causal knowledge directly into the decision loop, CausalPlan constrains planning to intervention-consistent behaviors without requiring fine-tuning. We evaluated CausalPlan on the Overcooked-AI benchmark across five multi-agent coordination tasks and four LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B and Llama-70B. Experimental results show that CausalPlan consistently reduces invalid actions and improves collaboration in both AI-AI and human-AI settings, outperforming strong reinforcement learning baselines. Our findings highlight the value of causality-driven planning for deploying efficient, interpretable, and generalisable multi-agent LLM systems.", "tldr": "We propose CausalPlan, a framework that embeds structural causal model into LLM planning. Using a learned causal graph to score and reweight LLM-generated plans, CausalPlan reduces invalid actions and improves coordination.", "keywords": ["Reinforcement Learning", "Large Language Models", "Causality", "Collaborative Planning", "Embodied Agent"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed21d28ffcabb505461fa0c5ad6e2a7658046119.pdf", "supplementary_material": "/attachment/918745472c9ab8409b611924018172e9d6b7f3ca.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces CausalPlan, a planner for two-agent collaborative tasks that reduces invalid actions by learning and exploiting a sparse policy structure over decisions. Given a set of trajectories, each timestep is factorized into binary features (agent states and environment state) and a one-hot previous action of the agent being controlled. Then a per-action head is trained with a sparsity mask using NLL (alternating masks/weights), indicating how much each input feature is used when choosing that action. The masks form a matrix whose entries reveal the propensity of choosing an input feature when choosing a decision (action). At test time, the agent a) prompts an LLM to propose candidate high-level actions, b) the actions are pruned based on feasibility using external rules/grounding, and c) each remaining candidate is scored by summing the mask weights from the currently active features. The score is blended with LLM scores  via a convex combination. If there are no valid actions, a fallback regime is used where the chosen action is the top-scoring action from the learned matrix. Empirically, this cuts invalid actions and improves cooperative rewards across layouts and LLM backbones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Tackles a real failure mode of LLM agents in collaborative tasks\n- Fills a real gap between pure prompting and heavy world-modelling.\n- Simple training pipeline: Learns from trajectories using a standard NLL+sparsity objective\n- The policy-structure matrix gives insight into which inputs support which actions.\n- The method shows consistent empirical gains over strong baselines.\n- The system is partner-aware (by including the partnerâ€™s state in the feature set) without requiring access to or having to learn the partnerâ€™s policy\n- Inference is simply masked feature sum+convex blend of LLM action probabilities, far cheaper than replanning/multiple model rollouts.\n- Deployability is practical, given that the same assumptions hold. There is minimal friction into existing agent stacks given the learned matrix; one matrix lookup, one re-weighing step.\n- The appendix is comprehensive and code was provided, facilitating reproducibility."}, "weaknesses": {"value": "I will combine weaknesses, remarks, and questions in one section for readability.\n\nThe comments below reflect my current reading of the paper and appendix; if Iâ€™ve misread any definitions or misinterpreted any claims, I  welcome pointers and will happily revise.\n\nTo my understanding, the paper does not build a causal model of the world, though the writing sometimes suggests it is. The learned object is a policy-structure over observed features that predicts the next action, not a dynamics model one could query with do operators or counterfactuals. The SCA takes parents $(a_{t-1}, s_t)$. This models cause-effect relationships within the agentâ€™s decision process, not the environmentâ€™s physics or tasks dynamics\n\nAccording to the definition at L229-L232, each row of $M$ corresponds to a possible next action and each column to a state of past-action features. Each entry is the learned probability that feature $j$ influences action $i$, given the learned structure. Querying $M$ sums the active parent entries to produce a \"causal score\". In effect, from my understanding, higher sum implies that the model has learned that the currently active features are predictive parents of that next action. This is not a causal effect estimate in the Pearl sense.\n\nThe proofâ€™s conclusion (L812-L814) that \"the causal action matrix â€¦ faithfully reflect the true cause-effect relations among states and actions\" reads too strongly. What is captured is a sparse dependence over $(s_t, a_{t-1})\\to a_t$, which is a property of the actorâ€™s policy, not of the worldâ€™s causal structure.\n\nA (hard) intervention (if atempted) would possibly be toggling columns $j$ (making a feature active/inactive) and seeing how the score changes. The paper does not define or use interventional queries over an SCM over the environmentâ€™s variables. Concretely, the method learns a decision structure, not environment causality.\n\nCan you clarify the above in the paper?\n\nMinor: â€œinterventionâ€ is used informally (L277, re-prompting) which can be confusing in a section that discusses causal effects and structural causal action models.\n\nFurthermore, there are two distinct issues with the proof:\n\nA) Proof-method mismatches\n- Estimator mismatch; The appendix analyses ridge on fixed basis features and reads parents from the support of $W$. The method itself trains neural Bernoulli heads via NLL and learns $\\eta$ jointly. These are not equivalent and, as far as I can tell, one does not imply the other.\n- ANM vs classifier; The proof invokes ANM-style identifiability but the trained model is a binary classifier\n- Acyclicity gap: The proof assumes a DAG. The methodâ€™s heuristic \"zero the smaller of each bidirectional pair\" only removes 2-cycles, leaving (3+)-cycles in the action-action portion of the graph). Take for example the following relations: $W(a \\to b) = 0.5 > 0.3 = W(b \\to a)$, $W(b \\to c) â€Žâ€‰=â€‰0.5 > 0.3 = W(c \\to b)$, $W(c \\to a) = 0.5 > 0.3 = W(a \\to c)$. The heuristic would remove the three arrows $b \\to a$, $c \\to b$, $a \\to c$ but $a \\to b \\to c \\to a$ remains. This is inconsequential when the learned matrix is used in a feedforward manner as done in the paper, but the claim (L236-238)\"â€¦ensure DAG property of a standard SCMâ€¦\" is not correct.\n- Observability: The proposition (L220) states $a_t$ is unobservable, but the proof in the appendix uses an observed $a_t$ to train the SCA model. Clarifying unobserved at test time, observed during training would help.\n\nAny of the above, in my view, render the statement proven in the appendix inapplicable to the method in the main paper (apart from the observability claim but thatâ€™s a wording inconsistency).\n\nQuestions: Either restate and prove a proposition for the actual model class and estimators, align the method to the ridge/basis estimator in the appendix or clearly state that this is a motivating surrogate that does not apply to the method.\n\n\nB) Standalone proof issues\n\nWhen considering the proof itself in isolation:\n\n- The specific regularizing conditions/assumptions are not clearly stated (i.e., L728: \"identifiability of causal direction relies on the function class having sufficient expressiveness and satisfying certain regularity conditions (e.g., nonlinearity, invertibility)\". Invertibility, as far as I can tell, is not relevant to the proof. Please state the exact assumptions used.\n- Ridge regression is used and the parents are identified by the support of $W_i$ (L808) with the claim (L809): \"one can recover the graph structure by examining which entries of $W_i$ are significantly nonzero, using thresholding or statistical tests.\". L2 regularisation has no sparsity guarantees and typically yields dense solutions. How exactly is this step justified and implemented?\n- The proof assumes causal faithfulness but the collaboratorâ€™s actions are not modelled (L899-901). If $u$ denotes the collaboratorâ€™s action, an implicit assumption is made: $a_t \\perp u_{t-1} \\mid (s_t, a_{t-1})$. If $s_t$ is intended to be sufficient to mediate all effects of the parentâ€™s last action, clearly state so, otherwise sufficiency is violated.\n- Eq (8) instantiates the dataset as sequences of the form $(s_t, a_{t-1}, a_t)$. Clearly state what is observable and what is not.\n\nFurthermore:\n- L244-246: Can you clarify how the actions are sampled by the LLM and how they are scored?\n- In the action pruning step, the method assumes access to an external verifier of feasibility. What does this verifier look like? What happens when such a verifier is unavailable (i.e., real-world robotics)? If it is required, can you add this as an explicit assumption?\n- Why does CausalPlan underperform in some settings (Table 1)? A rief discussion of failure modes would help.\n- L318: A short description of the baselines in the main text (with details in the appendix) would make the experiments easier to follow.\n- What is the impact of removing the previous action feature/removing partner-state features or adding the partnerâ€™s previous action?\n- How well does an SCA trained on trajectories from a behaviour policy paired with a partner transfer when deployed with different partners?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TrxH9ah2kI", "forum": "rLG74JvrxS", "replyto": "rLG74JvrxS", "signatures": ["ICLR.cc/2026/Conference/Submission16296/Reviewer_AtpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16296/Reviewer_AtpH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578614466, "cdate": 1761578614466, "tmdate": 1762926438525, "mdate": 1762926438525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CausalPlan, a framework that purports to integrate explicit structural causal reasoning into LLM-based multi-agent planning. The core contribution is a Structural Causal Action (SCA) model that learns relationships between prior actions, current states, and future actions from agent trajectories. These learned relationships are encoded in a ``Causal Action Matrix'' $M$, which is then used to reweight LLM-generated action probabilities during planning. The authors evaluate their approach on the Overcooked-AI benchmark across multiple LLM backbones and show empirical improvements in task success rates and reductions in invalid actions."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Consistent gains across very different LLM backbones and layouts, not just one setup.\n\n2. Human-partner results are stronger than baselines and include statistical testing; several settings reach p<0.05 and none show degradation when the method is enabled. \n\n3. The causal backup plan is an effective recovery mechanism when the planner proposes no valid actions; ablation shows it adds measurable benefit beyond the two-prompt tweak. \n\n4. The framework exposes a causal action matrix and publishes heatmaps, giving a degree of interpretability about which state/action factors influence next actions. \n\n5. Robustness to who collects the data for the buffer; using a stronger behavior policy helps, but even weaker LLM-collected data still benefits from the causal integration. \n\n6. Sensitivity analysis of the Î³ weighting shows a broad sweet spot. \n\n7. Explicit DAG enforcement by zeroing the weaker direction in any bidirectional pair prevents cycles in the learned structure. \n\n8. Modular drop-in over ProAgent with open-source LLMs, keeping the rest of the stack intact and making replication or extension straightforward. \n\n9. Extends beyond Overcooked to a long-horizon single-agent benchmark (Crafter) and outperforms both a causal-prompting baseline and Dreamer-V2 at 1M steps. \n\n12. Prompting design separates analysis from action selection, making the action extraction unambiguous; ablation indicates the components introduced to capture causal relationships drive most of the lift."}, "weaknesses": {"value": "1. The framework learns from trajectories generated by a fixed behavior policy in Overcooked-AI, which means each action is conditioned on the policyâ€™s internal decision process. Since actions arenâ€™t randomized or independently manipulated, the data are observational, not interventional.\n\n2. The Structural Causal Action model optimizes a likelihood loss ( -\\log P(a_t \\mid s_t, a_{t-1}) ), which captures conditional correlations rather than causal effects ( P(a_t \\mid s_t, \\text{do}(a_{t-1})) ). Without interventions or counterfactual adjustments, the learned structure reflects co-occurrence patterns, not causal mechanisms.\n\n3. Although Overcooked-AIâ€™s environment is deterministic, the data collection process is not interventionally controlled. The simulator ensures that actions deterministically affect states, but the trajectories used for learning are policy-dependent rollouts, not samples from systematically applied interventions.\n\n4. Because the same policy governs both state visitation and action choice, correlations between (s_t) and (a_t) can arise from shared dependencies on unobserved latent factors such as internal LLM reasoning or high-level strategy. The model treats these as causal links.\n\n5. The binary feature encoding used for states and actions is a coarse abstraction of the full simulator state. Hidden variables like spatial positioning or timing can confound stateâ€“action dependencies, violating causal sufficiency.\n\n6. The frameworkâ€™s only verification is improved prediction accuracy and task performance, which measure behavioral alignment, not causal correctness. A model can be highly predictive while causally wrong.\n\n7. The paperâ€™s theoretical identifiability proof relies on assumptions such as additive noise, faithfulness, full observability, and acyclicity, none of which are verified in Overcooked-AI. There is no empirical evidence that these assumptions hold in practice.\n\n8. Each entry of the causal action matrix represents a learned dependency weight, not an intervention-derived causal coefficient. The matrix is effectively a correlation matrix with sparsity regularization.\n\n9. The observed reduction in invalid actions and improved cooperation may result from regularized prediction smoothing or bias correction, not genuine causal reasoning. The gains demonstrate utility, not causal validity.\n\n10. Because the learned structure reflects policy-specific correlations, the matrix may not transfer to different partners, environments, or task variations, contradicting the stated goal of causal generalization."}, "questions": {"value": "1. How do you distinguish causal effects from correlations when all data come from a fixed behavior policy Ï€â‚áµ¦â‚Ž? What is your formal definition of causation in this context?\n\n2. Can you provide empirical evidence that the faithfulness, causal sufficiency, and additive noise assumptions hold in Overcooked-AI? For instance, conditional independence tests, checks for unobserved confounders, or validation of the additive noise model?\n\n3. What would an intervention experiment look like to validate your learned causal structure? For example, could you force an agent to take actions inconsistent with M and measure the deviation in outcomes?\n\n4. Why not compare against a model that learns P(aâ‚œ | sâ‚œ, aâ‚œâ‚‹â‚) with standard neural networks (e.g., feedforward or recurrent) without causal constraints? Does the DAG structure and sparsity actually matter, or are the gains from additional learned features?\n\n5. How does performance degrade when the partner policy changes? Does your â€œcausalâ€ matrix M transfer to new partners, or is it partner-specific?\n\n6. Can you show that the learned dependencies correspond to true causal mechanisms rather than artifacts of Ï€â‚áµ¦â‚Ž? For instance, by comparing M learned from different behavior policies?\n\n7. Have you tested whether M changes systematically under distributional shift? This would be evidence of instability inconsistent with causal invariance.\n\n8. Why is binary feature encoding sufficient when it discards causally relevant information such as spatial distances, timing, and interaction history?\n\n9. What is the causal graph G you claim to identify? Can you draw it explicitly (not just heatmaps of M) and verify it against ground truth or domain knowledge?\n\n10. In Proposition 1, you assume aâ‚œ is â€œunobservableâ€ during training, but clearly you observe aâ‚œ in the trajectory data ð“‘. Can you clarify this apparent contradiction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wbn3qpHgSE", "forum": "rLG74JvrxS", "replyto": "rLG74JvrxS", "signatures": ["ICLR.cc/2026/Conference/Submission16296/Reviewer_RXj8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16296/Reviewer_RXj8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013346813, "cdate": 1762013346813, "tmdate": 1762926438124, "mdate": 1762926438124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CausalPlan, a framework designed to improve LLM-based multi-agent collaboration by incorporating explicit causal reasoning into the planning process. The method introduces a Structural Causal Action (SCA) model that learns a causal graph from offline trajectories, modeling dependencies between state factors, prior actions, and next action choices. During inference, the causal graph is used to reweight sampled candidate actions from the LLM, promoting causally consistent planning and filtering out invalid or incoherent actions. \nExperiments are conducted on the Overcooked benchmark showing consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a real pain point in the LLM-planning space  LLM agents often rely on correlations and fail under causal inconsistencies  and proposes a targeted solution. The motivation aligns well with current challenges in multi-agent LLM systems.\n2. The method section is well-structured and easy to follow. The paper clearly explains the proposed causal model and the way it integrates with LLM action sampling. The theoretical argument that, under standard identifiability assumptions, the causal graph and functional relationships can be uniquely recovered adds credibility and supports why the approach should work.\n3. Implementation details are provided in good depth, including model architecture and prompting strategies."}, "weaknesses": {"value": "1. It does not compare against the most recent LLM-agent + causal reasoning  methods. For example, CausalMACE[1] and Causal-aware LLMs[2]. \n2. All evaluations are done in the Overcooked kitchen environment. While this benchmark is standard, it is still a fairly constrained action/state space in a stylized cooperative setting. It would be helpful to see results in a more diverse or general multi-agent domain (e.g., social games, robotics simulators). Otherwise, it's unclear how easily the method generalizes to richer or more realistic scenarios.\n3. The method depends on manual factorization of state/action features, and lower-level actions are ignored. This raises concerns about domain specificity and manual engineering effort. In complex environments, designing semantic factors may be non-trivial, and itâ€™s unclear how the method scales without strong prior knowledge.\n\n\n[1]https://aclanthology.org/2025.findings-emnlp.777/\n\n[2]https://www.ijcai.org/proceedings/2025/0478.pdf"}, "questions": {"value": "1. The paper claims not to rely on the LLMâ€™s causal reasoning ability, yet the pipeline still depends heavily on the LLM for analysis and candidate-action generation via a two-prompt design and knowledge library. Could the authors clarify whether the method truly disentangles causal reasoning from linguistic reasoning? To what extent could the observed gains stem from improved prompting workflow rather than causal modeling itself?\n2. Each decision step requires: extracting factorized features and querying the causal matrix, is the runtime overhead significant?\n3. Does the training buffer come from the same task distribution as evaluation? Are trajectories from them fully disjoint from test episodes? Could the causal structure overfit to the demonstration policy rather than reflect true task dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ughFMLoTYh", "forum": "rLG74JvrxS", "replyto": "rLG74JvrxS", "signatures": ["ICLR.cc/2026/Conference/Submission16296/Reviewer_Tiqn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16296/Reviewer_Tiqn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245705695, "cdate": 1762245705695, "tmdate": 1762926437657, "mdate": 1762926437657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key failure mode in LLM-based agents, particularly smaller models, which often generate causally invalid actions in multi-agent collaborative tasks. To address this, the authors propose **CausalPlan**, a two-phase framework. In Phase 1, \"Causal Action Structure Learning,\" a Structural Causal Action (SCA) model is learned from a dataset of agent trajectories to capture the influence of previous actions ($a_{t-1}$) and current states ($s_t$) on the next action ($a_t$). This is stored in a Causal Action Matrix (M). In Phase 2, \"Agent Planning with Causal Knowledge,\" this matrix M is used to guide the LLM's action selection. This is done via two modules: 1) \"Causal-Aware Planning,\" which re-weights the LLM's output probabilities with the causal scores from M, and 2) \"Causal Backup Plan,\" a fallback mechanism that greedily selects the highest-scoring causal action if the LLM fails to produce a valid one. Experiments on the Overcooked-AI benchmark and Crafter demonstrate that CausalPlan reduces invalid action."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed two-phase framework is intuitive and modular.\n2.\tThe paper is easy to follow.\n3.\tThe empirical results are extensive and show consistent performance gains across multiple LLM backbones (Gemma, Llama, Qwen) and evaluation settings (AI-AI collaboration and Human-AI collaboration), outperforming baselines on the Overcooked benchmark."}, "weaknesses": {"value": "1. The paper's primary claim rests on \"causality-driven planning\". However, the SCA model learns a supervised mapping from $(s_t, a_{t-1})$ to $a_t$ based on data collected from a single behavior policy (MEP). It is highly questionable whether this process discovers true \"causal\" structure as defined by Pearl or simply learns the strong correlations and biases within that specific policy's data. The proof of identifiability (Proposition 1) relies on strong, standard assumptions (e.g., causal sufficiency, additive noise) that are difficult to justify in a complex, dynamic environment like Overcooked.\n\n2. A major limitation, which is not adequately discussed, is that the Causal Action Matrix $M$ appears to be learned **per layout**. The heatmaps in Fig. 10 and 11 are specific to the \"CR layout\", and the offline training takes 3 hours per environment. This severely limits the method's scalability and flexibility, which is one of the primary advantages of using LLM-based agents. The authors provide no evidence or discussion on whether $M$ learned on one layout can generalize to another.\n\n3. The central idea of learning an external model from trajectory data to score and refine LLM-generated plans is not novel. The paper's related work section is missing key work [1] on this specific problem.\n-\tReAd [1] directly tackles the same problem of inefficient LLM grounding in multi-agent environments like Overcooked.\n-\tThe proposed \"Structural Causal Action (SCA) model\" is conceptually very similar to the local advantage function used in [1]. Both frameworks learn a function from agent trajectory data (collected from a behavior policy $\\pi_\\beta$ here) to score the utility of the proposed plan. While this paper formulates the scorer as a causal model $P(a_t | s_t, a_{t-1})$, ReAd [1] formulates it as an RL-based advantage function, the high-level approach of using a learned, data-driven scorer to refine LLM plans is highly overlapping. The authors must discuss this and other related works to properly situate their contribution."}, "questions": {"value": "1.\tCould the authors please clarify the novelty of the SCA model compared to other data-driven refinement models, such as ReAd [1] ? A thorough comparison in the related work section is necessary.\n2.\tCan the authors provide more evidence that the SCA model is learning true causal relationships rather than just the strong policy-specific correlations from the MEP dataset? What happens if a sub-optimal or random policy is used to generate the dataset $B$?\n3.\tDoes the Causal Action Matrix M learned for one layout (e.g., Cramped Room) have any utility when transferred to another layout (e.g., Asymmetric Advantages)? If not, doesn't this per-layout offline training requirement undermine the zero-shot generalization promise of using LLMs?\n\n[1] Zhang, Y., Yang, S., Bai, C., Wu, F., Li, X., Wang, Z., & Li, X. (2024). Towards efficient llm grounding for embodied multi-agent collaboration. ACL 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u7IRStD5UL", "forum": "rLG74JvrxS", "replyto": "rLG74JvrxS", "signatures": ["ICLR.cc/2026/Conference/Submission16296/Reviewer_gMwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16296/Reviewer_gMwk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762263438117, "cdate": 1762263438117, "tmdate": 1762926437337, "mdate": 1762926437337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,  \n\nThank you for taking the time to provide detailed feedback on our manuscript. **We have uploaded the revised version, with all modified content highlighted in red**. Overall, we have tried to address the questions and weaknesses raised in the previous version. In particular, we have clarified the causal interpretation of our method, distinguished it from missing related work, framed the appendix proof as a conceptual illustration with additional context on ridge regression and Bernoulli NLL, and provided empirical evidence regarding causation and policy correlation. \n\nWe hope these revisions make the contributions and findings clearer and more transparent, and we welcome further feedback to continue improving the manuscript."}}, "id": "ANmYjqn3mz", "forum": "rLG74JvrxS", "replyto": "rLG74JvrxS", "signatures": ["ICLR.cc/2026/Conference/Submission16296/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16296/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission16296/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763569081226, "cdate": 1763569081226, "tmdate": 1763569101826, "mdate": 1763569101826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}