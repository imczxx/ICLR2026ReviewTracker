{"id": "JXA9HbpeOp", "number": 2524, "cdate": 1757133051214, "mdate": 1763017364090, "content": {"title": "Lookahead-LSTM Optimizer: A Meta-Learning K-steps Method", "abstract": "All of the parametric machine learning models need to be optimized. At present, there are various optimization algorithms base on gradient, and they all have one thing in common---hand-designed. The purpose of Meta-learning is learning to learn, this thought can be used to do optimization automatically, without algorithm selection and hyper-parameters tuning. Previous works have trained a model to optimize other models using gradient information, but there still have some issues. We want to further improve the previous works in generalization and data transferability. then we propose k-steps Lookahead-LSTM optimization algorithm for this", "tldr": "", "keywords": ["Optimizer", "LSTM", "gereralization", "optimization", "gradient"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b06bc7d0c7905f2d3ed18b85b738a2fa081e1063.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The submission aims to meta-learn optimizers parameterized by LSTM to eliminate the bias introduced by the hand-drafted optimizers. However, this is a very general motivation in the Learn to Optimize area. The submission failed to clarify their motivation and main contribution, with very limited novelty. Besides, the experiment settings are not diverse and concrete."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The osiliting problem in the meta-learned optimizers is interesting."}, "weaknesses": {"value": "1. The writing is not appropriate for a lot of types, and a lot of sentences are confusing. \n2. The motivation of the submission is not clear in terms of identifying the problems in the current learning to optimize the community. \n3. The novelty is limited, and the references of the related work are old. \n4. The experiment settings are not concrete; the stand L2O benchmarks are not utilized."}, "questions": {"value": "The paper remains at an unfinished level. The method part is not well-written. What is the main technical contribution of this submission? \nIs the proposed method able to defeat the handcrafted optimizer on the standard benchmarks, such as CIFAR-10 and CIFAR-100?\nWhat is the runtime cost and training cost of the learned optimizer? \nCan the learned optimizer from one domain be deployed to a novel domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WKTvKDqoKn", "forum": "JXA9HbpeOp", "replyto": "JXA9HbpeOp", "signatures": ["ICLR.cc/2026/Conference/Submission2524/Reviewer_DgnK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2524/Reviewer_DgnK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755424092, "cdate": 1761755424092, "tmdate": 1762916266865, "mdate": 1762916266865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "c3mUdK8e4N", "forum": "JXA9HbpeOp", "replyto": "JXA9HbpeOp", "signatures": ["ICLR.cc/2026/Conference/Submission2524/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2524/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763017363240, "cdate": 1763017363240, "tmdate": 1763017363240, "mdate": 1763017363240, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author identify some limitation in existing optimizers. Most optimizer are hand-designed, which is hard to adapt to different training scenario. In this paper, the author propose to use transfering learning techinique to learn an optimizer under different network. So the author use the LSTM network combine with k-step look ahead to form a new optimizer which can combine the experience of optimizer from other problems."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The overall idea is interesting. Designing a learned optimizer with transfer learning capabilities represents a novel and promising direction."}, "weaknesses": {"value": "1: The current version of the paper appears to be in an early draft stage. It contains a significant amount of exploratory discussion and incomplete experimental results, and does not yet present a clear or solid conclusion.\n\n2: Based on the description and algorithmic details, it is unclear whether this work truly introduces a meta-learning-based optimizer. The method appears to closely resemble the Lookahead optimizer, and I do not see clear elements of meta-learning or transfer learning as claimed. The authors should clarify what differentiates their approach from existing optimizers and how meta-learning is incorporated.\n\n3: The experimental evaluation is not sufficiently convincing. In the reported results, a simple momentum-based optimizer performs better than the proposed method while also being more computationally efficient. More rigorous comparisons and stronger empirical evidence are needed to support the claims."}, "questions": {"value": "Based on the current version of the paper, I would like the authors to clarify their method and clearly explain how it relates to meta-learning. In particular, please specify which component reflects a meta-learning mechanism and how it differs from existing optimizers such as Lookahead."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TSnSXxzfiY", "forum": "JXA9HbpeOp", "replyto": "JXA9HbpeOp", "signatures": ["ICLR.cc/2026/Conference/Submission2524/Reviewer_3Ls2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2524/Reviewer_3Ls2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948519729, "cdate": 1761948519729, "tmdate": 1762916266205, "mdate": 1762916266205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of insufficient generalization of optimizers in learning to optimize, proposing meta-learning and transfer learning strategies across multiple training tasks, as well as introducing an improved K-steps Lookahead-LSTM method. Overall, the contribution is limited."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors attempt to improve the optimizer's performance on new tasks by increasing the diversity of training tasks and utilizing transfer training."}, "weaknesses": {"value": "The proposed approach lacks sufficient novelty, and the experimental validation is not comprehensive. The writing of the paper also requires further improvement, as some sections are not clearly presented. Although it is claimed that this work was done in 2020, the references and compared methods are primarily focused on studies prior to 2020, without considering recent developments in this field after 2020."}, "questions": {"value": "please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jo6m0STFz9", "forum": "JXA9HbpeOp", "replyto": "JXA9HbpeOp", "signatures": ["ICLR.cc/2026/Conference/Submission2524/Reviewer_L13N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2524/Reviewer_L13N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762267897514, "cdate": 1762267897514, "tmdate": 1762916266060, "mdate": 1762916266060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes k-step LSTM optimizer, an effective lookahead optimizer for deep learning. They show performance improvements over alternative optimizers on a suite of tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I think the paper is interesting in that it proposes a lookahead optimizer for deep learning tasks and reaffirms some of the motivations behind the motive of the original Zhang et al 2019 paper."}, "weaknesses": {"value": "I feel the DL field is in general quite wary of yet another optimizer especially one whose ideas that have been more or less discussed in Zhang et al 2019, the paper lacks clarity in writing and requires more technical solidity in comparison."}, "questions": {"value": "=== *clarity on LSTM* ===\n\nI would love to clarity with the author that the LSTM refers to optimizing LSTM networks, not that the optimizer itself is making use of LSTM architecture? The presentation on this part seems a bit confusing in the early sections of the paper.\n\n=== *baseline comparison* ===\n\nOnce again I think DL field is quite wary of a new optimizer so ablations are required to be highly rigorous. For all runs in the comparison eg Fig 2 there appears to be a single seed to the experiment and though the setups are generally about test time generalization, they also seem to be rather synthetic and would be of interest to test optimizers in more realistic scenarios (train on sigmoid test on relu does not sound very common in practice despite being a decent synthetic task).\n\nAll optimizers must be optimized heavily wrt their own set of hyperparameters and a clear explosion or pathological evolution of the loss seem likely to be a result of ill-tuned hyperparameters. In other words, the baseline must be tuned more heavily to make it a good comparison. There also does not seem to be comparison against Zhang et al 2019, and if I understand right the paper adopts quite similar idea so is just another application of the method? Can the author please clarify.\n\n=== *losses* ====\n\nAs stated, in some comparisons, loss spikes seem related to ill-tuned optimizer hypers and it seems that LSTMOptimizer has lower starting loss  (fig 3, 4) compared to alternatives, any reason for that?\n\n=== *presentations* ====\n\nI think captions in various places Fig 6 can be improved to make the plots more informative. Also I think there are very simple typos in various places that need fixing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wncDUqj6Vi", "forum": "JXA9HbpeOp", "replyto": "JXA9HbpeOp", "signatures": ["ICLR.cc/2026/Conference/Submission2524/Reviewer_WgZ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2524/Reviewer_WgZ6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762599739384, "cdate": 1762599739384, "tmdate": 1762916265870, "mdate": 1762916265870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}