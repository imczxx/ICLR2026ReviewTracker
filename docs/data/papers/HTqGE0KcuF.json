{"id": "HTqGE0KcuF", "number": 2912, "cdate": 1757300083364, "mdate": 1759898119596, "content": {"title": "WAFT: Warping-Alone Field Transforms for Optical Flow", "abstract": "We introduce Warping-Alone Field Transforms (WAFT), a simple and effective\nmethod for optical flow. WAFT is similar to RAFT but replaces cost volume with\nhigh-resolution warping, achieving better accuracy with lower memory cost. This\ndesign challenges the conventional wisdom that constructing cost volumes is nec-\nessary for strong performance. WAFT is a simple and flexible meta-architecture\nwith minimal inductive biases and reliance on custom designs. Compared with\nexisting methods, WAFT ranks 1st on Spring, Sintel, and KITTI benchmarks,\nachieves the best zero-shot generalization on KITTI, while being up to 4.1× faster\nthan methods with similar performance. Code and model weights will be available\nupon acceptance.", "tldr": "", "keywords": ["Optical Flow; Computer Vision; Warping; Dense Correspondences"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f32573ea4feba0b3840472f66b47ae1624d3887.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a simplified meta-architecture for optical flow estimation without using cost volumes. The proposed WAFT algorithm consists of an input encoder which leverages existing large-scale pre-trained models for feature extraction, and a recurrent update module based on vision transformers that can iteratively updates optical flow with large displacements. Experiment shows that the proposed WAFT algorithm achieves top rankings on various benchmarks, including Spring and KITTI, furthermore, it does so with significantly lower memory cost and up to 4.1x faster inference times."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The design of WAFT without cost volume computation is very simple, flexible and effective, making it a significant contribution for computer vision research community\n2. By avoiding cost volumes computation, WAFT can perform warping on original resolution feature maps, which can help achieving sharper boundary predictions in optical flow estimation\n3. WAFT has shown best zero-shot cross-dataset generalization on KITTI, which is an important property towards generalization capability on unseen data.\n4. The paper is well-structured and clearly written."}, "weaknesses": {"value": "1. The iterative recurrent update module may restrict the algorithm's potential for parallel optimization to achieve low latency. \n2. WAFT relies on existing pre-trained vision foundation models, which may limits its potential for further computational efficiency improvement on feature extraction.\n3. Compared with improved memory and computational efficiency improvement, the improvement on flow accuracy is relatively limited."}, "questions": {"value": "1. In table 2, while the ratio of MACs reduction is high (CCMR+'s 12653 vs. WAFT-DAv2-a1's 853), the speed up of latency is not at the same scale (CCMR+'s 999 vs. WAFT-DAv2-a1's 240), it would be good to give further explanation on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FJe8nkLrSP", "forum": "HTqGE0KcuF", "replyto": "HTqGE0KcuF", "signatures": ["ICLR.cc/2026/Conference/Submission2912/Reviewer_abBA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2912/Reviewer_abBA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468571336, "cdate": 1761468571336, "tmdate": 1762916439919, "mdate": 1762916439919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper points out the drawback of constructing cost volume in the optical flow field, and propose to replace the cost volume with warping. To achieve competitive performance, the authors propose to utilize: 1) stronger feature encoder 2) high-resolution warping 3) attention- (and pretrained) based updater."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper is well written.\n2. This paper has a clear and extensive ablation study to show the effectiveness of each design choice.\n3. The author introduced an attention-based updater to replace the cost volume for feature similarity computation. This design is resonable and novel."}, "weaknesses": {"value": "1. Since the author replaces the commonly used CNN updater with attention-based one, it is better to provides more details of the layers.\n2. For the models used in Table 2, what is the downsampled ratio? And which line corresponds to the statement in the abstract \"while being up to 4.1× faster than methods with similar performance\". From my understanding, WAFT-Twins-a2 uses the same feature encoder as FlowFormer++, achieves similar performance but not significant speedup?\n3. Can the authors provide more explaination about why context encoder is not useful in the WAFT architecture?"}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no ethics concerns for me."}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lZWXgYLKrC", "forum": "HTqGE0KcuF", "replyto": "HTqGE0KcuF", "signatures": ["ICLR.cc/2026/Conference/Submission2912/Reviewer_oRyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2912/Reviewer_oRyW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900415556, "cdate": 1761900415556, "tmdate": 1762916439734, "mdate": 1762916439734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to abandon the cost volume that's a standard component in deep optical flow architectures, and uses the warped target feature vector alone (plus the source feature vector) for flow estimation. It implicitly builds a global context via self-attention in the recurrent update module, which is why it can get rid of the cost volume."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Removing the cost volume is a good contribution, which may make estimation of optical flow on high-res images much more feasible."}, "weaknesses": {"value": "1. No detailed evaluation of how the model performs on large displacements, on which WAFT might be slightly weaker than models using a cost volume. The authors could make artificial displacements to stress-test WAFT to see where its limit lies.\n2. No details are given for the Recurrent Update Module. For example, how many layers (esp. self attention layers), what's the total param count?"}, "questions": {"value": "1. Since no cost volume is adopted, I'm worried that the initial errors may accumulate and become larger as the model iterates. The authors could consider such a perturbation test: in the first iteration, perturb the flow prediction with random values, then see how well the model recovers from it.\n2. Another challenging scenario is if there are multiple similar objects (e.g. a table with many cups), how well WASP would perform. Chance is the self attention may overly smooth features across these similar objects and make the prediction more random. Of course this would be challenging for methods **with** a cost volume, but I'm curious if it would be more challenging for WASP.\n\n (Note: I would not lower my rating if the model is not so robust under such perturbations; this is just to better inform readers whre are the \"sweet spots\" in which the method performs well.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CUraorfbdT", "forum": "HTqGE0KcuF", "replyto": "HTqGE0KcuF", "signatures": ["ICLR.cc/2026/Conference/Submission2912/Reviewer_ZeZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2912/Reviewer_ZeZq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990837598, "cdate": 1761990837598, "tmdate": 1762916439552, "mdate": 1762916439552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}