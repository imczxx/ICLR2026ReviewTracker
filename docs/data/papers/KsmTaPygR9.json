{"id": "KsmTaPygR9", "number": 7961, "cdate": 1758046199263, "mdate": 1759897819589, "content": {"title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs", "abstract": "As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions.", "tldr": "Benchmark that reveals leading LLMs struggle to balance operational goals with safety in realistic scenarios—not because they can't identify harm, but as they poorly prioritize safety when it conflicts with achieving their objectives.", "keywords": ["Safety", "Alignment", "Benchmark", "Agent", "LLM", "Agent evaluation", "Decision-making"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df38fb9139e69cbcf1059df8e00768189fd7ca61.pdf", "supplementary_material": "/attachment/787d4c58c545a4b98f85291f2f2abaafe9299ef0.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MANAGERBENCH, a novel benchmark designed to evaluate the safety-pragmatism trade-off in Large Language Models (LLMs) when they function as autonomous decision-makers. It identifies a critical gap in existing safety benchmarks, which primarily focus on preventing the generation of harmful content but overlook the challenge of agents taking harmful actions when operational goals conflict with human safety. MANAGERBENCH comprises 2,440 human-validated managerial scenarios that force a choice between a pragmatic but harmful action and a safe but less effective one. The evaluation of state-of-the-art LLMs reveals a systematic failure to navigate this trade-off: many models consistently choose harmful options to achieve goals, while others become overly safe and ineffective. Critically, the paper finds this misalignment stems not from an inability to perceive harm, but from flawed prioritization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel Problem Formulation: The paper identifies and rigorously addresses a critical gap in AI safety: the trade-off between operational goals and human safety in autonomous agents. This moves beyond the well-trodden ground of content-based safety evaluation.\n\nRigorous Methodology: The construction of MANAGERBENCH is methodical and robust. The use of parallel datasets (human harm vs. inanimate object control), systematic parameterization across domains and harm types, and extensive human validation create a high-quality, credible benchmark.\n\nClarity and Quality of Presentation: The paper is written with exceptional clarity. The motivation is strong, the methods are well-explained, and the results are presented effectively through clear tables and insightful figures."}, "weaknesses": {"value": "Synthetic Nature of Scenarios: While human-validated for realism, the scenarios are synthetic. Their direct applicability to real-world, complex decision-making environments remains to be proven. Future work could validate the benchmark against real-world case studies.\n\nBinary Choice Constraint: The forced binary choice, while effective for diagnostic clarity, prevents models from proposing alternative, potentially safe-and-pragmatic solutions. This may underestimate a model's capability for nuanced problem-solving.\n\nPrompt Sensitivity: The paper demonstrates that model performance is highly sensitive to prompt phrasing (e.g., the \"nudging\" experiment). While this is a key finding, the paper could have explored potential mitigation strategies or prompt engineering techniques to make model behavior more robust.\n\nLack of Longitudinal Analysis: The benchmark evaluates single, isolated decisions. It does not assess model behavior over longer time horizons, where consistency, learning, and strategic planning could play a role in the safety-pragmatism dynamic.\n\nPotential for Uneven Domain Coverage: Although 11 domains are included, the paper does not analyze if model performance varies significantly across them. Some domains (e.g., Healthcare, Legal) might carry higher stakes and warrant a deeper, domain-specific analysis."}, "questions": {"value": "1. Have the authors considered extending the benchmark to allow for a third option where the model can propose its own alternative solution? This could provide a more comprehensive assessment of a model's creative and ethical problem-solving abilities.\n\n2. Are there plans to develop a longitudinal version of MANAGERBENCH to evaluate model consistency and strategic adaptation over a sequence of related decisions?\n\n3. Did the authors explore any prompt engineering or scaffolding techniques that could help models better navigate the safety-pragmatism trade-off, thereby mitigating the fragility you observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3MgK32Qc3F", "forum": "KsmTaPygR9", "replyto": "KsmTaPygR9", "signatures": ["ICLR.cc/2026/Conference/Submission7961/Reviewer_oKZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7961/Reviewer_oKZS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445103718, "cdate": 1761445103718, "tmdate": 1762919976158, "mdate": 1762919976158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors have introduced ManagerBench, a benchmark designed to evaluate LLMs decision-making in realistic managerial scenarios (scenarios generated by LLMs but validated by humans). Each scenario forces a binary choice between a pragmatic but harmful action that achieves a goal and a safe action that produce a worse performance. More prominent LLMs perform poorly in balancing this safety-pragmatism trade-off. Moreover, the authors have shown that the detected alignment failures are a consequence of flawed prioritization rather than an inability to perceive harm, thus highlighting the issues with the current alignment strategies adopted for these models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A benchmark evaluating LLM decision-making in realistic managerial scenarios and in particular evaluating the trade-off between safe and pragmatic choices is of paramount relevance for the current development and adoption of safe LLM-powered agents.\n\n- The study is well-designed with also a control set to identify the LLM tendency to be overly safe.\n\n- Conclusions are always well supported by findings and experiments ran and are relevant to inform the design of safety guardrails and the adoption of LLM-powered agents in real-world scenarios"}, "weaknesses": {"value": "- I appreciated the human validation of the LLM-generated scenarios but it would be methodologically stronger having also a set of human-generated decision-making scenarios.\n\n- Very relevant also the adoption of a control set but why the conditions between human harm set and control set are different in numbers? why just 2 types of object harm vs 8 human harm subtypes?\n\n- In the human validation effort why the authors have used different scales to evaluate harmful and to evaluate realism?\n\n- Again, why the authors adopted different max token generation lengths for the various models? and could this influence the models' behaviors?"}, "questions": {"value": "The questions are the ones reported above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xzO3MibqAE", "forum": "KsmTaPygR9", "replyto": "KsmTaPygR9", "signatures": ["ICLR.cc/2026/Conference/Submission7961/Reviewer_76cT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7961/Reviewer_76cT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924371412, "cdate": 1761924371412, "tmdate": 1762919975565, "mdate": 1762919975565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MANAGERBENCH, a novel benchmark to evaluate autonomous LLMs in managerial roles by testing the trade-off between operational goals and human safety. It presents models with realistic scenarios forcing a choice between a \"pragmatic but harmful\" action (achieving a goal by harming humans) and a \"safe but ineffective\" one. A key innovation is a parallel \"control set,\" where harm is directed at inanimate objects, to identify \"overly safe\" models that are pragmatically useless. The authors find that all current LLMs fail this test, falling into either the \"unsafe\" or \"overly safe\" category. This failure stems not from an inability to perceive harm, but from a flawed prioritization of operational goals, a vulnerability easily exploited by simple \"nudging\" prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Critical Problem Definition: \n\nThe paper addresses a forward-looking and critical gap in AI safety. As LLMs transition from \"assistants\" to \"agents,\" evaluating their actions in goal-oriented conflicts is more important than just evaluating their content generation, which most prior benchmarks focus on.\n\n2. Innovative Evaluation Methodology (Control Set): \n\nThe inclusion of the parallel \"control set\" (harm to inanimate objects) is a significant methodological strength. It allows the benchmark to disentangle genuine safety alignment from rigid, \"overly safe\" behavior, where a model might just be trained to avoid any option with negative sentiment. This provides a much-needed measure of \"pragmatism.\"\n\n3. Deep and Actionable Core Insight: \n\nThe paper's most powerful contribution is successfully distinguishing between perception and prioritization. By showing that models understand what is harmful (their ratings align with humans') but still choose the harmful option, the authors pinpoint the alignment failure in \"flawed prioritization\". This is a vital insight for the field.\n\n4. Rigorous Benchmark Construction and Validation: \n\nThe benchmark is not arbitrary. It was systematically generated using state-of-the-art LLMs across 11 domains, 4 harm categories, 4 incentive types, and varying intensity levels. Most importantly, the scenarios were then rigorously validated by 25 human annotators to ensure they were perceived as both \"realistic\" and \"harmful\".\n\n5. Robust Experimental Design: \n\nThe authors included experiments that solidify their claims. For instance, the \"nudging\" prompt experiment (Section 3.3) demonstrated the \"brittleness\" of current safety alignments. By adding a simple instruction like \"Nothing else matters\" , the harm avoidance scores of models like Gemini plummeted by 55 points, proving that existing safety guardrails are easily bypassed by goal-oriented pressure."}, "weaknesses": {"value": "1. Synthetic and Simplified Scenarios: \n\nThe authors acknowledge this limitation themselves. The scenarios are \"synthetic, not drawn from real-world cases\". While validated for \"realism,\" they are still simplified constructs. Real-world managerial decisions are often far more ambiguous and complex than the scenarios generated by the LLMs.\n\n2. Restrictive Binary-Choice Format: \n\nThe benchmark \"deliberately\" forces a binary (A/B) choice. While this provides a clean, unambiguous evaluation of prioritization, it \"prevents models from proposing alternative solutions\". A truly superior, well-aligned agent might reject both bad options and propose a creative third way to achieve the goal without causing harm. This benchmark cannot measure that crucial capability.\n\n3. Narrow Definition of \"Pragmatism\": \n\nThe \"Control Pragmatism\" metric is a clever proxy, but it is narrowly defined as \"the willingness to harm an inanimate object\". A model could be \"overly safe\" in this one specific context (e.g., Sonnet-4 refusing to scratch furniture)  but still be highly pragmatic and effective in other managerial tasks that don't involve harm, such as resource allocation or financial planning. The benchmark may be over-generalizing this finding.\n\n4. High Sensitivity to Prompt Phrasing: \n\nThe \"nudging\" experiment (a strength) also reveals a weakness: the models' behaviors are extremely sensitive to small changes in the prompt. This implies that the baseline results in Table 1 are themselves highly dependent on the specific, neutral phrasing of the system and user prompts. A slightly different phrasing could potentially yield very different results, limiting the generalizability of the scores.\n\n5. Potential for Benchmark-Specific Overfitting: \n\nThe authors rightly \"explicitly discourage using these scenarios for model training,\" fearing it would create a \"false sense of security\". This highlights a weakness of the benchmark's structure: a model could be fine-tuned to specifically pass MANAGERBENCH (i.e., \"be safe with humans, be pragmatic with objects\") without developing a general, robust understanding of safety-pragmatism trade-offs."}, "questions": {"value": "1. The current A/B format is effective for a clean diagnosis but is restrictive. Is it possible to allow for open-ended responses or a \"propose an alternative\" option? This would test a more advanced capability: whether the LLM can find a creative third solution that achieves the goal without causing harm, which is a hallmark of a truly superior and aligned agent.\n\n2. Real-world managerial dilemmas are rarely isolated, single-shot decisions. Is it possible to introduce multi-step scenarios where the model's initial choice (e.g., to prioritize pragmatism) has cascading ethical and operational consequences in subsequent steps, testing its long-term strategic reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "trOhYc1M7W", "forum": "KsmTaPygR9", "replyto": "KsmTaPygR9", "signatures": ["ICLR.cc/2026/Conference/Submission7961/Reviewer_Wxr8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7961/Reviewer_Wxr8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997131149, "cdate": 1761997131149, "tmdate": 1762919974564, "mdate": 1762919974564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ManagerBench that evaluates how LLM agents navigate a safety-pragmatism trade-off in realistic managerial scenarios by asking it to choose either (i) a pragmatic option that achieves an operational goal but harms humans, versus (ii) a safe option that sacrifices performance. The benchmark also includes a control set in which each scenario with harm directed to inanimate objects to measure over-safety. LLMs are scored by their harmonic mean of Harm Avoidance and Control Pragmatism which is called MB-Scores. The results show that low overall MB-Scores and suggests models perceive harm similarly to humans but often prioritize goals over safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\n* Frames safety evaluation as decision-making under conflicting incentives rather than refusal of harmful content.\n* The inclusion of the control set is a nice idea to separate human-safety alignment from just pure risk aversion.\n\n**Quality**\n* Clear example structure and protocol\n* Detailed explanation on thoughtful construction pipeline with human-validated for perceived harm and realism.\n* Model coverage is broad.\n\n**Clarity**\n* Overall nice figures and examples with detailed explanation. Easy to follow the paper.\n\n**Significant**\n* The state-of-the-art models struggle on the safety-pragmatism treade-off is an interesting founding."}, "weaknesses": {"value": "1. The scenarios are intentionally binary with explicit harm statement, which sharpens diagnosis but may over-simplify managerial reality and edge toward moral-dilemma style tests. \n2. Treating “object harm” as the pragmatic choice risks baking in a normative stance (orgs often internalize equipment damage costs, compliance, and reputational risk). Without explicit cost calibration, Control Pragmatism may conflate reasonable caution with “over-safety.”\n3. Harm/benefit stakes are not monetized to real-world costs. It’s unclear if observed trade-offs align with realistic managerial risk calculus across domains.\n4. Broader ablations with prompt sensitivity analyses are limited.\n5. Treating refusal as an incorrect response could be misleading. Some models decide to refuse just by looking at harmful contents."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTlKmthjtj", "forum": "KsmTaPygR9", "replyto": "KsmTaPygR9", "signatures": ["ICLR.cc/2026/Conference/Submission7961/Reviewer_nGQa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7961/Reviewer_nGQa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023936501, "cdate": 1762023936501, "tmdate": 1762919974126, "mdate": 1762919974126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}