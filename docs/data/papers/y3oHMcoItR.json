{"id": "y3oHMcoItR", "number": 2270, "cdate": 1757045407889, "mdate": 1759898159050, "content": {"title": "RealBench: A Benchmark for Complex Physical Systems with Real-World Data", "abstract": "Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealBench, the first benchmark that integrates real-world measurements with paired numerical simulations. RealBench consists of five datasets, three tasks, eight metrics, and nine baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark nine representative scientific ML baselines, including state-of-the-art models and pretrained PDE foundation models. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment.", "tldr": "We propose the first benchmark for complex physical systems with paired real-world data and simulated data, and explore how to bridge simulated and real-world data.", "keywords": ["complex physical system", "PDE", "benchmark", "real-world data", "prediction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50d255151f46098fec248e43e854fa59618f0555.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new dataset featuring paired simulation and experimental data for a variety of systems. The authors then benchmark a variety of state-of-the-art models on this data exploring the impact of simulated data on learning the real-world dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This type of dataset is so valuable to the community that I would strongly recommend acceptance even if the paper itself had major issues. Almost all work in this space today uses simulation data exclusively which is a space where learned models can at best offer speed improvements. This work and future datasets like it can offer a chance to explore in which regimes we may be able to achieve improved accuracy over numerical simulation and could great inform research directions over time.\n\nThat said, the paper itself is quite strong. Some particular reasons:\n1. The presentation is excellent. It explains the problem very cleanly and describes prior work and the intended goal of this dataset.\n2. The appendix is quite extensive providing data on the models and data generation process. In general, the division between main text and appendix content feels very well planned. \n3. Experimental fluids data is extremely rare and valuable to the community for developing models that can operate independently of the standard numerical solver framework."}, "weaknesses": {"value": "The benchmarking has a few issues, but I'm largely reading this as a dataset paper, so I'm not marking down for them. However, in the interest of improving the paper, I will still list them out here.\n\nMajor:\n1. The major missing component of this submission is an evaluation of how accurately the numerical simulation models the real scenario. If this is possible from the data, providing this information would drastically increase the potential impact of the paper. \n2. The model comparisons aren't very convincing due to the vastly different scales between them. It would improve the submission as a benchmarking effort to apply some form of normalization - FLOPs, parameter count, run time on fixed hardware - but I'm largely treating this as a dataset paper so not marking off for this. \n3. Similarly, one-step prediction is a useful reference for how well the models do what they're trained for, but longer rollout evaluations are more reflective of real tasks. I'd add more details on longer autoregressive rollouts if possible. \n4. Frequency domain error (in space) is generally going to be more informative when normalized. In 4.5 the text notes that a decrease of error in the high frequency is remarkable, but it's actually very much expected when comparing absolute metrics. Most fluid systems follow a polynomial decay law in spatial frequency, so the values being compared should be much smaller. It is actually concerning (for the baselines, not this submission) that this seems to be rare.\n\nMinor notes\n1. 3.1 prediction task - this is all discretized data, so it feels inaccurate to describe the task as a mapping between the continuous spaces. \n2. (paragraph beginning at 155) It would make sense to also include a canonical validation/test set of simulated data to evaluate the level of overfitting to simulation data and the overall sim2real gap. \n3. Table 4 is pretty hard to read. I don't know that this is the best way to demonstrate this data."}, "questions": {"value": "1. Often one of the cited limitations of numerical simulation is the difficulty of simulating regimes (Reynolds/schmidt/ect) that occur commonly in real world settings. For CFD, the datasets currently contain mostly DNS data. Have you considered further comparisons between experimental data in regimes where DNS struggles and the more approximate simulations used to model them?\n2. Do any of the models evaluated compare favorably to the numerical simulation for predicting the state of fields? Is the pairing sufficiently close that this is a question it is possible to ask with this data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PZwCwlLtxZ", "forum": "y3oHMcoItR", "replyto": "y3oHMcoItR", "signatures": ["ICLR.cc/2026/Conference/Submission2270/Reviewer_YXXo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2270/Reviewer_YXXo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761156400, "cdate": 1761761156400, "tmdate": 1762916169546, "mdate": 1762916169546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RealBench, which evaluates the generalizability of simulated-data trained ML models on real-world measurements. It is well written, and covers an obvious gap in the literature. The authors clearly put significant thought into solving this issue, and it appears that the paper is as close to reproducible as is possible. II see they even posted their code on anonymous github. I looked through their code and it has the same quality/usability as PDEBench, which has set the standard for this kind of release."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is original, and tackles a difficult challenge of pairing real and simulated data. This a clear gap in the literature, where simulated data was the only solution before. This provides a unique insight into many of the claims made on various architectures aimed at training surrogates for PDEs. It is comprehensive, well written, and thoughtfully formulated (especially the figures). It appears to achieve the maximum possible level of reproducibility through the use of the anonymous github repo."}, "weaknesses": {"value": "The only weakness is the obvious one - out of domain regimes are not covered. However, this is probably the biggest area of weakness for this area of study as a whole. The complexity of fluid dynamics makes that a separate challenge entirely (one I dont see being solved any time soon). Surrogates typically cover some precise range of reynolds numbers around a specific geometry. It is the nature of this domain."}, "questions": {"value": "I don't have any questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MJSr1mbZan", "forum": "y3oHMcoItR", "replyto": "y3oHMcoItR", "signatures": ["ICLR.cc/2026/Conference/Submission2270/Reviewer_1VNT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2270/Reviewer_1VNT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905169468, "cdate": 1761905169468, "tmdate": 1762916169235, "mdate": 1762916169235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a new physics-oriented benchmark and dataset that focuses on the gap between simulated and real experimental data. \n\nThe paper presents $5$ applications and provides metrics for evaluation, alongside classical and recent benchmark methods. \n\nThe description of the \"TASK DEFINITION\" should be improved and linked to the experiments, for example, in Table.1. \n\nThe dataset is not well described; a summary table is necessary (how many samples, what is the size of each sample, what is the total real time duration, step size, dimension of the problem 1d, 2d, 3d, ...). \n\nThe authors provide code to run the experiment, some sample data (real and simulated), but do not provide the script to generate the numerical data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This dataset addresses the important aspect of bridging the gap between the simulated (numerical) and the physical system. \n\nWhile is not possible to cover a large experimental setup, the authors provide $5$ taks with both numerical and physical data.\n\nThis work will therefore help in evaluating new models, even if may not cover all possible scenarios."}, "weaknesses": {"value": "It is hard to say, but it is not possible to cover all possible physical experimental conditions. Nevertheless, the paper is a good contribution in the right direction. \n\nThe main point is that the numerical generation scripts are missing, therefore not possible to extend the data (at least numerical) to other scenarios. \n\nOn the experimental side, I am not able to judge if the information is sufficient. \n\nI found the task description disconnected to the actual experiments. I would encourage the authors to improve that section."}, "questions": {"value": "One possible source of difference between experimental and numerical experiments is the measurement noise, but i can assume there could be a larger difference, for example, if the state is not directly measurable (pressure is not available, but only velocity).\n\nCould the author expand and position this paper in this context? What are the possible main and most critical differences between experiments and numerical simulations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8245mXvN5V", "forum": "y3oHMcoItR", "replyto": "y3oHMcoItR", "signatures": ["ICLR.cc/2026/Conference/Submission2270/Reviewer_kvd1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2270/Reviewer_kvd1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918132086, "cdate": 1761918132086, "tmdate": 1762916168986, "mdate": 1762916168986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark that pairs real measurements with matched numerical simulations across five scenarios (cylinder wake, controlled cylinder, FSI, foil, and combustion; governing equations span Navier-Stokes, coupled FSI, and reactive Navier-Stokes with species transport). The benchmark defines three training regimes (train on simulation, train on real, pretrain on simulation then finetune on real), includes a mix of pixel and physics flavored metrics, and evaluates a set of neural PDE baselines including a pretrained foundation model. The headline empirical messages are: there is a nontrivial gap between simulation and laboratory data; pretraining on simulation generally helps downstream on real; and the codebase makes it straightforward to add models or datasets.\n\nI think this is timely and potentially useful. If we are serious about sim2real for scientific ML, we need carefully curated real data and a shared protocol. However, the current paper mixes benchmarking with a sim2real narrative in ways that are a bit loose, the experimental details are too thin for others to trust or extend the datasets, and some of the metrics and literature framing are not well aligned with fluid mechanics and combustion practice.\n\nAm open to potentially increasing my score if the authors narrow the claims, remove the robotics sim2real framing argument and instead point to real sim2real problems in fluid dynamics, add significantly more context to place this in the existing fluid dynamics literature, and substantially improve the experimental documentation and physics grounded evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark collects paired real and simulated trajectories for several nontrivial systems instead of yet another synthetic-only PDE suite. This is likely to be useful for the community.\n- The split of training regimes (simulation only, real only, pretrain on simulation then finetune on real) is useful, and the pretraining result is consistent with what many of us have seen in practice.\n- The code appears modular enough to add a new dataset or baseline without painful surgery, and using a single file format lowers friction for adoption.\n- Including both data oriented metrics and physics oriented diagnostics is better than reporting only RMSE. The autoregressive evaluation option is also a good idea.\n- The combustion scenario is ambitious and, if documented properly, could become a valuable stress test beyond the usual laminar toy problems.\n- The baseline measurements for their benchmark are very extensive."}, "weaknesses": {"value": "Major concerns:\n\n- The documentation about the experiment is unacceptably thin in its current form. If the experimental data was created or modified from another source, you need to cite it. If someone else created the dataset for you, you need them to write documentation for it. The current documentation on experimental data generation (which, of course, can be included in the appendix) is simply unacceptable for publication, especially for a paper which is supposed to be about this very dataset.\n- The paper motivates the sim2real gap by citing mostly work from robotics, which I found very strange, almost as if the authors are guessing there is a sim2real gap in fluids, without actually surveying the literature. Robotics has a much different sim2real gap than turbulence research does. In fluids, the sources of discrepancy, data acquisition, and noise models can be quite different. Please ground the narrative in fluids and combustion references. If you are addressing the sim2real gap in fluids, you must speak from the context of the fluids community, and discuss the ways in which the fluids community has quantified this gap.\n- Please state clearly whether you will release raw data (e.g., the PIV frames), calibration files, and the full processing scripts, not just the final HDF5 arrays, so that it can be checked by others. If raw data cannot be released, say so and justify it. Benchmarks live or die by their data hygiene, and biases in a benchmark can leak into biases in the community's preferred models.\n- Several figures quantify differences using frequency or Fourier errors over image-like arrays. This is admittedly a start, but it is not a physics grounded measure of mismatch between experiment and simulation, and certainly not something people in the fluids community would actually use as a robust measure of discrepancies between simulation and real data. In fact, for several real-world experimental problems, there is a difference here that is simply due to the nature of the real-world experiment, yet the simulation and experiment can actually have no discrepancy. This is because you would only care about some summary statistic, and not care about some wave mode that you know does not affect your statistic. Yet, your metric would completely miss this. I recommend reviewing and citing the fluids literature and how people measure discrepancies between simulations and real data. Note that this is a problem people have studied for literally decades in the fluids community. These additions would make your claims about a \"gap\" more convincing.\n- Simulation contains modalities that are not observed in the lab, and the current strategy randomly masks channels and adds noise. That is a start, but it does not reflect the actual sensor physics. Please consider sensor specific degradations (camera noise models, optical blur, saturation, PIV algorithmic artifacts) and state explicitly which channels are used for training and which are hidden. It would also help to define tasks that force parity, for example training all models only on the modalities that the lab provides.\n- The paper claims to be the first benchmark that integrates real-world measurements with paired numerical simulations across complex physical systems. Within fluids, this is far from true; as just one demonstration, \"ERCOFTAC\" has hosted combined experimental and numerical reference cases since 1995 across a wide range of flows. Please narrow the novelty claim to something that accurately represents existing datasets and benchmarks, and perhaps cite existing databases.\n\n\nAdditional comments and suggestions\n\n- The update ratio metric is interesting, but it conflates pretraining data scale with optimization effects.\n- Please make the train, validation, and test splits explicit at the parameter level so that generalization across Reynolds number, control frequency, mass ratio, or equivalence ratio is clear. I consider those to be most interesting axes. And if you already do this, show it more prominently.\n- The autoregressive evaluation stops very early. If you want to make claims about stability, show longer horizons and add probe based diagnostics, not only field RMSE.\n- The baselines are modern ML models, which is fine for a benchmark, but the story would be stronger if you add one or two domain baselines for each scenario (for example a simple reduced order model, or even a physics based filter) so readers have a calibration point.\n- Throughout the paper there are small terminology issues. I would prefer \"numerical error\" over \"computational error\" (computational error sounds like a code bug). Be precise about whether errors arise from discretization, closure modeling, boundary conditions, or measurement.\n- Not a criticism, but I think it would be better to rename the benchmark. \"RealBench\" seems far too broad and will clash with many domains. Something like \"RealPDEBench\" or \"RealFlowBench\" might be more appropriate."}, "questions": {"value": "[Several questions discussed above]"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F57t6Rab9l", "forum": "y3oHMcoItR", "replyto": "y3oHMcoItR", "signatures": ["ICLR.cc/2026/Conference/Submission2270/Reviewer_6AF9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2270/Reviewer_6AF9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2270/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241594100, "cdate": 1762241594100, "tmdate": 1762916168832, "mdate": 1762916168832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}