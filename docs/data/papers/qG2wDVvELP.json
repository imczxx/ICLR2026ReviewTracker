{"id": "qG2wDVvELP", "number": 11095, "cdate": 1758189024990, "mdate": 1759897609058, "content": {"title": "Dynamic Evaluation of Reward Models via Pairwise Maximum Discrepancy Competition", "abstract": "Reward models (RMs) are essential for aligning large language models with human preferences, making their rigorous and comprehensive evaluation a critical task. However, traditional evaluation methods rely heavily on closed datasets with pre-annotated preference pairs, which often fail to assess the generalization ability of RMs across unseen prompts in open-world scenarios. To overcome these limitations, we introduce the Pairwise Maximum Discrepancy Competition (PMDC) framework, a dynamic and annotation-efficient evaluation approach that adaptively selects informative test cases from a large, unlabeled, open-domain prompt pool. Specifically, the PMDC framework operates by first identifying input pairs that elicit significantly divergent preference scores from two RMs. These discriminative pairs are subsequently evaluated by an advanced large language model (LLM) acting as an oracle, determining which RM produces judgments more closely aligned with human preferences.  The resulting pairwise comparisons are aggregated via the Bradley-Terry model, yielding an overall ordinal evaluation of the assessed RMs. We apply PMDC to re-evaluate 10 representative RMs from the RewardBench collection. The results reveal noticeable inconsistencies in RM rankings compared to those derived from conventional benchmarks. Further analysis uncovers the strengths and weaknesses of each model, providing valuable insights for future improvements in reward modeling.", "tldr": "We propose a dynamic and cost-efficient framework for evaluating reward models via pairwise maximum discrepancy competition.", "keywords": ["Reward Models", "LLMs", "Dynamic Evaluation", "Maximum Discrepancy Competition"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed5e405a72d98decfa33ba74a765da8a97fd07f0.pdf", "supplementary_material": "/attachment/b2d06572ee2569cbcf881516a08a9f6d37a30754.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PMDC, A dynamic, annotation-efficient evaluation framework that selects “maximum discrepancy” prompt-response pairs using multiple reward models. An RM evaluation  dataset via active probing is constructed to identify highly divergent reward model responses. Experiments demonstrate that it not only enables discrimination-rich evaluation of reward models but also enhances downstream alignment performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Targeted evaluation via discrepancy sampling: The selection objective explicitly maximizes |(sA(q,a1)−sA(q,a2)) − (sB(q,a1)−sB(q,a2))|, which focuses the RM evaluation efforts on the most contentious cases. \n\n2. Clear and implementable benchmark construction pipeline: The framework specifies the data path end-to-end: prompt pooling, response sampling, per-RM scoring, top-k discrepancy selection per RM pair, LLM-judge 2AFC, and Bradley–Terry modeling."}, "weaknesses": {"value": "1. The main contribution of this work is that it demonstrates a significant ranking inconsistencies of reward models between PMDC and  traditional benchmarks. These inconsistencies stem from the difference in benchmark data construction: PMDC focuses on the most contentious cases, while traditional benchmarks do not. This bias seems obvious and intuitive, making the contribution of this work unclear.  What are the useful insights that can be derived or concluded from this benchmark? \n\n2. There exists inconsistency in normalization vs discrepancy computation. Eq. 1 introduces min–max normalization, but Eq. 3/4 use s_A,s_B without primes. If discrepancies are computed on unnormalized scores, scale differences between RMs can distort sample selection; if they are normalized, the notation and implementation details need to be corrected and made explicit.\n\n3. Bradley–Terry likelihood specification (Eq. 7) appears incorrect: It shows [wij log Pi,j + wji log Pi,j], but the second term should be wji log Pj,i.\n\n4. 2AFC protocol conflicts with “oracle can’t decide” narrative: Section 3.3 mentions “oracle cannot determine a superior response,” yet PMDC uses a strict 2AFC oracle prompt with no tie. \n\n5. Sensitivity to extreme scores and sampling composition: Min–max normalization across a large, mixed prompt set is highly sensitive to outliers, impacting both preference discretization and discrepancy magnitudes.\n\n6. The fine-tuning setup of Section 4.5 lacks details such as training data construction and training details."}, "questions": {"value": "1. Compatibility of the evaluated reward models (Table 1) and datasets. What in specific do these reward models optimize, helpfulness, honesty or others? Are these objectives compatible with the evaluation datasets?\n\n2. In Equation (8), Agreement(Ri) is measured only on PMDC-selected contentious cases. Can you report agreement on a matched random subset to quantify how much your metric reflects “performance on controversy” vs overall accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ahjE5o46q2", "forum": "qG2wDVvELP", "replyto": "qG2wDVvELP", "signatures": ["ICLR.cc/2026/Conference/Submission11095/Reviewer_FdQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11095/Reviewer_FdQs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200003548, "cdate": 1761200003548, "tmdate": 1762922273594, "mdate": 1762922273594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission is in the \"datasets and benchmarks\" area, which proposes Pairwise Maximum Discrepancy Competition (PMDC) — a dynamic and annotation-efficient framework for evaluating RMs. Instead of evaluating each RM on a fixed dataset (e.g., RewardBench 1/2), PMDC adaptively identifies prompt–response pairs where two RMs most disagree, then asks a strong LLM to decide which RM’s preferences align better with \"human\" judgment. The method aggregates these pairwise comparisons to derive global rankings of RMs.\n\nExperiments re-evaluate ten public RMs using prompts aggregated from multiple sources (MMLU, GSM8K, HumanEval, TruthfulQA, etc.), finding large deviations from RewardBench2 rankings. The authors claim PMDC offers better generalization assessment and significant annotation-cost savings. They also show small downstream gains (+3.1%) when fine-tuning an RM on PMDC-identified high-discrepancy samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "There are several strengths of this submission: \n\n1. The dynamic-evaluation framing is new to the reviewer's knowledge. PMDC adapts the “Maximum Discrepancy Competition” ideas to RM evaluation, emphasizing active probing over static test sets. \n\n2. The arguments around efficiency and automation of the annotation process is valid. Using an LLM-as-judge oracle avoids costly human annotation and identifies the most discriminative cases, though clearly with caveats listed below.\n\n3. The study compares ten prominent RMs from multiple families, reporting both ranking and oracle-agreement metrics, and analyzes stability across seeds, judges, and top-k values. This setup is good.\n\n4. The paper also demonstrates that PMDC-selected samples can improve RMs through targeted fine-tuning, which is interesting."}, "weaknesses": {"value": "However, though with several novel points, the reviewer is not convinced that the proposed evaluation framework is sufficiently sound and will be preferred over existing evaluation protocols, due to the following weaknesses:\n\n1. Dependence among RMs and limited practical scalability. PMDC compares RMs pairwise and selects “maximum discrepancy” prompts jointly over all N(N−1)/2 pairs. This inherently introduces dependency among models — unlike existing evaluations that independently score each RM. Such dependency limits the method’s use for leaderboards or community evaluation due to clear scalability issues.\n\n2. The reliance on an “advanced LLM oracle” as ground truth clearly has caveats. The paper reports high cross-judge Spearman correlations but lacks convincing human validation to confirm that any oracle is trustworthy.\n\n3. Because each RM pair may be evaluated on different prompt subsets, PMDC does not ensure that all models are assessed on the same samples. The resulting ranking may have unknown statistical variance or dependence on sampling randomness. No rigorous or convincing analysis is provided on this front.\n\n4. High probability of potential leakage. It seems that the paper focuses on more the \"methodology\", but the exact setup is critically important for the topic concerned, which is unfortunately not very rigorous to the reviewer. Specifically, the “open-domain prompt pool” is built by aggregating well-known datasets such as MMLU, GSM8K, TruthfulQA, etc. all potentially heavily used in pre-training or alignment of the RMs being evaluated.\n\n5. Limited discussion of how the community can use PMDC. This is important for a Dataset & Evaluation-track paper. Without a public pipeline or easy-to-extend design, its community impact may be small."}, "questions": {"value": "Please refer to the detailed reviews. Also:\n\nHave you performed any human-oracle agreement studies to quantify the accuracy of the LLM judge compared to real human preferences?\n\nCan PMDC support incremental evaluation (the typical scenario when a researcher has one new RM)?\n\nCan you provide confidence intervals for the Bradley–Terry scores or conduct bootstrap analysis to show ranking robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9t3Ywsd8fT", "forum": "qG2wDVvELP", "replyto": "qG2wDVvELP", "signatures": ["ICLR.cc/2026/Conference/Submission11095/Reviewer_d9BR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11095/Reviewer_d9BR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407250543, "cdate": 1761407250543, "tmdate": 1762922273251, "mdate": 1762922273251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PMDC as a dynamic, annotation-efficient way to evaluate reward models by actively sampling the the data pairs eliciting maximum discrepancy across the two reward models under comparison, judging them with a strong oracle, and aggregating pairwise comparison using a Bradley-Terry model. On 10 representative RMs, PMDC method reveals a different hierarchy than static benchmarks RewardBench2. Finetuning an RM on PMDC-selected, oracle-labeled pairs improves RewardBench2 overall score by +3.1%, especially on math and tie-handling. The paper provides a novel, competition-based reward model benchmarking, different from traditional static benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: I think this competition based ranking of reward model is quite novel and I like the idea of sampling maximum discrepancy data for judging, largely reducing the efforts of data labeling.\nQuality: The paper does try to provide different ablation studies like testing result robustness against different oracle judges, or different prompt/responses creation. \nClarity: The results for establishing the new evaluation method are clearly presented and the case studies examples in the Appendix are helpful for understanding. \nSignificance: The method offers a potential alternative to static benchmarks if the authors can provide more details on how to scale up the practical usage."}, "weaknesses": {"value": "1. Section 4.2 and Figure 3: I think the claim that \"6 out of 10 models exhibit rank differences of 3 or fewer, indicating general consistency\" is not rigorous. RewardBench2 is a non competition based benchmark so its ranking is absolute ranking. Besides these top 10 models there are many other models. In the experiment, authors only limit to pairwise comparison within the 10 models so there are only 10 rankings these models can take. Hypothetically if using this method to evaluate the whole pool, the ranking might not be limited to that difference. \n2. Point 1 leads to another potential weakness. The method is motivated partly such that the evaluation is more data efficient (only picking the high discrepency data points). However, when ranking a large pool of reward models (like the numbers in rewardbench 2), pairwise comparison will end up be more costly. This limits the impact and practical usefulness of the method\n3. I think Experiment 4.4's logic is a bit problematic. The authors discusses that using these maximum discrepancy pair data for finetuning reward model can improve the performance and the evaluation criteria is rewardbench2. However, in earlier part Figure 3, the authors just demonstrate that sometimes their evaluation method can be quite different from rewardbench2 and suggest that this is a potential weakness of traditional benchmark. I am confused by what should be a gold standard here."}, "questions": {"value": "1. Regarding weakness 1 mentioned, do authors have other methods to compare the result with the standard benchmarks? \n2. Regarding weakness 2 mentioned, do authors have any methods to generalize this method to large pool reward model comparison?\n3. Regarding weakness 3 mentioned, can authors provide better logic on how they view the difference of their evaluation vs traditional benchmarking methods? if they think theirs is superior, are there other methods to illustrate the performance gain of training on maximum discrepancy pairs data (as rewardbench is no longer reliable)? if the authors think rewardbench is still valid/gold standard, please provide more reasoning on the contribution of this evaluation method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MyiYHmWeFY", "forum": "qG2wDVvELP", "replyto": "qG2wDVvELP", "signatures": ["ICLR.cc/2026/Conference/Submission11095/Reviewer_9evW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11095/Reviewer_9evW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449234549, "cdate": 1761449234549, "tmdate": 1762922272853, "mdate": 1762922272853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Pairwise Maximum Discrepancy Competition (PMDC), a framework for evaluating reward models (RMs) by identifying prompts that cause large disagreements among models and resolving them with an LLM-based oracle judge. The judgments are aggregated using a Bradley–Terry (BT) model to derive a global ranking. Experiments on ten reward models from RewardBench show different rankings than static benchmarks, and fine-tuning on PMDC-selected samples slightly improves downstream performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear formulation and structure. The PMDC pipeline and selection rule are mathematically consistent and easy to follow.\n2.\tDynamic testing idea. The notion of adaptively probing contentious prompts is intuitive and could help uncover edge cases that static benchmarks miss.\n3.\tReadable and reproducible. The authors describe implementation details, oracle prompts, and ablations clearly."}, "weaknesses": {"value": "1.\tMeasures relative ranking, not true accuracy. The framework produces pairwise rankings rather than evaluating how well each RM aligns with human preferences in absolute terms. Without a human reference or calibrated oracle, it cannot measure quality—only which model “wins” more often.\n2.\tHeavy dependence on LLM oracles. The oracle’s biases and inconsistencies directly shape the final ranking, yet no human or inter-oracle validation is presented to confirm reliability.\n3.\tScalability issues. The Bradley–Terry model requires O(N^2) comparisons for N reward models, which becomes infeasible for large collections (e.g., hundreds of RMs). Sparse comparisons lead to unstable or ill-defined rankings.\n4.\tLimited empirical depth. Experiments cover only ten RMs and one oracle; no statistical error bars, human correlation checks, or domain diversity analyses are provided.\n5.\tIncremental conceptual value. The method mostly repackages known maximum-discrepancy and BT ideas, with limited novelty in algorithmic insight or theoretical grounding."}, "questions": {"value": "1.\tHow can PMDC’s ranking be converted into an absolute evaluation of each RM’s correctness or calibration, rather than a relative order?\n2.\tHave you verified that the LLM oracle’s judgments correlate with human preference annotations?\n3.\tHow would PMDC scale to large RM sets (e.g., 100–300 models)? Is there a way to approximate the BT rankings without quadratic comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2gfisCdZMM", "forum": "qG2wDVvELP", "replyto": "qG2wDVvELP", "signatures": ["ICLR.cc/2026/Conference/Submission11095/Reviewer_AE36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11095/Reviewer_AE36"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773079719, "cdate": 1761773079719, "tmdate": 1762922272098, "mdate": 1762922272098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}