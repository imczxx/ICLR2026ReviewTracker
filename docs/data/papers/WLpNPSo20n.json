{"id": "WLpNPSo20n", "number": 15543, "cdate": 1758252535346, "mdate": 1759897300592, "content": {"title": "Geometri-Disentangelment Unlearning", "abstract": "Machine unlearning, the removal of a training subset‚Äôs influence from a deployed model, is critical for privacy preservation and model reliability, yet gradient ascent on forget samples often harms retained knowledge. Existing approaches face a persistent tradeoff between effective forgetting and preservation on the retain set. \nWhile previous methods provide useful heuristics, they often lack a formal analysis on how exactly forgetting updates harm retained knowledge, and whether the side effects can be removed with theoretical guarantees. \nTo explore a theoretically sound and simple solution, we start from the first principle on how performance on the retain set is actually affected: a first-order analysis of the local change of the retain loss under small parameter updates during model training. We start from\na crisp equivalence: the retain loss is unchanged to first order iff the update direction is orthogonal to the subspace spanned by retain gradients (``retain-invariant''). This identifies the entangled component as the tangential part of forget update within the retain-gradient subspace, and characterizes disentanglement as orthogonality. Guided by this, we propose a geometric-disentanglement unlearning (GU) that decomposes any candidate forget gradient update into tangential and normal components to retain space and executes only the normal component. Under a standard trust-region budget, the projected direction aligned with the raw forget gradient is optimal among all first-order retain-invariant moves, and we also derive the optimal projected direction for joint forget-retain updating objectives. Our method is plug-and-play and can be attached to existing gradient-based unlearning procedures to mitigate side effects.\nGU achieves consistent improvement on various methods across three benchmarks TOFU, MUSE, and WMDP. Plugging GU into SimNPO yields up to 62\\% lower Extraction Strength (ES), 32\\% higher retention ES, 8\\% higher utility, and 60\\% higher MIA-closeness on TOFU.", "tldr": "", "keywords": ["Large Language Models Unlearning", "Knowledge Entanglement"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9f8ede671627610f7a797cdb1552385e808854d.pdf", "supplementary_material": "/attachment/3ab923ae0436bb6dc263b5b6f5d2357eca8eba8b.zip"}, "replies": [{"content": {"summary": {"value": "With the goal of unlearning a target model while preserving its performance on the remaining data, this paper first provides conditions for the retain set loss to remain unchanged. Then it proposes an unlearning method that decomposes the forgetting signal into tangential and normal components to the retain space, preserving only the normal one. Specifically, the authors demonstrate that, to first order, the retain set loss remains unchanged when the weight update is orthogonal to the subspace spanned by the retain gradients. Therefore, the authors propose geometric-disentanglement unlearning (GU), which projects forget gradients to the normal component of the subspace spanned by retain gradients that guarantee the steepest descent, under a trust-region budget. Results in established benchmarks for LLMs (TOFU, MUSE, WMDP) demonstrate the effectiveness of the proposed method in mitigating the retain set performance degradation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper provides the first theoretically grounded representation of the entanglement between retain and forget datasets, showing, under minimal assumptions, which update direction jointly leaves the retain loss locally invariant while unlearning the forget set.\n2. Rigorous theoretical proofs support every claim in the paper.\n3. I appreciate that the authors submitted the code used in their experiments, allowing full reproducibility.\n4. Although some steps are somewhat abstracted, the writing is clear, and the notation is consistent and appropriate."}, "weaknesses": {"value": "**Major weaknesses**\n1. **Comparison with similar approaches.** Although I believe the results section is extensive, showing that the proposed method, when paired with existing unlearning algorithms, improves the unlearning/retaining trade-off, comparing this method with other approaches that propose orthogonal projections [1,2](this list is non-exhaustive) would have strengthened the paper's claims.\n2. **The extent to which this method works.** It is not clear to me to what extent this method works because of the following two concerns: \n    1. First, it is true that in most cases this approach benefits unlearning; yet, retain set improvements are almost always mild and clearly depend on the effectiveness of the underlying unlearning algorithm. For instance, taking Table 1, under a 10% unlearning ratio and Llama 1B, GradDiff scores 0.12 ES Re. from the original retaining performance of ~0.65. By applying the proposed GU, it reaches 0.15. Similarly, with DPO, the score passes from 0.26 to 0.28. There are a few cases where the improvement is stronger, like in UNIDIAL (from 0.26 to 0.69) and SatImp.\n    2. I am unsure about the effectiveness of the proposed method under highly entangled forget and retain spaces. The forget and retain identities in TOFU are probably well disentangled; therefore, the proposed method is shown to be somewhat effective. Instead, the GU performance improvements are more subtle and sometimes questionable in MUSE and WMDP. My intuition is that forget and retain datasets for these two benchmarks are much more entangled (e.g., MUSE forget set contains actual Harry Potter books, while the retain set contains knowledge crawled from open wikis). Thus, GU cannot properly find an unlearning direction orthogonal to the retain set subspace. Bringing this to the extreme case, I do not know whether this approach can work in the random unlearning scenario.\n3. **Hyperparameter search.** Some results are a bit weird to me. For instance, CEU is shown to perform very poorly, but with the proposed GU it substantially improves its performance. Yet, the CEU paper reports results that are far better than those highlighted here, although with Llama 2 instead of Llama 3. As it is well known that hyperparameter selection is of paramount importance for machine unlearning to achieve satisfactory results [3,4,5], this makes me question the hyperparameter search and selection of this paper. So, is the proposed method actually improving the baselines, or does it make up for improper hyperparameter tuning? Additionally, the hyperparameter search and selection for this paper is not reported; likewise, a study on the hyperparameter sensitivity of GU.\n\n**Minor weaknesses/suggestions**\n1. L.57 states that the objective is to \"reduce forgetting loss while leaving the retained knowledge unchanged [...].\" This is partly correct for the forget loss, as, e.g., in gradient ascent, the loss increases.\n2. L.106 states that for the forget set, the update applies gradient ascent. Yet, L.121-129 correctly state that this is one of the possible choices. I'd suggest rewriting the L.106 statement, taking this into account.\n3. Although Appendix B reports full details about the algorithm implementation and the submission also comes with the code, I suggest adding a pseudocode in Appendix B to show the high-level general steps to execute GU; otherwise, it is a bit cryptic in my opinion.\n\n[1] Sendera, Marcin, et al. \"SEMU: Singular Value Decomposition for Efficient Machine Unlearning.\" arXiv, 2025.\\\n[2] Hoang, Tuan, et al. \"Learn to unlearn for deep neural networks: Minimizing unlearning interference with gradient projection.\" WACV, 2024.\\\n[3] Cadet, Xavier F., et al. \"Deep unlearn: Benchmarking machine unlearning.\" EuroS&P, 2024.\\\n[4] Kim, Hyoseo, Dongyoon Han, and Junsuk Choe. \"Negmerge: Consensual weight negation for strong machine unlearning.\" ICML, 2025.\\\n[5] He, Zhengbao, et al. \"Towards natural machine unlearning.\" TPAMI, 2025."}, "questions": {"value": "1. Could the authors provide a comparison with at least one orthogonal projection method in the machine unlearning literature? Or an intuition for why the comparison was excluded in the first place.\n2. Could the authors elaborate on the performance improvements of GU in relation to weaknesses 2.1 and 2.2 (W2.1, W2.2)?\n3. Could the authors report the hyperparameters chosen and how they were searched for? Additionally, I would also like to see the hyperparameter sensitivity analysis for GU.\n\n**Motivation for my score**\\\nAlthough some results are not convincing, I believe the community will benefit from the theoretical insights of this paper; therefore, I positively recommend this work, yet I am still reluctant to assign an accept score, which I'll reconsider after the authors' rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b18JZyi1u4", "forum": "WLpNPSo20n", "replyto": "WLpNPSo20n", "signatures": ["ICLR.cc/2026/Conference/Submission15543/Reviewer_LTCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15543/Reviewer_LTCV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761222807441, "cdate": 1761222807441, "tmdate": 1762925820098, "mdate": 1762925820098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary:\nThis paper proposes a projection-based plug-in for approximate unlearning methods. At each unlearning training step, it decomposes the forget gradient into components parallel and orthogonal to a retain-gradient subspace and applies only the retain-orthogonal component. Authors provided theoretical proofs and experimental results."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The problem is clearly defined, and the idea of solution is easy to follow.\n\n2. GU is plugged into diverse baselines."}, "weaknesses": {"value": "1. A key concern for the reviewer is that the geometric decomposition and orthogonal filter for gradients will decrease the unlearning effect.\n\nIn this paper, authors said that the original approximate unlearning update direction will have a negative impact on the retained data, so they split the unlearning update into orthogonal retained gradient and non-orthogonal retained gradient. However, if the unlearned data will have an impact on the retained data, wouldn‚Äôt it be equivalent to no unlearning after the gradients being filtered?\n\n2. The second concern is about the efficiency. This method need to decompose the unlearning gradients to ensure only update the retain-gradient-orthogonal parts. Is the efficiency acceptable in LLM to decompose each unlearning gradient? And the experimental evaluations have not reported the computation cost and storage cost.\n\n3. The method is conceptually close to null-space calibration/orthogonal projection ideas, and the paper mentions UNSC in Related Work but does not compare empirically.\n\n4. The theoretical guarantees only provide the proof of retain safety but have not provided gaurantees for unlearning.\n\n5. Typos such as the definition $D_r = D \\/ D_r$.\n\n6. The metrics definition is not clear, how are the MIA closeness and privacy leakage calculated."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "koLQwdoMFH", "forum": "WLpNPSo20n", "replyto": "WLpNPSo20n", "signatures": ["ICLR.cc/2026/Conference/Submission15543/Reviewer_pzAp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15543/Reviewer_pzAp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761280658673, "cdate": 1761280658673, "tmdate": 1762925819379, "mdate": 1762925819379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Geometric-Disentanglement Unlearning (GU), a theoretically grounded method to mitigate the trade-off between forgetting and retaining in machine unlearning for large language models (LLMs). Unlike heuristic or empirical approaches, the authors formulate a retain-invariance principle that links the absence of side effects on retained knowledge to the orthogonality between forgetting updates and the retain-gradient subspace. Based on this, GU projects forgetting gradients onto the orthogonal complement of retain gradients, achieving provable first-order safety and optimality. Experimental evaluations on TOFU, MUSE, and WMDP demonstrate consistent Pareto improvements in forgetting effectiveness, retention fidelity, privacy preservation, and utility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Solid theoretical foundation: The work provides a rigorous geometric formulation of unlearning, defining retain-invariance in terms of orthogonality and deriving first-order optimal updates with theoretical guarantees (Propositions 3.1‚Äì3.5). This bridges an important gap between heuristic disentanglement and provable unlearning.\n2. General and plug-and-play design: The proposed GU method can be integrated with existing gradient-based unlearning frameworks (e.g., NPO, CEU, SimNPO) without modifying their objectives or architectures, showing strong practical adaptability.\n3. The experimental results are clear and straightforward: The experimental results of \"The higher, the better\" and \"the lower, the better\" were not represented by the traditional bolding method. Instead, the results were indicated by shadows, such as blue for higher-is-better metrics and red for lower-is-better metrics.\n4. Evaluations across three benchmarks and multiple model sizes (Llama-2-7b-hf, zephyr-7b-beta) demonstrate consistent improvements in key metrics ‚Äî reduced Extraction Strength (forget set), improved retention ES and ROUGE, enhanced privacy (MIA-closeness), and stable utility."}, "weaknesses": {"value": "1. Limited discussion on computational cost: Although GU involves orthogonal projections in the gradient space, the paper does not analyze its time or memory overhead, especially for large-scale LLMs. A complexity analysis or ablation study quantifying the additional cost (e.g., basis rank k, update frequency) would strengthen the practical claims.\n2. Empirical results lack statistical validation: While improvements appear consistent, the paper does not report variance or significance testing across runs. Including error bars or standard deviations would increase the reliability of experimental conclusions."}, "questions": {"value": "1. The theoretical analysis heavily depends on the optimizer-induced symmetric positive definite (SPD) metric H. Would replacing H with a simple Euclidean metric (H=I) degrade results, and if so, by how much? Providing an empirical comparison or ablation here could validate whether the theoretical geometry actually matters in practice.\n2. Theoretically, GU is compatible with any gradient-based optimizer, but implementation details might differ. Does GU behave differently when combined with adaptive optimizers (AdamW) versus simple SGD? Such analysis could help readers reproduce results across different training pipelines.\n3. Such analysis could help readers reproduce results across different training pipelines. Could the authors provide standard deviations or significance tests across multiple random seeds? Are the reported improvements (e.g., +0.05 utility, ‚àí0.1 ES) statistically consistent across runs?\n4. GU requires constructing the retain-gradient subspace using a basis matrix ùëà and updating it periodically. How is the rank ùëò of the subspace chosen? How often is the subspace recomputed during training, and what is the resulting time or memory overhead? This information would clarify whether GU is truly lightweight as claimed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xck2b19Hh9", "forum": "WLpNPSo20n", "replyto": "WLpNPSo20n", "signatures": ["ICLR.cc/2026/Conference/Submission15543/Reviewer_8QAC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15543/Reviewer_8QAC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899605656, "cdate": 1761899605656, "tmdate": 1762925818870, "mdate": 1762925818870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Machine unlearning aims to remove selected knowledge from a pretrained model, while preserving performance on the retain data. The authors propose a plug-and-play method, geometric-disentanglement unlearning (GU), which disentangles the gradient updates associated with the forget data and the retain data through orthogonal projection. The addition of this method to several existing approaches is evaluated on multiple tasks and language models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The motivation is clear, and Figure 1 provides a good illustration of the overall method. \n* The experimental section is extensive, and includes several datasets, architectures, and existing methods. \n* The proposed method improves Extraction Strength on the retain split (ES Re.) across different forgetting percentages and methods on both LLMs.(Table 1)"}, "weaknesses": {"value": "* In Table 1, adding GU does not show a consistent improvement on other metrics, specifically Extraction Strength on the forget split (ES Un.), and the gains are mainly limited to SatImp and SimNPO. The improvements on Priv. and MU metrics are also not as significant or consistent as ES Un. \n* A similar pattern is observed in table 2: the improvements are not as significant across all methods. It is unclear whether GU offers a clear overall advantage across all settings.\n* While the paper provide related work on knowledge conflict in LLMs, it is still not fully clear how GU differs from similar approaches that use gradient projection for unlearning. A more comprehensive comparison  would strengthen the novelty claims."}, "questions": {"value": "Can the authors provide some qualitative results (sample answers from LLMs) showing the successful removal of forget data in comparison to a baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H5OkxbU00q", "forum": "WLpNPSo20n", "replyto": "WLpNPSo20n", "signatures": ["ICLR.cc/2026/Conference/Submission15543/Reviewer_oJEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15543/Reviewer_oJEz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984771296, "cdate": 1761984771296, "tmdate": 1762925818360, "mdate": 1762925818360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}