{"id": "IMWeaUtZCM", "number": 9664, "cdate": 1758133155762, "mdate": 1763766332692, "content": {"title": "Can transformers truly understand dynamical systems?", "abstract": "Transformer architectures have recently surged as promising solutions for nonlinear dynamical systems, often proposed as foundation models capable of zero-shot dynamics reconstruction and forecasting. Despite this success, it remains unclear whether they can truly serve as reliable digital twins of dynamical systems, i.e., whether they capture the underlying physics in distinct parameter regimes. In nonlinear dynamics, reservoir computing (RC) has already demonstrated broad success, as it is intrinsically a dynamical system capable of capturing not only the dynamical climate of the target system but more importantly, how the climate changes with parameter. Transformers, in contrast, rely on permutation-invariant attention mechanisms, which can limit their ability to capture how temporal structure changes with parameter. To address this issue, we take predicting catastrophic collapse, which occurs when bifurcation parameters cross critical thresholds, as a benchmark task. Models are trained on trajectories in normal parameter regimes and then tested on parameters in an unseen regime with system collapse. Our results show that Transformers, across configurations, consistently fail to capture collapse, while RC reliably predicts the transitions. This surprising finding raises questions about the generalization ability of Transformers to dynamical systems, a topic warranting future research.", "tldr": "", "keywords": ["Dynamical system; Critical transition; Transformer; Reservoir computing"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a441e3dd2fbca2f411776825c30721c3ecf6355.pdf", "supplementary_material": "/attachment/753d75e4fa32f20319675d8e9f2f6a8a3c52e9c6.zip"}, "replies": [{"content": {"summary": {"value": "The authors show that reservoir computing can anticipate critical transitions and tipping points while transformers cannot, casting doubt on transformers' generalization ability in learning dynamical systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Understanding the strengths and weaknesses of the transformer architecture in learning dynamical systems is an important problem. Anticipating tipping points is also a challenging task that requires the ML model to go beyond its training data, with many potential practical applications."}, "weaknesses": {"value": "* The authors only study one specific task: anticipating critical transitions. The title \"Can transformers truly understand dynamical systems?\" is too grand for the actual content of the paper and might mislead the readers.\n* All experiments only included three toy systems, which feels a bit thin to support the strong conclusions.\n* There is no theoretical insights into why transformers fundamentally cannot capture critical transitions. Without such insights, it is unclear whether transformers failed because it is not a suitable architecture, or because the authors' implementation is suboptimal."}, "questions": {"value": "* In parameter-aware RC, the bifurcation parameters are used to drive the reservoir state. However, there is no such internal dynamics in transformers. How did the authors use the bifurcation parameters to \"drive\" the transformers? It is possible that the transformers failed simply because the bifurcation parameters weren't utilized effectively in this particular setup.\n* Anticipating tipping points is a difficult task that requires out-of-distribution generalization. Can you provide a theory to explain why RC can successfully extrapolate and why transformers cannot?\n* I got the impression that RC also cannot reliably predict the collapse time and collapsed states of a system past the critical transition point, so in this sense RC also does not truly \"understand\" dynamical systems, right?\n* It was mentioned that \"One direction is the design of hybrid models, such as reservoir-attention architectures, that combine the dynamical embedding ability of RC with the scalability of Transformers.\" What do authors mean exactly by the reservoir-attention architecture?\n* The authors keep saying that transformers may not be suitable for dynamical systems because the attention mechanism is permutation invariant. But didn't positional encoding address this problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "noXoLSCvI5", "forum": "IMWeaUtZCM", "replyto": "IMWeaUtZCM", "signatures": ["ICLR.cc/2026/Conference/Submission9664/Reviewer_aMYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9664/Reviewer_aMYQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760740789862, "cdate": 1760740789862, "tmdate": 1762921185909, "mdate": 1762921185909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study evaluates whether Transformer architectures can function as faithful digital twins of nonlinear dynamical systems. Using a benchmark task of predicting catastrophic collapse when bifurcation parameters cross critical thresholds, the authors train Transformers and reservoir computing (RC) models on time series generated from safe parameter regimes, and test them on unseen regimes past the point at which the dynamics collapse. Across three nonlinear systems (food chain, power grid, and Ikeda map), Transformers achieve strong short-term forecasts but consistently fail to predict state collapse, instead producing sustained oscillations. In contrast, RC models reliably anticipate critical transitions with high accuracy and minimal data. The findings suggest that permutation-invariant self-attention lacks sensitivity to parameter-induced regime shifts, in contrast to the intrinsic dynamical embedding of RC. This work aims to challenge the claimed generalization capabilities of Transformers for dynamical systems, and proposes critical-transition prediction as a benchmark for assessing digital twin fidelity"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Motivation.** This is a great idea for a paper. Isolating generalization in dynamical systems by studying cessation of chaotic dynamics makes sense. In language modeling, we often see the training data as a distribution over token sequences, and in-distribution generalization represents the ability to successfully generate new samples from this distribution, while out-of-distribution generalization represents the ability to generate samples from a new distribution, given a few examples as context. In a nonlinear system, training on the pre-collapse attractor might cause a transformer to overindex on this specific distribution. Recent works showing that Transformers often approximate Markov chains on language datasets support the idea that these models could be treating dynamical attractors as fixed token distributions, leading to poor generalization. \n\n**Metrics.** I like the idea of probing trained models for their implicit critical points. Bifurcation theory represents a natural way to formalize what “large” versus “small” distribution shift looks like, and the authors’ experiments can, in principle, be applied to any forecast model used to model a dynamical system.\n\nThe authors make an effort to fairly compare models. They try several variants of the transformer architecture, and they make sure to give the transformer as context the same information that they give to the RC as warmup, thus ensuring that both models see the same information.\n\n**Exposition.** The quality of the motivation, exposition, and presentation is high. While I can’t give the paper a full endorsement at this stage, the idea and potential quality are both there, and I can see this paper improving substantially with revision. The paper is timely, particularly given recent interest in non-transformer models (like SSMs) that show signs of exhibiting comparable capabilities.\n\nThe Ikeda map demonstration is a great example, because, as the authors highlight, it contains a nested non-polynomial nonlinearity that cannot easily be expressed using standard terms in an equation library, thus establishing that that RC is doing more than just approximating the right hand side of the dynamics."}, "weaknesses": {"value": "**Data leakage.** I am concerned that the use of parameter-aware RC makes the comparison unfair. While Wp in Eq. 10 is not trained, it provides a generalization signal because the authors train using three different values of the critical parameter, and so, implicitly, information about the directional effect of the parameter is available to the RC.\nThe authors also pass this parameter information to the transformer by appending a channel for it. However, it is not clear from the paper how exactly this is done (the transformer model is only briefly described in the appendix). Do the authors simply append a constant channel to the time series passed to the transformer during training?  If that’s the case, then wouldn’t the fairest comparison be to use a standard RC with that channel also appended to the input time series, rather than isolating it into a special input in Eq. 10? Or, if we want to use parameter-aware RC, we should instead benchmark against a conditional generative transformer. \n\n**Novelty.** I’m a bit concerned that the results simply represent the bias-variance tradeoff in action. The authors repeatedly point out that the RC models use fewer parameters, and require less computing, in order to achieve a given accuracy. But, presumably, RC have substantial limitations in the classes of systems to which they are applicable, which is why they have so far proven unsuccessful for many general time series forecasting benchmarks. I understand that the typical argument in favor of RC is that they are better specifically for dynamical systems, but presumably, given a system I want to forecast, I rarely know in advance how much it acts like a deterministic system versus a (potentially smoothed) noisy one. Finding that a model with strong inductive biases outperforms a model with low inductive biases, particularly in the low-data or low-compute limit, is a standard expectation of the bias-variance tradeoff.\n\nAlong the same lines, this paper’s critique of transformers fails to engage with their broader capabilities that emerge as they scale, like in-context learning. Out-of-distribution generalization has been repeatedly shown for transformers on language tasks. Why aren’t the authors seeing it here? Is the claim that time series are somehow “special” relative to language? My concern is that the reason the authors aren’t seeing any generalization is the experiment design, or hyperparameter choices rather than a fundamental capability in limitation of transformers.\n\n**Reproducibility.** The authors have not opted to make their code available for review. While this is not required by the conference, a study making strong normative claims about the relative merits of two methods should at least make the experiment design and setting clearer. This particularly concerns me, because the authors are benchmarking against a domain-specific model (RC) rather than standard choices like recurrent neural networks. How do we know that hyperparameters were chosen fairly? Were the transformers sufficiently regularized?\n\nOverall, while I am sympathetic to the authors’ efforts to identify limitations of Transformers, the limited choice of datasets, limited baselines, issues with experiment design, and lack of reproducibility make the current paper’s claims overstated."}, "questions": {"value": "1. Why not randomly vary the subcritical parameter across the dataset? Why pick exactly three values for each system? Most in-context learning experiments with transformers treat variation of in-context examples as continuous variables.\n\n2. Can the authors confirm my understanding: the Transformers are given, during test, the exact same time series snippet used to warm up the reservoir computers? So the two models have access to equal information about the out-of-distribution case before they make predictions?\n\n3. Can you clarify how the critical parameter information was passed to the Transformer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wx4yEmTmbR", "forum": "IMWeaUtZCM", "replyto": "IMWeaUtZCM", "signatures": ["ICLR.cc/2026/Conference/Submission9664/Reviewer_L6LU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9664/Reviewer_L6LU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530780298, "cdate": 1761530780298, "tmdate": 1762921185396, "mdate": 1762921185396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether Transformers can serve as faithful digital twins for nonlinear dynamical systems. Through experiments on few benchmark chaotic systems, it finds that while vanilla Transformers perform well in short-term forecasting, they may fail to predict critical transitions and collapses, unlike reservoir computing models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper reveals a failure mode of vanilla Transformers in modeling parameter-dependent dynamical systems, providing a valuable benchmark and case into the limitations of current vanilla transformer models for physical dynamics."}, "weaknesses": {"value": "## **Weaknesses**\n\n1. **Overly broad and potentially misleading title**  \n   The title *“Can Transformers Truly Understand Dynamical Systems?”* is somewhat exaggerated and misleading.  \n   The paper only investigates a narrow empirical phenomenon — the failure of vanilla Transformers to predict bifurcation-induced collapses in a few low-dimensional chaotic systems — rather than addressing the full scope of “understanding dynamical systems.”  \n   A more precise title would better reflect the actual content and contribution.\n\n2. **Unjustified generalization from vanilla Transformer to all Transformer architectures**  \n   The authors evaluate only a **vanilla decoder-only Transformer** and then generalize their conclusions to the entire Transformer family.  \n   This is not rigorous, as many recent Transformer variants — such as physics-informed, causal, continuous-time, or state-space Transformers — are specifically designed to handle temporal causality and dynamical structure.  \n   The observed failure may therefore stem from the limitations of the vanilla configuration, not from fundamental flaws in the Transformer paradigm itself.\n\n3. **Limited experimental scope and absence of pretrained or large-scale models**  \n   The study trains small Transformers from scratch on a few parameter regimes.  \n   To make the conclusions more convincing, the authors should test **transformer-based pretrained or foundation models for dynamical systems**, such as *PANDA: A Pretrained Forecast Model for Chaotic Dynamics*, or other time-series foundation models (e.g., Chronos, TimesFM).  \n   Without such experiments, it is unclear whether the reported failure reflects model design or simply insufficient training diversity.\n\n4. **Lack of theoretical insight beyond empirical observation**  \n   While the paper aims to argue that Transformers fail to learn the true underlying dynamics, the evidence is entirely empirical.  \n   The superior performance of reservoir computing can already be explained by its established theoretical grounding in dynamical systems (e.g., generalized synchronization, echo state property).  \n   In contrast, the Transformer’s failure is not analyzed from a mathematical or dynamical perspective.  \n   Without deeper theoretical or mechanistic insight — such as linking self-attention to Lyapunov stability or bifurcation sensitivity, the work remains observational rather than explanatory.\n\n5. **No code release for reproducibility**  \n   The paper does not mention any code or data release."}, "questions": {"value": "## **Questions for Authors**\n\n1. **Model generalization**  \n   Why do the authors generalize their findings from a vanilla Transformer to all Transformer architectures?\n\n2. **Experimental scope**  \n   Have the authors considered testing pretrained or large-scale time-series models such as *PANDA* or *Chronos*?\n\n3. **Lack of theoretical insight**  \n   Can the authors provide any theoretical analysis or dynamical explanation for why Transformers fail to capture bifurcation behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WbGSaWQr9a", "forum": "IMWeaUtZCM", "replyto": "IMWeaUtZCM", "signatures": ["ICLR.cc/2026/Conference/Submission9664/Reviewer_27Cm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9664/Reviewer_27Cm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683597954, "cdate": 1761683597954, "tmdate": 1762921184966, "mdate": 1762921184966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors challenge the efficacy of transformers for learning from dynamical systems, and then begin able to produce accurate predictions in unseen regimes during training. In particular, the authors challenge transformers against being able to predict the phenomenon of “system collapse” in chaotic systems. They choose 3 low-dimensional parametrized chaotic dynamical systems, which exhibit system collapse only for after some critical parameter value. They generate a training dataset by solving these systems at values below the critical parameter, and they include these parameter values as additional information in the training dataset. This results in no information about the system collapse phenomenon being passed to the model during training. After making sure the model is trained properly on the parameter range it has seen in its training dataset, it is tested beyond the critical parameter point where collapse is expected, and it is evaluated by checking whether it is able to predict the collapse phenomenon. This experimental methodology is applied to 2 types of models: the transformer which is the model under investigation, and to Reservoir Computing (RC) models as a benchmark to compare against. What the results show is that the transformer fails catastrophically at deducing the collapse phenomenon, which highlights limitations that Transformers have in their current state. But also, the alternative model tested by the authors (RC) shows remarkably good ability to capture the collapse phenomenon, a very impressive result."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The results of the paper are strong, the transformer architecture catastrophically fails at the prediction task designed, while the success of the proposed alternative proves that the task is not unreasonably difficult. Further, failure of the transformer architecture is justified in the sense that the training setup is fair and unbiased (see more below)\n- A novel methodology for evaluating the ability to extrapolate to unseen phenomena in the training dataset is established. This sets a significant precedence beyond the types of architecture relevant to this paper (RCs and Transformers).\n- As mentioned, the authors do not only address the stated research question but also provide a strong alternative (RC) which is very successful in the difficult prediction task designed.\n- The investigation of the research question is thorough and detailed. Many potentials pitfalls to the experimentation methodology are addressed such as the fact that there may be a model specific critical parameter beyond which the model collapses, or the fact that the model may eventually collapse if the trajectories are propagated further in time. The reader feels confident on the experimentation (see ‘Weaknesses’ for more details) and the choices taken by the authors are justified.\n- The presentation of the material is of high quality, providing plenty of clear discussion and interesting avenues for future research, such as the attempt to mitigate the scaling limitations of RC by defining a hybrid approach combining them with transformers. Perhaps most interesting is the attempt to better understand the success of RCs (and respectively failure of transformers), in order to design better methodologies for learning from dynamical systems."}, "weaknesses": {"value": "- The claim that (Line 88-89) “…this is the first work to systematically challenge the effectiveness of transformers as digital twins of dynamical systems” seems a bit too broad in scope.  A potential counterexample could be the 2023 paper published in IEEE CSS with title “Can Transformers Learn Optimal Filtering for Unknown Systems?”, Du Z. et. al. Limiting the scope of the statement, by e.g., specifying the chaotic phenomenon under investigation (system collapse), or even just the fact that this paper’s focus is chaotic dynamical systems, will improve on this point.\n- Another point of criticism is the complexity of the 3 example dynamical systems chosen. The authors do mention that the last one (Ikeda map) is a particularly challenging one, and the examples are clearly “difficult-enough” to show that the Transformer architecture fails in the task. Nevertheless, the RC seems to be highly performant in all of them. In this respect, it could be beneficial (in order to highlight the limitations of the alternative proposed) to have included an example where the RC is showing significantly lower performance, perhaps something of higher dimensionality, since as the authors say one of the limitations of RC is its scalability issues. In particular, in a higher dimensional chaotic system exhibiting system collapse (even an artificially constructed one, made by building on one of the examples in a cartesian way), these scalability issues of RC might become more apparent. In any case, RC is not the main thesis of this paper, so I consider this a minor point, especially when weighted against the difficulty of incorporating an extra example.\n- A final minor point is the convergence of the reported statistics. It could have been good to include in the appendix (which is not being reviewed) some indicative convergence plot for the statistics being reported in Table 1. There is not really a serious doubt about convergence, though a convergence plot with respect to number of simulations (in this case 1000) would have increased the confidence of the reader even further.\n\nMinor Typos/ Styling **Suggestions**:\n- Line 51-52: “An interesting perspective for interpreting RCs is that the dynamics…”\n- Line 52: Replace “an generally” with “a generally”\n- Line 107: Consider removing “for dynamical systems”.\n- Line 203: Expand “NLP” definition, it is not defined anywhere above.\n- Line 318-319: I’m having trouble interpreting the last sentence, it has some grammatical errors. Consider changing it to “In this setting, none of our experiments with Transformers resulted in success, despite extensive tuning…”.\n- Line 401-402: Consider removing “but then”."}, "questions": {"value": "Questions:\n\n- When it comes to deducing the collapse phenomenon not observed in the training dataset, I am curious whether it is fair to expect this from any model. The answer is seemingly positive, since RCs perform so well, but still I wonder if it is possible to design a counterexample by adjusting one of the presented dynamical systems, such that its restriction on the training parameter range performs **identically** with the original one, but beyond the critical parameter it does not exhibit collapse, but remains oscillatory in nature. If such an example exists, then the results would be reversed, RCs catastrophically failing and Transformers succeeding (probably). I would like to better understand whether such an example is possible in 3D (resp. 4D or Complex1D for the other examples) or if there are smooth continuation constraints that do not allow it to exist. Do you think that such an example can be designed? If not, why? If yes, how come the RCs predict accurately?\n- I noticed in Figure 3d, that the RC collapse is different than the system's collapse (they collapse to different values). Granted, the collapse phenomenon is predicted by the RC, but I am wondering if you have any suggestions for designing a methodology for RCs that would allow not only the prediction of collapse phenomena, but also the exact way the collapse will take place.\n- Have you/ did you consider other phenomena than system collapse? I am curious what would be the next phenomenon you would test against, perhaps more challenging than system collapse, where you expect RCs to begin having trouble. In general, (though I definitely am not suggesting you attempt to include this in the paper, given how much effort is required) I would be very curious to see a table similar to Table 1, where an array of different phenomena are evaluated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8OmfVxPp4L", "forum": "IMWeaUtZCM", "replyto": "IMWeaUtZCM", "signatures": ["ICLR.cc/2026/Conference/Submission9664/Reviewer_iggh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9664/Reviewer_iggh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866125650, "cdate": 1761866125650, "tmdate": 1762921184352, "mdate": 1762921184352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of main changes"}, "comment": {"value": "In response to the referees' comments, we have\n\n1. revised our title to ``Can transformers predict system collapse in dynamical systems?''\n\n2. clarified heuristically the differences in the main mechanism between reservoir computing and transformer,\n\n3. provided new results on the spatiotemporal Kuramoto-Sivashinsky system,\n\n4. evaluated different transformer configurations and foundations models on system collapse prediction,\n\n5. updated our codes on OpenReview,\n\n6. corrected minor issues, such as typos, symbols, and grammars."}}, "id": "tMfbngdWYF", "forum": "IMWeaUtZCM", "replyto": "IMWeaUtZCM", "signatures": ["ICLR.cc/2026/Conference/Submission9664/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9664/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission9664/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763767040252, "cdate": 1763767040252, "tmdate": 1763767040252, "mdate": 1763767040252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General reply"}, "comment": {"value": "Dear Area Chair:\n\nThank you very much for obtaining four referee reports on our manuscript #9664 previously entitled ''Can transformers truly understand dynamical systems?'' (revised title: ``Can transformers predict system collapse in dynamical systems?''). We have carefully revised the paper to fully address all the referee comments. A summary of the main changes made and a point-to-point response to all referee comments are provided. The changes in the text are marked blue.\n\nWe believe that a thorough discussion with all referees would help further clarify any remaining concerns, so that they can choose to reassess our paper. We sincerely appreciate your assistance throughout this process.\n\nWe would like to thank you again for your wonderful and professional handling of our manuscript. We hope our revised manuscript can be judged to have met the high standards of ICLR.\n\nBest regards, \n\nAll authors"}}, "id": "sGDigCsYRV", "forum": "IMWeaUtZCM", "replyto": "IMWeaUtZCM", "signatures": ["ICLR.cc/2026/Conference/Submission9664/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9664/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission9664/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763767258997, "cdate": 1763767258997, "tmdate": 1763767258997, "mdate": 1763767258997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General reply"}, "comment": {"value": "Dear Referees:\n\nWe thank you all for your careful evaluations and thoughtful feedbacks on our manuscript. Your comments have been extremely valuable, helping us better understand how researchers from different perspectives view this line of work, and inspiring several directions we had not previously considered.\n\nA point-by-point response to each report has been uploaded, and all changes in the text are mark in blue. We greatly appreciate the time and effort you invested in the review, and we hope that our revisions adequately reflect your suggestions.\n\nWe welcome any further feedback you may have on the updated version, and we would be glad to continue constructive discussion as needed. Thank you very much for your time and consideration.\n\nBest regards,\n\nAll authors"}}, "id": "PW7I3kuDhE", "forum": "IMWeaUtZCM", "replyto": "IMWeaUtZCM", "signatures": ["ICLR.cc/2026/Conference/Submission9664/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9664/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission9664/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763767348419, "cdate": 1763767348419, "tmdate": 1763767348419, "mdate": 1763767348419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}