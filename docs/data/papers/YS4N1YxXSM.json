{"id": "YS4N1YxXSM", "number": 14170, "cdate": 1758229589925, "mdate": 1759897386247, "content": {"title": "QuoKA: Query-Oriented KV Selection for Efficient LLM Prefill", "abstract": "We present QuoKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QuoKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselecting the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3× reduction in time-to-first-token, 5× speedup in attention on Nvidia GPUs and up to nearly a 7× speedup on Intel Xeon CPUs, QuoKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.", "tldr": "", "keywords": ["Efficient LLM Inference", "LLM Prefill Acceleration", "Sparse Attention", "KV Cache Subselection", "Training-Free"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb037e9657c1080adab16fcf3d228d8975f81173.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present QUOKA, a training-free sparse attention algorithm for efficient LLM prefill. The method prioritizes queries with low cosine similarity to the mean to subselect keys. This achieves a 3-7x speedup and an 88% reduction in key-value pairs, while maintaining near-baseline accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The paper introduces a heuristic for query selection based on the hypothesis that queries with greater angular distance from the mean are more informative. This provides a geometric perspective on the multi-query attention problem that is distinct from conventional approaches focused on representativeness.\n- The algorithm is designed to be training-free and hardware-agnostic by avoiding the use of custom kernels. This design allows for its potential integration into various inference systems without requiring model retraining or fine-tuning.\n- The paper's experiments report reductions in Time-to-First-Token and attention latency across multiple hardware platforms. The results also show that these efficiency improvements are achieved while task accuracy is maintained close to that of the dense attention baseline."}, "weaknesses": {"value": "- The paper does not include a comparison to alternative query selection strategies, such as selecting representative queries via clustering (e.g., K-Means centroids). Without this comparison, it is difficult to fully assess the performance of the proposed \"outlier query\" heuristic relative to more established methods for summarization.\n- The evaluation of the method's applicability to generation tasks appears less developed. Since the core query subselection component is bypassed in the single-query decoding scenario, a comparison against baselines specifically designed for decoding-phase KV management would be necessary to fully substantiate the method's competitiveness in this setting.\n- The potential impact of quantization on the method's performance is not discussed. The algorithm's reliance on precise geometric relationships (via cosine similarity) means its robustness in low-bit precision environments, which are common on its target hardware, remains an important but unevaluated factor."}, "questions": {"value": "- Could you provide a more direct comparison or discussion against a \"central representativeness\" approach, such as selecting K-Means query centroids? This would help to empirically situate the performance of your proposed heuristic.\n- The assumptions in Theorem 1 are central to the method's motivation. How consistently do these geometric conditions hold empirically across different models and layers? Supporting statistics or visualizations would be valuable.\n- For the Math500 experiments where query subselection is not applied, could you please clarify the exact mechanism of QUOKA? Specifically, how are keys selected for the single active query during the generation phase?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pHTbkvPR0P", "forum": "YS4N1YxXSM", "replyto": "YS4N1YxXSM", "signatures": ["ICLR.cc/2026/Conference/Submission14170/Reviewer_JhZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14170/Reviewer_JhZf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556754210, "cdate": 1761556754210, "tmdate": 1762924629328, "mdate": 1762924629328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Due to the quadratic nature of attention, long-context prefill remains a major challenge for large language models (LLMs). However, each query only needs to attend to a subset of key–value (KV) pairs to achieve reasonable performance. Existing sparse attention methods either rely on fixed attention patterns or are optimized primarily for the generation phase.  \nThis paper presents **QUOKA**, which identifies representative queries from each chunk based on the smallest cosine similarity to the mean query vector, then uses these representative queries to locate important KV pairs. The model finally performs full query–subset KV attention. QUOKA achieves strong performance on long-context benchmarks and is efficient to implement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The evaluation is comprehensive and the reported results are impressive.  \n- The proposed method is clearly described, and the paper is easy to follow."}, "weaknesses": {"value": "- Hardware efficiency may degrade due to the small chunk size and discontinuous KV selection."}, "questions": {"value": "Thanks for submitting to ICLR 2026. This paper introduces an interesting idea of filtering query vectors using cosine similarity, inspired by DiffKV’s approach to KV cache filtering. However, I still have some concerns about the motivation and the efficiency claims.  \n\n## 1. Intuition behind the “critical” query vectors\nThe intuition for using “critical” query vectors is not fully convincing. It is true that such queries are closer to the key vector space and may attend to a wider range of keys or exhibit higher variance in attention scores. However, since the softmax operation is applied independently to each query, the proposed approach only ensures that these selected queries have smaller attention errors. It does not necessarily guarantee that other queries in the same group will also exhibit small errors. Intuitively, it is unclear why these tokens should be more important for overall generation accuracy.  \n\n## 2. Limited benefit in the generation phase\nIt is unclear how this method can lead to meaningful speedups during the generation phase. QUOKA estimates the similarity between each query and all keys, but since generation involves only one query vector, this step effectively performs half of the full attention computation. Even after selecting top-k keys and multiplying by the corresponding values, the overall computational reduction—and thus the speedup—appears minimal.  \n\n## 3. Hardware inefficiency due to small chunks\nIn the evaluation, the block size is set to 128. However, this configuration is inefficient on modern hardware, as each GEMM or attention operation on such small blocks yields low arithmetic intensity and thus lower TFLOPs. This effect is particularly noticeable on H100 and B200. In your latency test, do you also use block size 128 for the full attention baseline? A fairer comparison would allow full attention to use larger block sizes (e.g., 1024 or 2048), which are more hardware-efficient.  \n\n## 4. Constraints on KV selection\nAre there any constraints imposed on the selected KV pairs? If the selected KVs are discontinuous, how is self-attention computed efficiently using existing kernels? Discontinuous memory access patterns can severely hinder performance unless carefully optimized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dTCGbCfmSz", "forum": "YS4N1YxXSM", "replyto": "YS4N1YxXSM", "signatures": ["ICLR.cc/2026/Conference/Submission14170/Reviewer_nDFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14170/Reviewer_nDFH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903952001, "cdate": 1761903952001, "tmdate": 1762924628787, "mdate": 1762924628787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a better chunked prefill technique which only uses a small subset of KV cache for each chunked prefill. The algorithm goes like this -- choose a subset of queries to select KV cache, compute scores for these queries and aggregate across heads and queries, choose topk scoring KV for chunked prefill. They show that their method outperforms a bunch of baselines at same sparsity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n1. Great experimental breadth and strong performance compared to baselines. The experiments cover multiple benchmarks and baselines. \n2. Easy to understand algorithm."}, "weaknesses": {"value": "1. The approach is not very principled. While it is true that queries with high similarity with K will have low similarity with Mean(Q) due to OOD nature of query and key distributions (this is what theorem says), the converse is not true (this is what you want for efficiency) . It is especially not true in high dimensions -- where it is highly likely that queries with low similarity with Mean(Q) would also have low similarity with K. \n\nSo this being a critical component of algorithm is unsettling. I would assume that most queriers chosen are actually even worse than Mean(Q) w.r.t similarity with K.  Can we have distribution plots of cosine similarities of chosen queries vs. all the queries. \n\nHaving said that their experimental section strongly supports their method.\n\n2. some latest baselines are missing -- duoattention, xattention, spargeattention, might be good to add discussion / results for these."}, "questions": {"value": "1. Can we have plots for cosine similarities (K, q) for chosen queries and all the queries."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TyCuna1DpX", "forum": "YS4N1YxXSM", "replyto": "YS4N1YxXSM", "signatures": ["ICLR.cc/2026/Conference/Submission14170/Reviewer_gdtt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14170/Reviewer_gdtt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952228293, "cdate": 1761952228293, "tmdate": 1762924628217, "mdate": 1762924628217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel sparse attention method for improving the efficiency of transformer models during the prefill stage with chunked prefill. The idea is to select a subset of important queries by divergence to the mean query and only compute attention for those important queries and important key-value pairs. The method is evaluated on SOTA open-source LLMs against other sparse attention baselines and shows better efficiency and performance frontier."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is well-motivated with insight experiments\n- The pseudo-code is extremely helpful in understanding the method\n- The experiment is solid"}, "weaknesses": {"value": "- Some explanations of the claims are confusing"}, "questions": {"value": "Thank you for your submission. I like the paper overall and think the method is well motivated. I particulary enjoy the insight experiments in Figure 2 and 3, which make the motivation very clear. However, some claims and descriptions in the paper are poorly explained and a bit vague to me. I would appreciate it if the authors can clarify the following questions:\n- What is the relationship with chunked prefill? The proposed method seems to be highly dependent on chunked prefill, but the relationship is not very clear to me. I can understand that some previous sparse attention methods can be inefficient under prefill with multiple queries due to aggregated sparsity. But why is chunked prefill specifically needed for the proposed method? Is it possible to use the proposed method without chunked prefill?\n- \"however, due to dynamic compute graph and KV cache memory bandwidth overhead under chunked prefill, their benefits are limited.\" Can you please elaborate more on this point? Why dynamic compute graph and KV cache memory bandwidth overhead limit the benefits?\n- \"During prefill, when relevant KVs are selected for many queries at once, this can result in significant performance degradations. Under chunked prefill, where important KVs are repeatedly subselected for multiple queries, these degradations become more pronounced.\" These two sentences are particulary confusing to me. Why chunked prefill makes the performance degradation more pronounced?\n- The gather operator in algorithm 1 has inconsistent notations (at line 4 and line 12).\n- “As discussed in Section 2, existing sparse attention methods face limitations in prefill efficiency and portability.” What do you mean by portability here?\n- I don't fully understand the query selection process. Many of the previous sparse attention methods also reduce among the KV dimension, so the attention socres are approximated with partial KV, however, we still get the full attention scores for all queries. In this paper, it seems that you only select a subset of queries, does this means some of the queries are pruned? Does this means it is somehow similar to the previous work on token pruning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IL1r83bJgo", "forum": "YS4N1YxXSM", "replyto": "YS4N1YxXSM", "signatures": ["ICLR.cc/2026/Conference/Submission14170/Reviewer_gbPS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14170/Reviewer_gbPS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970997506, "cdate": 1761970997506, "tmdate": 1762924627926, "mdate": 1762924627926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}