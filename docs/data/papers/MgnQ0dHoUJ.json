{"id": "MgnQ0dHoUJ", "number": 2403, "cdate": 1757073936412, "mdate": 1759898151209, "content": {"title": "UPrompt: Bidirectional Multi-granularity Learning for Vision-Language Models", "abstract": "The prompt learning paradigm for vision-language models is effective yet faces the dilemma of balancing granularity: global prompts lack fine-grained semantic awareness, while local prompts ignore overall contextual associations, leading to limited cross task generalization. This dilemma exists in dense prediction tasks.\nInspired by the U-Net framework that unifying multi-level representations across different granularities, we propose UPrompt, a novel bidirectional multi-granularity prompt learning framework for vision-language models.\nSimilar to how U-Net integrates fine and coarse features through symmetric encoder-decoder pathways with cross-level connections, UPrompt constructs parallel multi-granularity representations in both visual and textual modalities, where coarse-to-fine cascaded enhancement propagates global contextual information to refine local details, while fine-to-coarse hierarchical supervision ensures semantic consistency across scales. \nExtensive experiments on 17 benchmarks validate our effectiveness. Our method outperforms MAMET and VPKE by +4.1 and +7.3 rSum on MSCOCO, surpasses CoCoA-Mix by +5.09\\% in base-to-novel generalization, while maintaining competitive performance with minimal overhead (coarse-grained) and matching PSRC with 1/3 cost (medium-grained).", "tldr": "", "keywords": ["prompt learning", "multi-granularity learning", "vision-language models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea7356270454636fa9eea0421d3f168822a89b90.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a coarse-to-fine prompt learning approach that considers different levels of granularity. A set of learnable embeddings are progressively downsampled and upsampled through a UNet, with per-level exits for each granularity. Using two similar UNets for each modality, a per-level supervision can be applied to each granularity. The outputs, along with the visual and text features at each granularity, are then forwarded to the corresponding encoders, giving rise to the standard similarity loss. The results show marginal improvement w.r.t. existing methods, and ablation studies showcase the contribution of each of the elements introduced in the paper."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes an interesting paradigm to prompt learning whereby different levels of granularity are combined and can be used according to the task under consideration. There is some merit in the method, as well as novelty, that are worth considering."}, "weaknesses": {"value": "The paper is poorly written and notation is loosely used. Figure 2, which is essential to the understanding of the method, lacks a proper legend and flow explanation. It is not clear to me for example if the visual representations from the input image are computed at different granularities or these are directly taken from the CLIP’s last layer output. My understanding is that only E^K is learnable and the rest of the layers are computed from the previous ones, but I am not completely sure from examining Figure 2. It would be also good to mention in Section 3 how the prompts are gradually made coarser or finer (it is just mentioned in l. 295 that Llava is used). \n\nIn l. 182 it is mentioned that “Visual features integrate granularity-specific prompts”. How? How are the granularity-specific prompts defined? \nIn l. 192 it is mentioned how the alignment is measured in a granularity-specific layer, but it is still not clear at this point exactly what is being learned and what is computed from pooling the learned parameters. Please clarify.\nIn l. 199-202 it is mentioned that “Simple granularity stacking in (…). To address these challenges (…) (Fig. 2)”. How exactly this is depicted in Fig. 2 is not clear to me, given the loosen informative nature of Fig. 2.\nIs there any form of attention between the input image and the image embeddings during the forward to the UNet? It is not clear from Fig. 2 if this is actually the case. I understand that different level of visual features are used but it is not clear how. For the text case, this is more clear as it is specifically mentioned in Fig. 2. \nMy understanding considering the above is that the learnable prompts are a 14x14 tensor with M (undefined?) channels, that are forwarded to a 4-layer UNet producing in the decoding part a set of 4 level-specific prompts. The UNet, along with the embeddings, is learned using per-level supervision and the full prompt learning Lguide loss. Please clarify. \n\nThe results in Table 2 are not really promising compared to state of the art works. \nHow sensitive is the method to using a different caption rewriter (i.e. instead of Llava)? Similarly, how sensitive is the method to using a different prompt resolution (i.e. beyond 14x14)? \n\nIn summary, the writing and presentation give rise to many concerns and doubts regarding the reproducibility of the proposed approach, which need further clarification. The method is technically sound and seems to produce reasonable results, and therefore I am borderline, leaning towards accept, with this paper, hoping for further clarification from the authors."}, "questions": {"value": "Please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5X5dyAq5bP", "forum": "MgnQ0dHoUJ", "replyto": "MgnQ0dHoUJ", "signatures": ["ICLR.cc/2026/Conference/Submission2403/Reviewer_KDVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2403/Reviewer_KDVr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767367115, "cdate": 1761767367115, "tmdate": 1762916220860, "mdate": 1762916220860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UPrompt, a U-Net-inspired framework to fix the granularity trade-off in VLM prompt learning—where global prompts miss fine details and local prompts lack global context. It uses two key components: Coarse-to-Fine Cascaded Enhancement (CE, injects global context into fine features via cross-attention) and Fine-to-Coarse Hierarchical Supervision (HS, uses finest-grained alignment to regularize coarser levels). Tested on 17 benchmarks, UPrompt outperforms baselines in cross-modal retrieval (e.g., 571.1 rSum on Flickr30K), few-shot classification (85.13% 16-shot accuracy), and OOD generalization, while offering performance-efficiency flexibility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Targets a clear, unaddressed gap: single-granularity limits in VLM prompting, with direct links to performance flaws in existing methods.\n\n2. Innovative U-Net adaptation: modality-specific granularity (spatial pooling for vision, semantic enrichment for text) plus bidirectional flow—backed by theoretical proofs (Propositions 1-2) rare in multi-granularity prompt work.\n\n3. Rigorous experiments: ablations isolate CE/HS value, efficiency analyses show UPrompt-M matches PSRC’s accuracy with 1/3 cost, and visualizations confirm CE/HS work as intended.\n\n4. Practical: adaptive granularity lets users pick coarse (low cost) or fine (high performance) setups."}, "weaknesses": {"value": "1. Manual granularity design: 4 levels for classification/3 for retrieval are chosen without guiding heuristics, reducing usability for non-experts.\n\n2. Llama 3-8B dependence: no tests on smaller LLMs (e.g., Llama 3-1B) to see if text hierarchy quality holds, or how LLM overhead offsets UPrompt’s efficiency gains.\n\n3. Limited HS failure analysis: reversed “Coarse-to-Fine Supervision” performs poorly, but no examples (e.g., bad attention maps) show why coarse signals mislead fine modeling."}, "questions": {"value": "1. Do you have preliminary data on how granularity count (3,5,6) affects tasks like FGVCAircraft vs. MSCOCO? Can you give a simple heuristic for choosing levels?\n\n2. How does swapping Llama 3-8B for smaller models (e.g., T5-small) hurt/help text granularity and downstream performance? What’s the LLM’s share of UPrompt’s total compute?\n\n3. If the finest-grained alignment has errors (e.g., mislabeled pairs), does HS spread those errors to coarser levels? Any tests on HS robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e7vBCqBl05", "forum": "MgnQ0dHoUJ", "replyto": "MgnQ0dHoUJ", "signatures": ["ICLR.cc/2026/Conference/Submission2403/Reviewer_gXNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2403/Reviewer_gXNe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966507117, "cdate": 1761966507117, "tmdate": 1762916220649, "mdate": 1762916220649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UPrompt, a U-Net–inspired multi-granularity prompt-learning framework for adapting Vision–Language Models. It addresses the well-known granularity dilemma in prompt learning—global prompts capture overall semantics but miss fine detail, while local prompts capture details but lose global context. Extensive experiments on 17 benchmarks show consistent improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well organized and easy to follow.\n- Introduces U-Net's philosophy into multi-granular prompt learning, exploring bidirectional information flow across modal granularities with demonstrated effectiveness;\n- Multi-granularity attention maps qualitatively support the claims of semantic consistency and contextual coherence."}, "weaknesses": {"value": "- Limited Novelty. Several recent works like TAP and HiCroPL, already explore multi-level or hierarchical prompts. UPrompt mainly formalizes these ideas within a U-shaped structure rather than introducing a fundamentally new mechanism. Besides, in the design, the number of granularities and textual levels are manually predefined, which lacks adaptive granularity selection, which limits scalability and automation.\n- The textual side depends on Llama-3 generation heuristics. This may inject bias and complicate reproducibility. The paper does not analyze sensitivity to prompt-generation quality.\n- The methodologies for constructing multi-level image and text granularities require further enrichment. Current approaches primarily rely on pooling and text attribute addition, lacking comprehensive comparisons between different granularity construction techniques\n- In the granularity ablation study (Figure 4, left), performance continues to rise. It remains uncertain whether the peak performance has been reached, and how different granularity interval strategies might affect results"}, "questions": {"value": "Please highlight the significant novel designs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x2N27JARmI", "forum": "MgnQ0dHoUJ", "replyto": "MgnQ0dHoUJ", "signatures": ["ICLR.cc/2026/Conference/Submission2403/Reviewer_GiLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2403/Reviewer_GiLu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978221816, "cdate": 1761978221816, "tmdate": 1762916220514, "mdate": 1762916220514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UPrompt, a U-Net-inspired bidirectional multi-granularity prompt learning framework for vision–language model (VLM) adaptation. The core ideas are:\n1. Coarse-to-Fine Cascaded Enhancement (CE) – injects global context from coarse layers into fine layers;\n2. Fine-to-Coarse Hierarchical Supervision (HS) – distills knowledge from the finest layer to coarse ones to mitigate semantic drift.\n\nThe method achieves consistent gains across cross-modal retrieval, few-shot classification, base-to-novel generalization, and out-of-distribution (OOD) tasks, while enabling a controllable trade-off between accuracy and computational cost.\n\nOverall, this paper proposed a reasonable idea, and the results looks reasonable. The multi-granularity + bidirectional flow idea is intuitive and broadly applicable. If the authors can supplement comparisons with stronger backbones, provide the resolution × layer ablation, and clarify reproducibility for LLM-generated text hierarchies, I would lean toward accept. However, the authors should also clearly address my concern in order to at least keep my original score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea is conceptually intuitive yet systematic, the transfer of U-Net’s “multi-scale + skip connection” idea into “multi-granularity prompts + bidirectional information flow” looks intuitive to me.\n\nThe coarse-to-fine CE and fine-to-coarse HS sounds reasonable. The proof seems to be correct and easy to understand.\n\nComprehensive experiments covering retrieval, classification, base-novel, and OOD, with clear reporting of cost-vs-performance trade-offs.\n\nImplementation details (e.g., temperature, prompt length) are good enough to understand conceptually."}, "weaknesses": {"value": "1. Text hierarchy generation via Llama 3-8B introduces possible prior leakage and reproduction variability; comparisons with purely templated or rule-based text construction would clarify fairness.\n\n2. Lack of comparison with stronger or newer VLM backbones such as SigLIP/SigLIP-2, EVA-CLIP, or E5-V; the reported results are all on CLIP ViT-B/16.\n\n3. Self-distillation bias: HS uses the model’s own fine-grained layer as the teacher; if that layer mis-aligns, the error may propagate.\n\n4. Complexity analysis could be deeper: CE’s cross-granularity attention likely adds overhead, but FLOPs/memory/throughput statistics are not that detailed. Fig.5 is very complex and a little bit hard to understand.\n\n5. Statistical significance: averages are reported across datasets, but per-dataset variance or confidence intervals are missing. Also, Table 6 in supp has wrong bold for UCF task. Is this a typo? This increases my skepticism towards the overall soundness of the reported results."}, "questions": {"value": "1. Experiments fix the input at 224×224 (14×14 tokens). If higher-resolution inputs (336/384) or denser backbones (ViT-L/H) are used, does the optimal number of granularities K change? Could you provide a resolution × layer-count ablation to disambiguate “benefit from more layers” vs. “compensation for limited resolution” or at least ablation study results with another resolution?\n\n2. Have the authors tested UPrompt on or against SigLIP / SigLIP-2 and other recent contrastive VLMs? Since UPrompt acts at the prompt level, it should in principle transfer; evidence of such portability would strengthen the claim of generality.\n\n3. If a different LLM (e.g., Qwen, Mixtral) or a rule-based templating scheme is used to form the text hierarchies, how sensitive are the results? A cross-LLM study or a purely templated baseline would clarify robustness, though I think this won't be a big problem. The authors can discuss this if time is not allowed to add experiments.\n\n4. When the finest layer provides noisy supervision, does HS amplify the error? Have the authors explored soft teacher mixing (e.g., weighted fine + medium layers) or consistency regularization to mitigate self-distillation bias? This would provide more insightful understanding to the effectiveness of UPrompt.\n\n5. Currently the inference layer is manually chosen. Could a lightweight gating or uncertainty-based controller dynamically select the appropriate granularity at test time for adaptive efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Aq7sB9W79", "forum": "MgnQ0dHoUJ", "replyto": "MgnQ0dHoUJ", "signatures": ["ICLR.cc/2026/Conference/Submission2403/Reviewer_WPAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2403/Reviewer_WPAs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077389578, "cdate": 1762077389578, "tmdate": 1762916220360, "mdate": 1762916220360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}