{"id": "NX6oMuoLxu", "number": 609, "cdate": 1756753756294, "mdate": 1759898250629, "content": {"title": "Layer-Wise Analysis in Exploring the Normalization Strategies in Mamba", "abstract": "The Mamba architecture achieves linear time and memory complexity in long-sequence modeling and vision tasks through a dynamic, input-conditioned state transition mechanism and hardware-efficient scan operations. However, as network depth increases, the state space model (SSM) component tends to amplify activation magnitudes during the forward pass, often leading to gradient explosion. This highlights the urgent need for a systematic normalization design to balance training stability and convergence speed. To address this, we analyze training stability by tracking (i) the spectral norm of the output projection weights and (ii) the largest eigenvalue of the joint input-output covariance matrix, demonstrating the effectiveness of Norm2 (post-SSM) in suppressing activation and gradient scale inflation. From an optimization efficiency perspective, we use K-FAC to pproximate the Fisher Information Matrix and show that Norm1 (pre-SSM) significantly reduces the condition number of per-layer gradients, thereby accelerating convergence. Furthermore, we propose a composite normalization strategy (BN→SSM→LN), combining BatchNorm at the input and LayerNorm at the output of SSM. We evaluate this strategy across a broad range of benchmarks. Experimental results demonstrate that the composite scheme consistently outperforms single or no normalization in both convergence speed and final accuracy. We hope this work provides both theoretical insights and empirical guidance for normalization in designing SSM-based models.", "tldr": "Too Long; Didn't Read", "keywords": ["Mamba", "Normalization", "Stability", "Optimization", "Scale invariance", "Condition number"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a15a54549f7420efcaedb33e09257e05483c6de7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how different normalization layer combinations affect the training stability and efficiency of Mamba1 models. The authors identify that the state space module (SSM) tends to amplify activations, leading to instability in deep networks. To address this, they propose a composite normalization strategy (BN→SSM→LN), applying BatchNorm before and LayerNorm after the SSM to balance stability and optimization efficiency. Using Kronecker-Factored Approximate Curvature (K-FAC) analysis, they show that this configuration improves the conditioning of the optimization landscape and accelerates convergence. Experimental results across language, vision, and sequence tasks demonstrate that the BN→SSM→LN scheme consistently outperforms single-normalization or unnormalized baselines. Overall, the study provides a principled normalization guideline for improving the trainability of Mamba-based architectures."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The study provides a novel perspective on normalization in state space models (SSMs), addressing an underexplored yet important aspect of SSM training. The use of Kronecker-Factored Approximate Curvature (K-FAC) as a metric to assess model trainability is both innovative and insightful. Furthermore, the proposed normalization configuration is comprehensively evaluated across multiple domains, including natural language processing (WikiText-103, IMDB), computer vision (ImageNet-100, COCO, ADE-20K), and sequential reasoning tasks (ListOps, Pathfinder), demonstrating its robustness and broad applicability."}, "weaknesses": {"value": "While the paper presents an interesting investigation into normalization strategies for SSM-based architectures, I have several concerns:\n- Lack of clarity in experimental setup: The authors do not clearly specify the model architectures and their sizes in the main text, which makes it difficult to assess or compare the reported results and accuracies. Although some of these details appear in the appendix, including key configurations and parameter counts in the main paper would improve transparency and reproducibility.\n- Insufficient theoretical connection for K-FAC analysis: While the use of Kronecker-Factored Approximate Curvature (K-FAC) as a metric for evaluating trainability is novel, the paper does not establish a clear theoretical link between this metric and the optimization behavior of SSMs, leaving the interpretation somewhat empirical.\n- Lack of theoretical justification for the proposed scheme: The BN→SSM→LN normalization strategy, though empirically effective, lacks theoretical grounding or formal analysis to support why this configuration works better than alternatives."}, "questions": {"value": "- Could the authors extend their proposed normalization strategy to the Mamba2 architecture, evaluating its effectiveness across models of different scales (e.g., from 130M to 8B parameters)?\n- How does the BN→SSM→LN configuration compare to the default RMSNorm→SSM→RMSNorm normalization used in Mamba2? A direct comparison would help clarify whether the proposed scheme provides consistent benefits across newer Mamba variants.\n- What is the rationale for placing normalization before the gated multiplication? In the Mamba2 design (see Figure 6 in the Mamba2 paper), normalization is applied after the gating operation. Could the authors discuss the design difference and its potential implications?\n- Could the authors theoretically establish the relationship between K-FAC metrics and model optimization, beyond empirical correlation? This would strengthen the justification for using K-FAC as a measure of trainability.\n- Finally, could the authors provide a theoretical explanation or formal proof for why the BN→SSM→LN configuration outperforms other normalization combinations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KsGCpzmWVZ", "forum": "NX6oMuoLxu", "replyto": "NX6oMuoLxu", "signatures": ["ICLR.cc/2026/Conference/Submission609/Reviewer_jioT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission609/Reviewer_jioT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760821680125, "cdate": 1760821680125, "tmdate": 1762915565191, "mdate": 1762915565191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing the training instability and optimization inefficiency of the Mamba architecture. Authors propose the BN->LN. Next, they use optimization metrics to prof its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The author has proposed an excellent research topic. It is indeed true that SSM will lead to the expansion of the activation range, posing relatively significant challenges to training stability. The author has elaborated on the background of the article quite clearly, and the structure of the paper is complete and reasonable."}, "weaknesses": {"value": "Main issues:\n1. The main modification of the paper to the Mamba architecture is BN->LN, but I do not figure out a clear motivation for it, i.e., why not other combinations (like LN->BN or other normalization methods)? Though authors provide related ablations, a more theoretical analysis is required.\n2. Experimental results are not compared with mentioned baselines in the \"Introduction\" section.\n3. The experiment is not convincing enough, like more models (Mamba-ND) and more taks (zero-shot).\n\nOther issues:\nThe font size in Figure 4.5.6. is too small."}, "questions": {"value": "Since the author proposes using the method in 3.4 to illustrate the effectiveness of BN->LN, how to demonstrate the effectiveness of the method in 3.4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pxvyX2zWwl", "forum": "NX6oMuoLxu", "replyto": "NX6oMuoLxu", "signatures": ["ICLR.cc/2026/Conference/Submission609/Reviewer_MVpq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission609/Reviewer_MVpq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914333035, "cdate": 1761914333035, "tmdate": 1762915564950, "mdate": 1762915564950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates training instability in the Mamba architecture, focusing on the amplification of activations and gradient explosion as network depth increases. The authors propose a composite normalization strategy (BN→SSM→LN) to improve both stability and optimization efficiency. Through spectral norm, input–output covariance eigenvalue, and K-FAC condition number analyses, they show that post-SSM LayerNorm (LN) stabilizes training, while pre-SSM BatchNorm (BN) accelerates convergence. Experiments across vision, language, and long-sequence benchmarks demonstrate consistent improvements over single normalization or none."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies instability caused by SSM amplification and constructs a logical two-stage normalization solution supported by layer-wise metrics and optimization theory.\n2. The use of spectral norms, eigenvalue tracking, and K-FAC condition numbers provides interpretable evidence linking normalization placement to training stability and optimization behavior."}, "weaknesses": {"value": "1. The methodological novelty primarily lies in systematic analysis and positional design rather than new normalization algorithms. The relation to Transformer Pre-LN, DeepNorm, and scaling techniques is discussed but not deeply unified.\n2. BatchNorm’s inference-stage instability remains underexplored. Although training improvements are shown, robustness under small-batch or distribution-shift conditions is not evaluated, limiting applicability to large-scale NLP systems[1].\n3. The ablation on alternative normalizations (RMSNorm, ScaleNorm, GroupNorm) is limited, leaving unclear the efficiency–accuracy trade-offs and generalization of the findings.\n4. The study could benefit from stronger head-to-head comparisons with top-performing Mamba variants such as vim, simba, local mamba, plain mamba, etc.\n5. The computation and practicality of real-time K-FAC monitoring for adaptive normalization placement are not evaluated.\n\n[1] Wang et al., *Understanding the Failure of Batch Normalization for Transformers in NLP*, NeurIPS 2022."}, "questions": {"value": "1. How does the BN→SSM→LN configuration balance stability, convergence speed, and throughput compared to LN-only or RMSNorm-only baselines across tasks? Is there a task-adaptive rule or heuristic for normalization placement selection?\n2. How are BatchNorm statistics handled during inference to mitigate training–inference inconsistency? Were robust alternatives such as RBN or GhostBN tested under small-batch or domain-shift scenarios?\n3. If only one normalization can be used due to efficiency constraints, is there a general rule linking task type, batch size, or data modality to the optimal choice (LN vs BN)?\n4. How does the proposed strategy theoretically and empirically relate to Transformer stabilization techniques such as Pre-LN, Post-LN, and DeepNorm [1][2]? Could a unified interpretation framework be provided?\n\n[1] Xiong et al.,*On Layer Normalization in the Transformer Architecture*, 2020.\n\n[2] Shleifer et al.,*Normformer: Improved Transformer Pretraining with Extra Normalization*, ICLR 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xMOsQnkirJ", "forum": "NX6oMuoLxu", "replyto": "NX6oMuoLxu", "signatures": ["ICLR.cc/2026/Conference/Submission609/Reviewer_ti93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission609/Reviewer_ti93"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987321296, "cdate": 1761987321296, "tmdate": 1762915564650, "mdate": 1762915564650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}