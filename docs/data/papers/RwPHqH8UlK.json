{"id": "RwPHqH8UlK", "number": 14982, "cdate": 1758246453609, "mdate": 1759897337701, "content": {"title": "Dynamic Preference Calibration: Meta-Learning Soft Labels for Robust Alignment", "abstract": "Noise in preference data significantly impedes the robust alignment of large language models (LLMs) with human values. Existing methods that rely on global noise assumptions or static pre-processing heuristics are often insufficient, as they fail to address the instance-specific and dynamic nature of preference noise. To overcome these limitations, we introduce Dynamic Preference Calibration, a novel framework that meta-learns to generate adaptive soft labels directly from noisy data. Our approach employs a lightweight meta-learner that maps a perplexity difference (PPLDiff) signal to a calibrated soft label. Crucially, the power of our dynamic approach stems from calculating this PPLDiff signal online, using the main, evolving LLM itself. This creates a symbiotic loop where the main model's improving understanding continuously informs and refines the calibration strategy, allowing it to co-evolve. Guided by a small, clean meta-dataset, the meta-learner is optimized to produce labels that maximize alignment performance. Extensive experiments on benchmark datasets demonstrate that our method establishes a new state-of-the-art for noisy preference alignment, significantly outperforming strong baselines. It maintains high performance and stability even under extreme noise levels up to 40\\% label flips, highlighting the promise of meta-learning for building fundamentally more robust and reliable alignment techniques.", "tldr": "", "keywords": ["Large Language Models", "Direct Preference Optimization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1239c29509809b390ed14944dc898e51fd520d2e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce Meta Soft Preference Optimization (MSPO), a novel framework that utilizes a lightweight meta-learner in a bilevel optimization setup. This network learns to generate adaptive soft preference labels by interpreting the dynamic Perplexity Difference (PPLDiff) signal from the evolving LLM. This dynamic calibration results in a stable learning algorithm that demonstrates robustness against high levels of label noise, successfully aligning models even with 40% flipped data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method shows strong improvement across all settings compared to the baseline.\n\nExperiments with up to 40\\% flipped labels and in difficulty-dependent noise scenarios clearly demonstrate the approach's robustness against noise."}, "weaknesses": {"value": "I would like to see a better explanation of the Meta-Learner network, only after looking at the Appendix C I realized that this network has one scalar as input and output and not the whole vocabulary.\n\nIt would be valuable to see at least one training run with a larger model (>20B), though I understand this is fairly costly.\n\nIt would be good to have comparisons on how much compute each method uses. These additional steps will add some overhead, but how much is it?"}, "questions": {"value": "At which noise level does this method stop working? You report a fairly small drop between the clean data and 40% flipped labels. What happens at over 50%? Is the feedback from the small clean meta-dataset enough to keep the learning process stable even when the noisy training data is worse than random chance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XEj8Rf8tK9", "forum": "RwPHqH8UlK", "replyto": "RwPHqH8UlK", "signatures": ["ICLR.cc/2026/Conference/Submission14982/Reviewer_hRhE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14982/Reviewer_hRhE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658672455, "cdate": 1761658672455, "tmdate": 1762925314057, "mdate": 1762925314057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key weakness of current preference-based alignment methods: they treat noise as either globally uniform or correct it via static heuristics, ignoring the instance-specific and evolving nature of label errors. The authors propose Dynamic Preference Calibration, instantiated as Meta Soft Preference Optimization (MSPO). A lightweight meta-network receives the current policy’s perplexity difference between two responses and outputs a soft preference strength; a bilevel loop then trains the meta-network on a small clean set while updating the main LLM on the (noisy) training set. Experiments on Llama-2-7B and Phi-2 with up to 40 % random or difficulty-dependent label flips show consistent gains over DPO, GDPO, cDPO, rDPO and the strongest baseline PerpCorrect-DPO. Ablations and visualizations confirm that the meta-learner quickly learns a sigmoid mapping that separates clean from flipped pairs, producing a bimodal soft-label distribution that stabilizes training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper cast robust preference alignment as an online meta-learning problem, using the policy’s own perplexity signal rather than a frozen surrogate.\n2. The ablation study is thorough: it removes the meta-learner, freezes the PPL-diff signal, and still reports large drops, proving every component matters.\n3. Relative to the static PerpCorrect, the newly introduced Step 1 and Step 2 incur roughly 25–30 % additional computational overhead."}, "weaknesses": {"value": "1. The paper has not been verified on more recent and larger-parameter models; it is recommended to add corresponding experiments to further illustrate its generalizability."}, "questions": {"value": "1. If the clean meta-data are drawn from a dataset different from the one that provides the noisy training data, will MSPO still remain effective?\n2. What would happen if we simply employed PPLDiff as an online criterion to dynamically select or discard training instances, discarding all other components of the proposed design?\n\nOthers questions see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8jbfvQt8LL", "forum": "RwPHqH8UlK", "replyto": "RwPHqH8UlK", "signatures": ["ICLR.cc/2026/Conference/Submission14982/Reviewer_1fa7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14982/Reviewer_1fa7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963575890, "cdate": 1761963575890, "tmdate": 1762925313634, "mdate": 1762925313634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dynamic Preference Calibration, a paradigm for improving the robustness of LLM alignment under noisy preference data. The key contribution is Meta Soft Preference Optimization (MSPO), a meta-learning framework that dynamically generates adaptive soft labels based on online Perplexity Difference (PPLDiff) signals from the evolving main model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is very simple and intuitive.\n\n2. It provides a comprehensive analysis both theoretically and empirically."}, "weaknesses": {"value": "1. Despite the clear formulation, the core innovation of MSPO may be viewed as incremental. The method essentially extends GDPO by integrating a meta-learned mapping from Perplexity Difference to soft labels. \n\n2. The experimental results show that existing methods such as GDPO, rDPO, and PerpCorrect-DPO already recover performance from severe noise degradation (50% -> 90% win rate). MSPO’s further improvement is often within single-digit percentages, which might fall within the variance range of GPT-as-Judge evaluations. The claimed state-of-the-art improvement thus appears modest, and more challenging or diverse benchmarks are needed to demonstrate true superiority.\n\n3. The approach relies solely on a single scalar feature, the PPL difference, as input to the meta-learner. This seems overly simplistic and may not capture the complexity of preference noise. For instance, two instances with the same PPL difference but vastly different absolute perplexities or prompt difficulties will receive identical soft labels. This feature choice overlooks known biases in perplexity measures such as response length or prompt ambiguity.\n\n4. MSPO assumes the availability of a small but perfectly clean meta-dataset for meta-optimization. While the paper studies sensitivity to meta-set size, it does not examine robustness to meta-data noise, which is a major limitation for real-world alignment scenarios where even “trusted” labels can be imperfect.\n\n5. The proposed bilevel optimization framework introduces extra computational steps (virtual updates and meta-gradients). The paper provides no quantitative analysis of training cost, convergence speed, or scalability to larger models and datasets, leaving uncertainty about its practical feasibility."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oMrszL2hMt", "forum": "RwPHqH8UlK", "replyto": "RwPHqH8UlK", "signatures": ["ICLR.cc/2026/Conference/Submission14982/Reviewer_wsyG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14982/Reviewer_wsyG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978168745, "cdate": 1761978168745, "tmdate": 1762925312859, "mdate": 1762925312859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Preference Calibration for LLM alignment, addressing the challenge of noisy human preference data. The proposed Meta Soft Preference Optimization (MSPO) is a bilevel meta-learning framework where a lightweight meta-learner utilizes dynamically computed perplexity difference (PPLDiff) signals from the main LLM to generate instance-specific soft preference labels. This meta-learner is trained using a small, clean meta-dataset to calibrate noise in training labels. Experimental results demonstrate improved robustness and alignment quality compared to strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe idea of changing from static, heuristic-based noise correction to a dynamic, learned calibration strategy is highly compelling and addresses a clear limitation in existing methods. \n2.\tThe method leverages the evolving main LLM itself to generate the PPLDiff signal, creating a feedback loop where the meta-learner’s calibration strategy becomes coupled to the model’s understanding as it improves over time, rather than relying on a fixed proxy.\n3.\tThe consistent and significant outperformance of MSPO across two benchmarks against strong baselines shows its effectiveness. The implementation of the ablation study is detailed."}, "weaknesses": {"value": "1.\tThe method's performance is inherently tied to the quality of the PPLDiff signal. The failure case presented in Table 9 of the appendix rightly highlights a key limitation: the main model's perplexity may not always correlate with nuanced aspects of preference like factual accuracy, potentially leading the meta-learner to reinforce the model's existing biases (e.g., towards fluency). The usage of PPLDiff needs to be carefully considered.\n2.\tThe paper acknowledges a 25-30% computational overhead compared to standard DPO due to the bilevel optimization. While a reasonable cost for the significant performance gains, a more detailed analysis of the overhead would be helpful.\n3.\tThis paper contains some typos, and here are a few examples: in Figure 2, \"vitural\" should be \"virtual\"; on page 3, line 157, \"The choice of input signal for the meta-learner is critical\" is repeated twice."}, "questions": {"value": "1.\tThe dynamic PPLDiff is the sole input to the meta-learner. Did the authors experiment with incorporating additional features to make the input signal more robust?\n2.\tSince the meta-learner is a simple MLP, could a more complex model capture more complex calibration functions, or is there any risk of overfitting to the small meta-dataset?\n3.\tWhether the performance gain of MSPO is primarily due to a more efficient or powerful use of the clean data compared to the simpler baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PZJMW21eaz", "forum": "RwPHqH8UlK", "replyto": "RwPHqH8UlK", "signatures": ["ICLR.cc/2026/Conference/Submission14982/Reviewer_c49x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14982/Reviewer_c49x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988389987, "cdate": 1761988389987, "tmdate": 1762925312179, "mdate": 1762925312179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}