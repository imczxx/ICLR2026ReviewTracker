{"id": "EEs6I4cO7S", "number": 20518, "cdate": 1758306995066, "mdate": 1759896973713, "content": {"title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features", "abstract": "Sparse autoencoders (SAEs) are a cornerstone of interpretability for large language models (LLMs), aiming to decompose hidden states into meaningful semantic features. While several SAE variants have been proposed, there remains no principled framework to derive SAEs from the original dictionary learning formulation. In this work, we introduce such a framework by unrolling the proximal gradient method for sparse coding. We show that a single-step update naturally recovers common SAE variants, including ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation of existing SAEs: their sparsity-inducing regularizers enforce non-negativity, preventing a single feature from representing bidirectional concepts (e.g., male vs. female). This structural constraint fragments semantic axes into separate, redundant features, limiting representational completeness. To address this issue, we propose AbsTopK SAE, a new variant derived from the $\\ell_0$ sparsity constraint that applies hard thresholding over the largest-magnitude activations. By preserving both positive and negative activations, AbsTopK uncovers richer, bidirectional conceptual representations. Comprehensive experiments across four LLMs and seven probing and steering tasks show that AbsTopK improves reconstruction fidelity, enhances interpretability, and enables single features to encode contrasting concepts. Remarkably, AbsTopK matches or even surpasses the Difference-in-Mean method—a supervised approach that requires labeled data for each concept and has been shown in prior work to outperform SAEs.", "tldr": "", "keywords": ["Sparse Autoencoder", "Mechanistic Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7900a29dc921cf485c1660ae49bb266e91e9c59d.pdf", "supplementary_material": "/attachment/7a373b6cebee870c1504d32cbb67c4432bd6d3c1.zip"}, "replies": [{"content": {"summary": {"value": "The authors mathematically prove an equivalence between commonly used SAE architectures and sparse regularizers. They then use the insights gained from this connection to propose a new type of SAE which is able to encode bidirectional features as a single dictionary element. They empirically demonstrate that this new SAE architecture outperforms pre-existing SAEs on many commonly used evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors make a nontrivial connection between SAEs and sparse regularizers using proximal operators, which gives a new perspective on the differences between SAE architectures. This seems to be well-executed (though I didn’t go through the math and proof in too much detail).  \n2. The evaluations are quite comprehensive in terms of covering all the relevant axes of SAE quality. They demonstrate that AbsTopK are a clear improvement over JumpReLU and TopK SAEs (even though the size of the improvement is relatively small on most evaluations).  \n3. The SAE variant they propose is clean and elegant in how it addresses the problem of bidirectional features."}, "weaknesses": {"value": "1. The evaluations should include more baselines. They currently include only 2: TopK and JumpReLU. They should include a handful of others (even if they don’t neatly fit into the framework introduced in section 2). ReLU, Matryoshka, and gated SAEs would all be welcome additions.  \n2. I want to see more empirical results showing how bidirectional features are encoded in AbsTopK SAEs. The single example in Figure 1 is nice, but it’s just a single example which isn’t sufficient to convince me that this is representative.\n3. I’d like to see results from more layers. Currently as far as I can tell they only test on 2 layers in each model, and they are always somewhere roughly in the middle of the model. Include early layers and late layers would give me more confidence that the observed improvements generalize.  \n4. For the experiments Table 1, it would be good to see 4ish different models rather than just 2\\. I agree with the reasoning for excluding the very small models here, but it would be nice to add e.g. a Llama model. I’d also weakly recommend turning table 1 into a series of plots (e.g. with MMLU on the x-axis and HarmBench on the y-axis); it would make it more easily digestible.  \n5. More generally in all experiments, it would be nice to include one or two larger models (e.g. 10B+ parameters).  \n6. Typo: the legend inside of each subfigure in Figures 2, 3, and 4 refers to “AbsoluteK”. I’m assuming this refers to AbsTopK, perhaps the authors switched from one name to the other at some point during the writing process but forgot to update the figures."}, "questions": {"value": "See the Weaknesses section. I’d particularly recommend focusing on points 1 and 2, I see those as the main weaknesses of the paper, if you address them then I’m likely to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "88ZFlo9tty", "forum": "EEs6I4cO7S", "replyto": "EEs6I4cO7S", "signatures": ["ICLR.cc/2026/Conference/Submission20518/Reviewer_1nSy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20518/Reviewer_1nSy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760892752260, "cdate": 1760892752260, "tmdate": 1762933941973, "mdate": 1762933941973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a principled framework that derives sparse autoencoders (SAEs) from the proximal gradient method for sparse coding, showing that common variants like ReLU, JumpReLU, and TopK naturally emerge as single-step updates. The analysis reveals a key limitation of existing SAEs—their non-negativity constraints prevent features from capturing bidirectional semantics (e.g., male vs. female), leading to fragmented representations. To address this, the authors propose AbsTopK SAE, which applies magnitude-based hard thresholding to preserve both positive and negative activations. Experiments across multiple LLMs and interpretability tasks show that AbsTopK improves reconstruction, enhances interpretability, and enables single features to represent contrasting concepts, matching or surpassing supervised baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tAlthough sparse autoencoders have emerged as a promising approach to improving the interpretability of large language models, their theoretical understanding remains limited. This paper proposes a novel framework based on a proximal perspective to analyze SAEs. The theoretical analysis is natural, rigorous, and insightful, providing meaningful guidance for future research in this area.\n2.\tThe paper convincingly demonstrates that ignoring negative components in the latent space degrades the performance of sparse autoencoders. Replacing traditional activation functions with AbsTopK is a reasonable and well-motivated solution, supported by both solid theoretical reasoning and comprehensive empirical validation.\n3.\tThe writing and presentation are clear, coherent, and well-structured. The theoretical and empirical sections complement each other effectively, and the overall logic flow is easy to follow. The theoretical part offers sufficient intuition without being overly mathematical. I appreciate the clarity and accessibility of the presentation.\n4.\tThe authors provide thorough experimental verification showing that AbsTopK SAEs outperform existing variants in both reconstruction and interpretability-related tasks."}, "weaknesses": {"value": "1.\tMy main concern is whether AbsTopK may compromise the monosemanticity property of sparse autoencoders. One of the most desirable characteristics of SAEs is that each dimension is activated primarily by a single concept. However, since AbsTopK activates both positive and negative top-K components, this property might be weakened. For instance, if a single feature dimension responds to both male and female concepts, it becomes difficult to isolate them semantically. In addition, the paper does not provide an evaluation of monosemanticity metrics, such as auto-interpretability scores. I would be happy to raise my evaluation if the authors can clarify or empirically address this concern.\n2.\tAs the effectiveness of AbsTopK SAEs remains somewhat uncertain, I consider the main contribution of this paper to be its theoretical framework for understanding SAEs. Therefore, in the related work section, it would strengthen the paper to include a more explicit comparison with existing theoretical analyses of SAEs, such as [1], [2], and related studies.\n\n\n[1] Chen, Siyu, et al. \"Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders.\" arXiv preprint arXiv:2506.14002 (2025).\n[2] Cui, Jingyi, et al. \"On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond.\" arXiv preprint arXiv:2506.15963 (2025)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6pECgW4gv5", "forum": "EEs6I4cO7S", "replyto": "EEs6I4cO7S", "signatures": ["ICLR.cc/2026/Conference/Submission20518/Reviewer_mJ7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20518/Reviewer_mJ7S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707858305, "cdate": 1761707858305, "tmdate": 1762933941522, "mdate": 1762933941522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AbsTopK SAE, a new variant of sparse autoencoder derived from the proximal gradient perspective on dictionary learning. The authors revisit the mathematical foundation of SAEs, showing that existing variants (ReLU, JumpReLU, TopK) can be unified under the proximal operator framework. From this analysis, they identify a structural limitation: current SAEs enforce non-negativity in activations, preventing single features from representing bidirectional concepts (e.g., male–female, positive–negative sentiment).\nTo overcome this, they introduce the AbsTopK operator, which selects the top-|k| activations by absolute magnitude, thus allowing both positive and negative activations. Experiments across several LLMs and benchmarks (probing, steering, MMLU, HarmBench) show that AbsTopK achieves better reconstruction fidelity, preserves model utility, and can encode contrasting semantics within a single feature dimension."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Principled theoretical framing – The proximal-operator derivation provides a unifying mathematical lens for understanding existing SAE variants and justifies design differences between ReLU, JumpReLU, and TopK in a coherent way.\n- Simple and reproducible modification – The AbsTopK operator is straightforward to implement and can be directly integrated into existing SAE pipelines."}, "weaknesses": {"value": "- Lack of feature interpretability evaluation – The paper’s central claim concerns semantic bidirectionality, yet no rigorous analysis or quantitative metric is provided to assess whether positive and negative activations of a single feature correspond to semantically opposite concepts.\n- Many linguistic or conceptual axes are not naturally symmetric or have no meaningful “opposite” (e.g., tree, city).\n- Without systematic qualitative or quantitative validation (e.g., feature visualization, activation clustering, or concept alignment), the interpretability claim remains speculative.\n- Interpretability vs. utility conflation – Improved reconstruction or downstream task performance does not necessarily imply better interpretability; this distinction should be explicitly discussed."}, "questions": {"value": "Could you quantitatively evaluate whether positive/negative activations of a single AbsTopK feature correspond to semantically opposite text examples?\nIt would be useful to add qualitative visualizations (e.g., top positive vs. negative activating examples) for multiple features, not just one or two, to support the bidirectionality claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "voD7Q9Iz0D", "forum": "EEs6I4cO7S", "replyto": "EEs6I4cO7S", "signatures": ["ICLR.cc/2026/Conference/Submission20518/Reviewer_zJyM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20518/Reviewer_zJyM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814323877, "cdate": 1761814323877, "tmdate": 1762933940850, "mdate": 1762933940850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AbsTopK SAE, a novel sparse autoencoder variant that removes the non-negativity constraint in conventional SAEs.\nWhile existing SAEs such as ReLU, JumpReLU, and TopK enforce non-negative activations, they cannot represent bidirectional semantic axes (e.g., male–female, positive–negative sentiment), leading to feature fragmentation.\n\nThe authors rederive SAEs from a proximal gradient framework of sparse coding, revealing that the non-negativity arises from implicit regularizers.\nThey introduce AbsTopK, which performs hard thresholding over the largest-magnitude activations (i.e., L0 sparsity without sign restriction), enabling both positive and negative activations to coexist in a single feature.\nExperiments across four LLMs (GPT2-Small, Pythia-70M, Gemma-2B, Qwen-4B) and seven probing/steering benchmarks show that AbsTopK improves reconstruction fidelity, interpretability, and bidirectional concept encoding, matching or even surpassing the supervised Difference-in-Mean baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- AbsTopK consistently outperforms TopK and JumpReLU across reconstruction, probing, and steering tasks.\n\n- Derivation from the proximal gradient method grounds the design of SAEs in dictionary-learning theory, showing why prior variants enforce non-negativity.\n\n- Both theoretical and empirical validations are presented coherently."}, "weaknesses": {"value": "- The paper assumes that features should be bidirectional (e.g., male-female, positive-negative). However, not all concepts have clearly defined opposites. For such unipolar or abstract concepts (e.g., syntax awareness, topic consistency), the interpretation of negative activations remains ambiguous. It is unclear whether these activations correspond to an absence of a feature, an opposing property, or noise.\n\n- While the experiments use standardized benchmarks such as SAEBench, the paper would benefit from direct analyses showing how AbsTopK changes the feature space compared to the standard SAEs.\n\n- The evaluation focuses mainly on a single activation type (residual stream) and a limited set of layers. Since SAE behavior often depends strongly on layer position and activation type (Attention, MLP, Residual Stream) [1], extending experiments across multiple layers and activation sources would make the evidence more comprehensive and convincing.\n\n- In addition, the results are promising, but the paper does not report variability across random seeds or runs. Providing standard errors or confidence intervals would strengthen the reliability of empirical conclusions.\n\n[1] Rethinking evaluation of sparse autoencoders through the representation of polysemous words, ICLR2025"}, "questions": {"value": "- Several recent studies challenge the Linear Representation Hypothesis [1,2]. How does the proposed framework relate to or address these counterarguments, or do you have any discussion?\n\n- Could the AbsTopK approach be extended or adapted to Transcoder[3] or CrossCoder[4] architectures?\n\n[1] Interpreting Neural Networks through the Polytope Lens.  \n[2] Not All Language Model Features Are One-Dimensionally Linear, ICLR2025.    \n[3] Transcoders Find Interpretable LLM Feature Circuits, Neurips2024.    \n[4] Sparse Crosscoders for Cross-Layer Features and Model Diffing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7JOdBiOkuz", "forum": "EEs6I4cO7S", "replyto": "EEs6I4cO7S", "signatures": ["ICLR.cc/2026/Conference/Submission20518/Reviewer_EYut"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20518/Reviewer_EYut"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818321501, "cdate": 1761818321501, "tmdate": 1762933940486, "mdate": 1762933940486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}