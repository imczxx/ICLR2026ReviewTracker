{"id": "bFYfV6c9zu", "number": 10027, "cdate": 1758156811007, "mdate": 1759897680037, "content": {"title": "Differentiable Model Predictive Control on the GPU", "abstract": "Differentiable model predictive control (MPC) offers a powerful framework for combining learning and control. However, its adoption has been limited by the inherently sequential nature of traditional optimization algorithms, which are challenging to parallelize on modern computing hardware like GPUs. In this work, we tackle this bottleneck by introducing a GPU-accelerated differentiable optimization tool for MPC. This solver leverages sequential quadratic programming and a custom preconditioned conjugate gradient (PCG) routine with tridiagonal preconditioning to exploit the problem's structure and enable efficient parallelization. We demonstrate substantial speedups over CPU- and GPU-based baselines, significantly improving upon state-of-the-art training times on benchmark reinforcement learning and imitation learning tasks. Finally, we showcase the method on the challenging task of reinforcement learning for driving at the limits of handling, where it enables robust drifting of a Toyota Supra through water puddles.", "tldr": "", "keywords": ["differentiable optimization", "model predictive control", "optimal control", "gpu-accelerated optimization", "reinforcement learning", "imitation learning", "robotics"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74d52ec7e0ce5adc14a0e525f045e677fb47f21b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a solver for Model Predictive Control (MPC) which is optimized for running on a GPU. The optimization tool called DiffMPC is differentiable and can be integrated in RL or imitation learning tasks. The method uses line search, warm start of both forward and backward pass, and parallelization over time steps to improve efficiency on the GPU. Experiments show that it runs faster on the GPU than other MPC solvers, and can be used effectively for RL and imitation learning tasks, including a simulation of driving in new domains."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is tested in randomized environments as well as real applications, and the results in Figure 3 and Table 3 in the appendix show that it consistently outperforms the other methods.\n2. There are some ablations of individual aspects of the method, such as the warm start, which is convincing.\n3. The paper motivates the approach well and puts it into context with prior work, pointing out the shortcomings of GPU acceleration. It provides a good background section on differentiable control. It is convincing that this method can improve the integration of learning and optimization.\n4. The limitations are described transparently."}, "weaknesses": {"value": "1. One limitation seems to be that only equality constraints are supported. This seems to impact comparability to the baselines (e.g. the appendix states that mpc.pytorch performance suffers when inequality constraints are disabled, which was tackled with some modifications to the code.)\n2. Only the synthetic randomized experiments compare the performance between all optimizers. In the imitation learning experiment, only trajax is compared, and in the driving-application, no other optimizer is tested."}, "questions": {"value": "1. How much do the solver outputs (the optimized action u etc) in the RL experiment differ? I.e., does the DiffMPC solver achieve the same performance in one iteration as other solvers, or can the modifications lead to suboptimal results?\n2. In Figure 4, it is not visible that there is a 2x speedup; the loss of DiffMPC over time looks the same as trajax. Where is the 2x speedup shown?\n3. Why is line search disabled in the RL experiment? Is it not needed when the problem is convex? It seems to be a core part of the method. Is it used for imitation learning or the other experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BIqJYgHv13", "forum": "bFYfV6c9zu", "replyto": "bFYfV6c9zu", "signatures": ["ICLR.cc/2026/Conference/Submission10027/Reviewer_4NTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10027/Reviewer_4NTm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920315058, "cdate": 1761920315058, "tmdate": 1762921436435, "mdate": 1762921436435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiffMPC, a GPU-accelerated differentiable optimization solver for model predictive control (MPC) written in Jax. DiffMPC uses sequential quadratic programming with a line search and preconditioned conjugate gradient (PCG) with tridiagonal preconditioning to exploit the structure of their optimal control problem (OCP) for efficient GPU parallelization. Based on their formulation, DiffMPC benefits from warm-starting for both the forward and backward passes. Experimental results against existing solutions for GPU-accelerated differentiable MPC demonstrate that DiffMPC substantially benefits from GPU acceleration compared to baselines. DiffMPC is also demonstrated for real-world vehicle control."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is extremely well written and easy to follow. The authors provide comprehensive discussion on prior work in differentiable optimization and clearly explain their approach for differentiable MPC. \n\n- DiffMPC is 2-7x faster than Trajax, a comparable Jax-based differentiable MPC library, and two PyTorch-based differentiable MPC libraries, Mpc.pytorch and and Theseus. Surprisingly, the authors also show in Figure 3 that Mpc.pytorch and Theseus obtain similar runtimes when running on both CPU and GPU, so their implementations do not benefit from GPU-acceleration, while DiffMPC does.\n\n- The authors thoroughly explain the limitations of the proposed DiffMPC approach in Section 6. It is great to see this upfront, and the wider community may benefit when trying to build on the provided open source implementation. Based on a brief look at the Github repository, the code also appears to be well organized and easy to understand. \n\n- DiffMPC is deployed for real-world control of a vehicle drifting through water puddles, to showcase practical use of the approach."}, "weaknesses": {"value": "- DiffMPC does not directly handle boundary conditions.\n\n- DiffMPC is slower on CPU than GPU compared to existing solutions, since DiffMPC avoids sequential Riccati recursions to better leverage GPU parallelism. \n\n- There is no evaluation on the quality of the gradients computed by DiffMPC. Some analysis comparing between DiffMPC and finite difference gradients would be helpful."}, "questions": {"value": "L404: Writing that DiffMPC is used within reinforcement learning is a little confusing. This makes it seem like this experiment is doing something similar to DiffTORI (Wan et al., 2024), which uses Theseus for differentiable trajectory optimization within a model-based RL approach with a learned world model. Based on my understanding, Section 5 is not doing this, but just directly using DiffMPC with a given differentiable model? \n\nL484: How sensitive is DiffMPC to the initial guesses to PCG? How much tuning was done?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/a"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UyBxcGKIPP", "forum": "bFYfV6c9zu", "replyto": "bFYfV6c9zu", "signatures": ["ICLR.cc/2026/Conference/Submission10027/Reviewer_cUhS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10027/Reviewer_cUhS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989948652, "cdate": 1761989948652, "tmdate": 1762921435989, "mdate": 1762921435989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DiffMPC, a GPU-accelerated, differentiable model predictive control (MPC) solver built in JAX. The core idea is to solve and differentiate through optimal control problems (OCPs) using an SQP loop whose linear KKT systems are handled by a preconditioned conjugate gradient (PCG) routine with a symmetric stair (block-tridiagonal) preconditioner, exposing parallelism over time for both forward and backward passes. Warm-starting and reusing forward KKT factorizations in the backward VJP further reduce cost. Empirically, DiffMPC achieves 4–7× speedups over Trajax/mpc.pytorch/Theseus on GPU for RL/IL benchmarks, trains ~2× faster than Trajax on a nonlinear imitation-learning cart-pole, and supports an RL domain-randomization pipeline that improves robustness, with successful real-vehicle transfers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The PCG with symmetric stair preconditioning cleanly leverages the OCP’s block-tridiagonal structure; both forward (solve) and backward (VJP) reuse the same KKT structure and benefit from warm-starts. The data-flow diagram (Fig. 2) and derivations (Eqs. 5–11) are clear. \n\n2. Across RL timing benchmarks, DiffMPC on GPU is ~4× faster than the best baseline for the shown problem and 4–7× across others. For IL (cart-pole), training progress per wall-clock roughly doubles vs Trajax. \n\n3. The RL+domain-randomization pipeline learns cost and tire parameters that eliminate spinouts in simulation (70% to 100% success) and transfer to a real Toyota Supra drifting donuts through puddles, illustrating robustness and policy generalization from differentiable MPC.\n\n4. The paper documents solver choices including line search schedule, tolerances, and reports warm-start gains, explains baseline configurations."}, "weaknesses": {"value": "1. RL timings disable line search and cap all solvers to a single iteration on convex QPs. mpc.pytorch is also modified to remove a serial list comp. While justified for timing, this setting may favor DiffMPC’s parallel linear algebra over sequential Riccati recursions. A complementary benchmark with full nonconvex solves would strengthen claims.\n\n2. Warm-start gains are modest at strict tolerances (depending on $\\epsilon$), and CPU performance is acknowledged to be worse. A more systematic analysis of the relation between batch size/horizon/tolerance and speed/accuracy/memory would clarify operational regimes.\n\n3. Control bounds and other inequalities are handled outside OCP (e.g., in simulator) or via penalties, and the authors note future work for AL/IPM approaches. The absence of constrained OCP experiments limits conclusions for tasks where active sets matter."}, "questions": {"value": "1. Can you report gradient-accuracy diagnostics, e.g., cosine similarity vs finite-difference or “exact” KKT-curvature gradients, on small OCPs, and show their effect on IL/RL convergence? This would directly test the stated trade-off in Sec. 3.4.\n\n2. Could you include an experiment with explicit inequality constraints (e.g., torque/steering bounds) solved inside DiffMPC via an augmented-Lagrangian or interior-point variant, comparing robustness and speed to CPU solvers? Even a limited prototype would evidence extensibility.\n\n3. For the drifting study, can you provide wall-clock training curves, GPU memory use, and ablations over batch size, horizon, and PCG tolerance, plus a sensitivity to differentiable vs non-differentiable simulator components? This would help others scope hardware needs and replicate behavior."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lza38FQrrx", "forum": "bFYfV6c9zu", "replyto": "bFYfV6c9zu", "signatures": ["ICLR.cc/2026/Conference/Submission10027/Reviewer_weUS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10027/Reviewer_weUS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049032875, "cdate": 1762049032875, "tmdate": 1762921435564, "mdate": 1762921435564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}