{"id": "D8DWjNjyXG", "number": 13477, "cdate": 1758218368904, "mdate": 1759897434574, "content": {"title": "What Makes a Representation Relightable? Probing Visual Priors via Augmented Latent Intrinsics", "abstract": "Image-to-image relighting requires a representation that disentangles scene properties from illumination. Recent methods use latent intrinsic representations but remain under-constrained and often fail on challenging materials like metal and glass. A natural hypothesis is that injecting powerful, pretrained visual priors should resolve these failures. We find the opposite is true: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. This paper investigates what makes a representation ``relightable.\" We introduce Augmented Latent Intrinsics (ALI), a method that resolves this trade-off by strategically fusing features from a dense, pixel-aligned visual encoder into a latent-intrinsic framework while leveraging self-supervision refinement to overcome the scarcity of paired real-world training data. Trained only on unlabeled, real-world image pairs, ALI achieves strong relighting improvements with the largest gains on complex materials.", "tldr": "", "keywords": ["Relighting", "Probing", "Latent Intrinsics", "Visual Priors", "Representations"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bd7376e4f9394f7340564aa59500056e15d9f74.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper builds upon LumiNet (Xing et al., 2025\\) and Latent Intrinsics (Zhang et al., 2024\\) for image-to-image relighting. The proposed method incorporates an additional semantic encoder (a vision foundation model) to augment the extracted latent intrinsics. The paper is framed within the context of how visual priors from foundation models help the relighting performance and there is some discussion on which vision foundation models are good for the relighting task. The method is trained solely on image pairs without the need for privileged information such as depth or GT lighting and the training protocol includes an additional self refinement step. The method is evaluated on the MIIW dataset and ablations show the performance on different material sub-groups and the interpretability of the latent extrinsic."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces semantic context from a vision foundation model (semantic encoder) for the task of image-to-image relighting. The proposed method improves upon the performance of prior works trained on diverse datasets, such as LumiNet, over the MIIW dataset.  \n* The authors show that the latent intrinsics and extrinsic are well trained and in particular show that interpolating between two extrinsics still produces plausible lighting.  \n* The authors highlight specific limitations of the metrics (PSNR, SSIM, RMSE) and made clear that this is still an open research problem to properly evaluate the improvements for relighting specific challenging material classes."}, "weaknesses": {"value": "* The central claim of the paper on “A controlled protocol and operational criteria for probing relightability, explaining why popular semantic encoders fail despite strong recognition performance.” could be better explained. Figure 1 (right) shows a negative correlation between ImageNet 1K linear probing accuracy and relighting performance for pretrained vision foundation models. The authors also touch on the choice of vision encoder in Section 5 and Table 4\\. Perhaps, the authors could share more details on the experimental settings for Figure 1 (right) and include discussions on why the proposed method, based on RADIOv2.5, overcomes the limitations. Another suggestion would be to compare performance with the different model sizes for the different semantic encoders (e.g. RADIOv2.5B, RADIOv2.5-L, RADIOv2.5-H).  \n* The paper relies primarily on qualitative examples to demonstrate the real-world generalizability of the proposed approach, and also the need for Stage III in the training pipeline. LumiNet, following the practice of LightIt (Kocsis,et al., 2024), conducted a user study to assess the performance across three metrics: image quality (I-PQ), lighting quality (L-PQ) and prompt alignment (P-PQ). A similar study could strengthen the claims of the paper.  \n* It is not clear to this reviewer how the proposed approach (ALI), where features from a dense, pixel-aligned encoder are “strategically” fused (line 464). The fusing occurs in Stage 1 of the training, where N (what is the number of N) feature maps are selected from $E_{sem}$, upsampled and concatenated to form H, followed by a learnable P\\_theta layer. Details of N, dimensions of $P_\\\\theta$ and other fusion details would make this paper more convincing in terms of the insights it yields. This will help the reader appreciate the main contribution of this so-called light weight fusion adapter (line 97\\)\n\n**Comments**:\n\n* MAE paper repeated twice in the references  \n* Define MIIW early (line 91\\)  \n* Please put in the definitions of E\\_theta, E\\_sem etc. into Fig. 2, else it is really hard to follow.  \n* Fig. 4 – comparisons with IC-Light, this work was not mentioned in the literature survey, do add it in so readers can appreciate the comparison better  \n* Table 1: which is the raw and which is the color-corrected?   \n\n**References**:  \nKocsis, P., Philip, J., Sunkavalli, K., Nießner, M., & Hold-Geoffroy, Y. (2024). Lightit: Illumination modeling and control for diffusion models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 9359-9369).\n\nXing, X., Groh, K., Karaoglu, S., Gevers, T., & Bhattad, A. (2025). Luminet: Latent intrinsics meets diffusion models for indoor scene relighting. In *Proceedings of the Computer Vision and Pattern Recognition Conference* (pp. 442-452).\n\nZhang, X., Gao, W., Jain, S., Maire, M., Forsyth, D., & Bhattad, A. (2024). Latent intrinsics emerge from training to relight. *Advances in Neural Information Processing Systems*, *37*, 96775-96796."}, "questions": {"value": "* How does the computational complexity/runtime of the proposed method compare to existing methods such as LumiNet and Latent Intrinsics  \n* Do any specific aspects of the unique training strategies for RADIOv2.5 contribute to the improved performance?  \n* Is the relighting encoder, E\\_theta consists of the “Intrinsic encoder” and “Extrinsic encoder”? (fig. 2). If so, where is the projection layer $P_\\\\theta$ in fig 2.? From the VRM (“$E_{sem}$”?).   \n* How is the training of the “Extrinsic encoder” done as shown in Fig. 2? It is not clear from the description of Stage 1 in sec. 4 how it was involved.  \n* In fig 2, how are the “latent-extrinsics” trained/frozen? Can elaborate better with the description in stage 1 or stage 2 (or both?)  \n* Table 1: is it possible to train ALI with just MIIW train-set only so it is comparable with the upper portion, also the bold results are confusing, or is this an error?  \n* (Suggestion for improvement). Table 3 can include  latent-intrinsics as well, so that the improvements and comparisons for the difficult materials are even more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OjG1tEksFK", "forum": "D8DWjNjyXG", "replyto": "D8DWjNjyXG", "signatures": ["ICLR.cc/2026/Conference/Submission13477/Reviewer_MbbU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13477/Reviewer_MbbU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900194072, "cdate": 1761900194072, "tmdate": 1762924095331, "mdate": 1762924095331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to answer the question of what makes a representation relightable? They show interesting trade-off between global and local contexts for relighting task and clasification task. They propose Augmented Latent Intrinsics (ALI) - a fusion adapter that injects semantic information from frozen priors into the latent intrinsic relighting pipeline. They propose a three-state training scheme that first computes the ALI featires, then retrains the diffusion decoder and finally refines the predictions through a self-supervised approach. Despite training on unlabelled real image pairs, the model predicts more accurate relighting on specular and glossy surfaces as compared to prior work. They show the benefit of their proposed approach through various ablations studies. However, the metrics are not better than prior works. Qualitative results have some big artifacts (see weaknesses) and multi-stage pipeline does not seem justified. Further, several key details are missing and some need more explanation (see weaknesses and questions)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well motivated. The authors try to answer the question - what makes a feature relightable. They explain the trade-off between semantic abstraction and photometric fidelity for relighting tasks. Fig 1 is very interesting. \n- Leverage powerful visual priors and show the benefit of semantic feature injection towards improved image relighting performance. The features also enable generalization on imagenet classification and relighting. Most prior work generalizes to one of the two tasks, but not both.\n- Nice practical idea to generate pseudo pairwise images in stage 3 of training to overcome lack of pairwise real-image dataset.\n- Despite being trained only on same scene relighting, the model generalizes to relighting across different scenes.\n- The method achieves very good relighting on images with specular and reflective surfaces, improving upon other methods.\n- Ablation studies show the benefit of the multi-stage training pipeline and the proposed ALI features."}, "weaknesses": {"value": "- A key motivation of the paper is Fig 1. But there is no explanation on how the models were trained to obtain the metrics in Fig 1. Do you replace the VRM module in your pipeline with CLIP or DINO modules? How was the model trained to obtain metrics in table 4? Similarly, how was the imagenet classification model trained? Were the features passed through a few additional layers or directly to a softmax layer to get class labels?\n- Stage 3 training does not seem to help much. In Fig 5a, there is limited benefit to stage 3 as compared to stage 2. Fig 5b shows several artifacts in the cloud regions in stage 3 as compared to stage 2. From Fig S.1, it is seen that for the same scene, there are several artifacts across the different images in the lighting zoo. This design choice could be reason for artifacts in the predicted image.\n- Improvement over latent intrinsics is unclear. In tables 1 and 2, latent intrinsics has better metrics. (The note about metrics not capturing specular highlights is true and it is acknowledged) Fig 3 shows the improvement of the proposed method over latent intrinsics, especially on specular surfaces. However, in Fig 4, the proposed method introduces significant artifacts (rows 3,4), affects the photorealism of the generated image. I would prefer the trade-off in results from latent intrinsics since maintaining scene consistency is vital in image relighting applications. Further, the authors state that latent intrinsics is trained only on MIIW dataset, which consists of narrow FOV images. Despite this, latent intrinsics has better performance on wider FOV images in Fig 4.  \n- Authors state that the model improves upon the state of the art model performance on the MIIW benchmark. But in tables 1 and 2, the performance is worse than latent intrinsics. And both methods do not use any privileged label information. So this claim is overstated. The paper should clarify the context of the results more carefully. \n- Despite training on both MIIW and big time dataset, the authors evaluate the models only on MIIW dataset. Even if direct comparisons to other methods are not possible, evaluating the generalization of the paper across different test datasets would strengthen the paper.\n- Cross-dataset generalization was not evaluated. For example, the generalization of a model trained on MIIW dataset but evaluated on big time dataset was not conducted. This can further strengthen the claims about the benefits of ALI feature representation.\n- In Fig 5b, stage 2 and 3 images have a dark shadow on the floor area near the lamp. This is not photorealistic. LumiNet result is bright in that area and that is more photorealistic.\n- The lighting zoo is an important aspect of the paper. So it should be included in the main paper rather than the appendix."}, "questions": {"value": "- Does Fig 1 show the performance of different encoders used instead of the VRM encoder? If not, how much of the model's performance can be attributed to the feature contribution vs pipeline design.\n- During stage 3 training, was the model initialized with stage 2 weights? If yes, this could lead to catastrophic forgetting. Given the input image in stage 3 is from the stage 2 predictions, the stage 3 model could be compensating for the mistakes in the predicted image. This might be the cause for artifacts in stage 3 predictions (see Fig 5b). Fig S.1 has several artifacts in across images of the same scene. It is observed in rows 1 (banana and bottles near the window pane), row 4 (jacktes and building outside the window), row 5 (buildings outside and carry bag), etc. Maybe a weighted average of stage 2 and stage 3 weights might help balance the model better. \n- Eq 2 formulation seems a bit too simplistic. Adding is sensitive to scale of the features and does not extract meaningful information between the features. Were alternate methods explored to combine the features? Some of them are - learnable weighted addition, FiLM layer or cross attention layer.\n- During stage 3 training, the paper states that the network is trained to ignore artifacts and focus in essential properties. How is this achieved?\n- Why does stage 3 performance not improve on MIIW dataset? Is it because the dataset is too simplistic?\n- In table 1, SA-AE has best metrics for the first two columns. It is indicated wrongly in the paper.\n- In Fig 5b, is the target light colour yellow or white? It is difficult to determine from the image and hence, it is hard to say if lumiNet prediction is more accurate or your prediction is more accurate.\n- An interesting result: In fig 4, the ceiling light is ON is one case and OFF in the other case depending on ambient light of the target image in that area. How is the modelling learning this? It is quite interesting.\n- Despite using bypass decoder, the text on the bottles in the generated image is not reconstructed properly (Fig S2, S3). Why is that? While the diffusion models struggle on text reconstruction due to VAE compression, one would expect that the bypass decoder would address it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TP1qC9rDZu", "forum": "D8DWjNjyXG", "replyto": "D8DWjNjyXG", "signatures": ["ICLR.cc/2026/Conference/Submission13477/Reviewer_oftv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13477/Reviewer_oftv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935233351, "cdate": 1761935233351, "tmdate": 1762924094317, "mdate": 1762924094317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Augmented Latent Intrinsics (ALI) for image relighting tasks. It is a three-stage training pipeline that is built around the prior relighting method LumiNet by freezing and training different components at different stages. The main idea is to inject semantic features from a frozen vision encoder into the intrinsics processing pathway for LumiNet. In the experiment section, relighting results are reported with comparison to IC-Light, Latent-intrinsics, and LumiNet. Additional experiment is conducted to show the effect of using different vision encoders for the semantic features. \n\nOverall, the paper asks a very interesting research question, but the current analysis does not really provide an answer to it. The main part of the paper is more like a standard relighting paper with a minimally designed component to inject semantic features into the existing relighting framework LumiNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper asks a very interesting question: what makes a representation relightable? This could lead to a deeper understanding of a more effective way of leveraging existing feature encoders for tasks beyond relighting. \n\n- The analysis in Table 3 on relighting performance grouped by different material types is interesting."}, "weaknesses": {"value": "- Unfortunately, after reading the paper, I don't think the authors actually answer the question of what makes a representation relightable. The whole paper is written in a confusing way of mixed messages: the title and introduction focus more on the semantic representation, while the actual content section reads like a regular relighting paper. The only part that echoess with the main question is in Table 4, but only the relighting performance from different encoder backbones are reported without any further insight. This analysis is lacking depth. It is still unclear to the reader what makes a representation relightable. \n\n- Regarding the actual design for the relighting, the only novel or interesting part is in Stage I with the injection of vision encoder features into the LumiNet features. However, fusion of different features is a common practice in multimodal learning. There are not a lot of new insights here. Other designs such as the Lighting Zoo is reasonable but not significantly novel. \n\n- Tables 1 and 2 suggest that the proposed method is slightly better than LumiNet but worse than SOTA. Combining with the above point on novelty, it is unclear if the proposed method is making a significant contribution."}, "questions": {"value": "1. Can the authors discuss more on the answer to 'what makes a representation relightable'? \n\n2. Can the authors discuss more about the performance of the proposed method in Tables 1 and 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8TisEgCp3J", "forum": "D8DWjNjyXG", "replyto": "D8DWjNjyXG", "signatures": ["ICLR.cc/2026/Conference/Submission13477/Reviewer_aB99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13477/Reviewer_aB99"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944701542, "cdate": 1761944701542, "tmdate": 1762924093980, "mdate": 1762924093980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates which representation is most condusive to relighting and also proposes training their own. Using a frozen encoder, the authors learn an adversarial decoder on multi-illumination data to perform relighting. Table 4 summaraizes the results of thier findings which is that dense pixel encoders outperform semantic encoders (eg. DINOv2) for relighting. While this intuition may in of itself not be too suprising, this paper is the first one that tests it as far as I am aware. They perform this test via their own 3 stage training process\n\n1) First, the features from the visual encoder are added onto the predicted latent from the input image. This latent is then used to condition LuminetDiffusion to generate the reconstructed latent. This stage is regularizes via a mean regression loss in the latent space and hyperspherical regularization\n\n2) Next, the LumiNet Diffusion model is finetuned to generate better outputs\n\n3) Finally, supervised data is generated using the model and trained on\n\nThis method has modest improvements over prior work quantitatively and, to my eyes, much better qualitative results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) This paper is well written and easy to follow\n3) Experiments are relatively thorough\n4) Very useful insights are gained vis-a-vis lighting representations."}, "weaknesses": {"value": "Perhaps my only reservation about this model is the use of LumiNet diffusion for generating the image. I fear the prior training of LumiNet would confound the results of the best representation. Ideally a few more decoders should be ablated."}, "questions": {"value": "N/A see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fvOVGkWcL4", "forum": "D8DWjNjyXG", "replyto": "D8DWjNjyXG", "signatures": ["ICLR.cc/2026/Conference/Submission13477/Reviewer_Ru4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13477/Reviewer_Ru4M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762407622135, "cdate": 1762407622135, "tmdate": 1762924093555, "mdate": 1762924093555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}