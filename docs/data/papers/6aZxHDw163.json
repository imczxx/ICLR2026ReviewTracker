{"id": "6aZxHDw163", "number": 4875, "cdate": 1757781544168, "mdate": 1763612182452, "content": {"title": "Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning", "abstract": "Continual reinforcement learning (continual RL) seeks to formalize the notions of lifelong learning and endless adaptation in RL. In particular, the aim of continual RL is to develop RL agents that can maintain a careful balance between retaining useful information and adapting to new situations. To date, continual RL has been explored almost exclusively through the lens of risk-neutral decision-making, in which the agent aims to optimize the expected long-run performance. In this work, we present the first formal theoretical treatment of continual RL through the lens of risk-aware decision-making, in which the behaviour of the agent is directed towards optimizing a measure of long-run performance beyond the mean. In particular, we show that the classical theory of risk measures, widely used as a theoretical foundation in non-continual risk-aware RL, is, in its current form, incompatible with continual learning. Then, building on this insight, we extend risk measure theory into the continual setting by introducing a new class of ergodic risk measures that are compatible with continual learning. Finally, we provide a case study of risk-aware continual learning, along with empirical results, which show the intuitive appeal and theoretical soundness of ergodic risk measures.", "tldr": "", "keywords": ["Continual Reinforcement Learning", "Average-Reward Reinforcement Learning", "Risk-Aware Decision-Making", "CVaR"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d557bd7ad40bd67edf2a6c6112a12830b9e41b95.pdf", "supplementary_material": "/attachment/0a2b2e6ddc5d998c28a59ec5557ca3639e1a6c47.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a study of risk and its role in continual reinforcement learning. The work is motivated by an argument that there is a necessary link between risk and continual learning agents: if an agent needs to survive indefinitely (a proposed prerequisite for continual learning), then the agent must be-risk aware. With this motivation in mind, the paper argues that existing notions of risk are ill-fit to accommodate the continual RL problem setting. Of special interest is the stability-plasticity dilemma, one of the central challenges facing continual learning agents. That is, an agent must be suitably capable of retaining what it has learned from the past (stability), while also be sufficiently adaptive to any new information (plasticity). The core argument of the paper comes in section 4.1 through two definitions of risk measures (Def 4.1 and Def 4.2), and two subsequent lemmas (Lemma 4.1, Lemma 4.2), that together present an argument that standard risk-measures are not well-suited to extend to continual RL. In light of this proposed shortcoming, the paper then proposes \"ergodic risk measures\" (Def 4.4) and argues this is one correct way to conceive of risk in continual RL. Then, in section 5 the work presents a case study of this risk measure using CVaR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Goals, risk, and continual learning are extremely fundamental topics. As such, scrutinising them and their relationship is admirable, given the potential for long-lasting benefit that can come from clarifying the necessary relationship between say risk-awareness and continual learning.\n\n- The approach to invoke axioms and desiderata for thinking carefully about how risk should be conceived of in continual RL is a powerful framing, and I believe has a high potential to contribute something fundamental to the field."}, "weaknesses": {"value": "I believe the paper is asking a deep and important question, but the actual development of the answer to that question leaves a great deal of conceptual confusion, and lacks bedrock justification for the many choices that go into the core arguments. This, to me, the central weakness of the paper.\n\nFor the sake of clarity, I perceive the core reasoning of the paper to be as follows:\n1. Continual learning implies survival.\n2. Survival requires risk-awareness.\n3. Thus, risk-awareness is essential to continual RL.\n4. However, standard notions (from, say, non-continual RL) are not fit for continual RL, because of their incompatibility with the stability-plasticity dilemma.\n5. Therefore, we require a new notion of risk that _is_ fit for continual RL, which motivates the design of Definition 4.4.\n6. Definition 4.4 is fit for continual RL.\n\nIf the authors disagree with my summary, please feel free to correct me, and we can discuss during the rebuttal period.\n\n___[Points 1-3]___ Assuming that I have well-understood the logical flow of the work, let's dive in to the key pieces. While I find points (1) and (2) open for debate, I do not believe they are actually critical to the narrative of the present paper. With that in mind, let us take it as granted that (3.) is a reasonable starting point for motivating the work: risk has a long history throughout decision theory, philosophy, and RL, and as such it is useful to recover a suitable account of risk in continual RL.\n\nUnder this view, the core of the paper is about demonstrating points (4-6), given (3) is taken as a framing assumption.\n\n___[Points 4-6]___ To me, the biggest weakness of the work is in providing a convincing argument in support of each of 4, 5, and 6. I anticipate my reasoning here will be somewhat lengthy, and I am happy to discuss during the rebuttals as well. Let me say more about why:\n- 4. The argument given to show why existing risk measures (such as a \"static\" risk measure) is presented in Section 4.1. The reasoning is effectively that the stability-plasticity dilemma requires a new kind of risk. However, this reasoning is carried out by proposing two new notions of plasticity that are about risk (Defs 4.1 and 4.2); these definitions are not well-motivated, and may not capture the intuitive spirit of what the literature means about plasticity, and do not obviously make contact with the basic intuition of the concept of plasticity. Lemmas 4.1 and 4.2 are then used to show that standard risk measures fail to meet the criteria of these new kinds of plasticity.\n\nMy primary critique of the work is that Definitions 4.1 and 4.2 are not, as of yet, well-motivated. Why should we conceive of plasticity in a way that is _not_ about an organism, agent, or learning system, but is about a risk measure? This seems like a fundamental conceptual confusion. Again I can see an argument relating risk and plasticity, and this argument feels like the essential logical piece that the paper requires to separate standard risk from continual RL. However, this argument is not yet presented. Moreover, the stability-plasticity dilemma emerges in the __context__ of continual learning---the nature of the objective that the learning system pursues gives rise to the dilemma. Here, we see that plasticity is invoked first, then risk is assessed after the fact. Shouldn't the dilemma emerge as a consequence of the objective a learning system is pursuing? \n\n- 5. This is echoing some of my reasoning above: just because one existing kind of risk fails to adhere to the two proposed definitions, it does not mean that an entirely new kind of risk is needed. In general, much more is needed to pull apart, precisely, what desiderata are needed to define a coherent and appropriate kind of risk for continual RL.\n\n- 6. Similar to my reasoning in (5.), I worry that not enough bedrock justification or clarity is offered as to what precisely is sought in terms of an account of risk for continual RL. As such, Definition 4.4 is left as an ad-hoc, so far. Furthermore, given the inherently complex nature of continual RL (the cited work by Ring, Kumar et al., and Abel et al. each invoke an arbitrary, history-based environment, that may not contain any recurrence), I am highly skeptical of invoking Assumptions 4.1 and 4.2 in continual RL to get a notion of risk off the ground.\n\nTo summarise: I applaud the authors for exploring such an important and fundamental question. I believe there are many important ingredients to this work, though the core argument in point (4.) misses the mark for me: why would we reject a type of risk on the basis of this kind of definition of plasticity about risk? What is needed is a deeper conceptual analysis of the stability-plasticity dilemma, how it relates to different kind of objects that learning systems may face, and to make the point clearly as to why continual learning necessitates risk measures of a certain form.\n\nFurthermore, it is unclear what, precisely, we are after for a measure of risk in continual RL to begin with, but any measures that invoke unichain and communicating assumptions should invalidate the proposal, as such assumptions are antithetical to the essence of continual RL.\n\nLastly, I believe the work could better engage with the literature on plasticity: see Section 4 of the cited Kumar et al. paper, or work by Chen et al. (2023), Raghavan and Balaprakash (2021), or recent work by Abel et al. (2025).\n\nReferences:\n- Chen, Q., Shui, C., Han, L., & Marchand, M. (2023). On the stability-plasticity dilemma in continual meta-learning: Theory and algorithm. Advances in Neural Information Processing Systems.\n- Raghavan, K., & Balaprakash, P. (2021). Formalizing the generalization-forgetting trade-off in continual learning. Advances in Neural Information Processing Systems.\n- Abel, D., Bowling, M., Barreto, A., Dabney, W., Dong, S., Hansen, S., ... & Singh, S. (2025). Plasticity as the Mirror of Empowerment. Advances in Neural Information Processing Systems.\n\n\nAs one short aside, I believe the paper could also do a better job of presenting mathematical definitions. Definition blocks are sometimes nearly eight lines of text. The paper would be much more clear if the needed ingredients were introduced one at a time in an intuitive way so that essential definitions are at most a couple of lines."}, "questions": {"value": "My questions are distillations of the weaknesses tab above.\n\nQ1. Why is it sensible to tether plasticity to a measure of risk, rather than as a property of an agent, learning system, or organism? If an agent could, in principle, \"solve\" continual RL while avoiding the stability-plasticity dilemma entirely, might this suffice?\n\nQ2. What are the bedrock assumptions or properties we do want out of a measure of risk in continual RL? (Can you provide this argument in the absence of any references to plasticity or stability?) Why should we grant ourselves Assumptions 4.1 and 4.2, when these could arguably invalidate the notion of continual RL to begin with (effectively, if the environment mixes, then we can analyze learners at convergence, but this is precisely not what we want in continual RL).\n\nQ3. Is risk viewed as important in terms of the space of _solutions_? Or is it truly the case that we want to order agents in terms of their risk tolerance? In other words, in the language of the vNM axioms, one way to think about risk is to get rid of the Independence axiom (at a minimum). Why should we abandon the Independence axiom, specifically in continual RL?\n\nI realize I am asking quite challenging questions. To give a full account of all of these is probably beyond the scope of a conference paper. However, for the results to stand as valuable to the community, I do believe we need at least partial clarity on the above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ig1qE0JYlA", "forum": "6aZxHDw163", "replyto": "6aZxHDw163", "signatures": ["ICLR.cc/2026/Conference/Submission4875/Reviewer_6n6g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4875/Reviewer_6n6g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586208374, "cdate": 1761586208374, "tmdate": 1762917628133, "mdate": 1762917628133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors formulated a mathematical framework for risk-aware continual reinforcement learning (RL). They first presented the existing framework for risk-aware episodic RL, which uses either a static or nested risk measure. They then proved that both measures are incompatible with the stability-plasticity dilemma in continual RL, as the static measure never adapts and the nested measure never forgets. In the second part of the paper, they defined the notion of ergodic risk measure desirable for the continual setting, as it satisfies asymptotic plasticity and local time consistency. They further demonstrated that, under two ergodicity-like assumptions, simply replacing the expectation E[.] in the average reward with any risk measure rho[.] gives a risk-aware objective that is an ergodic risk measure. Finally they tested their newly developed framework by letting rho be a popular risk measure called conditional value-at-risk (CVaR). They conducted experiments with a q-learning agent that optimizes the corresponding risk-aware objective in the continual red-pill blue-pill environment. They demonstrated the agent’s adaptive risk awareness by observing the change in its policy when the risk attitude (controlled by a CVaR param) or the reward distribution is changed."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper formalizes important properties of a risk-aware objective for continual learning, which could be useful for the continual learning community.\n\nThe experimental design (with either the risk tolerance or reward changing) was sensible and clearly explained. \n\nThe theory is correct."}, "weaknesses": {"value": "The primary weakness is the contrast to the episodic RL setting, rather than the average reward setting. The paper uses the typical assumptions for the average reward (continuing setting) and even uses the algorithm previously developed for risk-aware average reward (by Rojas and Lee, 2025). The same CVaR decomposition as in Rojas and Lee is used. Yet this paper is pitched saying that the risk-aware measures used in RL are not compatible with continual learning (I think what is meant here is that the episodic risk-aware measures are not compatible). \n\nIt is possible that the objectives frm Rojas and Lee and this paper are slightly different, though these differences were not discussed (nor was the Rojas and Lee paper cited when the CVaR objective was proposed in the experimental results section). One difference I do note is that the objective uses pi_t, changing with time, rather than pi. However, I am actually not sure if this is an error or intentional. Are all pi_t independently optimized? The LHS of the equation has CVaR_{pi_t}, and then the RHS has pi_t changing with t in the sum. \n\nThe primary contribution seems to be in showing which measures do or do not satisfy the plasticity and local consistency properties. It could be more clear to both explain that episodic RL objectives do not satisfy these properties and that this risk-aware average reward objective (that was recently introduced, in Rojas and Lee) does satisfy these properties. Further, you could then highlight the more general class of ergodic risk measures, and potentially provide some useful examples. \n\nTo this point, this paper is currently hard to read and it is largely definitional and it can be hard to determine what is new. Are all the definitions new? Which ones are new? \n\nAdditionally the properties are very technical and it is hard to gain much intuition. There is intuition given, but at times it is too brief or too high-level. For example, parts of the plasticity definition are clear and others are not. After Definition 4.2 about asymptotic plasticity, the intuition is clear: the risk evaluation should only depend on recent history. Earlier in this section, it is stated: “That is, the risk evaluation at a given time step should depend only on the recent history leading up to that time step, rather than the entire history.” This leads me to think the static risk measures suffer from using the whole history (do they?) \nBut then in the proof it says: “Conversely, a static risk measure is a single mapping corresponding to a single point in time. That is, it is not time-indexed, and therefore cannot adapt as new information arrives.” If it depends on the whole history, won’t it provide a different evaluation as new information arrives? Providing slightly more detailed descriptions (not just high-level), in plainer language, on these technical definitions would be very helpful. \n\nAs a specific actionable suggestion, it would be useful for this section to \n(a) better motivate why we need this plasticity property, potentially with an example \n(b) give an example of a static risk measure.\nIt seems, based on the proof, that Lemma 4.1 is a trivial fact that could just be stated about static risk measures. What is more useful here is to help the reader understand why existing measures are ineffective and why it is important to have this property. I remain unsure if these properties are necessary for continual learning (e.g., with changing rewards as in the experiments) or necessary just for the average-reward setting. \n\nFinally, the goal of the experimental results is not fully clear. There is a demonstration that this algorithm can adapt in this environment, by optimizing this average reward objective. It seemed to me that this was the algorithm proposed by Rojas and Lee, but I could be missing something here. \n\n1. Is the algorithm new? If yes, this should be made more clear.\n\n2. Are there any pertinent baselines to include, to highlight why this new risk-aware objective is useful? For example, could static or nested risk measures be included in the experiments as baselines to show explicitly why they do not work in the continual setting?"}, "questions": {"value": "Summarizing the questions outlined above:\n1. It would be useful to explain more if this is different from the average reward setting, and potentially even comment on why these typical ergodic assumptions are appropriate given the continual learning motivation. Do they match the experiments? If not, then it would also be useful to comment on why this mismatch is ok.\n\n2. Is the objective the same as from Rojas and Lee, and what does it mean to optimize over pi_t?\nIs the algorithm new?\n\n3. What are the goals of the experiments?\n\n4. Do the static risk measures suffer from using the whole history?\n\n5. The key novelty seems to be in characterizing risk measures according to the plasticity and temporal coherence criteria. Are these properties only about plasticity-stability, or are they generally necessary to have sensible risk measures for the average reward setting? It would be useful here to give more insight into why we care about these definitions, and also if these risk measures have previously been used for average reward."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PY9S4ZQD26", "forum": "6aZxHDw163", "replyto": "6aZxHDw163", "signatures": ["ICLR.cc/2026/Conference/Submission4875/Reviewer_ocLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4875/Reviewer_ocLh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945849911, "cdate": 1761945849911, "tmdate": 1762917627402, "mdate": 1762917627402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response to Rebuttals"}, "comment": {"value": "We want to thank all the reviewers for their insightful reviews of our work. In this overall response, we summarize the changes made to the paper based on the reviewers’ comments (the updated draft is now available), as well as address two common concerns brought up by multiple reviewers. We address more specific concerns and comments under each review.\n\n**Summary of Changes**:\n\nBased on a suggestion from one of the reviewers, instead of proposing our own definition of plasticity for a risk measure, we now utilize the definition of plasticity of an agent (and subsequent continual RL framework) from [1]. More specifically, we generalize the framework from [1] to the risk-aware case, and use the resulting framework to propose a set of axioms that establish the conditions necessary for risk-awareness in the continual setting. These axioms are motivated by limitations of the agent itself, rather than the risk measure.\n\nThen, using these axioms, we follow a similar process as with the submitted draft to show that static and nested risk measures are not compatible with the axioms and that ergodic risk measures are.\n\nWe now address the two common concerns:\n\n**Ergodicity Assumptions and Continual RL**:\n\nMost of the reviewers argued that the ergodicity assumptions are too strong for the continual setting. In this regard we have a few comments:\n\nFirst, it is worth clarifying that in this work we propose a class of risk measures which we call ergodic risk measures. However, the ergodicity assumptions are not used to define this class of risk measures. Rather, the assumptions are used to show that the risk-aware version of the standard average-reward objective satisfies the definition of an ergodic risk measure, if the assumptions hold. Hence, the assumptions apply to the specific (average-reward-based) objective used, but not necessarily to the entire class of proposed ergodic risk measures.\n\nNext, we note that, in our work, the risk-aware average-reward objective was introduced as the objective of an infinite sequence of average-reward MDPs. In this regard, we note that the ergodicity assumptions need only apply independently for each MDP, rather than holistically for the entire sequence of MDPs. For example, Assumption 4.4 (in the updated draft) only requires that each state in a given MDP, is accessible from every other state in that same MDP, but not the states of other MDPs. We acknowledge that this was not made clear in the submitted draft, and have made this clarification in the updated draft. \n  \nFinally, we disagree that ergodicity is incompatible with continual RL. To the contrary, having the ability to (eventually) reach any state in a given MDP is in alignment with continual RL. Consider the contrary, if the ergodicity assumptions do not hold, then the agent could get ‘stuck’ in one part of the MDP indefinitely, which would not be ideal in a continual RL setting. \n\n**Ergodic Risk Measures vs Prior Works in Average-Reward RL:**\n\nWe received a few questions along the lines of: “You say that risk measure theory from non-continual RL does not work in the continual setting, but then use a prior algorithm from a non-continual setting [2] as an example of the proposed class of ergodic risk measure. How does that make sense?”\n\nEssentially, there is a theory-practice gap that this paper addresses. In particular, there have been prior works that have explored the risk-aware version of the average-reward objective (e.g. [2, 3]) in non-continual settings. However, none of these prior works formalized this objective in the context of risk measure theory. Accordingly, until this work, risk measure theory in non-continual RL consisted only of static and nested risk measures. As such, not only does this work propose a risk measure that is compatible with continual learning, but it also formalizes the risk-aware average-reward objective in the context of measure theory, thereby closing a theory-practice gap that had existed until now. We have clarified this in the updated draft of the paper as Remark 4.1.\n\nWe again thank the reviewers and look forward to further discussion.\n\n**References:**\n\n[1] Abel et al. 2025. “Plasticity as the Mirror of Empowerment.”\n\n[2] Rojas et al. 2025 “Burning RED: Unlocking Subtask-Driven Reinforcement Learning and Risk-Awareness in Average-Reward Markov Decision Processes.”\n\n[3] Xia et al. 2023 “Risk‐sensitive Markov Decision Processes with Long‐run CVaR Criterion”"}}, "id": "cKaimi4dkp", "forum": "6aZxHDw163", "replyto": "6aZxHDw163", "signatures": ["ICLR.cc/2026/Conference/Submission4875/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4875/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4875/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763610061445, "cdate": 1763610061445, "tmdate": 1763610280106, "mdate": 1763610280106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission provides an attempt to formalize the problem setting of risk-aware continual reinforcement learning. The authors give a definitions for properties of conditional risk measures they argue are appropriate for continual learning setting and provide an objective that satisfies those properties.  An experiment shown demonstrating that a RED CVaR Q-Learning agent can learn in an environments with risk preferences and reward functions that change."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The introduction motivates the problem well. The paper is ambitious in that it is the first attempt at formalizing risk-aware RL for the continual learning."}, "weaknesses": {"value": "Section 3.1 Regarding the definition of continual learning.  It is important to acknowledge that there is not a widely accepted definition of continual reinforcement learning. To my knowledge, the most popular definition was provided by Abel et al. 2023 (which is cited in this text but not discussed). The notation given here frames continual reinforcement learning as a sequence of MDPs indexed by a function of time. This is certainly a possible definition, although, it seems more appropriate to call it a definition of non-stationary RL.  Allowing the sequence of MDPs to be infinite where they \"may differ in its state-space\" certainly does lend itself to requiring continual learning (as per Abel et al. 2023).  However the definition of an infinite sequence of MDPs is not used again and the rest of the paper assumes a single MDP!  This disconnect makes it difficult to see how the prescribed properties and objective relates to the continual learning setting as proposed.\n\n(Definition 4.2 Asymptotic Plasticity) is a weak property. The description in the text describing Definition 4.2 in English does not match how it is defined. The sentence says \"...the influence of any past history vanishes, such that the risk evaluations effectively depend only on the more recent history\". This is a claim about any finite length of past history. That is, for all $n$, in the limit of infinite time, the influence of the past history vanishes. But this is not what the definition says.  The definition says there exists finite $n$ such that the risk measure does not depend on the history prior to time step $n$. The only things we are given is that $n$ exists and is finite. What if $n=1$? Then the plasticity property is saying the influence of only the first timestep in history is forgotten and all other history matters?  \n\n(Definition 4.4 Local Time Consistency) I have a similar concern as above with respect to the logical quantifiers in definition 4.4. The statement is there exists an $n$ and an $m$ such that time consistency holds on the interval $n \\leq t \\leq n+m$. What if there is one $n$ and $m=1$, then we only require that the risk metric is consistent for one time step? I would be more interested if the definition was something like for all $n$ there exists a finite $m$ such that time consistency holds on the interval $n \\leq t \\leq n+m$. \n\n(Equation 2) The common objective in RL is to maximize expected return, that is, the sum of rewards. Of course because expectation is linear, one can put the expectation inside the sum and write it as sum of expected rewards. This however, does not hold for general risk measures. It is strange that the objective in equation 2 is written as the sum of risk-measures of rewards instead of risk-measure of the sum of rewards. \n\nAlso, I believe there is a typo in the policy subscript of Equation (2). I assume it is supposed to be $\\rho_{\\pi_{t_0}}$  and not $\\rho_{\\pi_{t}}$ as is written. Right now, $t_0$ is an unbound variable. The same typo appears in the proof.\n\n(Assumptions 4.1 and 4.2) Both assumptions are making assumptions about a single MDP, breaking away from the continual learning setting given in section 3.1.\n\nOverall it's not clear that the claims from the abstract are adequately supported with evidence.\n\nThe abstract states that \"we show that the classical theory of risk measures, widely used as a theoretical foundation in non-continual risk-aware RL, is, in its current form, incompatible with the continual setting\".  The main evidence is that the authors present a particular property (asymptotic plasticity) and then claim that a particular choice of nested risk measures doesn't satisfy this property (or time consistency).  The argument for why these properties are necessary is not clear.  Or why these properties are sufficient either.  Furthermore, the author's proposed objective seems to rely on classical risk measures with $\\rho$.  And it seems you could even generalize it to allow $\\rho_t$ or $\\rho_{t_0}$.  All of this is to say that the word incompatibility suggests to me a much stronger claim than what's supported. \n\nThe abstract also states that \"We extend risk measure theory into the continual setting by introducing a new class of ergodic risk measures that are compatible with continual learning\".  None of this feels like an extension of risk measure theory.  It's then not clear what problem the class of ergodic risk measures is solving, or in what way that class would uniquely be compatible.\n\nFurthermore, it's unclear exactly what the experiments are trying to show (see Question below)."}, "questions": {"value": "I'm somewhat confused what the experiments are trying to show. The experiments provided are non-stationary variants of the red-pill blue-pill experiments in Rojas and Lee (2025).  In that work, convergence was proved for tabular RED CVaR Q-Learning and empirically demonstrated within 4k time steps (See Figure 3 of Rojas and Lee (2025)). In these experiments, they are run for 100k time steps. Is the point to demonstrate that RED CVaR Q-Learning can recover from changing in risk tolerance? I'd appreciate if the authors can comment on what was the intention here as the experiments don't seem fit with the rest of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BOvFaTixDF", "forum": "6aZxHDw163", "replyto": "6aZxHDw163", "signatures": ["ICLR.cc/2026/Conference/Submission4875/Reviewer_7NtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4875/Reviewer_7NtN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970427264, "cdate": 1761970427264, "tmdate": 1762917626856, "mdate": 1762917626856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies risk-aware continual reinforcement learning (RL), which addresses lifelong learning with endless adaptation and balances stability and plasticity. Existing risk-aware RL methods are risk-neutral or static/nested risk measures, which are incompatible with continual RL due to the stability-plasticity dilemma. This paper introduces ergodic risk measures, a new class of non-nested dynamic risk measures satisfying asymptotic plasticity and local time consistency, making them suitable for continual RL. The authors theoretically show the compatibility of ergodic risk measures with continuous settings under ergodicity assumptions, and then empirically validated the proposed risk measures by optimizing Conditional Value-at-Risk (CVaR) using RED CVaR Q-learning in continual learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**The following are the strengths of the paper:**\n1. This paper considers the risk-aware continual reinforcement learning and proposes an episodic risk measure for continual RL that addresses the stability-plasticity dilemma.\n\n2. The authors show that the proposed episodic risk measures satisfy asymptotic plasticity and local time consistency.\n\n3. Finally, the authors empirically validate the proposed risk measures by optimizing Conditional Value-at-Risk (CVaR) using RED CVaR Q-learning in continual learning."}, "weaknesses": {"value": "**The following are the weaknesses of the paper:**\n1. The ergodicity-like assumptions (unichain and communicating) can be strong and may limit applicability in non-ergodic real-world applications, e.g., when RL dynamics may be changing.\n\n2. The overall experiments are limited, as the authors have tested only two variations of a toy red-pill blue-pill continual learning task.\n\n3. As existing risk-aware RL works also give theoretical results (regret minimization, convergence) to show the optimality of their proposed methods, this paper does not consider the optimality of the proposed method. \n\n4. It is unclear why the risk defined in Eq. 7 is selected, as it may not be able to capture long-term dependencies (across states). Considering non-nested risk measures may not be able to capture the case where the risk will propagate through the states."}, "questions": {"value": "Please address the weaknesses of the paper. I have a few more questions/comments:\n1. Define the stability-plasticity dilemma upfront in the paper.\n\n2. Line 326: What does possibly-coherent risk measure mean?\n\n3. How does ergodic risk measure behave and adapt in highly non-stationary or non-ergodic environments where assumptions do not hold? How can it be extended using function approximation or deep RL architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not find any ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5R7w8F7xeQ", "forum": "6aZxHDw163", "replyto": "6aZxHDw163", "signatures": ["ICLR.cc/2026/Conference/Submission4875/Reviewer_avn6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4875/Reviewer_avn6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4875/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762930061085, "cdate": 1762930061085, "tmdate": 1762930061085, "mdate": 1762930061085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies risk-aware continual reinforcement learning (RL), which addresses lifelong learning with endless adaptation and balances stability and plasticity. Existing risk-aware RL methods are risk-neutral or static/nested risk measures, which are incompatible with continual RL due to the stability-plasticity dilemma. This paper introduces ergodic risk measures, a new class of non-nested dynamic risk measures satisfying asymptotic plasticity and local time consistency, making them suitable for continual RL. The authors theoretically show the compatibility of ergodic risk measures with continuous settings under ergodicity assumptions, and then empirically validated the proposed risk measures by optimizing Conditional Value-at-Risk (CVaR) using RED CVaR Q-learning in continual learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**The following are the strengths of the paper:**\n1. This paper considers the risk-aware continual reinforcement learning and proposes an episodic risk measure for continual RL that addresses the stability-plasticity dilemma.\n\n2. The authors show that the proposed episodic risk measures satisfy asymptotic plasticity and local time consistency.\n\n3. Finally, the authors empirically validate the proposed risk measures by optimizing Conditional Value-at-Risk (CVaR) using RED CVaR Q-learning in continual learning."}, "weaknesses": {"value": "**The following are the weaknesses of the paper:**\n1. The ergodicity-like assumptions (unichain and communicating) can be strong and may limit applicability in non-ergodic real-world applications, e.g., when RL dynamics may be changing.\n\n2. The overall experiments are limited, as the authors have tested only two variations of a toy red-pill blue-pill continual learning task.\n\n3. As existing risk-aware RL works also give theoretical results (regret minimization, convergence) to show the optimality of their proposed methods, this paper does not consider the optimality of the proposed method. \n\n4. It is unclear why the risk defined in Eq. 7 is selected, as it may not be able to capture long-term dependencies (across states). Considering non-nested risk measures may not be able to capture the case where the risk will propagate through the states."}, "questions": {"value": "Please address the weaknesses of the paper. I have a few more questions/comments:\n1. Define the stability-plasticity dilemma upfront in the paper.\n\n2. Line 326: What does possibly-coherent risk measure mean?\n\n3. How does ergodic risk measure behave and adapt in highly non-stationary or non-ergodic environments where assumptions do not hold? How can it be extended using function approximation or deep RL architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not find any ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7QjwKraqFZ", "forum": "6aZxHDw163", "replyto": "6aZxHDw163", "signatures": ["ICLR.cc/2026/Conference/Submission4875/Reviewer_avn6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4875/Reviewer_avn6"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4875/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762930128920, "cdate": 1762930128920, "tmdate": 1762930128920, "mdate": 1762930128920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}