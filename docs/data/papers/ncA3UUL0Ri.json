{"id": "ncA3UUL0Ri", "number": 6153, "cdate": 1757954819173, "mdate": 1759897932709, "content": {"title": "Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding", "abstract": "We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug and play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.", "tldr": "", "keywords": ["Privacy Preservation", "Video Understanding"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f69b98cba42ddce055969722d9f59a3cee0ff5a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SPLAVU, a lightweight anonymizing adapter placed on top of a frozen video encoder to remove spatially private information from video features while preserving temporal information for downstream tasks. The method is trained with three objectives: a clip-level self-supervised privacy objective to reduce mutual information between static clips, task-specific utility losses, and a latent consistency term to preserve transferability. Experiments on action recognition, temporal detection, and anomaly detection show about a 35% drop in private-attribute predictability with minimal utility loss. Privacy is evaluated on VISPR and CASIA-B, and a bias protocol is included on NTU RGB+D and Toyota Smarthome. The approach is simple and does not require retraining the video backbone."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well motivated. Privacy preservation in video representations is an important problem, and the community should work more on this topic.\n- The paper introduces the novel idea of anonymizing latent features instead of input pixels or frames.\n- The proposed anonymization adapter (AAM) is lightweight, plug-and-play, and works on top of a frozen video encoder, making the approach simple and efficient to train.\n- The visual explanation in Figure 2 helps readers understand the method.\n- The evaluation includes privacy, fairness, and bias analysis, which broadens the impact and shows awareness of social implications."}, "weaknesses": {"value": "- The privacy evaluation is not clear. It appears that the authors train a separate classifier on anonymized features f_A(f_E(x)) for VISPR, but the implementation details (e.g., classifier type, training schedule, multi-label setting, etc) are missing. Clarifying this would help assess reproducibility and fairness when comparing to prior privacy-preserving frameworks that integrate the classifier during training.\n\n- The paper enforces a latent-consistency loss to keep anonymized features close to the encoder's latent space, aiming for generalization to unseen tasks. However, this design could also make the anonymized representations vulnerable to feature inversion or reconstruction attacks, since $h'_t=f_A(f_E(x_t))$ may remain close to $h_t=f_E(x_t)$. An attacker with access to the frozen encoder or a decoder trained on its features could potentially approximate the original frames. Specifically, an adversary could learn an approximate inverse $f_E^{-1}$ or a regression model $g: h'_t \\rightarrow x_t$ trained on pairs $(h_t, x_t)$ or $(h'_t, x_t)$, enabling recovery of frames from anonymized features. A discussion of this risk would strengthen the paper's privacy claims.\n\n- The paper claims generalization to \"unseen tasks,\" yet the evaluated ones (action recognition, temporal action detection, and anomaly detection) are all closely related motion-centric problems. Assessing the method on more diverse downstream video understanding tasks, such as video retrieval, video object segmentation, video captioning, or person re-identification, would provide stronger and more convincing evidence of true task-level generalization.\n\n- Unlike prior works such as SPAct, this paper does not include an explicit privacy classifier or adversarial optimization loop. While this design choice simplifies training, it also weakens the link between the training objective and actual privacy protection. The self-supervised \"privacy budget\" loss acts only as an indirect proxy for removing spatial identity information and may not guarantee robustness. It would also be helpful to visualize the input to the privacy classifier model.\n\n- Since the privacy and utility objectives are inherently competing, I wonder if the joint optimization of $f_A$ and $f_T^{*}$, considering $L_B$ and $L_{LC}$, remains stable.\n\n- The ablation on the latent-consistency (cycle-consistency) loss is evaluated only on HMDB51, whereas the main results (Table 1) include multiple datasets and tasks. It would be more convincing to show the same ablation across others.\n\n- The paper fixes the clip sampling and length configuration but does not analyze its impact on privacy or utility. Since privacy leakage and temporal consistency can depend strongly on clip duration and sampling strategy, an ablation on clip length or selection would help clarify whether the proposed method's gains generalize across different temporal windows.\n\n- The structure could be improved for clarity. While $f_A$ appears in the equations of Section 3.1, its definition is only given in Section 3.2. Introducing it earlier would make the methodological flow easier to follow.\n\n- The notation is not clearly defined and is sometimes hard to follow. For example, $T^n$ is used in the loss formulation but never explicitly defined; it's unclear whether n indexes tasks, samples, or temporal segments."}, "questions": {"value": "My questions are based on the weakness above. Specifically:\n1. Can the authors please clarify the implementation details of the privacy classifier and provide more information on how privacy was evaluated?\n2. Do the authors evaluate the robustness of their method against feature inversion or reconstruction attacks? Can the anonymized features $h'_t$ be used to approximate or reconstruct the original frames $x_t$?\n3. Does the proposed method actually generalize to unseen tasks such as video retrieval, video object segmentation, captioning, or re-identification?\n4. If there is no adversarial classifier or adversarial optimization during training, how do the authors ensure that the optimization process is actually guided to remove privacy-related attributes rather than destroying useful features? Simply reducing mutual information between frame features does not seem to be a strong proxy for privacy removal.\n5. How does the input of the privacy classifier look like?\n6. How stable is the proposed approach? Since the privacy and utility objectives are inherently competing, I wonder wheter the joint optimization of $f_A$ and $f_T^{*}$, considering $L_B$ and $L_{LC}$, remains stable during training.\n7. Why is the ablation on the latent-consistency loss reported only on HMDB51? Could you extend it to another dataset shown in Table 1?\n8. Did the authors analyze how clip length or sampling strategy affects privacy and utility? Would longer or shorter clips change the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G4TjVQyeF6", "forum": "ncA3UUL0Ri", "replyto": "ncA3UUL0Ri", "signatures": ["ICLR.cc/2026/Conference/Submission6153/Reviewer_dNWy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6153/Reviewer_dNWy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728982079, "cdate": 1761728982079, "tmdate": 1762918505230, "mdate": 1762918505230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies privacy preserving action recogniton under the umbralle of video foundation models. It suggests adapating the feature output of VFMs with a goal of 1) hidding the privacy information and 2) maintaining the capability of performing well on down-stream tasks. To achieve that, it introduces three objectives to be learnined together to an ANONYMIZE the latent embeddings - contrastive loss to reduce the mutal information, down-stream utiliy loss and latent space regulaization loss. Extensive experiment have been done to demonstrate the success of the method, including both appearance based privacy and motion-based privacy (e.g., gaiting)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A. The most notable strengths of the paper is the extensive experiments - testing across benchmarks and domains. The scope is comprehensive enough to prove the point. Works across multiple downstream tasks (action, temporal detection, anomaly detection) using the same anonymized features. The engineer efforts are huge without saying. Hope the code will be released for reproduction purpose.\n\nB. The delivery of the paper is clear -well structured wording, tables and figures."}, "weaknesses": {"value": "A. The focus on latent space, rather than pixel space, poses a challenge on interpreting the ANONYMIZED video with human-eye level intuition - Can not visually see the privacy hideout anymore. Let's consider a situation where a well-trained VFMs decoder exist (e.g., video generative model that takes VideoMAE-v2 latent embedding as input), is it possible such as generative decoder can still produce privacy revealing outputs in pixel spaces with the adapted embedding as input? This is not necessarily an attack on the paper but an open discussion chat. Wondering how would authors think about the interpretabiliy of the proposed method? \n\nB. Table 5 shows that the L_T is very important to the recogntion performance. And the training of L_T is expensive - it requires labels, and it is concerning how much label it takes for the adapted embedding to be generalized again for a large scope of video understanding tasks, which is the original mission of VFMs. The fact that the method uses multiple downstream tasks as L_T make it hard to draw conclusion on this question. This reviwer has critical objections on this matter. It can be the case that the contrastive loss is too strong so that it destroys the useful information in the embedding space after massive data pre-training, again on the opposite direction to the VFMs. Any thoughts on how to reduce the impact of L_T training, like reduce the amount of label needed, the amount of data needed, etc,  so that the entire method is less data-hungry and by nature maintaining represenatation power with cheap buget?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J9OtJOsOsQ", "forum": "ncA3UUL0Ri", "replyto": "ncA3UUL0Ri", "signatures": ["ICLR.cc/2026/Conference/Submission6153/Reviewer_mpGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6153/Reviewer_mpGY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937811375, "cdate": 1761937811375, "tmdate": 1762918504900, "mdate": 1762918504900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a privacy-preserving framework based on video foundation models by introducing a lightweight AAM at the backend of the video encoder. The AAM operates in the feature space to remove privacy-related information, and features a plug-and-play design that maintains task performance across multiple downstream tasks while achieving effective anonymization. Additionally, the method demonstrates potential in mitigating biases related to attributes such as gender."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The AAM module is designed to be lightweight and modular, enabling seamless integration with various video foundation models. It introduces minimal architectural interference to the original model and ensures efficient inference, reflecting strong modularity and scalability.\n\nExtensive experiments are conducted on diverse datasets, including Kinetics-400 (K400), UCF101, HMDB51, ToyotaSmartHome (ToyotaSH), UCF-Crime, and THUMOS14, covering multiple downstream tasks such as action recognition, temporal action localization, and anomaly detection. This broad evaluation demonstrates the method’s generalizability and reliability, thereby strengthening the validity of the experimental conclusions.\n\nThe work introduces an assessment of gender bias within the context of video-based privacy protection, which adds social relevance and reflects forward-looking consideration of ethical implications in vision systems."}, "weaknesses": {"value": "The design rationale of the AAM module lacks sufficient elaboration. There is inadequate theoretical justification for key design choices—such as the selection of the module’s architecture and the weighting scheme in the loss function—and insufficient ablation studies to validate their effectiveness and superiority. As a result, the optimality and robustness of the proposed design remain insufficiently supported.\n\nPrivacy preservation performance is primarily evaluated using the VISPR dataset. This evaluation approach is limited in scope and lacks validation under diverse privacy attack benchmarks or against dynamically defined sensitive attributes, which may undermine the comprehensiveness of the privacy analysis.\n\nWhile the paper presents preliminary analysis on gender bias mitigation, it lacks systematic comparison with existing debiasing methods. Consequently, it is difficult to assess the relative advantages or practical efficacy of the proposed approach in addressing algorithmic bias."}, "questions": {"value": "What motivated the choice of a 3-layer multi-head self-attention (MH Self-Attention) structure as the core component of the AAM? Were alternative lightweight architectures or different depths considered and compared through controlled ablation experiments to justify the effectiveness and necessity of this specific configuration?\n\nOn what basis were the weights assigned to the individual terms in the loss function determined? Was a systematic hyperparameter tuning procedure conducted to ensure that the selected weight combinations achieve an optimal trade-off between anonymization performance and task utility across different datasets and tasks?\n\nIn the anomaly detection task (e.g., on UCF-Crime), the reported AUC values do not reach state-of-the-art levels. Could the authors provide an analysis of potential factors contributing to this performance gap? For instance, does the anonymization process inadvertently filter out subtle but critical cues necessary for detecting anomalies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UEC9NEuaOX", "forum": "ncA3UUL0Ri", "replyto": "ncA3UUL0Ri", "signatures": ["ICLR.cc/2026/Conference/Submission6153/Reviewer_u3Uc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6153/Reviewer_u3Uc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997108867, "cdate": 1761997108867, "tmdate": 1762918504592, "mdate": 1762918504592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces SPLAVU, a latent-space anonymization framework that leaves the video encoder frozen and inserts a lightweight Anonymizing Adapter Module (AAM) to strip private attributes from clip features while preserving utility across tasks. The training couples (1) a clip-level, self-supervised privacy objective that maximizes NT-Xent on static clips to reduce shared spatial information, (2) co-training utility losses for action recognition, temporal action detection, and anomaly detection, and (3) a latent consistency loss to retain the encoder’s generalization on unseen tasks.\nEvaluation measures privacy via VISPR cMAP with a linear attacker on static-clip representations and probes temporal sensitive attributes through Casia-B gait retrieval, while utility is tested on Kinetics-400, UCF101, HMDB51 (AR), THUMOS14 (TAD), and UCF-Crime (WSAD); the paper also proposes gender-bias protocols on NTU-Bias and Toyota Smarthome. Across backbones and tasks, SPLAVU reports 35% lower privacy leakage with ≤1–2% utility drop, and the self-attention AAM outperforms MLP adapters in privacy utility balance; in bias evaluations, it narrows gender-subclass gaps on Toyota Smarthome."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript proposes a latent‐space anonymization scheme (SPLAVU) that keeps the encoder frozen and adds a plug-and-play self-attention Anonymizing Adapter Module. This enables practical integration with low computational overhead and no backbone retraining.\n\nThe manuscript demonstrates a consistent privacy utility balance across multiple tasks (AR, TAD, WSAD) and different backbones, achieving sizable reductions in privacy leakage with ≤1–2% utility drop, and providing controllable trade-off curves.\n\nThe manuscript introduces a latent consistency loss that prevents overfitting the privacy budget and supports generalization to unseen tasks; ablations show that removing it degrades privacy and utility.\n\nThe manuscript evaluates temporally sensitive attributes and gender-bias protocols, showing mitigation of subclass gaps without sacrificing downstream performance."}, "weaknesses": {"value": "The privacy evaluation is limited to VISPR with a linear attacker on static clips, which can overestimate the achieved protection and does not cover realistic threat scenarios (white/black-box, access to $f_E$ or $f_A$). Therefore, the manuscript should make the threat model explicit and test stronger attackers (e.g., MLP/ResNet or margin-based), also reporting ROC/PR curves to characterize risk more fully.\n\nThe choice to maximize NT-Xent as a privacy objective has limited theoretical support: it assumes shared information is “broken,” but it is unclear which attributes are actually removed and which persist. Therefore, the manuscript should expand the justification (e.g., connection to mutual information/subspace suppression) and add diagnostics showing which sensitive signals are effectively attenuated.\n\nThere is a modal mismatch between the problem (video) and the main privacy metric (attributes from static images); although a gait test is included, it remains a singular case. To better align the evaluation with the objective, it would be valuable to incorporate clip-level temporal attackers and to make the threat model explicit.\n\nQualitative visual results are missing, which would help readers interpret the practical effect of anonymization in video understanding. Therefore, the manuscript should include before/after examples, t-SNE projections of latents, activation maps, and representative failure cases to illustrate where privacy is gained and how utility is preserved.\n\nHyperparameter transparency and sensitivity of the composite objective are incomplete: the weights $\\omega_{LC}$, $\\omega_{T}$, and $\\omega_{B}$ steer the privacy–utility balance, but their selection is not centralized. Therefore, the manuscript should consolidate a table with values per task/backbone and report sensitivity sweeps that guide configurations by use case"}, "questions": {"value": "Equation (7) shows a notation inconsistency: the objective uses $\\omega_{LC}$, $\\omega_{T}$, $\\omega_{B}$, but the prose immediately refers to $\\omega_{R}$ instead of $\\omega_{LC}$.\n\nUnify the symbol for the latent consistency weight across the manuscript (see lines 340--357).\n\nIn Equation (5) and its explanation, the text first suggests that minimizing increases similarity, but later states that the privacy loss is maximized; this back-and-forth can confuse readers.\n\nPlease clarify explicitly that the privacy term is maximized via the negative sign in (7), i.e., the total objective is minimized with $-\\omega_{B} L_{B}$ (see lines 286--305 and 340--357).\n\n\nIn Criterion-1 (Equation 1), there is an extra argument “, $T_n$” inside $L_{T_n}(\\cdot, T_n)$ on both sides of the approximate equality; this looks like a notation carryover.\n\nClean up the loss signature to avoid ambiguity (see lines 200--215).\n\nIn Appendix B.1 (Architecture), there is a typo: “Mulit-headAttention” should read “Multi-Head Attention.” Fix the spelling of the multi-head attention block (see lines 788--809).\n\nIn Section 5.2 (Bias), “complimentary protocol” should be “complementary protocol.” Adjust the term to the standard usage (see lines 386--399).\n\nIn Appendix A (NTU Bias protocol), “lets” should be “let’s.” Correct the contraction for grammatical accuracy (see lines 780--809).\n\nFigure 1: the right panel (“Overall score / Task Accuracy (%) | Privacy (↓)”) is hard to read due to font size and density. Increase font sizes and clarify how the “overall score” is computed (refer to the corresponding formula; see lines 33--53 and 1026--1045).\n\nFigure 2: label saturation (“AAM”, “MH Self-Attn Layer”, overlapping arrows). Simplify the legend (e.g., a symbol box) and scale typography for readability (see lines 162--215).\n\nVariable naming standardization: Section 3.2 introduces $f_T^{*}$ and variants $f_{TAR}$, $f_{TTAD}$, $f_{TAD}$, while Algorithm 1 later uses $f_{\\text{reco}}$, $f_{\\text{tad}}$, $f_{\\text{wsad}}$. Harmonize head names throughout text and code for traceability (see lines 232--269 and 1100--1133).\n\nDefinitions and units: in Section 4.2, metrics (Top-1, mAP, AUC) are not always paired with units/protocol specifics (e.g., clips per video, IoU thresholds, frame-level vs. segment-level). Add a brief table with metrics, units, and splits per dataset for clarity (see lines 324--377)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vlJaUYLty4", "forum": "ncA3UUL0Ri", "replyto": "ncA3UUL0Ri", "signatures": ["ICLR.cc/2026/Conference/Submission6153/Reviewer_SgKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6153/Reviewer_SgKC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015769642, "cdate": 1762015769642, "tmdate": 1762918504159, "mdate": 1762918504159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}