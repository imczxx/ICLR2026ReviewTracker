{"id": "ZBj3Qp1bYg", "number": 14452, "cdate": 1758235782254, "mdate": 1759897369097, "content": {"title": "Energy-Based Transformers are Scalable Learners and Thinkers", "abstract": "Inference-time computation, analogous to human System 2 Thinking, has recently become popular for improving model performance. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question “Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?” We find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs)---a new class of Energy-Based Models (EBMs)---to assign an energy value to every input and candidate-prediction, enabling predictions through energy minimization until convergence. To support this approach, we introduce several key techniques for stable and parallelizable training, which enable the emergence of strong System 2 Thinking capabilities and scalable EBMs. Across discrete and continuous modalities, we find EBTs outperform the Transformer++ approach, scaling up to 35% faster during pretraining, and improving inference-time performance by up to 29%. EBTs also surpass Diffusion Transformers on image denoising while requiring 99% fewer forward passes. Moreover, System 2 Thinking with EBTs yields larger performance gains on data that is farther out-of-distribution, and EBTs achieve better results than existing models on most downstream tasks despite achieving the same or worse pretraining performance, enabling EBTs to generalize better than existing approaches. Consequently, EBTs are a flexible and exciting new approach for scaling both the learning and thinking capabilities of models.", "tldr": "We introduce Energy-Based Transformers, a scalable new approach for learning how to think from unsupervised learning, generalizing current System 2 Thinking/reasoning approaches.", "keywords": ["Energy-Based Models", "System 2 Thinking", "Reasoning", "Verification", "Scaling", "Transformers", "Generative Modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5c1f17dbfdebf892a3632b91afeadc9cf135e52.pdf", "supplementary_material": "/attachment/be80e4edb32afef701343deee33ada2aa2c2631b.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Energy-Based Transformers (EBTs), a framework that reconceptualizes prediction as an optimization process rather than direct generation. Instead of emitting outputs (tokens, pixels, etc.) in a single forward pass, EBTs learn an energy function that evaluates the compatibility between inputs and candidate predictions. Inference then proceeds through gradient-based refinement toward lower-energy (more consistent) states."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The approach enables two appealing properties often associated with \"System 2\" reasoning: (i) adaptive computation, where more challenging inputs naturally require more optimization steps, and (ii) explicit self-evaluation, since energy levels can serve as confidence or verification signals.\nEmpirically, the paper demonstrates competitive performance across discrete and continuous domains and reports improved robustness under distribution shifts. The idea of learning to verify rather than learning to generate feels conceptually elegant, and the result ,especially the scaling behavior and cross-domain generality, suggest that EBTs may offer a promising direction for inference-time reasoning emerging directly from unsupervised pretraining."}, "weaknesses": {"value": "Weaknesses\n\n- Due to computational limitations, experiments are limited to models of up to 800M parameters, making claims about scaling advantages at the billion-parameter or foundation scale speculative. We can discount the requirement for experiments in billion/trillion parameter range, due to constraint of academic resources. However,  the projected breakeven point at which EBTs would become FLOP-efficient relative to Transformer++ is estimated at roughly 10⁵² total FLOPs, a scale that is effectively beyond practical reach. For context, even a state-of-the-art exascale system (≈10¹⁸ FLOPS) would require approximately 10³⁴ seconds of continuous computation to reach this point. Hypothetically distributing the workload across all 8 billion people on Earth, each operating a dedicated exascale supercomputer 24/7 (≈8×10²⁷ FLOPS combined), the breakeven computation would still demand about 1.25×10²⁴ seconds, equivalent to 3.96×10¹⁶ years, or nearly 400 trillion human lifetimes (assuming 100 years per lifetime) .Such an extreme requirement implies that the nominal scaling crossover occurs far beyond any conceivable computational regime. Consequently, the reported scaling advantage, while theoretically positive, lacks practical significance: in realistic operating regimes, EBTs remain orders of magnitude less compute-efficient than Transformers, making it difficult to justify claims of superior scalability.\n\n\n- EBTs require roughly 10× more compute to achieve the same validation perplexity, calling into question their current efficiency. The suggestion that EBTs could surpass advanced Transformer variants remains speculative without stronger empirical backing. The argument that the slope \"suggests\" that EBTs would surpass transformer++ at high levels of compute, while correct, would benefit from more backing. \n- The scaling law analysis lacks transparency: the procedure for estimating exponents and the dataset used for fitting are not described, making reproducibility difficult. Including a detailed table of fitted data points would improve credibility.\n- The iterative inference procedure introduces multiple hyperparameters (e.g., step count, learning rate, noise levels) that require tuning. This makes deployment and fair comparison more involved than with standard feedforward Transformers.\n\n\n- The method incurs substantially higher computational cost per token, yet the comparisons are not normalized for equal FLOPs. Without such baselines, it is unclear whether the observed gains stem from architectural benefits or simply increased compute. For text generation in particular, where predictions are locally conditioned, the incremental benefit appears modest relative to diffusion-style alternatives.\n\n\n- The reported “2.91% faster scaling” claim (highlighted in Figure 5) lacks methodological clarity, no explanation is given for how this value was computed or whether variance in scaling fits was accounted for.\n\n\n- It is not specified whether models in Table 3 were compared under matched computational budgets. Clarifying this would improve interpretability of the downstream comparisons."}, "questions": {"value": "------\nSome additional questions:\n- Thinking longer presents an interesting argument. I know that some prior ideas like \"HRM/TRM\", and \"Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach\", have explored similar ideas (thinking longer, although they don't have explicit verification mechanism).-\n- It would be great to see where EBTs perform well in CONSTRAINED data settings, for eg, solving extreme-sized sudoku puzzles, mazes, and unsupervised object discovery."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gaTXkSTDdD", "forum": "ZBj3Qp1bYg", "replyto": "ZBj3Qp1bYg", "signatures": ["ICLR.cc/2026/Conference/Submission14452/Reviewer_KJ9J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14452/Reviewer_KJ9J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760738075170, "cdate": 1760738075170, "tmdate": 1762924855985, "mdate": 1762924855985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EBTs that learn to verify the compatibility between inputs and candidate predictions through energy minimization. EBTs address limitations of existing System 2 Thinking approaches by enabling dynamic computation allocation and prediction verification across modalities without requiring additional supervision. The work introduces techniques for stable and parallelizable training of EBMs, demonstrating improved performance over Transformer++ and DiTs on language modeling and image denoising tasks while using fewer forward passes."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This paper is well written with clear motivation analysis, comprehensive experimental design, and informative figures that effectively support the main arguments. \n- The core insight of building generation models based on the principle that verification is easier than generation is novel and insightful. This approach provides a fresh perspective on how to design generative models by leveraging verification capabilities.\n- The dynamic computation allocation based on problem difficulty is an interesting contribution that offers an alternative technical pathway for adaptive computation. This differs from common approaches that rely on layer skipping and provides a new way to handle varying computational requirements.\n- The experiments are comprehensive, particularly in the language modeling domain where multiple scaling dimensions are explored to validate the proposed method. The extensive evaluation across different model sizes, data scales, and computational budgets demonstrates the robustness and effectiveness of the approach."}, "weaknesses": {"value": "- The verification approach in this work does not align with intuitive expectations. For language tasks, meaningful verification should operate on complete sentences or full responses to questions. However, EBT uses autoregressive generation where each token is verified independently during generation. This raises questions about the validity of verification at the token level rather than at the semantic level of complete thoughts or responses.\n\n- This work lacks sufficient analysis of computational efficiency. The paper primarily uses forward pass counts as an indirect measure, but absolute metrics such as latency or other time-based measurements would provide a more objective assessment of EBT's computational performance compared to Transformer++. Such analysis is crucial for understanding the practical viability of EBT.\n\n- The implementation details of EBT are not clearly explained. The training process involves gradient descent optimization for token generation, which likely requires second-order gradient computation. However, this paper does not provide sufficient explanation of how second-order gradients are computed, their computational cost, or their impact on training efficiency.\n\n- The attention mechanism in EBT may face efficiency challenges with second-order gradient computation. Modern LLMs and DiTs rely heavily on optimized implementations like Flash Attention for CUDA acceleration. However, these optimizations may not be compatible with second-order gradients, potentially limiting the computational efficiency of EBT's modified attention mechanism.\n\n- The DiT experiments focus on image denoising, which is not representative of typical industrial applications. Most DiT usage in practice involves image generation or video generation tasks where denoising acceleration is highly demanded. This work would benefit from experiments on these more relevant applications to demonstrate EBT's effectiveness in real-world scenarios.\n\n- Figure 5(a) and (b) show concerning patterns where EBT curves appear above the baseline in the upper right region. Given that the x-axis represents model size or compute and the y-axis represents perplexity, this suggests EBT achieves higher perplexity under similar conditions, which is not a positive trend. This may indicate a plotting issue."}, "questions": {"value": "Please refer to the weaknesses section for specific technical questions. This work presents a novel approach to System 2 Thinking through Energy-Based Transformers, which provides a very novel perspective and contributes to the community. The work meets the acceptance bar for ICLR, but based on the noted issues, cannot receive an absolutely high score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NK5s2qHgP5", "forum": "ZBj3Qp1bYg", "replyto": "ZBj3Qp1bYg", "signatures": ["ICLR.cc/2026/Conference/Submission14452/Reviewer_8YzV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14452/Reviewer_8YzV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761202786069, "cdate": 1761202786069, "tmdate": 1762924855013, "mdate": 1762924855013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Energy-Based Transformers (EBTs), a new framework that integrates energy-based modeling with Transformer architectures to enable what the authors describe as “System 2 Thinking.” Rather than directly predicting outputs, EBTs assign an energy value to input–output pairs, representing their compatibility, and perform inference through energy minimization. This allows the model to iteratively refine predictions and dynamically allocate computation based on task difficulty. The authors frame this as a form of unsupervised reasoning or self-verification. To make this scalable, they introduce several key techniques for stabilizing EBM training, including energy landscape regularization, Langevin noise, and replay buffers. EBTs are evaluated across language and vision domains, showing improved scaling behavior over standard Transformers, better generalization on out-of-distribution data, and significantly higher efficiency than Diffusion Transformers for denoising tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core contribution is genuinely novel and conceptually elegant. The insight that verification is easier than generation (grounded in complexity theory) is well-motivated, and coupling the verifier and generator through energy gradients avoids adversarial dynamics. The cross-modal validation across language, video, and images demonstrates generality beyond domain-specific tricks. Most compellingly, Figure 6 showing that thinking gains increase with distributional shift mirrors human cognition and suggests the approach addresses fundamental generalization challenges. The experimental analysis is thorough with good ablations and comprehensive scaling experiments across six axes.The technical execution is solid, with practical solutions to known EBM training challenges (replay buffers, Langevin dynamics, path randomization) and honest discussion of limitations including text-to-image generation failures. The OOD generalization results showing better downstream performance despite worse pretraining perplexity are particularly interesting. The extensive implementation details and promised code support reproducibility, and the work opens important research directions around reasoning without external supervision and uncertainty quantification in continuous spaces."}, "weaknesses": {"value": "The scale limitations severely undermine the paper's claims. All experiments max out at 800M parameters while making assertions about foundation model behavior through extrapolation. The paper uses extrapolation to larger sized for many claims, which is speculative given that scaling laws often break at different regimes. Without validation the central claims about foundation model potential remain unsubstantiated.\nThe computational costs (3.33-6.66× FLOPs for training, gradient computation overhead at inference) are dismissed too casually as a temporary inconvenience rather than a fundamental barrier. At foundation model scale, this multiplier translates to enormous real-world costs. The paper lacks wall-clock time comparisons, memory analysis, or ablations showing what standard transformers could achieve with the same computational budget. The \"System 2 Thinking\" here is essentially inference-time optimization and the analogy to human reasoning is somewhat overstated, as it doesn't incorporate concrete verbose reasoning.\nCritical baselines are missing, particularly comparisons to recent inference-time scaling methods. The Transformer++ baseline showing \"no improvement\" with more compute is somewhat unfair since it wasn't designed for that."}, "questions": {"value": "1. You report that EBTs require 3.33-6.66× more FLOPs than standard Transformers during training. However, the paper doesn't compare what standard Transformers could achieve with an equivalent computational budget (e.g., training with 3× more parameters, 2× depth, or simply training longer). Could you provide an ablation where Transformer++ is given the same total compute budget as EBT? This would help disentangle whether improvements come from the EBM formulation itself or simply from using more computation. Additionally, what are the wall-clock training times and memory requirements compared to Transformer++ on identical hardware?\n2. Section B.3 says that EBTs fail on text-to-image generation with multiple modes, producing blurred images. This seems fundamental, yet the paper broadly claims superior generalization for EBTs. Can you clarify: (a) What classes of problems will inherently fail with EBTs due to such limitations? (b) Is there a fundamental trade-off between the strong OOD generalization you demonstrate and the ability to handle multimodal output distributions? (c) For language modeling specifically, how does this limitation manifest (e.g. does it affect creative generation tasks that benefit from diversity)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "df2KXzh3cm", "forum": "ZBj3Qp1bYg", "replyto": "ZBj3Qp1bYg", "signatures": ["ICLR.cc/2026/Conference/Submission14452/Reviewer_nhmE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14452/Reviewer_nhmE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822670000, "cdate": 1761822670000, "tmdate": 1762924854022, "mdate": 1762924854022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework of an energy-based transformer model. This model integrates a new method that allows efficient and scalable training of EBMs, and thus allows inference-time verification."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- I think the core idea, integrating self-verification in inference time, is promising.\n- The empirical results verify the effectiveness of the proposed model."}, "weaknesses": {"value": "- Although this paper is overall clear, I think the authors have spent too much space introducing related concepts, instead of the technical details of the proposed method. It is unclear how the model specifically works. It would be good if there could be an extra section to explain the details of the model, perhaps through a toy example.\n- It is unclear why \"frame EBM as an optimization problem\" can avoid the curse of dimensionality. All evidences provided in the paper are just vague discussions, and I don't see any formal analysis. Also, it is unclear how this \"frame EBM as an optimization problem\" actually works."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3K0YLg3RXV", "forum": "ZBj3Qp1bYg", "replyto": "ZBj3Qp1bYg", "signatures": ["ICLR.cc/2026/Conference/Submission14452/Reviewer_k52K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14452/Reviewer_k52K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762228651811, "cdate": 1762228651811, "tmdate": 1762924853680, "mdate": 1762924853680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}