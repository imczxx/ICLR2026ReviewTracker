{"id": "VtWkPIbAQ8", "number": 20288, "cdate": 1758304469787, "mdate": 1763742186649, "content": {"title": "Taming Polysemanticity in LLMs: Theory-Grounded Feature Recovery via Sparse Autoencoders", "abstract": "We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. \nExisting SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. We rethink this problem from the perspective of neuron activation frequencies, and through controlled experiments, we identify a striking phenomenon we term neuron resonance: neurons reliably learn monosemantic features when their activation frequency matches the feature's occurrence frequency in the data.\nBuilding on this finding, we introduce a new SAE training algorithm based on ``bias adaptation'',  a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 2 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees and practical effectiveness for LLM interpretation.", "tldr": "We present a theoretically grounded sparse‐autoencoder training algorithm that provably recovers underlying features while outperforming existing benchmark methods", "keywords": ["sparse autoencoder; training dynamics; superposition; feature learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fda864abbf2117a79892ca6c4f6410d2d0eb37bf.pdf", "supplementary_material": "/attachment/067d60556bd5dd1cb23610d172241bbaec8b5b92.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel way of selecting SAE encoder bias (or, I think sometimes JumpReLU biases too) by adjusting the bias during training to force the SAE latent to fire within a pre-determined frequency range. The paper introduces a theorem stating that in order for an SAE latent to recover an underlying feature, it needs to fire with roughly the frequency of that feature. The paper turns this into an SAE architecture, called \"Bias adaptation\" (BA) or \"Group Bias Adaptation\" (BA), and evaluates the SAE on LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The algorithm for tuning the bias is novel and simple\n- The paper provides a theorem around when features can be recovered by an SAE"}, "weaknesses": {"value": "- This paper seems confused about how SAEs work currently. There is an implicit assumption that current SAEs do not naturally find the correct firing frequencies for latents, but this is easily disprovable. Toy model experiments with standard SAE architectures will show that current SAEs already naturally find the correct firing frequencies for latents.\n- It is hard to tell what the authors are concretely doing in experiments. The background only talks about tied SAEs, but adds an encoder bias which does not make sense for tied SAEs. I suspect that the paper is not using tied SAEs at all, but it's very hard to tell. It is also confusing what specific bias is being used where, for instance, the authors talk about JumpReLU GBA SAEs, where it would make sense to control the JumpReLU bias, not the encoder bias.\n- The experiments in this paper are not rigorous and confusingly do not compare to SOTA architectures. The authors train a JumpReLU version of their SAEs (JumpReLU is considered SOTA), but do not compare this to any SOTA architectures (BatchTopK or JumpReLU).\n- The SAEs where SAEBench metrics are run are very strange, with L0 dramatically higher than I have ever seen before. SAEBench also tends to be noisy, and requires multiple seeds to be run to get a good signal, but this is not done.\n- This paper is built on the idea that SAE latents need to be fire at certain frequencies to learn the underlying features, but then hardcodes arbitrary feature firing bands for the latents to fire at.\n\nOverall, I believe the claim that SAEs latents should match the underlying firing frequencies of the features they track, but do not believe that the methods described in this paper are a valuable or valid way to adjust SAE bias thresholds. I also do not believe that the experiments are done in a valid and rigorous way."}, "questions": {"value": "### Larger questions\n- Why do SAEs not already naturally find the correct feature firing frequencies for features? In toy models SAEs seem to naturally learn the correct features and fire them the correct times, especially for toy models with independent features like you use. It sounds like your paper is claiming this only happens if you manually set the encoder bias? This seems hard to believe.\n### Section 2\n- Tied SAEs do not usually have an encoder bias, as setting a non-zero encoder bias breaks the tied SAE's ability to accurately recover features. Why was this added? If this is being used, it seems like it should harm the SAE. encoder bias only makes sense in untied SAEs.\n### Section 3\n- L150: \"We construct s-sparse coefficient matrices H, which have uniform feature occurrence frequency\" - does this mean that every input activation has the same number of active features, or just that each feature is sampled independently with a set frequency? Does every feature have the same frequency?\n- Does the SAE have the same width as the number of true features?\n- In section 2 it says that $b_m$ is a trainable parameter, but it seems you're not training it and setting it via a different method?\n- Figure 2, right: The text says optimal recovery occurs when p = f, but this plot seems to show that optimal recovery occurs when p > f. Recovery is very low at f in this plot. \n\n### Section 4\n- Algorithm 1: Line 17 and 18, I think you've mixed up \"min\" and \"max\"\n\n### Section 5\n- L286: I don't understand this line. Up to now your SAEs have all been L1 SAEs with no mention of JumpReLU. What does it mean that \"All methods employ JumpReLU activation\"? How are you training the JumpReLU SAE? There are at least 2 different ways of training these (Deepmind's version and Anthropic's version). Is this a tied SAE? Why are you not comparing to a JumpReLU SAE if you're using JumpReLU SAEs for your method?\n- In the appendix, you say: \"JumpReLU decouples the neuron output magnitude from its bias\". Are you controlling the *JumpReLU bias* with your method or the *encoder bias*? These are different things.\n- Figure 4: Are all the SAEs you're comparing with tied SAEs? I'm confused if you're using tied SAEs at all, despite section 2 claiming you are using tied SAEs. \n- Figure 4: You using JumpReLU, a state-of-the-art method for your own technique, but comparing against non-SOTA architectures (ReLU, TopK). This is not a valid comparison.\n- Table 2: The L0 for these SAEs is unreasonably high. I have never seen anyone go above L0 of 400 for an SAE on Gemma-2-2b, yet these SAEs are all L0 600+, with your method having the highest L0. These experiments should be conducted at a reasonable L0, ideally multiple L0s.\n- Table 2: SAEBench can be very noisy, so you should run multiple seeds and report mean and stdev. The results as-is do not mean much.\n\n### Minor / formatting\n- The use of the word \"neuron\" for the hidden activation of the SAE is confusing. I initially thought this was referring to model neurons. SAE hidden neurons are typically referred to as \"features\" or \"latents\" in the literature.\n- L92: It looks like this is missing subtracting the decoder bias, based on your definition of an SAE in L132"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "emXZIvCPUG", "forum": "VtWkPIbAQ8", "replyto": "VtWkPIbAQ8", "signatures": ["ICLR.cc/2026/Conference/Submission20288/Reviewer_VpeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20288/Reviewer_VpeD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771478537, "cdate": 1761771478537, "tmdate": 1762933760574, "mdate": 1762933760574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies when SAEs can recover interpretable monosemantic features from LLM activations and introduces a frequency-aware training scheme. The authors model activations as sparse nonnegative mixtures of latent features X = H V, then analyze when features are identifiable and recoverable under this statistical framework. They observe a “neuron resonance” effect: neurons reliably learn a feature when their activation frequency p lies in a band around that feature’s occurrence frequency f. Building on this, they propose Group Bias Adaptation (GBA), which partitions SAE neurons into groups with target activation frequencies and adapts biases to match those targets. Theoretically, they prove recovery guarantees for a simplified single-group Bias Adaptation variant under decomposable data and Gaussian feature assumptions, and argue the conditions extend group-wise to GBA. Empirically, on Qwen2.5-1.5B and Gemma2-2B layers, GBA matches TopK on the reconstruction–sparsity frontier and improves cross-seed consistency over TopK, while keeping a high alive-neuron fraction and competitive SAE-Bench interpretability metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper gives a formal recovery guarantee for a Bias Adaptation variant and motivates Group Bias Adaptation from the “neuron resonance” effect, then connects the two cleanly with Algorithm 1 and the resonance-to-algorithm narrative.\n- Consistency is reported over six seeds with explicit MCS definitions and tables. GBA exceeds TopK across selection schemes, and the results seem robust.\n- SAE-Bench metrics on Gemma2-2B show GBA competitive or best on multiple axes while keeping a very high alive-neuron fraction."}, "weaknesses": {"value": "- The formal guarantees assume a Gaussian feature dictionary V; the paper notes this is for convenience and claims methods work when V is non-Gaussian, but it does not provide a systematic analysis of how violations affect results. This remains under-discussed.\n- Hyperparameter choices for HTF/LTF are supported by theory and ablations, but there is no measurement or estimation of the actual feature-frequency distribution in a target LLM to motivate ranges."}, "questions": {"value": "1. How sensitive are your guarantees and empirical results to violations of the Gaussian and incoherence assumptions on V? \n2. Can you provide an empirical estimate or proxy for the distribution of feature occurrence frequencies in the target models, and explain how this supports your HTF/LTF choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ls2Q7kxqUb", "forum": "VtWkPIbAQ8", "replyto": "VtWkPIbAQ8", "signatures": ["ICLR.cc/2026/Conference/Submission20288/Reviewer_73R2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20288/Reviewer_73R2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900937244, "cdate": 1761900937244, "tmdate": 1762933759929, "mdate": 1762933759929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an alternative SAE training approach designed around adapting the bias of subset of neurons / SAE Latents to create latents which track features of varying frequencies. This approach is motivated by an observed phenomena called \"neuron resonance\" - neurons learn monosemantic features when their frequencies are similar. They benchmark their method against other SAEs using SAE bench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clarity: The paper is mostly well written and has useful explanatory diagrams. The key points / insights and core questions are made clear to the reader. \n- Originality: Directly attempting to mediate SAE latent activation frequency is an interesting proposal. \n- Significance: Training GBA SAEs on language models and not just synthetic examples makes the results much more interesting / potentially relevant to use in the real world."}, "weaknesses": {"value": "- It is trivially true that if a latent is going to track a feature, it needs to fire at a similar frequency. Other SAE approaches hope that sparsity inducing loss will match features to their underlying frequencies indirectly which has the benefit of not requiring that you set target activation frequencies. How should we decide what these target activation frequencies should be? Ideally, SAEs are as unsupervised as possible and this approach risks creating challenging hyper-parameter tuning.\n- The paper doesn't compare there SAEs to Matrioshka SAEs (\"Learning Multi-Level Features with Matryoshka Sparse Autoencoders\") which are indirectly related by virtue of learning higher and lower level features which may relate to more or less frequent features. More generally, the L0 of SAEs in Table 2 is fairly high. Ideally, use of SAE bench should show results for a range of sparsity levels. If the point of these SAEs is to avoid using multiple SAEs with different sparsity levels (and often capture features of different frequencies), this should be addressed more directly."}, "questions": {"value": "- SAE quality is often measured by inspection of SAE latent dashboard - max activating examples, activation frequencies etc. The absence of feature dashboards is conspicuous -> can you please share feature dashboards? Ideally, comparing features with those found by other architectures and looking for signs that wSAEs are providing a more useful lens on the analysis of model activations. It seems plausible that they might."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oFvsUBPQdE", "forum": "VtWkPIbAQ8", "replyto": "VtWkPIbAQ8", "signatures": ["ICLR.cc/2026/Conference/Submission20288/Reviewer_nKpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20288/Reviewer_nKpu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995355523, "cdate": 1761995355523, "tmdate": 1762933759583, "mdate": 1762933759583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to provide theoretical grounding for understanding and improving monosemantic feature recovery in Sparse AutoEncoders (SAEs). First, authors show how the phenomenon of \"neuron resonance\" -- i.e., where SAEs best recover features when their feature (neuron) activation frequency falls within a certain \"resonance band\" around the ground-truth monosemantic feature occurrence frequency -- manifests in a toy setting, then theoretically characterize this phenomenon more generally. The primary contribution is a novel training algorithm based on neuron resonance, group bias adaptation (GBA), which provably (under a given statistical model) recovers monosemantic features. Empirically, GBA achieves performance that is competitive with prior SAE baselines across a variety of common evaluations, while substantially improving the consistency of learned features across seeds and hyperparameter settings and reducing the proportion of dead features."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The paper provides the first (to my knowledge) theoretical treatment of feature recovery in SAEs. Given the very limited current theoretical understanding of SAE training, this is an important and timely contribution to the field, which has so far relied overwhelmingly on empirical analysis alone.\n- Experiments across a variety of settings show clear empirical benefits of GBA relative to leading baselines, in addition to their improved theoretical grounding with respect to feature recovery."}, "weaknesses": {"value": "The paper's greatest weakness is poor organization and clarity.\n- **Organization:** The main paper should be relatively self-contained, including all the necessary high-level information to understand key contributions. A huge amount of critical content -- including comparison with related work, proposed evaluation metrics, and key information about experiments that are strictly necessary for understanding empirical results -- is provided only in the appendix. For instance:\n    - Introductions and basic definitions of novel evaluation metrics in appendix C.2 should be moved into corresponding sections where these terms are used (even if some lower-level details remain in appendices).\n    - The related work section (appendix A) *must* appear in the main paper. (This case in particular feels like an instance where authors might have simply moved the entirety of a necessary main-paper section into the appendix in order to save space; but this could easily have been done much more appropriately by compressing the introduction or moving some experiments/ablations to the appendix instead.)\n- **Clarity:** Even after looking through the appendix, I am still unable to find definitions of several important experimental details, new terms, etc. necessary to provide a complete assessment (see the Questions section, below).\n\nPlease note: **I am completely open to raising my rating (and confidence score) if authors are able to provide this missing information** -- given my (currently incomplete) understanding of the contributions, I feel that the paper would very likely merit a higher score once I have the necessary information to make a full assessment."}, "questions": {"value": "**Essential missing information required for full assessment** (as discussed above, I cannot find any definitions of the following in either the main paper or appendix, making it impossible to interpret key results):\n- Missing from section 3 (and appendices):\n    - What are m and $\\mu$ appearing on the y-axis of fig 2?\n- Missing from sec 5 (and appendices):\n    - What are $\\alpha$ and the corresponding \"top $\\alpha$ selection rule\"?\n    - What are Highest Target Frequency (HTF) and Lowest Target Frequency (LTF)? \n\nGeneral questions:\n- In sec 3:\n    - What/where is the supposed \"phase transition\" at $d = \\sqrt{n}$? Wouldn't this correspond to the rightmost column of the right heatmap (where $d \\approx \\sqrt{n} = 256$), which doesn't seem to exhibit a \"phase transition\" with respect to the preceding columns? I would appreciate it if authors could provide: \n        - (a) an explanation of what, precisely, \"phase transition\" is intended to mean here; \n        - (b) a clear criterion for categorizing a given result as showing a phase transition, rather than simply a gradual change in feature recovery around good hyperparameter settings; \n        - (c) an explanation of how neuron resonance is expected to correspond to either a phase transition or a more gradual change (such as mentioned in (b)), and how this relates to the feasible frequency range in sec 6; and \n        - (d) a description of what, specifically, in figure 2 is taken to be evidence of a phase transition. (My understanding is that the \"narrow band\" in the right $d < \\sqrt{n}$ plot is interpreted as the phase transition, where the larger high-FRR region in the left plot is *not* interpreted as such, but rather a more gradual change -- is this correct?)\n    - Sec 3 experiments are based on the setting with uniform feature occurrences, unlike the \"feature spectrum\" (multi-group) settings discussed in later sections. What would the experiments in sec 3 look like given such a feature spectrum (either discretized into groups or varying continuously over a geometric distribution)?\n- In sec 4:\n    - Is there any particular reason for partitioning neuron TAFs based on a geometric distribution rather than, e.g., a [Zipfian distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) (as is often observed in the context of linguistic features)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LGjc9qSJ7n", "forum": "VtWkPIbAQ8", "replyto": "VtWkPIbAQ8", "signatures": ["ICLR.cc/2026/Conference/Submission20288/Reviewer_fF2q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20288/Reviewer_fF2q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762467730916, "cdate": 1762467730916, "tmdate": 1762933758965, "mdate": 1762933758965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}