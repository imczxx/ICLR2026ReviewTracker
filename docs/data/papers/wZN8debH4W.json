{"id": "wZN8debH4W", "number": 16379, "cdate": 1758263937575, "mdate": 1759897244557, "content": {"title": "Planned Diffusion", "abstract": "A central challenge in large language model inference is the trade-off between generation speed and output quality. Autoregressive models produce high-quality text but generate tokens sequentially. Diffusion models can generate tokens in parallel but often need many iterations to match the same quality. We propose planned diffusion, a hybrid method that combines the strengths of both paradigms. Planned diffusion works in two stages: first, the model creates a short autoregressive outline that breaks the output into smaller, independent spans. Second, the model generates these spans simultaneously using diffusion. This approach expands the speed–quality Pareto frontier and provides a practical path to faster, high-quality text generation. On AlpacaEval, a suite of 805 instruction-following prompts, planned diffusion achieves Pareto-optimal trade-off between quality and latency, achieving 1.84x speedup over autoregressive generation with only a 6.8\\% drop in win rate. Our sensitivity analysis confirms that the internal planning of our model is reliable and offers tunable control over the trade-off between generation speed and quality.", "tldr": "Planned diffusion speeds up LLM inference by denoising parallelized spans from a previously generated plan.", "keywords": ["diffusion", "LLM", "parallel generation", "fast inference", "autoregressive", "planning", "hybrid model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d81b6555b4a1351e77183b1a703d940d80691468.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Planned Diffusion, a hybrid decoding framework that first generates a short autoregressive plan (with control tags like <async>…</async>, <sync/>, topic and predicted span length), then diffuses multiple spans in parallel under a bidirectional mask. It integrates KV-cache reuse across stages and uses an energy-ordered unmasking rule for diffusion. On AlpacaEval (805 prompts), it reports 1.27×–1.81× speedup over AR with 0.87%–5.4% LC win-rate drop, and shows ablations on plan components, span-length prediction, and quality–latency knobs (step-ratio r, confidence threshold τ)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "# Strengths\n\n1) Clear plan-then-parallel idea  \n- First produce a short AR plan, then decode multiple spans in parallel with a bidirectional mask.  \n- Turns semantic parallelism into practical decoding parallelism.\n\n2) Simple, single-model pipeline  \n- Causal attention for planning; bidirectional attention within spans during diffusion.  \n- KV-cache reuse and stage transitions are straightforward to add to existing inference stacks.\n\n3) Transparent quality–latency control  \n- Tunable knobs (e.g., diffusion-step ratio *r*, confidence threshold *τ*) expose a smooth speed–quality trade-off.  \n- Pareto curves make the effect of these knobs easy to interpret.\n\n4) Useful diagnostics  \n- Component-wise ablations (control tags, span-length prediction, unmasking rule) clarify each module’s role.  \n- The explicit plan → execute interface helps surface failure cases.\n\n5) Reasonable reproducibility details  \n- Checkpoints, fine-tuning settings, and prompt snippets are documented sufficiently for re-implementation."}, "weaknesses": {"value": "# Weaknesses\n\n1) Modest speedup and weaker quality\n- End-to-end acceleration is limited (≈ **1.27×**).\n- Reported quality can trail standard AR decoding (**49.2 vs. 50**).\n\n2) Reliance on prompt-model data construction\n- Plan annotations depend on a separate prompt model, introducing data-generation overhead and potential bias.\n- The dependence raises questions about scalability of the approach as model/data sizes grow.\n\n3) Narrow experimental scope\n- Evaluation is conducted on a **single benchmark**.\n- Lacks comparisons against a broader set of **model baselines** and decoding accelerators.\n\n4) Planning limitations on complex tasks\n- For tasks with strong cross-span dependencies, plan segmentation and span sizing are uncertain.\n- Observed speedup may be confounded by **implicit output-length constraints** rather than true parallelism."}, "questions": {"value": "ref weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kfhU7n99xi", "forum": "wZN8debH4W", "replyto": "wZN8debH4W", "signatures": ["ICLR.cc/2026/Conference/Submission16379/Reviewer_Yrr4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16379/Reviewer_Yrr4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761197468211, "cdate": 1761197468211, "tmdate": 1762926501760, "mdate": 1762926501760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces planned diffusion, a hybrid text generation method combining autoregressive planning with diffusion-based parallel execution. The model first generates a structured plan with control tags defining independent text spans, then generates these spans simultaneously via discrete diffusion. Evaluated on AlpacaEval, the method achieves 1.84× speedup over autoregressive generation with a 6.8% drop in win rate, establishing a new point on the latency-quality Pareto frontier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First text-only model combining discrete diffusion with autoregression in a unified architecture, addressing the speed-quality tradeoff from a novel angle\n2. Hybrid attention masking elegantly enables both causal and bidirectional attention; KV caching strategy is well-designed for this architecture\n3. Establishes new Pareto frontier point; sensitivity analysis confirms model learns accurate length prediction without systematic bias\n4. Method is orthogonal to other acceleration techniques and continues improving with more training data"}, "weaknesses": {"value": "1. Only AlpacaEval benchmark; no evaluation on diverse tasks (summarization, QA, code generation, creative writing). How does performance vary across task types?\n2. No direct comparison to other semantic parallelism methods (e.g., Skeleton-of-Thought, APAR, ParaThinker) despite extensive related work discussion. This is critical for establishing true contribution.\n3. Relies on Gemini for training data annotation. What is annotation quality? How many examples were rejected? Could this be learned end-to-end without synthetic supervision?"}, "questions": {"value": "1. How does planned diffusion compare quantitatively to other semantic parallelism methods?\n\n2. How does performance vary across task types beyond instruction-following (e.g., summarization, code generation, creative writing)?\n\n3. What is the speedup variance across examples? Are there cases where planning overhead makes it slower than baseline?\n\n4. What percentage of generations have poor plans? Can you provide failure case examples and error analysis?\n\n5. What content types decompose well vs. poorly? Does the method struggle with sequential reasoning or narratives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SybpOiYc3f", "forum": "wZN8debH4W", "replyto": "wZN8debH4W", "signatures": ["ICLR.cc/2026/Conference/Submission16379/Reviewer_Zq7e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16379/Reviewer_Zq7e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782864026, "cdate": 1761782864026, "tmdate": 1762926501152, "mdate": 1762926501152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Planned Diffusion, a hybrid text generation approach that first plans a response autoregressively (producing structure/length tags) and then fills multiple spans in parallel via discrete diffusion, aiming to shift the latency–quality Pareto frontier. On AlpacaEval, the method reportedly delivers ~1.8× speedup with a modest quality drop versus autoregressive decoding, and includes sensitivity analyses on span-length scaling and denoising step ratio."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Clear, appealing idea: formal two-stage factorization (planning then parallel diffusion), with an explicit algorithm and attention-masking design. \n- Well-specified control language (<topic>, <async>, <sync/>) that makes semantic parallelism concrete and implementable.\n- Empirical evidence of a new speed/quality trade-off vs. AR and diffusion baselines (latency–quality plots, critical-path analysis, scaling behavior).\n- Sensitivity analyses help demystify behavior: best performance when using the model’s predicted span lengths (scale=1.0) and a tunable quality–latency knob via step ratio."}, "weaknesses": {"value": "- Benchmark scope: Results focus on AlpacaEval with an LLM-as-judge (LCWR). This is a useful proxy but not a robust test of coherence/faithfulness across diverse tasks (e.g., reasoning, long-form, safety). Lack of human evals or broader benchmarks (e.g., MT-Bench, GSM-8K reasoning slices, instruction-following suites) weakens generality.\n- Baselines & fairness details: Diffusion is configured with steps equal to token count, and fast-dLLM with specific hyperparameters; however, broader ablations (other drafting/verification, semi-AR methods, SoTA speculative decoding stacks) are limited. This makes it harder to judge the absolute Pareto gains.\n- Robustness & failure modes: The method assumes reliable plan quality (topic labels, span counts). What happens when planning is wrong (e.g., underestimates length; cross-span dependencies appear late)? The sensitivity section is a good start, but a qualitative error analysis is missing.\n- Training data annotation relies on a proprietary LLM (Gemini) to insert tags under constraints. Potential concerns: annotation consistency, domain shift to noisier instructions, and whether the model overfits the tag scheme rather than learning general “semantic parallelism.” More diagnostics would help."}, "questions": {"value": "- Plan robustness: How often does the planner significantly under/over-estimate span length in the wild? Could a lightweight “repair” step (e.g., local AR patching) recover quality when spans are misplanned?\n\n- Generalization: Have you tried the approach on models with different pretraining (e.g., pure AR LLMs plus diffusion fine-tune) or at different scales?\n\n- Error modes: Is there any qualitative examples where planned diffusion fails (e.g., subtle cross-span dependencies) and discuss mitigation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2XvxJ1a33", "forum": "wZN8debH4W", "replyto": "wZN8debH4W", "signatures": ["ICLR.cc/2026/Conference/Submission16379/Reviewer_UEZx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16379/Reviewer_UEZx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956430308, "cdate": 1761956430308, "tmdate": 1762926500806, "mdate": 1762926500806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* Problem: Autoregressive models produce high-quality text but are slow because they generate tokens sequentially, while diffusion models generate in parallel but often require many iterative steps to achieve similar quality.\n* Solution: A new hybrid architecture, termed planned diffusion, that leverages (1) an autoregressive “planning” stage that decomposes the output into semantically independent spans, and (2) a parallel diffusion “execution” stage that denoises these spans simultaneously.\n* Evaluation: Experiments and analysis including demonstrating that the method achieves a 1.84x speedup over autoregressive generation with a 6.8% drop in win rate on AlpacaEval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated: very relevant problem and one that addresses a key weakness in diffusion language models\n2. Novelty: combining an autoregressive planning stage with a diffusion-based parallel generation stage within a single unified model.\n3. Implementation: Proposes reasonable set of methods that includes a new control tag language, model training methodology, and inference algorithm that enable planned diffusion and navigation of a Pareto frontier between speed and performance."}, "weaknesses": {"value": "1. Evaluation Scope: Evaluation is only on AlpacaEval and lacks any other benchmarks, tasks, or domains. \n2. Baselines: There is only one baseline that is not the vanilla baselines of autoregressive models and diffusion LLMs. \n3. Complexity: Quite a lot of complexity without full ablation to justify each design choice\n4. Trade-off: A performance loss of 6.8% is still pretty substantial and it is not clear how much speed-up one could get with say a smaller model or speculative decoding."}, "questions": {"value": "1. The improvement in speed comes at a cost which is performance. Performance is a much harder thing to raise, so it is hard to understand exactly how much is being sacrificed for the speedup. Is there a way to quantify the speedup with the performance kept constant or to show the performance with the same speed?\n2. The following is mentioned: \"To the best of our knowledge, this is the first text-only model that uses both discrete diffusion and autoregression.\" Does the emphasis of \"text-only\" mean that there are other models in different modalities that use both discrete diffusion and autoregression?\n3. How does the model determine the optimal number and boundaries of spans in the autoregressive plan, and how sensitive are results to this segmentation?\n4. Could you clarify how the computational cost of the diffusion stage scales with the number of spans and denoising steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lk9gvh1A0g", "forum": "wZN8debH4W", "replyto": "wZN8debH4W", "signatures": ["ICLR.cc/2026/Conference/Submission16379/Reviewer_p8N9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16379/Reviewer_p8N9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158390998, "cdate": 1762158390998, "tmdate": 1762926500304, "mdate": 1762926500304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}