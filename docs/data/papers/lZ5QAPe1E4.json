{"id": "lZ5QAPe1E4", "number": 11567, "cdate": 1758201781069, "mdate": 1763736893874, "content": {"title": "Creat3r: Confidence Reaggregation for Exploration-aware Active 3D Reconstruction", "abstract": "We introduce Creat3r, an active view selection framework designed for efficient and high-quality 3D reconstruction using a limited subset of image-pose pairs. Given an initial set of selected views, our method iteratively identifies the most informative candidate views to maximize reconstruction accuracy while adhering to computational constraints. Our approach begins by generating an intermediate 3D point cloud through dense pixel correspondences and stereo triangulation, refining point estimates via the Direct Linear Transform (DLT). To assess reconstruction reliability, we introduce a 3D confidence field that integrates camera support and view consistency, enabling a quantitative evaluation of point quality. This confidence information is then propagated to all candidate views using an efficient Gaussian projection technique, generating 2D confidence and exploration maps for each potential viewpoint. We define an exploration measure based on these maps to evaluate and optimally select the next best view. By balancing exploration, reconstruction accuracy, and computational efficiency, Creat3r is well-suited for applications in autonomous 3D scanning, robotic vision, and multi-view scene reconstruction. To demonstrate its effectiveness, our method is evaluated against baselines using the standard 3DGS representation for 3D reconstruction from the selected views. The experimental results show that our method excels in novel view synthesis and surface reconstruction, achieving significant improvements in SSIM and F1-score.", "tldr": "Creat3r builds a robust geometric scaffold on the fly and uses novel \"exploration\" and \"confidence\" maps to guide view selection, achieving state-of-the-art results with significantly less data and computation.", "keywords": ["Novel View Synthesis", "Surface Reconstruction", "Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10baf9881b7668a299a3fe2e4ef560daa4247670.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a viewpoint selection criterion based on correspondence and visibility for the task of active viewpoint selection in 3DGS. This method does not rely on camera pose for viewpoint selection, relying solely on image information. The authors claim that this method can avoid information leakage issues."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes an active view selection method for efficient 3DGS training. The key difference to previous approaches is it requires no camera pose for selection. The proposed method uses on-the-shelf cross-view correspondence to evaluate the view relationships and perform view selection under the budget limit."}, "weaknesses": {"value": "1. The wording of the paper is imprecise. The method name ‘Creat3r’ is not aligned with its handling task. 3R models are for feed-forward 3D reconstructions, while this method is for active view selection. The definition of ‘confidence’ is also different from 3R models.\n2. The camera poses of candidate views are freely available information. This is because reconstructing the poses of the selected sparse view effectively reconstructs the poses of the candidate views simultaneously. To verify information leakage, the authors should only recover the poses from the selected sparse view and train 3DGS.\n3. The NVS precision of comparative methods are questionable. The precision of original FisherRF before adapted to 3DGS should be reported. The precision of Lyu et al. is significantly lower than reported."}, "questions": {"value": "No more questions, see weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P4MBiucK8u", "forum": "lZ5QAPe1E4", "replyto": "lZ5QAPe1E4", "signatures": ["ICLR.cc/2026/Conference/Submission11567/Reviewer_KrRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11567/Reviewer_KrRH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784639358, "cdate": 1761784639358, "tmdate": 1762922654061, "mdate": 1762922654061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “CREAT3R: Confidence Reaggregation for Exploration-Aware Active 3D Reconstruction” presents a novel active view selection framework aimed at improving data efficiency and reconstruction quality in 3D Gaussian Splatting (3DGS). The authors propose CREAT3R, which eliminates the dependency on iterative 3DGS optimization and instead builds a lightweight, geometry-based proxy model for view selection. The method introduces two key components: a 3D confidence field that quantifies reconstruction reliability based on camera support and color consistency, and a 2D exploration map that highlights unobserved or weakly constrained regions. These maps are combined into an exploration–exploitation measure to identify the next best view. Through robust 3D point triangulation using correspondence networks (e.g., LightGlue, MASt3R) and Direct Linear Transform (DLT). Experiments on Mip-NeRF 360 and Tanks & Temples benchmarks show that CREAT3R achieves state-of-the-art performance in both novel view synthesis and surface reconstruction, significantly outperforming FisherRF, Lyu et al., and Kopanas & Drettakis in SSIM and F1-score while reducing computation time by more than half. The paper’s contributions include a decoupled active view selection mechanism, a robust confidence reaggregation strategy, and empirical validation of superior data efficiency for 3DGS-based reconstruction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The paper introduces a genuinely novel active view selection framework that departs from conventional uncertainty-based or optimization-coupled approaches. By decoupling the selection process from 3DGS optimization and introducing the concept of confidence reaggregation, the authors provide a fresh and elegant geometric formulation.\n\nClarity: The paper is clearly written, well-structured, and easy to follow. The authors carefully motivate their design choices and provide detailed algorithmic explanations with equations and visualizations that effectively illustrate how confidence and exploration maps interact. The methodology section is self-contained and reproducible, with sufficient implementation details and ablation studies that make the contributions transparent and verifiable.\n\nSignificance: The proposed framework achieves notable improvements over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, F1-score) and computational efficiency. By substantially reducing optimization overhead while maintaining or improving reconstruction fidelity, the method advances the practicality of active view selection for real-world 3D reconstruction and robotic vision tasks. The combination of robust geometry, efficiency, and general applicability makes this work a significant step forward for the field of active 3D perception."}, "weaknesses": {"value": "Lack of analysis on limitations and failure modes: The paper does not explicitly discuss cases where CREAT3R might underperform, such as scenes with repetitive textures, highly reflective surfaces, or extreme lighting variations. An analysis of such failure cases would provide valuable insights into the robustness and generalizability of the approach, as well as guidance for future improvements or hybrid strategies.\n\nInsufficient empirical evaluation of efficiency claims: Although the method claims improved computational efficiency and reduced optimization time, the experimental section lacks a detailed quantitative analysis of runtime, memory consumption, or scalability compared to existing methods. Reporting metrics such as per-iteration runtime, GPU memory usage, and end-to-end reconstruction time would substantiate the paper’s efficiency claims and better demonstrate its practical advantages for real-world deployment."}, "questions": {"value": "Pose acquisition without SfM: The paper emphasizes that CREAT3R avoids Structure-from-Motion (SfM) to prevent information leakage from unseen views, yet it still requires known camera poses for all candidate images. Could the authors clarify how these poses are obtained in practice without relying on global SfM or similar reconstruction pipelines? Are the poses assumed to come from external sensors (e.g., IMU, SLAM, or GPS-based localization)? If so, how sensitive is CREAT3R to pose noise, and have the authors evaluated the robustness of the framework under pose perturbations or inaccuracies?\n\n\nInitialization strategy for known views: The paper mentions that the process begins with “a small set of known views,” but the method for selecting or initializing these views is not clearly described. Are the initial views chosen randomly, uniformly distributed in pose space, or based on heuristic criteria such as viewpoint diversity or coverage? Given that initialization can significantly influence subsequent selection quality, providing either an ablation or a justification for the initialization strategy would help clarify the reproducibility and stability of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8gSnRa9DfY", "forum": "lZ5QAPe1E4", "replyto": "lZ5QAPe1E4", "signatures": ["ICLR.cc/2026/Conference/Submission11567/Reviewer_LSqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11567/Reviewer_LSqm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807818207, "cdate": 1761807818207, "tmdate": 1762922653526, "mdate": 1762922653526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Creat3r, an active view selection framework for 3D reconstruction. Starting from a set of initial views, Creat3r leverages an off-the-shelf correspondence network to extract matched keypoints from image pairs. These matches are then used to triangulate 3D points. Given the reconstructed 3D points, 2D confidence and exploration maps are built by reprojecting the 3D points onto candidate views and checking for in-frustum and color consistency. Finally, an exploration measure is designed to select the next best view.\n\nThe proposed method eliminates the need for repeatedly running 3D Gaussian Splatting. Experimental results show that it outperforms existing baselines in both efficiency and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method avoids repetitive Gaussian splatting initialization and reconstruction, achieving higher efficiency than previous approaches.\n2. The experiments demonstrate that the proposed method is effective for both novel-view synthesis and surface reconstruction."}, "weaknesses": {"value": "1. In L72–76, the authors claim that previous methods use SfM point clouds from all candidate views and therefore suffer from information leakage. However, projecting spherical Gaussians onto candidate views still requires their camera poses. How are these camera poses obtained if the SfM poses are not known beforehand? The proposed method may still suffer from information leakage.\n2. Given the camera poses, why not directly perform multi-view depth estimation (e.g., with MASt3R)? The resulting 3D points would likely be more complete and provide a better initialization."}, "questions": {"value": "1. How is r chosen in Equation (7)? Also, it seems that in L246, λ is not defined.\n2. In L360–362, regarding subsampling initialization, why not simply use a subset of all views to run SfM and use the resulting points for initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N9df6TwHc5", "forum": "lZ5QAPe1E4", "replyto": "lZ5QAPe1E4", "signatures": ["ICLR.cc/2026/Conference/Submission11567/Reviewer_5zWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11567/Reviewer_5zWZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883445461, "cdate": 1761883445461, "tmdate": 1762922652861, "mdate": 1762922652861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an active view selection method, Creat3r, to iteratively select the next most informative candidate views until reach the constraints on the number of views. A 3D point cloud is first constructed using dense pixel correspondences and triangulation with Direct Linear Transform (DLT). Then a 3D confidence field is introduced using camera visibility and view consistency. Using a Gaussian projection technique, 2D confidence map and 2D exploration are obtained and leveraged to select the next best view."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic of active view selection is interesting and important to reduce redundant views and select most informative views.\n2. The proposed method Creat3r outperforms other view selection methods based on the quantitative and qualitative results.\n3. Creat3r doesn't depend on 3DGS optimization, therefore is faster than other optimization-based methods."}, "weaknesses": {"value": "1. A limitation section or cases where Creat3r is not optimal should be discussed and added.\n2. A qualitative figure of ablation study is preferred to understand the effectiveness of proposed method better.\n3. For novel view synthesis, when all methods are using the same 3DGS framework and initialization points, the main difference would be the views that are selected from my perspective, or other components also contribute to construction quality. It would be good to visualize the selected views comparison to draw more insights."}, "questions": {"value": "1. In Section 3.2, how does Creat3r handle occlusion as the occluded pixels would introduce photometric differences?\n2. If the proposed method is 3D model agnostic, should the experiments being done with 2DGS in surface reconstruction as 3DGS is not designed for this task?\n3. Around line 428, if Kopanas & Drettakis (2023) has higher precision and lower recall, it should have fewer false positives and more false negatives."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MoBgMGoyV0", "forum": "lZ5QAPe1E4", "replyto": "lZ5QAPe1E4", "signatures": ["ICLR.cc/2026/Conference/Submission11567/Reviewer_CpM6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11567/Reviewer_CpM6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954052724, "cdate": 1761954052724, "tmdate": 1762922652391, "mdate": 1762922652391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revisions and Responses"}, "comment": {"value": "We sincerely thank the reviewers for their constructive feedback and careful reviews. We summarize our responses to all reviewers and outline the corresponding updates made to the revised manuscript.\n\n&nbsp;\n### Issues of Interest to Multiple Reviewers\n\n* **Problem Setting:** In the active view selection setting, the camera poses for a candidate pool are defined *a priori*, while the corresponding image content remains inaccessible until a specific pose is selected. Once a pose is chosen, the image is then captured and exposed to the view-selection algorithm.\n\n* **Information Leakage:** The benchmark established by Lyu et al. introduces a methodological flaw by utilizing a sparse point cloud initialized via global Structure-from-Motion (SfM). This pre-processing step incorporates image data from all candidate poses (which are intended to be unseen), thereby violating the fundamental constraints of active vision. We refine the benchmark by replacing the global SfM point cloud with a rigorous, leakage-free initialization protocol.\n\n* **Limitations:** We have added a dedicated *Limitations and Discussion* section (Section 5) in the revision. We explicitly discuss outward-facing scenarios, where candidate views often share minimal overlap with the initially estimated geometry, presenting a distinct challenge.\n\n&nbsp;\n### Revisions Incorporated into the Manuscript\n\n* **Qualitative Comparison of Ablation Study:** We have included a qualitative comparison in Figure 6 of the revised manuscript, demonstrating the distinct functionalities of the ablated components.\n\n* **Visualization of Active View Selection Sequences:** We visualize the selected sequences in Figure 4 to illustrate the selection preferences of different criteria.\n\n* **Surface Reconstruction Evaluation:** The discussion regarding the precision-recall trade-off has been revised in Lines 459-462.\n\n* **Determination of Influence Region $r$:** The formulation of the influence region $r$ and its projection $r^{2D}$ has been revised in Lines 247-251.\n\n&nbsp;\n### Additional Points\n\n* **Occlusion Handling:** The confidence scoring mechanism quantifies viewing consistency using the metric $H(p)$. When a scene point is subjected to occlusion across certain perspectives, the resulting decrease in $H(p)$ proportionally diminishes the overall confidence score assigned to that occluded region.\n\n* **Choice of 3D Models:** Creat3r is agnostic to the underlying 3D representation. We specifically utilized 3DGS in this work to ensure a fair and direct comparison with the baselines.\n\n* **Comparison with MASt3R Prediction:** We have included a comparison between our robust point estimation and direct MASt3R prediction in the ablation study, as shown in the first row of Table 4.\n\n* **Evaluation of Efficiency:** We have included a detailed per-iteration runtime comparison in Table 5; Creat3r requires only 10 seconds per iteration.\n\n* **Initialization Strategy:** The initialization strategy is detailed in Appendix A.1. We adhere to the specific set of three initial views defined in the ReconFusion benchmark.\n\n* **Clarification on the Naming of \"Creat3r\":** We respectfully clarify that the term \"3R\" (3D Reconstruction) is a general descriptor and is not strictly constrained to feed-forward methods in the literature. For instance, the work \"3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS\" proposes an optimization-based (rather than feed-forward) method for camera pose estimation.\n\n* **Performance Gap with Lyu et al.'s Benchmark:** The results reported in Lyu et al.'s benchmark inadvertently utilized a global SfM point cloud derived from all images (including the supposedly \"unseen\" views). Our reported results differ because we implement a strictly leakage-free protocol."}}, "id": "7QTOXIX00m", "forum": "lZ5QAPe1E4", "replyto": "lZ5QAPe1E4", "signatures": ["ICLR.cc/2026/Conference/Submission11567/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11567/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission11567/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727699368, "cdate": 1763727699368, "tmdate": 1763736981117, "mdate": 1763736981117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}