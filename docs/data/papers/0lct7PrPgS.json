{"id": "0lct7PrPgS", "number": 1179, "cdate": 1756860486655, "mdate": 1759898223287, "content": {"title": "Learning linear state-space models with sparse system matrices", "abstract": "Due to tractable analysis and control,  linear state-space  models (LSSMs) provide a fundamental mathematical tool for time-series data modeling in various disciplines. In particular, many LSSMs have sparse system matrices because interactions among variables are limited or only a few significant relationships exist.  However, current learning algorithms for LSSMs lack the ability to learn system matrices with the sparsity constraint due to the similarity transformation. To address this issue, we impose sparsity-promoting priors on system matrices to balance modeling error and  model complexity. By taking hidden states of LSSMs as latent variables, we then explore the expectation--maximization (EM) algorithm to derive a maximum a posteriori (MAP) estimate of both hidden states and system matrices from noisy observations. Based on the Global Convergence Theorem, we further demonstrate that the proposed learning algorithm yields a sequence  converging to a local maximum or saddle point of the joint posterior distribution. Finally, experimental results on simulation and real-world problems illustrate that the proposed algorithm can preserve the inherent topological structure among variables and significantly improve prediction accuracy over classical learning algorithms.", "tldr": "", "keywords": ["linear state-space models", "expectation-maximization algorithm", "system identification", "state estimation"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/933f8c35e1fb6d506f66902169ec076c4273d8f1.pdf", "supplementary_material": "/attachment/a5b9f619c880858429064f49d0d3ccc655e0521b.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of learning linear state-space models (LSSMs) with sparse system matrices, a setting relevant to many real-world dynamical systems where interactions are limited. The authors propose a maximum a posteriori (MAP)–EM algorithm that introduces Student-t sparsity-promoting priors on the system matrices and derives closed-form row-wise updates for all parameters. The algorithm is theoretically supported by a convergence guarantee in the EM sense (Theorem 3.3) and empirically evaluated on both synthetic and industrial datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated problem.**\nLearning LSSMs with sparse system matrices is both practically and theoretically relevant, as sparsity naturally arises in many real systems with limited or localized interactions.\n\n2. **Clear algorithm design and convergence guarantee.**\nThe proposed MAP–EM algorithm is clearly described, with detailed derivations and an explicit convergence guarantee (Theorem 3.3), which adds credibility to the approach."}, "weaknesses": {"value": "1. **Lack of a formal sparsity definition.**\n “Sparse system matrices” are introduced implicitly through element-wise priors rather than through a clear structural definition or quantitative metric for topology recovery. As a result, the structural claims rely mainly on visual inspection rather than measurable topology-level metrics.\n\n2. **Limited theoretical support for the “generalized permutation” claim.** The conclusion that the admissible similarity transformation $\\Phi$ reduces to a generalized permutation matrix is demonstrated only through an example, without a general proof or stated conditions under which this holds.\n\n3. **Potential unfairness in sparsity comparisons.** The authors apply post-hoc thresholding (“parameters below a predefined threshold are truncated to zero”) to their own results to emphasize sparsity, but it is unclear whether the same was done for the MLE baseline. Given that in Figures 1 and 2 the MLE method already yields a structure and MRE very close to the proposed approach, applying the same thresholding to MLE might lead to similarly sparse and accurate results, thus weakening the claimed structural advantage."}, "questions": {"value": "1. **Thresholding fairness:** If the same post-hoc thresholding used in your method is applied to the MLE estimates, how do the resulting structure plots and MRE values change? Please report results for “MLE + threshold” using the identical threshold schedule.\n\n2. **Threshold sensitivity:** How sensitive are the topology recovery and MRE results to the chosen truncation threshold? How is this threshold selected in practice?\n\n3. **Non-invertible systems:** The examples presented involve invertible $A$ matrices. Can your proposed algorithm be applied to systems where $A$ is singular (non-invertible)? If yes, please provide an experiment to illustrate this case.\n\n4. **Hyperparameter robustness:** How sensitive are your results to the Student-t hyperparameters ($a_0, b_0$) and to the initialization of $\\Gamma$? Do you have recommended defaults or practical guidance for choosing these values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wL1Lhmu8yq", "forum": "0lct7PrPgS", "replyto": "0lct7PrPgS", "signatures": ["ICLR.cc/2026/Conference/Submission1179/Reviewer_jpHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1179/Reviewer_jpHJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761202101953, "cdate": 1761202101953, "tmdate": 1762915699188, "mdate": 1762915699188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to learn sparse linear state-space models. This is achieved by a hierarchical Student's t-distribution prior on system matrices which results in sparsity bias. The system matrices, hyperprior parameters and noises are then iteratively optimized via an expectation-maximization procedure. For each minimization step, the authors derive closed-form update rules based on block-coordinate descent on each parameter. They validate their results on some synthetic and real-world systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper seems to be a natural extension of prior work on sparse bayesian learning (e.g., of the ideas in Wang et al., 2024). I am not sure how original the technical ideas are but the paper is well-executed and derives the necessary results in a more complicated setting.\n\nThe technical results are complete in the sense that there seems to be nothing left on the table given the introduced setting. Additional improvements can only be made by imposing other constraints or relaxed versions of the algorithm for computational efficiencies which I find out of the scope. Therefore, I consider the theoretical results complete and good quality. The ideas and the results are presented very clearly."}, "weaknesses": {"value": "1. The results are promising empirically but more can be done if the authors think the algorithm presented is practical. For example, they can include runtime comparisons.\n2. I think the paper puts the right results together from previous literature and has rather minimal technical contributions."}, "questions": {"value": "1. Can the authors discuss technical novelties (if there is any) compared to the related work?\n2. Can you discuss the computational requirements a bit more in depth compared to other algorithms? Isn't it normal that each iteration takes $T$ time as that's the size of the input. Why is it more computationally expensive? Is it the inverse operations in the closed-form updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O97Uz24Oek", "forum": "0lct7PrPgS", "replyto": "0lct7PrPgS", "signatures": ["ICLR.cc/2026/Conference/Submission1179/Reviewer_ggGN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1179/Reviewer_ggGN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761402527281, "cdate": 1761402527281, "tmdate": 1762915699068, "mdate": 1762915699068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new EM algorithm for learning linear state-space models (LSSMs) with explicitly sparse system matrices, a critical aspect for interpretability and efficiency in various scientific domains. The authors impose sparsity-promoting Student’s t-distribution priors on system matrices and develop a principled MAP estimation framework. The resulting EM procedure leverages the Rauch-Tung-Striebel smoother in the E-step and block coordinate descent in the M-step, with global convergence to local MAP optima proven via the Global Convergence Theorem. The paper claims its approach avoids the notorious similarity transformation pitfalls of classical LSSM learning algorithms, thus better recovering the true sparse topology. Experiments on synthetic and several real-world benchmark datasets show improvements in prediction accuracy and topological interpretability compared to established baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates methodological rigor through a principled MAP-EM formulation for LSSMs under sparsity priors, with complete derivations, update steps, and connections to established statistical techniques. Its theoretical soundness is reinforced by a formally proven convergence guarantee via the Global Convergence Theorem. The work explicitly addresses the similarity transformation issue, ensuring identifiability and interpretability by restricting transformations to generalized permutation matrices. The experiments on synthetic and real-world industrial datasets validate the method’s effectiveness, with figures and tables clearly illustrating preservation of topological structures and improved quantitative performance. The paper’s mathematical transparency and detailed derivations enhance reproducibility and clarity."}, "weaknesses": {"value": "- The algorithm's reliance on full EM with smoothing and coordinate updates (Algorithm 1) and complexity discussed in Section 6, makes it computationally intensive. The paper only briefly mentions the limitation with respect to large-scale settings, without offering profiling, timing, or strategies for acceleration (e.g., stochastic EM, parallelization).\n\n- As stated in Section 5.1 and experimental appendices, sparsity is only enforced \"up to a predefined threshold\" with truncation to accelerate convergence. While this is common in Bayesian learning, an explicit analysis of how the choice of this threshold affects success rates, stability, and meaningful recovery of structures is not present, nor is there a discussion of methods that enforce exact zeros by construction.\n\n- The argument that sparsity-promoting priors restrict similarity transformations to generalized permutation matrices is supported only for very specific classes of topologies; most of the argument is illustrated rather than proven generally. Real-world systems with denser interconnections may still pose identifiability challenges; this should be discussed further.\n\n- While the paper is mathematically rigorous, much of the machinery (Student’s $t$ prior for sparsity, hierarchical Bayesian treatment, EM updates, block coordinate descent, even the use of convergence theorems) is largely a synthesis/extension of established sparse Bayesian learning and system identification literature. The main differentiating insight (identifiability under sparsity priors limiting transformations to permutation/scaling) though interesting, is not deeply explored beyond special-case analysis.\n\n- Although synthetic and 3 real-world datasets are experimented on, comparison to more recent or advanced sparse LSSM learning methods, such as those using Lasso or regularized MLE approaches (e.g., [1]), is missing. Moreover, evaluation is limited mostly to MRE; the case for learning the \"correct\" topology is largely qualitative except for synthetic scenarios, which could be expanded quantitatively (e.g., with precision/recall on support recovery).\n\n**Refs:**\n\n[1] Fattahi, Salar, Nikolai Matni, and Somayeh Sojoudi. \"Learning sparse dynamical systems from a single sample trajectory.\" 2019 IEEE 58th Conference on Decision and Control (CDC). IEEE, 2019."}, "questions": {"value": "I would wish author(s) can clearify my concerns around thresholding for sparsity, hyperparameter selection, and comparison to Lasso/regularized MLE-style estimators.\n\nThe constraints on similarity transformations in the presence of sparsity-promoting priors are presented for special cases. Can these insights be generalized, or are there pathological systems where the identifiability argument fails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FqwfA09xeq", "forum": "0lct7PrPgS", "replyto": "0lct7PrPgS", "signatures": ["ICLR.cc/2026/Conference/Submission1179/Reviewer_ttg7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1179/Reviewer_ttg7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946814219, "cdate": 1761946814219, "tmdate": 1762915698798, "mdate": 1762915698798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}