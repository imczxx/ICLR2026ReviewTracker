{"id": "Tf29oMgErW", "number": 24909, "cdate": 1758361811252, "mdate": 1759896742836, "content": {"title": "ReynoldsFlow: Spatiotemporal Flow Representations for Video Learning", "abstract": "Representation learning for videos has largely relied on spatiotemporal modules embedded in deep architectures, which, while effective, often require heavy computation and heuristic design. Existing approaches, such as 3D convolutional modules or optical flow networks, may also overlook changes in illumination, scale variations, and structural deformations in video sequences. To address these challenges, we propose ReynoldsFlow, a physics-inspired flow representation that leverages the Helmholtz decomposition and the Reynolds transport theorem to derive principled spatiotemporal features directly from video data. Unlike classical optical flow, ReynoldsFlow captures both divergence-free and curl-free components under more general assumptions, enabling robustness to photometric variation while preserving intrinsic structure. Beyond its theoretical grounding, ReynoldsFlow remains lightweight and adaptable, combining frame intensity with flow magnitude to yield texture-preserving and dynamics-aware representations that substantially enhance tiny object detection. Experiments on benchmarks with various target scales demonstrate that ReynoldsFlow is consistently comparable to or outperforms existing flow-based features, while also improving interpretability and efficiency. These results position ReynoldsFlow as a compelling representation for video understanding and a strong foundation for downstream model learning. The code will be made publicly available.", "tldr": "We propose ReynoldsFlow, a physics-inspired spatiotemporal flow representation that is lightweight, interpretable, and robust to photometric and structural variations for efficient video representation learning.", "keywords": ["Video Representation Learning", "Spatiotemporal Modeling", "Physics-Inspired Flow", "Helmholtz Decomposition", "Reynolds Transport Theorem"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f684429d37a7e3c2489e07985f6eda3f13e9c2d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ReynoldsFlow, a physics-driven spatiotemporal flow representation derived from the Helmholtz decomposition and Reynolds transport theorem (RTT). Unlike conventional optical flow, it explicitly models both divergence-free and curl-free motion components, yielding representations robust to illumination and scale variations. The method is unsupervised, training-free, and computationally lightweight, serving as a plug-and-play motion prior for downstream tasks. When applied to pose estimation and UAV object detection, ReynoldsFlow consistently outperforms classical and deep optical-flow baselines, demonstrating strong generalization and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "S1. Technically novel and theoretically solid: ReynoldsFlow generalizes conventional optical flow by jointly modeling divergence-free and curl-free components, providing a theoretically consistent and physically grounded formulation that rigorously supplements existing flow representations.\n\nS2. Efficiency: The method is training-free and lightweight, making it practical for real-time and embedded settings.\n\nS3. Broad applicability: ReynoldsFlow shows consistent gains across pose estimation and UAV detection, demonstrating task-agnostic adaptability as a plug-and-play flow representation."}, "weaknesses": {"value": "W1. Lack of controlled robustness analysis: Although the authors claim resilience to illumination and scale changes, no controlled experiments isolate or quantify these effects. For example, perturbation tests with varying lighting or object scaling would substantiate these claims.\n\n\nW2. Limited validation on temporal understanding tasks: Most experiments focus on image-level tasks (pose and object detection). Evaluation on video-level understanding tasks (e.g., UAV action recognition, object tracking) would better demonstrate generalization to broader spatiotemporal understanding.\n\nW3. Loss of directional information: ReynoldsFlow representation omits explicit motion direction in its representation, retaining only the magnitudes of divergence-free and curl-free components. While this simplifies visualization, direction cues are important for tasks like action recognition or tracking. A justification or ablation on this design choice would strengthen the argument.\n\nW4. Insufficient figures & tables: It would be better if the authors provided figures and tables about results in Sec.4.3. to clearly demonstrate the effectiveness of the proposed methods."}, "questions": {"value": "Q1. The paper highlights that ReynoldsFlow performs particularly well on tiny objects; could the authors clarify how its representation enables more accurate motion capture for small-scale targets?\n\nQ2. Are there observed failure cases where ReynoldsFlow underperforms?\n\nOverall, I found the paper interesting, with clear technical novelty and strong theoretical grounding. However, a well-controlled analysis comparing ReynoldsFlow with existing flow estimation methods is essential (see W1). Without a detailed and controlled analysis of the proposed method, I have no choice but to give a borderline reject, though the rating could be raised if these weaknesses are adequately addressed. Additional improvements on W2–W4 would also be beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y1XatOqV6N", "forum": "Tf29oMgErW", "replyto": "Tf29oMgErW", "signatures": ["ICLR.cc/2026/Conference/Submission24909/Reviewer_CWTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24909/Reviewer_CWTf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565213708, "cdate": 1761565213708, "tmdate": 1762943240294, "mdate": 1762943240294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a physics-inspired video spatiotemporal flow representation method called ReynoldsFlow, which directly derives spatiotemporal features with physical interpretability from video data. The method adopts an unsupervised and training-free design, constructing texture-preserving and dynamic-aware feature representations by integrating flow field magnitude with frame intensity. It performs exceptionally well in tasks such as small object detection and pose estimation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Lightweight plug-and-play utility: ReynoldsFlow directly computes from raw video frames without training process and large-scale annotated data, with computational cost comparable to traditional optical flow methods. Its three-channel representation (optical flow magnitude, complementary flow magnitude, frame intensity) can be seamlessly integrated into existing detection and pose estimation models, without task-specific preprocessing or model reconstruction, offering strong engineering practicality and compatibility.\n\n2. The paper breaks through the limitations of traditional optical flow that rely on the brightness constancy assumption, introducing fluid dynamics into video spatiotemporal feature modeling, and first proposes ReynoldsFlow which simultaneously includes traditional optical flow (divergence-free component) and complementary flow (curl-free component). This design covers information beyond motion, such as illumination changes and non-rigid deformations, from a principled perspective, solving the robustness issues of traditional optical flow in complex scenarios, and the physical modeling endows features with stronger interpretability."}, "weaknesses": {"value": "1. The generalization capability of the task coverage is insufficient: the experimental validation focuses on two specific tasks, UAV target detection and golf swing pose estimation, and lacks performance evaluation on more general video understanding tasks (such as action recognition, video semantic segmentation). The current results are insufficient to fully demonstrate the generalization ability of ReynoldsFlow, especially its performance in scenarios with non-rigid targets and dense motion trajectories is still unclear. I believe that the dataset's performance on more video tasks will help in verifying the model's capabilities.\n\n2. The data typesetting in Table 1 is chaotic and the data is missing, which affects the readability of the results."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yMWk5m33Pu", "forum": "Tf29oMgErW", "replyto": "Tf29oMgErW", "signatures": ["ICLR.cc/2026/Conference/Submission24909/Reviewer_hLGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24909/Reviewer_hLGr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975348702, "cdate": 1761975348702, "tmdate": 1762943239962, "mdate": 1762943239962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel spatiotemporal flow representation method named ReynoldsFlow for video representation learning. Unlike traditional 3D convolutional modules or optical flow-based networks, ReynoldsFlow is inspired by physics, utilizing Helmholtz decomposition and the Reynolds transport theorem to extract theoretically-grounded spatiotemporal features directly from video data. This method is unique in its ability to capture both divergence-free and curl-free components, making it robust to photometric variations (such as illumination and scale changes) while preserving the intrinsic structure of the video. Experiments demonstrate that ReynoldsFlow is a lightweight and flexible representation, combining frame intensity and flow magnitude, and significantly enhances performance in tasks like tiny object detection."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces fundamental principles from fluid dynamics (Helmholtz decomposition, Reynolds transport theorem) into deep learning, providing a novel and physically-constrained framework for spatiotemporal feature extraction, which possesses high originality in the video learning domain.\n2. Results show that the method can \"substantially enhance tiny object detection,\" a recognized challenge in video analysis, highlighting its practical value in high-resolution or complex scenes.\n3.The paper mentions ReynoldsFlow is \"lightweight and adaptable,\" suggesting it likely avoids the massive computational overhead associated with 3D convolutions or complex optical flow networks."}, "weaknesses": {"value": "1. Details of Main Experiments: Are the OF method results in Table 1 reproduced by the authors? What representation method is used for optical flow?\n2. GolfDB Performance: ReynoldsFlow may not outperform OF on GolfDB, as classical TV-L1 achieves 0.81 and learning-based methods typically improve with fine-tuning. When v_n^c​ is removed, the method relies on OF information yet underperforms the best OF results.\n3. Ablation Study: The ablation study appears limited. Additional combinations of F_R^n should be tested to better understand each component's contribution.\n4. Complementary Claims: The paper claims complementarity to OF but only evaluates datasets where CF excels. Testing on general datasets would strengthen this claim, as true complementarity should not degrade performance on any task."}, "questions": {"value": "Please see Weaknesses 1-3"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OFCZraypBr", "forum": "Tf29oMgErW", "replyto": "Tf29oMgErW", "signatures": ["ICLR.cc/2026/Conference/Submission24909/Reviewer_NJ3u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24909/Reviewer_NJ3u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762399804257, "cdate": 1762399804257, "tmdate": 1762943239778, "mdate": 1762943239778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}