{"id": "Ex5MuveSoO", "number": 7535, "cdate": 1758026312950, "mdate": 1759897847455, "content": {"title": "GUARD: General Unsupervised Adversarial Robust Defense for Deep Multi-View Clustering via Information Bottleneck", "abstract": "The integrity of Deep Multi-View Clustering (DMVC) is fundamentally challenged by adversarial attacks, which corrupt the learning process by injecting a malicious, task-misaligned informational signal. Existing adversarial defense methods for DMVC are model-specific, non-transferable, and limited to complete multi-view scenarios. To address this, we introduce Multi-view Adversarial Purification (MAP), a novel defense paradigm that reframes unsupervised purification as a principled, information-theoretic problem of signal separation. We present GUARD, the first framework to operationalize the MAP paradigm, which instantiates the principles of the Multi-View Information Bottleneck. GUARD is designed to satisfy a dual objective: 1) it maximizes informational sufficiency with respect to the benign data, ensuring the preservation of all task-relevant information; and 2) it enforces purity against the adversarial signal by creating a bottleneck to discard it. Crucially, GUARD achieves this duality not with an explicit penalty term, but through a unique self-supervisory design where the information bottleneck emerges as a property of the optimization dynamics. Extensive experiments validate that our model-agnostic and unsupervised framework effectively purifies adversarial data, significantly enhancing the robustness of a wide range of DMVC models.", "tldr": "A New Adversarial Defense method for Deep Complete and Incomplete Multi-View Clustering via Information Bottleneck.", "keywords": ["multi-view learning", "clustering", "deep learning", "adversarial defense"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2db099eaa223cdb2bef83b763dcb0e33c7644bb1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This article proposes an unsupervised defense framework called GUARD based on the \"information bottleneck\". By \"purifying\" the malicious perturbation signals in the input data, it significantly enhances the robustness of multi-view clustering models against adversarial attacks, while also maintaining high efficiency and generalization capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed GUARD purifier does not rely on a specific clustering model structure. It can seamlessly integrate with various DMVC methods, and demonstrates certain transferability in experiments across datasets and models.\n2. This paper for the first time models multi-view adversarial defense as an \"information separation\" problem. By leveraging the Multi-view Information Bottleneck (MIB) principle, it integrates \"preserving task-related information\" and \"removing adversarial noise\" into a unified theoretical framework, demonstrating high innovation and theoretical value."}, "weaknesses": {"value": "1. Although the paper claims to be an unsupervised method, the purifier needs to use the original clean data as a supervisory signal to align the output during training. This is often difficult to obtain in real attack scenarios, which limits the method's usability in practical applications. \n2. Could the authors further explore whether their approach can be extended to other tasks, such as classification or detection, or evaluated on larger and more complex datasets?"}, "questions": {"value": "1. GUARD needs supervision signals based on clean data, but this is somewhat contradictory in an unsupervised scenario. The core of the loss function is to use clean data to supervise the output. In real attack scenarios, it may not be possible to obtain completely clean data or only partial data, which will affect the practical usability of the method.\n2. Although the paper claims that it is approximately 400 times faster than diffusion purification. How will it perform in large-scale and high-resolution tasks?\n3. If the network capacity is large, or if the noise and data are strongly correlated, could the model possibly retain some of the noise instead of being \"naturally\" discarded?\n4. Does the author fail to include \"adaptive attack\"? If the attacker is aware that the GUARD purifier exists, will the GUARD perform well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0QIISLFYep", "forum": "Ex5MuveSoO", "replyto": "Ex5MuveSoO", "signatures": ["ICLR.cc/2026/Conference/Submission7535/Reviewer_FEdh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7535/Reviewer_FEdh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760871610337, "cdate": 1760871610337, "tmdate": 1762919636964, "mdate": 1762919636964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GUARD, a novel multi-view adversarial purification framework that introduces the new paradigm of Multi-view Adversarial Purification (MAP). The authors reformulate adversarial defense as an information-theoretic signal separation problem. Specifically, GUARD trains a plug-and-play purifier based on the information bottleneck principle to extract task-relevant clean signals from adversarially perturbed inputs while filtering out adversarial noise.\n\nA key innovation lies in its self-supervised training mechanism, which focuses solely on information sufficiency—by enforcing consistency between purified and clean data at both pixel and feature levels, “purity” naturally emerges during optimization without any explicit regularization. Overall, the paper is technically sound, well-motivated, and supported by comprehensive experiments demonstrating strong model-agnostic performance, cross-dataset generalization, and high computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is highly portable and practical. It functions as an independent front-end module that requires no dataset- or model-specific parameter tuning. The purifier generalizes well across datasets and significantly enhances adversarial robustness.\n2. Experiments show that GUARD restores attacked models to nearly their clean accuracy while achieving up to 400× faster inference than diffusion-based purification baselines, maintaining comparable or better performance. This makes it highly suitable for real-world deployment.\n3. The paper is overall well-executed, with rigorous method design and comprehensive experiments that convincingly demonstrate the effectiveness and efficiency of the proposed approach."}, "weaknesses": {"value": "1. Although the purifier exhibits strong generalization ability, the experiments are limited to image-based multi-view datasets and relatively old downstream models. Expanding evaluations to more diverse datasets and new downstream models would strengthen the paper.\n2. This article lacks a detailed complexity analysis of the purifier. Please provide a complete supplement.\n3. At present, it mainly targets PGD attacks. Additional robustness analysis can be conducted under other perturbation attacks to verify the stability and effectiveness of the method."}, "questions": {"value": "1. Although the purifier exhibits strong generalization ability, the experiments are limited to image-based multi-view datasets and relatively old downstream models. Expanding evaluations to more diverse datasets and new downstream models would strengthen the paper.\n2. This article lacks a detailed complexity analysis of the purifier. Please provide a complete supplement.\n3. At present, it mainly targets PGD attacks. Additional robustness analysis can be conducted under other perturbation attacks to verify the stability and effectiveness of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aptKSsA1a8", "forum": "Ex5MuveSoO", "replyto": "Ex5MuveSoO", "signatures": ["ICLR.cc/2026/Conference/Submission7535/Reviewer_oqXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7535/Reviewer_oqXW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559305594, "cdate": 1761559305594, "tmdate": 1762919636111, "mdate": 1762919636111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GUARD, an unsupervised adversarial defense method for deep multi-view clustering (DMVC). The authors reframe adversarial purification as a signal separation problem through an information-theoretic lens, inspired by the Multi-View Information Bottleneck (MIB) principle. GUARD operationalizes this concept via a self-supervised loss designed to enforce both informational sufficiency and purity without explicit adversarial regularization. The method is evaluated across several DMVC models (e.g., EAMC, SiMVC, InfoDDC, AR-DMVC) and datasets (EdgeMNIST, NoisyMNIST, RegDB), showing improved adversarial robustness over baselines, including diffusion-based purifiers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well-structured, with extensive experimental results.\n\n2. The empirical section is thorough, covering both complete and incomplete multi-view clustering."}, "weaknesses": {"value": "1. The concept of enforcing information sufficiency through the information bottleneck for achieving multi-view robustness has already been explored in prior works such as [1] and [2].\n\n2. The theoretical treatment of information sufficiency and mutual information estimation closely mirrors that of Federici et al., The “purification” component is only superficially motivated, being described primarily as adding and mitigating adversarial noise, without deeper analytical justification.\n\n3. It remains unclear why the authors reuse the same mutual information estimation strategy as Federici et al. since directly adopting it does not constitute a theoretical advance. Moreover, the rationale for not directly minimizing I(X, Z) is not discussed or justified.\n\n\n[1] Yu, Xi, et al. \"Improving adversarial robustness by learning shared information.\" Pattern Recognition 134 (2023): 109054.\n\n[2] Zhang, Qi, et al. \"Multi-view information bottleneck without variational approximation.\" ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022."}, "questions": {"value": "1. The paper claims that “purity emerges implicitly from the optimization process.” Can the authors provide theoretical justification or empirical evidence supporting this assertion? Additionally, would incorporating an explicit information bottleneck loss further improve or stabilize the purification process?\n\nI would be willing to raise my score if the authors can adequately address my concerns regarding the novelty of the work and the questions raised above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JjMCK28fng", "forum": "Ex5MuveSoO", "replyto": "Ex5MuveSoO", "signatures": ["ICLR.cc/2026/Conference/Submission7535/Reviewer_mwxm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7535/Reviewer_mwxm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791457228, "cdate": 1761791457228, "tmdate": 1762919634881, "mdate": 1762919634881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the vulnerability of deep multi-view clustering (DMVC) models to adversarial attacks by proposing GUARD (General Unsupervised Adversarial Robust Defense), a novel unsupervised and model-agnostic defense framework. The authors reconceptualize adversarial defense as an information-theoretic signal separation problem based on the Multi-view Information Bottleneck (MIB) principle. GUARD aims to simultaneously maximize the preservation of task-relevant information (“informational sufficiency”) and suppress adversarial noise (“purity”). Unlike traditional adversarial training approaches, GUARD introduces no explicit penalty terms and instead allows the information bottleneck to emerge naturally through optimization dynamics. Extensive experiments on five benchmark datasets and nine DMVC models demonstrate that GUARD significantly enhances clustering robustness, restoring performance to near-clean levels while maintaining efficiency—achieving up to 400× faster inference compared to diffusion-based purification methods. The results highlight GUARD’s strong generalizability, transferability, and practicality for robust multi-view clustering in adversarial environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper reformulates adversarial defense for multi-view clustering through an information-theoretic lens.\n2. GUARD is fully unsupervised and does not rely on model-specific training or labels. This design enables seamless integration with various DMVC models and broad applicability across complete and incomplete multi-view scenarios.\n3. Compared to diffusion-based purification methods, GUARD achieves up to a 400× inference speedup, making it highly practical for real-world applications where efficiency matters."}, "weaknesses": {"value": "1. The paper assumes that deep multi-view clustering (DMVC) models are meaningfully threatened by adversarial attacks, but provides no convincing real-world evidence. In unsupervised settings, where no labels exist, the impact of such attacks is ambiguous and less critical than in supervised tasks.\n2. The threat model and defense scenario appear largely theoretical. It is unclear why adversarial robustness should be prioritized for clustering tasks that are typically exploratory rather than deployed in high-stakes applications.\n3. The proposed information bottleneck perspective is conceptually appealing but lacks rigorous derivation or analytical depth. The method mostly reinterprets existing adversarial purification techniques under new terminology, without offering novel theoretical insights.\n4. Experiments are restricted to small, synthetic datasets such as MNIST and FashionMNIST variants, which fail to capture the complexity and diversity of real-world multi-view data. This limits the generalizability and credibility of the empirical results.\n5. Considering the unclear real-world significance and the heavy experimental setup, the overall impact of the work may not justify the complexity of the framework. The contribution risks being perceived as a technically competent but conceptually weak exercise."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RhRXy5d0UP", "forum": "Ex5MuveSoO", "replyto": "Ex5MuveSoO", "signatures": ["ICLR.cc/2026/Conference/Submission7535/Reviewer_onzn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7535/Reviewer_onzn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973026848, "cdate": 1761973026848, "tmdate": 1762919634233, "mdate": 1762919634233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the adversarial vulnerability of deep multi-view clustering models, this paper proposes GUARD. The innovation of GUARD is mainly reflected in three aspects. Firstly, it reconstructs the adversarial defense problem into a signal separation problem based on the multi-view information bottleneck theory. Secondly, the proposed method achieves adversarial robustness without manual labeling. Finally, its model-independent property makes it widely applicable. Different from the traditional adversarial training that requires explicit regularization term, GUARD naturally realizes the information bottleneck effect through the optimization process, which improves the purity of information while ensuring the sufficiency of information. Experimental evaluation shows that the test results on five benchmark datasets and nine mainstream models support the effectiveness of GUARD. The authors claim that the framework is not only able to restore the performance of the attacked model to near the level of clean data, but also achieves inference speed up to 400 times faster than the diffusion-based sanitization method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper  re-examines the problem of adversarial defense in multi-view clustering from the perspective of information theory, and give a new interpretation.\n\n2. GUARD operates completely unsupervised and does not rely on model-specific training or labels, so it can be easily adapted to different DMVC models for both complete and incomplete multi-view scenarios.\n\n3. Compared with the diffusion model-based cleaning method, GUARD can be speeded up by up to 400 times in the inference stage, which shows a significant efficiency advantage in practical applications."}, "weaknesses": {"value": "1. This paper assumes that the Deep Multi-view Clustering (DMVC) model is seriously threatened by adversarial attacks, but does not give convincing real application scenarios as support. In an unsupervised setting, where there are no labels and the impact of an attack is relatively obscure, the importance is much less prominent than in a supervised task.\n\n2. The threat model and defense scenarios set in this paper are more at the theoretical level, which makes it difficult to understand why the adversarial robustness needs to be emphasized in clustering tasks, which are mainly based on exploratory analysis and are not often used in high-risk scenarios.\n\n3. Although the information bottleneck perspective proposed by the authors is conceptually appealing, the derivation is not rigorous and the theoretical depth is limited. The overall approach is more like a rebranding and packaging of existing adversarial decontamination techniques than a real new theoretical breakthrough.\n\n4. The experiment is only based on small-scale synthetic datasets such as MNIST and FashionMNIST. The structure of such data is too simple to reflect the complexity and diversity of real multi-view data, which limits the representation of empirical results."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RhRXy5d0UP", "forum": "Ex5MuveSoO", "replyto": "Ex5MuveSoO", "signatures": ["ICLR.cc/2026/Conference/Submission7535/Reviewer_onzn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7535/Reviewer_onzn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973026848, "cdate": 1761973026848, "tmdate": 1763658237360, "mdate": 1763658237360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}