{"id": "hUii0ju5aA", "number": 11097, "cdate": 1758189132187, "mdate": 1759897608836, "content": {"title": "Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models", "abstract": "Safety alignment is essential for building trustworthy artificial intelligence, yet it remains challenging to enhance model safety without degrading general performance. Current approaches require computationally expensive searches for the optimal proportion of safety-critical and general-purpose data to balance safety and general performance, incurring high costs with limited gains. In this work, we show that LoRA-based Refusal-training enables performance-preserving safety alignment even when trained solely on safety data, demonstrating that LoRA serves as \\textbf{cost-efficient}, \\textbf{performance-preserving}, and \\textbf{plug-and-play} safety patches. Beyond empirical findings, we provide both theoretical and experimental evidence that LoRA effectively decouples safety into a low-rank subspace largely orthogonal to the model’s intrinsic transformation space, ensuring that safety enhancements do not interfere with inherent capabilities.", "tldr": "This study shows, both theoretically and empirically, that LoRA-based safety alignment decouples safety into an orthogonal low-rank subspace, enabling efficient, performance-preserving, and plug-and-play safety patches", "keywords": ["Large Language Models", "Safety Alignment", "Low Rank Adapter (LoRA)"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/462051fc5713533842166466c659851dc3c17e8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates LoRA-based refusal training as a cost-efficient approach for safety alignment of large language models (LLMs). The authors argue that LoRA fine-tuning constructs a “safety subspace” that is largely orthogonal to the model’s intrinsic weight space, thereby enhancing safety (i.e., reducing jailbreak success rate) while degrading general capabilities less than full fine-tuning. The paper provides both a theoretical intuition based on SVD analysis of LoRA update matrices and empirical evaluations across several instruction-tuned models, including Qwen2.5-7B-IT, LLaMA3.1-8B-IT, and Mistral-7B-IT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The figures and tables are well-organized, making it easy to follow\n* Experiments cover multiple instruction-tuned backbones (Qwen2.5-7B-IT, LLaMA3.1-8B-IT, Mistral-7B-IT)"}, "weaknesses": {"value": "* **Limited conceptual novelty.** Most of the paper’s conclusions are already well established in prior literature. Earlier works such as [1] have systematically shown that LoRA updates will forget less and better keep general ability. [2] has shown that safety subspace is usally low rank. This paper mainly repackages these known observations without introducing a new mechanism or deeper insight.\n\n* **The “safety gap between lora & finetuning” result is weak .** The only somewhat new finding, that the difference between LoRA and full fine-tuning is smaller in the safety domain than in other domains (code, finance) is not  convincing enough. The training datasets are limited, and it is unclear whether this effect is general or simply dataset-specific.\n\n* **Missing robustness analysis.** The paper lack investigation about how well the learned “safety subspace” persists under further finetuning.\n\n* **theoretical analysis is a bit trival**. The theoretical part provides a simple SVD-based explanation of “orthogonality” between LoRA updates and the base model weights.  the analysis is largely descriptive and does not yield new insight into how a safety subspace is actually formed, what governs its geometry, or how it differs from other task-specific subspaces. \n\n[1] LoRA Learns Less and Forgets Less\n\n[2] Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"}, "questions": {"value": "In section 5.3, the paper compares the right singular vectors $V_0$ of the original weight matrix $W_0$ and $V_{\\Delta}$ of the LoRA update $\\Delta V$, , both truncated to $r$ dimensions. However,  I think $W_0$  is typically higher rank, truncating  $V_0$  to be the same as $V_{\\Delta}$ seems not that reasonable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "263Kybn6L6", "forum": "hUii0ju5aA", "replyto": "hUii0ju5aA", "signatures": ["ICLR.cc/2026/Conference/Submission11097/Reviewer_cYMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11097/Reviewer_cYMV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761293685152, "cdate": 1761293685152, "tmdate": 1762922275648, "mdate": 1762922275648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using LoRA-based finetuning as a \"safety patch\" for LLMs. The central claims are (1) efficacy: LoRA-based fine-tuning can achieve strong safety alignment while preserving general performance; (2) efficiency: lora is more cost-efficient than full-parameter fine-tuning; (3) mechanism: the paper argues this works because LoRA decouples the safety update into a low-rank subspace that is \"largely orthogonal\" to the original model's \"intrinsic transformation\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Cost-efficiency and simplicity: The method is presented as highly cost-efficient because it successfully achieves safety alignment by training solely on safety-critical data and also utilize lora technique. \n- The paper demonstrates that LoRA-based SFT can substantially enhance model safety."}, "weaknesses": {"value": "- The authors claim the theoretical insight as a part of the contributions. However, the concept of using orthogonal subspaces to prevent catastrophic forgetting in adapter-based tuning is not new. The paper itself cites O-LoRA, a method explicitly designed for \"Orthogonal Subspace Learning\" in continual learning. The core idea of this paper that a new task can be learned in a subspace orthogonal to the original model's to prevent interference is exactly the central thesis of O-LoRA. The paper fails to adequately differentiate its core mechanism from this prior work. \n- One experiment is missing from the paper, that is LoRA with safety-general data. It is possible that LoRA-based SFT on the safety-general would perform even better, which would invalidate the paper's claim that mixing general data compromises safety. \n- Regarding the life-long learning experiment, the authors seem to choose a rather weak baseline as DPO + full parameters seems to lead to low performance in a variety of tasks. \n- There are also some related works that the authors could cite, for example [1,2]\n\n[1] Refusal in Language Models Is Mediated by a Single Direction\n[2] LoX: Low-RankExtrapolation Robustifies LLMSafetyAgainst Fine-tuning"}, "questions": {"value": "Please refer to the the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5sToKNAmtb", "forum": "hUii0ju5aA", "replyto": "hUii0ju5aA", "signatures": ["ICLR.cc/2026/Conference/Submission11097/Reviewer_iG1W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11097/Reviewer_iG1W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698798628, "cdate": 1761698798628, "tmdate": 1762922275152, "mdate": 1762922275152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper conducts extensive experiments to investigate and show that Low Rank Adaptation (LoRA) only using a refusal dataset is a better and more reliable approach for weight-space safety alignment/tuning compared to full finetuning (SFT) with a mixed dataset of refusal and general-purpose data. Experiments are mostly focused on supporting this claim by showing that LoRA-based safety-tuning induces parameter updates that influences safety-related input more than general-purpose tasks (hence, less degradation of general utility)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Writing:** Paper is well-organized with plenty details while maintaining coherent logical flow.\n2. **Motivation:** The problem being addressed is well-motivated and Section 2 sets up the stage by quantifying how uncareful safety-tuning degrades general performance.\n3. **Extensive empirical analysis:** Convincing empirical analysis is provided that LoRA is more reliable for the task by comparing and visualizing $\\Delta W$ and $W_0$ using various matrix alignment metrics."}, "weaknesses": {"value": "1. Although a plenty of prior works from safety alignment literature are brought up and discussed, some works that are directly related to LoRA-based safety alignment seem to be overlooked. Particularly, [1] also shows that LoRA on refusal dataset can bypass the \"safety tax\" (negative impact on general utility) in the case of reasoning abilities. Also, I think [2] takes it further by implementing an extra projection for LoRA updates to ensure it lies in the subspace present in the aligned model parameters.\n2. The theoretical explanation also remains quite generic. To be more precise, the approximate orthogonality of $\\Delta V$ and $V_0$ is a necessary condition for the observed experimental results. It would be more meaningful contribution if, for example, Figure 8(b) could be explained through this argument. *\"Why does safety tuning induce more orthogonal parameter updates compared to finance and code?\"*, which I would guess it is because the pre-training dataset had finance and code-related data already."}, "questions": {"value": "Stemming from the weaknesses section:\n\n1. Could authors compare and pinpoint portions of their contributions complementary/orthogonal to [1] and [2]? I admit that [1] is only recently publicly available (first arXiv version on July 22), while [2] is old enough and peer-reviewed.\n2. Could authors comment on the pretrained models and validate/invalidate my guess in Weakness 2? Easiest way to test it would be to reproduce Figure 8(b) with a safety-aligned model as a baseline, and compare safety, finance, and code cosine similarities.\n\n___\n\nOverall, I find the investigations in this paper thorough and useful in most aspects. However, lack of comparison with directly related works makes it hard for me to place the paper in the map of current safety research state. I am open to change my score if both of my questions are addressed in a satisfactory manner.\n\n___\n\n### References\n\n1. Yihao Xue, Baharan Mirzasoleiman. LoRA is All You Need for Safety Alignment of Reasoning LLMs. arXiv:2507.17075\n2. Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang. Safe LoRA: The Silver Lining of Reducing Safety Risks when Finetuning Large Language Models. NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lu4tL5l71y", "forum": "hUii0ju5aA", "replyto": "hUii0ju5aA", "signatures": ["ICLR.cc/2026/Conference/Submission11097/Reviewer_u1tT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11097/Reviewer_u1tT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798139435, "cdate": 1761798139435, "tmdate": 1762922274596, "mdate": 1762922274596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. We are encouraged that reviewers found our work well-motivated (u1tT), methodologically novel and practical (LmEX, Rd8i), supported by solid and comprehensive experiments (LmEX, Rd8i, u1tT), theoretically valuable for safety alignment (LmEX), and clearly written (LmEX, u1tT).\n\nThe reviewers’ primary concerns center on two areas: (1) **comparison with related or concurrent works**, and (2) **the scope and clarity of our theoretical contribution**. We address both issues directly and in depth in the General Response below."}}, "id": "h6VPTU75bc", "forum": "hUii0ju5aA", "replyto": "hUii0ju5aA", "signatures": ["ICLR.cc/2026/Conference/Submission11097/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11097/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11097/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763723947113, "cdate": 1763723947113, "tmdate": 1763725284506, "mdate": 1763725284506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the use of LoRA-based Refusal-training for safety alignment in Large Language Models (LLMs), proposing it as a cost-efficient, performance-preserving, and plug-and-play alternative to traditional full-parameter fine-tuning methods. A key finding is that LoRA-based alignment, even when trained solely on safety-critical data, significantly enhances safety (near-zero Attack Success Rate or ASR) while incurring minimal degradation to general capabilities (performance-preserving).\nThe authors provide a theoretical explanation based on transformation subspace orthogonality. They propose that the LoRA-induced safety update (ΔW) lies in a low-rank subspace that is largely orthogonal to the original model's intrinsic transformation space (W_0 ). This orthogonality, quantified using Sim(V_Δ,V_0)=V_Δ^⊤ V_0 ≈0 , minimizes the interference (or \"catastrophic forgetting\") between the safety adjustments and the model's inherent knowledge and abilities.\n\nEmpirical analyses support this claim through comparisons of:\n1.Parameter Update Magnitude: Counter-intuitively, LoRA produces larger parameter updates than full-parameter training in most layers, yet it better preserves general performance.\n2. Layer-wise Hidden State Shifts: LoRA-based Refusal-SFT induces smaller hidden state shifts on benign inputs but larger shifts on jailbreak attacks compared to full-parameter methods.\n3. Orthogonality: LoRA-based alignment yields the lowest similarity (highest orthogonality) between the safety subspace and the model's intrinsic transformation space.\nFurthermore, the paper demonstrates LoRA’s utility for lifelong safety alignment in multi-round red-teaming and shows that the safety subspace is more orthogonal and less intrusive than subspaces induced by domain-specific fine-tuning (e.g., code and finance)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "*Originality & Significance: The paper presents a novel and highly effective method for safety alignment using LoRA-based Refusal-SFT and introduces subspace orthogonality as a powerful theoretical lens to explain the mechanism. This theoretical perspective is a significant conceptual contribution to understanding parameter-efficient fine-tuning (PEFT) in the context of safety and catastrophic forgetting.\n*Quality & Clarity: The work is supported by rigorous and extensive empirical validation across multiple LLM architectures (Qwen, LLAMA, Mistral) and alignment paradigms (SFT, DPO). The results consistently and strongly support the core hypothesis, particularly the superiority of LoRA's safety-utility trade-off shown in Figure 1(a). The writing is excellent, making the complex technical concepts (SVD, orthogonality) accessible.\n*Practical Value: The demonstration that LoRA-based Refusal-SFT performs best using only safety data is a crucial finding for practical deployment, as it eliminates the costly and difficult task of searching for the optimal proportion of safety-critical and general-purpose data. The plug-and-play feature for lifelong alignment is highly relevant for continuous model maintenance."}, "weaknesses": {"value": "*Orthogonality Measurement Robustness: The current measure of orthogonality, Sim(V_Δ,V_0)=V_Δ^⊤ V_0 , relies on the ΔW from SVD, which itself is an approximation for LoRA weights (which are already low-rank AB matrices). While the results in Appendix H show a clear link between LoRA rank and the number of non-negligible singular values, a clearer theoretical or empirical connection between the inherent LoRA structure (ΔW=AB) and the guarantee of orthogonality to W_0 would strengthen the claim. The current finding is empirical; a theoretical bound or analysis on the orthogonality of span(V_Δ) for LoRA updates is missing.\n*Generalizability of Safety Domain: The cross-domain analysis in Section 5.4 only compares safety with code and finance domains. To fully support the claim that the safety subspace is uniquely orthogonal and less intrusive, it would be beneficial to compare against other, potentially less \"entangled\" domains such as factual knowledge updates or style transfer.\n*LoRA Rank Selection and Scaling: Although Appendix F and G discuss the effect of LoRA rank, the best rank for larger models (rank 16 for Qwen2.5-14B-IT) suggests the optimal rank is model- and size-dependent. A systematic study or guideline for selecting an appropriate LoRA rank, perhaps based on the initial W_0  properties (e.g., singular value decay of W_0 ), would improve the methodology's completeness and practical application."}, "questions": {"value": "1. Theoretical Link to LoRA Structure: Can the authors provide a more formal theoretical justification, beyond empirical SVD, that the specific structure of LoRA updates (ΔW=AB) inherently biases the resulting subspace span(V_Δ) towards orthogonality with the initial weight matrix subspace span(V_0)? This would significantly elevate the theoretical contribution from an observation to a mechanism.\n\n2. Orthogonality vs. Interference in Practice: The derivation in Appendix I (Non-Orthogonal Case) shows that the interference terms are \nW_0x_Δ​+ΔWx_0 . The paper claims that LoRA-based SFT produces larger weight updates (∣ΔW∣) but smaller hidden state shifts (Δh^(l) ) on benign inputs. Since the hidden state shift is directly related to the interference, can the authors specifically quantify and compare the magnitude of the interference terms (∣∣W_0x_Δ+ΔWx_0∣∣) between LoRA-based and full-parameter models on benign inputs? This would directly connect the theoretical explanation to the key empirical findings.\n\n3. Impact of Initial Model Alignment: The paper focuses on aligning instruction-tuned models (e.g., LLaMA3.1-8B-IT). How does the initial degree of safety alignment of the base model (e.g., a highly safe Llama-Guard-like model vs. a non-aligned base LLM) affect the final orthogonality of the LoRA safety patch? Does an already safe base model force the LoRA update to be more orthogonal, or does it make the effect negligible?\n\n4. Beyond Refusal-SFT: The primary success is observed with LoRA-based Refusal-SFT. Given that DPO also exhibits better parameter and hidden state stability than SFT (Sections 5.1, 5.2) , why does LoRA-based DPO not achieve a similarly dominant trade-off as LoRA-based Refusal-SFT (e.g., lower safety gains for LoRA-based DPO in Table 1)? A deeper analysis into the mechanism difference between SFT and DPO loss functions in the orthogonal subspace framework would be insightful."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "It utilizes jailbreak attacks (WildJailbreak, SG-Bench) and safety-critical data (SafeEdit-Train). While the work is intended to improve LLM safety, the use and potential public release of models/data trained on adversarial content, as well as the discussion of attack/defense methods, requires an ethics review to ensure adherence to responsible research practices and mitigation strategies are appropriately followed."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RQqy6l1jEt", "forum": "hUii0ju5aA", "replyto": "hUii0ju5aA", "signatures": ["ICLR.cc/2026/Conference/Submission11097/Reviewer_LmEX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11097/Reviewer_LmEX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762392073370, "cdate": 1762392073370, "tmdate": 1762922273834, "mdate": 1762922273834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses how to implement safety finetuning for LLMs using LoRA weight updates. The authors argue that full-parameter safety finetuning on safety datasets alone still causes unnecessary changes in weight directions that are indicative of performances; fine-tuning on both safety and general data compromises safety guarantees. Thus, they hypothesize that performance and safety can be decomposed into (almost) orthogonal directions and using LoRA allows for safety finetuning to focus on the 'safety' directions only."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely topic. Already in this conference, there is another submission that also attempts to use LoRA for safety finetuning. \n\nThe experiments performed in the paper is comprehensive and well-demonstrates the points being made, which is that LoRA directions are generally almost orthogonal to base weight directions. \n\nThe proposed methodology is simple enough to be tested in practice with ultra-large LLMs, making the research valuable not just to academia but also to the industry."}, "weaknesses": {"value": "The theoretical justifications of the paper is rather lacking in several ways:\n- No orthogonal decomposition guarantees. The authors assumed that performance and safety can be orthogonalized (at least approximately) without giving arguments, even heuristical arguments, on why that should be the case. While experimental data shows that LoRA update has small (matrix) inner product with the base weights, that fact can be induced by implicit or explicit regularizations (since LoRA is constraint to have small rank, it must packs all gradient update information in a few dimensions; and this is done most efficiently with dimensions that are orthogonal to the base weights). In the above scenario, the orthogonality behavior may be at odd with 'true' safety dimensions (which was shown to exist, for instances, in Wei et al., \"Assessing the brittleness of safety alignment via pruning and low-rank modifications\"), and by forcing the update to be orthogonal via LoRA, safety guarantees are weakened.\n- Orthogonality of weights does not imply independence of performance. While it is a point usually made in the literature that orthogonal updates to the weights preserve original performance, it is still a heuristical argument, since the final model is nonlinear in the transformer weights. It is fine to use this idea as a motivation and test empirically again (which the authors did a decent job empirically), the final results are inherently experimental in nature and the theoretical contribution would be too weak to be counted as a major contribution of the paper. This is also relevant to the next point, since the authors distinguish themselves from literature mostly by this 'orthogonalization' concept. \n- Missing direct comparison to Wei et al  \"Assessing the brittleness of safety alignment via pruning and low-rank modifications\", which identifies 'safety directions' and 'safety neurons' quantitatively. Why, or under which condition, is the methodology in the current paper more correct than that proposed in Wei et al (pruning least safety-relevant neurons or removing safe-relevant directions (via LoRA) to improve safety)? While the reviewer think that there are enough differences between the two papers, a direct comparison should be done to previous LoRA methods for LLM safety. \n- Minor: the authors have pointed out that different finetuning methods (DPO vs SFT-safety, etc.) induce different norms/magnitude in weight changes, yet the (dis)similarity score (matrix inner product of base and update weights) are left unnormalized. This may result in the small values observed simply coming from the matrix weights values being small, rather than orthogonality. Some normalization should be done in this comparison. \n\nThe weaknesses pointed out above are mainly conceptual and theoretical. Since the paper's impact and focus is purely experimental, I still recommend weak acceptance but would consider increasing my score if my concerns are addressed."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FDmRdLgHQb", "forum": "hUii0ju5aA", "replyto": "hUii0ju5aA", "signatures": ["ICLR.cc/2026/Conference/Submission11097/Reviewer_Rd8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11097/Reviewer_Rd8i"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11097/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762799244702, "cdate": 1762799244702, "tmdate": 1762922273562, "mdate": 1762922273562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}