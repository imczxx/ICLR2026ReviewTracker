{"id": "jjrzADWOD5", "number": 10142, "cdate": 1758161911228, "mdate": 1759897671435, "content": {"title": "Puzzle Everything: Benchmarking Knowledge Reasoning through Generalized Puzzle Solving", "abstract": "The validity of existing benchmarks is challenged by data leakage, which risks reducing current evaluation from genuine problem-solving to mere recitation. This concern highlights a fundamental need for robust methods to assess reasoning based on a model's internalized knowledge. To address this issue, we introduce the Puzzle Everything Benchmark (PEB), which fundamentally expands the puzzle-solving application for reasoning evaluation. By treating any learned concept, term, or entity as a solvable puzzle, our approach enables a direct assessment of a model's ability to reason over any specialized knowledge. Since it is well-established that Wikipedia serves as a cornerstone of the training corpora for most models, PEB facilitates this with a Wikipedia collection of 960 puzzles of doctoral-level difficulty drawn from eight distinct disciplines. Our evaluation on PEB reveals a counter-intuitive phenomenon: most open-source reasoning models significantly underperform their standard, no-thinking counterparts. This finding uncovers a critical gap between proficiency in algorithmic reasoning and the application of reasoning to specialized knowledge domains. Thus, the construction of the PEB introduces a scalable method for rapidly generating diagnostic benchmarks in any specialized domain, enabling a robust evaluation of reasoning that is insensitive to prior data exposure.", "tldr": "We created a new benchmark by treating any learned specialized concept as a puzzle, revealing a critical gap between a model's algorithmic reasoning skill and its ability to reason about what it actually knows.", "keywords": ["Large Language Models", "Data Leakage", "Evaluation", "Puzzle"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4879202c909ca25edc001939463d4eacc205f128.pdf", "supplementary_material": "/attachment/677061db80d3675ae14e2d53778441488aca62a2.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents the Puzzle Everything Benchmark (PEB), which is designed to test reasoning over *internal knowledge* by presenting the models with a specific form of puzzle. In each puzzle, there is a hidden piece of information, e.g. a name or a concept, and the model being tested must iteratively ask questions to figure out the information. Each round, it receives feedback from an *environment agent* which has access to the concept's Wikipedia page in-context. Successful identification of the concept requires asking sensible questions and using the returned information effectively. This benchmark tests models' abilities to navigate the search for unknown information and to update on new information. The methodology can be broadly applied to information from different domains."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality:** This paper presents an interesting and novel method for testing LLMs. The motivation for the paper is that many benchmarks test knowledge directly, which is a recall task. This becomes trivial if models have been trained on the information, which is very possible given the current paradigm of training. The benchmarking situation is an urgent problem demanding new solutions.\n- **Comprehensiveness:** The benchmark is broad, containing 960 questions on 8 major topics. This is a relative strength. Similarly, it can be applied to any new piece of information. It reminds me a little bit of the capability being tested in [1], which might be an interesting reference. \n- **Interesting metrics:** The analysis and error analysis sections are particularly interesting. The incorrect guessing metric is a real insight into the overconfidence of different models. This is a nice novel idea. Additionally, the CoT analysis to work out the strategies that models take is interesting (though it may be wise for the authors to hedge that CoT is not necessarily faithful here, e.g. [2]).\n- **Reverse scaling laws:** One of the potentially highly impactful contributions of the paper is an example of negative returns to test-time compute. This would be a significant discovery if it were the case! However, this hypothesis needs to be explored further to be convincing (see *weaknesses* section).\n\n[1] The reversal curse: LLMs trained on \"A is B\" fail to learn \"B is A\". Berglund et al. 2024.\n[1] Chain-of-Thought Is Not Explainability: Barez et al. 2025."}, "weaknesses": {"value": "- **Phenomenon:** It is unclear to me exactly what the specific phenomenon being tested is or how generalisable this is to other tasks. The authors present it in the title as *knowledge reasoning* and expand this to reasoning over *internal knowledge* throughout the paper. If the authors could add precise definitions of these terms and the phenomenon the benchmark targets, it would aid the paper. For example, one question I had was, what are examples of this skill outside of the specific problem domain? Furthermore, it is unclear whether better performance comes from better factual knowledge or better knowledge of good strategies for the specific game mechanics. This is noted as a limitation of *existing* puzzle benchmarks (L081), hence it would be nice if the paper made it clearer why this does not apply here.\n- **Method explanation:** Key parts of the methodology are not properly explained in the main paper which made it hard to follow, e.g. How were the six million article titles screened to be doctoral-level difficulty or greater? How were they grouped into higher-level topics? Clearer explanation here would help. \n- **Writing and presentation:** There could be some quick improvements on the writing and presentation. In places, the language is a bit imprecise (e.g. L142, L161) or excessively descriptive/not in standard academic paper style (e.g. L262). Similarly, some of the figures could be iterated on, e.g. Figure 4 is unclear - what does the shading mean, and what does the size of each sub-block mean (does this represent the proportion of questions belonging to that sub-topic?)\n- **Reasoning vs non-reasoning:** The authors discover a potentially very interesting finding that reasoning models perform worse than their non-reasoning counterparts. This is cool! However, more experiments are needed to fully support this claim. For example, GPT-5 could be run on low, medium, and high thinking modes to see if returns to test time compute really are negative. I would be particularly interested in seeing the results of this. One potential reason for this might be that the reasoning models are being run at temperature 0.2. There is fairly good evidence that reasoning-style models perform worse at low temperatures [1], so it might be worth doing an ablation with each model's recommended inference parameters. In addition, I was under the impression that OpenAI don't let you set the temperature parameter for GPT-5 (I think for this reason), so I'm confused what parameters were used here.\n- **Importance of the environment agent:** The environment agent seems to have a highly non-trivial task of accurately answering the questions. Even with the Wikipedia page in-context, I would imagine this is a hard task! It would be good to show some accuracy metric of the environment agent or a proxy of agreement between different models on the same tasks. I would expect a fair amount of variations, which complicates the benchmark. Similarly, the paper reports that the choice of environment agent is not important, yet Figure 6 in the appendix shows that 3 different environment agents would lead to 3 different best models on the benchmark. This seems very significant. Perhaps it would be best to average over different agents?\n- **Lack of statistical analysis:** In a similar vein, the results are missing statistical analysis, like error bars, which makes comparison hard. For example, it is hard to compare the relative performance of the DeepSeek, Gemini and Qwen variants without this.\n\nThis is definitely an interesting benchmark that could be published with further revision; however, in my opinion, there are currently a few significant issues that mean it's not quite ready.\n\n[1] Qwen 3 8B Huggingface: https://huggingface.co/Qwen/Qwen3-8B"}, "questions": {"value": "A few questions on things I didn't pick up from the paper (I may have missed these, so would appriciate help understanding them)\n\n[1] **Inference settings:** As before, the paper says models are run on temp. 0.2. Are all models run on that temp?\n\n[2] **CoT Access:** To what extent can models see their previous CoTs? With open source models this would be possible, with closed models, this is semi-possible, e.g. the Claude API has some functionality for this. I belive this is not possible with OpenAI.\n\n[3] **Code:** Can you release the code in an anonymous repository (e.g. https://anonymous.4open.science)?\n\n**Other points:**\nIn Figure 2, the model appears to misunderstand the task initially, starting its response: “I’ll help you identify the disease.” How common is this misunderstanding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fah5EVjFrF", "forum": "jjrzADWOD5", "replyto": "jjrzADWOD5", "signatures": ["ICLR.cc/2026/Conference/Submission10142/Reviewer_Jwi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10142/Reviewer_Jwi6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757611754, "cdate": 1761757611754, "tmdate": 1762921514084, "mdate": 1762921514084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Puzzle Everything Benchmark (PEB), a novel evaluation framework for measuring reasoning over internalized knowledge in large language models (LLMs).\nInstead of static question–answer tasks such as MMLU or BIG-Bench, PEB frames reasoning as an interactive puzzle game: the model must identify a hidden concept (drawn from doctoral-level Wikipedia topics) by iteratively asking multiple-choice questions and receiving correctness feedback only.\nThe benchmark includes 960 puzzles across eight academic domains and can be extended to other specialized areas (e.g., IC back-end EDA tools).\nExperimental results show a counterintuitive finding—open-source “reasoning-enhanced” models (e.g., DeepSeek-R1, Qwen-Thinking) perform worse than their instruction-tuned counterparts, suggesting a gap between procedural reasoning and applied knowledge reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Original framing:\n\nThe paper proposes a creative and well-motivated paradigm that generalizes “puzzle solving” into a formal reasoning benchmark. The multi-turn, non-leaking setup is novel and effectively mitigates data leakage issues present in traditional benchmarks.\n\n2.Comprehensive evaluation:\n\nThe authors evaluate 13 strong closed- and open-source models, providing detailed cross-domain analysis, behavior-level metrics (e.g., probing length, incorrect guesses), and ablation on environment agents.\n\n3.Insightful empirical finding:\n\nThe “reasoning-enhanced models perform worse” result is interesting and challenges current assumptions about chain-of-thought and reinforcement-based reasoning improvements."}, "weaknesses": {"value": "1.Lack of quantitative validation for benchmark reliability.\n\nWhile the authors describe a multi-stage filtering pipeline from Wikipedia, no data or statistics are provided to verify item accuracy, semantic clarity, or cross-domain balance.There is no human verification or sampling analysis to ensure that the puzzles are unambiguous, correctly labeled, or appropriately difficult. The claim of “doctoral-level difficulty” is purely procedural, not empirically supported.\n\n2.Unverified accuracy of the environment agent (“judge”).\n\nAll correctness feedback relies on DeepSeek V3 (or alternatives in ablation), but the paper does not report any accuracy or agreement score compared to human judgments. Even minor parsing or comprehension errors in the judge could propagate systematic noise, which may partially explain the reported rank inversions between “thinking” and “no-thinking” models.\n\n3.Interpretation of performance differences is speculative.\n\nThe authors attribute lower reasoning-model performance to “procedural reasoning hindering applied reasoning,” but provide no direct evidence.Alternative explanations (e.g., ambiguous items, noisy feedback, or overfitting to chain-of-thought formats) are not empirically ruled out."}, "questions": {"value": "1. Have you manually validated a subset of puzzles to quantify item accuracy or ambiguity?\n2. What is the estimated reliability (agreement) of the environment agent’s correctness feedback?\n3. Could the observed rank inversion remain if human-verified items were used?\n4. How sensitive are results to the choice of reference corpus (Wikipedia vs. domain textbooks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cpSUbrMBZg", "forum": "jjrzADWOD5", "replyto": "jjrzADWOD5", "signatures": ["ICLR.cc/2026/Conference/Submission10142/Reviewer_ka3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10142/Reviewer_ka3X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920427897, "cdate": 1761920427897, "tmdate": 1762921513743, "mdate": 1762921513743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the pervasive problem of data leakage in current LLM benchmarks, which the authors argue degrades evaluation from a test of genuine problem-solving ability to a mere assessment of recitation.\n\nTo solve this, the paper introduces a new evaluation framework: the Puzzle Everything Benchmark (PEB). PEB proposes a novel \"generalized puzzle-solving\" paradigm. It treats any specialized concept (e.g., \"Reticular Dysgenesis\") sourced from Wikipedia as a puzzle to be solved. The model being evaluated (the \"strategic agent\") must, through a multi-turn interactive process, pose a series of four-option multiple-choice questions to an \"environment agent\" to progressively narrow down the possibilities and finally identify the target concept.\n\nThe benchmark contains 960 puzzles of \"doctoral-level difficulty\" from 8 distinct disciplines. The paper's core finding is counter-intuitive: the vast majority of \"reasoning-enhanced\" open-source models (like DeepSeek R1, Qwen3 Thinking) perform significantly worse on PEB than their corresponding standard (no-thinking) versions. The authors argue this reveals a critical gap between a model's \"procedural reasoning\" capabilities and its ability to \"apply that reasoning to specialized, internal knowledge.\""}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "High-Impact Problem: The paper directly tackles one of the most critical and urgent problems in LLM evaluation: data contamination. As model training data explodes, designing leakage-resistant benchmarks to\ngenuinely assess reasoning is an unsolved challenge.\n\nNovel Methodology: The \"generalized puzzle-solving\" paradigm (Figure 2) is highly innovative. By forcing the model into a multi-turn, constrained (via 4-choice questions) information-seeking process, it makes it difficult to cheat by simply \"reciting\" an answer. This method compels the model to utilize its internal knowledge for reasoning, hypothesis generation, and validation, providing a more authentic assessment of knowledge-based reasoning.\n\nSurprising and Counter-Intuitive Finding: The paper's most significant contribution is the finding shown in Figure 1: models specifically optimized for \"thinking\" or \"reasoning\" (e.g., Qwen3 235B Thinking, DeepSeek R1) actually perform worse. This \"anomaly\" poses a serious challenge to the common assumption that \"CoT = better reasoning."}, "weaknesses": {"value": "The paper's core finding is that \"thinking\" models perform worse, which the authors attribute to a flawed reasoning strategy (Figure 5). However, is it possible that these models were optimized for long-form, unconstrained CoT reasoning, and are therefore ill-suited to PEB's constrained (must ask a 4-choice question), game-like evaluation? The evaluation paradigm itself may be a confounding variable."}, "questions": {"value": "Regarding the environment agent: In the ablation study (Appendix C, Figure 6), Claude Opus 4's performance spikes dramatically and anomalously (from ~30% -> 45%) when o3 (03) is used as the environment. Does this imply that the choice of environment agent has a larger (and non-stable) impact on the evaluation of specific models (like Opus 4) than anticipated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QE2QkaAXCj", "forum": "jjrzADWOD5", "replyto": "jjrzADWOD5", "signatures": ["ICLR.cc/2026/Conference/Submission10142/Reviewer_WaAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10142/Reviewer_WaAN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929750116, "cdate": 1761929750116, "tmdate": 1762921513393, "mdate": 1762921513393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the **Puzzle Everything Benchmark (PEB)**, a novel evaluation framework that addresses data leakage concerns in existing benchmarks by treating any concept, term, or entity as a solvable puzzle. The authors construct 960 puzzles of doctoral-level difficulty from Wikipedia across eight disciplines (Biology, Human Disease, Chemistry, Physics, Mathematics, Computer Science, Economics, and Philosophy). The evaluation protocol employs a multi-turn strategic assessment where models interact with an environment that provides only correctness feedback until they identify the target concept. The main finding reveals that reasoning-enhanced models (e.g., DeepSeek R1, Qwen3-235B-A22B-Thinking) significantly underperform their standard counterparts, with GPT-5 achieving the highest average success rate of 33.4%. The work proposes a scalable methodology for generating diagnostic benchmarks resistant to data contamination."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel approach to contamination-resistant evaluation**\n- The generalized puzzle concept enables assessment of reasoning over internalized knowledge without explicit memorization (Section 3.1; Abstract). This addresses a fundamental challenge in current benchmark validity.\n- Multi-turn strategic protocol requires synthesis and hypothesis refinement rather than direct retrieval (Section 3.1; lines 551-650). This matters for distinguishing genuine reasoning from memorization.\n2. **Comprehensive benchmark construction with rigorous methodology**\n- Multi-stage coarse-to-fine filtering pipeline from 6+ million Wikipedia articles to 960 doctoral-level puzzles ensures quality control (Figure 3; Section 3.2). This demonstrates systematic curation and technical rigor.\n- Detailed domain composition (120 puzzles per domain) with granular sub-field representation ensures balanced assessment (Section 3.2; Appendix F reference). This enhances experimental validity.\n- Eight distinct academic domains with 7-8 sub-disciplines each provides broad coverage across STEM and humanities (Figure 4; Section 3.2). This supports comprehensive evaluation scope.\n3. **Thorough experimental design and analysis**\n- Evaluation covers 13 leading models across closed and open-source families with consistent protocols (Section 4.1; Table 1). This ensures comprehensive assessment.\n- Detailed behavioral analysis including probing length and incorrect guess patterns provides mechanistic insights (Section 4.3; Table 2 reference). This enhances understanding of failure modes."}, "weaknesses": {"value": "1. **Insufficient analysis of mathematical formulations and evaluation metrics**\n- Success rate as the primary metric lacks nuanced assessment of partial progress or reasoning quality (Section 3.1; Table 1). The binary success/failure criterion may miss important gradations in reasoning capability.\n- No formal analysis of the multi-turn protocol’s statistical properties or convergence guarantees (Section 3.1). Mathematical rigor in evaluation design is limited.\n2. **Incomplete experimental coverage and baseline comparisons**\n- Missing comparisons with specialized domain-specific benchmarks or reasoning tasks to establish relative difficulty (throughout results). This affects positioning within existing evaluation landscape.\n- No analysis of model scaling effects or architectural differences beyond the thinking/no-thinking distinction (Table 1; Section 4.2). This limits insights into what drives performance differences."}, "questions": {"value": "What specific criteria define “doctoral-level difficulty” in your filtering pipeline, and how was this validated? Could you provide inter-rater reliability scores or expert validation studies for the difficulty assessments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ARQ04VxIpO", "forum": "jjrzADWOD5", "replyto": "jjrzADWOD5", "signatures": ["ICLR.cc/2026/Conference/Submission10142/Reviewer_YdhY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10142/Reviewer_YdhY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990903848, "cdate": 1761990903848, "tmdate": 1762921513063, "mdate": 1762921513063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}