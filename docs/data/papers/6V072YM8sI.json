{"id": "6V072YM8sI", "number": 4041, "cdate": 1757589837831, "mdate": 1763120689579, "content": {"title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency", "abstract": "With the rapid advancement of large multimodal models (LMMs), recent text-to-image (T2I) models can generate high-quality images and demonstrate great alignment to short prompts. However, they still struggle to effectively understand and follow long and detailed prompts, displaying inconsistent generation. To address this challenge, we introduce LPG-Bench, a comprehensive benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench features 200 meticulously crafted prompts with an average length of over 250 words, approaching the input capacity of several leading commercial models. Using these prompts, we generate 2,600 images from 13 state-of-the-art models and further perform comprehensive human-ranked annotations. Based on LPG-Bench, we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor consistency with human preferences on long-prompt-based image generation.\nTo address the gap, we introduce a novel zero-shot metric based on text-to-image-to-text consistency, termed TIT, for evaluating long-prompt-generated images. The core concept of TIT is to quantify T2I alignment by directly comparing the consistency between the raw prompt and the LMM-produced description on the generated image, which includes an efficient score-based instantiation TIT-Score and a large-language-model (LLM) based instantiation TIT-Score-LLM. Extensive experiments demonstrate that our framework achieves superior alignment with human judgment compared to CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT methods together offer a deeper perspective to benchmark and foster the development of T2I models. All resources will be made publicly available.", "tldr": "Evaluating Long-Prompt Alignment in Text-to-Image Models", "keywords": ["Text-Image", "LMM", "AIGC", "prompt", "Embedding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/700ba32a04561dfe842d7fcda14201d71c7ff9ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LPG-Bench, a benchmark with 200 long prompts for evaluating text-to-image models, and proposes TIT-Score, a “text-to-image-to-text” framework that converts a generated image back to text and then measures semantic similarity with the original prompt."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LPG-Bench is relatively large and well-documented, with manual refinement and human preference annotations.\n\n2. the proposed metric correlates well with human judgment and outperforms common baselines."}, "weaknesses": {"value": "1. Novelty and conceptual depth are limited: The TIT-Score framework is essentially a straightforward pipeline combining an existing captioning VLM + text embedding similarity, a known idea explored in prior “image caption consistency” or “caption-re-caption” works (e.g., CLIP-I2T, BLIP-I2T).\n\n2. No discussion on domain coverage (e.g., scene diversity, cultural context, abstract concepts).\n\n3. The paper overlooks some directly relevant prior works on long or paragraph-level text-to-image generation, such as ParaDiffusion, which not only tackles long, information-rich text inputs but also includes its own long-prompt evaluation. Ignoring these studies weakens the novelty and positioning claims, as TIT-Score should be compared or at least discussed alongside such baselines.\n\n4. Lack of generalization analysis or simple disscusion(e.g., shorter prompts, cross-domain tests, multilingual prompts).\n[1] Paragraph-to-Image Generation with Information-Enriched Diffusion Model"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pnqxgZP45I", "forum": "6V072YM8sI", "replyto": "6V072YM8sI", "signatures": ["ICLR.cc/2026/Conference/Submission4041/Reviewer_zgR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4041/Reviewer_zgR9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492018115, "cdate": 1761492018115, "tmdate": 1762917147892, "mdate": 1762917147892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "nmO9VeTJTG", "forum": "6V072YM8sI", "replyto": "6V072YM8sI", "signatures": ["ICLR.cc/2026/Conference/Submission4041/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4041/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120687955, "cdate": 1763120687955, "tmdate": 1763120687955, "mdate": 1763120687955, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LPG-Bench, a benchmark specifically designed to evaluate text-to-image (T2I) generation for long prompts (averaging 250+ words). To address the limitations of existing metrics like CLIP-Score on these long prompts, the authors propose TIT-Score (Text-to-Image-to-Text). This \"decoupled\" framework first uses a VLM to caption the generated image, then measures the semantic similarity between this new caption and the original long prompt using either a text embedding model (TIT-Score) or an LLM (TIT-Score-LLM)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark addresses a clear gap in current evaluation by focusing specifically on very long, narrative-style prompts (>250 words), which approach the context limits of modern commercial models.\n\n- The \"describe-then-compare\" approach is a logical attempt to handle long contexts, avoiding the noise inherent in asking a VLM to directly score a complex image-text pair end-to-end.\n\n- The authors provide a good ablation study demonstrating that end-to-end \"LMM Scoring\" performs poorly on these long prompts, justifying their decoupled approach."}, "weaknesses": {"value": "- One of the main contributions of this work is proposing a new metric for evaluating text-to-image generation. However, the paper fails to cite or compare against established question-decomposition metrics that actually do what they seemed to incorrectly ascribe to VQAScore. Key missing baselines include:\n\n  - TIFA [1]: A standard metric that generates specific questions from the prompt to verify details.\n\n  -  DSG  [2]: Another prominent metric that uses scene graphs to generate fine-grained evaluation questions.\n\n  - Gecko [3]: A recent benchmark that also utilizes diverse QA pairs for evaluation. Omitting these standard, directly relevant \"decompose-and-verify\" metrics while mischaracterizing VQAScore is a severe flaw in evaluating the state of the art.\n\n- The authors state that \"VQA-Score... verify factual details by decomposing prompts into questions\" (Section 2.3). This is factually incorrect. As per Eq. 1 of the VQAScore paper, this metric typically computes the probability of a \"Yes\" answer given the entire prompt as a single question (e.g., \"Does this image match the text: [prompt]?\"), specifically to avoid the decomposition they claim it uses. This fundamental misunderstanding of a key baseline calls into question the rigor of their literature review and baseline comparisons.\n\n- The TIT-Score metric hinges on the VLM's ability to perfectly capture a 250-word prompt in a single caption. This introduces a severe information bottleneck. If the VLM omits a detail (common in dense images) or hallucinates elements, the subsequent text similarity score is fundamentally flawed. The paper fails to account for standard VLM captioning biases or at least providing some empirical evidence that this issue won’t happen in practice. \n\n- Lack of reporting critical details on human evaluation: While Appendix A.11 details a rigorous-sounding mechanism for aggregating human judgments, it fails to report how many of the 12,832 pairs fell into the \"High Disagreement\" category requiring expert arbitration. Without these statistics, it is impossible to gauge the actual reliability of the human ground truth.\n\n\n[1] Hu et al. TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering, ICCV, 2023\n\n[2] Cho et al., Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation, ICLR 2024\n\n[3] Wiles et al., Revisiting text-to-image evaluation with gecko: On metrics, prompts, and human ratings, ICLR 2025"}, "questions": {"value": "- Can you clarify exactly how you implemented VQAScore? Your description in Section 2.3 (\"decomposing prompts into questions\") seems to contradict the standard implementation of that specific metric.\n\n-  In Appendix A.11, you define three categories for aggregating human judgments. What percentage of your 12,832 pairs fell into the \"High Disagreement\" category requiring expert arbitration? Can you report standard metrics such as Krippendorff’s alpha reliability?\n\n- How does TIT-Score distinguish between a model failing to generate a detail and the VLM simply failing to caption that detail? Are there ways to prevent VLM hallucinations from distorting the final score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0DTG2OzG7f", "forum": "6V072YM8sI", "replyto": "6V072YM8sI", "signatures": ["ICLR.cc/2026/Conference/Submission4041/Reviewer_erxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4041/Reviewer_erxp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585920077, "cdate": 1761585920077, "tmdate": 1762917147600, "mdate": 1762917147600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LPG-Bench, a benchmark with 200 long prompts (average >250 words) and 2,600 images from 13 SOTA T2I models with human rankings, to address the challenge of evaluating long-prompt image generation. The authors propose TIT (Text-to-Image-to-Text consistency), a zero-shot evaluation framework that converts generated images back to text via VLMs and measures semantic consistency with the original prompt. TIT-Score-LLM achieves superior alignment with human judgments, improving pairwise accuracy by 7.31% over the strongest baseline, while providing efficient and near-SOTA performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LPG-Bench is the first comprehensive benchmark for evaluating T2I model consistency with long prompts, comprising 200 curated prompts (average >250 words) and 2,600 images from 13 SOTA models with 12,832 human pairwise comparisons. It validates that long text understanding is a critical bottleneck for T2I systems and provides a solid evaluation foundation.\n\n2. TIT (Text-to-Image-to-Text consistency) decouples cross-modal evaluation into two stages: (1) converting images to text descriptions via VLMs, and (2) computing semantic alignment between original prompts and descriptions. Offering two implementations—TIT-Score (embedding-based, efficient) and TIT-Score-LLM (LLM-based, accurate)—TIT achieves superior alignment with human judgments, with TIT-Score-LLM reaching 66.51% pairwise accuracy, a 7.31% absolute improvement over the strongest baseline, while providing near-SOTA performance with high efficiency."}, "weaknesses": {"value": "1. While the proposed metric is reasonable, the fundamental motivation warrants scrutiny. Current user inputs are typically brief; when prompts are extended, LLMs typically handle expansion. In such scenarios, the LMM's expansion capability becomes the critical factor, not long-text comprehension per se. Does evaluating long-prompt consistency actually reflect real-world usage patterns and needs?\n2. Although LPG-Bench is proposed, the work provides insufficient analysis of underlying causes. Why do certain models perform poorly on long prompts? The contribution amounts to benchmark construction and metric validation without deeper investigation or innovation. The paper lacks diagnostic insights into model failures, architectural limitations, or systematic factors affecting long-text understanding in T2I systems. A more compelling contribution would include root-cause analysis or design principles for improving long-prompt generation."}, "questions": {"value": "What insights can LPG-Bench offer for subsequent model optimization? Why do we need to consider long text capabilities when user input is short text or text modified by LMM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D25xln2TMX", "forum": "6V072YM8sI", "replyto": "6V072YM8sI", "signatures": ["ICLR.cc/2026/Conference/Submission4041/Reviewer_2fr7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4041/Reviewer_2fr7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962878171, "cdate": 1761962878171, "tmdate": 1762917147354, "mdate": 1762917147354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LPG-Bench, a benchmark aimed at evaluating text-to-image (T2I) models on long, detailed prompts. It comprises 200 prompts averaging >250 words, from which the authors generate 2600 images across 13 image generation models and collect human preference annotations. The study finds that common metrics (e.g., CLIP-score, LMM-score) correlate poorly with human judgments for long-prompt generation. To close this gap, the authors propose TIT (TIT-Score and TIT-Score-LLM) that compare the raw prompt to an LMM-generated description of the produced image. Experiments show TIT aligns better with human preferences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good results: the experiment results show the alignment between TIT and human preferences largely outperform baselines. \n - Clear presentation: the plots (e.g. Fig 3), tables (e.g. Table 2) clearly explained the effectiveness of LPG-Bench and TiT."}, "weaknesses": {"value": "- Dataset scale: 200 prompts may be small for a general-purpose benchmark.\n - Bias from LLM/LMM: TIT relies on an LMM to describe images, which might inject bias into the evaluation results."}, "questions": {"value": "- Failure cases: it’ll be great if authors could qualitatively show some failure cases where TIT disagrees with humans, and discuss why TIT will fail on these use cases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PfHb3QiUEF", "forum": "6V072YM8sI", "replyto": "6V072YM8sI", "signatures": ["ICLR.cc/2026/Conference/Submission4041/Reviewer_ozba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4041/Reviewer_ozba"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968868736, "cdate": 1761968868736, "tmdate": 1762917146836, "mdate": 1762917146836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}