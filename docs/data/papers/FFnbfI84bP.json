{"id": "FFnbfI84bP", "number": 13715, "cdate": 1758221459435, "mdate": 1759897417654, "content": {"title": "FairMedQA: Benchmarking Bias in Large Language Models for Medicine and Healthcare", "abstract": "Large language models (LLMs) are reaching expert-level accuracy on medical diagnosis questions, yet their underlying biases pose life-critical risks. Bias linked to race, sex, and socioeconomic status is well documented in clinical settings, but a consistent, automatic testbed and a large-scale empirical study across models and versions remain missing. To fill this gap, we present FairMedQA, a benchmark for evaluating bias in medical QA via adversarial testing. FairMedQA contains 4,806 adversarial descriptions and counterfactual question pairs generated from a multi-agent framework sourced from 801 clinical vignettes of the United States Medical Licensing Examination (USMLE) dataset. Using FairMedQA, we benchmark 12 representative LLMs and observe substantial statistical parity differences (SPD) between the counterfactual pairs across models, ranging from 3 to 19 percentage points. Compared with the existing CPV benchmark, FairMedQA reveals 15\\% larger average accuracy gaps between privileged and unprivileged groups. Moreover, our cross-version analysis shows that upgrading from GPT-4.1-Mini to GPT-5-Mini significantly improves accuracy and fairness simultaneously. These results demonstrate that LLMs' performance and fairness in medicine and healthcare are not inherently a zero-sum trade-off, while ``win–win'' outcomes are achievable.", "tldr": "We introduce FairMedQA and use it benchmark medical bias in LLMs cross models and versions", "keywords": ["LLM Bias", "Medical QA", "Adversarial Testing", "Bias Benchmark"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75e97ee5350db5db67048c904e7d4a7ceadbaceb.pdf", "supplementary_material": "/attachment/0248c32368ad3da61705594a5fc755ba3eb7435d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents FairMedQA, a new benchmark for evaluating bias in medical LLMs using adversarial questions. Generated via a multi-agent framework from USMLE vignettes, it tests 12 LLMs, revealing significant biases. The study also shows that model upgrades can simultaneously improve both accuracy and fairness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* **Novel Benchmark:** Introduces FairMedQA, a scalable, automated benchmark for detecting medical bias using adversarial testing.\n* **Multi-Agent Generation:** A novel multi-agent pipeline (Generate, Fuse, Evaluate) creates high-quality, clinically consistent adversarial variants.\n* **Comprehensive Evaluation:** Provides a large-scale empirical study of bias across 12 different LLMs and 6 model versions."}, "weaknesses": {"value": "* **Limited/unclear human oversight:** The most critical aspect of the work involves the manual evaluation of the QA pairs with humans. Among the team of four individuals, only two have medical training (with unspecified credentials). Details of this process are minimally discussed, especially with some numerical results (like indicating the cross-reviewer agreements). \n* **Simplified Demographics:** Bias is tested using limited binary categories for race, sex, and socioeconomic status.\n* **Generation Model Dependency:** The quality of adversarial variants depends on the foundation models (like GPT-4O) used for generation."}, "questions": {"value": "- Did each individual review all of the 4,806 vignette pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LDIiUvBkyx", "forum": "FFnbfI84bP", "replyto": "FFnbfI84bP", "signatures": ["ICLR.cc/2026/Conference/Submission13715/Reviewer_W5JX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13715/Reviewer_W5JX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922725021, "cdate": 1761922725021, "tmdate": 1762924261730, "mdate": 1762924261730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FairMedQA, a counterfactual/adversarial benchmark for measuring demographic biases of LLMs on medical multiple-choice QA. The authors construct 4,806 adversarial vignette pairs derived from 801 USMLE-style questions by neutralizing demographic cues and then inserting adversarial demographic descriptions (race, sex, socioeconomic status) via a multi-agent LLM pipeline plus human review. They evaluate 12 representative models (proprietary and open-source), measure counterfactual fairness rate (CFR) and statistical parity difference (SPD), compare FairMedQA to an existing CPV baseline, and perform cross-version analyses showing that newer model versions (e.g., GPT-5 vs GPT-4.1) often improve both accuracy and fairness. Major empirical findings: FairMedQA elicits larger bias gaps than CPV, SPD across models ranges ~0.03–0.19, CFR varies by model (0.71–0.94), and upgrades frequently lead to “win–win” gains in accuracy and fairness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Careful dataset construction: multi-stage pipeline (Generation/Fusion/Evaluation agents) plus manual QC yields a large set of plausible counterfactual vignette pairs while limiting clinical-relevance leakage.\n\n* Scale and breadth: 4,806 adversarial variants across three sensitive attributes evaluated on 12 diverse models and multiple version pairs; cross-version analysis is especially informative."}, "weaknesses": {"value": "* The narrow task format (multiple choice) limits external validity: real clinical interactions are open-ended, contextual, and multimodal; results may not generalize to diagnosis dialogue or real EHR notes. The paper should more explicitly qualify this limitation (beyond the short Limitations section) and, if feasible, include a small pilot on one open-ended or free-text clinical dataset to show whether similar bias patterns appear. \n\n* The scope of experiments is limited, focusing primarily on multiple-choice QA and three demographic dimensions (race, sex, SES) across 12 models. While these experiments are thorough within this scope, further analysis, such as existing fairness-enhanced tuning/prompting on medical scenarios, could provide more actionable insights for future works."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GsjTempiKf", "forum": "FFnbfI84bP", "replyto": "FFnbfI84bP", "signatures": ["ICLR.cc/2026/Conference/Submission13715/Reviewer_haDW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13715/Reviewer_haDW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762824887154, "cdate": 1762824887154, "tmdate": 1762924261410, "mdate": 1762924261410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FairMedQA, a benchmark aimed at evaluating fairness in medical QA systems. The authors construct counterfactual variations of USMLE questions by adding demographic background information and altering sensitive attributes to assess how model predictions change. Experiments on 12 LLMs show that the dataset can reveal larger fairness gaps than previous work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and timely topic in medical AI fairness. The idea of enriching counterfactuals with contextual demographic information rather than simple attribute substitution is reasonable. The evaluation covers multiple model families and versions, providing some useful observations."}, "weaknesses": {"value": "**Major Concerns:**\n\nThe overall contribution appears limited. The core idea is to take existing medical QA questions, add demographic background, and flip basic sensitive attributes to create synthetic counterfactual data. This feels like a rather straightforward application of fairness testing techniques to the medical QA domain and does not demonstrate substantial novelty.\n\nThe distinction from prior work is not clearly established. After reading the related work, it is still unclear how this benchmark meaningfully differs from existing fairness datasets, for example FMBench. The paper claims stronger bias triggering capability, but the conceptual and methodological differences are not clearly argued. As a result, the contribution feels incremental.\n\n**Other Concerns:**\n\n- The human evaluation process is not described in sufficient detail. Although the paper mentions four annotators and reports agreement levels, the protocol itself is unclear. It is not stated how the annotation was carried out, whether a guideline was used, how disagreements were resolved, and whether any annotation interface or tool supported the process. \n\n- The interpretation of the results regarding performance and fairness improvement seems overstated. The paper suggests that model upgrades can deliver simultaneous gains in accuracy and fairness. This should be described more cautiously, as the current findings only show such a pattern for the partial tested models. It would be more accurate to say that this was observed, rather than implying a broader trend.\n\n- The demographic design is too simplified. The benchmark only considers White versus Black, Male versus Female, and High versus Low socioeconomic status. Such binary groupings oversimplify real demographic diversity, especially in a medical context. More nuanced or non binary demographic attributes would improve realism and external validity. \n\n- The composition and roles of the annotators raise questions. Two of the annotators have an AI background rather than a medical background. It is unclear how these annotators ensured that clinical correctness."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hWGb7PRMUA", "forum": "FFnbfI84bP", "replyto": "FFnbfI84bP", "signatures": ["ICLR.cc/2026/Conference/Submission13715/Reviewer_cWHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13715/Reviewer_cWHL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762840410566, "cdate": 1762840410566, "tmdate": 1762924261111, "mdate": 1762924261111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}