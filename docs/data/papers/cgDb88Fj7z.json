{"id": "cgDb88Fj7z", "number": 4331, "cdate": 1757663909517, "mdate": 1759898039171, "content": {"title": "Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures", "abstract": "Sparse Mixture of Experts (SMoE) has emerged as a promising solution to achieving unparalleled scalability in deep learning by decoupling model parameter count from computational cost. By activating only a small subset of parameters per sample, SMoE enables significant growth in model capacity while maintaining efficiency. However, SMoE struggles to adapt to distributional shifts, leading to reduced robustness under data contamination. In this work, we introduce SymphonySMoE, a novel family of SMoE that introduces a social graph to model interactions among experts. This graph-based structure enhances the token routing process, addressing the robustness challenges that are inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular, and integrates seamlessly with existing SMoE-based models such as the XMoE and the Generalist Language Model. We provide both theoretical analysis and empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE. Extensive experiments on language modeling and visual instruction tuning validate our method's effectiveness. We further highlight the scalability of SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its applicability in fine-tuning tasks for large-scale systems.", "tldr": "We introduce SymphonySMoE, a novel family of SMoE that introduces a social graph to model interactions among experts.", "keywords": ["Sparse Mixture of Experts", "expert graph", "robustness"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b22849b6598ddb604c9ea360fb1bc2d97eb5a211.pdf", "supplementary_material": "/attachment/45211779f746e7781c9a5751d75aff1c80be7d17.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SymphonySMoE, an extension of Sparse Mixture of Experts (SMoE). The paper first views routing as a probabilistic graphical model. Next, it incorporates a social graph to model interactions between experts and adjusts the SMoE gating values accordingly. This approach seeks to enhance token routing by promoting the co-selection of expert pairs with high-confidence activations. The authors present a theoretical analysis and empirical evaluation across language modeling (WikiText-103), visual instruction tuning (LLaVA), and GLUE fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength is the introduction of a novel social graph framework for modeling expert-to-expert interactions within  SMoE system.\n- The paper also presents a strong theoretical analysis that rigorously formalizes the co-selection properties of experts.\n- Extensive empirical evaluation on several models and multiple domains."}, "weaknesses": {"value": "- The paper's probabilistic graphical model is primarily conceptual; the practical method simply uses an adjacency matrix to smooth gating scores, with the PGM adding no material impact to the final routing.\n- The paper does not benchmark against other recent advanced routing strategies (see [1] for a list of possible baselines). \n- Across most benchmarks, the reported improvements are modest (e.g., 1–3% absolute in some multimodal tasks, ~0.5–1 perplexity drop in WikiText-103). Without a rigorous statistical significance analysis or evaluation on more challenging datasets, such as mathematical reasoning, it is difficult to conclude that the improvements are meaningful in practice.\n- The GLUE experiments are limited to Phi3-SMoE with top-2 selection among 4 experts, which again does not stress test the method’s scalability to larger, more realistic SMoE architectures.\n- While the adjacency matrix update is claimed to be lightweight, according to the complexity analysis in Table 5 for large N (e.g., long sequences) or large M, this could become non-negligible.\n\n[1] Do et al. \"On the Effectiveness of Discrete Representations in Sparse Mixture of Experts.\", TMLR 2025."}, "questions": {"value": "- How sensitive is the method to the way the adjacency matrix is constructed (e.g., co-activation frequency, normalization, smoothing)?\n- What are the results with more experts in the GLUE benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cNj2QH7VtY", "forum": "cgDb88Fj7z", "replyto": "cgDb88Fj7z", "signatures": ["ICLR.cc/2026/Conference/Submission4331/Reviewer_Tv3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4331/Reviewer_Tv3D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761192736334, "cdate": 1761192736334, "tmdate": 1762917302401, "mdate": 1762917302401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SymphonySMoE, a novel MoE routing mechanism that incorporates experts' co-selection information into the routing decision. \nThe authors frame the routing process in MoE as a graphical model and provide a theoretical analysis of their design. \nThey conduct experiments to demonstrate that this routing design enables MoE to adapt to distributional shifts, leading to more robust routing. \nThis approach presents an interesting perspective to MoE routing design, though further empirical validation and improved paper presentation would enhance the overall impact of the paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of using experts' co-occurrence information to provide a smoothing signal for MoE routing is novel. To the best of my knowledge, there is little prior work in this area, making it an interesting contribution.\n\n- The authors provide a therotical analysis to support their idea."}, "weaknesses": {"value": "- The authors' claim that SymphonySMoE addresses the distributional shifts in traditional MoEs lacks a logical foundation.\nI do not see, nor can I understand, any motivation linking SymphonySMoE to this concept of robustness throughout the paper.\nI understand that SymphonySMoE uses the mutual information between experts to help MoE routing, but the connection between this mechanism and robustness is unsubstantiated, lacking proper explanation and empirical validation.\n\n-  Some of the author's claims are not adequately supported by experimental evidence, as the experiments suffer from significant setup issues.\n\nI question the validity of the authors' claims (i), (ii), and (iv) in the overview of Section 4.\n\nRegarding claim (i) that \"SymphonySMoE enhances model performance across both pre-training and fine-tuning tasks,\" I have the following concerns:\n\n(1) In Section 4.1, the authors train a MoE model with a total of 200M parameters from scratch on only **100M** tokens (WikiText-103) and report this as a **pre-training** task. \nIt is difficult to draw convincing conclusions from such a limited **100M** token **\"pre-training\"** experiment and believe it can justify a new MoE routing strategy. \nCould the observed results simply be due to SMoE enabling faster convergence?\n\nFurthermore, I cannot accept the results of a language model **without any pre-training** on the attacked dataset as sufficient evidence to support the claim that SymphonySMoE is more robust. (claim (ii))\n\n(2) I do not consider the experiments in Sections 4.1 and 4.2 as fine-tuning tasks, as there is **no** pre-trained MoE model involved. \nThese experiments are conducted with MoE initialized from the upcycled dense model, without any further training. \nSimilar to point (1), I find it difficult to accept conclusions drawn from tuning a newly initialized MoE with billions of parameters on such a limited dataset.\nAs a result, this also fails to support the conclusions of claim (iv).\n\n- The presentation of this paper could be improved. Most theoretical proofs in the main text do not focus on addressing the problem this paper try to resolve and could be moved to the appendix, while some key experimental details that support the effectiveness of the approach are placed there instead."}, "questions": {"value": "Q1: Can the authors conduct experiments pre-training language models with more tokens and a bigger model scale?\n\nFor instance, pre-train the language model with 80B tokens, similar to the setup in Appendix E.1.\n\nQ2: Can the authors provide an explanation for their choice to upcycle dense models into MoE in the experiments presented in Sections 4.2 and 4.3?\nWhat's the performance of continual pre-training performance of an MoE model into SymphonySMoE in the same experimental setup?\n\nQ3: What's the performance of the fine-tuned dense Phi-3 mini's performance on GLUE?\n\nCould the authors consider testing on other benchmarks, as GLUE may not fully capture the capabilities of modern LLMs?\nAs a kind reminder, the statement \"this setup reflects a realistic deployment scenario\" seems somewhat overstated, as modern MoEs are clearly much sparser in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ff0yJeev9J", "forum": "cgDb88Fj7z", "replyto": "cgDb88Fj7z", "signatures": ["ICLR.cc/2026/Conference/Submission4331/Reviewer_gs8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4331/Reviewer_gs8E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657345007, "cdate": 1761657345007, "tmdate": 1762917302113, "mdate": 1762917302113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SymphonySMoE, a novel framework for improving the robustness and interpretability of Sparse Mixture of Experts (SMoE) models by explicitly modeling expert-to-expert interactions through a graph structure.\nTraditional SMoE frameworks select top-K experts independently for each token, which leads to unstable routing under distribution shift or noisy inputs. SymphonySMoE addresses this by constructing a social graph among experts, where edges represent co-activation frequency. This graph is dynamically updated via exponential moving average and used to smooth routing logits during expert selection.\nExperiments on large-scale benchmarks—including WikiText-103 (language modeling), GLUE (text classification), and LLaVA-665K (vision-language instruction tuning)—demonstrate consistent performance gains over strong SMoE baselines (e.g., X-MoE, GLaM, Switch Transformer), particularly under noisy or adversarial conditions.\nThe paper further provides theoretical analysis showing that the learned adjacency matrix converges to an ideal co-activation measure, explaining the enhanced routing stability."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces a new perspective on SMoE routing, framing it as a graph-based probabilistic inference problem that models dependencies among experts, rather than treating expert activations as independent.\nThe concept of a “social graph of experts” is both intuitively appealing and technically original, bridging ideas from graph neural networks, probabilistic modeling, and mixture-of-experts learning.\nIt offers a lightweight and modular extension that can be integrated into existing SMoE frameworks with minimal architectural modification.\nThe method is mathematically well-motivated and empirically validated across multiple modalities (text, vision-language).\nExperiments are comprehensive, ablation studies isolate the impact of graph modeling, and robustness tests under data corruption demonstrate practical benefits.\nTheoretical analysis provides a convergence guarantee for the adjacency matrix, which strengthens the credibility of the approach."}, "weaknesses": {"value": "The paper reports stable gains on multiple benchmarks (such as table results in the directions of WikiText-103, GLUE, and LLaVA), and conducts a detailed complexity/runtime analysis of the overhead, but does not characterize the theoretical or empirical upper limit of Symphony routing: How far is the current improvement from the \"ideal route\", under what conditions will it reach its peak, and where will the diminishing returns occur?"}, "questions": {"value": "1.How sensitive is model performance to the EMA decay rate in updating the adjacency matrix? Would a fully learnable adjacency (trained via gradient) perform better or risk overfitting?\n2.The experiments show improvement on text and vision-language tasks—does the method generalize similarly to purely visual MoE models (e.g., ViT-MoE) or speech experts?\n3.Could the authors provide quantitative metrics for “expert interaction strength” or visualize how the graph evolves across training stages? This might better substantiate the social-graph analogy.\n4.Please answer Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HNm56CfFH7", "forum": "cgDb88Fj7z", "replyto": "cgDb88Fj7z", "signatures": ["ICLR.cc/2026/Conference/Submission4331/Reviewer_kqhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4331/Reviewer_kqhn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710672724, "cdate": 1761710672724, "tmdate": 1762917301722, "mdate": 1762917301722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work enhances the expert routing process in SMoE by incorporating the experts co-activation information. The proposed method, SymphonySMoE, construct an a social graph (co-occurrence matrix) to model the co-activation frequency of experts during training and modify the routing process. The authors provided theoretical analysis of SymphonySMoE and validate its efficacy on several scenarios, from pre-training to fine tuning and visual instruction tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of incorporating expert co-activation frequency is well-motivated.\n- SymphonySMoE is quite elegant. Despite its simple implementation, it is theoretically-grounded and the empirical results are encouraging."}, "weaknesses": {"value": "- My major concern of this work is the empirical evaluation is quite limited. \n    - First, the pre-training experiment is very small. Training ~220M models on WikiText-103 is quite limited. Furthermore, evaluation is also on the same dataset, such in-domain evaluation is not used in modern SMoE settings, most of which focus on zero-shot evaluation. A minimum scale for pre-training should be MoEUT [A] or preferably OLMoE [B].\n    - Second, finetuning Phi 3 on Glue seems to be unnecessary as it is a very old benchmark and Phi 3 is likely to see the data during its pre-training. For this experiment, it is mandatory to report the original Phi 3 performance, and also consider challenging benchmarks like SuperGlue. Preferably, the authors should consider finetuning on more recent datasets like OpenCodeInstruct [C], or even doing RLHF.\n    - Lastly, the visual instruction tuning experiment followed LibMoE, which reported 11 benchmarks, why did the authors only consider 7?\n- Some presentation/typos/citation errors at L128, L139, L156, L161, etc. Table 1 appears too early before it was first mentioned. \n\n[A] Csordás, Róbert, et al. \"Moeut: Mixture-of-experts universal transformers.\" Advances in Neural Information Processing Systems 37 (2024): 28589-28614.\n\n[B] Muennighoff, Niklas, et al. \"Olmoe: Open mixture-of-experts language models.\" arXiv preprint arXiv:2409.02060 (2024).\n\n[C] Ahmad, Wasi Uddin, et al. \"OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs.\" arXiv preprint arXiv:2504.04030 (2025)."}, "questions": {"value": "- The empirical evaluation of SymphonySMoE is quite limited.\n\n- It is nice to see that the overheads during evaluation is minimal. What is the wall clock training time of SymphonySMoE compared to the baselines?\n\n- The number of baselines considered in all experiments is quite limited. The authors should try to include more recent baselines such as MoEUT, Autonomy-of-Experts Models [D], etc.\n\n[D] Lv, Ang, et al. \"Autonomy-of-Experts Models.\" ICML (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QBMlnDgLrj", "forum": "cgDb88Fj7z", "replyto": "cgDb88Fj7z", "signatures": ["ICLR.cc/2026/Conference/Submission4331/Reviewer_ZsKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4331/Reviewer_ZsKP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848868543, "cdate": 1761848868543, "tmdate": 1762917301485, "mdate": 1762917301485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}