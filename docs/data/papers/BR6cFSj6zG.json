{"id": "BR6cFSj6zG", "number": 9846, "cdate": 1758143577612, "mdate": 1759897692110, "content": {"title": "Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints", "abstract": "We present an algorithm guaranteeing dynamic regret bounds for online omniprediction with long term constraints. The goal in this recently introduced problem is for a learner to generate a sequence of predictions which are broadcast to a collection of downstream decision makers. Each decision maker has their own utility function, as well as a vector of constraint functions, each mapping their actions and an adversarially selected state to reward or constraint violation terms.  The downstream decision makers select actions ``as if'' the state predictions are correct, and the goal of the learner is to produce predictions such that all downstream decision makers choose actions that give them worst-case utility guarantees while minimizing worst-case constraint violation. Within this framework, we give the first algorithm that obtains simultaneous \\emph{dynamic regret} guarantees for all of the agents --- where regret for each agent is measured against a potentially changing sequence of actions across rounds of interaction, while also ensuring vanishing constraint violation for each agent. Our results do not require the agents themselves to maintain any state --- they only solve one-round constrained optimization problems defined by the prediction made at that round.", "tldr": "", "keywords": ["omniprediction", "online learning", "dynamic regret", "long-term constraints"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1496078640c036564d34bc628dd910d6c7c0343.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies a recently introduced setting in the field of online learning with unknown constraints, that is, online omniprediction with long term constraints. In such a setting, for every round $t\\in[T]$, an adversary selects a feature vector and a distribution over an outcome space. Then, a forecaster (namely, the learner) observes at each round the adversarially generated feature vector $x_t$ and then outputs a prediction $p_t$ on the outcome space (notice that, both the utility and costs of the environment at time $t$ depend on the real outcome). Then, the prediction $p_t$ is observed by $\\mathcal{N}$ agents which select an action accordingly. The goal is to minimise the violation and the regret (in this paper, not only fixed benchmarks are studied, but also dynamic ones) over the $T$ rounds for all the agents. The authors propose an algorithm for the forecaster, which employs existing techniques in order to output  ”unbiased” predictions for the agents. Thus, the agents select the action optimising greedily on the per-round reward and constraints computed given $p_t$. The algorithm attains sublinear regret and violation which depends on the various benchmarks employed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I believe that this work may hide some interesting results on learning against dynamic benchmarks which can go beyond the setting proposed in the paper. Nonetheless, the presentation does not allow to clearly recognise them. Indeed, there are several weaknesses that worsen the impact of the work."}, "weaknesses": {"value": "I have many concerns on both the presentation and the significance of the results presented in the work. Specifically,\n\n1. It is not clear to me which are the technical contributions of this work. Given the current presentation of the work, the main contribution lies in extending Behaved et al. (2025) to dynamic benchmarks. It is not clearly discussed how the techniques of the two papers differ. Moreover, from a technical perspective, it seems to me that the algorithm of the forecaster is an adaptation of Noarov et al. (2023), while the agents simply optimises greedily. Thus, I do not see any particular algorithmic novelty and the fact that the algorithm is deferred to the Appendix does not help in this sense.\n 2. Similarly, the authors could better highlight whether the bounds obtained against dynamic benchmark have been already provided for the standard setting of online learning with unknown constraints or not.\n 3. The current version of both the introduction and the preliminaries makes the setting not particularly easy to understand, even for readers familiar with the online learning with constraints literature. For instance, if I understood properly, the paper focuses on discrete action spaces. But as far as I see, it is specified only at line 56. Similarly, I assume that the feature space is somehow related to the outcome one, since the forecaster has to predict over outcomes observing first a feature vector. Nonetheless, I do not see any discussion on this in the preliminaries. \n 4. I have some concerns on the setting itself. To me, the main difficulty of this setting is to predict a good $p_t$ (or $\\pi_t$) at each round, since both the per round utility and costs of the agents are directly and deterministically estimated given $p_t$. Thus, I do not understand how the existence of unknown constraints should make this particular setting more interesting or challenging. The goal would be predicting a good estimate of the per round outcome even in unconstrained settings.\n 5. Another concern is about the benchmarks employed in the work. It seems to me that many of the bounds are computed with respect to solutions which are “strictly feasible at each round”. In footnote 2 the authors state that it is standard to assume Slater’s condition in the literature. I agree with that, but assuming that a strictly feasible solution exists (as in Slater) is very different from using strictly feasible solutions as benchmarks in the regret. \n 6. Finally, as a minor comment, the presentation could be improved in many ways throughout the paper. For instance, the equations clearly go beyond the template margins."}, "questions": {"value": "Please see weaknesses. I will be happy to reconsider my score if my concerns are properly tackled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mCA4fNYT0K", "forum": "BR6cFSj6zG", "replyto": "BR6cFSj6zG", "signatures": ["ICLR.cc/2026/Conference/Submission9846/Reviewer_ivPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9846/Reviewer_ivPL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760891556055, "cdate": 1760891556055, "tmdate": 1762921321851, "mdate": 1762921321851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all reviewers"}, "comment": {"value": "We thank all of the reviewers for their careful reading and thoughtful comments. We respond to individual reviewer questions below. Here we want to make a couple of global comments about the novelty of our work and its relationship to the existing learning with long term constraints literature.\n\nThe most important thing to emphasized is:\n\n1. The learning with long term constraints literature focuses on single decision makers, and derives algorithms they can use that offer them various individual guarantees. Our paper studies a different, more difficult problem: how a single learner who offers ``predictions as a service'' can serve predictions that can simultaneously be consumed by many downstream decision makers, who each have different objectives and constraints, in simple ways that nevertheless offer them strong guarantees. In particular we want our decision makers to be stateless, and to simply be able to trust our predictions and  myopically solve  constrained optimization problems as if our predictions were correct. This is the core innovation of our setting beyond the standard problem of learning with long term constraints. \n\n2. We remark that even when considering the special case of a single decision maker (which maps to the traditional setting of learning with long-term constraints), our work introduces and competes with stronger benchmarks than the ones previously studied. Namely, our introduced notion of swap-dynamic regret generalizes  dynamic benchmarks studied in this literature literature. This is the ``dynamic'' analogue of swap regret (in comparison to the more standard external or marginal regret), and has not been previously studied in the learning with long term constraints literature. Of course this contribution is secondary to our main contribution 1). \n\nWe also acknowledge that due to space constraints in our initial submission, the details of the actual algorithm were opaque (and deferred to the appendix). In the revision we have used our extra page of space to bring these details back to the main body."}}, "id": "PywjS4XqjK", "forum": "BR6cFSj6zG", "replyto": "BR6cFSj6zG", "signatures": ["ICLR.cc/2026/Conference/Submission9846/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9846/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9846/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763685899476, "cdate": 1763685899476, "tmdate": 1763685899476, "mdate": 1763685899476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a version of the online omniprediction problem, which is an online learning problem where the learner produces a prediction of the adversary's action in each round and an agent uses this prediction to choose an action. In particular, they consider the version of this problem where there is both a utility function and a constraint function. The goal is to guarantee sublinear regret and cumulative constraint violation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper presents a well-defined framework for studying the online omni prediction under constraints."}, "weaknesses": {"value": "1. To my understanding, this setting fits under the problem of constrained online convex optimization (COCO). In particular, the decision rule of the agent (Definition 5) can just be viewed as a computation Oracle within the COCO framework. For example, this could be taken to be quadratic minimization that is frequently used in the COCO setting (i.e. as in Neely and Yu (2017) and following works). In that sense, the proposed framework in the paper can be seen as COCO with a constrained minimization Oracle. However, it is not clear that the paper makes substantial contribution to the COCO literature for the following reasons:\n   - The $O(\\sqrt{T})$ regret and CCV bounds in the paper are only against benchmarks that are $\\lambda$ strictly-feasible (i.e. $a : c(a) \\leq -\\lambda$). This is a stronger assumption than the typical one to get $O(\\sqrt{T})$ regret and CCV bounds in the COCO literature: the existence of a $\\lambda$ strictly-feasible point. This is widely known in the COCO literature since Neely and Yu (2017).\n   - There does not appear to be computational benefits of the proposed agent decision rule/computation oracle over existing ones in the literature (e.g. unconstrained quadratic minimization).\n   - The setting is restricted to the case of linear utility and constraint functions.\n2. The paper mentioned that the regret notion it considers is stronger than the dynamic regret considered by prior work in COCO. However, it is unclear how this is the case. In particular, it seems that they consider a very specific variation notion being the number of times that the benchmark sequence changes. I did not see any discussion about how this variation notion relates to existing ones in the COCO dynamic regret literature."}, "questions": {"value": "1. How does the proposed variation notion in the dynamic regret relates to those studied in the literature? For example, as in Guo et al. (2022) or Liu et al. (2022).\n2. What is the order-wise dynamic regret bound in the paper? How does this compare with the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KTDzvFN2DK", "forum": "BR6cFSj6zG", "replyto": "BR6cFSj6zG", "signatures": ["ICLR.cc/2026/Conference/Submission9846/Reviewer_59re"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9846/Reviewer_59re"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780430702, "cdate": 1761780430702, "tmdate": 1762921321496, "mdate": 1762921321496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the framework of online omniprediction with long-term constraints to obtain dynamic regret bounds. The authors propose an algorithm that provides regret and cumulative constraint violation guarantees on arbitrary subsequences, with dependence on the number of subsequences that is logarithmic rather than linear. This exponential improvement enables meaningful dynamic regret bounds which were not previously possible. Agents employ a stateless prediction-based decision rule contrary to prior works which need to maintain a notion of state. The paper shows $\\tilde{O}(\\sqrt{T})$  regret and constraint violation when a strictly feasible benchmark exists and $\\tilde{O}(T^{2/3})$ bounds when feasibility is not strict."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper studies an interesting extension of the omniprediction problems and provides interesting technical solutions. I found the result for $\\lambda=0$ quite surprising."}, "weaknesses": {"value": "The paper claims that the standard benchmark in online problems with long term constraints is the best action satisfying the constraints in every round. However, several works instead compare against distributions over actions which are feasible on average (e.g., Castiglioni et al. (2022) which is already cited, and Immorlica et al. (2022)). \n\nWhile the authors note that most literature assumes a single feasible action across all rounds, there are papers that relax this (e.g., Castiglioni et al. 2024). Clarifying how this work fits relative to those assumptions would improve positioning.\n\nThe introduction could expand paragraph 79 to explain why the omniprediction framework is important and provide a bit of additional context for readers not familiar with the problem. \n\nThe paper assumes the adversary selects a distribution over outcomes each round, from which the actual outcome is sampled. Is there any assumption on the distributions in the paper (e.g., not too peaked)? Otherwise what’s the advantage in describing the adversary in this way? It would be good to explicitly state that no smoothed assumptions are used it that’s the case.\n\nI find a bit counterintuitive that the benchmarks for dynamic regret still requires per-round feasibility. While with static baselines having the constraint to hold for each t makes sense, here I would expect the constraint to hold over the entire time horizon. Since the learner’s constraint satisfaction is averaged over time, it seems more natural to allow the comparator sequence to be feasible on average as well. This is an important point that should receive further discussion.\n\nStudying the case in which the feasibility parameter is set to 1/sqrt{T} feels somewhat arbitrary. What if we know there exists a strictly feasible solution but we don’t know the exact value of lambda? Can I still obtain some guarantees? This should be properly discussed.\n\nCompared to standard online problems with long-term constraints, I found the guarantees in absence of Slater’s condition to be quite surprising. It would strengthen the paper to provide an intuitive explanation of what structural properties make this possible, especially since no comparable results exist in standard online learning with constraints. Right now this is only mentioned in one remark in the main paper.\n\nSection 3 lacks an explicit high-level description of the algorithm (paragraph 343). Even a short overview would improve readability.\n\nThe introduction is five pages long, while some technical sections (notably 3.4) are dense and quite opaque. Condensing the intro and expanding the discussion of the main technical ideas would make the paper much more readable.\n\nMinor comments \n\n* In Definition 4, formally define “piecewise feasible” to prevent ambiguity.\n* Both the main text and appendix contain several out-of-margin equations that need formatting adjustments.\n\n\nImmorlica, N., Sankararaman, K., Schapire, R., & Slivkins, A. (2022). Adversarial bandits with knapsacks. Journal of the ACM, 69(6), 1-47.\nCastiglioni, Celli, and Kroer. 2024. Online learning under budget and ROI constraints via weak adaptivity. In Proceedings of the 41st International Conference on Machine Learning (ICML'24)"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y0bhVas3lp", "forum": "BR6cFSj6zG", "replyto": "BR6cFSj6zG", "signatures": ["ICLR.cc/2026/Conference/Submission9846/Reviewer_CAP7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9846/Reviewer_CAP7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926909030, "cdate": 1761926909030, "tmdate": 1762921321239, "mdate": 1762921321239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies online omniprediction with long term constraints, where, in each round, a learner predicts an outcome then multiple agents select their respective actions based on the prediction. Agents gain utilities from their actions based on actual outcomes. The learner aims to generate predictions such that agents choose actions that maximize worst-case utilities while minimizing worst-case constraint violation. \n\nThe paper proposes an algorithm building on the Unbiased-Prediction algorithm from literature: in each round, learner makes predictions that satisfy decision calibration and infeasibility calibration, and agents play constrained best response to the predictions. The authors prove that the algorithm has regret and constraint violation guarantees on arbitrary subsequences. Moreover, the provided regret guarantees are with respect to newly proposed regret definitions that are stronger than the conventional dynamic regret definition in online learning, so the paper’s regret guarantees imply stronger bounds on dynamic regret compared to literature."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper introduces new regret benchmarks that expand the current scope of online omniprediction with long-term constrains. \n\n2.\tThe paper accounts for practicality by considering a stateless decision rule for agents.\n\n3.\tTheoretical results on both constraint violation and regret performance are presented."}, "weaknesses": {"value": "1.\tInsufficient discussion of algorithm: The paper provides very limited discussion of the actual algorithm, which was only mentioned from line 343-350 in page 7. The paper says that the proposed algorithm builds upon Unbiased-Prediction algorithm from Noarov et al. to make predictions that attain decision calibration and infeasibility calibration, but the implementation details are not discussed. How to ensure decision calibration and infeasibility calibration? Compared with Noarov et al., what, if any, computational challenges are introduced by the two calibration constraints? \n\n2.\tLack interpretation of theoretical results: The paper has many theoretical results that are quite dense, and all results are presented with no or little interpretation. I think the authors should spend some efforts providing “plain-language” type explanation for some of the results and their implications. For example, how to think about the bounds on different regret definitions in Section 3.4? When applying the proposed algorithm in practice, how can the derived theory guide learner?"}, "questions": {"value": "Please see my questions listed in weaknesses. In addition, I have a question about agent’s decision rules. How robust is the provided theoretical guarantees to agents’ decision rules? For example, if agents are unable to choose the actual constrained best response, but only able to choose a suboptimal action or choose an action randomly, can the paper’s analysis extend to conclude performance guarantees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mAaStVTY6c", "forum": "BR6cFSj6zG", "replyto": "BR6cFSj6zG", "signatures": ["ICLR.cc/2026/Conference/Submission9846/Reviewer_Djoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9846/Reviewer_Djoj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031081810, "cdate": 1762031081810, "tmdate": 1762921320855, "mdate": 1762921320855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}