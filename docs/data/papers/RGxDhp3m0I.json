{"id": "RGxDhp3m0I", "number": 12827, "cdate": 1758210608604, "mdate": 1759897482543, "content": {"title": "Parallel Training in Spiking Neural Networks", "abstract": "Spiking neurons mimic the spatiotemporal dynamics of biological neurons and their spike-based communication, endowing Spiking Neural Networks (SNNs) with biological plausibility and low-power operation. Yet these dynamics impose strict temporal dependencies on neuronal states, preventing parallel training and creating a fundamental bottleneck to efficient, scalable optimization. This work introduces a novel functional perspective to address this challenge. Specifically, we argue that the reset mechanism, which induces state dependencies, should be removed. However, any modification must satisfy two principles: i) preserving — and even enhancing — the functions of reset as a core biological mechanism; and ii) enabling parallel training without sacrificing SNNs’ inherently serial inference, which underpins their energy efficiency. To this end, we identify functions of the reset mechanism and analyze how to reconcile parallel training with serial inference, upon which we propose a dynamic decay spiking neuron that combines a causal convolution structure with an optimized spike firing pattern. We demonstrate the efficiency and effectiveness of our approach across diverse network architectures and task benchmarks, including image classification, neuromorphic event processing, time-series forecasting, and language modeling.", "tldr": "", "keywords": ["parallel training", "spiking neural networks"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9462122b3f51b1fe5ee4e0a9cd0202c4bfecb2f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel parallel spiking-neuron model to accelerate SNN training. Specifically, the membrane potential is generated via Eq. (19), which resembles the charging equation of an LIF neuron; because this equation is fully linear, it can be rewritten as a non-recurrent sum and efficiently implemented with matrix multiplication. The authors further enhance the model by making the coefficients in the charging equation depend on the inputs of several previous time-steps, yielding a learnable gating effect. Finally, integer spikes are emitted during training while binary spikes are used at inference, speeding up training. Compared with the conventional LIF neuron and the recent PSN, the proposed method is faster than LIF but still slower than PSN at T = 32. Extensive experiments on multiple tasks show that it outperforms PSN in most cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is presented in detail, allowing readers to grasp its mechanics easily.  \n- The experimental section is very comprehensive, covering almost every scenario one could consider."}, "weaknesses": {"value": "The proposed neuron remains slower than PSN; although the authors attribute this to the absence of a customized CUDA kernel, whether a CUDA implementation could actually surpass PSN still needs verification."}, "questions": {"value": "Is the use of an internal sigmoid operation reasonable? It may be unsuitable for deployment on neuromorphic chips."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vb3Klb3Gfu", "forum": "RGxDhp3m0I", "replyto": "RGxDhp3m0I", "signatures": ["ICLR.cc/2026/Conference/Submission12827/Reviewer_ZTCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12827/Reviewer_ZTCS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761015797037, "cdate": 1761015797037, "tmdate": 1762923631632, "mdate": 1762923631632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an insightful analysis of the reset mechanism in spiking neurons and introduces a DSN model that supports parallel training while maintaining serial inference capability. The topic is relevant, and the motivation is clear. However, the paper lacks a theoretical analysis to justify the effectiveness of DSN’s nonlinear operations. The experimental design also raises concerns regarding fairness. Furthermore, the manuscript does not clarify whether DSN preserves the sparse firing property essential to SNN efficiency or analyze the additional inference latency introduced by the model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The authors provide a detailed analysis of the reset process in spiking neurons, attributing its functions to introducing nonlinearity and regulating the membrane potential.\n\n2.The authors propose a DSN model that enables parallel training while preserving the serial inference capability."}, "weaknesses": {"value": "1.Lack of Theoretical Justification: Although the authors identify the reset process as introducing nonlinearity, they do not provide a theoretical explanation showing that DSN achieves a more principled or effective nonlinear behavior.\n\n2.Fairness of Model Comparison: DSN introduces additional parameters (137M) compared to SPiKE-SSM (75M) and SpikingSSM (75M), which could partly account for the observed improvement.\n\n3.Potential Loss of Sparsity: The binary behavior of the reset mechanism enforces the sparse firing property fundamental to SNN efficiency and biological plausibility. The manuscript does not clarify whether DSN retains this property.\n\n4.Higher Inference Latency: The introduction of the additional N-dimensional may incur extra inference latency[1], particularly for DVS datasets and sequential datasets."}, "questions": {"value": "1.The authors are encouraged to discuss the design choice of N across different datasets and include ablation studies on N to better demonstrate the effectiveness of the proposed method.\n\n2.DSN appears to introduce additional MAC operations, which may increase deployment complexity on neuromorphic hardware. The authors should clarify whether the energy consumption analysis includes the internal neuronal dynamics.\n\n3.The authors should further elaborate on how DSN preserves the event-driven property of spiking neurons during inference, particularly within the Transformer architecture.\n\n[1]Scaling Spike-driven Transformer with Efficient Spike Firing Approximation Training, T-PAMI 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k76AywwwDK", "forum": "RGxDhp3m0I", "replyto": "RGxDhp3m0I", "signatures": ["ICLR.cc/2026/Conference/Submission12827/Reviewer_DKoD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12827/Reviewer_DKoD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704412713, "cdate": 1761704412713, "tmdate": 1762923631343, "mdate": 1762923631343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors use spiking neural networks for image and neuromorphic dataset classification. They propose a new spiking neuron model called the Dynamic Decay Spiking Neuron (DSN)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The DSN is new.\n* The DSN can be computed either sequentially or in parallel"}, "weaknesses": {"value": "1) The advantages w.r.t. an earlier proposal, the PSN / sliding PSN  (Fang et al. 2023) are not clear. The DSN is significantly more complex, and less neuromorphic-hardware-friendly:\n- it uses integer spike, not binary spikes (Eq 7). With binary spikes, the DSN is less accurate (87.45%) than the PSN (88.45%) on Seq CIFAR10 (Table 2)\n- it uses a dynamic, input-dependent leak rate (Eq 8 and 9)\n- the Enhanced DSN uses a non-local neuron mixing operation.\n\nThe DSN is about 3 times slower than the PSN (Table 1)\n\nThese disadvantages are not always compensated by a boost in accuracy. For example, on CIFAR10-DVS the DSN and the sliding PSN have the same accuracy (Table 4).\n\nAs the authors correctly say, the PSN has T^2 parameters, \"resulting in substantial\nmemory and computational overhead\" when T is large. But the sliding PSN has only k parameters. So the authors should add the sliding PSN in tables 1 (throughput) and 7 (energy).\n\n2) The DSN accuracy is lower than the SOTA on ImageNet and CIFAR10-DVS (Table 4)"}, "questions": {"value": "Do you think the DSN is implementable on existing neuromorphic chips?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "F1ayB9ja9c", "forum": "RGxDhp3m0I", "replyto": "RGxDhp3m0I", "signatures": ["ICLR.cc/2026/Conference/Submission12827/Reviewer_ZuXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12827/Reviewer_ZuXb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732492559, "cdate": 1761732492559, "tmdate": 1762923630944, "mdate": 1762923630944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a detailed review of previous parallel spiking neuron models, argues that the \"reset\" mechanism in SNNs serves two core functions: introducing nonlinearity and controlling membrane potential, and claims that traditional soft/hard reset mechanisms are inefficient. Based on this analysis, the paper proposes a Dynamic Decay Spiking Neuron (DSN). This model removes the reset mechanism, replaces the fixed membrane potential decay with an input-dependent causal convolution, and incorporates an integer-valued firing pattern. The DSN is designed to support both parallel training and serial inference. Experiments on diverse network architectures and benchmarks—including image classification, neuromorphic event processing, time-series forecasting, and language modeling—validate the efficiency and effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper analyzes the role of the reset mechanism from a novel \"functional perspective,\" using this as a guiding principle to design the new neuron.\n2. The DSN model is extensively validated across diverse data modalities and network architectures, demonstrating state-of-the-art or highly competitive performance.\n3. The paper is clearly written and well-structured."}, "weaknesses": {"value": "1. **Limited Model Innovation:** The core mechanism of the proposed DSN can be understood as applying a sigmoid function to the output of a sliding PSN [1] and using that as a gating signal for the previous state $H_{t-1}$ and the current input $X_t$. And it integrates the integer-valued firing mechanism from ILIF [2], using integers instead of binary spikes as the neuron's output.\n\n2. **Unfair Experimental Setup:** The paper's experimental comparisons suffer from significant fairness issues. According to the characteristics of ILIF, during inference,  a single-timestep (T=1) integer firing (e.g., N=4) is equivalent to the spike accumulation of multiple time-steps (e.g., T=4, N=1). Therefore, studies of ILIF typically adopt a (T=1, N=4) configuration for a fair comparison against other spike-fire methods using (T=4, N=1). However, this paper uses a (T=4, N=4) configuration, which is effectively equivalent to T=16 (with N=1), and compares against baseline models using a (T=4, N=1) setup. This comparison is not equivalent and may lead to a significant overestimation of the proposed model's performance.\n\n3. **Unreasonable and Incomplete Energy Analysis:** The paper's energy consumption analysis is seriously flawed. Section 5.3 states that its evaluation follows [3]. However, [3] uses LIF neurons, whose operational energy is negligible compared to the synaptic layer, allowing the authors to ignore the neuron layer's energy cost. In contrast, the DSN includes computationally intensive operations at the neuron layer (such as floating-point causal convolution and the expensive sigmoid function). The energy cost of these operations is likely comparable to, or even higher than, that of the synaptic layer. Therefore, an analysis method that only considers synaptic energy is unreasonable and incomplete.  Furthermore, the DSN outputs an integer rather than a binary spike. This distinction is critical and must be factored into the synaptic energy analysis. It is unclear whether the paper has properly considered this in its synaptic energy calculations. The authors should separately list and compare the energy consumption of both the synaptic and neuron layers.\n\n4. **Inefficient Computation, Contradicting the Goal of PSN:** As shown in Table 8, the DSN's runtime is significantly slower than that of PSN and even slower than the CUDA-implemented LIF neuron [4]. The authors attribute this to their Triton implementation being slower than CUDA, but this explanation is unconvincing. Furthermore, the original motivation for Parallel Spiking Neurons is to accelerate SNN training on GPUs. The fact that the DSN's training is even slower than traditional serial neurons runs counter to this design goal.\n\n> [1] Fang W, Yu Z, Zhou Z, et al. Parallel spiking neurons with high efficiency and ability to learn long-term dependencies[J]. Advances in Neural Information Processing Systems, 2023, 36: 53674-53687.\n>\n> [2] Luo X, Yao M, Chou Y, et al. Integer-valued training and spike-driven inference spiking neural network for high-performance and energy-efficient object detection[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 253-272.\n>\n> [3] Yao M, Hu J, Zhou Z, et al. Spike-driven transformer[J]. Advances in neural information processing systems, 2023, 36: 64043-64058.\n>\n> [4] Fang W, Chen Y, Ding J, et al. Spikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence[J]. Science Advances, 2023, 9(40): eadi1480."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "90ZIeGOiZ4", "forum": "RGxDhp3m0I", "replyto": "RGxDhp3m0I", "signatures": ["ICLR.cc/2026/Conference/Submission12827/Reviewer_z46U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12827/Reviewer_z46U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890852335, "cdate": 1761890852335, "tmdate": 1762923630123, "mdate": 1762923630123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}