{"id": "oxgcPoDkNv", "number": 6665, "cdate": 1757991664005, "mdate": 1759897902182, "content": {"title": "Bridging Radiology and Pathology Foundation Models via Concept-Based Multimodal Co-Adaptation", "abstract": "Pretrained medical foundation models (FMs) have shown strong generalization across diverse imaging tasks, such as disease classification in radiology and tumor grading in histopathology. While recent advances in parameter-efficient finetuning have enabled effective adaptation of FMs to downstream tasks, these approaches are typically designed for a single modality. In contrast, many clinical workflows rely on joint diagnosis from heterogeneous domains, such as radiology and pathology, where fully leveraging the representation capacity of multiple FMs remains an open challenge. To address this gap, we propose Concept Tuning and Fusing (CTF), a parameter-efficient framework that uses clinically grounded concepts as a shared semantic interface to enable cross-modal co-adaptation before fusion. By incorporating task-specific concepts that are relevant across modalities, CTF aligns radiology and pathology representations, thereby enhancing their complementarity and enabling interpretation. We further design a Global–Context–Shared Prompt (GCSP) mechanism, which employs a small set of learnable tokens to capture domain-specific priors, shared patient-level information, and cross-domain context. The resulting concept alignment scores from each modality are then fused to produce a final prediction. Extensive experiments demonstrate that CTF outperforms strong unimodal, latent-fusion, and adapter-based baselines (e.g., AUC 0.903 on TCGA-GBMLGG). Notably, CTF achieves these gains without finetuning the full FMs, requiring only 0.15\\% additional parameters, thus highlighting the effectiveness of concept-based multimodal co-adaptation. Our code is anonymously available at: https://anonymous.4open.science/r/CTF-27C2.", "tldr": "We propose a novel prompt-tuning method that uses clinical concepts to dynamically co-adapt radiology and pathology foundation models, improving multimodal fusion and interpretability.", "keywords": ["multimodal learning", "concept-based learning", "foundation models", "parameter-efficient fine-tuning", "medical imaging", "survival analysis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09ee353ce26fba5f839e3aec24b482a3a6241822.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposed a feature alignment scheme for bridging the radiology and histopathology image foundation models. A list of concepts is first selected and then used as the linkage feature between radiology and histopathology image features, with separate adaptors trained. The performance evaluation is conducted on two downstream tasks, i.e., survival analysis and tumor grading. Three datasets (two public and one private) are adopted in the experiments, and only marginally improved results of the proposed method are reported in comparison to previous multi-modal feature fusion methods, e.g., 1.6-2.6% increase in C-index for survival analysis. The manuscript is not easy to follow overall. Many details seem to be missing from the introduction, along with several flaws detailed below."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper proposed a multi-modal alignment scheme that required light tuning efforts, which could be easily adopted in the downstream tasks.\n+ The downstream in the evaluation is clinically relevant."}, "weaknesses": {"value": "- There are many details of the proposed methods missing. 1. It is not clear how the 3D volumes of radiology images and WSI\n Images are encoded. It could largely influence the semantics of the image embedding, especially for the alignment task. 2. Why is the context prompt only computed for the histopathology images? 3. Details of the MoE layer? 4. Is P_tuned computed for all the concepts in both sets? 5. How are the images associated with the concepts? 6. Are there any other inputs other than f_r and f_h?\n- The overall lack of clarity in the introduction makes the paper a bit hard to follow.\n- Radiologists and histopathologists speak quite different languages, which is also reflected by the selected concepts shown in the appendix. How often can these concepts be aligned, and it will be helpful to show what percentage of these concepts are matched. \n- The proposed method utilized the global features of the images, while the concepts are composed in more detail. How will this mismatch of the semantic feature levels affect the performance? How different could the features be after the adoption in comparison to the original image feature? It is also related to the following point.\n- The overall performance gain after adopting the proposed method is marginal, e.g.,1.6-2.6% increase in C-index for survival analysis in comparison to the recent baseline. Is the performance gain sourced from the additionally concept injected? What will be the performance using solely the two image features and textual features (e.g., simple concatenation)\n-"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X0cndTRuf1", "forum": "oxgcPoDkNv", "replyto": "oxgcPoDkNv", "signatures": ["ICLR.cc/2026/Conference/Submission6665/Reviewer_N7n7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6665/Reviewer_N7n7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553216414, "cdate": 1761553216414, "tmdate": 1762918973698, "mdate": 1762918973698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CTF (Concept Tuning and Fusing), a framework that bridges radiology and pathology foundation models using clinically meaningful concepts as an interpretable interface. Instead of fusing static features, CTF employs a Global-Context-Shared Prompt mechanism that dynamically adapts concept interpretations based on cross-domain information, allowing each modality to inform the other's concept understanding. Tested on survival prediction and cancer grading across multiple datasets, CTF achieves state-of-the-art performance while training only 0.15% additional parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tNovel methodology: The Global-Context-Shared Prompt (GCSP) strategy dynamically conditions concept interpretations in one modality based on features from the complementary modality, enabling deeper synergy than traditional static feature fusion approaches.\n\n2.\tParameter efficiency with strong performance: CTF achieves state-of-the-art results while requiring only 0.15% additional trainable parameters by keeping foundation models frozen and learning only lightweight prompt modules."}, "weaknesses": {"value": "1.\tThe citations in this paper are **all incorrect**. Please correctly use the \\citet and \\citep LaTeX commands to distinguish proper references for different scenarios.\n\n2.\tBaselines such as PIBD, MOTCAT are originally developed for the fusion of pathological and genomic data. The comparison might be unfair for pathology and radiology data as inputs.\n\n3.\tTask “Center-1 GC” in Table 2 shows relatively low performance (all models) in AUC and especially ACC, suggesting this is not a proper task to choose for model evaluation."}, "questions": {"value": "1.\tCould the authors elaborate more on the registration of WSIs and radiology data, such as CT scans? How to make sure the data are paired since they are in different macro- and micro-scales?\n\n2.\tFor the computation of the alignment score for concepts, why is it not normalized or scaled? Meanwhile, it’s then discretized into a variable, but how? Could the authors elaborate more, or specify where the reader can find related details in the Appendix? Moreover, since here the model requires the label to compute the mutual information, what’s the strategy during inference where there is no patient label?\n\n3.\tIn Section 3.3, the authors mentioned that the concept score vectors “provide an interpretable representation of the patient’s condition”. Could the authors elaborate more on this conclusion? \n\n4.\tCTF trains in an end-to-end manner. Although most of the parameters of foundation models are frozen, this still introduces concerns about efficiency. Could the authors provide the least requirement to train CTF compared to uni-modal or late fusion baselines (which, as far as I know, can be trained in one GTX 3090 card)? Also, how many GPUs are used for the actual model training? (Appendix only mentions types of GPU)\n\n5.\tSelection strategy for pathology VLMs. It seems CONCH contributes largely to the performance of CTF from Table 3. But this ablation study only compares CONCH with the general-purpose CLIP.  What about other pathology VLMs such as PLIP [1] and MUSK [2]?\n\n$\\quad$ [1] Huang, Zhi, et al. \"A visual–language foundation model for pathology image analysis using medical twitter.\" Nature medicine 29.9 (2023): 2307-2316.\n\n$\\quad$ [2] Xiang, Jinxi, et al. \"A vision–language foundation model for precision oncology.\" Nature 638.8051 (2025): 769-778.\n\n6.\tCould the author please add these papers in the Introduction or Related Work sections since they are relevant:\n\n$\\quad$ [1] (Pathology Foundation Model) Chen, Richard J., et al. \"Towards a general-purpose foundation model for computational pathology.\" Nature medicine 30.3 (2024): 850-862.\n\n$\\quad$ [2] (Pathology Foundation Model) Ma, Jiabo, et al. \"A generalizable pathology foundation model using a unified knowledge distillation pretraining framework.\" Nature Biomedical Engineering (2025): 1-20.\n\n$\\quad$ [3] (Pathology Foundation Model) Xu, Hanwen, et al. \"A whole-slide foundation model for digital pathology from real-world data.\" Nature 630.8015 (2024): 181-188.\n\n$\\quad$ [4] (Foundation Model Adaptation / Prompt Generation and Selection) Guo, Zhengrui, et al. \"Focus: Knowledge-enhanced adaptive visual compression for few-shot whole slide image classification.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n$\\quad$ [5] (Foundation Model Adaptation / Parameter-efficient learning) Che, Haoxuan, et al. \"Llm-driven medical report generation via communication-efficient heterogeneous federated learning.\" IEEE Transactions on Medical Imaging (2025).\n\n**I will definitely consider raising my score if authors solve my concerns above.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ApR8teSP7l", "forum": "oxgcPoDkNv", "replyto": "oxgcPoDkNv", "signatures": ["ICLR.cc/2026/Conference/Submission6665/Reviewer_LEUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6665/Reviewer_LEUR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664447804, "cdate": 1761664447804, "tmdate": 1762918973183, "mdate": 1762918973183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a model called Concept Tuning and Fusing (CTF), a parameter-efficient framework that uses clinically grounded concepts to bridge radiology and pathology foundation models (FMs) for survival prediction and cancer grading. Instead of performing multimodal fusion using static latent features, this paper proposes to use concepts as a shared semantic interface and dynamically co-adapt the concept semantics. The framework contains i) concept generation and selection by querying LLMs, and ii) global-context-shared prompt tuning. Empirical evaluations are done using the public TCGA dataset and a private dataset on gastric cancer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The use of clinically grounded concepts as a bridge for multimodal co-adaptation is original. It provides enhanced interpretability and improved performance. The idea of the global-context-shared prompt (GCSP) conditions the interpretation of findings from one modality on another, which looks quite novel and interesting to me.\n- The paper articulates a concept selection strategy that balance prognostic relevance and semantic diversity.\n- Overall, the empirical evaluation is strong, showing consistent improvements across datasets and tasks.\n- The ablation study and hyperparameter sensitivity analysis are performed to allow a better understanding of the architectural design.\n- By and large, the paper is organized and well-written, the logic is smooth and the paper is easy to follow."}, "weaknesses": {"value": "- One major issue I am not quite following is the motivation of the GCSP strategy. In Section 3.2, it is argued that the prognostic *importance* of findings in radiology may be amplified by a specific histological finding. This implies that the basis of performing GCSP is that correct and good concepts (findings) are identified from each modality. In this case, a naive solution would be to feed all concepts from different modalities to a neural network and let the network to figure out the synergy between them. The GCSP strategy, in contrast, seems to solve this problem with a much more complex design. I am not sure why the naive way would fail and why GCSP could be a better solution than a naive one.\n- As reported in Table 1, M4Survive and CTF both have large standard deviation values. To ensure the improvement does not purely arise from noise, statistical tests should be performed to examine the significance of the improvements. \n- Other minor issues:\n\t- For inline citations, `\\citep{}` should be used instead of `\\citet{}`.\n\t- Some notations are not consistent throughout the paper, for example the prompt is noted by $P^{\\text{tuned}}$ in Section 3.2 and $P_{tuned}$ in Section 3.3."}, "questions": {"value": "- In Section 3.3, after generating $P_{tuned}$, a set of tuned textual embeddings are obtained from the text encoder. Is the text encoder the frozen ones as shown in Fig. 2? How is $P_{tuned}$ used in this text encoder?\n- Please also refer to the weaknesses section as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H7KFGnrPo4", "forum": "oxgcPoDkNv", "replyto": "oxgcPoDkNv", "signatures": ["ICLR.cc/2026/Conference/Submission6665/Reviewer_c1KR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6665/Reviewer_c1KR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980216795, "cdate": 1761980216795, "tmdate": 1762918972650, "mdate": 1762918972650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a parameter-efficient framework that uses clinically grounded concepts to bridge radiology and pathology, where a set of learnable tokens is employed to learn modality-specific and common knowledge, as well as cross-modal interactions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is well-motivated, and the idea is new but technically complex.\n2. The experiments are comprehensive, including comparisons to SOTA methods, thorough ablation studies and intuitive visualization.\n3. The proposed method outperforms SOTA methods."}, "weaknesses": {"value": "1. The pool of basic concepts is static by asking LLM to obtain relevant concepts, which may limit the performance and new concept discovery. It would be better if there is a mechanism to update the pool as the model is optimized. \n2. The scalability of LLM can be investigated to see if the performance can be further improved with a stronger LLM.\n3. Since MOTCAT and PIBD are designed for histology-genomics data, how were they applied to the datasets used in this work? Specifically, MOTCAT is unidirectional, which utilizes genomic data to guide the learning of pathological modeling. What about CTF? More details about experimental settings should be uncovered.\n4. Given that the concepts would be updated via a learnable prefix, can the learned concepts be decoded via an LLM decoder? This can facilitate new concept discovery. \n5. The figure of GCSP can be improved. Currently, it is challenging to understand how the embedding of each modality interacts with the mentioned three types of prompts."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TyTNoRRVPl", "forum": "oxgcPoDkNv", "replyto": "oxgcPoDkNv", "signatures": ["ICLR.cc/2026/Conference/Submission6665/Reviewer_q1cP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6665/Reviewer_q1cP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990783598, "cdate": 1761990783598, "tmdate": 1762918972259, "mdate": 1762918972259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}