{"id": "fnKaWiM6LW", "number": 1275, "cdate": 1756868700338, "mdate": 1759898217566, "content": {"title": "Efficient Conformal Prediction with Order-Preserving Predictions for Classifiers", "abstract": "Conformal prediction provides prediction sets with distribution-free, finite-sample coverage guarantees for machine learning classifiers. Numerous methods reduce set size by retraining classifiers or designing novel non-conformity scores, but they often suffer from high computational cost or inflexibility. To address this issue, we propose Flexible Prediction Sets (FPS), a post-hoc framework that learns an  order-preserving transformation which preserves the order of model's predicted class-probability  while reshaping their magnitudes, enabling smaller conformal prediction sets. This transformation is obtained by optimizing a smooth surrogate of the set-size objective on a tuning dataset, then applied to the predicted class-probability before conformal calibration.  This process yields smaller prediction sets while  maintaining the coverage level. Theoretically, we prove coverage preservation under transformation, provide generalization bounds for the function class and surrogate risk, and show convergence to a stationary point. Empirically, extensive experiments on image and text benchmarks with multiple base machine learning classifiers demonstrate consistent reductions in set size at various nominal coverage rates, outperforming conformal prediction baselines.", "tldr": "We propose Flexible Prediction Sets, a framework that uses an order-preserving transform on model predictions to shrink conformal prediction sets while maintaining coverage guarantees.", "keywords": ["classification", "predictive uncertainty", "conformal prediction"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eab38eb3e396ffefe74ee2053947df276cf82743.pdf", "supplementary_material": "/attachment/315c217408ec0172336c51a7cfc1c1dabfa69d1f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an intermediate step that can be inserted into the APS or RAPS conformal procedures to reduce set sizes while maintaining a marginal coverage guarantee. This step involves applying a learned transformation of the softmax vector before applying the APS or RAPS conformal score function. This transformation is optimized on a holdout dataset to minimize expected set size."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem studied in the paper is an important one: in many practical settings, prediction sets that are extremely large are not useful, so we want to minimize set size subject to the desired coverage constraints. \n2. The experiments are extensive, spanning multiple model architectures, datasets, and coverage levels."}, "weaknesses": {"value": "**Main concern**: Although the high-level problem of optimizing prediction set size is an important one, the authors focus on a restricted version of this problem for which I am not convinced they provide a practically useful solution. There are three commonly used conformal score functions for classification: the *LAC* score corresponding to the least ambiguous set-valued classifier from Sadinle et al., 2019, *APS*, and *RAPS*. At a fixed marginal coverage level, LAC generally produces much small sets that APS or RAPS. So why would someone ever use APS or RAPS? The answer is that these score functions were created with more than marginal coverage in mind; the goal of these functions is to create prediction sets with approximate _X-conditional coverage_ --- that is,\n$$P(Y \\in C(X) \\mid X = x) \\geq 1- \\alpha$$\nfor all $x$.  This is explained in the papers that proposed APS and RAPS. \n\nThe paper restricts itself to studying the APS and RAPS conformal score functions. This in and of itself is not a problem; it simply implies that we are in a problem setting where we care about having good conditional coverage. However, the experiments in the main paper do not evaluate the conditional coverage of the methods. The authors did perform experiments that evaluate conditional coverage, but these are hidden in Appendix F (which is never referenced in the main paper!). In Table 9 of Appendix F, we see the Size-Stratified Coverage Violation (a measure of conditional-coverage) of the proposed methods, but the values for the baselines (raw APS and raw RAPS) are not provided. \n\nTo summarize: if I can be convinced that the procedure proposed in this paper reduce APS/RAPS set sizes _without significantly harming class-conditional coverage_, I would be willing to raise my score. \n\n**Other weaknesses**: There are some places the writing could be improved.\n* “RAPS yields smaller sets with valid coverage but keeps predicted probabilities\nfixed, limiting flexibility and leaving potential gains untapped” (line 48) — at this point in the introduction I don’t know what this means, and I feel as though it should either be made more vague so that I don't expect to understand it, or more specific so that I do fully understand it.\n* “Thereby adhering to the posthoc principle”  (line 69) - I'm not sure what this means\n* \"Besides\" on line 96 -- I would replace this with \"In addition\" \n* \"converges to a stationary point in the sense of limit points\" (line 99) -- this reads strangely.\n* “vector of predicted class-probability” (line 181) -- should be \"vector of predicted class probabilities\"\n* Algorithm 1 should be referenced in the text. \n* Set size objective equation (line 191): I believe this definition is only valid if s is assumed to be sorted, so it should be defined as such in the preceding paragraph"}, "questions": {"value": "1. I would like to see tables of SSCV and set size for APS, RAPS, and LAC with and without the proposed method. Can it be shown that the proposed method reduces set size without hurting SSCV? \n2. What is the randomness in the experiments? I assume the random seeds determines the splitting of data into FPS tuning, calibration, and test sets? It may be useful to explicitly state this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bkvBbMxi8k", "forum": "fnKaWiM6LW", "replyto": "fnKaWiM6LW", "signatures": ["ICLR.cc/2026/Conference/Submission1275/Reviewer_AfPW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1275/Reviewer_AfPW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760468496554, "cdate": 1760468496554, "tmdate": 1762915725085, "mdate": 1762915725085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a method named Flexible Prediction Sets (FPS), a post-hoc method for improving the size-efficiency of conformal prediction (CP) in classification tasks. \nThe core idea is to learn a monotonic, order-preserving transformation that rescales a model's predicted class probabilities. \nThis transformation, learned by optimizing a smooth surrogate for the set-size objective on a tuning set, reshapes the probability magnitudes to make the true label more distinct, thereby yielding smaller prediction sets while rigorously maintaining the target coverage guarantee."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novelty and Significance: The proposed method is simple, intuitive, and highly effective. It addresses the key challenge of size-efficiency in CP without requiring model retraining or complex non-conformity score design, making it a practical and widely applicable tool.\n2. Theoretical Soundness: The work is well-supported by strong theoretical results, including a formal proof that the transformation preserves the finite-sample coverage guarantee of CP. The authors also provide a generalization bound for the learned function and prove the convergence of their optimization algorithm to a stationary point.\n3. Empirical Validity: The experimental evaluation is comprehensive and convincing. FPS demonstrates consistent and significant reductions in prediction set size across diverse benchmarks, multiple modern classifiers, and some CP baselines (APS and RAPS). The gains are particularly notable under distribution shift, highlighting the method's robustness."}, "weaknesses": {"value": "This paper, while presenting a well-executed empirical study, suffers from significant weaknesses in its positioning relative to prior art and the framing of its contributions. \nThese issues call into question the novelty and overall significance of the work.\n\n**1. Overlap with Prior Work and Unclear Novelty**\n\nThe core contribution, a post-hoc learned adapter for improving CP efficiency, lacks novelty due to its strong resemblance to the \"C-Adapter\" method proposed by Liu et al. (2024). \nBoth FPS and C-Adapter learn a transformation on the classifier's outputs using a tuning set to minimize a surrogate of the prediction set size. The paper fails to cite or discuss this highly relevant work, which is a critical omission.\n\nThe primary discernible difference appears to be in the parameterization of the transformation: FPS uses an exponentiated trigonometric polynomial to structurally guarantee monotonicity, whereas C-Adapter often uses a small MLP with constraints on its weights. However, the conceptual gap between these two approaches is not articulated and may be marginal. Without a direct comparison, it is impossible to assess whether the specific design choice of FPS offers any substantive advantage over C-Adapter.\n\nThe authors *shall* thoroughly discuss C-Adapter and other similar methods. They need to clearly delineate their technical and conceptual contributions over this prior work. An empirical comparison with C-Adapter on at least one benchmark is essential to justify the claim of a novel contribution(if any, I will rise the rating). If the performance is similar, the authors must provide a compelling argument for why their specific parameterization or theoretical analysis is a significant advance. \n\n**2. Insufficient Literature Review on Order-Preserving Functions**\n\nThe emphasis on \"order-preserving\" transformations is presented as a key principle, yet the paper fails to engage with the existing literature on this topic within deep learning. Rahimi et al. (2020) introduced \"Intra Order-Preserving Functions\" specifically for calibrating multi-class neural networks. Since improved calibration is a likely mechanism behind FPS's success, this work is directly relevant. The authors should have discussed whether FPS is essentially a new form of post-hoc calibration and how it relates to methods like Rahimi's. \nOther related works include Rahimi et al. (2021) and Zeng et al. (2025) about the order-preserving regularization. \nThis oversight weakens the foundation of the work.\n\nThe related work section requires a major revision. The authors must incorporate a discussion of order-preserving functions in the context of both model calibration and conformal prediction, citing the works mentioned above and others. This will properly contextualize their contribution and clarify what is truly new about their approach. A rising rating also possible for completing related  work. \n\n**3. Overstated Theoretical Contribution**\n\nThe paper presents three main theoretical results, but their significance is not uniform and could be seen as misleading.\n\n*   **Trivial Coverage Guarantee:** The main result, Theorem 1 (coverage guarantee), is a direct and standard consequence of using a split-data approach in the conformal prediction framework. As long as the transformation is learned on data independent of the calibration set, the guarantee holds for *any* such function. This is a prerequisite for the method to be valid, not a novel theoretical finding.\nThe more incremental theoretical contributions are the generalization bound (Theorem 2) and the convergence analysis (Theorem 3) for the specific learning procedure proposed.\n\nThe authors should reframe their theoretical contributions. They should clearly state that the coverage guarantee is a known property of the split-CP framework and instead position their analysis of the surrogate loss and optimization procedure as the core theoretical novelty of the paper. This would provide a more accurate representation of their work's contribution.\n\n[Reference]\n\n> [1] Liu, K., Zeng, H., Huang, J., Zhuang, H., VONG, C. M., & Wei, H. (2024). C-adapter: Adapting deep classifiers for efficient conformal prediction sets.\n>\n> [2] Rahimi, Amir. 2021. “Learning with Limited Data and Supervision.” PhD Thesis, The Australian National University.\n>\n> [3] Rahimi, Amir, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. 2020. “Intra Order-Preserving Functions for Calibration of Multi-Class Neural Networks.” Neural Information Processing Systems 33: 13456–67.\n>\n> [4] Zeng, Hao, Kangdao Liu, Bingyi Jing, and Hongxin Wei. 2025. “Parametric Scaling Law of Tuning Bias in Conformal Prediction.” Paper presented at International Conference on Machine Learning. Forty-Second International Conference on Machine Learning."}, "questions": {"value": "**1. Clarification on the Gap with C-Adapter**\n\nThe core idea of your paper, learning a post-hoc, order-preserving transformation on classifier outputs to improve conformal prediction efficiency, appears to have a significant conceptual overlap with the recently proposed C-Adapter (Liu et al., 2024). This prior work is not discussed, making it difficult to assess the novelty of your contribution.\n\n*   Could you please articulate the key conceptual and technical differences between FPS and C-Adapter?\n*   Your primary technical novelty seems to be the use of an exponentiated trigonometric polynomial to guarantee monotonicity. What are the specific advantages of this parameterization over the constrained MLP often used in C-Adapter? Are there theoretical benefits (e.g., better approximation power, tighter generalization bounds) or practical advantages (e.g., optimization stability, reduced hyperparameter sensitivity) that justify this new approach?\n*   To substantiate these claims, a direct empirical comparison, even on a single benchmark, would be highly valuable. Could you provide such a comparison or a compelling argument for why FPS is a significant advance over C-Adapter?\n\n**2. On the Choice of the Order-Preserving Function Class**\n\nThe choice to parameterize the transformation via an exponentiated trigonometric polynomial is an interesting design choice that provides a structural guarantee of monotonicity. However, this specific parameterized class $\\mathcal{G}$ is a strict subset of the space of all continuous, differentiable, and monotone functions $\\mathcal{F}$.\n\n*   Could you elaborate on the trade-offs made with this choice? Specifically, what is the potential \"approximation error\" (as implied by $\\delta_M$ in Theorem 2) introduced by restricting the search space to $\\mathcal{G}$?\n*   Is it possible that a different, perhaps more flexible, function class, like monotone splines or a constrained neural network, could achieve a better trade-off between approximation power (capturing a more optimal transformation) and estimation variance (overfitting to the tuning set)?\n*   And, what specific optimization challenges does your parameterization solve that might arise with other methods for enforcing monotonicity (e.g., projection, weight clipping, or complex reparameterizations)? A deeper discussion on why this specific class was chosen over others would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6daWYA39Au", "forum": "fnKaWiM6LW", "replyto": "fnKaWiM6LW", "signatures": ["ICLR.cc/2026/Conference/Submission1275/Reviewer_rYPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1275/Reviewer_rYPi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760790853897, "cdate": 1760790853897, "tmdate": 1762915724985, "mdate": 1762915724985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims at decreasing the set size in conformal prediction through optimizing an order preserving transformation. They optimize the parameter of the function over a tuning set and then apply it to the main conformal prediction pipeline ensuring that the guarantee holds."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem of decreasing the set size is an interesting problem where many papers have contributed to. The authors try to solve it by optimization which I believe is a promising direction towards the problem."}, "weaknesses": {"value": "**Unnecessarily complicated notations.** I believe the paper at some points are presented in a more complicated way than they really are. For instance I would directly write Eq. 1 with the quantile definition as there is no need for writing it following the notation of the conformal risk control setup. \n\n**Major issue with the goal of the paper and evalutions.** The authors define an order-preserving transformation meaning that within the score of one input x, the rank of the scores are preserved — which by definition keeps the top label at the same rank and preserves the accuracy. Is this rank preserved between any pairs of x, and y — I mean does it hold between class 4 of one input and class 2 of another input, or is it only over classes for a single input? I guess this transformation preserves the rank only between the labels of one input, otherwise it should not effect the set size as all the comparisons remain the same. \n\nNow assuming that the order between the classes of different inputs change, then the question is if we are ignoring the main motivation behind adaptive prediction sets (APS) to have an adaptive prediction set. Which informally means that the conditional coverage remains high. If that is not the case then one can argue if the threshold prediction sets (LAC) also results in a smaller prediction sets. \n\nThe only place I see any notion close to conditional coverage is size-stratified coverage violation, which is not a proper estimator for conditional coverage. Originally Romano et al 2020 use worst slice coverage which however far from accurate is a better indicator of whether there is a region in the dataset that gets systematically miscovered. Importantly even on SSCV metric (which is not looking at the conditional coverage properly)  the authors only compare APS + theirs to LAC. When you are applying a transformation to a score function (in this case APS and RAPS) the first thing to check is whether you are sacrificing adaptivity for a better set size, and for that I would expect the authors to compare APS and APS + theirs in table 9."}, "questions": {"value": "1. Please provide comparison on worst slice (slab) coverage (see Romano et al 2020, appendix) between your method and APS.\n2. Please compare your method and APS on the set size coverage violation metric."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9q73RHo15I", "forum": "fnKaWiM6LW", "replyto": "fnKaWiM6LW", "signatures": ["ICLR.cc/2026/Conference/Submission1275/Reviewer_TJxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1275/Reviewer_TJxr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755374273, "cdate": 1761755374273, "tmdate": 1762915724873, "mdate": 1762915724873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Flexible Prediction Sets (FPS), a post-hoc framework that learns order-preserving transformations of predicted class probabilities to reduce conformal prediction set sizes while maintaining coverage guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive theoretical analysis including coverage preservation (Theorem 1), generalization bounds with approximation and estimation error decomposition.\n\nExperiments across ImageNet, ImageNet-V2, and Banking77 with multiple base architectures demonstrate consistent set size reductions."}, "weaknesses": {"value": "The fundamental insight of transforming scores before conformal prediction overlaps significantly with existing calibration methods like temperature scaling. Using sigmoid function to construct a differentiable set size loss already exists in conformal training literatures. While order-preservation is a useful constraint, the core contribution is incremental.\n\nRequires separate tuning dataset and hyperparameter optimization $M, \\gamma, \\beta$, adding complexity compared to standard CP methods. The need for a held-out tuning set may be prohibitive in data-scarce scenarios.\n\nThe paper describes FPS as a \"post-hoc and computationally light\" framework but provides no explicit quantitative measures of the time required to learn the transformation $g_a$. \n\nThe exponential trigonometric polynomial parameterization appears tedious given that established monotonic models already exist in the literature. The authors justify this choice for \"structural guarantee,\" \"approximation power,\" and \"optimization stability\" but don't adequately compare against simpler alternatives like monotonic splines or constrained neural networks.\n\nThe transformation framework appears specifically tailored to APS-type cumulative scoring functions, raising questions about its applicability to other conformal prediction methods. While the authors briefly mention extension to RAPS, the method's compatibility with other important scoring functions (e.g., margin-based scores, entropy-based scores, or model-specific scores) remains unexplored.\n\nAssumption 2 (vanishing loss update) appears restrictive with limited empirical justification beyond Appendix C. The assumption that calibration component updates remain small may not hold generally across different base models and datasets.\n\nFocus primarily on marginal coverage with limited conditional coverage analysis. The brief SSCV analysis in Appendix F shows some degradation in group conditional coverage, which could be concerning for fairness-sensitive applications."}, "questions": {"value": "The authors should address the concerns raised in the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xmpkUOt0qu", "forum": "fnKaWiM6LW", "replyto": "fnKaWiM6LW", "signatures": ["ICLR.cc/2026/Conference/Submission1275/Reviewer_r1Mf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1275/Reviewer_r1Mf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842418209, "cdate": 1761842418209, "tmdate": 1762915724767, "mdate": 1762915724767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}