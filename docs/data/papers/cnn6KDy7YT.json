{"id": "cnn6KDy7YT", "number": 21764, "cdate": 1758321476936, "mdate": 1759896904354, "content": {"title": "FindingDory: A Benchmark to Evaluate Memory in Embodied Agents", "abstract": "Vision-Language models (VLMs) have recently demonstrated impressive performance in planning and control tasks, driving interest in their application to robotics. Yet their deployment in embodied settings remains limited by the challenge of incorporating long-term experience, often spanning multiple days and represented by vast image collections. Current VLMs typically handle only a few hundred images at once, underscoring the need for more efficient mechanisms to manage long-term memory in embodied contexts. To meaningfully evaluate these models for long-horizon control, a benchmark must target scenarios where memory is essential. Existing long-video QA benchmarks neglect embodied challenges like object manipulation and navigation, which require low-level skills and fine-grained reasoning over past interactions. Moreover, effective memory integration in embodied agents involves both recalling relevant historical information and executing actions based on that information, making it essential to study these aspects together. In this work, we introduce FindingDory, a new benchmark for long-range embodied tasks in the Habitat simulator. FindingDory evaluates memory-centric capabilities across 60 tasks requiring sustained engagement and contextual awareness in an environment. The tasks can also be procedurally extended to longer and more challenging versions, enabling scalable evaluation of memory and reasoning. We further present baselines that integrate state-of-the-art closed-source and fine-tuned open-source VLMs with low-level navigation policies, assessing their performance on these memory-intensive tasks and highlighting key areas for improvement.", "tldr": "A scalable benchmark in Habitat testing agents on long-horizon embodied tasks requiring memory, contextual reasoning, and navigation.", "keywords": ["Embodied Agents", "Memory", "Long Context VLMs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88a8706de28f23ce598cad21d51013ae24866975.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce FindingDory, a new benchmark designed to evaluate long-term memory in embodied agents, addressing the limitations of VLMs in handling long-term experiences. Set in the Habitat simulator, this benchmark includes 60 procedurally extensible tasks that require agents to reason over vast image collections from past interactions to solve spatial, temporal, and semantic challenges. The paper evaluates several state-of-the-art VLMs, including GPT-40 and Gemini-2.0-Flash, and finds they struggle significantly, with the best proprietary models achieving success rates below 28%. These models performed especially poorly on multi-goal tasks and temporal reasoning, and their performance often degraded when provided with more video frames, highlighting the need for more efficient memory mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper addresses a crucial and underexplored area in embodied AI: evaluating long-horizon memory in realistic, interactive scenarios. The introduction persuasively argues for the gap between static video QA benchmarks and the memory requirements of active embodied tasks.\n+ The benchmark uses well-specified, multi-faceted metrics, such as HL-SR/HL-SPL for high-level goal selection and LL-SR/LL-SPL for navigation. It also includes relaxed variants like DTG-SR and SC-SR to diagnose specific failure modes. The success criteria are concrete, defined by distance, angle, semantic coverage, and room region, and the evaluation procedures are clearly detailed.\n+ A broad, templated task suite is provided, featuring 60 templates that cover spatial, temporal, and semantic reasoning and include multi-goal variations. This task suite is also procedurally extensible, allowing for scalable evaluation.\n+ The paper delivers concrete, category-level findings. For example, it shows that VLMs can semantically detect large receptacles but fail to precisely localize them (where SC-SR is much higher than DTG-SR). Other findings include a significant gap between HL-SR and HL-SPL, which highlights VLM failure in selecting the nearest correct entity , and the extreme difficulty models face with multi-goal tasks."}, "weaknesses": {"value": "+ The evaluation treats \"unsolvable\" instructions as failures. Although the paper quantifies that 3.85% of tasks are \"Originally Unsolvable\", the agent is not given a way to abstain or declare a task unsolvable and must output frame indices, which diverges from realistic agent requirements.\n+ Despite being mentioned by the authors as one of the limitations, data-generation artifacts may confound memory signals; specifically, the \"magic grasp\"  used by the oracle agent for pick-and-place actions creates abrupt, non-naturalistic transitions. This could weaken temporal cues and confuse VLMs trying to track interactions.\n+ Attribute tasks rely on GPT-4o-generated object descriptions and are validation-only. This reduces reproducibility/consistency between train and val and may introduce bias from an external model. \n+ A navigation bottleneck in the hierarchical policy evaluation may mask the VLM's true memory quality. The overall success rate (LL-SR) drops sharply when the high-level Qwen VLM is paired with a low-level ImageNav policy. This is attributed to a distribution shift, as VLM-selected goal frames (e.g., from pick-place routines) often provide poor visual cues compared to those the navigation policy was trained on. This makes it difficult to isolate the VLM's memory contribution in the end-to-end system.\n+ The paper reports all key results as point estimates (e.g., Figure 2) without confidence intervals or standard deviations from multiple runs. While the high cost of full evaluations is acknowledged, this lack of uncertainty reporting also extends to smaller-scale analyses like the video length subsampling study and the cross-benchmark transfer experiments, where multi-run statistics might have been more feasible."}, "questions": {"value": "1. How might the \"unsolvable\" task design and \"magic grasp\" data artifacts skew the evaluation of an agent's true memory capabilities?\n2. Given the significant navigation bottleneck, how could the benchmark better isolate the VLM's high-level memory reasoning from low-level execution failures?\n3. Considering the lack of uncertainty reporting and the use of external models for attribute tasks, what steps could improve the benchmark's statistical reliability and reproducibility?\n4. Do authors plan to include diverse embodiments beyond the Fetch robot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EifWwnfSHp", "forum": "cnn6KDy7YT", "replyto": "cnn6KDy7YT", "signatures": ["ICLR.cc/2026/Conference/Submission21764/Reviewer_63kF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21764/Reviewer_63kF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441397488, "cdate": 1761441397488, "tmdate": 1762941924237, "mdate": 1762941924237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark designed to evaluate long-horizon memory in embodied agents. \n\nThe benchmark operates in the photorealistic Habitat simulator and is structured in a two-phase process: an initial experience collection phase where an oracle agent performs a series of pick-and-place interactions over a long trajectory, followed by an interaction phase where a test agent is given the complete history (video, poses, actions) and must complete memory-dependent tasks. The paper argues that this setup effectively isolates memory capabilities from exploration challenges. \n\nFINDINGDORY features 60 diverse, procedurally generated task templates that probe spatial, temporal, and conditional reasoning. The authors evaluate a range of state-of-the-art Vision-Language Models (VLMs) within a hierarchical policy framework, where a high-level VLM selects a goal frame from the history and a low-level policy navigates to it. The results demonstrate that current VLMs, even powerful closed-source models, struggle significantly with these tasks, particularly those involving multi-hop temporal reasoning or multiple sequential goals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper addresses a clear and critical gap in current research: the lack of rigorous benchmarks for long-horizon memory in embodied agents. The authors convincingly argue why existing video QA and embodied QA benchmarks are insufficient. The problem it tries to solve is of great significant.\n\n(2) I think the two-phase setup that decouples experience collection from the evaluation phase is a very strong methodological choice. This design effectively isolates an agent's memory and reasoning capabilities from its exploration skills, allowing for a more controlled and focused evaluation.\n\n(3) The paper provides a comprehensive analysis of modern VLMs, revealing their significant shortcomings in long-context embodied reasoning. The finding that performance often degrades with more frames for frozen VLMs, and that all models struggle with multi-goal and fine-grained spatial tasks, offers valuable insights to the community."}, "weaknesses": {"value": "(1) My main reservation is that the evaluation is tightly coupled with a specific hierarchical policy (VLM for goal-frame selection + navigation controller). The paper itself shows in Figure that the performance of this system drops massively when the low-level navigation policy is introduced. This makes it difficult to disentangle the source of failure. For example, is a low success rate on a task due to the VLM's inability to recall the correct information, or is it because the VLM correctly identified a goal, but the chosen frame provided poor visual cues for the navigation policy, causing it to fail? I think this confounding factor complicates the claim that the benchmark purely evaluates memory.\n\n(2) Moreover, I wonder if the current evaluation truly measures memory in an agent-centric sense (i.e., the ability to build and maintain a compressed, internal world state) or if it primarily tests the long-context retrieval capabilities of VLMs on a single, massive prompt. The paper criticizes needle-in-a-haystack tasks but the baseline setup, which feeds the entire video history to the VLM at once, feels conceptually similar."}, "questions": {"value": "(1) Could you please elaborate on the significant performance drop between HL-SR and LL-SR? How can we be confident that the benchmark results reflect the memory limitations of VLMs, rather than the fragility of the chosen low-level controller or the distribution shift between its training data and the VLM-selected goal frames?\n\n(2) How does the FINDINGDORY setup, particularly for the evaluated baselines, differentiate itself from long-video QA tasks that test retrieval from a long context? Have you considered alternative agent architectures that would force the model to build an explicit and evolving memory representation, rather than processing the full history at each step? （If the answer is already in the appendix and I overlook it (this might happen) , please also kindly point it out）"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "utDPhUwEwr", "forum": "cnn6KDy7YT", "replyto": "cnn6KDy7YT", "signatures": ["ICLR.cc/2026/Conference/Submission21764/Reviewer_Akdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21764/Reviewer_Akdb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576859806, "cdate": 1761576859806, "tmdate": 1762941923946, "mdate": 1762941923946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FINDINGDORY, a benchmark developed to evaluate the long-horizon memory capabilities of agents within the Habitat simulator. Its primary focus is on assessing long-range spatiotemporal reasoning that relies solely on memory, while mitigating confounding factors particularly associated with exploration. The benchmark comprises 60 diverse navigation tasks, categorized along dimensions of memory requirements. It integrates dynamic environments, which necessitate agents to reason over evolving contexts, and supports procedural extensibility—allowing task complexity to scale in tandem with advancements in embodied agents. The paper’s main contributions are threefold: 1) the creation of 60 diverse navigation tasks that demand spatiotemporal reasoning; 2) the evaluation of high-level policies combined with low-level navigation policies in memory-intensive scenarios; and 3) the derivation of insights to inform the development of memory-efficient embodied agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The design of navigation tasks emphasizes that agents rely exclusively on past interaction information for high-level goal selection. This explicitly assesses VLMs' memory retrieval capabilities across diverse dimensions, including single-target spatial tasks, single-target temporal tasks, and multi-target tasks.\n2. The integration of high-level and low-level policies reveals that different low-level navigation skills lead to varying degrees of task completion when performing memory-based practical navigation.\n3. Tasks can be progressively extended in complexity in accordance with the evolving capabilities of VLMs."}, "weaknesses": {"value": "Oracle agents were employed during memory collection, which appears to introduce several stringent assumptions:\n1. The memory construction assumes that all consecutive subtasks are successfully completed, and each is executed in the most efficient manner, i.e., via the shortest path. The experiences collected through this method are excessively \"clean\" and inconsistent with real-world scenarios. This is because completing multiple subtasks is highly challenging—evidenced by a mere 26.4% success rate on the GOAT-Bench [1]. This assumption restricts the approach’s applicability in complex, real-world memory settings.\n2. All memories are stored in video format. Due to the upper limit on the number of images that VLMs can process, downsampling becomes necessary during memory retrieval. However, frame-rate-based downsampling has been shown in Mobility VLA [2] experiments to cause information loss, leading to a certain degree of performance degradation—particularly in the processing of small objects. This seems to introduce a level of unfairness in the experimental setup (lines 82-83). A more efficient memory storage method should perhaps be adopted to standardize the input image sets across different VLMs, such as using informative snapshot images to store key information, as implemented in 3D-Mem [3].\n\n[1] GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation. \n[2] Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs. \n[3] 3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning. \n\nTypo: Line 818: \"the and effector\" -> \"the end effector\""}, "questions": {"value": "1. What exactly is the distance threshold for success determination? In lines 426-427 of the paper, a threshold of 1.0 meter is selected for receptacles; however, in lines 855-856, the thresholds are specified as 2.0 meters for objects and 0.1 meters for receptacles.\n2. Although both SPL and SR are expressed as percentages, they describe distinct aspects of navigation performance. Combining them in a single visualization, as done in Figures 3c and 3d, makes the comparison rather unintuitive.\n3. It is suggested to add an analysis of failure causes for high-level retrieval, particularly by comparing the performance of Qwen before and after fine-tuning. This analysis would clarify the specific capabilities enhanced through the fine-tuning process.\n4. In Figure 4a, the fine-tuned Qwen demonstrates a trend where success rate increases with video length. However, the experimental results are truncated at 96 frames. Could extending the video length further improve the success rate? Additionally, the input limits of different VLMs should be explicitly indicated in this figure.\n5. The paper repeatedly mentions the advantage of visual realism. It is recommended to supplement with real-robot experiments to demonstrate that the fine-tuned Qwen exhibits a smaller sim-to-real gap in terms of visual perception."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YLHoatHF7z", "forum": "cnn6KDy7YT", "replyto": "cnn6KDy7YT", "signatures": ["ICLR.cc/2026/Conference/Submission21764/Reviewer_NYTL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21764/Reviewer_NYTL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716777341, "cdate": 1761716777341, "tmdate": 1762941923581, "mdate": 1762941923581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a benchmark focused on challenging and evaluating embodied AI agents memory-intensive tasks within a virtual environment. Among the manuscripts claims are that the proposed benchmark is best-suited for evaluating long-horizon tasks, provides a comprehensive evaluation for VLM-based high-level (goal-prediction) policies as well as low-level navigation policies, and provides new effective and extensible metrics for studying/developing memory-efficient agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The manuscript is well-written and well-organized\n- The manuscript considers compelling tasks and reasoning mechanisms in Embodied AI\n- The manuscript provides a reasonable set of initial experiments that provide sufficient headroom for subsequent research\n- The paper provides a good amount of experiments"}, "weaknesses": {"value": "The manuscript is missing a principled discussion of why the tasks were generated in the way that they were. Why were the memory tasks generated according to the templates in Section 3.1, specifically? How was it ensured in the task design that these tasks are meaningful in some way, e.g., resemble naturally-occurring tasks?"}, "questions": {"value": "Nothing additional; please see questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5DHkvohUj4", "forum": "cnn6KDy7YT", "replyto": "cnn6KDy7YT", "signatures": ["ICLR.cc/2026/Conference/Submission21764/Reviewer_AukX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21764/Reviewer_AukX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982737909, "cdate": 1761982737909, "tmdate": 1762941923314, "mdate": 1762941923314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}