{"id": "6FFQ007qLX", "number": 4825, "cdate": 1757772847288, "mdate": 1759898011033, "content": {"title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models", "abstract": "Recent advances in video generation have produced powerful diffusion models capable of generating high-quality, temporally coherent videos. We ask whether space-time tracking capabilities emerge automatically within these generators, as a consequence of the close connection between synthesizing and estimating motion. We propose a simple but effective way to elicit point tracking capabilities in off-the-shelf image-conditioned video diffusion models. We simply place a colored marker in the first frame, then guide the model to propagate the marker across frames, following the underlying video’s motion. To ensure the marker remains visible despite the model’s natural priors, we use the unedited video's initial frame as a negative prompt. We evaluate our method on the TAP-Vid benchmark using several video diffusion models. We find that it outperforms prior zero-shot methods, often obtaining performance that is competitive with specialized self-supervised models, despite the fact that it does not require any additional training.", "tldr": "propose a simple and effective zero-shot tracking approach: by placing a colored marker in the first frame, we guide the model to propagate the marker across frames, following the underlying video’s motion.", "keywords": ["video diffusion models", "tracking", "tracking any point", "diffusion", "corresponding", "matching", "video generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60409733e75de2d83d0d915459d82404e15b7117.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Point Prompting, a novel method for zero-shot point tracking using pre-trained I2V diffusion models. It achieves strong performance without any training by adding a visual marker to the first frame and leverage the internal priors of the model to propagate the marker through the generated video."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposed method is novel and simple. It utilizes the existing priors of video diffusion models without requiring costly fine-tuning or specialized architectures.\n\n2. The paper provides thorough ablation studies that validate the contribution of its different components.\n\n3. This paper demonstrates the model-agnostic performance covering diffusion model (CogVideo X) and flow-based model (Wan 2.1 and 2.2)"}, "weaknesses": {"value": "1. A major practical limitation is the computational expense. As noted by the authors in L338-339, tracking a single point requires take 7 to 30 minutes. This makes the approach impractical and unfeasible for large-scale offline analysis.\n\n2. It is unclear if the method can handle tracking multiple points simultaneously.\n\n3. More analytical experiments are needed. For instance, the paper would benefit from an attention-based analysis to explore how the model focuses on the point, or an investigation into how tracking paths vary with different random seeds.\n\n4. I wonder how the performance would be affected by using DDIM inversion for the video generation process, rather than the simpler noising approach used."}, "questions": {"value": "1. The explanation of the evaluation metrics (Positional Accuracy, Occlusion Accuracy, and Average Jaccard) could be more detailed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0e7AQnNzkr", "forum": "6FFQ007qLX", "replyto": "6FFQ007qLX", "signatures": ["ICLR.cc/2026/Conference/Submission4825/Reviewer_nWQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4825/Reviewer_nWQb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634861891, "cdate": 1761634861891, "tmdate": 1762917597687, "mdate": 1762917597687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores repurposing large-scale pre-trained video generative models for object tracking. It leverages the rich spatio-temporal representations learned by these models and adapts them in a training-free manner to track targets across frames. The paper is well-written, logically structured, and presents clear quantitative results that complement the experimental tables."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* Clearly motivated and interesting idea of repurposing large-scale pre-trained video generative models for object tracking.\n* Intuitive and easy-to-understand method.\n* Comprehensive experiments with both quantitative metrics and qualitative results demonstrating the approach’s effectiveness."}, "weaknesses": {"value": "* Runtime comparison of each component in the pipeline (i.e., the time cost of steps listed in Table 4)?\n* What are the effects of different denoising steps? Was a simple Euler solver used, and what is the time schedule?\n* Is the tracking performance evaluated on real-world data or synthetic data (e.g., self-generated clips)?\n* Can you confirm whether the models are run in image-to-video mode?\n* Can you compare with the latest works on flow-matching model editing? [1-2]\n\n\nRef:\\\n[1] Jiao, G., Huang, B., Wang, K.C. and Liao, R., 2025. Uniedit-flow: Unleashing inversion and editing in the era of flow models. arXiv preprint arXiv:2504.13109.\\\n[2] Kulikov, V., Kleiner, M., Huberman-Spiegelglas, I. and Michaeli, T., 2025. Flowedit: Inversion-free text-based editing using pre-trained flow models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 19721-19730)."}, "questions": {"value": "Please see the [Weakness] section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jaftBxykCb", "forum": "6FFQ007qLX", "replyto": "6FFQ007qLX", "signatures": ["ICLR.cc/2026/Conference/Submission4825/Reviewer_sTu5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4825/Reviewer_sTu5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637421223, "cdate": 1761637421223, "tmdate": 1762917597379, "mdate": 1762917597379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether pretrained image-conditioned video diffusion models exhibit emergent point-tracking capabilities. The authors propose a “point prompting” technique in which a red dot is placed on the first frame of a real video, the video is regenerated using SDEdit, and the propagated dot is tracked via color-based detection. Several heuristics (color rebalancing, negative prompting, and inpainting refinement) are introduced to stabilize the dot’s visibility across frames. Experiments on TAP-Vid show improvements over image-based zero-shot tracking baselines, and the authors claim that these results indicate temporal reasoning and object permanence in video diffusion models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Limitations are clearly articulated, with helpful visual examples that make failure modes easy to interpret.\n- Unlike some zero-shot correspondence approaches, the work attempts to handle occlusion."}, "weaknesses": {"value": "### 1. Limited novelty and lack of conceptual advancement\n\nThe core claim that video diffusion models contain emergent temporal correspondences, has already been demonstrated by prior work such as DiffTrack [1]. The statements in lines 35–36 and 92–93 suggest novelty in analyzing emergent tracking in video diffusion models, but the conceptual contributions closely follow existing findings and provide little new insight into the temporal behavior of DiT-based models.\n\n\n### 2. The method is not suitable for either analysis or tracking\n\n(1) Misalignment with analysis goals\n\nThe methodology relies heavily on pixel-space operations rather than model-level signals. The pipeline includes removing all red pixels from the input, adjusting global color balance, reducing marker saturation after generation, performing inpainting refinement when the dot drifts. These operations substantially alter the video content and disconnect the analysis from the model’s inherent behavior. Because the method depends on multiple rounds of video regeneration, it does not capture the model’s natural temporal consistency. Table 4 further shows that performance sharply degrades without the heuristic refinements, indicating that consistency comes from the heuristics rather than from the generative model itself.\n\n(2) Inefficiency as a point tracking method\n\nThe method tracks the dot purely based on pixel color, ignoring positional encoding and geometry, and failing on rapid motion. Because each point requires re-generating the entire video often more than once, the pipeline is extremely inefficient. \n\nOverall, the method behaves more like a handcrafted video-editing pipeline than a principled analysis of temporal correspondences.\n\n### 3. Writing and presentation issues\n- The definition of the tracking problem is unclear (pixel-level vs. semantic vs. object-level).\n- Lines 36–38 contain vague phrasing (“high-level understanding tasks”) and ambiguous pronoun (\"these capabilities\") use.\n- The Related Work section blends supervised, self-supervised, and counterfactual modeling approaches without clear structure.\n\n### 4. Missing comparisons and limited generalization\n- The most relevant baseline, DiffTrack [1], is not included in quantitative comparisons, which weakens the empirical evaluation.\n- The method applies only to image-conditioned video diffusion models and cannot be used for text-to-video models, contradicting claims of architectural generality.\n- Experiments do not analyze how point radius interacts with input resolution, potentially biasing comparisons across models.\n\n\n[1] Nam, Jisu, et al. \"Emergent Temporal Correspondences from Video Diffusion Transformers.\" (NeurIPS 2025)"}, "questions": {"value": "- Lines 107–108 claim that DINOv2 has been adapted for temporal correspondence. What specific prior work supports this claim? A citation is required.\n- The paper reports (line 338-341) runtime for a single 50-frame generation, but the full pipeline requires at least two rounds (generation + refinement). What is the actual cost per tracked point?\n- In Table 3, what exactly is the configuration represented by the second row (“DAVIS 256×256 up.”)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k3izE4wGlT", "forum": "6FFQ007qLX", "replyto": "6FFQ007qLX", "signatures": ["ICLR.cc/2026/Conference/Submission4825/Reviewer_duWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4825/Reviewer_duWk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982133756, "cdate": 1761982133756, "tmdate": 1762917597135, "mdate": 1762917597135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method that adopts video diffusion models for point tracking.  It first puts a colored marker on the query point in the first frame, then asks the video diffusion models to propagate the marker across frames by generating new videos. To avoid the loss of the marker in video generation, this paper proposes to use the unedited video’s initial frame as a negative prompt. The proposed method is evaluated on the TAP-Vid benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using a colored marker to indicate the query point is interesting and insightful. \n2. It is also interesting to use an unedited video’s initial frame as a negative prompt to make the marker visible.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. This paper requires video generation to get the tracking results. What is the computational cost of this method for tracking one point, compared to methods that do not use diffusion models?\n\n2. This approach requires generating a video for each tracked point, which is difficult to use in real applications. \n\n3. Previous work[1] already shows that video diffusion models have an inherent ability for point tracking. Simlilar observation is also proposed in [2].  \n\n[1] Emergent Temporal Correspondences from Video Diffusion Transformers\n[2] Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation.\n\n4. The tracking performance is much lower than non-diffusion methods such as CoTracker3. Although it performs better than DIFT and SD-DINO, these two methods are not designed for point tracking in videos.  Considering the high computation cost, the tracking accuracy is not good enough.\n\n5.  According to line 305, the proposed method allows the video diffusion models to \n>  generate only regions near the potential tracked point. \n\nWhat if there are occlusions or significant object motions in the videos？\n\n6. The accuracy of the tracking method is limited by the generation ability of video diffusion models, which limits the effectiveness of the proposed method in diverse scenarios. This also raises my concern about the robustness of the proposed method in cases such as \nsmall objects,  corrupted videos, or videos with poor weather, and so on.\n\n7. How does the method track points in long videos that exceed the maximum length supported by video diffusion models? \n\n8. This paper lacks the text “Under review as a conference paper at ICLR 2026” in the header, which is unexpected according to the official template."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PrPs7q2WFC", "forum": "6FFQ007qLX", "replyto": "6FFQ007qLX", "signatures": ["ICLR.cc/2026/Conference/Submission4825/Reviewer_yFzS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4825/Reviewer_yFzS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993579300, "cdate": 1761993579300, "tmdate": 1762917596586, "mdate": 1762917596586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}