{"id": "grurs3mktz", "number": 13078, "cdate": 1758213372782, "mdate": 1759897466988, "content": {"title": "Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning", "abstract": "We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complementary information between images and metadata. Melanoma detection and skin lesion classification based solely on images, pose significant challenges due to large variations in imaging conditions (lighting, color, resolution, distance, etc.) and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient and for deciding which lesions may be malignant and need to be excised by considering the patient's medical history as well as the appearance of other lesions of the patient. Inspired by this, SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information. By fully exploiting all available data modalities throughout the learning process, the proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesion classification tasks, highlighting the learned representations quality.", "tldr": "SLIMP leverages nested contrastive learning to fuse visual and clinical data for skin lesion analysis in all stages of the training process. The approach yields richer representations, improving downstream classification performance.", "keywords": ["melanoma", "skin cancer", "self-supervision", "representation learning", "tabular metadata"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bb9d28596dd85ed9df18c44e81d3237b8688044.pdf", "supplementary_material": "/attachment/ce90f2bd15c303aa532db0b5c720f9e2cb2f9ade.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SLIMP (Skin Lesion Image–Metadata Pre-training), a multi-modal self-supervised framework combining lesion images, lesion-level metadata, and patient-level metadata. It introduces a two-level (“nested”) contrastive learning scheme and extends it with (1) continual pre-training for addressing differences in metadata attributes and imaging modalities and (2) retrieval-based metadata extrapolation to handle missing modalities. Experiments on multiple dermatology datasets show consistent improvements over self-supervised and multi-modal baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The retrieval-based approach for getting pseudo-metadata is an interesting idea for coping with missing modalities in clinical datasets.\n2. Experiments are comprehensive, covering multiple datasets, ablations, and low-shot settings."}, "weaknesses": {"value": "1. The key idea, “Nested Contrastive Multi-Modal Learning,” is not well-defined. The method essentially applies two standard InfoNCE losses at the lesion and patient levels. The “nested” terminology seems to only indicate the two-level setup rather than a novel framewrok.\n2. Related work lacks discussion on dermatology-specific foundation models (e.g., PanDerm[1]). \n[1] Yan, S., Yu, Z., Primiero, C., Vico-Alonso, C., Wang, Z., Yang, L., ... & Ge, Z. (2025). A multimodal vision foundation model for clinical dermatology. Nature Medicine, 1-12.\n3. The methodological section is logically disorganized. The description merges multiple concepts—nested contrastive learning, continual pre-training, and retrieval-based extrapolation—without clear separation between training phases, losses, and objectives. e.g.  The handling of catastrophic forgetting in Sec. 3.2 is reduced to a hand-wavy “fine-tune only a restricted set of parameters,” without specifying which. \nThe methodology is not yet publication-ready and requires substantial rewriting to clarify the model pipeline and training logic."}, "questions": {"value": "1. In Table 2, what exactly differentiates supervised learning (TFormer) from continual pre-training (SLIMPImage) in terms of training data, loss functions, and supervision signals? How should readers interpret “continual pre-training” as a self-supervised adaptation step without reference labels? In addition, please clarify the relationship between continual pre-training and linear probing.\n2. Could the authors provide a clear process of training stages (pre-training → continual pre-training) and specify which datasets and losses are used at each step? e.g., The statement like \"For continual pre-training on target datasets, we fine-tune the embedding layers of the image and metadata encoders, keeping their attention layers frozen.\" is confusing. \n3. In the retrieval-based metadata extrapolation, how is retrieval quality validated? Could mismatched metadata harm classification performance or introduce bias?\n4. Regarding comparisons with generic and multi-modal foundation models (e.g., MAE, CLIP, SigLIP) — were those baselines also pre-trained on skin lesion data? If not, this comparison may be unfair since SLIMP benefits from in-domain pre-training. Please clarify how the domain mismatch was handled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GglXhGJuDZ", "forum": "grurs3mktz", "replyto": "grurs3mktz", "signatures": ["ICLR.cc/2026/Conference/Submission13078/Reviewer_4Q9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13078/Reviewer_4Q9R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677350822, "cdate": 1761677350822, "tmdate": 1762923807048, "mdate": 1762923807048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SLIMP (Skin Lesion Image-Metadata Pre-training), a novel nested multi-modal contrastive learning approach for learning rich representations of skin lesions. The method combines lesion images with both lesion-level and patient-level metadata to improve performance on downstream skin lesion classification tasks. The authors claim that by capturing complementary information from different modalities, SLIMP produces more representative and generalizable features. They also propose methods for adapting the model to target datasets with different metadata structures, including a continual pre-training approach and a metadata extrapolation strategy for image-only datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper addresses an important problem in medical imaging—the effective use of multi-modal data for diagnosis. Improving skin lesion classification can have a direct impact on early melanoma detection.\n2.  The overall approach is presented clearly, and the motivation is well-explained. Figure 1 provides a good visual summary of the SLIMP architecture.\n3.  The paper considers the practical challenges of working with multiple medical datasets, such as diverging metadata schemas, and proposes reasonable solutions (continual pre-training and metadata extrapolation)."}, "weaknesses": {"value": "1.  The novelty of SLIMP appears to be rather limited. In Figure 1, the most crucial contribution seems to be the InfoNCE loss, which was proposed in 2018. Additionally, the feature concatenation approach is quite common.\n2.  The authors claim several contributions (nested loss, continual pre-training, metadata extrapolation). However, there are no ablation studies to quantify the individual impact of each component. For example, how does the nested loss compare to a \"flat\" contrastive loss that combines all metadata? How much does continual pre-training improve performance compared to zero-shot transfer?\n3.  The idea of transferring metadata based on image similarity is interesting but potentially risky. The paper should provide a more in-depth analysis of when this works and when it might fail (e.g., if visually similar lesions have very different metadata due to patient history)."}, "questions": {"value": "1.  Could you please provide ablation studies to demonstrate the individual contributions of the nested contrastive loss, the continual pre-training, and the metadata extrapolation? For instance, what is the performance if you only use a single-level contrastive loss?\n2.  Regarding the metadata extrapolation, have you analyzed the failure cases? Are there instances where visually similar images from the reference and target datasets have significantly different (and clinically important) metadata, leading to incorrect \"extrapolation\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7wtll2fs0Y", "forum": "grurs3mktz", "replyto": "grurs3mktz", "signatures": ["ICLR.cc/2026/Conference/Submission13078/Reviewer_CbJ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13078/Reviewer_CbJ2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917940202, "cdate": 1761917940202, "tmdate": 1762923805191, "mdate": 1762923805191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a contrastive learning module to align the skin disease image data with patients' meta data and diseases' meta data to achieve collaborative learning within all these elements. Through such a multimodal contrastive learning module, classification accuracy is higher and more robust."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance of the proposed model is better than existing methods due to the complementary information from metadata.  \n2. Sufficient ablation studies prove the effectiveness of the proposed modules."}, "weaknesses": {"value": "1. The proposed method requests a visual image, disease meta data and patients meta data. So many information requirements will constrain the model generalization in different situations.  \n2. The proposed method introduces contrastive learning loss between image and tabular data. What is the difference from standard contrastive learning loss?  \n3. Some skin disease datasets are not similar to the used dataset. How does the model ensure the transferability of these datasets?  \n4. The evaluation mainly relies on benign simplification, and more fine-grained details are needed for real diagnosis"}, "questions": {"value": "1. How does the model ensure the smooth performance when patients' meta data is missing or noisy?  \n2. In the metadata extrapolation setting, how to deal with the situations if the reference and target datasets are very different.  \n3. Please address the difference with standard contrastive learning loss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m7qjwET1lt", "forum": "grurs3mktz", "replyto": "grurs3mktz", "signatures": ["ICLR.cc/2026/Conference/Submission13078/Reviewer_b9ns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13078/Reviewer_b9ns"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950571671, "cdate": 1761950571671, "tmdate": 1762923804674, "mdate": 1762923804674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SLIMP, a novel self-supervised pre-training framework for learning skin lesion representations by using a nested multi-modal contrastive learning on image-metadata pairs. The authors propose aligning lesion images with lesion-specific metadata and aggregating these representations to patient-specific images and metadata. The authors also propose a continual pre-training to adapt to other datasets and a retrieval-based method to extrapolate metadata in case of missing metadata. The proposed SLIMP outperformed existing methods on downstream classification tasks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposed a well-motivated approach in contrastive pre-training for medical image-metadata pairs and offered practical solutions like continual pre-training and metadata extrapolation to adapt pre-trained methods to other datasets, even when the metadata is not available."}, "weaknesses": {"value": "The SLICE-3D dataset has 393 malignant samples and around 400k benign samples. On the patient level, there are patients for whom there is no malignant sample, and even for patients with malignant samples, the number of malignant samples can be very small compared to the number of benign samples. Unlike supervised training that would have a detector specifically focus on the feature representing malignant lesions, pre-training on this dataset with only optimizing the distance between the image and metadata representation is expected to have difficulty teaching accurate representations to the model that can distinguish malignant samples. \n\nMoreover, in their patient-level representation learning, they are average pooling over all the lesions of a patient, which is expected to further drown out the information from a malignant lesion over the information from a considerably higher number of benign lesions.\n\nTheir proposed positive sampling would work on the patients that actually have malignant lesions, which makes it less effective since there are generally more patients that have no malignant lesions in SLICE-3D and similar datasets.\n\nThe authors have mentioned that they have split the target datasets randomly for downstream tasks, which does not ensure there is no overlap between patients in the training and test sets and can potentially lead to overoptimistic performance metrics."}, "questions": {"value": "1. Could the authors clarify more information regarding the downstream evaluation splits? It would strengthen the paper if the performance of SLIMP were shown on patient-disjoint training and testing sets.\n\n2. In cases when a dataset has a distribution different from the SLICE-3D dataset (such as lesion types or patient demographics not well represented in SLICE-3D), would there be any performance benefit in retrieving the closest matching metadata?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lOfy5o4oGs", "forum": "grurs3mktz", "replyto": "grurs3mktz", "signatures": ["ICLR.cc/2026/Conference/Submission13078/Reviewer_SBYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13078/Reviewer_SBYQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244112203, "cdate": 1762244112203, "tmdate": 1762923804156, "mdate": 1762923804156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}