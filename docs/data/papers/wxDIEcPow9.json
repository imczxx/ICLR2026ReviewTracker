{"id": "wxDIEcPow9", "number": 24659, "cdate": 1758359076270, "mdate": 1759896756378, "content": {"title": "RECTOR: Masked Region-Channel-Temporal Modeling for Cognitive Representation Learning", "abstract": "Affective and cognitive disorders are characterized by complex, distributed brain network dynamics across distinct functional regions, channels, and time, posing a significant challenge to learning robust representations for clinical diagnosis. We introduce $\\textbf{RECTOR}$ (Masked $\\textbf{Re}$gion–$\\textbf{C}$hannel–$\\textbf{T}$emp$\\textbf{or}$al Modeling), the first end-to-end, self-supervised brain region modeling framework that unifies region, channel, and temporal representation learning in a single architecture. At its core is $\\textbf{RECTOR-SA}$, a novel hierarchical self-attention mechanism that incorporates anatomical priors with data-adaptive gating to efficiently model region-channel-temporal interactions in a sparse, block-wise paradigm. RECTOR further integrates $\\textbf{RECTOR-Mask}$, a multi-view masking strategy that generates intra-sample region-channel-temporal blocks. It enables self-supervision driven by $\\textbf{NC}^\\textbf{2}$$\\textbf{-MM}$, our learning objective that synergistically optimizes multi-view masked modeling, block-level contrastive learning, and variance-covariance regularization to learn robust representations. Finally, we introduce $\\textbf{RCReg}$, a tailored regularization on region–channel tokens that prevents trivial region features and enables learning of both region-common and channel-specific representations. Across diverse benchmarks, RECTOR sets a new state-of-the-art in EEG emotion recognition and sEEG task-engagement classification. In addition, it achieves superior computational efficiency in spatio-temporal self-attention, demonstrates strong potential for large-scale pre-training, and provides interpretable, multi-view insights into neural representations at both brain region and channel levels.", "tldr": "RECTOR is a novel self-supervised EEG/sEEG framework that models region, channel, and temporal dynamics via a hierarchical self-attention mechanism for cognitive representation learning.", "keywords": ["self-supervised learning", "self-attention mechanism", "masked modeling", "contrastive learning", "EEG", "sEEG", "affective and cognitive disorders"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a45ce6bff41f00a5891fb541e73975a16e3373a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes RECTOR, a self-supervised framework for EEG and sEEG cognitive representation learning that explicitly models region–channel–temporal interactions. The key contributions include: (1) RECTOR-SA, a hierarchical sparse attention mechanism incorporating anatomical priors and dynamic gating; (2) RECTOR-Mask, a structured multi-view masking strategy that creates region- and time-aware masked modeling targets; (3) NC²-MM, a unified learning objective that combines masked modeling and contrastive learning within one architecture; and (4) RCReg, a specialized regularization for improving region–channel token representations. The model achieves state-of-the-art results across EEG emotion recognition and sEEG task-engagement classification benchmarks, with supporting ablations and interpretability analyses."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Ambitious attempt to integrate spatial priors and self-supervision in neural signal modeling.  \n\n2. Structured masking and hierarchical attention are intuitively motivated.\n\n3. Experimental results are comprehensive, covering multiple datasets, protocols, and baselines, including ablations that validate each core component of the architecture.  \n\n4. The method provides neuroscientifically interpretable results at both region and channel levels, demonstrating alignment with known physiological patterns."}, "weaknesses": {"value": "1.The figures in the manuscript need improvement, especially Figures 2 and 3. Figure 4 is significantly clearer in comparison.\n\n2.The method’s novelty appears incremental rather than fundamental. Most components (structured masking, region tokens, gated attention, variance/covariance regularization) are adaptations of well-known techniques with domain-specific adjustments rather than a distinctly new contribution.\n\n3.The writing is dense, and the paper tends to overstate its contributions relative to the demonstrated novelty.\n\n4.The anatomical prior design is under-justified. Region partitioning is treated as fixed and universally correct, but inter-subject anatomical variability is substantial in EEG/sEEG. The paper does not assess the robustness or validity of this assumption.\n\n5.Pretraining only on each target dataset weakens the claim of general-purpose self-supervised learning.\n\n6.Critical methodological details are placed in the appendices and should be included in the main paper to ensure clarity and reproducibility.\n\n7.Considering the complexity of the proposed method, the absence of released code makes reproducibility difficult."}, "questions": {"value": "1.Could Figures 2 and 3 be redesigned with improved layout and clearer color schemes to enhance readability?\n\n2.What is the key novel contribution beyond combining existing components such as structured masking and hierarchical attention?\n\n3.How do you justify the strength of your claims relative to the demonstrated novelty?\n\n4.How robust is the anatomical prior (fixed region partitioning) to inter-subject variability in EEG/sEEG?\n\n5.How does pretraining only on each target dataset support claims of general SSL generalization?\n\n6.Can you move critical methodological details from the appendix into the main text to improve clarity and reproducibility?\n\n7.How do you conduct the leave-one-subject-out (LOSO) evaluation? Is there a hold-out validation set used to determine the number of training epochs and hyperparameters?\n\n8.Will the code and pretrained models be released to ensure reproducibility given the complexity of the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yJlRm4b3f9", "forum": "wxDIEcPow9", "replyto": "wxDIEcPow9", "signatures": ["ICLR.cc/2026/Conference/Submission24659/Reviewer_v9Zs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24659/Reviewer_v9Zs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580043967, "cdate": 1761580043967, "tmdate": 1762943150668, "mdate": 1762943150668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper *RECTOR: Masked Region–Channel–Temporal Modeling for Cognitive Representation Learning* proposes RECTOR, a self-supervised learning framework for EEG and sEEG representation learning that jointly models region-, channel-, and temporal-level dependencies. Its core contributions include a novel **RECTOR-SA** hierarchical self-attention mechanism integrating anatomical priors for efficient region-channel-temporal modeling, a **RECTOR-Mask** structured multi-view masking strategy for more challenging pretext tasks, and **NC2-MM**, a combined non-contrastive × contrastive learning objective. Additionally, **RCReg** regularizes region-channel tokens to enhance feature disentanglement. The model achieves state-of-the-art results on EEG emotion recognition and sEEG task-engagement classification while claiming higher computational efficiency and interpretability compared to prior works. However, despite strong empirical results, the paper largely repackages existing ideas—masked modeling, contrastive loss fusion, and anatomical priors—into a composite architecture. The novelty is incremental and primarily architectural, lacking theoretical rigor or strong neuroscientific grounding. The extensive ablations and comparisons suggest solid engineering, but the work leans more toward technical aggregation than conceptual innovation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive ablations: Evaluates the effect of masking ratios, loss weights, and feature hierarchies.\n\n2. Multi-dataset evaluation: Demonstrates generalization across EEG (emotion) and sEEG (task) domains.\n\n3. Integrated pipeline: Combines anatomical priors with deep self-supervised frameworks, improving biological plausibility relative to generic transformers.\n\n4. Engineering soundness: Implementation is well-optimized and includes clear reproducibility details and comparison tables."}, "weaknesses": {"value": "1. Limited novelty: The key ideas—masked modeling, hierarchical attention, hybrid contrastive objectives—are incremental reuses of prior designs (MAE, BYOL, DINO, MoCo-v3, etc.) rather than a fundamentally new direction.\n\n2. Weak theoretical grounding: No analysis of why combining non-contrastive and contrastive terms yields better cognitive representations.\n\n3. Poor interpretability: Despite claiming cognitive alignment, there is little neuroscientific analysis (e.g., brain-region relevance or neurobiological validation).\n\n4. Superficial discussion: Results are over-interpreted as “state-of-the-art” without effect size reporting or significance testing.\n\n5. Unclear scalability: It is uncertain whether RECTOR can scale to large, multi-site EEG datasets or handle real-world noise.\n\n6. Dataset limitations: The training and evaluation datasets are small (dozens to hundreds of subjects), which limits generalization claims.\n\n7. Overclaiming contributions: The claim of being “the first unified region–channel–temporal framework” ignores earlier hierarchical EEG models (e.g., EEG-GraphMAE, ST-MAE, Brain-MAE)."}, "questions": {"value": "1. How does RECTOR-SA differ fundamentally from existing spatio-temporal attention modules used in EEG-GraphMAE or ST-MAE?\n\n2. What motivates the NC²-MM hybrid loss—can you show a theoretical analysis of how it prevents representation collapse?\n\n3. How are anatomical priors encoded? Are these static adjacency matrices or learned embeddings, and how sensitive is performance to parcellation choice?\n\n4. Can you report per-subject and per-session variance or confidence intervals for downstream metrics?\n\n5. How does RECTOR perform under noisy or low-density EEG setups—does the hierarchical attention degrade gracefully?\n\n6. Have you compared against self-distillation approaches (e.g., DINO-style EEG pretraining) to isolate the benefit of your masking strategy?\n\n7. How interpretable are the learned representations—do any attention maps align with known cortical functional networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Vg1cPg8lW", "forum": "wxDIEcPow9", "replyto": "wxDIEcPow9", "signatures": ["ICLR.cc/2026/Conference/Submission24659/Reviewer_4y2h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24659/Reviewer_4y2h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702614075, "cdate": 1761702614075, "tmdate": 1762943150312, "mdate": 1762943150312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This papers introduces a complete and exhaustive method for self-supervised deep learning framework for EEG and sEEG data, accounting the various aspects and dynamics of these brain activity modalities (region, channels, temporal). It introduces multiple modules, in particular RECTOR-SA and RECTOR-Mask, for feature extractions at different scales and combines masked modelling, contrastive learning, and variance–covariance regularisation for training objectives. The methodology is benchmarked against multiple models and training schemes (supervise-only, self-supervised) and outperforms many models on two main tasks: EEG emotion recognition and sEEG cognitive states."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is generally well-written. The figures are complete, very descriptive (even though some of the legends could benefit from thorough descriptions, see below for more details). \n\nThe methodology, although quite exhaustive, is addressing one of the blindspot of many EEG (and even in some sense fMRI) studies, which is taking into account the spatial (and regional) and temporal dynamics in EEG signal. In particular accounting for regional specific features rather than aggregating spatial information is a nice contribution. \n\nThe evaluation framework is comprehensive and well thought with comparison against many training frameworks (supervised, self-supervised models) and multiple datasets. The ablation studies are also welcome considering the many modules that are introduced by the paper. \n\n The colour coding in the tables makes the results very easily readable."}, "weaknesses": {"value": "One important issue with the current state of the submission is the general arrangement of information within the paper. \n1. It is (very) difficult to understand at first what is the training objective of the model (what is going to be predicted), what the model aims at solving and how it aims at doing it. All this could be clearer from the beginning of the 2. Methodology section. \n2. The paper tends to over-complexified some of the notations (Figure 2) and wordy terminology e.g. \"sparse region-channel-temporal self-attention embedded with anatomical priors and dynamic functional attention\", which can obscur a bit the interesting concepts introduced by the method.\n3. Most importantly, it seems that most of the main modules are not fully described within the main text of the paper, but are detailed in the appendix. Many points in the methods are referring to the appendix, which make it impossible to understand from the \n\nAnother remark would be that the paper is extremely dense, some might say too dense, at a point were it is difficult to apprehend the entirety of the method with only the main text of the submission. This kind of paper would probably benefit from simpler iterations, to appreciate the real value of every added module. This is also considering that some concepts are only introduced in the appendix. This amount of content can be seen as detrimental for the overall appreciation of the paper. Therefore, I would recommend to streamline the paper and remove the unnecessary content for that publication. \n\nIn particular, I would recommend switching some of the paragraphs and reorganising/rewriting the methodology section in order to have full descriptions of the RECTOR modules (in particular RECTOR-SA) in the text (not in the appendix) - section 2.2 is too high level and figure 2 is not explained enough to be stand-alone, clear description of the pipeline (it is difficult to understand how the modules interact with each other), explanation of concepts such as the brain partitioning (which is in Appendix E) but is one of the key point of the paper. Instead the \"Complexity\" paragraph could be added to the appendix. In the current shape, the methodology is difficult to understand\"\n\nFigure 3 would also benefit from more descriptive legend."}, "questions": {"value": "- It is not clear from the main text how RECTOR is fine-tuned on the downstream task?\n\nOther remarks were listed in the previous section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MRe4HLciDG", "forum": "wxDIEcPow9", "replyto": "wxDIEcPow9", "signatures": ["ICLR.cc/2026/Conference/Submission24659/Reviewer_HqHh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24659/Reviewer_HqHh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941013986, "cdate": 1761941013986, "tmdate": 1762943149946, "mdate": 1762943149946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RECTOR, a self-supervised framework for EEG/sEEG data that integrates region, channel, and temporal representation learning through a novel hierarchical self-attention mechanism (RECTOR-SA). The model incorporates anatomical priors and functional attention to capture complex spatio-temporal interactions in neural data. Evaluated across several EEG datasets (SEED, SEED-IV, DEAP, MSIT, ECR), RECTOR demonstrates state-of-the-art performance in emotion recognition and task engagement classification. The paper claims that the model not only improves computational efficiency but also provides strong interpretability through attention visualizations, paving the way for its application in neurocognitive diagnostics and personalized interventions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed RECTOR framework introduces a novel approach by combining self-supervised learning with anatomical priors and dynamic functional attention for EEG/sEEG data, a method previously explored in fMRI but applied to EEG data for the first time. \nThe manuscript is well-written and clearly structured. The experiments are thorough and effectively address the research question, providing solid evidence for the physiological plausibility of the learned representations. The results are presented clearly, with supporting analyses that validate the model's performance."}, "weaknesses": {"value": "The paper's evaluation is limited to the SEED and DEAP datasets, and it would benefit from validation on a broader range of downstream tasks to better assess RECTOR's generalizability. The use of only F1-score as the evaluation metric is restrictive; incorporating other standard metrics such as Cohen’s Kappa, weighted F1, and additional classification metrics would provide a more comprehensive performance analysis. The ablation studies, while useful, lack a detailed examination of the self-attention mechanism, and the gating mechanism within RECTOR-SA is not addressed. Furthermore, the paper provides visualizations of learned representations but lacks a deeper interpretability analysis, especially in terms of attention maps, spatial EEG features, and feature attribution. Lastly, the pretraining details, including hyperparameters,  pretraining dataset, training time, are unclear, which raises concerns about reproducibility and scalability."}, "questions": {"value": "1. Downstream Evaluation: Given the model’s promising performance on SEED and DEAP, can RECTOR be extended to other EEG/sEEG tasks with different cognitive states or sensor modalities? Evaluating RECTOR on a wider variety of tasks would clarify how well the model generalizes across different applications, such as emotion recognition, cognitive task engagement, or even clinical diagnostics for neurological disorders.\n2. Evaluation Metrics: The paper primarily uses F1-score, which is valuable but limited. Would the authors consider evaluating RECTOR using other metrics like Cohen’s Kappa (which accounts for agreement between class predictions) and weighted F1 (to account for class imbalance)? Comparing RECTOR with task-specific models rather than foundation models (such as those designed specifically for EEG emotion recognition) would provide more meaningful insights into its performance.\n3. Ablation Study on Attention and Gating: While ablation studies are provided, could the authors conduct more detailed experiments focusing specifically on the RECTOR-SA attention mechanism? How does each component (e.g., region-based vs. global attention) contribute to the model’s overall performance? Additionally, the gating mechanism within RECTOR-SA is mentioned but not ablated—what role does it play in the model, and how does it impact performance across tasks?\n4. Interpretability and Visualization: The paper includes some visualizations of learned representations, but a deeper analysis of the model’s internal behavior is lacking. Could the authors include attention maps, coherence heatmaps, or feature attribution to show how RECTOR attends to relevant spatial and temporal patterns in EEG? This would help validate whether the spatial awareness captured by the model is truly driving the observed performance improvements.\n5. Pretraining Process: The paper does not provide sufficient details on the pretraining process, such as hyperparameters, optimization strategy, or batch size. Understanding these details would help in replicating the results and assessing the model’s scalability to other datasets. Could the authors clarify the pretraining procedure and explain how these choices impact the model’s final performance?\n6. Usage of LLM missing.\n7. No code revealed"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LirLHZlNYT", "forum": "wxDIEcPow9", "replyto": "wxDIEcPow9", "signatures": ["ICLR.cc/2026/Conference/Submission24659/Reviewer_hYFg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24659/Reviewer_hYFg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957921523, "cdate": 1761957921523, "tmdate": 1762943149670, "mdate": 1762943149670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}