{"id": "CjviBpoetJ", "number": 2735, "cdate": 1757228792867, "mdate": 1759898130330, "content": {"title": "See the Text: From Tokenization to Visual Reading", "abstract": "People see text.\nHumans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively.\nModern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. \nWhile effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation.\nIn this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. \nOur method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text–vision alignment abilities learned from large-scale multimodal training.\nAcross three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43× fewer tokens and reducing FLOPs by 70.5\\%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy.\nSeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.", "tldr": "Our method signals a shift from symbolic tokenization to human-like visual reading by processing text visually.", "keywords": ["Multimodal Large Language Models", "Vision-centric Tokenization", "Text Tokenization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11691ae0207c021b8956ef74504d03f9fa061fa8.pdf", "supplementary_material": "/attachment/350f2a1a42950964910c1ac8aeba4a22dbb0da30.zip"}, "replies": [{"content": {"summary": {"value": "This paper challenges the dominant subword tokenization paradigm in LLMs by proposing SEETOK, a vision-centric alternative inspired by human reading. Instead of breaking text into discrete subword units, SEETOK renders text as an image and leverages the vision encoder of a pretrained Multimodal Large Language Model (MLLM) to process it. This \"visual tokenization\" is followed by a lightweight LoRA-based instruction tuning phase to adapt the MLLM to understand instructions presented in this visual-text format."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experimental validation is extensive and convincing. The authors evaluate on a diverse set of tasks (QA, translation, sentiment) and languages (13+), and perform deep-dive analyses into efficiency (FLOPs, latency), robustness (multiple perturbation types), and compositionality. The results are consistently strong across the board."}, "weaknesses": {"value": "A similar idea in VisInContext: Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ka60SkZJoE", "forum": "CjviBpoetJ", "replyto": "CjviBpoetJ", "signatures": ["ICLR.cc/2026/Conference/Submission2735/Reviewer_Kw1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2735/Reviewer_Kw1w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583996363, "cdate": 1761583996363, "tmdate": 1762916353054, "mdate": 1762916353054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SEETOK, a vision-centric tokenization method that renders textual input as images and leverages pretrained vision-language models (MLLMs) to process language tasks through visual pathways. The authors propose treating the vision encoder and projector as a pluggable “visual tokenizer,” which replaces traditional subword tokenization. Coupled with lightweight LoRA-based instruction tuning, SEETOK enables existing MLLMs to process textual tasks with reduced token count, lower FLOPs, and improved robustness to orthographic perturbations. The paper presents comprehensive evaluations covering multilingual efficiency, perturbation robustness, and compositionality, and conducts hierarchical geometric analysis via orthogonal Procrustes to support the claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear engineering contribution: The paper proposes a practical and well-motivated framework that treats rendered text as input for vision encoders in existing MLLMs. This vision-as-tokenizer approach is modular, minimally invasive, and compatible with off-the-shelf MLLMs, requiring only LoRA-based tuning without retraining the full model.\n\n2. System-level benefits: The authors go beyond token count reduction to quantify end-to-end FLOPs and latency improvements (e.g., 70.5% FLOP reduction and 33.5% latency drop on TriviaQA), making a compelling case for the method’s efficiency in real-world deployments.\n\n3. Solid empirical support: The experimental section is thorough, with multilingual translation, robustness testing, compositionality probing, and ablation studies. The reported results indicate competitive performance in multilingual and perturbed settings, as well as encouraging signs of compositional alignment."}, "weaknesses": {"value": "**Major Issues**\n\n1. Incremental novelty: While the integration into MLLMs is elegant, the central idea, rendering text as images and processing it via vision encoders, has been extensively explored in prior work (e.g., PIXEL, CLIPPO, and CLIP-style MLLMs). Many of the claimed benefits (e.g., robustness to perturbation, fertility reduction, multilingual fairness) are inherent not specific to SEETOK. The primary contribution lies in integrating this paradigm into general-purpose MLLMs with lora finetuning.\n\n2. Lack of evaluation on vision-native tasks: The paper does not assess whether SEETOK compromises the model’s original visual reasoning capabilities, particularly on standard benchmarks such as VQA. This raises questions about whether the instruction tuning deteriorates native visual-language performance.\n\n3. Underwhelming absolute accuracy: On some core benchmarks like MMLU (Table 5 and 6), SEETOK lags behind pure-text tokenization, with the performance gap only partially compensated by its efficiency or robustness advantages. The narrative leans heavily on efficiency and multilinguality, but the trade-off in task-specific accuracy should be more candidly discussed.\n\n4. Missing citations: Key related works are absent, such as:\n\n[1] Textural or Textual: How Vision-Language Models Read Text in Images, ICML 2025.\n\n[2] How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads, EMNLP 2025.\n\n**Minor Issues**\n\n1. Fairness of perturbation evaluation: In Section 4.4, the comparison between pure-text and vision-centric models on homoglyph or character attacks may be inherently biased. Pure-text models are expected to be more sensitive to glyph-level noise, whereas vision-based models may be more vulnerable to stroop-style interference (e.g., visually red “blue”) or font-style mismatches. These asymmetric vulnerabilities deserve more nuanced analysis.\n\n2. Missing evaluation on LLaVA-family models: The current experiments primarily cover Qwen2.5 and JanusPro backbones. Adding results on LLaVA or other open-source MLLMs could further validate generalizability."}, "questions": {"value": "1. Capability retention on standard VQA tasks: How does SEETOK impact performance on vision-native tasks (e.g., VQA, TextVQA, DocVQA)? Including such evaluations would strengthen the claim of SEETOK as a general-purpose plug-in.\n\n2. Finer-grained ablation: Table 4 reports the effect of tuning different components. Could the authors include intermediate ablations, e.g., “only tune projector” vs. “only tune vision encoder”? This would clarify the contribution of each module.\n\n3. Absolute accuracy plots for perturbation: Figure 4 shows accuracy drops, but absolute accuracy values under each noise setting would help readers gauge practical utility more clearly.\n\n4. Compositional proximity analysis: Similar to Figure 5 (left), it would be interesting to report whether SEETOK embeddings of \"lemon\" are closer to lime or demon."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jPI2gfD3KX", "forum": "CjviBpoetJ", "replyto": "CjviBpoetJ", "signatures": ["ICLR.cc/2026/Conference/Submission2735/Reviewer_DoBY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2735/Reviewer_DoBY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651971803, "cdate": 1761651971803, "tmdate": 1762916352723, "mdate": 1762916352723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an approach to replace all the text tokens with vision tokens of the text image in vision language models for language-only tasks, and conducts comprehensive experiments to evaluate the advantages of doing so. Their experiments show that after further instruction-tuning, models using visual-text tokens achieve performance comparable to those using pure text tokens across various language-only benchmarks. Moreover, they show that such an approach may enable more holistic text perception, shown by less performance drop when perturbing the elements in the text."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using pure visual tokens as input is well motivated. The experiments to reveal the benefit of using visual-centric tokenization are comprehensive. The Perturbation Probing study is an interesting investigation that reveals the unique advantage of holistic perception using visual tokens to represent text."}, "weaknesses": {"value": "1. My major concern remains whether the performance gain comes from the finetuning process or SEETOK itself,  especially considering the low performance on SST5 for the original Qwen2.5-VL 3B model. Although it’s discussed in Table 6, it would strengthen the paper’s conclusion to also show the evaluation results for all the datasets evaluated in Table 1 and Table 3, rather than only showing the results of MMLU. I would be more curious about the results of TriviaQA and SST5.\n\n2. Line 429: “even though the finetuning was performed using visual-text data, the model benefits from better cross-format generalization, enhancing its pure text performance. ”. It’s also hard to get such a conclusion, given that finetuning with the same data already has +0.30 improvement, and it’s even under a reduced training dataset size. Would expect to see similar results evaluated by other datasets to make the conclusion solid.\n\n\n\nMinor: Page 9 has too much bold text that actually influences the reading. I don’t think method names like “Orthogonal Procrustes Analysis” and “residual norm” really need to be bolded."}, "questions": {"value": "Question:\nFor Table 6, what does it mean to have SEETOK with pure-text as both training input and inference input (line 4)? I thought the whole point of SEETOK is to utilize the visual token as a representation of text and finetune both the vision encoder and LLM decoder. This should be just considered as a LORA fine-tuning for the baseline model, or does it have anything different from a common fine-tuning process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VPs5zpZyCT", "forum": "CjviBpoetJ", "replyto": "CjviBpoetJ", "signatures": ["ICLR.cc/2026/Conference/Submission2735/Reviewer_QVXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2735/Reviewer_QVXp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933476074, "cdate": 1761933476074, "tmdate": 1762916352398, "mdate": 1762916352398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SEETOK, a vision-centric tokenization method for large language models that renders text as images and leverages pretrained multimodal LLM vision encoders to interpret them. Instead of using traditional subword tokenization, SEETOK processes text visually through the vision pathway of models like Qwen2.5-VL and JanusPro. The method uses LoRA adapters for lightweight fine-tuning to enable visual-text instruction following. Experiments across natural language understanding tasks, multilingual translation, and robustness evaluations demonstrate that SEETOK achieves comparable or superior performance to text tokenization while requiring 4.43× fewer tokens, reducing FLOPs by 70.5%, and showing improved cross-lingual generalization, robustness to typographic noise, and compositional understanding.\n\nOverall, it is an incremental paper to continue to explore the vision tokenizer for LLMs, the approach proposed in this work is okay, which tries to solve some issues in the encoder part, but the decoder part is still traditional text tokenizer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a way to leverage the visual tokenization for text processing. By leveraging the pretrained MLLMs with LoRA adaptation is efficient and pragmatic, avoiding expensive training from scratch. \n\n2. The proposed SEETOK demonstrates good performance on several categories of tasks, including QA, translation, cross-lingual transfer etc. And also shows it robustness.\n\n3. The proposed SEETOK tokenizer also shows its superb efficiency in token comprehension and FLOP reduction."}, "weaknesses": {"value": "1. Overall, it is interesting to see that introducing the vision tokenization as the tokenzier for text. While, for the detokenization part, it still rely on the traditional text tokenizer, may still suffer from the existing issues for text tokenizer. This might be not a perfect point. So, how to implement a full vision-centric tokenizer here for text?\n\n2. Table 1 shows SEETOK underperforms on MMLU (52.52 vs 61.91) and NQ (24.14 v.s. 29.31), this might be a significant limitation for knowledge-heavy tasks?"}, "questions": {"value": "There are some existing work on leveraging the vision for LM tokenization, for example, screenshot LM [1], PIXAR [2], Pix2Struct[3] etc. Maybe it is better to discuss with them, or compare with them on the shared tasks.\n\n\nAnother question is - how does SEETOK scale to very long sequence/documents (for example, 100K+ tokens in text form)? What happens to memory and computational costs?\n\n\n1. Improving Language Understanding from Screenshots, https://arxiv.org/abs/2402.14073\n2. PIXAR: Auto-Regressive Language Modeling in Pixel Space, https://arxiv.org/abs/2401.03321\n3. Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding, https://arxiv.org/abs/2210.03347"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nWFakUieXO", "forum": "CjviBpoetJ", "replyto": "CjviBpoetJ", "signatures": ["ICLR.cc/2026/Conference/Submission2735/Reviewer_Gw3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2735/Reviewer_Gw3z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953657861, "cdate": 1761953657861, "tmdate": 1762916352034, "mdate": 1762916352034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}