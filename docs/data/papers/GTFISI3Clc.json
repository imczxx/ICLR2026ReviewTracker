{"id": "GTFISI3Clc", "number": 6093, "cdate": 1757952562147, "mdate": 1759897935782, "content": {"title": "Ego-centric Predictive Model Conditioned on Hand Trajectories", "abstract": "In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human–object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions—often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs—visual observations, language, and action history—and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.", "tldr": "We propose a novel egocentric predictive model conditioned on hand trajectories that unifies action prediction and future video generation.", "keywords": ["Egocentric Video", "Multi-modality", "World Model", "Video understanding and generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf07e87a594434dd4ac351e3eb62cc1cf6d818f3.pdf", "supplementary_material": "/attachment/a3373596b6dc756319bbe6086c06b30ce0fde715.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Ego-PM, a unified two-stage egocentric predictive model designed to jointly predict actions and generate future visual frames, conditioned on hand trajectories. Extensive experiments have demonstrated the effect of each proposed component on multiple real-world and robotic datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper combines action modeling with video generation in a single framework.\n\n2. This model demonstrates consistent improvements across multiple benchmarks (Ego4D, BridgeData, RLBench) and modalities."}, "weaknesses": {"value": "1. One of the main contribution of this paper is the joint prediction of action and future frames, however, I do not see the qualitative/quantitative analysis showing the mutual benefit. \n\n2. It is not clear how the model fuses multi-modal information when not using CCA in Table 5. \n\n3. The text format could be further improved (L84-L85).\n\n4. This paper misses some discussions with recent works on conditioned egocentric image/video generation [1][2][3].\n\n[1] Luo et al. \"Put myself in your shoes: Lifting the egocentric perspective from exocentric videos.\" ECCV 2024.\n[2] Xu et al. \"Egoexo-gen: Ego-centric video prediction by watching exo-centric videos.\" ICLR 2025.\n[3] Liu et al. \"Exocentric-to-egocentric video generation.\" NeurIPS 2024."}, "questions": {"value": "1. In the experiments, the authors explored using one or two consecutive frames as context, what about using more time steps?\n\n2. What is the advantage of CCA compared to bi-directional attention in your model?\n\n3. The ablation study on consecutive predictions is not very clear. Please explain in detail.\n\n4. In L456-457, the authors claimed the failure of LWM as its exocentric bias, however, the authors already fine-tuned LWM on egocentric data (L260). It is a bit confusing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ra4Zlppb1L", "forum": "GTFISI3Clc", "replyto": "GTFISI3Clc", "signatures": ["ICLR.cc/2026/Conference/Submission6093/Reviewer_LcVE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6093/Reviewer_LcVE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749281643, "cdate": 1761749281643, "tmdate": 1762918459734, "mdate": 1762918459734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical gap in egocentric AI: the disjointed modeling of future actions and their visual outcomes. This paper proposes Ego-PM, a novel two-stage framework that unifies action prediction and future video generation conditioned on hand trajectories. The core idea is to use predicted hand motion as an intermediate, guiding representation to ensure that generated future frames are both visually plausible and physically consistent with the intended action. Below are contributions： \n1.\tFirst Unified Action-Visual Predictive Model: This is the first model capable of jointly predicting both the upcoming action (as a hand trajectory) and the future visual frames resulting from that action. \n2.\tNovel Architectural Innovations: Consecutive State Modeling (CoSMo): In Stage I, the model predicts future actions by conditioning on two consecutive previous states. Causal Cross-Attention (CCA): In Stage II, the action embedding from Stage I serves as the Query to perform causal cross-attention over the visual and textual context (Keys/Values). \n3.\tDemonstrated Generality and Practicality: The same model architecture, without task-specific modifications, achieves state-of-the-art or competitive performance on three distinct benchmarks: Ego4D (human egocentric videos), BridgeData V2 (real-world robot demonstrations), and RLBench (simulated robotic tasks). Crucially, the model requires no external action annotations at inference time, predicting the hand trajectory autonomously, which enhances its practical applicability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The core originality lies in its novel problem formulation: the joint modeling of egocentric action prediction and future video generation within a single, unified framework. This is a distinct advance over prior work that addressed these tasks separately. \nQuality: The technical quality is high. The two-stage architecture is well-motivated and built upon strong, modern components (LLaVA, LDM). The experimental quality is exceptional, featuring rigorous evaluation on three diverse and challenging datasets (Ego4D, BridgeData V2, RLBench). \nClarity: The paper is generally well-structured and clearly written. The logical flow from problem identification to solution and evaluation is easy to follow. \nSignificance: The work is highly significant for the research community. It directly addresses a critical capability for embodied AI systems: understanding the coupling between an action and its perceptual consequences."}, "weaknesses": {"value": "Analysis of Error Propagation in Long-Horizon Prediction: The experiments primarily focus on predicting one or two frames into the future (e.g., t=2, t=3). A key challenge in predictive modeling is error accumulation over longer sequences. The paper would be significantly strengthened by an analysis of the model's performance degradation when generating longer video horizons (e.g., 20 frames). Does the CoSMo strategy provide robustness against compounding errors compared to baselines? \n\nComputational Efficiency and Latency: The proposed two-stage pipeline, involving a large autoregressive model and an iterative diffusion model, is computationally intensive. For real-world applications like robotic planning, inference speed is critical."}, "questions": {"value": "Long-Horizon Generalization: The model is evaluated on very short-term predictions. Could the authors demonstrate or discuss its potential for longer-horizon prediction (e.g., 20 frames)? What are the main failure modes (e.g., object deformation, trajectory drift) when the prediction horizon extends, and how might the architecture be adapted to address them? \n\nAblation on Action Representation: The action is represented as hand trajectory coordinates for Ego4D and 7D robot poses for Bridge/RLBench. How critical is the specific form of this action representation? Did the authors experiment with other representations, such as a more abstract latent action space (e.g., akin to AdaWorld), and if so, how did it impact performance? \n\nCausal Cross-Attention Analysis: The CCA module is a key innovation. Could the authors provide more analysis or visualization of the attention maps within the CCA? For example, when the action query attends to the visual keys, which parts of the historical frames (e.g., the object, the hand's current position) does it primarily focus on? \n\nComputational Cost and Latency: The paper would benefit from a discussion (and ideally, metrics) regarding the computational cost and latency of the full pipeline compared to the baselines (especially the faster VLA models and single-stage video predictors)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MJn1MLXhej", "forum": "GTFISI3Clc", "replyto": "GTFISI3Clc", "signatures": ["ICLR.cc/2026/Conference/Submission6093/Reviewer_15WE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6093/Reviewer_15WE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792030030, "cdate": 1761792030030, "tmdate": 1762918459247, "mdate": 1762918459247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "is a tow-stage framework for action anticipation and frame generation coupling VLM and LDM through Causal Cross-Attention (CCA). VLM predicts action trajectories that then condition LDM to generate future frames. CCA implements cross-attention with future masking, and is applied to encode and fuse video, text and action for conditioning future frame generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The design is intuitive and leads to an architecture that achieves sota result when compared to few recent baselines."}, "weaknesses": {"value": "Causal Cross Attention, a main element upon which the contribution of the paper is build, is not particularly novel: standard cross-attention with causal masking.\n\nWhy the query in CCA is chosen as the embedding of previous action is not well motivated. It is just adopted this way.\n\nThere is no feedback loop that enforces generated frames to contain generated action trajectories following conditioning action trajectories.\n\nOverall, there is no particularly significant novelty. The method is two-stage and not integrated, and there is no grounding loop in the diffusion with the action trajectory conditioning. Loss terms for training are standard or previously published.\n\nIt is not clarified in the paper why generating future frame in a 2-stage approach without grounding (see above) should be beneficial. For what should the generated video be used?\n\nTables are spread around the second part of paper, and the arrangement of frames in figure 3 could be improved. Figure 3 contains only four generated, sparse frames and this makes it difficult to assess the quality of generated future video."}, "questions": {"value": "Please address any of the weaknesses identified above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0oFQNCYOE9", "forum": "GTFISI3Clc", "replyto": "GTFISI3Clc", "signatures": ["ICLR.cc/2026/Conference/Submission6093/Reviewer_ivaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6093/Reviewer_ivaC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919337432, "cdate": 1761919337432, "tmdate": 1762918458720, "mdate": 1762918458720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method which, given the action history, visual input and language prompt of either an egocentric video or a robot task rollout video, simultaneously generates imagined action rollouts and, conditioned on these, imagined future video frames. The method makes use of a novel cross-attention scheme to condition a latent diffusion model for the future frame synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The design choices of the method are thoroughly ablated in Table 5. The method shows strong performance against baselines in Tables 2 and 3. The inclusion of both a robotic dataset and a human hand dataset showcases the usefulness and versatility of the method."}, "weaknesses": {"value": "The submission could have benefited from the inclusion of more qualitative examples in the supplementary material, comparing the imagined and real future frames. This is crucial to assess the quality of the generations.\n\nI believe Table 3 should be split into two separate tables, evaluating frame prediction and action prediction separately.\n\nIn Table 4, as it stands, the method is strongly outperformed by two baselines depending on 3D point clouds. An interesting comparison would be running the baselines without providing them the additionally required 3D point cloud (e.g. by passing dummy values), to assess the extra gain from its inclusion. I am willing to increase my score if a sensible answer is provided to this point.\n\nIn the paper, please clarify how you calculate the hand IoU. It took me a long time to find the detail that you use hand bounding boxes in the supplement. I believe this information is important enough to be included in the main paper. \n\nA bounding box is arguably far less useful than, for instance, a wrist pose or even hand pose, either in 2D or 3D. Thus, I would encourage the authors to study predicting more informative hand representations for egocentric videos.\n\nTable 1 and Table 2 could benefit from the inclusion of more baselines."}, "questions": {"value": "Why is \"Ours\" bolded for LPIPS in Table 3 when it is outperformed by This&That?\n\nHow do you calculate the GT hand bounding boxes when Ego4D does not provide such poses out-of-the-box? Could the baseline Hand IoU in tables 2 and 3 be calculated by using that same method on the imagined future frames of the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RRbwfCB1Gk", "forum": "GTFISI3Clc", "replyto": "GTFISI3Clc", "signatures": ["ICLR.cc/2026/Conference/Submission6093/Reviewer_qvyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6093/Reviewer_qvyv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976069221, "cdate": 1761976069221, "tmdate": 1762918458196, "mdate": 1762918458196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}