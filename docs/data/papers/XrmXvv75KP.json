{"id": "XrmXvv75KP", "number": 7490, "cdate": 1758024370501, "mdate": 1763727481001, "content": {"title": "SwiftTS: A Swift Selection Framework for Time Series Pre-trained Models via Multi-task Meta-Learning", "abstract": "Pre-trained models exhibit strong generalization to various downstream tasks. However, given the numerous models available in the model hub, identifying the most suitable one by individually fine-tuning is time-consuming. In this paper, we propose \\textbf{SwiftTS}, a swift selection framework for time series pre-trained models. To avoid expensive forward propagation through all candidates, SwiftTS adopts a learning-guided approach that leverages historical dataset-model performance pairs across diverse horizons to predict model performance on unseen datasets. It employs a lightweight dual-encoder architecture that embeds time series and candidate models with rich characteristics, computing patchwise compatibility scores between data and model embeddings for efficient selection. To further enhance the generalization across datasets and horizons, we introduce a horizon-adaptive expert composition module that dynamically adjusts expert weights, and the transferable cross-task learning with cross-dataset and cross-horizon task sampling to enhance out-of-distribution (OOD) robustness. Extensive experiments on 14 downstream datasets and 8 pre-trained models demonstrate that SwiftTS achieves state-of-the-art performance in time series pre-trained model selection. The code and datasets are available at \\href{}{https://anonymous.4open.science/r/SwiftTS-395C}.", "tldr": "We propose a swift selection framework for time series pre-trained models via multi-task meta-learning without fine-tuning.", "keywords": ["time series forecasting", "model selection", "transfer learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/663275868024706b34a8e5b305de0cd16f38cb0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SwiftTS, a learning-guided framework for rapid selection of time series pre-trained models using a multi-task meta-learning strategy. The approach employs a dual-encoder (temporal-aware data encoder and knowledge-infused model encoder) to encapsulate both dataset and model features, leveraging patch-wise cross-attention to compute compatibility scores for model selection across various horizons. To improve generalization across datasets and forecasting horizons, the method incorporates horizon-adaptive expert composition as well as a cross-task meta-learning protocol. SwiftTS is evaluated on 14 real-world time series datasets and 8 pre-trained models, demonstrating strong results against a wide range of feature-analytic and learning-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe idea of selecting more suitable pretrained models for different downstream tasks and datasets is both interesting and valuable, addressing a practically important yet underexplored challenge in time-series foundation modeling.\n\n2.\tThe dual-encoder architecture is well-motivated for heterogeneous time series model pools and directly addresses the issue of costly, inconsistent feature extraction in prior work. The use of a patch-wise attention mechanism reflects a careful design choice that captures local temporal structures relevant for model-dataset compatibility.\n\n3.\tSwiftTS introduces a horizon-adaptive expert composition module that flexibly and effectively addresses horizon-specific variability, as showcased in both the framework diagram and associated experimental tables."}, "weaknesses": {"value": "1.\tThe paper emphasizes that SwiftTS avoids costly forward passes through all candidate models. Yet, the functional embedding module still requires each candidate model to be evaluated (albeit offline) on synthetic inputs such as Gaussian noise. This operation remains linearly proportional to the number of candidate models and does not scale well to continuously evolving model pools. The efficiency claim is therefore only partially valid and should be quantified more carefully.\n2.\tDoes the sampling strategy used in the Temporal-Aware Data Encoder introduce randomness that leads to different results across runs? If so, how is this variance controlled or mitigated during training and evaluation?\n3.\tWhat exactly does the topological structure represent? Specifically, what are the semantic meanings of the nodes and edges in this context?\n4.\tThe results only report the alignment metric between the estimated and true rankings, but not the actual forecasting performance of different selection methods. Without these results, it is difficult to assess the real effectiveness of the proposed selection strategy.\n5.\tWhat are the forecasting results of the pretrained models on these datasets without applying the selection process? Reporting these would provide a clear baseline for evaluating the benefit of the proposed selection mechanism.\n6.\tThere exist many other meta-learning-based methods for time-series forecasting, such as AutoForecast [1], which should be included for comparison in the experimental section.\n7.\tCould the authors clarify the rationale and empirical justification for using Gaussian noise to generate the functional embeddings of candidate models? Would using real or synthetic time-series inputs affect the informativeness or stability of these embeddings?\n8.\tHow sensitive are the framework’s predictions to the number of candidate models in the hub? Does performance degrade with larger, more heterogeneous model pools, and what are the observed scaling behaviors?\n9.\tCould the authors provide more clarity about the construction of meta-training tasks, especially the sampling policy for forecasting horizons and dataset divisions?\n10.\tCould the authors clarify how they handle cases where meta-training tasks share overlapping datasets or closely related forecasting horizons, which may cause distribution leakage or task redundancy?\n\n[1] Abdallah M, Rossi R, Mahadik K, et al. Autoforecast: Automatic time-series forecasting model selection[C]//Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2022: 5-14."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YP50i3fSpI", "forum": "XrmXvv75KP", "replyto": "XrmXvv75KP", "signatures": ["ICLR.cc/2026/Conference/Submission7490/Reviewer_SnbV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7490/Reviewer_SnbV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471386998, "cdate": 1761471386998, "tmdate": 1762919604861, "mdate": 1762919604861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SwiftTS, a learning-based framework for selecting pre-trained time series models efficiently. It uses a dual-encoder architecture (data encoder + model encoder) and multi-task meta-learning to predict model–dataset compatibility without exhaustive fine-tuning. Experiments on 14 datasets and 8 models show strong gains over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses an important and under-explored problem in time series foundation model selection.\n- Well-designed method combining meta, topological, and functional model embeddings.\n- Extensive experiments with clear, consistent improvements."}, "weaknesses": {"value": "- The design choices for the data and model encoders appear somewhat heuristic and lack sufficient justification. For example, why does the model encoder capture domain information while the data encoder does not? The paper could be strengthened by clarifying the design rationale of these encoders.  \n- The meta-learner is trained on a relatively small pool (14 datasets × 8 models); a data-efficiency analysis or a discussion explaining why this scale suffices to learn reliable dataset–model correlations would improve credibility.  \n- The functional embedding is obtained by probing models with Gaussian noise, which seems heuristic. Random noise may not reflect how models respond to real-world temporal structures; a justification for using Gaussian inputs would be helpful."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZmDxr7YJZ", "forum": "XrmXvv75KP", "replyto": "XrmXvv75KP", "signatures": ["ICLR.cc/2026/Conference/Submission7490/Reviewer_ERT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7490/Reviewer_ERT9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549045571, "cdate": 1761549045571, "tmdate": 1762919604568, "mdate": 1762919604568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SwiftTS, a framework for fast and scalable selection of pre-trained time series models via multi-task meta-learning. Rather than fine-tuning each candidate model or relying on computationally expensive feature extraction for each selection, SwiftTS uses a lightweight dual-encoder to embed datasets and models, computes patchwise compatibility via cross-attention, and applies a horizon-adaptive mixture-of-experts approach. Further, the method leverages transferable cross-task meta-learning across datasets and forecasting horizons to enhance out-of-distribution robustness. The framework is evaluated on 14 real-world datasets and 8 pre-trained model families, showing state-of-the-art ranking accuracy and efficiency across a variety of horizons and domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dual-encoder architecture is well-conceived and technically sound. One encoder incorporates temporal awareness through the use of patching and attention mechanisms, while the other enables knowledge injection by integrating architectural metadata, graph-based topological structures, and functional embeddings derived from model behavior. This design is particularly well-justified for highly diverse model repositories.\n\n2. The manuscript presents extensive experimental results and visualizations, which provide strong and multifaceted evidence supporting the effectiveness of the proposed model.\n\n3. The application of meta-learning to address the heterogeneity of time-series domains and various pretrained models is highly appropriate and demonstrates solid methodological reasoning."}, "weaknesses": {"value": "Although the paper shows runtime savings over fine-tuning, there's insufficient discussion of the the practical scaling beyond a fixed model zoo. For example, how does graph2vec embedding scale with hundreds or thousands of models with complex DAGs? Is there a resource bottleneck for functional embedding inference as the number of candidate models grows? The scalability arguments are more empirical than architectural; a more detailed analysis would be valuable."}, "questions": {"value": "1. How is the meta information embedded and utilized? In particular, how are the five types of meta information combined within the model?\n\n2.SwiftTS performs selection among multiple pretrained models. Could it support the addition or removal of models without retraining SwiftTS? This consideration may have implications for the scalability of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1bSK5dLtsP", "forum": "XrmXvv75KP", "replyto": "XrmXvv75KP", "signatures": ["ICLR.cc/2026/Conference/Submission7490/Reviewer_SQsf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7490/Reviewer_SQsf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920421622, "cdate": 1761920421622, "tmdate": 1762919604092, "mdate": 1762919604092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}