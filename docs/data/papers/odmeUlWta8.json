{"id": "odmeUlWta8", "number": 24182, "cdate": 1758353765735, "mdate": 1759896778390, "content": {"title": "Task-Agnostic Amortized Multi-Objective Optimization", "abstract": "Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50–1000× versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.", "tldr": "We introduce a fully amortized (surrogate model + acquisition function), dimension-agnostic policy for multi-objective optimization.", "keywords": ["Multi-Objective Optimization", "Bayesian Optimization", "Transformers", "Neural Processes"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db9b2006249fb5c81739071080478e835ffc274c.pdf", "supplementary_material": "/attachment/c8234f69d8617ce2451c26cc56b88b3e7178c57f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TAMO, a Transformer-based algorithm for black-box multi-objective optimization using a pre-trained policy. It is trained on synthetic Gaussian Process tasks with varying input and output dimensions, combining reinforcement learning and supervised losses for model-agnostic optimization. Experiments show that TAMO matches or surpasses state-of-the-art methods while being 50–1000 times faster at inference time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a _Model-amortized Multi-objective Optimization_ algorithm that achieves significantly lower inference time compared to other MOBO methods. It could be one of the earliest works to leverage the scalability and in-context learning capability of Transformer Neural Process models to improve inference efficiency in MOBO context.\n    \n2. The paper introduces a  Dimension-agnostic embedder to the Transformer that enables dimension-agnostic learning across various MOBO tasks, making it highly generalizable to unseen tasks with varying dimensions, using a single pre-trained model. By adapting this architecture to the MOO setting, the method goes beyond a naive extension of similar approaches in single-objective Meta-BO [2] and BOFormer [1], which also implements a Transformer policy, but without dimension-agnostic learning."}, "weaknesses": {"value": "Besides the limitations mentioned in the paper, there are some non-negligible issues:\n- **Lack of statistical significance in task generalization performance** . The confidence bars of all the baselines, including TAMO, in the out-of-distribution evaluations of problems GP-DX3-DY2, GP-DX3-DY3 in Figure 4 (a) are not well-separated. Thus, Figure 4. only demonstrate limited evidence for the statistical significance of TAMO's superiority in task generalization.\n- **Lack of benchmarks.** Although both synthetic and real-world benchmarks were evaluated, the input and output dimensions are relatively small (both ≤ 4), which limits the results to small-scale MOO problems. Note that the most similar and recent work BOFormer [1] includes benchmark performance with **input dimension up to 30**. Also, there is only a single real-world benchmark compared in the experiments, which is not sufficient to demonstrate the general performance of TAMO in real-world settings.\n    \n- **Lack of baseline comparisons.**\n    \n    1. The information-theoretic approaches discussed in the related work section were not included in the experimental comparisons. Other Transformer-based policies applicable to MOBO settings , such as **Optformer** [4], should also be considered for comparison, which is also compared in the paper of BOFormer [1].\n        \n    2. The performance gap between GP-based and Transformer-based models is not well studied experimentally. Given that GP and Transformer models differ substantially in scale, direct comparison is difficult. To better evaluate the usefulness of the Transformer Neural Process in terms of regret performance, variants of TAMO should be included in the ablation study, e.g., versions with RL policies using other policy parametrizations should be considered, e.g., smaller neural networks (as in [3]). \n- **Necessity of the RL policy.** Although the use of Transformers is well-motivated, which possess strong in-context learning capabilities, the paper provides limited evidence supporting the need for a Transformer-based RL policy. Future experiments could include a baseline that performs GP-based MOBO (e.g., qNEHVI, qParEGO) but replaces the GP surrogates with Transformer Neural Process surrogates (pre-trained on GP priors). Comparing TAMO against such a baseline would help clarify whether using RL to learn a policy is necessary, compared with directly employing a Transformer surrogate. Moreover, this proposed baseline should also benefit from reduced inference time through forward-pass inference, thus, this comparison would also help evaluate the effectiveness of TAMO’s RL policy under similar low-inference-cost conditions"}, "questions": {"value": "See Weakness,\n\nand \n\n- Why is a **mixed-normal regression head** (equation 1) used for the prediction head? Would it be possible to apply a **Bar distribution (Riemann distribution) regression head** instead? [5] has shown that normal regression heads may underperform bar distribution regression heads in terms of supervised-learning efficiency in a similar setting of fitting GP priors.\n    \n- Is there a specific reason for using the **normalized hypervolume level** as the reward, rather than the **normalized hypervolume improvement** [1]? Using net improvement as reward signal is also common in RL-based BO policies [2,3] for single-objective problems. Conceptually, would using the normalized hypervolume level allow repeatedly rewarding the same query, potentially slowing down TAMO’s exploration?\n    \n- **Potentially high training time?** Although TAMO achieves significantly lower inference time than other methods, Transformer-based BO policies have been shown to require substantial training resources (as noted in [2]), even for single-objective BO. There may be a potential issue of high training cost for complex real-world problems with high input/output dimensionality. It would be helpful for the authors to discuss TAMO’s training time.\n\n- In Section 3.2, the definition of positional tokens is confusing for me. In Line 178,\n\t \"These positional tokens are randomly sampled for each batch from fixed pools of learned embeddings\"\n\tWhat's is the definition and the practical implementation of this \"fixed pools of learned embeddings?\"\n- The Section 3.2 is difficult to read, I think it could be improved by providing a full model architecture illustration in Figure 2., explicitly drawing the $B_1, B_2$ blocks of layers and use arrows or attention maps to indicate which tokens can attend to which tokens in each phase.\n\n**Minor issues**\n- Figure 4. is confusing to me before I found (a), (b) in the subfigures. They look very small and are in the top of the subfigures. \n- Some Figure references in the appendix are not well-compiled.\n\n\n**Citations**\n\n[1] Hung, Y. H., Lin, K.-J., Lin, Y.-H., Wang, C.-Y., Sun, C., & Hsieh, P.-C. (2025). BOFormer: Learning to solve multi-objective Bayesian optimization via non-Markovian RL. The Thirteenth International Conference on Learning Representations. [https://openreview.net/forum?id=UnCKU8pZVe](https://openreview.net/forum?id=UnCKU8pZVe)\n\n[2] Alexandre Max Maraval, Matthieu Zimmer, Antoine Grosnit, and Haitham Bou Ammar. End-to-end\nmeta-bayesian optimisation with transformer neural processes. Advances in Neural Information\nProcessing Systems, 34, 2023.\n\n[3] Michael Volpp, Lukas P. Fr¨ohlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter,\nand Christian Daniel. Meta-learning acquisition functions for transfer learning in bayesian opti-\nmization. In International Conference on Learning Representations, 2020.\n\n[4] Yutian Chen, Xiaoxi Song, Chung-Ching Lee, Zihang Wang, Ruoxi Zhang, David Dohan, Kenji\nKawakami, Greg Kochanski, Arnaud Doucet, Marc’Aurelio Ranzato, et al. Towards learning\nuniversal hyperparameter optimizers with transformers. In Advances in Neural Information Pro-\ncessing Systems, volume 35, pp. 32053–32068, 2022.\n\n[5] Müller, S., Hollmann, N., Pineda Arango, S., Grabocka, J., & Hutter, F. (2022). Transformers can do Bayesian inference. International Conference on Learning Representations. [https://openreview.net/forum?id=KSugKcbNf9](https://openreview.net/forum?id=KSugKcbNf9)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "axIPELnwDP", "forum": "odmeUlWta8", "replyto": "odmeUlWta8", "signatures": ["ICLR.cc/2026/Conference/Submission24182/Reviewer_mKPB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24182/Reviewer_mKPB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760539837578, "cdate": 1760539837578, "tmdate": 1762942979132, "mdate": 1762942979132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TAMO, a fully amortized, task-agnostic, dimension-agnostic policy for multi-objective black-box optimization. A single transformer backbone, trained offline with (i) an in-context prediction warm-up and (ii) a policy-level RL objective that maximizes trajectory hypervolume (HV) progress, maps histories and a candidate set to the next query in one forward pass—eliminating per-task surrogate fitting and acquisition optimization. Key architectural elements include a dimension-agnostic embedder, encoder–decoder with task-specific tokens, and dual prediction/policy heads; the policy is trained with REINFORCE on HV-normalized rewards. Empirically, TAMO achieves **50×–1000×** lower proposal time than GP-based MOBO and BOFormer while delivering competitive regret on synthetic and real tasks; it also shows transfer to unseen input/output dimensionalities and decoupled observations. Code is promised upon acceptance; hyperparameters, algorithms, and synthetic pretraining data generation are documented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **End-to-end amortization:** Eliminates per-task surrogate fitting and acquisition optimization; single forward-pass proposals reduce decision latency by **50×–1000×**. \n* **Dimension-agnostic architecture:** Embedder + task-tokens operate across varying dx, dy; supports heterogeneous pretraining and cross-dimensional transfer. \n* **Non-myopic training signal:** RL objective directly optimizes trajectory HV, aligning learning with Pareto-front discovery over horizons. \n* **Generalization studies:** Evidence of transfer to unseen dimensionalities and decoupled observations under fixed budget/cost accounting. \n* **Methodological transparency:** Clear preliminaries, training/inference algorithms, hyperparameters; baselines (qNEHVI, qNParEGO, qHVKG, BOFormer) implemented with standard toolchains. \n* **Performance profile:** Competitive or better regret across several synthetic and real tasks, with consistent runtime advantage."}, "weaknesses": {"value": "* **OOD gaps & sensitivity:** Underperforms classic MOBO on Branin–Currin and LaserPlasma; authors attribute mismatch to pretraining length-scales—this warrants a systematic analysis. \n* **Discrete candidate pool assumption:** Inference relies on a fixed candidate set; implications for high-dimensional continuous or combinatorial design spaces are acknowledged but unresolved."}, "questions": {"value": "1. **Timing fairness & ablations:** Please report per-candidate acquisition latency (µs/ms) and GPU-to-GPU comparisons against GPyTorch/BoTorch with matched MC budgets; include breakdown of surrogate refit vs. acquisition time. \n2. **Pretraining-prior sensitivity:** Provide a controlled study varying kernel families, length-scales, ARD, and output correlations to quantify transfer sensitivity and explain gaps on Branin–Currin/LaserPlasma. \n3. **Continuous action spaces:** Can TAMO couple to a continuous proposal mechanism (e.g., gradient-guided refinement, learned sampler) to move beyond pool-based scoring? Any preliminary results? \n4. **RL stability:** What variance-reduction techniques were used for REINFORCE (e.g., learned baseline, advantage normalization)? Any ablation on γ and λ_{rl}? \n5. **Decoupled observations policy:** How are costs integrated at training time? Could explicit cost-aware reward shaping further improve decoupled performance (e.g., on Ackley–Rosenbrock)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qD3Mw0fPi4", "forum": "odmeUlWta8", "replyto": "odmeUlWta8", "signatures": ["ICLR.cc/2026/Conference/Submission24182/Reviewer_avNh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24182/Reviewer_avNh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916143991, "cdate": 1761916143991, "tmdate": 1762942978736, "mdate": 1762942978736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets three flaws with traditional multi-objective Bayesian optimization methods; that they require re-training from scratch for each use, the reliance on a large number of hyperparameters, and the focus on single-step gains. The authors introduce TAMO, which leverages a transformer-based optimizer trained on diverse synthetic tasks using reinforcement learning to learn multi-step strategies. Once trained, users use iterative forward passes of TAMO and a tokenized history of the steps inputted into TAMO to run the optimization process. The authors claim to match or improve the quality of state of the art methods while reducing proposal time."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The authors present a novel architecture which functions as both the acquisition function and surrogate model. The dimension-agnostic embedder is an interesting solution to generalizing over spaces with varying dimensions. TAMO is demonstrated to have equivalent or superior metrics to many other methods, and is demonstrated to run in a shorter time. The paper is overall pretty clear."}, "weaknesses": {"value": "1. In which cases do multi-step strategies actually matter? When is myopia actually a limiting factor for Bayesian optimization? Can the authors demonstrate an ablation where multi-step strategies provide a clear advantage over traditional BO?\n\n2. There is only one real-world benchmark. Can the authors demonstrate applicability to other real-world scenarios, such as for Gaussian splatting as done in the Boformer paper (Yu-Heng Hung, et al. 2025) or neural network hyperparameter selection as in OptFormer (Yutian Chen, et al. 2022).\n\n3. Can the authors demonstrate clear advantages in practice to other transformer-based MOO methods, such as Optformer?\n\n4. The authors mention that the pretraining data composition is important for generation. The authors should provide a study to experimentally demonstrate how the diversity of the pretraining data affects optimization performance. For the out-of-distribution experiments, authors should demonstrate"}, "questions": {"value": "Is the prediction task fitting to the raw data? Does the policy part of the model not converge without the prediction task? Why is this? Is there any practical application to the prediction mechanism post-training? This seems to be a central part of the architecture and is not thoroughly justified. Can the authors demonstrate an ablation for training with and without the prediction task?\nCan the authors include a figure for the architecture for 3.2 (II) and (III)? The exact model architecture (where self vs cross attention is applied, where task-specific tokens are inputted) is unclear simply due to wordiness and a simple diagram could very clearly explain it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YesbfleeA8", "forum": "odmeUlWta8", "replyto": "odmeUlWta8", "signatures": ["ICLR.cc/2026/Conference/Submission24182/Reviewer_GinK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24182/Reviewer_GinK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932498592, "cdate": 1761932498592, "tmdate": 1762942978413, "mdate": 1762942978413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TAMO, a novel amortized policy for multi-objective black-box\noptimization, aiming to address the high computational cost and task-specific nature of GP-based\nmethods. The core idea is to pre-train a single, dimension-agnostic Transformer on a diverse\ncorpus of synthetic tasks. This pre-trained model can then be deployed on new, unseen problems,\nreplacing the slow, iterative refitting of a surrogate model with a single, fast forward pass to\npropose the next query. The model is trained using a combination of a prediction loss and a\nnon-myopic RL objective based on cumulative hypervolume improvement ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "● Paper is very well written and easy to understand\n● By replacing the iterative GP refitting and acquisition optimization process with a single\nneural network forward pass, the method reduces inference latency by 50-1000x.\n● The proposed dimension-agnostic embedder is a clever architectural contribution . It\nallows a single Transformer backbone to be pre-trained on and deployed to problems of\nvarying input and output dimensions"}, "weaknesses": {"value": "● The framework relies on a pre-defined discrete candidate pool from which the policy\nhead selects the next query. This is a significant limitation as it makes the approach\nunusable for true continuous-domain optimization or generative tasks (like de novo drug\ndesign).\n● The \"task-agnostic\" claim is weakened by the model's sensitivity to the pre-training data.\nThe authors hypothesize that the poor performance on BraninCurrin stems from not\nseeing those objective properties in the synthetic pre-training corpus. The model isn't\ntruly \"agnostic\" but is rather \"multi-task\" for a specific family of synthetic GP-based\ntasks."}, "questions": {"value": "● The model has two heads (Prediction and Policy) and is trained with a joint loss.\nHowever, the contribution of the auxiliary prediction task L(p) is not ablated. How much\ndoes the \"warm-up\" and joint training contribute to the final policy versus simply training\nthe policy head alone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "j5ZhNqf3aM", "forum": "odmeUlWta8", "replyto": "odmeUlWta8", "signatures": ["ICLR.cc/2026/Conference/Submission24182/Reviewer_eMcs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24182/Reviewer_eMcs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762663088256, "cdate": 1762663088256, "tmdate": 1762942978192, "mdate": 1762942978192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}