{"id": "Lo7YJOK6cv", "number": 6789, "cdate": 1757995752532, "mdate": 1759897893869, "content": {"title": "SNN-Driven Multimodal Human Action Recognition via Sparse Spatial-Temporal Data Fusion", "abstract": "Recent multimodal action recognition approaches that combine RGB and skeleton data have achieved strong performance, but their high computational cost and poor energy efficiency hinder deployment on edge devices. To address these limitations, we propose the first spiking neural network (SNN)-based framework for multimodal human action recognition, to the best of our knowledge, offering an energy-efficient and scalable solution that fuses sparse spatiotemporal data of event cameras and skeletons within a unified spiking architecture. The framework leverages the sparse and asynchronous nature of event and skeleton data and the energy-efficient properties of SNNs. It achieves this through a series of tailored components, including modality-specific feature extraction, a sparse semantic extractor, spiking-based cross-modal fusion via Spiking Cross Mamba, and task-relevant feature compression utilizing a Discretized Information Bottleneck (DIB). To support reproducible evaluation, we further introduce a data construction pipeline that generates temporally aligned event-skeleton pairs from existing RGB-skeleton datasets. Extensive experiments demonstrate that our approach achieves state-of-the-art accuracy among SNNs while significantly reducing energy consumption, providing a practical and scalable solution for neuromorphic multimodal action recognition.", "tldr": "", "keywords": ["Multimodal", "Action Recognition", "Spiking Neural Networks (SNNs)"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0be6c02d1b0640befac702b9a99a31091e530a90.pdf", "supplementary_material": "/attachment/2edd5548d6ee26ec36ebc4fa7f51d4f2c3253222.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an SNN-based framework for multimodal human action recognition by fusing event camera data and skeleton sequences. The approach addresses the high computational cost and energy inefficiency of traditional RGB-skeleton fusion methods by leveraging the sparse, asynchronous nature of both event and skeleton data within a unified spiking architecture. Key components include SGN (Spiking Graph Network), SSE (Sparse Semantic Extractor), SCM (Spiking Cross Mamba) DIB (Discretized Information Bottleneck). The authors construct event-skeleton datasets from existing RGB-skeleton benchmarks using V2E transformation. Experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA show competitive accuracy among SNNs with reduced energy consumption."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. First work to explore SNN-based multimodal fusion for action recognition, combining event and skeleton modalities\n\n2. Comprehensive and competitive results. Most of the experiments achieve higher performance than previous works with iso-parameter architecture. Furthermore, authors implement extensive ablation studies and analysis.\n\n3. Appendix A rigorously analyzes why classical Gaussian IB fails for SNNs and justifies the DIB formulation with discrete KL divergence and cosine surrogates."}, "weaknesses": {"value": "1. Novelty\n- While the authors claim this as the first SNN-based multimodal action recognition framework, the novelty is questionable. Except for the DIB module, all components are directly adopted from prior works with minimal modification (Spiking Mamba, SGN, etc.). The contribution essentially reduces to replacing activation functions with spiking neurons and introducing DIB. This appears more like an ad-hoc engineering integration rather than a fundamental methodological advance. \n\n2. Pseudo-event data\nThe entire evaluation relies on V2E-synthesized events from RGB, not real DVS camera data. This is acknowledged in Appendix J, but undermines claims about \"event camera\" advantages like high dynamic range and low latency. \n\n2. Paper presentation\n- Even though every module is well-explained in text, the figures are very hard to see due to small text. \n- I feel that the Figure 1 is redundant. The figure 1 does not convey any core technical contribution.\n\n3. Energy calculation\n- Several input tensors to Linear Projection (LP) layers are not binarized due to residual connections. However, based on equation (32), the authors used $E_{MAC}$ only for first layer. I strongly believe that the energy calculation should be corrected based on the architecture."}, "questions": {"value": "1. Could you elaborate on the core innovations or novelties of this work beyond the DIB module? I am happy to discuss about this.\n\n2. In the Spiking Cross Mamba (Figure 2e, Table 4), why do event features feed into the State Space Model (SSM) path while skeleton features go to the Selective Path (gate)? Is there a theoretical or empirical justification for this asymmetric design?\n\n3. For better understanding of the model's efficiency and to validate energy calculations, could you provide firing rates for each module\n\n4. Could the authors discuss what specific characteristics of real DVS data are not captured by V2E and how they might affect your model? Also, could you clarify whether the current framework would remain valid for real DVS inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oFYzjnTlHF", "forum": "Lo7YJOK6cv", "replyto": "Lo7YJOK6cv", "signatures": ["ICLR.cc/2026/Conference/Submission6789/Reviewer_USqB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6789/Reviewer_USqB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855813039, "cdate": 1761855813039, "tmdate": 1762919062921, "mdate": 1762919062921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel Spiking Neural Network (SNN) framework for multimodal human action recognition, fusing event camera data and skeleton sequences. Key contributions include:\n--The first SNN-based multimodal fusion architecture for event-skeleton data.\n--Introduction of Spiking Cross Mamba (SCM) for cross-modal interaction and a Discretized Information Bottleneck (DIB) for task-relevant feature compression under spiking constraints.\n--A pipeline to construct aligned event-skeleton datasets from existing RGB-skeleton benchmarks.\n--State-of-the-art accuracy among SNN methods with significantly lower energy consumption (1.73 mJ)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "--Quality: Strong empirical results, thorough ablation, and theoretical grounding.\n--Significance: Demonstrates a practical pathway for low-power multimodal recognition on edge devices.\n--Clarity: The overall pipeline and experimental section are well-structured and described."}, "weaknesses": {"value": "Motivation: The introduction does not convincingly establish a strong \"why now\" or \"why this way\" for the proposed method. The limitations of prior ANN and SNN works are stated but not used to build a powerful narrative for the current approach.\n\nOriginality: The architectural innovations (SCM, DIB) feel more like competent engineering integrations of existing ideas (cross-attention, Mamba, IB) into the SNN domain, rather than a fundamental conceptual breakthrough.\n\nPresentation: Inconsistent reference formatting and occasionally dense technical passages reduce readability."}, "questions": {"value": "Could you better motivate the specific choice of Spiking Mamba and the SCM fusion mechanism? What specific limitations of prior SNN or ANN fusion methods do they address that simpler baselines cannot?\n\nThe DIB is a key contribution. Beyond the theoretical derivation, can you provide more intuition or analysis on how it selectively compresses features and improves performance?\n\nCould you compare your method with more ANN-based multimodal models under similar parameter budgets to better contextualize the performance-efficiency trade-off?\n\nHow would the performance change if real event camera data were used instead of V2E-simulated events?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OBxndBZDEq", "forum": "Lo7YJOK6cv", "replyto": "Lo7YJOK6cv", "signatures": ["ICLR.cc/2026/Conference/Submission6789/Reviewer_9E4n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6789/Reviewer_9E4n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878760289, "cdate": 1761878760289, "tmdate": 1762919061993, "mdate": 1762919061993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a SNN‑based framework for multimodal human action recognition that fuses event and skeleton streams end‑to‑end in spikes. The architecture comprises: (i) spiking encoders for skeleton (SGN) and events (Spiking‑Mamba) from prior works, (ii) a Sparse Semantic Extractor (SSE) with hypergraph generators and Global Spiking Attention (GSA), (iii) Spiking Cross Mamba (SCM) for cross‑modal interaction, and (iv) a two‑stage Discretized Information Bottleneck (DIB) that performs spike‑compatible fusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Event/skeleton are both sparse temporal modalities; an SNN‑native fusion is a coherent direction.\n2. Module‑wise gains are cleanly reported, and the DIB variants are systematically explored."}, "weaknesses": {"value": "1. The highest Xs achieved by your model on NRD/NRD-120 is 85.0/74.6, which is substantially lower than the best-performing ANNs, such as VPN at 93.5/86.3, and MMNet at 94.2/92.9. \n2. ANN models operating on the same magnitude of computational cost also perform better, eg., CTR-GCN at 89.9/84.9 with 1.97 G FLOPs, and Shift-GCN at 87.8/80.9 with 2.5 G FLOPs. The efficiency gain claim is week."}, "questions": {"value": "What is the compute profile? Please report GPU type&number, GPU hours, and peak memory to train your model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BShIhZg5LK", "forum": "Lo7YJOK6cv", "replyto": "Lo7YJOK6cv", "signatures": ["ICLR.cc/2026/Conference/Submission6789/Reviewer_Mimh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6789/Reviewer_Mimh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967620198, "cdate": 1761967620198, "tmdate": 1762919061231, "mdate": 1762919061231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the first spiking neural network–based framework for multimodal human action recognition, combining event camera and skeleton data for energy-efficient, real-time recognition on edge devices. The proposed system, termed SNN-driven multimodal fusion, integrates several novel components: a Spiking Graph Network (SGN) for skeleton encoding, Spiking Mamba for event encoding, a Sparse Semantic Extractor (SSE) for structured attention, Spiking Cross Mamba (SCM) for cross-modal fusion, and a Discretized Information Bottleneck (DIB) for task-relevant feature compression under spiking constraints. The model achieves strong performance across NTU RGB+D and NW-UCLA benchmarks, outperforming prior SNNs in accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces the first multimodal SNN framework for human action recognition, representing a novel direction in neuromorphic computing. The use of event and skeleton modalities is well-motivated, making them well-suited for low-power, energy-efficient computation on edge devices.\n\n2. The paper is technically thorough and clearly presented. \n\n3. Achieves state-of-the-art SNN accuracy with drastically reduced energy consumption compared to ANN baseline."}, "weaknesses": {"value": "1. My main concern lies in the degree of technical novelty. Each component (Mamba, SNNs, and the Information Bottleneck) appears to be based on existing techniques, and the overall contribution could be viewed as a careful integration rather than a fundamentally new design. Could the authors clarify what specific aspects of the proposed framework go beyond a modular combination of known components?\n\n2.  Although the paper reports improved fusion accuracy and energy efficiency, it provides limited qualitative or interpretive analysis (e.g., failure cases, feature attribution, or modality interaction visualization) to illustrate how the model effectively leverages the complementary cues of event and skeleton data. Including such analyses would significantly enhance interpretability and reader confidence in the proposed fusion mechanism.\n\n3. The experiments rely on synthetic event-skeleton pairs converted from RGB videos rather than real event-camera datasets. This limits the validity of the claimed neuromorphic efficiency. Results on a genuine event-based dataset would substantially strengthen the empirical contribution.\n\n4. Minor comment – Please consider enlarging Figures 2 and 4 for better readability and visual clarity."}, "questions": {"value": "Please address points in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4wqkdIbeBP", "forum": "Lo7YJOK6cv", "replyto": "Lo7YJOK6cv", "signatures": ["ICLR.cc/2026/Conference/Submission6789/Reviewer_AETU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6789/Reviewer_AETU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980905005, "cdate": 1761980905005, "tmdate": 1762919060849, "mdate": 1762919060849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}