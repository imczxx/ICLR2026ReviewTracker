{"id": "5ft8vd9rwc", "number": 12029, "cdate": 1758205288211, "mdate": 1759897538380, "content": {"title": "LightCtrl: Training-free Controllable Video Relighting", "abstract": "Recent diffusion models have achieved remarkable success in image relighting, and this success has quickly been reproduced in video relighting. Although these methods can relight videos under various conditions, their ability to explicitly control the illumination in the relighted video remains limited. Therefore, we present LightCtrl, the first controllable video relighting method that offers explicit control over the video illumination through a user-supplied light trajectory in a training-free manner. This is essentially achieved by leveraging a hybrid approach that combines pre-trained diffusion models: a pre-trained image relighting diffusion model is used to relight each frame individually, followed by a video diffusion prior that enhances the temporal consistency of the relighted sequence. In particular, to enable explicit control over dynamically varying lighting in the relighted video, we introduce two key components. \nFirst, the Light Map Injection module samples light trajectory-specific noise and injects it into the latent representation of the source video, significantly enhancing illumination coherence with respect to the conditional light trajectory. \nSecond, the Geometry-Aware Relighting module dynamically combines RGB and normal map latents in the frequency domain to suppress the influence of the original lighting in the input video, thereby further improving the relighted video's adherence to the input light trajectory. \nOur experiments demonstrate that LightCtrl can generate high-quality video results with diverse illumination changes closely following the light trajectory condition, indicating improved controllability over baseline methods. \nThe code will be released to facilitate future studies.", "tldr": "LightCtrl relight a given video via a user-specific light trajectory in a zero-shot manner.", "keywords": ["video relighting; controllable video editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bab79b01f1a1eb5c776540fcf75af78ac7b52087.pdf", "supplementary_material": "/attachment/9620cf406218d8104168386c0036f3d45fbf2f7a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free method for light trajectory editing in videos. The approach extends the single-image relighting model, IC-Light, to the video domain by incorporating priors from video diffusion models to ensure temporal consistency. The core technical contributions include a light map injection module, which introduces trajectory-aware noise into the VDM's latent space, and a geometry-aware relighting module. This second module processes both RGB frames and corresponding normal maps estimated via StableNormal to guide the relighting process. The results are visually compelling and demonstrate good adherence to the specified lighting trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a relatively unexplored task of lighting control in video diffusion models rather than just global style. The direction, ideas, and results are promising and the problem is quite interesting.\n2. By building on existing, well-known T2I and T2V models, the method is accessible and its components are easier to understand and potentially replicate.\n3. The strategy of injecting trajectory-aware noise into the VDM's initial latent space appears effective for guiding the lighting. This concept may be adaptable to other conditional video generation tasks.\n4. The design of the geometry-aware relighting module, which dynamically blends RGB and normal map information, is technically sound. It correctly reflects that surface geometry should remain invariant during a relighting task."}, "weaknesses": {"value": "1. The use of PSNR_y against a pure white reference is not convincing. Relighting is a complex task involving light, geometry, and material, and does not necessarily mean \"the brighter the better\". This metric mainly captures pixel-wise brightness in 2D space and fails to account for the geometric and directional correctness of the illumination. It cannot distinguish excessive lighting and ignores geometry and material reflectance.\n2. The evaluation is somewhat limited. The dataset contains only 50 videos. This small scale and likely limited diversity are insufficient to robustly validate the method's generalizability. The paper would significantly benefit from showcasing more results across a wider variety of video content and lighting conditions.\n\nMinor issues:\n\nThere are plenty of papers about generative portrait relighting. Though they deal with portrait videos, I think they are still related to this topic. It is better to cite and discuss these papers. Following are some examples:\n\n+ Lumos: Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation\n\n+ Neural Video Portrait Relighting in Real-time via Consistency Modeling\n\n+ Real-time 3D-aware Portrait Video Relighting"}, "questions": {"value": "1. In Eq. (3), what exact $\\omega$ values are used across videos and trajectories? Is it fixed or tuned per video?\n2. Could you elaborate on the temporal stability of the StableNormal predictions? Are the normal maps computed independently per frame and fed directly into the video VAE, or is some form of temporal smoothing or other consistency enforcement applied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tz9qphozCa", "forum": "5ft8vd9rwc", "replyto": "5ft8vd9rwc", "signatures": ["ICLR.cc/2026/Conference/Submission12029/Reviewer_fsKh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12029/Reviewer_fsKh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914909125, "cdate": 1761914909125, "tmdate": 1762923010359, "mdate": 1762923010359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LightCtrl, a training-free controllable video relighting framework that enables explicit, fine-grained control of video illumination via a user-specified light trajectory. The method combines two pre-trained diffusion models an image relighting model (IC-Light) and a video diffusion model (VDM, e.g., AnimateDiff) to achieve temporally coherent and controllable lighting effects without retraining.\n\nThe method achieves strong quantitative and qualitative improvements over baselines such as IC-Light, SDEdit, and Light-A-Video, showing superior controllability, temporal coherence, and visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: The proposed combination of LMI and GAR modules represents a creative hybridization of diffusion-based image and video models, enabling spatio-temporal control without additional data or model fine-tuning. \n\n2. Technical Quality. The methodology is clearly formulated. The Light Map Injection method is grounded in the diffusion process and introduces a principled way to guide illumination through noise manipulation. The Geometry-Aware Relighting component is well-justified. Leveraging normal maps via frequency-domain fusion to mitigate original light leakage is both technically sound and physically motivated.\nExperiments are comprehensive and clear.\n\n3. Clarity. The paper is well-written and visually rich, with clear figures illustrating the pipeline and ablation effects.\n\n4. Significance. This paper advances controllable generation from static to dynamic illumination domains, bridging image and video diffusion paradigms."}, "weaknesses": {"value": "1. Physical Realism and 3D Awareness. The paper acknowledges limited 3D understanding of illumination. The method cannot simulate light scattering, occlusion, or volumetric effects, which restricts realism when the light trajectory crosses 3D geometry.\n\n2. Geometry-Aware Relighting (GAR) module. Although the GAR module suppresses source illumination, it fuses normal and RGB latents via a frequency-domain filter with dynamically decreasing cutoff. But there is no quantitative evidence that this schedule balances structure preservation and lighting suppression optimally.\n\n3. Limited Analysis of Control Robustness. The method only test on linear, circular and top–bottom light trajectory. More trajectorys should be tested.\n\n\n."}, "questions": {"value": "1. GAR module. Empirically, the low-frequency fusion could still introduce temporal blurring or artifacts around illumination edges. A systematic perceptual analysis or patch-level FVD comparison would strengthen confidence in this design\n\n2. Light trajactory. Can this method be applied to : 1. Abrupt trajectory changes 2.Multiple moving light sources 3. Unaligned control inputs? Such tests would reveal whether LightCtrl’s latent-space conditioning is robust to domain shifts or natural illumination variance.\n\n3. Transferability across video diffusion backbones: Have you tested the method on alternative video diffusion priors (e.g., CogVideoX, VideoCrafter2)? Does controllability degrade or improve when switching to models with different motion representations or schedulers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qf8pxnJYi9", "forum": "5ft8vd9rwc", "replyto": "5ft8vd9rwc", "signatures": ["ICLR.cc/2026/Conference/Submission12029/Reviewer_v3AC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12029/Reviewer_v3AC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922348160, "cdate": 1761922348160, "tmdate": 1762923010064, "mdate": 1762923010064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for video relighting that is training-free and allows users to provide a light trajectory as lighting conditions for the video. The authors make changes to the existing video diffusion framework as well as the image relighting framework (i.e., IC-Light) to allow them to use the user-provided light trajectory. The quality of the video relighting is evaluated with comparison to IC-Light and Light-A-Video. A user study is also conducted with 40 volunteers on video smoothness, lighting controllability, lighting quality, and alignment between lighting and text aspects of the relighting videos."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of designing a training-free method to control the lighting of a video generation model is interesting.\n\n- It is known that the common metrics for relighting might not faithfully reflect the visual quality of the relighted results. The authors design and conduct a user study with 40 volunteers, making the evaluation more rigorous."}, "weaknesses": {"value": "- It is unclear how significant it is to have a user-provided light trajectory for relighting. The authors explain their motivation for using manually labeled light trajectories in lines 069-072, but there is no supporting evidence to indicate that this is a desired or much-needed feature for users. \n\n- The user study is very important in evaluating the performance of the relighting methods. But many important details are not there in the current draft: how are those volunteers selected? Do they understand the relighting task? How are they trained on the provided metrics (e.g., lighting controllability)? How reliable are their answers? Is the result statistically significant? \n\n- The proposed method is designed to handle light trajectories in the relighting video task and is evaluated with the same type of light trajectory data (lines 315-318). This testing seems to be limited. There are many other ways to control lighting in relighting methods (e.g., environment maps or added light sources at certain locations in the image). It is currently unclear how this method compares to those more general cases of lighting conditions. \n\n- The writing of the paper could be improved. There are typos and formatting issues in the paper. In particular, in line 208 it says the initial noisy latent is $\\hat{z}_m$, but line 243 says the initial noisy latent is $z_m$.\n\n- Finally, being training-free is of course attractive, but the current design mostly uses existing video diffusion model and IC-light to do the heavy-lifting while the new parts are preparing the trajectory to the video diffusion model. The difference between the variants of LightCtrl (Figure 5, last three rows) is not that substantial. This makes it unclear about the significance of the proposed method."}, "questions": {"value": "1. What is the supporting evidence to indicate that having a user-provided light trajectory is a desired feature for relighting tasks? \n\n2. Can the authors provide more details about the user study? Please see the weaknesses above for questions. \n\n3. Can the authors discuss more about the significance of the approach, given the ablation study? If the authors can provide more rigorous ablation with more insight, it will be better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "S4ukxi7GCc", "forum": "5ft8vd9rwc", "replyto": "5ft8vd9rwc", "signatures": ["ICLR.cc/2026/Conference/Submission12029/Reviewer_shr2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12029/Reviewer_shr2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940890402, "cdate": 1761940890402, "tmdate": 1762923009694, "mdate": 1762923009694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the controllable video relighting task in a training-free manner. For this, LightCtrl builds on a pre-trained single-image relighting model, i.e., IC-Light, and tailors it to video relighting with several designs. First, it proposes to conduct frame-wise relighting on the video diffusion model's paired VAE decoder. Besides, it proposes to progressively fuse the source video and relit video during the diffusion process to maintain temporal coherence. Further, a user-defined light trajectory injection module is applied. Finally, they use off-the-shelf normal estimation to provide the video's normal maps to enable a geometry-aware relighting module. Experiments demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- originality-wise: the idea of using a single-image relighting model to enable video relighting is interesting.\n- quality-wise: the qualitative and quantitative results are promising.\n- clarity-wise: the presentation is good.\n- significance-wise: the video relighting is important for a lot of downstream tasks, e.g., content creation."}, "weaknesses": {"value": "I feel the ablations in the paper are quite inadequate. Even though there are some ablations in the appendix that are good, they are not the core part of the model design.\n\nFor example, can authors provide both **quantitative and qualitative** results to show the gradual improvement from the pre-trained IC-Light? From my understanding, there are several enhancements, but I have no clue which one contributes. Feel free to add things that I miss.\n- decoded frames + residual instead of raw frames (L216)\n- progressive fusion in Eq. (2)\n- geometry-aware feature in latent space\n- geometry-aware in frequency space.\n\nI am actually confused why not directly use the raw frames in the source video? Since the authors add back the difference between the raw videos and the decoded videos (L216), isn't this the same as just using the original video?"}, "questions": {"value": "See \"weakness\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DWM6rgF1OM", "forum": "5ft8vd9rwc", "replyto": "5ft8vd9rwc", "signatures": ["ICLR.cc/2026/Conference/Submission12029/Reviewer_bAHR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12029/Reviewer_bAHR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957723461, "cdate": 1761957723461, "tmdate": 1762923009172, "mdate": 1762923009172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}