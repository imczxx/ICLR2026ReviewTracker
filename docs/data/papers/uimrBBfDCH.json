{"id": "uimrBBfDCH", "number": 10156, "cdate": 1758162362929, "mdate": 1759897670624, "content": {"title": "FlowGen: Synthesizing Diverse Flowcharts to Enhance and Benchmark MLLM Reasoning", "abstract": "Flowcharts are widely used to represent processes and relationships through intuitive visual representations. However, accurately interpreting these diagrams remains challenging due to their structural complexity and high visual diversity. Existing flowchart datasets often lack fine-grained control over key properties such as graph complexity and rendering style, limiting their utility for training and testing of multimodal large language models (MLLMs) on visual reasoning tasks. To address these limitations, we introduce FlowGen, a controllable synthesizer that generates flowcharts that have customizable structural features and supports multiple renderer backends. FlowGen enables fine-grained control over graph properties such as graph order and size, branched arrows, and nested subgraphs, facilitating systematic evaluation of MLLMs’ capabilities. Extensive experiments on open-source and proprietary MLLMs show that training on FlowGen substantially improves flowchart parsing and question answering (QA), while also enhancing generalization to other public datasets. Furthermore, FlowGen provides challenging test datasets that expose consistent weaknesses in current MLLMs, particularly related to high structural complexity and varied rendering styles. Our code and data are publicly available at https://anonymous.4open.science/r/FlowGen-.", "tldr": "", "keywords": ["flowchart parsing", "question answering", "data synthesis"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f30d187fae4d9f997d14e6a08f7456fcb66c98c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FlowGen, a controllable synthesizer designed to generate diverse and structured flowcharts for training and evaluating multimodal large language models (MLLMs). The authors conduct comprehensive experiments showing that fine-tuning on FlowGen significantly improves flowchart parsing and question answering, while also exposing weaknesses in current MLLMs under complex or cross-renderer conditions. However, some aspects of the dataset design and evaluation pipeline need further clarification and empirical support."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important gap in multimodal reasoning benchmarks by introducing a controllable and scalable flowchart synthesis framework, which allows explicit control over graph complexity and rendering diversity.\n\nThe proposed FlowGen synthesizer is technically sound and provides a reproducible pipeline for generating both training and testing data, with adjustable parameters such as graph order, branching patterns, and rendering style.\n\nThe experiments are comprehensive, covering multiple flowchart-related tasks and a variety of open-source and proprietary MLLMs, which strengthens the empirical validity of the results."}, "weaknesses": {"value": "Weaknesses and Questions\n\nThe semantic content of the synthesized flowcharts is said to be drawn from 120 predefined application domains, each providing 40 node names and 40 edge labels. However, the paper does not explain the distribution and diversity of these domains, nor does it provide examples of the node and edge vocabularies. A more detailed breakdown would help readers assess how representative and balanced these semantic domains are. Additionally, the process of generating content “with GPT-4o and refined through human verification” is mentioned but not described in sufficient detail—clarifying this step would strengthen the reproducibility and credibility of the dataset.\n\nWhile the experiments focus on several flowchart-specific tasks, flowchart understanding is arguably a fundamental multimodal reasoning skill. It would be valuable to evaluate whether models fine-tuned on FlowGen also exhibit transfer improvements on broader benchmarks such as MathVista or MMMU, to demonstrate that FlowGen enhances general reasoning rather than only task-specific performance.\n\nIn Section 3.2, the authors mention synthesizing a training set of 11,520 flowcharts but do not explain how the task components of each sample are structured. Are all samples associated with parsing tasks, or do they also include QA tasks or other forms of supervision? Beyond Table 2, more detailed statistics, such as the task type distribution and average tokens per sample, would help readers better understand the dataset’s design and scope.\n\nThe evaluation setup involves prompting models with triplets extracted by Qwen2.5-VL-7B fine-tuned on FlowGen data. This design is not fully end-to-end, and may introduce error propagation from the extraction stage. A cleaner approach might be to evaluate each model’s direct performance on FlowGen-derived tasks (e.g., QA) using its own fine-tuned checkpoint, without intermediate feature extraction.\n\nThe paper also introduces a synthetic test set generated using the same pipeline. However, it is unclear whether the test set underwent any manual inspection or quality assurance. Since the same generation process is used for both train and test data, some discussion of data overlap prevention and human verification would be helpful to ensure a fair and meaningful evaluation.\n\nTable 6 shows that models trained on specific subsets (e.g., Graph Easy, Medium, Hard) achieve their best results on the corresponding test subsets. This suggests possible limited cross-subset generalization. Have the authors tested a model trained on the entire training set (combining all difficulty levels) to examine whether it performs more robustly across test subsets? Such results would provide a clearer picture of the model’s generalization ability."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPuREXbJHJ", "forum": "uimrBBfDCH", "replyto": "uimrBBfDCH", "signatures": ["ICLR.cc/2026/Conference/Submission10156/Reviewer_TxLZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10156/Reviewer_TxLZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514838331, "cdate": 1761514838331, "tmdate": 1762921525968, "mdate": 1762921525968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FlowGen, a controllable flowchart synthesizer for training and benchmarking multimodal LLMs. The generator exposes seven knobs (graph order, split/merge arrows, branching factor, density, unlabeled-edge ratio, nested subgraphs) and renders with Mermaid, Graphviz, PlantUML, and Diagrams to induce style diversity (Fig. 2, p.3). Using FlowGen, the authors build a 11,520‑image training set (Table 2, p.6) and a six‑subset test suite spanning structural difficulty and scanned‑image degradation. Across six parsing benchmarks, fine‑tuning open‑source MLLMs on FlowGen yields large F1 gains (e.g., Qwen2.5‑VL‑7B: FlowLearn 43.2→60.1 strict F1; hdBPMN 16.6→18.3; Table 3, p.6), and supplying triplets extracted by a FlowGen‑tuned model improves flowchart QA accuracy for multiple models (Table 4, p.7). A new FlowGen test set exposes low base performance for both open‑source and proprietary models (e.g., GPT‑4o ≤25% F1 on most subsets) and shows that FlowGen‑SFT substantially helps (Table 6, p.9). Ablations indicate multi‑renderer training and explicit modeling of nesting/split‑merge structures matter (Table 5, p.7)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Combines controllable graph parameters with multi‑renderer outputs; introduces scanned‑style perturbations and a matched train/test pipeline\n- Extensive, multi‑model evaluation; clear ablations demonstrating the role of renderer diversity, nesting, and split/merge \n- Well‑specified metrics and prompts (Appendix D) and transparent training setup.\n- Offers a scalable path to stress‑test MLLM reasoning over structured diagrams; highlights consistent weaknesses"}, "weaknesses": {"value": "1. Evaluation Design Masks True Model Capabilities\n\nTable 4's approach—feeding triplets extracted by a FlowGen-tuned Qwen-7B to all models—introduces two fundamental biases. First, it conflates extraction ability with reasoning capacity, making it impossible to isolate genuine question-answering performance. Second, it privileges models that happen to align with the chosen extractor's error profile. The paper should instead report: (i) QA performance without triplets as a baseline, (ii) QA with gold-standard triplets to measure pure reasoning, and (iii) QA using each model's self-extracted triplets to properly separate parsing from comprehension.\n\n2. Virtual Node Encoding Introduces Unvalidated Structural Bias\n\nThe split/merge virtual-node representation deviates from standard BPMN conventions (gateways, arrow semantics) in ways that could fundamentally bias what models learn. While the authors acknowledge this as future work (Conclusion, p.9), no quantitative analysis compares this encoding against alternatives. Without understanding how much this choice shapes results, we cannot assess whether findings generalize to real-world diagram understanding.\n\n3. Underwhelming Performance on Real Data Raises Generalization Questions\n\nThe best FlowGen-SFT result on hdBPMN remains modest—just 18.3% F1 for Qwen-7B (Table 3). This gap between synthetic training performance and real-world evaluation suggests the approach may not transfer beyond stylized, template-generated data. The paper needs substantially larger, truly out-of-distribution real-world test sets to validate practical applicability.\n\n4. Train-Test Splits Are Distribution-Matched, Not Truly Adversarial\n\nFlowGen's test sets are disjoint from training but deliberately distribution-matched (Sec. 4.1.1, p.8). This design choice likely inflates apparent robustness compared to unconstrained deployment scenarios. Cross-renderer evaluations (where training and test use different rendering engines) and unseen-topic tests would provide more realistic difficulty estimates.\n\n5. \"Scanned\" Augmentations Are Synthetic, Not Ecological\n\nTable 10's scanned-style effects (p.20) come from Pillow/OpenCV filters, not actual scanning or photography. Real documents exhibit sensor noise, lighting variation, physical degradation, and artifacts that synthetic perturbations cannot fully capture. Evaluation on genuinely scanned, photographed, and hand-drawn diagrams is essential for validating real-world robustness.\n\n6. Vocabulary Verification Lacks Transparency\n\nNode/edge vocabularies are generated by GPT-4o then \"human-verified\" (Sec. 2.2.2, p.4), but verification coverage and inter-annotator reliability are not reported. Without knowing what percentage was checked, by how many annotators, and with what agreement rates, we cannot assess label quality or rule out systematic template artifacts.\n\n7. \"Unlimited Diversity\" Claim Is Unsupported\n\nThe assertion that FlowGen generates unlimited diverse flowcharts contradicts the method's reliance on 120 static domain dictionaries with fixed vocabularies. Examination reveals repetitive structures, especially in \"complex\" configurations. The system samples from finite pools—it does not model true domain variation or generate genuinely novel structures.\n\n8. Random Semantic Assignment Undermines Reasoning Validity\n\nLabels are randomly sampled from GPT-generated word lists without semantic validation or causal coherence. Many examples show nonsensical flows (e.g., \"Health Coaching → Detox Program → Journaling Wellness\"). This fundamentally undermines the benchmark's value as a reasoning test: models learn to parse arbitrary structures, not understand real-world logic or causal relationships.\n\n9. Methodologically Inappropriate Baseline Comparisons\n\nComparing fine-tuned open-source MLLMs to proprietary reasoning-focused models like Gemini-2.5 is not methodologically sound. Gemini is architecturally designed for multi-step reasoning chains, not direct diagram parsing, making performance comparisons misleading. The evaluations conflate architectural differences with actual diagram understanding capability.\n\n10. Missing Critical Semantic Ablation\n\nThe central claim—that FlowGen improves visual reasoning—depends on semantically labeled nodes and edges. Yet the paper never evaluates performance when semantic labels are removed or randomized. This omission is critical: observed improvements could stem entirely from textual cues in the image rather than genuine spatial or structural reasoning. Without this ablation, we cannot determine what models actually learn.\n\n11. Limited Conceptual Contribution\n\nBeyond reporting performance numbers, the paper provides little insight into why MLLMs struggle with flowcharts or how they might fundamentally improve. The work essentially demonstrates that synthetic fine-tuning increases scores on similar synthetic data—an expected result that doesn't advance our understanding of diagram comprehension or suggest paths toward more robust visual reasoning."}, "questions": {"value": "- Can you report QA with (a) gold triplets, (b) no triplets, and (c) per‑model self‑extracted triplets? How sensitive are Table 4 gains to extractor accuracy?\n- Have you tried ontology mapping, lemmatization, or embedding‑based matching to reduce synonym penalties in triplet evaluation? How do rankings change? \n- What is performance when training on one renderer and testing on another (full cross‑matrix), beyond the single‑renderer ablation in Table 5?\n- Can you quantify the gap between virtual‑node split/merge and a gateway/edge‑semantics encoding (e.g., BPMN‑style gateways) on hdBPMN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cRcL1WfEnF", "forum": "uimrBBfDCH", "replyto": "uimrBBfDCH", "signatures": ["ICLR.cc/2026/Conference/Submission10156/Reviewer_3UeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10156/Reviewer_3UeF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535188295, "cdate": 1761535188295, "tmdate": 1762921525589, "mdate": 1762921525589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FlowGen, a controllable synthesizer that generates flowcharts with adjustable structural and visual complexity to evaluate and fine-tune multimodal large language models (MLLMs). The system defines parameters for graph order, branching, density, unlabeled edges, and nesting depth, and renders diagrams using multiple backends (Mermaid, Graphviz, PlantUML, Diagrams). The authors generate 11,520 flowcharts across 120 topic domains and three difficulty levels, fine-tune ten MLLMs, and evaluate on multiple flowchart parsing and QA datasets. Fine-tuning with FlowGen consistently improves performance, particularly on structurally complex diagrams. In addition, the authors use FlowGen to create challenging validation sets designed to probe model robustness along two controlled axes (graph complexity and scanned document degradation) which systematically expose consistent weaknesses in both open-source and proprietary models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- **Novel and practical contribution:** FlowGen fills a significant gap in controllable synthetic flowchart generation for multimodal reasoning.\n- **Controlled and systematic flowchart synthesis:** The modular three-stage architecture (configuration, graph construction, rendering) allows manipulation of structural and stylistic parameters, producing flowcharts with diverse properties.\n- **Comprehensive experimentation:** Evaluations span multiple models, datasets, and difficulty levels, with well-designed ablations and detailed error analyses.\n- **Robustness dimension:** The introduction of degraded and complex test sets is a valuable step toward stress-testing MLLMs."}, "weaknesses": {"value": "- **Limits of flexibility and variability:** While FlowGen enables controlled chart variability, the actual range of possible flowcharts likely depends on the set of semantic topics and predefined parameter ranges. A quantitative study of generated flowchart diversity within the same topic would help clarify the boundaries and potential limitations of the method. Moreover, the 120 topic domains used for across the experiments are insufficiently described; the source and diversity of these domains need more explanation.\n- **Unclear evaluation protocol:** The authors do not specify whether reported results on flowchart parsing and QA datasets are in zero-shot settings or if models were fine-tuned on the target evaluation datasets. This ambiguity makes it difficult to interpret the performance gains and assess true generalization.\n- **Dataset independence concerns:** In section 4, training and test sets are both generated by FlowGen, raising questions about evaluation fairness and true generalization. Clarifying how overlap and near-duplicates are prevented is essential."}, "questions": {"value": "**Questions for Authors**\n\n1. Are the reported results on flowchart parsing and QA datasets obtained in zero-shot mode, or were the models fine-tuned on these evaluation datasets? Please clarify to properly interpret generalization performance.\n2. When using FlowGen-generated training and test sets, how do the authors ensure no overlap or near-duplicates exist between them?\n3. What is the source and diversity of the 120 topic domains used for generation? Are they drawn from existing ontologies?\n4. Have the authors computed any quantitative metrics to measure variability within the same topic and across difficulty levels? This could help to assess the true flexibility of FlowGen and potential limitations in coverage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i6FTH3u7Qz", "forum": "uimrBBfDCH", "replyto": "uimrBBfDCH", "signatures": ["ICLR.cc/2026/Conference/Submission10156/Reviewer_9ofn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10156/Reviewer_9ofn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665013712, "cdate": 1761665013712, "tmdate": 1762921524672, "mdate": 1762921524672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a system called FlowGen, which is a controllable flowchart synthesizer that generates diverse flowcharts with adjustable structural complexity and visual styles. The system enables large-scale, customizable datasets for both training and benchmarking multimodal large language models (MLLMs) on flowchart understanding and visual reasoning tasks. FlowGen supports multiple renderers (Mermaid, Graphviz, PlantUML, Diagrams) and configurable parameters (e.g., graph order, branching factor, nested subgraphs). Experimental results show that models trained with FlowGen data (e.g., Qwen2) achieve significant improvements in flowchart parsing and flowchart QA, with strong generalization to public datasets such as FlowVQA and FlowLearn. Moreover, FlowGen serves as a challenging evaluation benchmark, revealing weaknesses in even top proprietary models (GPT-4o, Gemini-2.5-Flash)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is strong. The authors motivate that prior work (including FlowchartQA, FlowVQA, FlowLearn) does not have control over structural and stylistic complexity, whereas FlowGen specifically introduces a parametric generation framework to address this limitation. The generation of synthetic data resembles the data augmentation which show indeed improvement in performance. \n\n2. The paper presents a good experimental design with many MLLMs and two types of evaluation metrics (exact and relaxed metrics). The Easy/Medium/Hard complexity of the dataset seems interesting and allows for various analysis. \n\n3. The paper contains good ablation studies that systematically isolate contributions from multiple perspectives: multi-renderer training; nested subgraphs; and split/merge arrows\nshowing their distinct impacts on performance."}, "weaknesses": {"value": "1. Semantic labels (node and edge names) are generated with LLMs and verified by humans. I wonder if this could introduce stylistic or linguistic biases, especially if similar LLMs are later tested on FlowGen data. \n\n2. FlowGen by design enables controllable generation, but it would be interesting to see if synthetic diagrams may or may not fully capture the diversity and imperfections of real-world flowcharts (e.g., hand-drawn figures, inconsistent layouts, textual clutter). \n\n3. Since training and test distributions share the same generative pipeline, models might implicitly learn renderer-specific biases, even if content instances differ. Did the authors observed such issues?"}, "questions": {"value": "What could happen if the authors generate data with different LLMs (open-source vs. closed-source MLLMs)? What is the impact of a less performant MLLM? To what extent the noise hurts the overall performance?\n\nDoes a combination of MLLM generated and human generated data could improve performance and avoid potential biases that could be brought by the MLLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lb87ZXnKpT", "forum": "uimrBBfDCH", "replyto": "uimrBBfDCH", "signatures": ["ICLR.cc/2026/Conference/Submission10156/Reviewer_i9ZK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10156/Reviewer_i9ZK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120132572, "cdate": 1762120132572, "tmdate": 1762921524083, "mdate": 1762921524083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}