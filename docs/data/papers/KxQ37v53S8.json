{"id": "KxQ37v53S8", "number": 21884, "cdate": 1758323096775, "mdate": 1759896898523, "content": {"title": "PALADIN: Privacy-Aware Learning with Adversary-Detection and INference suppression.", "abstract": "Agents trained via \\gls{rl} and deployed in sensitive settings, such as finance, autonomous driving, or healthcare, risk leaking private information through their observable behaviour. Even without access to raw data or model parameters, a passive adversary may infer sensitive attributes (e.g., identity, location) by observing the agent’s trajectory. We formalise this \\emph{behavioural leakage} threat and propose \\textbf{PALADIN}, a proactive privacy-shaping framework that integrates an adversarial inference model into the training loop. PALADIN jointly trains a transformation network to perturb observations and a co-adaptive leakage predictor, whose output shapes the agent’s reward via a curriculum-guided penalty. This allows the agent to first learn stable task policies, then progressively adapt its behaviour to resist inference. We evaluate PALADIN on autonomous navigation and financial trading, auditing leakage against multiple adversary architectures (MLP, GRU, Transformer). PALADIN achieves up to 43\\% (return 27.0 vs. 18.9 baseline) higher task returns and 57\\% (0.056 vs 0.131) lower adversarial leakage compared to strong baselines. Even against Transformer adversaries, where leakage confidence remains high, PALADIN raises returns by 38\\% (22.8 vs.15.9) without amplifying leakage, whereas static noise and \\gls{dp} baselines (returns less than 7) fail to reduce leakage. These results highlight the value of embedding adversary-aware privacy shaping directly into \\gls{rl} training to mitigate deployment-stage inference threats.", "tldr": "This paper introduces PALADIN,a proactive,adversary-in-the-loop Reinforcement Learning framework that embeds a learnable leakage estimator and curriculum-guided reward shaping to jointly optimise task performance and behavioural privacy.", "keywords": ["Reinforcement Learning", "Privacy Preservation", "Adversarial Learning", "Curriculum Learning", "Behavioural Leakage"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb6682217f01c645e2b55eff08235635a3a8aa91.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the behavioral leakage problem in reinforcement learning (RL) agents during the deployment phase. Specifically, the trajectories generated by an agent's interactions with its environment can be passively observed by an adversary, allowing them to infer sensitive user attributes (such as identity, policy, and whether the agent is under attack). The authors propose the PALADIN framework, whose core idea is to introduce an adversarial inference model into the RL training loop, actively guiding the agent's policy through privacy-shaping."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis article clearly defines the problem of deployment-time behavior leakage and distinguishes it from classic DP and adversarial robustness problems.\n2.\tUsing an \"adversary-in-the-loop\" approach (co-training $g_w$) is more robust than defending against a fixed, known adversary. This dynamic game forces the agent to learn deeper levels of confusion strategies."}, "weaknesses": {"value": "1.\tLack of formal privacy guarantees is the biggest weakness of this paper. All the protections provided by PALADIN are empirical; it only promises to defend against the \"adversary adversary\" $g_w$ that was trained together. It does not provide any provable privacy guarantees (such as (ε, δ)-DP).\n2.\tFailure against strong adversaries significantly weakens the core contribution of the paper. The experimental results (Tables 1 and 5) reveals that PALADIN's privacy leakage confidence (\"leak.conf\") hardly decreases when facing MLP-Transformer or GRU-Transformer, remaining as high as more than 0.99.\n3.\tTheorem 1 proposed in Section 3.3 and Appendix B of the paper is referred to as the \"privacy-utility boundary,\" but it is more like a heuristic derivation of an optimization objective than a rigorous privacy guarantee. The theory relies on a very strong and unrealistic assumption: that the \"proxy adversary\" $g_w$ is an unbiased estimator of the loss of the \"real adversary\".\n4.\tThe financial trading environment has been greatly simplified in NYSE dataset. It is a deterministic replay of historical data, and the agent's dummy actions have no impact on the environment state. This is far removed from real, dynamic markets. The AV-GPS task is essentially a trajectory window classification task. The effectiveness of this method in more complex and interactive RL environments, such as real-time games and robot control, has not yet been proven."}, "questions": {"value": "1. As shown in Weakness 4, could the authors validate PALADIN's effectiveness in a more complex, truly interactive setting (e.g., a financial simulator with market impact and slippage, or a robotics/driving simulator like CARLA) where the agent's policy must learn to balance task-critical, state-altering actions against the adversarial privacy objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iBZsGKxUj8", "forum": "KxQ37v53S8", "replyto": "KxQ37v53S8", "signatures": ["ICLR.cc/2026/Conference/Submission21884/Reviewer_BdYm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21884/Reviewer_BdYm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762848289, "cdate": 1761762848289, "tmdate": 1762941968592, "mdate": 1762941968592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PALADIN, a framework for Reinforcement Learning (RL) that attempts to resolve the issue of behavioural leakage at deployment time. The authors formalise this threat, where a passive adversary infers sensitive attributes (like user identity or strategy) by observing an agent's trajectory, as distinct from training-time privacy (which DP-SGD addresses). The new approach is applied to autonomous navigation (AV-GPS dataset) and financial trading (NYSE dataset) . The algorithmic contribution consists of four components: (1) a Transformation Network to perturb observations , (2) a co-adaptive Surrogate Leakage Predictor (an adversary) to estimate the privacy risk from these perturbed observations , (3) a Reward Shaping mechanism that penalises the agent based on the adversary's success , and (4) a Curriculum-Guided Penalty that slowly increases this penalty, allowing the agent to first learn the task before enforcing privacy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is well written\n\nS2. Novel Problem Formulation. The paper does a good job of defining and motivating an important, emerging problem: deployment-time privacy for RL agents. It differentiates this from training-time privacy (like DP) and adversarial robustness."}, "weaknesses": {"value": "W1. Contradict results. 1) The ablation study (\"no.curriculum\") in Appendix Table 4 shows **catastrophic failure** for a fixed penalty while 2) The main results (\"Adv_No_Cur\") in Table 3 show **stable, moderate** performance for what is also a fixed-penalty baseline. Both 1) and 2) apply the Curriculum Penalty that claimed necessary for stability and somehow the results seems very contradict, which might significantly lower the paper claims.\n\nW2. Weak Theoretical Justification. The paper presents \"Theorem 1\" but immediately disclaims it as \"not a formal guarantee\" and a \"heuristic shaping signal\".The proof itself relies on an unverified assumption that the surrogate adversary is an \"unbiased estimator\", which might not hold true in practice."}, "questions": {"value": "Please refer to the Weaknesses Section.\n\nOverall, I'm interesting in the paper and the problem that it trying to solve. However, the result is somewhat inconsistent and the theoretical justification is not strong, both combine lower the paper's contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jaj03ZlAwu", "forum": "KxQ37v53S8", "replyto": "KxQ37v53S8", "signatures": ["ICLR.cc/2026/Conference/Submission21884/Reviewer_oEpi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21884/Reviewer_oEpi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842178914, "cdate": 1761842178914, "tmdate": 1762941968342, "mdate": 1762941968342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates privacy concerns associated with reinforcement learning agents, specifically the behavioral leakage that occurs through prediction trajectories during inference. To address this issue, the authors propose PALADIN, a defense framework that incorporates an adversarial inference model into the training loop to suppress privacy leakage. Experimental evaluations across different model architectures demonstrate that the proposed method can reduce privacy risks to a certain extent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and addresses an important privacy threat arising from agent prediction trajectories.\n2. The proposed method is evaluated across diverse model architectures, including MLP, GRU, and Transformer-based agents."}, "weaknesses": {"value": "1. The metrics leak_nll and leak_conf are insufficiently explained. A clearer definition and discussion of their interpretability and limitations are needed.\n2. Results in Table 1 indicate that the proposed method does not consistently reduce privacy leakage. Moreover, the two privacy metrics occasionally point to conflicting conclusions, which calls for additional analysis and justification.\n3. The method relies on reward shaping approximation as shown in Equation (2). This approximation introduces assumptions that may bias the learned behavior or lead to a suboptimal policy."}, "questions": {"value": "1. How is the value of $\\lambda_t$ determined in practice for each experiment? Although the paper provides a sensitivity study regarding $\\lambda$, the connection between this analysis and the choice of $\\lambda_t$ remains unclear. \n2. Equation (1) does not explicitly incorporate a utility term. However, in some cases, the proposed method achieves higher utility than baseline methods. What is the potential explanation or underlying mechanism for this observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XkoQHrPwuC", "forum": "KxQ37v53S8", "replyto": "KxQ37v53S8", "signatures": ["ICLR.cc/2026/Conference/Submission21884/Reviewer_tc6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21884/Reviewer_tc6M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975199544, "cdate": 1761975199544, "tmdate": 1762941968105, "mdate": 1762941968105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets privacy leakage in sequential decision-making: a passive observer infers sensitive attributes from an agent's trajectories. The proposed method PALADIN couples 1) a learned transformation network (f_\\phi) that perturbs observations, 2) a co-trained surrogate adversary (g_w) estimating leakage, and 3) reward shaping with a curriculum schedule that gradually increases privacy pressure after the agent first attains task competence. Concretely, PPO is trained on shaped reward with the adversary's leakage proxy (e.g., NLL). A privacy-utility bound is presented as intuition rather than a differential-privacy guarantee. Experiments on an autonomous-vehicle GPS trajectory task and a financial time-series setup report higher task return and reduced leakage than static noise and DP baselines, with ablations showing the curriculum is important for stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method addresses deployment-time leakage from trajectories, complementing previous training-time privacy and robustness works. The adversary-in-the-loop perspective is well-motivated, for sequential data.\n2. Practical synthesis of three modules: learned feature perturbation, in-loop adversary penalty, and curriculum scheduling. The staged (\\lambda_t) schedule is empirically impactful for stability. These are practical insights and could be applied for future robust methods.\n3. The experiments are comprehensive: multiple adversary architectures (MLP/GRU/Transformer), train/test cross-pairings, and ablations (no-curriculum, no-transform) support the claim that all components matter."}, "weaknesses": {"value": "My major concerns are on the experiments:\n1. As the paper explicitly states that the NYSE environment uses:\n> A \"dummy continuous action space\" where \"actions do not affect transitions\", \"Deterministic replay of the historical window\"; The state simply advances through fixed historical sequences\nThis is not actually a reinforcement learning setting- it's a supervised sequence filtering problem. The sequence simply replays fixed windows, making it a supervised filtering problem wrapped to look like RL. This misrepresentation undermines the paper's claims about solving RL privacy problems in finance, and undermines claims about balancing control and privacy in this domain. The experiments should be more carefully conducted and explained to reflect this.\n2. Weak theory to practice: The privacy-utility bound mixes task reward units and adversary loss, relies on loose concentration arguments, and provides little guidance for choosing (\\lambda_t). As admitted, it is not a DP guarantee; currently it is more motivational than actionable.\n3. Because the surrogate adversary defines the penalty, overfitting to it is a risk. While the paper includes mismatched architectures, a stronger test would train large, external, held-out adversaries post-hoc with broad hyper-sweeps or mutual-information probes.\n4. Minor concerns on tiny dataset sizes (e.g. 85 episodes for NYSE), and high variance in many results.\n5. AV-GPS uses a spoofed-vs-normal label as the \"sensitive attribute\". Concealing spoofing may be a safety diagnostic rather than a \"privacy \"variable. The paper should justify this choice and discuss governance to avoid hiding attacks from monitors or audits."}, "questions": {"value": "Please check the weaknesses section for details."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "One of the experiments, on AV-GPS, uses a spoofed-vs-normal label as the \"sensitive attribute\". Concealing spoofing may be a safety diagnostic rather than a \"privacy \"variable. The paper should justify this choice and discuss governance to avoid hiding attacks from monitors or audits."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6hDAYvQQiv", "forum": "KxQ37v53S8", "replyto": "KxQ37v53S8", "signatures": ["ICLR.cc/2026/Conference/Submission21884/Reviewer_GDZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21884/Reviewer_GDZN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124655053, "cdate": 1762124655053, "tmdate": 1762941967864, "mdate": 1762941967864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}