{"id": "jDpe3c8gys", "number": 5912, "cdate": 1757945836552, "mdate": 1763749715779, "content": {"title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios", "abstract": "Determining which data samples were used to train a model—known as Membership Inference Attack (MIA)—is a well-studied and important problem with implications for data privacy.  Black-box methods presume access only to the model’s outputs and often rely on training auxiliary reference models. While they have shown strong empirical performance, they rely on assumptions that rarely hold in real-world settings: (i) the attacker knows the training hyperparameters; (ii) all available non-training samples come from the same distribution as the training data; and (iii) the fraction of training data in the evaluation set is known. In this paper, we demonstrate that removing these assumptions leads to a significant drop in the performance of black-box attacks. We introduce ImpMIA, a Membership Inference Attack that exploits the Implicit Bias of neural networks, hence removes the need to rely on any reference models and their assumptions. ImpMIA is a white-box attack -- a setting which assumes access to model weights and is becoming increasingly realistic given that many models are publicly available (e.g., via Hugging Face). Building on maximum-margin implicit bias theory, ImpMIA uses the Karush–Kuhn–Tucker (KKT) optimality conditions to identify training samples. This is done by finding the samples whose gradients most strongly reconstruct the trained model’s parameters. As a result, ImpMIA achieves state-of-the-art performance compared to both black and white box attacks in realistic settings where only the model weights and a superset of the training data are available.", "tldr": "ImpMIA is a membership inference attack that exploits neural networks' implicit bias to identify training data from model weights, outperforming prior methods in realistic settings.", "keywords": ["Membership Inference Attacks", "Implicit Bias", "Privacy Auditing", "Information Leakage"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9734c50b87365ee591b396d4b3e3e5f0a1a69cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper demonstrates (with empirical evidence) that removing assumptions underlying the standard MIA threat model, such as (a) the use of common training hyperparameters between target and reference models, (b) a known fraction of training samples in the candidate pool used to run the attacks, and (c) using in-distribution samples as non-members, significantly impacts their performance. Removal of some/ all of these assumptions tends to adversely affect the attack performance of SOTA MIAs. To address these shortcomings of SOTA MIAs, they propose a white-box alternative, **ImpMIA**, which relies on the implicit bias of neural networks and does not require training auxiliary reference models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It is based on the theory put forth by prior works of Lyu and Li [1] and Ji and Telgarsky [2] and thus has a solid theoretical foundation. The author(s) demonstrate empirically that the theory, which pertains to homogenous ReLU networks, is generalizable to other architectures.\n- The experiments in the paper duly establish that ImpMIA underperforms against SOTA black-box MIAs in the standard MIA threat model. However, the author(s) provide evidence to support their argument that ImpMIA is the best performing MIA in the absence of the assumptions underlying the standard MIA threat model. They also provide empirical evidence to demonstrate that these assumptions contribute significantly to the SOTA performance of black-box attacks such as LiRA/RMIA.\n- The proposed approach is less computationally intensive (as detailed in lines 648-650) compared to SOTA black-box MIAs.\nImpMIA scales well to large candidate pools, as shown in Appendix B2."}, "weaknesses": {"value": "- Missing comparison with SOTA white-box attacks such as the Inverse Hessian Attack (IHA) proposed by Suri et al. [3]. Per Suri et al., IHA outperforms SIF, though it may be due to SIF being designed assuming a different experimental setup. The author(s) should have acknowledged it in the paper and, at the very least, clarified their reasoning for not including it among the SOTA white-box attacks for comparison.\n- References in lines 145-146 do not include Shi et al. [4]. \n- Pre-filtering step of the attack could cause the practitioner to discard data points which are hard to learn and likely to be memorised by the model if the training continues. These constitute privacy-vulnerable training samples, and the attack might in fact end up underestimating the privacy risk. Could the author(s) provide an estimate of the fraction of samples discarded in this step for different experiments?\n- The method is tested only on the ResNet18 architecture, so it is unclear whether the ImpMIA retains an advantage against smaller or higher dimension architectures. An attack that generalises well to model dimensionality will be a major plus point in favour of the paper, since otherwise, it requires relatively less compute compared to SOTA reference-model-based MIAs.\n- In the block division step of the attack (lines 617-624), the author(s) mention using only convolutional layers for gradient matrix construction for models trained with CIFAR-100. However, this layer selection appears arbitrary. Could the authors clarify their reasoning behind this choice (if there is any) beyond computational feasibility?\n- What's $\\eta$ in the mathematical expression in line 646?\n- Experiments for Appendix B2 lack comparison to SOTA MIAs.\n- In Appendix B3, it says in line 722, \"but our attack also works well under partial training coverage.\" However, the author(s) do not provide the statistics for other attacks used in the paper, to believe that compared to other attacks, ImpMIA is relatively robust to the training sample fraction in the candidate superset.\n\n[1] Lyu, K., and Li, J. “Gradient Descent Maximizes the Margin of Homogeneous Neural Networks.” ICLR 2020.\n\n[2] Ji, Z., and Telgarsky, M. “Directional Convergence and Alignment in Deep Learning.” NeurIPS 2020.\n\n[3] Suri, A. et al. “Do Parameters Reveal More than Loss for Membership Inference?” TMLR 2024.\n\n[4] Shi, Y. et al. “Assessing Membership Inference Attacks under Distribution Shifts.” IEEE BigData 2024."}, "questions": {"value": "**Questions**: I am amenable to updating my initial assessment provided that the authors are able to address the concerns highlighted in the weaknesses as listed above.\n\n**Suggestions**:  The presentation of the paper could be improved by using one reference dataset for the results shown in the Appendix. While for some tables (such as T1 and T3) the author(s) use CIFAR-10, for some other tables  (such as T2) they use CIFAR-100 with no justification provided for these choices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0SHaoEnaPc", "forum": "jDpe3c8gys", "replyto": "jDpe3c8gys", "signatures": ["ICLR.cc/2026/Conference/Submission5912/Reviewer_TX9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5912/Reviewer_TX9K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761458941665, "cdate": 1761458941665, "tmdate": 1762918344274, "mdate": 1762918344274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Comment for All Reviewers"}, "comment": {"value": "We thank all reviewers for the time and effort invested in our paper. The reviews were very constructive, and we believe we were able to adequately address all concerns raised with additional experiments and clarifications. All additional tables and figures were added at the end of the appendix (pages 23-24) for convenience and will later be incorporated, together with other textual revisions, into the final version of the paper.\n\n\nBelow, we address two points that were raised in multiple reviews. \n\n***1. Performance of our method on additional models***\n\nWe have now conducted new experiments in our setting, comparing our attack with SOTA baselines on VGG16 and ResNet50 models trained on CIFAR-10 (see table below).  ImpMIA shows a significant improvement over the other baselines on both architectures without any adaptation (we use the same hyperparameters for all models). These results, together with additional runs on other datasets, will be included in the final version of the paper.\n\n\n| Attack |           |      VGG16         |               |   ·   |         |        ResNet50       |               |\n|-|:--:|:-:|:-:|:-:|:-:|:-:|:-:|\n|        | AUC             | @ 0.01%  | @ 0.0%   |   ·   | AUC              | @ 0.01%   | @ 0.0%   |\n| LiRA   | 0.75            | 0.62         | 0.18          |   ·   | 0.79             | 1.02         | 0.26          |\n| RMIA   | **0.78**            | 0.69         | 0.06          |   ·   | 0.78             | 0.09         | 0.02          |\n| Ours   | **0.78**            | **1.59**         | **0.56**          |   ·   | **0.82**             | **1.80**         | **0.78**          |\n\n\n\n***2. Attack dependence on training samples in the superset***\n\nWe have now added a direct comparison with the previous best methods under only 10% coverage in our realistic scenario (see table below). Even with this low coverage our results are comparable to the best competitors, and with higher coverage our method clearly surpasses them (Table 1 in the paper). We acknowledge that when the superset has low training data coverage, our method is only comparable to existing attacks. However, higher coverage is a likely scenario in many practical settings: in the real world the attacker often has access to large candidate pools (for example, all publicly available images in a given domain), and our method’s efficiency allows scaling to such large pools without training reference models. Even if large coverage is not available in all cases, our performance being comparable at low coverage and significantly better at high coverage still makes our method practically important.\n\n| Attack\t|  AUC  \t| @ 0.1%   | @ 0.0%  |\n|-|:-:|:-:|:-:|\n| Attack-P  | 0.76 | 0.38\t| 0.06\t|\n| Attack-R  | 0.74 | 1.73\t| 0.32\t|\n| LiRA  \t| 0.80 | 2.98\t| 1.20\t|\n| RMIA  \t| 0.80 | 2.43\t| 0.14 |\n| Ours  | 0.75 | 3.01\t| 0.94\t|"}}, "id": "CoRFLewnNK", "forum": "jDpe3c8gys", "replyto": "jDpe3c8gys", "signatures": ["ICLR.cc/2026/Conference/Submission5912/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5912/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5912/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763750073118, "cdate": 1763750073118, "tmdate": 1763750073118, "mdate": 1763750073118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new white-box membership inference attack (MIA) based on the assumption that a trained model’s parameters can be linearly represented by the margin gradients of a set of samples. The key idea is that member samples should contribute more (i.e., have larger coefficients) than non-members in this representation, which can be leveraged to distinguish them. The method is compared against several state-of-the-art MIAs and shows certain improvements under specific settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is grounded in an interesting insight that model parameters can be expressed as a linear combination of sample margin gradients, where members and non-members exhibit distinguishable coefficient differences.  \n2. The proposed approach is evaluated against several recent state-of-the-art membership inference attacks, ensuring a fair and up-to-date comparison.  \n3. The method shows empirical effectiveness, achieving improvements under specific settings — particularly when the reference model used in state-of-the-art attacks deviates significantly from the target model, and the target set contains samples from a different distribution."}, "weaknesses": {"value": "1. The method is based on a theory that is formally limited to homogeneous ReLU networks. Although the authors claim that the results can generalize to other architectures in practice, the experiments are conducted only on ResNet-18, without further discussion or validation on other architectures.  \n2. Despite assuming white-box access to model parameters, the proposed method performs worse than prior black-box attacks in most cases (Table 2). It only shows advantages when the reference model deviates significantly from the target model and the target set contains both in-distribution and out-of-distribution samples, though the authors claim this represents a realistic setting.  \n3. The paper lacks discussion and comparison with recent label-only membership inference works [1][2][3][4], which achieve comparable performance to black-box attacks in more realistic settings.  \n4. The success of the proposed method relies on the attacker using a target set containing many samples from both in-distribution and out-of-distribution data. The size and composition of this target set are likely to have a strong influence on attack performance, but this factor is not analyzed in depth.  \n\n[1] *Label-Only Membership Inference Attacks*, ICML 2021  \n[2] *Membership Leakage in Label-Only Exposures*, CCS 2021  \n[3] *You Only Query Once: An Efficient Label-Only Membership Inference Attack*, ICLR 2024  \n[4] *OSLO: One-Shot Label-Only Membership Inference Attacks*, NeurIPS 2024"}, "questions": {"value": "1. Can the proposed method infer the membership status of individual samples, or does it only work when applied to a set of samples?  \n2. How would the method behave if the target set contains only members or only non-members?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KQntK0wS8o", "forum": "jDpe3c8gys", "replyto": "jDpe3c8gys", "signatures": ["ICLR.cc/2026/Conference/Submission5912/Reviewer_irvp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5912/Reviewer_irvp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618757602, "cdate": 1761618757602, "tmdate": 1762919421380, "mdate": 1762919421380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a more practical MIA method, which exploits the implicit bias of neural networks for detecting member data. The method can perform without any reference models or their assumptions. Extensive experiments show the performance of the proposed method under a more practical setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper investigates a more practical scenario. Although existing methods achieve great performance for identifying members, they often require impractical assumptions (e.g., attackers know the training hyperparameters and distribution of training data). To address the issues, the authors proposed a more practical MIA method, which can identify training samples in more realistic settings where only the model weights and a superset of the training data are available.\n\n2. The proposed method demonstrates novelty. Based on the connection between KKT conditions and parameter representations, the authors compute the membership score by optimising the coefficient for each sample. Members tend to have larger coefficients, while non-members remain small.\n\n3. Experimental results are promising. The authors evaluate the proposed method against both black-box and white-box baselines on the CIFAR-10, CIFAR-100, and CINIC-10 datasets. The results demonstrate the effectiveness of the method. Moreover, the authors conducted experiments to investigate the performance of the method under different assumptions."}, "weaknesses": {"value": "1. The method is costly. Given a candidate set, the method needs to optimise the λ coefficients to satisfy the KKT conditions. The computation is much costly when the model has a large number of parameters. Moreover, the authors are encouraged to provide a comparison of runtime or computational efficiency with the baselines.\n\n2. The method requires access to partial training data. The method can achieve the best performance only when most of the training set is included in the candidate set. In real-world scenarios, the candidate set to be detected may contain only a small portion of the training data, which may significantly reduce the performance of the method."}, "questions": {"value": "1. In Table 1, what does the evaluation data consist of?\n2. What is the performance of the method on other models?\n3. In Table 2, what is the performance of the gradient-based attack method? Additionally, could you also provide the AUC score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0uVlMQPNem", "forum": "jDpe3c8gys", "replyto": "jDpe3c8gys", "signatures": ["ICLR.cc/2026/Conference/Submission5912/Reviewer_LWLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5912/Reviewer_LWLJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630696881, "cdate": 1761630696881, "tmdate": 1762918343599, "mdate": 1762918343599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ImpMIA, a white-box attack that leverages the KKT optimality conditions to identify training members as the samples whose gradients most effectively reconstruct the model's parameters. This approach can be viewed as a MIA score derived directly from the target model, eliminating the need for reference models and assumptions about training hyperparameters, data distribution, or member ratios. Experimental results demonstrate that ImpMIA achieves state-of-the-art performance in realistic settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is the first to connect the theory of implicit bias and KKT optimality conditions to the practical problem of MIAs, introducing a completely novel attack vector.\n2. It proposes a robust white-box attack (ImpMIA) that achieves state-of-the-art performance in these more realistic \"no-assumption\" settings."}, "weaknesses": {"value": "1. The proposed method cannot incrementally calculate scores for new test points. If auditors obtain new samples, they must recalculate the entire optimization process.  \n2. The calculated scores are not independent, making it difficult to determine a proper threshold to control the FPR using a reference non-member set.  \n3. As the authors have acknowledged, their method relies on the assumption that the attacker has a superset containing most of the training data. The ablation study shows that performance drops significantly as the training sample coverage in the superset decreases. Therefore, the attack is not \"assumption-free,\" and the authors are recommended to reframe their contribution.  \n4. The choice of hyperparameters $\\alpha$, $\\beta$, $k$, and $\\eta$ is not discussed.  \n5. Details on \"class-level boosting\" and \"sample-level margins\" are not provided.  \n6. The final score depends on pre-filtering, block-wise optimization, robust aggregation (e.g., trimmed means and SNR), and a series of post-processing steps, including class-level boosting, sample-level boosting, and distance scaling. It is unclear how much of the final performance is attributable to the core implicit bias insight versus the extensive, fine-tuned post-processing."}, "questions": {"value": "1. The white-box setting is justified by the availability of models on platforms like Hugging Face. How does the attack's computational cost and performance scale to larger models (e.g., ViT, LLMs) commonly found on such platforms?  \n2. In Table 2, ImpMIA achieves a 5.23% TPR in the \"No Assumptions\" setting but only 3.67% TPR in the standard setting. Could you explain this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TpeNLqPIzf", "forum": "jDpe3c8gys", "replyto": "jDpe3c8gys", "signatures": ["ICLR.cc/2026/Conference/Submission5912/Reviewer_mvHo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5912/Reviewer_mvHo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922982417, "cdate": 1761922982417, "tmdate": 1762918343380, "mdate": 1762918343380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}