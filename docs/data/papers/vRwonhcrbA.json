{"id": "vRwonhcrbA", "number": 15154, "cdate": 1758248361188, "mdate": 1763639301554, "content": {"title": "DeL: Biologically Plausible Dendritic Learning Enables Class-Incremental Learning", "abstract": "Class-incremental learning (CIL) enables models to acquire new knowledge while retaining prior knowledge, thereby adapting to continuous data streams. Because parameter drift and distribution shifts are inevitable, it suffers from catastrophic forgetting and the stability–plasticity dilemma.  There are various strategies to address these challenges. Nevertheless, they still remain limited by the \\textit{homogeneous representations}, which reduce inter-class diversity and exacerbate forgetting. To overcome this bottleneck, we introduce dendritic learning (DeL), a biologically inspired framework that reduces homogeneous representations and thereby mitigates catastrophic forgetting. DeL leverages synaptic plasticity and multi-branch dendrites to extract diverse, discriminative features, fostering heterogeneous representation learning. A membrane layer integrates these features, and a subsequent somatic layer adapts them for downstream classification. By strengthening class-specific features, DeL also promotes robust memory consolidation. Experiments show that augmenting state-of-the-art CIL methods with DeL consistently boosts accuracy. Furthermore, DeL encourages more efficient representation learning, allowing the model to rely on fewer discriminative features. Code is available at https://github.com/anonymous/DeL.", "tldr": "", "keywords": ["Continual Learning", "Lifelong Learning", "Dendritic Learning", "Biologically Plausible"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f060b7a251355b381891861bc8200206aa7615ec.pdf", "supplementary_material": "/attachment/93a328bf52af2105428aadb8bdbb3678db59870f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes dendritic learning (DeL), inspired by the biological structure of neurons, consisting of synaptic, dendritic, membrane, and somatic layers. The authors introduce a classifier that replaces the fully connected layer (FC layer) to alleviate forgetting in baseline algorithms (particularly CNNs), arguing that it can overcome the limitations of homogeneous representations. In the end, the method achieves improved average and final accuracy across various class-incremental learning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper aims to address the forgetting problem in continual learning through biologically inspired mechanisms and demonstrates that improved the attention map can maintain object attention even after sequential tasks.\n- The paper is well-written and well-organized.\n- The introduction provides a clear overview of the development of CIL, offering valuable information for readers interested in this field.\n- The experimental results are presented in sufficient detail, making it easy to identify the strengths and weaknesses of the proposed method.\n- The transition from homogeneous to heterogeneous representation could potentially benefit CIL, although a concrete mathematical proof of this claim is lacking.\n- The paper provides the corresponding official code."}, "weaknesses": {"value": "- In Table 1, the performance difference between the fine-tuning baseline and the proposed method is marginal. Moreover, the lower gain in last accuracy compared to average accuracy may actually indicate accelerated forgetting, which contradicts the claim that the proposed method helps mitigate forgetting in CIL. To validate the effectiveness of the proposed method regarding stability and plasticity, it would be necessary to report backward transfer (BWT) and forward transfer (FWT) [1].\\\n[1] Gradient Episodic Memory for Continual Learning, NeurIPS 2017.\n- To better understand the competitiveness of the proposed classifier design, comparisons with recently introduced classification approaches such as the Kolmogorov-Arnold Classifier [2] and the Error-based Classification Method [3] are recommended.\\\n[2] KAC: Kolmogorov-Arnold Classifier for Continual Learning, CVPR 2025.\\\n[3] Prediction Error-based Classification for Class-Incremental Learning, ICLR 2024.\n- Although minor, the citation style is not visually distinct from the main text, making it somewhat difficult to read.\n- By attaching DeL after CNN, the proposed model appears to result in increasing computational complexity than the baseline CNN, raising the question of whether the performance improvement is merely due to deeper network rather than the proposed mechanism by itself. \n- In this respect, the fairness of the comparison in Figure 2 is questionable. For instance, it would be interesting to see whether a standard attention layer added to CNN could achieve similar performance.\n- The proposed method introduces a large number of learnable parameters, and it is unclear how stable the training process remains with such complexity.\n- It would also be helpful to show failure cases in Figure 2."}, "questions": {"value": "- Experiments with pretrained model-based approaches [4,5] could provide more insights into the influence of the proposed DeL method since these methods use fixed feature extractors. The PILOT library [6], which supports pretrained model-based continual learning and shares a similar code structure to PyCIL, should make such experiments straightforward to implement.\\\n[4] Learning to Prompt for Continual Learning, CVPR 2022.\\\n[5] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning, ECCV 2022.\\\n[6] PILOT: A Pre-Trained Model-Based Continual Learning Toolbox.\n- The results for using only SN in Table 2 would be interesting to see."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QwXZOS7JuZ", "forum": "vRwonhcrbA", "replyto": "vRwonhcrbA", "signatures": ["ICLR.cc/2026/Conference/Submission15154/Reviewer_Utud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15154/Reviewer_Utud"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761205931059, "cdate": 1761205931059, "tmdate": 1762925468208, "mdate": 1762925468208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Dendritic Learning (DEL) is a biologically inspired framework that mitigates catastrophic forgetting in class-incremental learning (CIL). It reduces homogeneous representations by introducing synaptic plasticity and multi-branch dendritic processing. The framework is model-agnostic, allowing integration with existing CIL methods by replacing their final fully connected layer with a dendritic block composed of four layers: synaptic, dendritic, membrane, and somatic. DEL extracts discriminative features, enhancing adaptability and retention across sequential learning tasks. The authors conducted experiments using a ResNet-18 backbone on standard CIL benchmarks demonstrating improved performance and reduced forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "DEL framework is inspired by dendritic neuron models (DNMs) that mitigates catastrophic forgetting in CIL to balance stability and plasticity, fostering better generalisation across sequential tasks.\n\nThe authors provide comprehensive Grad-CAM visualisations that focus on relevant object regions across incremental tasks highlighting its capability for heterogeneous, discriminative, and class-specific feature learning.\n\nDEL framwork replaces the fully connected layer with a four-layer dendritic block, where Synaptic layers handle excitatory and inhibitory inputs using learnable sigmoids for synaptic plasticity, Dendritic layers perform multiplicative (logical AND) operations, Membrane layers aggregate signals via weighted fusion (logical OR), and Somatic layers apply threshold-based classification.\n\nThe framework is model-agnostic, lightweight, and can be integrated into various CIL methods without using extra parameters, memory buffers, or rehearsal mechanisms."}, "weaknesses": {"value": "While the proposed DEL framework mitigates catastrophic forgetting, several limitations reduce its overall impact. First, the method relies on fixed hyperparameters and a static architectural design that could hinder scalability to deeper backbones or multimodal tasks beyond the tested ResNet-18 and vision datasets. \n\nDeL depends on backpropagation that undermines the claim of biological plausibility. This could introduce issues such as global error propagation, a limitation that has been seen in other neuron-inspired frameworks.\n\nThe fixed dendritic branch configuration (M = 2) may not generalise well across datasets, potentially constraining the balance between stability and plasticity.\n\nFrom a theoretical standpoint, the paper lacks convergence proofs or a formal mathematical analysis of gradient behavior, particularly in high-dimensional spaces. The absence of such analysis limits confidence in the method’s robustness and stability. \n\nComparisons with more recent dendritic learning models are missing, which would help contextualize DeL’s novelty and contribution within the broader literature.\n\nAlthough relative improvements are demonstrated, performance on complex datasets remains modest, suggesting that the approach is not yet mature"}, "questions": {"value": "How stable is DeL’s usage on large-scale datasets? Is there any trade-off between DeL’s biological plausibility and its ability to scale in modern deep networks such as Transfrmers. It has only been tested using ResNet18 backbone, have you done additional experiments on any other deeper versions of ResNET\n\nCan DeL be extended to task-incremental, domain-incremental or online continual learning settings?\n\nGiven backpropagation's implausibility, have alternatives like evolutionary algorithms or localised updates been explored for training?\n\nWhy no direct comparisons to biologically plausible baselines like Dendritic Localized Learning (DLL) or dendritic SNNs? Do these yield similar gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RQiQkE1SC4", "forum": "vRwonhcrbA", "replyto": "vRwonhcrbA", "signatures": ["ICLR.cc/2026/Conference/Submission15154/Reviewer_CS3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15154/Reviewer_CS3z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953950030, "cdate": 1761953950030, "tmdate": 1762925467816, "mdate": 1762925467816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel neurobiology-inspired dendritic learning (DeL) module to address the challenges of catastrophic forgetting and homogeneous representations in class-incremental learning. The proposed DeL architecture, which consists of a Synaptic Layer, Dendritic Layer, Membrane Layer, and Somatic Layer, emulates synaptic plasticity through learnable weights to modulate feature strength. A key strength is its model-agnostic design, allowing it to be seamlessly integrated into vasrious mainstream CIL methods by replacing the standard classifier. Extensive experiments across six benchmark datasets demonstrate that DeL consistently improves the average incremental accuracy of baseline models. Furthermore, the authors provide a thorough analysis using Grad-CAM visualizations, weight distribution analysis, information entropy, and class selectivity, which improve that DeL mitigates forgetting by reducing parameter drift and increasing the proportion of class-specific neurons."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper analyzes the multi-branched dendrites and synaptic plasticity of biological neurons, transforming them into a clearly structured, integrable artificial neural network module. The approach is relatively novel. Besides，This paper benchmarks DeL against a wide range of CIL methodologies on multiple \ndatasets, providing strong empirical support for its effectiveness. In addition, the detailed visualizations significantly enhance the paper’s persuasiveness and interpretability."}, "weaknesses": {"value": "The paper identifies homogeneous representations as a limitation of existing methods and compares DeL with several CIL paradigms. However, it lacks detailed discussion and comparison with other CIL methods that also explore representation heterogeneity or biologically inspired plasticity. Including such comparisons could further highlight the unique advantages of DeL.Although the paper claims that DeL reduces memory usage and relies on fewer discriminative features, it lacks a systematic analysis of the computational and parameter overhead introduced by the DeL module itself (such as increases in parameter count, FLOPs, or training/inference time) compared with baselines. Adding this analysis would strengthen the paper’s credibility.Currently, the experiments are limited to relatively small-scale datasets. Evaluations on large-scale datasets (e.g., ImageNet-1K) and broader architectures (e.g., ViT) would make the empirical validation more convincing."}, "questions": {"value": "The ablation studies mainly focus on synaptic plasticity (Table 2). Could the authors provide additional ablations for other key components of DeL (such as the weighted fusion mechanism in the Membrane Layer and the threshold adjustment in the Somatic Layer) to more comprehensively demonstrate each component’s contribution?\n\nThe paper claims that DeL is particularly valuable in non-rehearsal settings where rehearsal buffers are limited. Are there experiments directly comparing DeL’s performance under “with-buffer” and “without-buffer” conditions to quantitatively support this claim?\n\nIn Table 1 and Figure 3, the dataset names (CIFAR100 vs. CIFAR B0 Inc10) appear inconsistent. It is recommended to unify the dataset naming to improve clarity and reduce potential confusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1jg9xEZTuU", "forum": "vRwonhcrbA", "replyto": "vRwonhcrbA", "signatures": ["ICLR.cc/2026/Conference/Submission15154/Reviewer_7Yo3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15154/Reviewer_7Yo3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992533941, "cdate": 1761992533941, "tmdate": 1762925467475, "mdate": 1762925467475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposes a biologically inspired classifier head for class-incremental learning called DeL. DeL replaces the standard fully connected layer with a four-stage module (synaptic gating, dendritic branching, membrane fusion, and somatic activation) intended to increase feature diversity and reduce forgetting. The module is evaluated as a plug-in to several existing CIL methods on small to medium-scale benchmarks, showing some rather small accuracy gains. While the implementation is general, the approach offers limited conceptual novelty: it functions as a gated multi-branch MLP trained with standard backpropagation, and its connection to biological learning is superficial. The experimental analysis lacks statistical rigor (single seed, no forgetting metrics, limited architectures) and omits comparisons to strong modern baselines such as cosine, prototype, or prompt-based heads. Overall, the work seems rather incremental and it does not meet the novelty and depth expected for ICLR acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "a. Simple, modular design that can replace a standard classifier layer in many CL frameworks\nb. Broad empirical evaluation across several CIL methods and datasets showing generally consistent (but small) improvements.\nc. Clear architectural description with ablation studies isolating some effects\nd. Use of visual analyses (Grad-CAM, weight distributions) to illustrate feature diversity and reduced forgetting"}, "weaknesses": {"value": "1. The main weakness, at least in my opinion is in terms of novelty. DeL is effectively a gated multi-branch MLP head with standard backpropagation. This is nothing new fundamentally.\n2. The “biological plausibility” claim is rather superficial. There is no local learning or biologically grounded mechanism\n3. The experimental section lacks rigor. If I understand well the authors use a single seed, no statistical variation, no forgetting or BWT metrics. Plus, only small datasets\n4. The problem framing is confusing/nconsistent: the paper claims a non-rehearsal setup but evaluates with rehearsal-based methods\n5. Missing comparisons to strong and directly relevant baselines (cosine, prototype, MoE, prompt-based, or attention-based heads).\n6. Ambiguity in classifier output. Unclear whether sigmoid or softmax is used for multi-class loss. The training objective not well defined.\n7. The design of “membrane” and “somatic” layers is rather unclear. No ablation or justification for these components.\n8. Limited architectural scope: only ResNet-18 tested. Unclear generalization to pretrained or transformer backbones.\n9. Qualitative visualizations are rather unconvincing and they do not substantiate claims of reduced forgetting or improved representation diversity."}, "questions": {"value": "First, I suggest that the authors also read the following papers because they are quite relevant -- even though they may not (all) be neuro-inspired:\n\nLearning to Prompt for Continual Learning\nDualPrompt: Complementary Prompting for Rehearsal-Free Continual Learning\nMixture-of-Experts Meets Prompt-Based Continual Learning\nLearning a Unified Classifier Incrementally via Rebalancing\nFew-Shot Class-Incremental Learning via Training-Free Prototype Calibration\nDynamic Adapter Tuning for Long-Tailed Class-Incremental Learning\nA rapid and efficient learning rule for biological neural circuits\nOrthogonal Subspace Learning for Continual Adaptation\n\nSome additional comments and suggestions:\n\n- plz clearly define the loss function and output layer semantics. If sigmoid activations are used, explain how multi-class probabilities are computed and justify the choice over softmax\n- plz report results over multiple random seeds and include standard metrics for continual learning (average accuracy, forgetting, backward transfer). This is all standard practice.\n- quantify the computational and parameter overhead of DeL compared to a linear classifier.\n- I suggest you add ablations for the membrane and somatic layers to show their individual contribution\n- Include comparisons to modern classifier-head designs such as cosine-normalized, prototype-based, attention pooling, and small MLP or gating heads with equal parameter count\n- Important to test DeL on stronger and more diverse architectures, especially pretrained ViTs, to assess generality.\n- Plz either rephrase or substantiate the “biologically plausible” claim. For example you can do so by introducing a biologically inspired local learning rule or by avoiding that terminology altogether\n- Improve the clarity of experimental settings: specify buffer sizes, rehearsal policies, and dataset protocols\n- Perform a rehearsal-free evaluation to verify whether DeL alone improves plasticity–stability balance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k75Sn9LW8Q", "forum": "vRwonhcrbA", "replyto": "vRwonhcrbA", "signatures": ["ICLR.cc/2026/Conference/Submission15154/Reviewer_M6Ph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15154/Reviewer_M6Ph"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091117919, "cdate": 1762091117919, "tmdate": 1762925467118, "mdate": 1762925467118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}