{"id": "uP6RDWHcs7", "number": 14943, "cdate": 1758245877770, "mdate": 1759897340032, "content": {"title": "Marginal Flow: a flexible and efficient framework for density estimation", "abstract": "Current density modeling approaches suffer from at least one of the following shortcomings: expensive training, slow inference, approximate likelihood, mode collapse or architectural constraints like bijective mappings. We propose a simple yet powerful framework that overcomes these limitations altogether. We define our model $q_\\theta(x)$ through a parametric distribution $q(x|w)$ with latent parameters $w$. Instead of directly optimizing the latent variables $w$, we marginalize them out by sampling them from a learnable distribution $q_\\theta(w)$, hence the name Marginal Flow. In order to evaluate the learned density $q_\\theta(x)$ or to sample from it, we only need to draw samples from $q_\\theta(w)$, which makes both operations efficient. The proposed model allows for exact density evaluation and is orders of magnitude faster than competing models both at training and inference. Furthermore, Marginal Flow is a flexible framework: it does not impose any restrictions on the neural network architecture, it enables learning distributions on lower-dimensional manifolds (either known or to be learned), it can be trained efficiently with any objective (e.g. forward and reverse KL divergence), and it easily handles multi-modal targets. We evaluate Marginal Flow extensively on various tasks including synthetic datasets, simulation-based inference, distributions on positive definite matrices and manifold learning in latent spaces.", "tldr": "We propose a density estimation model that allows for efficient exact density evaluation, fast inference and training, and manifold learning.", "keywords": ["density estimation", "exact likelihood", "variational inference", "manifold learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67fad608eccbfeb08b237e9e6b6bdeeb8e438a66.pdf", "supplementary_material": "/attachment/846a39bb88de9a134911e2887a1d408b0842c5b4.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Marginal Flow, a density estimation framework the directly marginalizes the latent parameters sampled from a learnable distribution induced by a flexible mapping function from z to w where the distribution of z is typically known (e.g., standard Gaussian or uniform distribution). The authors claim that the proposed method is highly flexible and also efficient in training, evaluation, and sampling. The authors also conducted experiments in several simulation datasets and showed some results in the image dataset where a dimension reduction tool (e.g., VAE) is used. The whole model can be considered as a neural network parameterized Gaussian mixture in the default setting. I list some constrains, limitations, and questions below."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe problem is well described and formulated.\n2.\tThe method is easy to understand.\n3.\tThe results show some advantage in the current settings."}, "weaknesses": {"value": "1.\tThe novelty is limited as the approach is essentially a mixture model where the mixing density is induced by a flexible neural network (e.g., MLP with a few layers). The marginalization is also not new as it is just Monte Carlo.\n2.\tThe scalability is very constrained. Since the method requires the evaluation of full likelihood on all samples and the all Ws from Monte Carlo. I can image that the method could not work efficiently as sample size grows to a large number.\n3.\tMany important baselines are missed for comparison or not even mentioned. E.g., Roundtrip, MAF, and RealNVP. Roundtrip uses exactly the same idea of marginalization in estimating p(x) by adopting a cycleGAN architecture.\n4.\tThe authors benchmarked the method mostly on toy datasets (e.g., 2 dimensional). The ability to handle high-dimensional density estimation is not actually demonstrated. \n5.\tThe model should be highly sensitive to Nc since it determined the Monte Carlo error. How to determine the optimal Nc is not discussed."}, "questions": {"value": "1.\tPlease show the benchmark results in high-dimensional settings since traditional statistical methods can already handle very well for density estimation in low-dimensional settings (e.g., d<10).\n2.\tPlease conduct sensitivity analysis to the parameter Nc. There should be at least an empirical guidance on how to choose Nc.\n3.\tIn terms of scalability, the current training requires the computation of full log-likelihood. Is it possible to improve the scalability by mini-batch update? If so, how performance will be affected?\n4.\tCan the authors provide scenarios where the proposed method is preferable in practice over normalizing flow, diffusion models?\n5.\tMarginal Flow seems to be a misleading name, since xxxFlow typically refers to methods with multiple transformations between the base distribution and the target distribution, such as normalization flow with multiple invertible transformations functions, and flow matching/mean flow with infinite steps (t=0 to 1). While the proposed approach does not involve multiple steps of transformation.\n6.\tSome errors or typos. Line 111: Sigma=diag(sigma_1^2,…, sigma_d^2). Line 129: f_theta(z) instead of f_theta(w)\n7.\tThe author claimed the approach can evaluate the density exactly is misleading since there is a Monte Carlo error that is not discussed at all in the paper, which depends on the Nc. So the equation (2) is still an approximated density.\n8.\tHow the variance term is learned? Any constrain on it or just set it to be a trainable parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QPsUFrmPxa", "forum": "uP6RDWHcs7", "replyto": "uP6RDWHcs7", "signatures": ["ICLR.cc/2026/Conference/Submission14943/Reviewer_sA1N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14943/Reviewer_sA1N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760473315045, "cdate": 1760473315045, "tmdate": 1762925282083, "mdate": 1762925282083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed marginal flow, supporting sampling and density estimation using a latent variable model, showing success in toy examples, low dimensional datasets and small images.\n\nHowever, unfortunately, I think this paper has a very limited contribution: the idea of marginalisation has existed in early generative models, like VAE or implicit models. In fact, one can also view diffusion models (like DDPM) as one example with many layers of latents. Then the question is, why did these models not use the approach mentioned in this paper? The answer is because given one $x$, it is very difficult to sample enough $w$ to yield a reliable estimation of $q(x)$, even though all the terms in the MC estimator (eq 2 in this paper) are tractable. \nTherefore, one can imagine that as the dimensionality grows, the efficiency of the proposed methods will significantly reduce, especially when $q(w|x)$ is far from $q(w)$. (I use $q$ to follow the notation of this paper).  And hence I am not convinced that this approach is efficient.\n\nEven not considering this inefficiency, the claim that marginal flow supports **exact** density evaluation is not accurate: it is only exact when $N_c\\rightarrow \\infty$. Given this assumption, many approaches (for example, diffusion density estimator by [1] and even VAE) are also exact. \n\n\n\nTherefore, given the fundamental limitation of this paper, I am sorry to say that I am not convinced that this work achieves the bar of publication. \n\n\n[1] Huang, Chin-Wei, Jae Hyun Lim, and Aaron C. Courville. \"A variational perspective on diffusion-based generative models and score matching.\" NeurIPS 2021."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Please see Summary"}, "weaknesses": {"value": "Line 128-129: $f(w)$ should be $f(z)$?"}, "questions": {"value": "One may also estimate a proposal $q(w|x)$ similar to VAE for importance sampling to boost the efficiency. This will also be exact when $N_c\\rightarrow \\infty$. Will this strategy help the method to scale up to higher-dimensional datasets?\n\nHow do you distinguish VAE and this proposed approach, except that the training strategy is different? \n\nHow does the performance change with respect to $N_c$?\n\nThe training is an important part of the algorithm, maybe its benefial to include it in the main text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9nlJSAf5Dk", "forum": "uP6RDWHcs7", "replyto": "uP6RDWHcs7", "signatures": ["ICLR.cc/2026/Conference/Submission14943/Reviewer_nQRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14943/Reviewer_nQRv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760776975050, "cdate": 1760776975050, "tmdate": 1762925281759, "mdate": 1762925281759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "What the paper claims. The paper introduces Marginal Flow, a density estimator defined as a Monte‑Carlo marginal over latent parameters $w$. Choose an easy‑to‑evaluate component family $q(x\\mid w)$ (often diagonal‑covariance Gaussian); draw $w$ by pushing base samples $z\\sim p_{\\text{base}}$ through an unconstrained neural map $f_\\theta$; then define the model as the finite average\n$q_\\theta(x) \\;=\\; \\frac{1}{N_c}\\sum_{i=1}^{N_c} q(x\\mid w_{\\theta,i}),\\quad w_{\\theta,i}=f_\\theta(z_i)$.\nCrucially, $w$ is resampled for every density evaluation and sampling call (Fig. 2; §2.2). The paper argues this yields (i) exact evaluation for the finite mixture in Eq. (2), (ii) single‑step sampling, (iii) architectural freedom (no bijectivity/Jacobians/ODEs), (iv) the option to choose $p_{\\text{base}}$ in lower dimension to “learn manifolds,” and (v) efficient training with forward or reverse KL. Experiments cover 2‑D toy densities, reverse‑KL fitting without observations, conditional SBI, SPD matrices via Wishart, and latent‑space “manifolds” for MNIST/JAFFE; runtime plots (Fig. 3) emphasize speedups over NF/FM/FFF."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Simplicity & speed: No Jacobians/ODEs; likelihood is a sum over $N_c$. Runtime plots (Fig. 3) back the speed advantage.  \n- Flexible component families: Wishart experiment on SPD matrices is a nice demonstration (Fig. 9).  \n- Both KL directions: Efficient sampling and evaluation allow reverse‑KL training without tricks; synthetic results look competitive with NFs in reverse‑KL (Fig. 8).  \n- Clear exposition & code: Equations (5–11) and Appendix A.1 make reproduction feasible, and specific $N_c$ choices are stated for some tasks."}, "weaknesses": {"value": "1. **Stochastic “exact” likelihood & evaluation protocol.**. \nLikelihoods depend on a fresh resample of w every call (pp. 3–4; Fig. 2). Report NLL mean+/-std over resamples, define a fixed‑mixture evaluation protocol (freeze one draw of w at test time), and separate “finite‑mixture exactness” from “marginal exactness.”  \n2. **No guidance on $N_c$ and no scaling study.**. \nProvide NLL/accuracy vs $N_c$ curves, wall‑clock trade‑offs, and dimension sweeps. Connect observations to known nonparametric rates to discuss when growth of $N_c$ becomes prohibitive.  \n3. **Manifold claims need precision or a different construction.**. \nEither adopt degenerate/anisotropic components (with care for normalization) or manifold‑native densities to achieve genuine lower‑dimensional support; otherwise rephrase as mass concentrated near low‑dimensional structure.   \n4. **Novelty vs MDN not delineated.**. \nExplicitly compare (conceptually and experimentally) to Mixture Density Networks [MDN] (network‑parameterized mixtures/kernels). Discuss statistical/compute advantages beyond not needing to evaluate $q_\\theta(w)$. Does this limit novelty? Also, can you show that the approach is going to learn something different than a Kernel Density Estimation method? \n5. **Comparisons emphasize low-dimensional micro‑benchmarks.**. \nMixture of Gaussians does not scale well with ambient space dimension $D$; curse of dimensionality. Theoretical properties of $N_c$ as a function of $D$ are missing. Introduce higher dimensional experiments to convince otherwise, e.g. on typical image generative modelling datasets (MNIST; CelebA; ImageNet 64x64).  \n6. **Universal approximation theory is missing.**. \nEven light‑touch results would help: e.g., approximation error of the Monte‑Carlo mixture to the ideal marginal as a function of $N_c$; and conditions under which the model inherits known universal approximation properties of mixtures (with references).\n\nReferences\n----\n[MDN] Mixture Density Networks. Bishop 1994."}, "questions": {"value": "1. Test‑time protocol & variance. Do you freeze one draw of $\\{w_{\\theta,i}\\}_{i=1}^{N_c}$ for the entire test set, or resample per‑example? Please report NLL mean+/-std over 20 resamples on your test sets and clarify which numbers appear in figures. The paper currently implies resampling at every evaluation (pp. 3–4; Fig. 2).  \n2. Guidance on N_c. Provide ablations of NLL/C2ST vs $N_c$ and discuss compute/variance trade‑offs. In Appendix A.4 you pick $N_c=$ half the training set, SBI uses $N_c=2048$, JAFFE $N_c=128$; what drives these choices? For latent-variable MNIST, is the VAE trained jointly with the Marginal Flow? What is the training procedure exactly?\n3. Dimension scaling. Have you tried ambient $D \\ge 32$ in data space (not latent) with quantitative benchmarks? How does runtime and NLL change with d when keeping a fixed compute budget? Report test log-likelihood and sample quality metrics (FID,KID) comparing against other generative modelling methods using networks with similar sizes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1p0DwngwWX", "forum": "uP6RDWHcs7", "replyto": "uP6RDWHcs7", "signatures": ["ICLR.cc/2026/Conference/Submission14943/Reviewer_R9aj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14943/Reviewer_R9aj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953483291, "cdate": 1761953483291, "tmdate": 1762925281235, "mdate": 1762925281235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}