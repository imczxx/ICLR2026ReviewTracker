{"id": "akbtPEZnDZ", "number": 14540, "cdate": 1758238428986, "mdate": 1759897363779, "content": {"title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "abstract": "We discover a novel and surprising phenomenon of unintentional misalignment in reasoning language models (RLMs), which we call \\textit{self-jailbreaking}.\nSpecifically, after benign reasoning training on math or code domains, RLMs will use multiple strategies to circumvent their own safety guardrails.\nOne strategy is to introduce benign assumptions about users and scenarios to justify fulfilling harmful requests. \nFor instance, an RLM reasons that harmful requests like \"outline a strategy for stealing customers' credit card information from a retail store'' could be associated with the benign intent of \"a security professional trying to test defense,''  despite no such benign context being provided as input.\nWe observe that many open-weight RLMs, including DeepSeek-R1-distilled, s1, Phi-4-mini-reasoning, and Nemotron, suffer from self-jailbreaking despite being aware of the harmfulness of the requests.\nWe also provide a mechanistic interpretation of self-jailbreaking: RLMs are more compliant after benign reasoning training, and self-jailbreaking reduces the perceived harmfulness of malicious requests so models can comply with them.\nTo mitigate self-jailbreaking, we find that including minimal safety reasoning data during training is sufficient to ensure RLMs remain safety-aligned. \nOur work provides the first systematic analysis of self-jailbreaking behavior and offers a practical path forward for maintaining safety in increasingly capable RLMs.", "tldr": "", "keywords": ["Reasoning Models", "Safety Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fbd330b5c27d05689ecb293f23a0b908f5083cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes the concept of Self-Jailbreaking, referring to the phenomenon where certain Reasoning Language Models (RLMs) produce unsafe or inappropriate outputs after engaging in multi-step reasoning (Chain-of-Thought, CoT). Through analysis, the authors argue that this behavior originates from a process of “self-persuasion” that occurs during reasoning: as the model deliberates, it gradually compromises between safety alignment and obedience (compliance), ultimately convincing itself that the user’s intent is benign and therefore proceeding to generate harmful or policy-violating content.\n\nThe authors conduct experiments on multiple open-source Reasoning Language Models (RLMs), demonstrating the widespread nature of the self-jailbreaking phenomenon. In addition, they propose an alignment method based on lightweight fine-tuning with a small amount of safety reasoning data, which effectively mitigates such behaviors while preserving the models’ reasoning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces the concept of Self-Jailbreaking for the first time, revealing a fundamental flaw in current Reasoning Language Models (RLMs).\n2.\tExperiments conducted on multiple open-source RLMs demonstrate the generality and reproducibility of this mechanism.\n3.\tThe authors provide a theoretically grounded analysis of the underlying causes, and the interpretation based on activation-direction projection is both logical and convincing."}, "weaknesses": {"value": "1.\tThe analysis of Self-Jailbreaking lacks specificity. The paper does not systematically investigate which types of questions or prompts are more susceptible to triggering this phenomenon.\n2.\tThe proposed mitigation relies on fine-tuning the model, which limits its applicability to closed or black-box models where retraining or parameter access is not possible.\n3.  The model lacks dynamic robustness. Since the proposed mechanism is based on SFT (Supervised Fine-Tuning), it remains unclear whether the model will exhibit self-jailbreaking tendencies again after long-term usage, continuous updates, or multi-turn interactions. Moreover, the paper does not discuss any decoding-time or inference-time defense mechanisms to prevent such reoccurrence."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HqxKBlf3NF", "forum": "akbtPEZnDZ", "replyto": "akbtPEZnDZ", "signatures": ["ICLR.cc/2026/Conference/Submission14540/Reviewer_9kKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14540/Reviewer_9kKa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650040699, "cdate": 1761650040699, "tmdate": 1762924929910, "mdate": 1762924929910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is really well-structured and highly motivative. I am really impressive by this self-jailbreaking phenomenon and get plenty of insights from this paper. The authors discover a concerning and previously uncharacterized safety failure mode \"self-jailbreaking\" that the reasoning model could reason itself out of safety alignment by introducing assumptions about user intent or context to justify fulfilling harmful requests or assuming that questions are only hypothetical to sidestep ethical considerations and so on. To address this, this paper shows minimal safety reasoning data during training is sufficient to ensure RLMs remain safety-aligned."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The investigated phenomenon is novel. Reasoning models struggle to defend against malicious queries and there are a few works[1][2][3] aiming at safety alignment improvement. However, this paper provides a new view of how and why reasoning models struggle in such settings.\n\n2. The testing models include a wide range type, which make the conclusion convincing.\n\n3. The self-jailbreaking rate in Figure 2 shows that this phenomenon is indeed a core reason responsible for the poor safety alignment of large reasoning models.\n\n4. The solution is concise but effective.  \n\n[1] Improving Safety Alignment with Introspective Reasoning\n\n[2] Safety Reasoning with Guidelines\n\n[3] SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities"}, "weaknesses": {"value": "1. Lack of human evaluation. Although gpt-5-2025-08-07 is a really strong model as a judge, human evaluation is still needed to reduce the  False Positive Rate and False Negative Rate. \n\n2. Lack of baseline. I personally really like the solution of this paper that using only 50 safety reasoning samples to strongly maintain the safety and helpfulness of reasoning models. As an investigation article, I’m willing to accept that it addresses only the issues it has explored, rather than achieving state-of-the-art (SOTA) results. However, I still recommend the authors to include some safety reasoning baselines to show how this method performs against these models, though it may lag behind.\n\n3. Lack of OOD evaluation. As this paper is investigating and solving \"self-jailbreaking\", it mainly focus on vanilla harmful queries. However, I am curious about whether this phenomenon exists on jailbreak harmful queries? Is the 50 safety reasoning samples training also enough for those OOD jailbreak scenes? \n\n4. The caption in Figure 7 has squeezed the space in the following text. It is recommended to make some modifications.\n\nI will increase my score if the above concerns are solved."}, "questions": {"value": "See the Weakness part."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dfWR5Dsf7H", "forum": "akbtPEZnDZ", "replyto": "akbtPEZnDZ", "signatures": ["ICLR.cc/2026/Conference/Submission14540/Reviewer_av33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14540/Reviewer_av33"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884315748, "cdate": 1761884315748, "tmdate": 1762924929358, "mdate": 1762924929358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reveals a new safety vulnerability in large language models, termed Self-Jailbreaking, and demonstrates that this phenomenon widely exists across various open-source reasoning language models (RLMs). Through interpretability analysis, the authors uncover its underlying mechanism and propose an effective mitigation strategy that restores the balance between safety alignment and reasoning capability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.It is first to systematically identify and name the phenomenon of Self-Jailbreaking, uncovering a paradoxical mechanism in which benign reasoning training unintentionally introduces safety risks — filling an important gap in current AI safety research. \n2.The experiments cover multiple families of RLMs and quantitatively illustrate how internal model states evolve throughout the reasoning process."}, "weaknesses": {"value": "1.Although the projection analysis provides intuitive evidence, it does not fully establish the causal chain between increased compliance, reduced perceived harmfulness, and self-jailbreaking; potential confounding factors may exist. \n2.Projection-based interpretability has been widely used; the paper should justify its suitability and validity for analyzing self-jailbreaking specifically. \n3.The study lacks quantitative experiments on different types of self-jailbreaking (e.g., hypothetical scenarios, educational rationalizations, or positive-outcome justifications). \n4.The mitigation experiment (SAFE-S1.1-7B) is conducted on a single model only, without evaluating generalizability or transferability to other RLMs."}, "questions": {"value": "1.Provide causal validation between compliance increase and harmfulness perception reduction. \n2.Clarify how the proposed interpretability approach differs from existing projection-based methods. \n3.Conduct proportional and quantitative analyses across different self-jailbreaking categories. \n4.Reproduce the SAFE training strategy on other RLMs (e.g., Phi-4, Llama) to evaluate its generalizability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bhmcnrVy6H", "forum": "akbtPEZnDZ", "replyto": "akbtPEZnDZ", "signatures": ["ICLR.cc/2026/Conference/Submission14540/Reviewer_TWPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14540/Reviewer_TWPH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922783891, "cdate": 1761922783891, "tmdate": 1762924928857, "mdate": 1762924928857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They define self-jailbreaking of reasoning models as the phenomenon of RLMs reasoning their way out of safety guardrails during reasoning to assist with malicious requests, without any jailbreaking or deception attempt from the user.  They measure the occurrences of self-jailbreaking of open-weight models and how harmful their responses become after benign reasoning training on math or coding tasks. Moreover, they analyze why RLMs generate harmful outputs through self-jailbreaking and show that RLMs after benign reasoning training have increased compliance. Lastly, they show that minimal safety reasoning data can sufficiently mitigate the harmful effects of self-jailbreaking and restore safety guardrail."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The work is the first study of self-jailbreaking\n- They evaluate various models to assess how frequently self-jailbreaking appears in the reasoning models after benign math/coding reasoning training\n- They try to mechanistically explain why the models show self-jailbreaking \n- Moreover, they propose how to mitigate self-jailbreaking"}, "weaknesses": {"value": "I like this paper. It not only identifies an important problem but also explores a way to mitigate self-jailbreaking. Moreover, they provide a mechanistic interpretability analysis that explains why self-jailbreaking emerges in models after benign math and coding training.\n\nHowever, there are some issues I notice. The paper doesn't describe details regarding the model training setup prior to the safety evaluation in Section 3.2. It is unclear what specific datasets were used, how many data points they contained, and whether a single dataset or multiple datasets were involved. In addition, the paper doesn't describe whether self-jailbreaking occurred consistently across different training datasets. Addressing these questions would be important to ensure generalization and robustness."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bV7gkz7W6c", "forum": "akbtPEZnDZ", "replyto": "akbtPEZnDZ", "signatures": ["ICLR.cc/2026/Conference/Submission14540/Reviewer_E1ZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14540/Reviewer_E1ZH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762486031961, "cdate": 1762486031961, "tmdate": 1762924928365, "mdate": 1762924928365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}