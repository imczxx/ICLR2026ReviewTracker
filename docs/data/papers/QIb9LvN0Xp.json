{"id": "QIb9LvN0Xp", "number": 19744, "cdate": 1758298989139, "mdate": 1759897021938, "content": {"title": "From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers", "abstract": "Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call meltdown. Using activation-patching from mechanistic interpretability, we localize meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a drop-in, test-time control that stabilizes sparse point-cloud conditioning. On Google Scanned Objects, PowerRemap has a stabilization rate of 98.3% for the state-of-the-art diffusion transformer WaLa. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.", "tldr": "", "keywords": ["mechanistic interpretability", "diffusion dynamics", "test-time intervention", "3D diffusion"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1478dd892e99b7250c468551ac416380adad4862.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reveals and deeply analyzes a catastrophic failure phenomenon that exists in the current most advanced 3D diffusion Transformers when performing surface reconstruction from sparse point clouds, which the authors call meltdown. Specifically, a tiny perturbation located on the surface of the input point cloud may cause the model output to melt from a complete single shape into a large number of unconnected fragments.\n\nThe author first demonstrated the existence of this phenomenon and found that the spectral entropy of this key activation could serve as a measurable proxy indicator for the occurrence of the collapse. Based on this insight, the author proposed a simple yet effective test-time intervention method called PowerRemap. The author provides an explanation for the meltdown phenomenon from the perspective of diffusion dynamics, linking it to the symmetry-breaking bifurcations of the potential energy landscape during the reverse diffusion process, thereby associating the discoveries at the circuit level with the deeper theoretical framework of generative models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a high-quality and fascinating study. It combines solid experiments, ingenious interpretability analysis, effective solutions and profound theoretical connections, which is of great significance for improving the robustness and interpretability of diffusion models, especially 3D generative models."}, "weaknesses": {"value": "The paper put forward the \"consensus\" hypothesis, but this has not yet been strictly verified.\n\nCurrently, the intensity parameter Œ≥ is a manually set global value (Œ≥=100), which is not optimal and also limits the robustness of the method."}, "questions": {"value": "1. Has the author ever attempted an adaptive Œ≥ selection strategy?\n\n2. Please discuss in more detail the reasons for the poor performance on Make-A-Shape.\n\n3. In the main text or appendix, the measurement criteria for point cloud sparsity should be more clearly stated. Can the average density of the point cloud relative to the surface area of the object or the sampling distance of the farthest point be provided? This helps readers better understand the challenge of the problem setting.\n\n4. There is still some speculation about the mechanism explanation for \"why does a decreased spectral entropy reduce invalid outputs?\". It is suggested that the author deepen this explanation. For instance, can we analyze whether there are any changes in the alignment or consistency of the outputs from different attention heads before and after the PowerRemap intervention? Or, can the assumption that \"the first singular vector represents the consensus feature of multiple heads\" be verified through the analysis of singular vectors? Even a preliminary analysis can significantly enhance the persuasiveness of the argument."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AZmdqF3gP9", "forum": "QIb9LvN0Xp", "replyto": "QIb9LvN0Xp", "signatures": ["ICLR.cc/2026/Conference/Submission19744/Reviewer_5ZNt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19744/Reviewer_5ZNt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760773937311, "cdate": 1760773937311, "tmdate": 1762931579947, "mdate": 1762931579947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a common catastrophic mode of faillure for 3D diffusion model in the task of surfuace completion from sparse point clouds . They name it as melt-down and performs activation-patching to localize the failure position. They leverages the singular-value spectrum of the located activation module to serve as a proxy for the failure mode. To tackle this issue, they proposes PowerRemap module to adjust the singular value as a test-time module. Experiments are done on GSO to illustrate the idea."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The meltdown phenomenon is a common issue in 3D diffusion models for shape completion and worth investigating.\n\n2. The finding that a single cross-attention module is primarily responsible for the observed failure is particularly interesting and provides useful insight into the model‚Äôs internal behavior.\n\n3. The discussion on diffusion dynamics is interesting and contributes to a better conceptual understanding of diffusion behavior."}, "weaknesses": {"value": "1. The experiments are insufficient. The observed meltdown failure is likely to depend strongly on the density of the input point cloud, yet this factor is neither analyzed nor explicitly specified in the experiments. In addition, all experiments are conducted solely on the GSO dataset, which limits the generality of the conclusions. Including results on at least one additional dataset would significantly strengthen the empirical support for the proposed theory.\n\n2. In Fig. 3, the trend of connectivity C does not fully align with that of H. Specifically, C rises sharply and reaches its maximum around œÅ=0.4, then slightly decreases, whereas H continues to increase. This raises questions about the claimed relationship between H and C‚Äî why would a decreasing H correspond to improved connectivity? As presented, the experiment is not sufficiently convincing and requires clearer explanation or additional analysis.\n\n3. The method appears to assume that the input point cloud is clean and contains only a single object. The definition of ‚Äúhealthy‚Äù results seems to rely on this restricted input condition. It is unclear how the approach would perform when the input represents a complete scene or includes significant sensor noise. Moreover, the theoretical formulation seems to inherently produce a single mesh, regardless of the input‚Äôs complexity or content."}, "questions": {"value": "Please refer to weaknesses. Although the experimental evaluation is somewhat limited and certain analyses are not entirely convincing, the findings are novel and valuable. I would be glad to see this work published if the authors can provide additional experiments to strengthen the empirical validation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SlhyA7T171", "forum": "QIb9LvN0Xp", "replyto": "QIb9LvN0Xp", "signatures": ["ICLR.cc/2026/Conference/Submission19744/Reviewer_9Hbz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19744/Reviewer_9Hbz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619560050, "cdate": 1761619560050, "tmdate": 1762931579089, "mdate": 1762931579089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a failure mode called ‚Äúmeltdown‚Äù in point-cloud-conditioned 3D diffusion transformers, such as WALA and MAKE-A-SHAPE. Tiny on-surface input perturbations cause fragmented, multi-component outputs. Using activation patching, the authors localize causality to a single early cross-attention. The singular-value spectral entropy of this write tracks failure/rescue. The authors also propose PowerRemap, a test-time SVD power transform that lowers spectral entropy and substantially stabilizes outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clean activation-patching grid over depth√ótime pinpoints a single early cross-attention write controlling meltdown; procedure and repair map are explicit.\n\nPowerRemap is model-agnostic, test-time only, and provably reduces spectral entropy without changing singular vectors.\n\nOn GSO, meltdown occurs widely, and PowerRemap rescues 98.3% of WALA failures."}, "weaknesses": {"value": "For make-a-shape, reported rescue is only 10.1% with the same ùõæ, suggesting sensitivity to architecture and hyperparameters and limiting generality. \n\nSpectral entropy is the only diagnostic evaluated; no comparison to effective rank, top-k energy, condition number, per-head concentration, or Jacobian norms.\n\n‚ÄúConnected components‚Äù may conflate legitimate multi-part objects with failures; precision/recall vs. human labels not reported.\n\nùõæ selection is ad-hoc (global ùõæ=100); the paper itself notes the open question of choosing ùõæ and the speculative nature of the ‚Äúconsensus via low entropy‚Äù explanation."}, "questions": {"value": "Compare spectral entropy to alternative spectral metrics for predicting meltdown\n\nProvide an adaptive ùõæ rule and show it fixes MAKE-A-SHAPE‚Äôs low rescue rate"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "r7fdvUgVxr", "forum": "QIb9LvN0Xp", "replyto": "QIb9LvN0Xp", "signatures": ["ICLR.cc/2026/Conference/Submission19744/Reviewer_Nujf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19744/Reviewer_Nujf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855113854, "cdate": 1761855113854, "tmdate": 1762931576078, "mdate": 1762931576078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses mechanistic interpretability to investigates the meltdown phenomenon of 3D diffusion transformers on surface reconstruction tasks, where small perturbations to the input point cloud can cause catastrophic fragmentation of the generated 3D surfaces. Using activation patching, this paper identifies a specific cross-attention head in the WALA model whose activations have causal connections with the connectivity of the reconstructed surface. The paper shows that intervening on the magnitude of the singular values of the decomposed cross-attention output can better recover the shape of generated object. Finally, the paper connected the meltdown phenomenon with bifurcation dynamics in the reverse diffusion process."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an interesting application of existing activation patching method to identify geometry-related representations within 3D latent diffusion models.\n2. The proposed meltdown phenomenon is novel and well-characterized, although it remains unclear whether similar behavior would be observed on other surface reconstruction datasets beyond Google Scanned Objects (GSO).\n3. The proposed PowerRemap intervention is simple yet effective, demonstrating strong recovery performance on WaLa model and the GSO dataset by intervening the cross-attention head outputs that have causal connections with output surface connectivity. Nonethelss, it is questionable whether this intervention method generalizes to other models (see weakness 2)."}, "weaknesses": {"value": "1. The generalizability of this finding is very limited. The experiment focused on two models (WaLa and MAKE-A-SHAPE) and evaluated the meltdown on only one dataset (Google Scanned Objects). It is unknown whether the meltdown phenonomon is unique to the GSO datasets, and if the cross-attention head that controls the meltdown can be found in latent 3D diffusion transformer, other than WaLa and MAKE-A-SHAPE.\n2.  As shown in Tables 2 and 3 in Appendix B.3 (p. 21), the effectiveness of PowerRemap differs significantly when applied to the WALA model versus the MAKE-A-SHAPE model. While PowerRemap recovers 98% of the meltdowned generation for WALA on GSO dataset, it recovers only about 10% of meltdowned cases for MAKE-A-SHAPE model. This large discrepancy again raises concerns about the generalizability of the proposed intervention and the causal role of the identified cross-attention head in MAKE-A-SHAPE model.\n3. The interpretation offered in this paper is also limited in depth. What exact geometric features have been learned by this cross-attention head? If one ablates this cross-attention head, will the meltdown phenomenon disappear? What is the trade-off between suppressing the spectral entropy of this head's output versus ablating it. \n4. The trends shown in Figure 3 differ between individual and population levels. The difference at population level (across seeds) was unexplained. Within the same random seed, the plot shows that the connectivity $C$ sharply increases after the spectral entropy exceeds a threshold. However, across seeds, the meltdowns (sudden jump in $C$) occur earlier even when the spectral entropy is lower. Current text does not explain thos trend at the population level.\n5. The influence of PowerRemap strength $\\gamma$ on reconstruction connectivity is not studied. It remains unclear how $\\gamma$ should be selected in practice or whether larger / smaller values introduce any trade-offs in reconstruction quality besides connectivity.\n6. It is also unclear what data and how many data points and random seeds are used to localized the meltdown circuit in section 3.2.\n7. Why search only the cross-attention outputs? The decision to restrict the search space to cross-attention outputs is insufficiently justified. Since the cross-attention outputs will be written back to the residual stream, will you obtain similar results if patch residual stream activations? How much worse does the activation patching on MLP layer outputs compared to the cross-attention outputs."}, "questions": {"value": "1. What are the patching results for other components (MLP, self-attention, and residual stream) in the latent diffusion transformer?\n2. As mentioned in Weakness 5, it is unclear what data and how many samples were used to produce Figures 2 and 3. Do the observed patterns in these plots generalize when evaluated on more data points and random seeds?\n3. In Figure 3, the change in connectivity is sudden, suggesting meltdown is relatively binary phenomenon. However, the spectral entropy of the cross-attention head outputs varies smoothly. Does this imply that later MLP blocks might also contribute to (or mitigate) the meltdown failures?\n4. For the qualitative examples shown in Figure 4, what are the corresponding ground-truth 3D shapes of these four objects?\n5. According to Appendix B.3 (p. 20, l. 1060), the PowerRemap intervention was applied only to failure cases of the model‚Äôs generation. What happens if PowerRemap is applied to successful (non-meltdown) cases? Does it alter the output quality or connectivity in any noticeable way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GvmNtpEUdr", "forum": "QIb9LvN0Xp", "replyto": "QIb9LvN0Xp", "signatures": ["ICLR.cc/2026/Conference/Submission19744/Reviewer_PbKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19744/Reviewer_PbKr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956110314, "cdate": 1761956110314, "tmdate": 1762931575292, "mdate": 1762931575292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}