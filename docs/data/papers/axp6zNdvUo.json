{"id": "axp6zNdvUo", "number": 22489, "cdate": 1758331807503, "mdate": 1759896863292, "content": {"title": "How Long Do Model Patches Last? A Temporal Perspective on PortLLM", "abstract": "As large language models (LLMs) undergo regular updates through continual pretraining, the temporal reliability of downstream fine-tuning methods becomes increasingly important. Parameter-efficient methods, such as low-rank adaptation (LoRA), offer scalable solutions for task adaptations without requiring full LLM retraining. More recently, PortLLM has been proposed as a training-free patching mechanism that permits patch reuse over consecutive LLM releases. Although these training-free methods are appealing when full fine-tuning is impractical, their temporal reliability remains underexplored. Using PortLLM-style patches as a baseline approach, we conduct large-scale experiments and found that PortLLM patching exhibits a statistically significant performance decline over time, even when the task and neural architecture remain unchanged. Our findings reveal that patch performance degradation is a general and measurable risk when PortLLM is applied over an extended period. The statistical observation of the declining performance trends forms the foundation for our proposed forecasting algorithms, which estimate failure dates and test hypotheses about target-date performance failures. These forecasting algorithms rely on historical performance indicators without requiring downstream fine-tuning or access to original training data. Our framework enables downstream developers to anticipate failure and make informed decisions about when retraining is necessary, thereby supporting reliable and cost-effective LLM maintenance.", "tldr": "", "keywords": ["LLM", "Portability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e71a083ff8fe56f76164d56d6229c36aab243cd.pdf", "supplementary_material": "/attachment/d49ff7b3e46d6f6d6440abb2dec262ad140173cb.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the longevity and stability of model patches — small, targeted modifications applied to large language models (LLMs) to correct factual errors, adjust alignment, or steer preferences without full retraining. The authors propose an experimental framework for evaluating patch persistence under different settings: Patch Types, Tasks, and Metrics. The study highlights a crucial but underexplored aspect of LLM maintenance: how long model edits or behavioral fixes truly last."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important yet under-discussed topic in LLM reliability — the temporal persistence of behavioral interventions, which is highly relevant to real-world LLM maintenance, safety, and continual learning research and very novel. \n  - The authors examine multiple patch types and task categories systematically. \n  - The observed rapid decay and patch interference patterns are convincing and empirically grounded.\n Figures (especially Fig. 5–7) effectively demonstrate non-linear patch degradation trends."}, "weaknesses": {"value": "- The chosen tasks (mainly factual correction and alignment) are useful but narrow. No reasoning, multi-hop, or tool-use scenarios are explored, which might reveal more complex patch dynamics.\n  - Limited comparison to recent model editing robustness papers such as MEND[1], MEMIT[2], and ROME[3].\n\n[1] Fast Model Editing at Scale\n\n[2] Mass-Editing Memory in a Transformer.\n\n[3] Locating and Editing Factual Associations in GPT."}, "questions": {"value": "- How do you define “decay” quantitatively? Is it based on performance drop relative to baseline, or absolute change in logits / accuracy?\n  - Have you tested patch persistence across different model scales (e.g., 7B → 13B → 70B)?\n  - Did you observe any cases where later fine-tuning strengthened rather than degraded patch effects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "clyjemhdL9", "forum": "axp6zNdvUo", "replyto": "axp6zNdvUo", "signatures": ["ICLR.cc/2026/Conference/Submission22489/Reviewer_fxQe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22489/Reviewer_fxQe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462174842, "cdate": 1761462174842, "tmdate": 1762942238471, "mdate": 1762942238471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PortLLM is introduced as a training-free patching mechanism that enables patch reuse across consecutive LLM releases. \nThis paper conducts large-scale experiments showing that PortLLM patches experience performance decline over time.\nThe results demonstrate that performance degradation is a general and measurable risk when PortLLM is used over extended periods. \nTo address this, the authors propose forecasting algorithms that estimate patch failure dates and test hypotheses about performance at future target points. \nTheir framework allows downstream developers to anticipate degradation and make informed decisions about when retraining or re-patching becomes necessary."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Important topic of studying when patching fails\n- Clean design that uses historical evals, with precision/AUC analyses and concrete usage guidance\n- Multi-checkpoint experiments consistently show measurable decay, motivating the need for forecasting rather than blind refresh schedules"}, "weaknesses": {"value": "- The whole analysis relies on the PortLLM paradigm. It is unclear how broadly the results transfer to other patching approaches.\n- The entire evolution study uses UpVoteWeb slices, and Appendix G argues that alternative slices don’t change the trend, but the single dataset study on UpVoteWeb may still not reflect vendor training distributions, evaluation policies, or update magnitudes.\n- Linear model assumption fits PortLLM in this setup, but does it generalize beyond the linear setting?\n- Forecasts are derived from a small number of checkpoints, which constrains the ability to model longer-term dynamics. It is unclear how well the estimators predict beyond the observed window."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b5IubEq7rH", "forum": "axp6zNdvUo", "replyto": "axp6zNdvUo", "signatures": ["ICLR.cc/2026/Conference/Submission22489/Reviewer_S9hM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22489/Reviewer_S9hM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482619473, "cdate": 1761482619473, "tmdate": 1762942238193, "mdate": 1762942238193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the temporal stability of PortLLM-style “training-free” patches: a patch trained once at ($t_0$) is applied to successive, continuously-pretrained base model checkpoints (${\\theta_t}$). It (i) builds a longitudinal evaluation pipeline to measure performance drift, (ii) shows that linear/exponential trends fit the drift better than a no-trend baseline, and (iii) proposes two lightweight, data-free decision tools—failure-time estimation and a target-date hypothesis test—to decide when re-training is needed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear problem framing with strong practical motivation (well-scoped RQs).\n\nThe paper pinpoints a real deployment frication: upstream checkpoints arrive frequently; downstream teams want to reuse a one-time patch without constantly re-tuning. The two research questions (“when will performance fall below tolerance?” and “will it still meet tolerance on a business-critical date?”) are decision-oriented, tie directly to capacity planning, and are answered with tools that require only historical eval metrics (no access to upstream data or re-training). This tight alignment between phenomenon, RQs, and actionable outputs is a genuine plus for applied research.\n\n2. A reusable longitudinal evaluation pipeline.\n\nThe setup—fixed patch from (t_0), successive base checkpoints under continual pretraining, uniform eval protocol, repeated runs—creates a controlled environment to isolate “patch–base misalignment over time.” The pipeline surfaces trend shape (linear vs. log-linear), supports repeated-measure statistics, and includes sensible ablations (segment granularity, token density, multiple samples per time step). This is a useful template others can adopt to stress-test patch transfer under temporal drift."}, "weaknesses": {"value": "1. External validity: single model lineage.\n\nThe main longitudinal results focus on one family (e.g., 7B-scale within a single architecture). Without cross-architecture and cross-scale validation (e.g., Llama/Gemma/MoE; 7B→13B→70B), it remains unclear whether the observed drift rates and the proposed decision tools generalize beyond this lineage.\n\n2. Upstream evolution is approximated with LoRA rather than full-parameter or heterogeneous vendor updates.\n\nModeling base-model evolution as LoRA updates on attentions/FFN risks baking in “low-rank additivity” assumptions that may not hold when vendors do full-parameter continual pretraining, change training recipes, or alter architectures. This gap limits confidence that the measured drift and the tool calibration transfer to real release cycles.\n\n3. Temporal axis is short and partially synthetic.\n\nThe five-month, evenly segmented timeline (two-week, equal-token slices) is convenient but not representative of actual release cadence (e.g., quarterly “big” releases plus intermittent patches). The auxiliary “composed” time series that stitches disparate corpora further departs from realistic evolution. Conclusions about trend linearity and predictability could change under longer, lumpier, or recipe-shifting timelines."}, "questions": {"value": "1.Task coverage and metrics.\n\nCan you expand to production-relevant workloads (code, tool-use, long-context, safety), and replace/augment BLEU for math with verifiable-correctness metrics (programmatic verifiers, unit tests, GSM-style exact correctness)? This would test whether drift rates and the decision tools remain calibrated on harder-to-game metrics.\n\n2.Mechanism: quantify “patch–base misalignment.”\n\nBeyond the qualitative explanation, can you measure representational/optimization drift—e.g., CKA similarity across layers, low-rank subspace angles for patched vs. re-tuned adapters, curvature/Fisher changes, or gradient alignment—to link measurable misalignment to observed performance decay? This would strengthen causal plausibility and might reveal layers/subspaces where patches are most robust."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qpkW8l9Rq0", "forum": "axp6zNdvUo", "replyto": "axp6zNdvUo", "signatures": ["ICLR.cc/2026/Conference/Submission22489/Reviewer_AHDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22489/Reviewer_AHDH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703695823, "cdate": 1761703695823, "tmdate": 1762942237979, "mdate": 1762942237979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Frequent updates to base LLMs lead to significant retraining costs for developers who adapt these LLMs to downstream tasks. To address this issue, PortLLM (Khan et al., 2025) proposed a data- and training-free patching method for portability of patches across temporally evolved LLMs. This paper focuses on this particular approach and studies its long-term effectiveness. In particular, the main goal here is to be able to answer this question: How long can PortLLM patching remain effective as the base model evolves?\n\nTo answer this, authors perform statistical analysis of patching performance trends across model updates, and develop a time\nseries modeling framework that characterizes patching performance as a structured temporal process. Based on this modeling framework, authors provide lightweight algorithms to determine when or whether to retrain, without requiring retraining at every base model release.\n\nExperiments were conducted using Mistral-7B model, UpVoteWeb as the continual pretraining corpus and four downstream evaluation datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper focuses on a practical problem.\n\nThe main claims of the paper and the experimental analysis are well presented."}, "weaknesses": {"value": "**Unrealistic experimental setup**\n* This work focuses mainly on base LLMs that are evolved via continual pretraining. However, in practice LLM training involves multiple stage/phases (pre/mid/post, SFT/Preference alignment/RL) and practitioners often use post-trained models in real world applications due to their superior instruction following capabilities and human aligned behaviors. Also, for most of the models out there today, the difference between various model versions is rarely as simple as continual pretraining.\n* In real world settings, model developers work to make sure that the future model releases are better than earlier model releases. Indeed, in practice, most models get better (on a wide variety of tasks) with successive model releases over time. The continual pretraining setup used this paper (continually pretraining on a small Redditt corpus) is unrealistic as it is not reflecting the realistic setup of base models that improve with time. If we look at Fig. 2 and Fig 27, the base models are getting significantly worse on BoolQ, WinoGrande, ARC-Easy with time and barely improves on MathGenie. Model developers would rarely release such continually degrading models to downstream developers.\n\n**Conclusions that do not extend beyond a particular pretraining corpus**\nAll the behaviors observed in this paper are strongly tied to the small pretraining corpus used. Continually pretraining on UpVoteWeb makes the base model consistently worse over time which results in the observed (linear/exponential) trends in performance decline. If we use a different corpus (Fig 28 in Appendix), such nice predictable trends may not exist.\n\n\n**Fairly obvious conclusion**\n* In my view, there is nothing surprising or significant in the following statement: \"If a model goes through several updates over time, a downstream task patch obtained with first checkpoint will start failing.\" This paper claims that showing this experimentally as one of their main contributions."}, "questions": {"value": "Authors should use more realistic experimental setup where continual training is not making the base model significantly worse."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gXSGkJFFDw", "forum": "axp6zNdvUo", "replyto": "axp6zNdvUo", "signatures": ["ICLR.cc/2026/Conference/Submission22489/Reviewer_jCAt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22489/Reviewer_jCAt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939669361, "cdate": 1761939669361, "tmdate": 1762942237765, "mdate": 1762942237765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}