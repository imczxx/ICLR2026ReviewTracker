{"id": "88RKxlFUNY", "number": 2911, "cdate": 1757300025795, "mdate": 1759898119595, "content": {"title": "AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild", "abstract": "Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes.  However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9\\% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.", "tldr": "", "keywords": ["UAV", "vision-language-action model", "robot learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ddf6ecc3afb5480a10a73a223d3be6490efa24a2.pdf", "supplementary_material": "/attachment/a3f732766c9be1749573a6dc7663c5866a3eea18.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes AutoFly, a vision-language-action (VLA) model for aerial vision-language navigation.  Without the need for detailed language instructions, AutoFly takes as input coarse-grained positional or directional guidance and RGB observations, and predict continuous-value velocity action control.  Specifically, AutoFly utilizes off-the-shelf monocular depth estimator to predict pseudo depth maps, followed by depth encoder, to facilitation 3D visual understanding.  In contrast to existing datasets that heavily depend on detailed instructions and unrealistic trajectory annotations, this paper proposes a new dataset based on AirSim, where (1) expert demonstrations are generated via RL, and (2) long-horizon trajectorizes are automatically segmented to distinguish avoidance and target-seeking phases.  Training with human-labeled trajectories, AutoFly demonstrates strong sim-to-real generalization while outperforming other VLA baselines in simulation by a large margin."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well written.  It clearly illustrates the limitations of prior args and the implementation details of the proposed method.\n2. This paper demonstrates the effectiveness of the proposed method with strong sim2real generalization.\n3. Aside from task success rate and path efficiency rate, this paper adopts an additional metric--collision rate, highlighting the importance of safety when deploying physical agents in the real world.\n4. This paper presents detailed ablation study, demonstrating the efficacy of each model component."}, "weaknesses": {"value": "1. This paper lacks the sim2real evaluation of previous simulation benchmark.  In the introduction, the authors state that \"These limitations manifest in two critical areas: ..., and (2) insufficient real-world data representation, creating a significant sim-to-real gap. To bridge this gap, we present a novel autonomous navigation dataset specifically designed to address each identified limitation\".  However, this paper does not showcase the sim2real gap of existing datasets in the experiments.  Without support from experimental results, I'd consider this as overclaimed statement.\n2. This paper lacks discussion of recent studies on training-free aerial navigation policies, which also do not required detailed instructions.  For example, See-Point-Fly [1] suggests that navigation tasks are inherent visual grounding tasks in static scenes. Since VLMs excel at visual grounding, one can directly use VLM models to generate 2D waypoints for 3D navigation, without the need for training navigation policies. I'd strongly encourage the authors to include discussions on these new perspectives.\n3. The newly introduced dataset only consider static scenes, while the real-world environments are often dynamic.  I'd highly encourage the authors to include discussion of static vs. dyanmic scenes in the limitation section.\n\n---\n\nReference\n\n[1] Hu, Chih Yao, et al. \"See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation.\" Conference on Robot Learning. PMLR, 2025."}, "questions": {"value": "1. What's the sim2real performance of AutoFly trained on existing navigation dataset?\n2. Can you discuss recent studies that reformulate navigation tasks as visual grounding tasks?\n3. Can you discuss the limitations of static scenes adopted in the proposed dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SkDuc6t98U", "forum": "88RKxlFUNY", "replyto": "88RKxlFUNY", "signatures": ["ICLR.cc/2026/Conference/Submission2911/Reviewer_hBdj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2911/Reviewer_hBdj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380054700, "cdate": 1761380054700, "tmdate": 1762916440152, "mdate": 1762916440152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AutoFly presents a vision-language-action model for autonomous UAV navigation using coarse directional guidance rather than detailed instructions. It employs a pseudo-depth encoder (Depth Anything V2) with Siamese MLP projectors for depth-visual alignment, achieving 47.9% simulation success rate (3.9% over OpenVLA) and 60% real-world success on a new 13K+ episode dataset. Strengths include strong problem motivation, comprehensive ablation studies, and real-world validation. Concerns: modest gains given the added complexity, a low obstacle encounter rate (10 avg vs. 83 in AerialVLN), suggesting easier scenarios; heavy simulation dependence (success drops 60%->10% with less sim data); and unclear depth-generation overhead affecting the 15 FPS inference rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The shift from detailed instruction-following VLN to autonomous navigation with coarse guidance addresses a real deployment gap.\n- Using monocular depth estimation (Depth Anything V2) instead of depth sensors is elegant and practical. It avoids sim-to-real depth sensor gaps while adding spatial reasoning capabilities with only RGB cameras.\n- The shared-weight design for depth and visual token projection is simple yet effective, enforcing consistent cross-modal representations."}, "weaknesses": {"value": "- The 3.9% improvement in success rate over OpenVLA (47.9% vs 44%) is modest, given the additional pseudo-depth encoder, depth generator, and specialized projectors. The paper lacks analysis to show whether simpler approaches (e.g., better vision encoders or data augmentation) could achieve similar gains without architectural changes.\n- The average obstacle encounter rate of 10 is significantly lower than comparable UAV datasets (AerialVLN: 83, OpenUAV: 104, CityNav: 26). This contradicts the paper's framing of \"autonomous navigation in the wild\" and \"dense, obstacle-rich environments.\"\n- Only 200 real-world trials in \"reconstructed real environments\" (suggesting indoor lab settings) do not validate outdoor autonomous navigation. The dramatic performance drop from 60% (10K sim + 1K real) to 10% (0K sim + 1K real) reveals poor sim-to-real transfer and heavy dependence on simulation data.\n- The paper only compares against manipulation-focused VLA models (RT-1, RT-2, OpenVLA), which are not designed for UAV navigation. There are no comparisons to established autonomous UAV navigation methods, classical planning algorithms combined with learned perception, or recent outdoor navigation works (e.g., the cited Zhou et al. 2021a,b; Xu et al. 2022, and not cited SPF [A] and OpenFly [B]).\n\n[A] Hu, Chih Yao, et al. \"See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation.\" Conference on Robot Learning. PMLR, 2025.\n\n[B] Gao, Yunpeng, et al. \"OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation.\" arXiv preprint arXiv:2502.18041 (2025)."}, "questions": {"value": "- Can you provide ablation studies isolating the contribution of depth information from other factors (e.g., additional training data, different augmentations, and architectural choices)? What is the success rate when using the same dataset without the pseudo-depth encoder?\n- What is the inference time breakdown for each component (Depth Anything V2, visual encoder, depth projector, LLM)? How does the 15 FPS compare to OpenVLA on the same hardware? Is real-time performance achievable without the extensive optimization pipeline (TensorRT, ONNX conversion, model parallelism)?\n- Why is your average obstacle encounter rate (10) so much lower than comparable UAV datasets? Does this metric measure total obstacles in the environment or actual collision-risk encounters during flight? Can you provide comparisons of trajectory length and obstacle density between AerialVLN and CityNav?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SP3XPTsYSs", "forum": "88RKxlFUNY", "replyto": "88RKxlFUNY", "signatures": ["ICLR.cc/2026/Conference/Submission2911/Reviewer_Hzvn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2911/Reviewer_Hzvn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528691621, "cdate": 1761528691621, "tmdate": 1762916439899, "mdate": 1762916439899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoFly, an end-to-end Vision-Language-Action (VLA) model designed to enable Unmanned Aerial Vehicles (UAVs) to navigate autonomously in unknown environments using only coarse, high-level linguistic instructions. This approach addresses a key limitation of current Vision-Language Navigation (VLN) systems, which rely on detailed, step-by-step instructions and predetermined routes that are unavailable in real-world scenarios. AutoFly enhances its spatial reasoning by incorporating a pseudo-depth encoder that generates depth-aware features from standard RGB camera inputs. To support this autonomous paradigm, the authors also developed a new dataset focused on continuous obstacle avoidance and planning rather than simple instruction-following, which includes extensive real-world trajectories to bridge the simulation-to-reality gap."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper correctly identifies a significant limitation in existing UAV VLN research: an over-reliance on detailed, step-by-step instructions that are often unavailable in real-world, unknown environments. The proposed shift to a paradigm using only coarse directional guidance is a practical and valuable step toward more robust, autonomous agents that can operate with minimal human guidance.\n2. The authors recognize that existing datasets are ill-suited for this new, autonomous navigation task. A major strength of this work is the construction of a new, large-scale autonomous navigation dataset, comprising over 13000 trajectories. Crucially, the dataset also includes 1000 real-world flight episodes to help bridge the sim-to-real gap.\n3. The paper introduces a novel \"pseudo-depth encoder\" to derive depth-aware spatial features directly from monocular RGB inputs. This directly addresses the critical need for 3D geometric understanding in UAV navigation, a known limitation of RGB-only systems. The reason for using a pseudo-depth generator instead of idealized simulator depth or specialized hardware is well-argued, as it aims to improve sim-to-real transfer and reduce the payload and cost of the final UAV platform. This architectural choice is validated by an ablation study showing a 3.9% improvement in success rate."}, "weaknesses": {"value": "1. The title and abstract promise autonomous navigation \"in the wild\". However, the real-world experiments are limited in scope and do not support this claim.\nThe paper states real-world data is acquired \"within controlled laboratory environments\".\nThe visualization of the real-world test in Figure 5, Figure 15 clearly shows a structured, indoor lab setting, not a dynamic \"wild\" environment.\n2. The model's formulation defines the policy as taking only the current RGB observation $o_t$ as input, along with the language instruction. This makes the model fundamentally memoryless. It has no mechanism to build a persistent understanding of the environment, remember areas it has explored, or recall the location of obstacles it has passed. This is a severe limitation for any non-trivial navigation task, especially in \"wild\" environments where a target may be occluded, requiring the agent to remember its location from previous observations and go around obstacles.\n3. I think there is major unclarity in the core model architecture, particularly the vision encoder. The main methodology states the model is initialized with a \"prism-siglip-7b\" configuration and in section 3.2, “Action De‑tokenizer,” Eq. 1 shows the model indeed only uses the SigLIP vision encoder. However, the main results in Table 2 report a 47.9% success rate (SR) and the ablation study in the Appendix (Figure 11) shows that a \"SigLIP\" only encoder achieves a 46.6% SR , while a \"DINO-SigLIP\" fusion achieves the 47.9% SR, the same as main results. This implies the final AutoFly model does use a DINO-SigLIP fusion for its visual encoder. Moreover, in section 4.4 the Table 4 shows that removing the pseudo-depth encoder drops the SR from 47.9% to 44.0%. These 44.0% SR, 24.5% CR and 75.1% PER results are the same as the OpenVLA baseline which can mean that your model is the OpenVLA with additional depth encoder."}, "questions": {"value": "1. Your model's policy is formulated as $\\pi(a_t | o_t, L)$, conditioned only on the current RGB observation $o_t$. This makes the agent purely without memory. Why is no visual/action history modeled? How can this architecture handle non-trivial navigation where the target is temporarily occluded by an obstacle? \n2. Please try to address the model architecture concerns from the Weaknesses section. As mentioned, I think your paper's core architecture is contradictory. Please clarify the vision encoder used in the final AutoFly model: Eq. (1) shows the LLM consuming SigLIP visual embeddings (plus depth), whereas the best ablation uses a DINO–SigLIP fusion that matches your main 47.9% SR score.\n3. Table 4 shows that removing the pseudo-depth encoder yields the same numbers as your OpenVLA baseline in Table 2. Does your model without the pseudo-depth encoder reduce to OpenVLA?\n4. The AutoFly training process is note clear. It is mentioned that in Stage 2 you jointly fine-tuning the pseudo-depth encoder alongside the pre-trained VLA backbone. However, in training details of section 4.1 you didn't mention the learning rate of pseudo-depth generator, only pseudo-depth projector and VLM. Is the pseudo-depth generator (Depth Anything V2) frozen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XMBIWQqwSP", "forum": "88RKxlFUNY", "replyto": "88RKxlFUNY", "signatures": ["ICLR.cc/2026/Conference/Submission2911/Reviewer_ytdu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2911/Reviewer_ytdu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662313024, "cdate": 1761662313024, "tmdate": 1762916439735, "mdate": 1762916439735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel task and dataset that uses only coarse-grained instruction guidance to assist UAV navigation, and leverages  VLM and depth modal information to alleviate obstacle avoidance challenges in dynamic and unpredictable environments, achieving good performance in both simulation and real-world environments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The novel integration of a pseudo-depth encoder. This enhances the model's geometric reasoning for obstacle avoidance and safe navigation without needing physical depth sensors, effectively bridging a critical gap for real-world deployment where detailed environmental data is unavailable.\n2. The creation of a comprehensive autonomous navigation dataset. The dataset uniquely emphasizes real-world challenges like continuous obstacle avoidance and includes real flight data, facilitating robust sim-to-real transfer. \n3. This paper conducted sufficient comparison and ablation experiments to prove that its method achieved excellent performance in both simulation and real environments."}, "weaknesses": {"value": "1. VLM approach may cause computationally intensive over the more efficient reinforcement learning approach for obstacle avoidance and navigation. The authors should clarify why the VLM approach is irreplaceable over established RL methods when only obstacle avoidance and basic navigation are required.\n2. The Pseudo-Depth Encoder only showed a 4% performance improvement in ablation study, raising doubts about the module's effectiveness. Could you provide examples of extreme scenarios such as dynamic obstacles to illustrate the module's robustness?\n3. The initial position ${a_0}$ should be crucial for coarse navigation, but the method doesn't explain how it's used.\n4. The two-stage training paradigm doesn't clearly explain how each stage is trained (which data is used, which module is fine-tuned, etc.) and the hyper-parameters such as epochs and learning rate."}, "questions": {"value": "1. How is the L1-norm loss function described in Training Details used in the base language model?\n2. What causes the task success rate to be only 50%-60%? The experiment needs to analyse more failure cases to explain the rationality of the success metric settings and the effectiveness of the depth encoder.\n3. How does the instruction generated in the dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZOr1DBsog4", "forum": "88RKxlFUNY", "replyto": "88RKxlFUNY", "signatures": ["ICLR.cc/2026/Conference/Submission2911/Reviewer_nXwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2911/Reviewer_nXwK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714917571, "cdate": 1761714917571, "tmdate": 1762916439569, "mdate": 1762916439569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}