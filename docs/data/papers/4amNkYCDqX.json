{"id": "4amNkYCDqX", "number": 15267, "cdate": 1758249578622, "mdate": 1759897316988, "content": {"title": "Data-Centric Lessons To Improve Speech-Language Pretraining", "abstract": "Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs. We focus on three questions fundamental to speech-language pretraining data: (1) how to process raw web-crawled audio content for speech-text pretraining, (2) how to construct synthetic datasets to augment web-crawled data and (3) how to interleave (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a 3.8B-parameter SpeechLM, called SpeLangy, that outperforms models that are up to 3x larger by 10.2% absolute performance. We hope our findings highlight the impact of effective data curation and guide future data-centric exploration in SpeechLMs.", "tldr": "We study three data-centric methods to improve speech-language interleaved pretraining with a focus on spoken question-answering capabilities", "keywords": ["data curation", "speech language models", "pretraining", "synthetic data"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3aedea16c16c6926335bdeec0d0142ca5414a4a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic data-centric study on improving speech-language pretraining for spoken question answering (SQA). The authors focus on how data processing and curation affect performance, exploring three key questions: how to process raw web audio, how to construct synthetic speech-text data, and how to interleave modalities during training. Through controlled experiments, they show that fine-grained interleaving, synthetic datasets generated via LLM rewriting and text-to-speech synthesis, and deterministic speech-text sampling each yield performance gains. Integrating these insights, their 3.8B-parameter model SpeLangy surpasses models in SQA accuracy, underscoring that careful data design is crucial for advancing SpeechLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper raises a critical point about pretraining Speech-Language Models (SLMs): although prior works mention the importance of data quality, few have conducted controlled ablations to determine which data factors drive performance. This study fills that gap with systematic and well-designed analyses.\n2. The paper is clearly structured and easy to follow. Each data-centric question is introduced with clear motivation, supported by quantitative results that highlight the impact of each design choice.\n3. Building on the insights from the proposed data interventions, the authors present **SpeLangy**, a 3.8B SpeechLM that consistently outperforms much larger baselines. This demonstrates the practical significance and scalability of their data-centric approach."}, "weaknesses": {"value": "1. While SQA is a suitable benchmark for evaluating multimodal alignment, the scope of evaluation could be expanded. Including open-ended spoken reasoning or dialogue tasks would better demonstrate the generalizability of the proposed data curation strategies beyond structured QA.\n2. The empirical benefits of some interventions are relatively modest. For example, as shown in Table 2, gains from incorporating *Krist* are minor, and the difference between stochastic and deterministic modality sampling (Table 3) is small. A deeper analysis of why certain methods contribute less or under which conditions they are more effective would strengthen the overall conclusions."}, "questions": {"value": "1. Several supporting materials, such as **Figure 8** and **Table 11**, are placed far from the sections where they are first referenced. Could the authors consider moving or cross-referencing them more clearly to improve readability and flow?\n2. Have the authors examined whether the proposed data-centric methods (e.g., fine-grained interleaving, deterministic sampling) interact synergistically, or whether their gains are largely independent? Clarifying this would help readers understand which factors are most crucial for reproducing the improvements."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper leverages large-scale web-crawled data for model pretraining but does not specify how data collection complies with copyright, licensing, or web-crawling policies. Clarification on data provenance, consent, and compliance with legal and ethical standards (e.g., GDPR) is needed."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vfj7qe2UxD", "forum": "4amNkYCDqX", "replyto": "4amNkYCDqX", "signatures": ["ICLR.cc/2026/Conference/Submission15267/Reviewer_jiGx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15267/Reviewer_jiGx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806997126, "cdate": 1761806997126, "tmdate": 1762925568591, "mdate": 1762925568591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores three key issues regarding data construction and usage in building large speech models. The authors find that: 1) Fine-grained mixing of speech and text tokens helps improve cross-modal learning; 2) Training speech LLMs with synthesized data collected from websites is beneficial; 3) Deterministic sampling for mixing speech and text data yields better results than random sampling. Based on these findings, the authors trained a 3.8B parameter model and evaluated it on a spoken question answering task, achieving performance comparable to other larger open-source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The article is clearly structured and easy to follow. It provides an analysis of the construction details of speech data in large speech models, a topic that has not been extensively explored in existing works."}, "weaknesses": {"value": "1. Overall, this work remains largely analytical, and the answers to several questions are relatively straightforward. For instance, the conclusion that \"fine-grained interleaved tokens are better than coarse-grained\" is not particularly surprising, as mainstream approaches in speech-to-speech LLMs already employ word-level interleaved strategies, which are inherently fine-grained. Therefore, this finding does not significantly impact the development of large speech models, and its contribution is limited. This paper might be more suitable for workshops or conferences focused on data engineering.\n\n2. There is a lack of comparison between the proposed foundation model and other text-based models to determine whether its performance is competitive. Furthermore, the evaluation is limited to a spoken question-answering task, which is not comprehensive enough to be fully convincing.\n\n3. The experimental comparisons are not entirely fair. The proposed \"SpeLang\" model is designed for speech-to-text tasks, while the compared models (e.g., GLM-4-Voice, Kimi Audio) are primarily for speech-to-speech. For Qwen2 Audio, the model is designed for a broader range of inputs (speech, sound, and music). Therefore, the comparison is not conducted on a level playing field."}, "questions": {"value": "1. Since you are investigating a speech-to-text model, what is its performance on traditional tasks such as Automatic Speech Recognition (e.g., on LibriSpeech) and Speech Translation (e.g., on CoVoST)? How does this performance compare to other specialized models?\n\n2. In Line 292, what is the \"audio-loss-masked\" setting? This concept lacks a detailed description in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lTgENbKOZG", "forum": "4amNkYCDqX", "replyto": "4amNkYCDqX", "signatures": ["ICLR.cc/2026/Conference/Submission15267/Reviewer_QDUW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15267/Reviewer_QDUW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814276724, "cdate": 1761814276724, "tmdate": 1762925568227, "mdate": 1762925568227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts comprehensive empirical study in order to find optimal solutions for 3 design choices in speech-LLM pretraining:\n1. how to process web-crawled audio content for speech-text pretraining\n2. how to construct synthetic datasets to augment web-crawled data\n3. how to interleave speech and text modality for pretraining.\nThe optimization goal is to improve the performance on spoken QA(SQA) task while remaining the the similar text understanding capability after speech-text pretraining. From the empirical study the authors induct the answers for all three questions and deliver a scalable and effective speech-text pretraining recipe. With these insights, they pretrain a 3.8B-parameter SpeechLM, SpeLangy, that outperforms the larger model on SQA task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Quality:\n1. The paper presents a well-controlled ablation study, 1 example is the study on different granularity of chunking and interleaving, deterministic vs stochastic sampling schemes.\n2. The paper presents data-driven diagnostics and analysis, for example: modality alignment analysis by KL divergence; topic-coverage analysis; contamination checks with n-gram matching. Such analysis provides deep insights that go beyond the benchmark comparison.\n\nClarity:\nThe paper employs proper diagram to explain the steps of data pipeline, offers clear schematics for analysis, highlights key conclusions of each ablation study. The paper is clear and easy to follow.\n\nSignificance:\nThe ablation study on data processing and training schema brings sufficient value provides data-driven and practical insights on how to conduct effective speech-text pre-training."}, "weaknesses": {"value": "1. Task scope. This paper limits the evaluation target in Spoken QA (plus text understanding) while positioning the goal of the proposed method as optimizing speech-text pretraining. There could be doubt whether solely SQA is representative. The author need to somewhat prove that correlation between SQA performance and speech-text pretraining quality.\nAuthor discusses about this in Addendum K:\n> One caveat preventing us from a direct comparison on such tasks is that we do not employ any task-specific training, unlike other SpeechLMs that explicitly add in a task-specific component into their training mixture (e.g., ASR-specific training datasets).\n\n    However, the fact that the author evaluates on SQA task contradicts their own point.\n\n2. There's a lack of some significant details that would affect the reproducibility, please find more in Questions section.\n3. Baselines are missing from some of the ablation study, please find more in Questions section."}, "questions": {"value": "1. Please provide more details about the backbone LM, has it gone through post-training (SFT, RLHF, etc) or just pre-training? Does the LM support speech tokens natively? Or do you have to extend the vocabulary to support speech tokens?\n2. Please provide more details about the speech tokenizer. Does it support multi-lingual input?\n3. Please provide baseline in Table 2: the base LM performance on text understanding without any training.\n4. Isn't training on Quest dataset effectively SQA task-specific finetuning, when the training sample is organized and constructed in the same format as SQA task? Is it fair to compare SpeLangy with other \"base\" SpeechLLM on SQA task while other models are not instruction-tuned which gives some advantages to SpeLangy? To answer this question and the first point I made in the last section, I think the author needs to give a clear definition of *speech-text pretraining*. Starting from there, we might need to revisit the entire story.\n\nGiven these I am rating this paper 4 and would be happy to discuss more with the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tV9wnYJ8IF", "forum": "4amNkYCDqX", "replyto": "4amNkYCDqX", "signatures": ["ICLR.cc/2026/Conference/Submission15267/Reviewer_Yb68"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15267/Reviewer_Yb68"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862944226, "cdate": 1761862944226, "tmdate": 1762925567884, "mdate": 1762925567884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the design decisions applicable to text-audio interleaved pretraining. First, they study the granularity of interleaving that should occur given aligned text and audio and find that fine grained interleaving is superior. Second, they study the usage of synthetically aligned text-audio using TTS and existing text corpora and find the addition of synthetic data offers advantages due to topical biases in existing natural audio data. Finally, they study the method for sampling chunks of data given a possible interleaved sequence at a given chunk granularity. Finally, they combine their best decisions to adapt a text-backbone model into a competitive pretrained text-audio LLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work does well designed low-level ablations that lead to clear suggestions about the design space of Speech-Language Pretraining. This type of non-glamorous but important study seems extremely likely to be valuable to other practitioners in the space and enables the authors to train a strong model themselves!\n- The paper goes above and beyond most works of any form in terms of experimental rigor, including running contamination analysis.\n- The synthetic data study in addition to the domain analysis of speech Pretraining data offers valuable insights for speech training even beyond the specific modeling paradigm studied in the rest of the work, which feels likely to remain a useful finding for some time."}, "weaknesses": {"value": "- The work primarily focuses on evaluations in which the model must generate text, but does not evaluate how these decisions impact the models ability to generate speech in either S->S settings or in TTS usage.\n- The works evaluations of the whole system does not compare to the simplest baseline of pipelining the text-init with a common ASR system such as Whisper.\n- For a data centric work, the work doesn't actually provide much in the way of details of what the original source 10M hours of audio data is. This makes it inherently unreproducible."}, "questions": {"value": "- What is the source of the original 10M hours of raw audio?\n- How is inference operationalized given the interleaved pretraining? Is each chunk surrounded by a special token to indicate the modality which should be generated? Or is logit masking performed in order to generate only tokens of one modality at inference time?\n- Does training on synthetic data collapse the ability of the model to generate voices beyond those generated in the synthetic data? Does it have any negative impacts on held out validation data from the speech domain? \n- This model should in theory be able to generate speech right? If so, is there any reason why you didn't benchmark on tasks such as sBlimp?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oee7qSA28u", "forum": "4amNkYCDqX", "replyto": "4amNkYCDqX", "signatures": ["ICLR.cc/2026/Conference/Submission15267/Reviewer_zgtr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15267/Reviewer_zgtr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892218319, "cdate": 1761892218319, "tmdate": 1762925567451, "mdate": 1762925567451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}