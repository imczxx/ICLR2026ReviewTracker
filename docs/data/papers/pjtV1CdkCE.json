{"id": "pjtV1CdkCE", "number": 9462, "cdate": 1758123324723, "mdate": 1763178204877, "content": {"title": "CCRA: Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment", "abstract": "Vision-Language Models (VLMs) face challenges in effectively coordinating diverse cross-attention mechanisms for visual-language alignment, leading to attention drift and suboptimal performance. We propose Consistent Cross-layer Regional Alignment (CCRA), which introduces Layer-Patch-Wise Cross Attention (LPWCA) to capture fine-grained regional-semantic correlations by jointly weighting patch and layer-wise embedding. Also, we employ a novel Progressive Attention Integration (PAI) that systematically coordinates patch-layer-wise, layer-wise, and patch-wise attention mechanisms in sequence. This progressive design ensures consistency from semantic to regional levels while preventing attention drift and maximizing each attention's benefits. Experimental results on ten diverse vision-language benchmarks demonstrate that our CCRA-enhanced VLMs achieves state-of-the-art performance, outperforming all baseline methods with only 3.55M additional parameters, while providing enhanced interpretability through more regionally-focused and semantically-aligned attention patterns.", "tldr": "We propose a progressive cross-attention framework (CCRA) that improves vision-language alignment by integrating layer-wise and region-wise features for better performance and interpretability.", "keywords": ["VLM", "Cross-Attention", "Multimodal Alignment", "Layer-Patch-wise Attention", "Model Interpretability"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fa6bbcc591083295c5b53344f3fd1da98614a92.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses attention drift and suboptimal alignment in vision-language models (VLMs) caused by poorly coordinated cross-attention mechanisms. It proposes Consistent Cross-layer Regional Alignment (CCRA), which introduces Layer-Patch-Wise Cross Attention (LPWCA) to jointly model spatial and semantic importance, and Progressive Attention Integration (PAI) to sequentially apply LPWCA, Gaussian-smoothed Layer-Wise Cross Attention (LWCA), and Patch-Wise Cross Attention (PWCA)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces LPWCA as a joint layer-patch attention operator that captures fine-grained regional-semantic correlations, going beyond prior decoupled LWCA/PWCA designs.\n2. CCRA adds only 3.55M parameters, negligible for 7B-parameter VLMs, yet surpasses LLaVA-v1.5-13B, balancing performance and efficiency.\n3. Evaluates on eleven diverse benchmarks spanning OCR, reasoning, instruction-following, and perception, ensuring broad generalization claims. Extensive ablation and sensitivity studies confirm component necessity and stability."}, "weaknesses": {"value": "1. Lack of novelty. Although the paper introduces the Consistent Cross-layer Regional Alignment (CCRA) framework, its main component, Layer-Patch-Wise Cross Attention (LPWCA), is essentially a structural combination of the existing Layer-Wise Cross Attention (LWCA) and Patch-Wise Cross Attention (PWCA) modules. It does not introduce a new attention formulation, optimization objective, or learning paradigm.\n2. PAI’s sequence (LPWCA→LWCA→PWCA) is empirically chosen and there is no theoretical discussion of why this order ensures optimal convergence. The ablation study shows variants perform worse, but lacks analytical explanation of causality.\n3. Eq. (9) uses *w~l,l~* but *w~l~* is a vector of length L, which is likely a typo. And in Eq. (5), the dimensions of F~stack~ and W*~lp~* are different without broadcasting clarification."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kzO2MaDD6k", "forum": "pjtV1CdkCE", "replyto": "pjtV1CdkCE", "signatures": ["ICLR.cc/2026/Conference/Submission9462/Reviewer_XkdG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9462/Reviewer_XkdG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661123363, "cdate": 1761661123363, "tmdate": 1762921053508, "mdate": 1762921053508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key issue in Vision-Language Models (VLMs) termed attention drift, where patch-wise and layer-wise attention mechanisms may be misaligned, harming performance and interpretability. To address this, the authors propose the Consistent Cross-layer Regional Alignment (CCRA) framework. The core contribution is a three-stage Progressive Attention Integration (PAI) strategy that sequentially applies a novel Layer-Patch-Wise Cross Attention (LPWCA) module, followed by refined layer-wise and patch-wise attention modules. The method is implemented on the LLaVA-v1.5-7B model and evaluated across 11 diverse benchmarks, demonstrating improved performance over the baseline with only a marginal increase in parameter count."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem of attention inconsistency is significant and well-articulated. As VLMs become more complex, ensuring that the model is \"looking\" at the right regions at the appropriate semantic depth is crucial for both performance on complex reasoning tasks and for model interpretability. This work tackles a fundamental and relevant challenge in the field.\n\n2. The primary novelty lies in the Progressive Attention Integration (PAI) strategy. Rather than simply combining different attention mechanisms, the authors propose a structured, sequential process that aims to refine visual features from a joint representation to specific semantic and spatial levels. This is an intuitive and thoughtfully designed approach. The introduction of the LPWCA module as a way to initialize this process is also a creative combination of existing ideas.\n\n3. The paper is generally well-written, and the motivation is easy to follow. The proposed method is described in sufficient detail, and the figures and tables effectively illustrate the proposed architecture and support the main claims."}, "weaknesses": {"value": "1. The paper repeatedly highlights the low parameter overhead (3.55M) as a primary advantage, framing the method as \"efficient.\" This is an incomplete and potentially misleading claim. The PAI strategy is a sequential pipeline of three distinct attention modules. This architecture will almost certainly introduce non-trivial computational overhead and increase inference latency compared to the baseline model's single projection layer. For a method proposed as a practical enhancement, the absence of any analysis on inference speed (e.g., tokens/second, latency per image) or total FLOPs is a major omission. Without this information, it is impossible for the reader to assess the true cost-benefit trade-off of CCRA.\n\n2. The paper's claims of general applicability are not substantiated by the experiments, which are confined to a single model: LLaVA-v1.5-7B. The VLM landscape is diverse, with different visual encoders, LLM backbones, and vision-language connectors. It is unclear whether the performance gains are universally applicable or an artifact of LLaVA's specific architecture. Furthermore, the experiments do not explore the effect of model scale. Does CCRA provide the same relative benefit for a 13B model as it does for a 7B one? This lack of evidence severely limits the perceived impact of the proposed method.\n\n3. The results show that the degree of improvement varies significantly across different benchmarks. For instance, the gains on compositional reasoning (GQA) appear more substantial than on benchmarks like MM-Vet or MME, where the improvement is marginal. The paper reports these numbers but fails to provide any analysis as to why this might be the case. A strong paper would not only show that the method works, but also provide a hypothesis for how it works differently across tasks.\n\n4. The paper motivates its contribution by categorizing models like LLaVA as \"VLMs without vision-language alignment.\" This description is fundamentally inaccurate. Models like LLaVA are explicitly designed around a projection module (e.g., an MLP) whose sole purpose is to align the feature spaces of the visual encoder and the language model. This alignment is the central principle of their design. The authors should reframe their argument: the issue is not a lack of alignment, but that the existing alignment mechanism is implicit and insufficient, leading to the \"attention drift\" problem they identify. This mischaracterization weakens the paper's foundational argument."}, "questions": {"value": "1. Could you please provide details on the inference latency of the CCRA-enhanced model compared to the LLaVA-v1.5-7B baseline? A table showing throughput (e.g., images processed per second) or latency per generation would be critical for evaluating the method's practicality.\n\n2. While I understand that re-running experiments on many models is resource-intensive, could you elaborate on why you believe CCRA would generalize to other VLM architectures? Furthermore, how do you expect the effectiveness of CCRA to change with model scale? Do you hypothesize the attention drift problem becomes more or less severe in larger models?\n\n3. Can you provide an analysis or a hypothesis on why CCRA is more impactful for certain types of tasks (e.g., compositional reasoning) compared to others (e.g., general VQA)? Does your method's sequential alignment process naturally lend itself better to tasks that require a hierarchy of visual understanding?\n\n4. Leading contemporary VLMs, such as Qwen-VL and InternVL, have achieved state-of-the-art performance often using relatively simple vision-language connectors (e.g., a simple MLP). This success seems to suggest that highly complex, multi-stage alignment modules like PAI may not be strictly necessary. Could you comment on this? What specific failure modes in these simpler architectures does CCRA address that justifies its added complexity?\n\n5. In the LWCA module, you mention using Gaussian smoothing. How sensitive is the model's performance to the choice of the kernel size k for this smoothing? Was this a crucial hyperparameter to tune?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cexdBLAECn", "forum": "pjtV1CdkCE", "replyto": "pjtV1CdkCE", "signatures": ["ICLR.cc/2026/Conference/Submission9462/Reviewer_bDBa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9462/Reviewer_bDBa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750779155, "cdate": 1761750779155, "tmdate": 1762921053182, "mdate": 1762921053182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Consistent Cross-layer Regional Alignment (CCRA), a progressive cross-attention framework designed to enhance vision-language alignment in Vision-Language Models (VLMs). By integrating layer-wise semantic features and region-wise spatial features through two core components—Layer-Patch-Wise Cross Attention (LPWCA) and Progressive Attention Integration (PAI), the framework aims to improve model performance across diverse vision-language tasks while boosting attention interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Integrating layer-wise and region-wise features for better feature extraction.\n2. Good writing, easy to follow."}, "weaknesses": {"value": "1. The proposed Layer-Patch-Wise Cross Attention (LPWCA) essentially combines existing patch-wise cross attention (PWCA) and layer-wise cross attention (LWCA) by jointly weighting patch and layer embeddings—this design follows a \"combination of existing components\" paradigm rather than introducing a fundamentally new attention mechanism. Additionally, the subsequent experiment on cross-attention coordination strategies (e.g., comparing PAI with decoupled/shuffled integration) are incremental improvements over existing coordination logic, lacking breakthrough innovations.\n\n2. Compared with existing methods, the performance advantage of the proposed method is not significant. For a framework targeting \"optimized vision-language consistency,\" these gains are not substantial enough to fully demonstrate its superiority in solving attention drift or alignment issues compared to state-of-the-art methods.\n\n3. The base model (Vicuna-v1.5-7B) and comparative baselines (e.g., Qwen-VL-Chat, LLaVA-v1.5-13B) are relatively old. Recent advances in VLMs (e.g., Qwen3-VL, LLaVA-NeXT) and more competitive baselines (e.g., SAILViT-enhanced VLMs, SEAL) are not included. Using outdated models limits the ability to validate CCRA’s generalizability and competitiveness in current VLM research."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6GmwOTRVg", "forum": "pjtV1CdkCE", "replyto": "pjtV1CdkCE", "signatures": ["ICLR.cc/2026/Conference/Submission9462/Reviewer_z7Cf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9462/Reviewer_z7Cf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970158429, "cdate": 1761970158429, "tmdate": 1762921052674, "mdate": 1762921052674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to improve the performance of vision-language models. It proposes a Consistent Cross-layer Regional Alignment (CCRA) framework to enhance alignment between visual and textual features. CCRA combines Layer–Patch-Wise Cross Attention and Progressive Attention Integration to ensure semantic and regional consistency. Experiments show that CCRA improves performance with minimal additional parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed modules are conceptually simple yet effective, directly corresponding to the identified challenges in cross-layer and regional alignment. Moreover, their lightweight and modular nature allows for easy integration into existing vision–language architectures.\n\n- The ablation experiments are extensive and carefully designed to isolate the contributions of LPWCA. For example, Table 4 shows that removing the LPWCA module leads to a notable drop in performance. Similarly, Table 3 shows that omitting the PAI module or altering its submodule order results in degraded performance, confirming that the proposed progressive integration strategy is both necessary and well-justified."}, "weaknesses": {"value": "- The high-level design rationale of PAI is unclear. The paper does not sufficiently explain why the serial order of LPWCA → LWCA → PWCA is chosen, or why this progressive integration is expected to be better than alternative arrangements. The justification is largely empirical, relying on ablation results, without a clear conceptual motivation for the module sequence.\n\n- The authors should provide more attention heatmap visualizations. Only a few examples are shown in the main text, which is insufficient to fully demonstrate the advantages of CCRA in guiding attention.\n\n- The abstract is hard to follow. It jumps abruptly from VLM challenges to technical details of LPWCA and PAI without sufficient context or motivation, and long, dense sentences reduce clarity. Briefly motivating the problem and linking each module to its purpose would improve readability.\n\nOverall, the paper is engineering-focused and effective, but lacks deeper analysis and theoretical insight."}, "questions": {"value": "Please address my concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "psEA5IiHY5", "forum": "pjtV1CdkCE", "replyto": "pjtV1CdkCE", "signatures": ["ICLR.cc/2026/Conference/Submission9462/Reviewer_HzMb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9462/Reviewer_HzMb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983163713, "cdate": 1761983163713, "tmdate": 1762921052267, "mdate": 1762921052267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}