{"id": "ktGmDGoWnB", "number": 11604, "cdate": 1758202379821, "mdate": 1763588865185, "content": {"title": "Obfuscated Activations Bypass LLM Latent-Space Defenses", "abstract": "_Latent-space_ monitoring techniques have shown promise as defenses against LLM attacks. These defenses act as scanners to detect harmful activations before they lead to undesirable actions. This prompts the question: can models execute harmful behavior _via inconspicuous latent states_? Here, we study such _obfuscated activations_. Our results are nuanced. We show that state-of-the-art latent-space defenses---such as activation probes and latent OOD detection---are vulnerable to obfuscated activations. For example, against probes trained to classify harmfulness, our obfuscation attacks can reduce monitor recall from 100% down to 0% while still achieving a 90% jailbreaking success rate. However, we also find that certain probe architectures are more robust than others, and we discover the existence of an _obfuscation tax_: on a complex task (writing SQL code), evading monitors reduces model performance.\nTogether, our results demonstrate white-box monitors are not robust to adversarial attack, while also providing concrete suggestions to alleviate, but not completely fix, this weakness.", "tldr": "We demonstrate that LLMs can execute harmful behaviors using inconspicuous activations that subvert latent-space defenses.", "keywords": ["Interpretability", "Adversarial Attack", "Jailbreaking", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95d58be595dd2577a55a64006eb378a64e066c4d.pdf", "supplementary_material": "/attachment/fdfba966aa5b287d6fdbb2f94b55039a720b43f5.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies obfuscated activations, i.e., inputs that make a latent-space monitor score low even when the targeted behavior (e.g., harmfulness, SQL generation) is clearly present. It formalizes obfuscation attacks with a joint loss, evaluates both discrete “hard-prompt” and white-box embedding suffix attacks, and compares several monitoring families (linear/MLP probes, generation- vs input-based scoring, OOD detectors). Key results: (1) ordinary jailbreaks are typically detected at low FPRs; (2) targeted obfuscation attacks can collapse recall, while preserving the harmful behavior, with MLP generation probes most robust; (3) adversarially trained monitors remain bypassable after >70 alternating attack/defense epochs; and (4) a “performance–obfuscation trade-off” (obfuscation tax) appears on the non-saturated SQLEval benchmark when attackers try to hide SQL behavior across more layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear formalization of obfuscated activations and attack objective; broad, apples-to-apples comparisons across monitor families.\n\n- Strong empirical finding: linear probes are brittle; MLP generation probes fare best under realistic hard-prompt attacks.\n\n- Convincing attack–defense alternation showing lack of convergence even after 70 epochs."}, "weaknesses": {"value": "- Realistic attacks are optimized against only 20 harmful queries with a gradient-free optimizer; suffixes average ~256 tokens. This may understate the strength/transferability of discrete attacks. A stronger discrete search or more tasks could change conclusions.\n\n- Adversarial training setup shows persistent bypassability after >70 epochs, but lacks capacity/regularization/early-stopping ablations that might separate “non-convergence” from underfitting/overfitting cycles.\n\n- The negative correlation vs. # obfuscated layers is convincing, but effect sizes and baselines (e.g., random/semantic-preserving suffixes) are not fully disentangled. Broader non-SQL complex tasks would strengthen the claim."}, "questions": {"value": "- For adversarial training, how do results change with wider MLPs, ensembles, or stronger regularization?\n\n- What happens with larger harmful sets, different suffix budgets, or universal hard prompts (which you note were hard to make work)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bi7LvPHMYk", "forum": "ktGmDGoWnB", "replyto": "ktGmDGoWnB", "signatures": ["ICLR.cc/2026/Conference/Submission11604/Reviewer_3SvY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11604/Reviewer_3SvY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890359489, "cdate": 1761890359489, "tmdate": 1762922682410, "mdate": 1762922682410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New PDF"}, "comment": {"value": "We would like to thank the reviewers for their detailed and thoughtful feedback! We have made a number of edits to the new version of the paper in response, which we believe has increased the paper quality. Changes to the PDF are in red so they are easier to see.\n\nWe have also run a number of new experiments that the reviewers recommended. Most of the new results can be found together in Appendix K."}}, "id": "VCo9tqsR96", "forum": "ktGmDGoWnB", "replyto": "ktGmDGoWnB", "signatures": ["ICLR.cc/2026/Conference/Submission11604/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11604/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11604/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763589160328, "cdate": 1763589160328, "tmdate": 1763589160328, "mdate": 1763589160328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of latent-space monitoring defenses for LLMs against adversarial attacks. The authors introduce \"obfuscated activations\", adversarial activations that produce harmful behavior while evading latent-space monitors such as activation probes and OOD detectors. They demonstrate that state-of-the-art latent-space defenses can be bypassed, reducing monitor recall from 100% to 0% while maintaining over 90% jailbreak success. Furthermore, the paper also identifies an \"obfuscation tax\" where evading monitors degrades model performance on complex tasks, and finds that certain probe architectures are more robust than others."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive experimental evaluation across multiple defense mechanisms (supervised probes, OOD detectors, adversarially trained monitors)\n- Implications of discoveries are carefully explored and discussed\n- Important study of the effectiveness of different monitors providing insight for the community as to when/what monitors are effective"}, "weaknesses": {"value": "- Many of these discoveries seem unsurprising/not novel given existing literature in adversarial/robust ML especially given papers written in other domains\n- While the paper shows obfuscation tax on SQL code generation, the implications it draws from this limited experiment seem like too big of a stretch to be a contribution to me\n- Lacking details on additional cost of obfuscation attacks, how much harder does this make the problem in terms of attack runtime/convergence time etc.\n- The paper has many claims which feel unsubstantiated (more details in questions below). For example, the abstract suggests that the paper provides \"concrete suggestions to alleviate, but not completely fix, this weakness.\" But aside from showing which monitors are stronger I don't see many other suggestions in this area."}, "questions": {"value": "[73] This statement seems too strong to me without proof. Trustworthy ML research outside of LLMs has a number of certified training techniques stronger than adversarial robustness that can provide verifiably robust models (certified training etc). A very well trained human could likely act as a robust monitor, and it seems hard to me to prove that a future monitor could not possibly mimic this behaviour. Since adversarial/robust ML is inherently a cat and mouse game, I would be more comfortable with softening this statement to current SOTA monitors. \n\n[178] Isn't the core question answered in question (1) answered by previous works on latent-space monitors? Can you explain a bit more in detail in this section how what you are investigating fundamentally differs from previous works which propose monitors that defend against standard jailbreaking methods?\n\n[Figure 3] Since there are a number of probes that do not seem to transfer to each other, could a combination of probes be used to defend against the obfuscation techniques presented in this paper? I see that this is mentioned in 284 but can you elaborate more? Doesn't this provide an almost trivial counter to this paper? Can you train an attack against multiple probes simultaneously?\n\n[261] Can you mention here more details about what is modified or point to where these details are mentioned in the paper?\n\n[305] The last sentence seems to contradict the claim made on line 73 and line 304 ('no monitor is robust' is not consistent with 'we cannot rule out running this process for much longer could result in a robust probe'). Furthermore, I do not fully see how the results suggest that adversarial training techniques do not lead to robust monitors. As far as I can tell at this point adversarial training has not been tried (there is a mention of using a robust Llama as the base model, but not adversarially training the probe). The graph of Figure 4 seems confusing to me as the models perfectly counter each other at each step, can you give a bit more details on how this plot is created? What techniques are used to train the defender against previous attacks? How long is an epoch? How long is the attacker given to find a new attack? It does not surprise me that there are multiple adversarial directions as this is a well known result from research into adversarial attacks pre LLMs but work in certified training and adversarial training can usually make finding these attacks significantly harder if not impossible. It is hard to tell from the limited details whether this is a surprising result or if it comes from a weak defense or insufficient adversarial training.\n\n[319] Can you comment on how the 'obfuscation tax' is different from many previous studies showing that adversarial robustness may or may not be at odds with accuracy? It seems to me that obfuscation tax is just another way of saying that adding conditions to the attacker makes attacking harder which does not seem surprising or new.\n\n[322] Can you cite StrongREJECT classifier here?\n\n[394] This seems like a slightly unfair experiment to me. The probe in this case should not be trained to just detect SQL but instead to detect unperformant SQL. In section 3, a probe was trained to find adversarial inputs and then the inputs were obfuscated to avoid this probe. The results in section 4 could also be interpreted as the probe being too broad and thus it becomes hard to create successful 'attacks'. Would this obfuscation tax still exist if the probe was trained only to find un performant SQL? In other words, it makes sense to me that if I was monitoring for any SQL it would be hard to avoid my monitoring while still generating performant SQL, but easier to avoid if I was only trying to monitor bad SQL.\n\n[458] While there may not be many works studying bypassing latent space defenses in the LLM domain, most work in adversarial/robust ML for the LLM domain closely mimics results from the image domain, can the authors comment why bypassing latent-space defenses might be a uniquely different challenge for LLMs vs models in image domains? In my view, traditional adversarial attacks show that model A is not robust, probes show that introducing monitor model B makes it so that attacks againstn only A are not effective, but works like the cited 'Athalye et al. 2018' show that attacking A + B easily avoids probes again. Is there something about this model that is different for LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zkYZsdrRfF", "forum": "ktGmDGoWnB", "replyto": "ktGmDGoWnB", "signatures": ["ICLR.cc/2026/Conference/Submission11604/Reviewer_26qK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11604/Reviewer_26qK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994580251, "cdate": 1761994580251, "tmdate": 1762922681848, "mdate": 1762922681848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new attack paradigm that makes models produce harmful behavior while keeping latent representations inconspicuous, thereby bypassing current activation-based monitors. The attacker jointly optimizes a behavior loss (task objective) and an obfuscation loss (which is directly targeted to fool detection objectives). Experiments span two attack surfaces (hard suffix vs. embedding suffix) and two tasks. (1) Detecting harmfulness, where the authors show that current jailbreaking attacks can be reliably detected by probes, and the proposed attack can be effective, transferable, and persistent towards defenses. This exposes a worst-case vulnerability. (2) Detecting SQL, where the authors show that the attacker will have to balance between obfuscation and model performance on such a complex task."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation is clear, and the paper clarifies concepts well.\n- The paper is technically solid with experiments across tasks, attacks, and defenses.\n- This attack is novel and is generalizable to current systems."}, "weaknesses": {"value": "- This attack is limited by the strong assumption of white-box access.\n- Some hyperparameters are not introduced: how many iterations does this attack need based on different model sizes/sequence lengths?\n- Benign performance is not reported; it’s unclear how stealthy this type of attack is on the output space. Also, for such jailbreaking attacks, not all successful attacks are equally harmful/misaligned. It would be nice to see whether the harmful output is closer to original semantics or not."}, "questions": {"value": "Current defenses are all layer-wise. Is it possible that, after this explicit obfuscation loss optimization, detecting the attack by aggregating layers or circuit-wise might be effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DVXZnNICQd", "forum": "ktGmDGoWnB", "replyto": "ktGmDGoWnB", "signatures": ["ICLR.cc/2026/Conference/Submission11604/Reviewer_5bUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11604/Reviewer_5bUL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998376009, "cdate": 1761998376009, "tmdate": 1762922681509, "mdate": 1762922681509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents obfuscation attacks on latent-space detection methdods, including worst-case continuous optimization attacks which perfectly evade all defenses in the case of jailbreaks, and attacks operating on discrete tokens which are of more limited effectiveness. The paper shows that adversarial training is ineffective at defending against the continuous embedding-based attacks. The authors show that even worst-case attacks are of limited effectiveness at simultaneously inducing complex behaviors (SQL generation) and obfuscating activations up to monitors aware of the injected task."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The authors show that existing latent space detection methods for jailbreaks and backdoored samples can be evaded with obfuscation attacks. The results suggest that models have multiple independent representations of a given harmful concept, posing difficulties for future latent space detection methods.\n- The paper is well written and the obfuscation attack formulation is clean and natural.\n- The discussion and the additional experiments presented are helpful, in particular the analysis of adversarial training. The results on multiple types of malicious behavior (jailbreaking, injection of a SQL question answering task, and backdoors) and the visualizations of tradeoffs between obfuscation and behavior and those of the effects of obfuscation are helpful."}, "weaknesses": {"value": "- The strongest results are obtained with optimization of raw embeddings and don't translate to real-world attacks; realistic attacks (the \"hard prompt\") setting were of significantly reduced effectiveness, in particular against MLP generation probes. Hence, it is unclear whether these defenses are indeed robust on distribution or whether the limited effectiveness of obfuscation attacks observed in this setting is due to the difficulty of discrete optimization.\n- The obfuscation attack formulation itself has limited novelty, and is similar to that used existing attacks on defenses with detection. However, this is not a major concern, I recognize that the empirical results are the main contribution.\n- The results on SQL consider monitors which are aware of the injected task (which differs from the target task of natural language question answering), a best-case scenario for the defender, and it is unclear whether the results generalize to injection of complex tasks in general."}, "questions": {"value": "The differences in effectiveness of worst-case attacks with respect to the target malicious task are interesting; I would appreciate additional discussion and analysis. In particular, is there any evidence that this generalizes to detection of complex injected tasks more generally, i.e. when the latent-space monitor is trained without knowledge of the specific injected task on a wide distribution of target tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "swx3E610Rt", "forum": "ktGmDGoWnB", "replyto": "ktGmDGoWnB", "signatures": ["ICLR.cc/2026/Conference/Submission11604/Reviewer_WqeC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11604/Reviewer_WqeC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762294081291, "cdate": 1762294081291, "tmdate": 1762922681043, "mdate": 1762922681043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}