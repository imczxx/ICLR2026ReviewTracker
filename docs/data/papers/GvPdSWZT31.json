{"id": "GvPdSWZT31", "number": 3509, "cdate": 1757453122495, "mdate": 1759898083917, "content": {"title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs", "abstract": "Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instruction by converting visual inputs to vision tokens. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs. While many algorithms have been proposed to reduce the number of vision tokens, most of them apply only unimodal information (i.e., vision/text) for pruning and ignore the inherent multimodal property of vision-language tasks. Moreover, it lacks a generic criterion that can be applied to different modalities. To mitigate this limitation, in this work, we propose to leverage both vision and text tokens to select informative vision tokens by the coverage criterion. We first formulate the subset selection problem as a maximum coverage problem. Afterwards, a subset of vision tokens is optimized to cover the text tokens and the original set of vision tokens, simultaneously. Finally, a VLM agent can be adopted to further improve the quality of text tokens to guide vision pruning. The proposed method MMTok is extensively evaluated on benchmark datasets with different VLMs. The comparison illustrates that vision and text information are complementary, and combining multimodal information can surpass the unimodal baseline with a clear margin. Moreover, under the maximum coverage criterion on the POPE dataset, our method achieves a 1.87× speedup while maintaining 98.7\\% of the original performance on LLaVA-NeXT-13B. Furthermore, with only four vision tokens, 87.7\\% of the original performance is still preserved on LLaVA-1.5-7B. These results highlight the effectiveness of coverage in token selection.", "tldr": "", "keywords": ["Efficient VLM Inference", "Multimodal Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40a2e8ebcf9e3d68c4bdf6eeb53f2102460aeea0.pdf", "supplementary_material": "/attachment/4d3483a8d04899092610d65cff9c08f43edb4d14.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel token pruning method for VLMs. The authors formulate token pruning as a maximum coverage submodular optimization problem: finding a subset of visual tokens that covers both the textual tokens (text-visual coverage) and the visual tokens themselves (visual-visual coverage). The authors also provide an optional agent (MMTok-Agent) that enriches text guidance by adding preliminary text output from a small VLM. Experimental results show that the proposed method achieves significant speedup while pruning most tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, with easy-to-understand figures and tables.\n\n2. The authors provide excellent theoretical support, supplemented by experiments. The design is well-designed.\n\n3. The experiments are solid, experimented on nine datasets and various virtual machine models, demonstrating robust generalization capabilities."}, "weaknesses": {"value": "1. The role of the agent is not very clear. From the experiments, it seems to bring only slightly improvements. Although the authors claim it does not affect inference efficiency, it still introduces additional computation, which somewhat contradicts the very movitation of token pruning.\n\n2. The authors provide solid theoretical justification and a novel selection methods; however, this method appears to be applicable only to the encoder side. Why not consider expand it to decoder? Please correct me if my understanding is mistake."}, "questions": {"value": "1. Unlike most approaches that select tokens at the decoder stage, what are the advantages of selecting tokens at the encoder stage? Why can't apply to both the encoder and decoder?\n\n2. Are there any datasets where performance drops significantly at higher pruning rates, while others remain relatively stable?\n\n3. To my limit knowledge, token pruning methods can ensure that overall performance does not drop significantly. However, they may introduce knowledge boundary drift, meaning that answers that were previously correct may now be incorrect, and vice versa. This possibility could have negative consequences in scenarios where critical questions must be answered correctly. If possible, could the authors conduct a simple analysis to see whether the proposed method leads to changes in individual case performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2B4o1epXtY", "forum": "GvPdSWZT31", "replyto": "GvPdSWZT31", "signatures": ["ICLR.cc/2026/Conference/Submission3509/Reviewer_EEd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3509/Reviewer_EEd7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394003630, "cdate": 1761394003630, "tmdate": 1762916771367, "mdate": 1762916771367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMTok, a multimodal token selection framework designed to enhance the inference efficiency of Vision-Language Models. The key idea is to leverage both vision and text modalities for vision token pruning, rather than relying on unimodal information as in prior work. The authors formulate the token selection as a maximum coverage problem, aiming to optimize a subset of vision tokens that best cover both text tokens and the original visual space. Additionally, a VLM agent is utilized to refine text representations, further improving pruning quality. Experimental results on multiple benchmark datasets and various VLM architectures demonstrate that MMTok effectively reduces redundancy and accelerates inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Formulating vision token pruning as a maximum coverage problem is both novel and insightful. This formulation naturally captures visual–textual interactions by optimizing vision tokens to jointly cover textual semantics and the original visual space, providing a solid mathematical foundation for multimodal token selection.\n\n2. The authors conduct impressive experiments on five models and demonstrate improvements over previous methods. Moreover, the ablation study is detailed, particularly in the efficiency analysis section.\n\n3. The visualizations presented in the Appendix are impressive and highly detailed."}, "weaknesses": {"value": "1. The proposed VLM agent offers limited contribution. As shown in Table 1, it yields at most a 0.2 improvement and even leads to performance degradation in some cases, while introducing additional time overhead.\n\n2. Compared with finetuned VisionZip, MMTok performs notably better under the 64-token setting. However, its advantage diminishes under the 128- and 192-token settings. I suggest evaluating MMTok on more challenging benchmarks, such as MMStar and MathVista, to better highlight its strengths."}, "questions": {"value": "1. Is there a **fundamental difference** between Coverage Maximization and maximizing Conditional Diversity for Token Pruning? This is important, as there are already works that perform token pruning based on differentiable diversity (e.g., [1]).\n\n[1] Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs, NeurIPS 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gVwWm6rpQu", "forum": "GvPdSWZT31", "replyto": "GvPdSWZT31", "signatures": ["ICLR.cc/2026/Conference/Submission3509/Reviewer_TSCU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3509/Reviewer_TSCU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549702676, "cdate": 1761549702676, "tmdate": 1762916771024, "mdate": 1762916771024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMTok, a training-free approach for vision token compression in VLMs. They formulate the multimodal token selection problem as a maximum coverage problem, define the coverage and select a subset of vision tokens to cover text tokens and original vision tokens via a greedy approach. Experimentally, they found MMTok achieve better performance compared to other visual token compression approaches on the basis of the LLaVA-Next and Qwen2.5-VL baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Reasonable idea.** Using cross-modal correlation for visual token compression is reasonable.\n\n2. **Solid experiments.** MMTok works with LLava-1.5, LLava-Next and Qwen2.5-VL baseline, especially with cutting-edge SOTA VLMs like Qwen2.5-VL. This demonstrates the robustness and effectiveness of this method.\n\n3. **Training-free.** MMTok is training-free and plug-and-play, easy to apply to multiple foundation VLMs."}, "weaknesses": {"value": "1. **Compare with resize baseline.** Experiment results in [1] show that simply resizing the raw image yields good performance and low latency. I am looking forward to the author comparing your approach with the simple resize approach and adding a discussion in the paper.\n\n2. **Work on multi-turn conversation.** MMTok relies on text instruction to prune vision tokens, therefore hard to apply with multi-turn conversation. The author should discuss this situation and propose some solutions.\n\n3. **About the MMTok-Agent.** As shown in Table 2, looks like the performance gain of MMTok-Agent is limited over MMTok, and tha author didn't show this result over Qwen2.5-VL in Table 3.\n\n4. **Runtime.** The author should show the runtime and flops for MMTok.\n\n[1] VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning"}, "questions": {"value": "1. **Performance over reasoning VLMs.** Reasoning VLMs also face the problem of token redundancy. I am curious if MMTok work with reasoning VLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "68rI1cNHSj", "forum": "GvPdSWZT31", "replyto": "GvPdSWZT31", "signatures": ["ICLR.cc/2026/Conference/Submission3509/Reviewer_FeoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3509/Reviewer_FeoR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643913978, "cdate": 1761643913978, "tmdate": 1762916770572, "mdate": 1762916770572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMTok, a training-free method for vision token selection in Vision-Language Models (VLMs) by formulating the problem as a multimodal maximum coverage problem.  The approach leverages both text and vision tokens to select a subset of vision tokens that maximally cover semantic information from both modalities.  The method is evaluated across multiple VLMs and benchmarks, showing strong performance in maintaining accuracy while reducing token counts and improving inference speed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe paper addresses an important and practical problem: improving inference efficiency of VLMs without fine-tuning. And the multimodal coverage formulation is well-motivated, combining both text-vision and vision-vision similarity in a principled manner.\n\n-\tExtensive experiments across multiple models (LLaVA-1.5, LLaVA-NeXT, Qwen) and datasets demonstrate the generality and effectiveness of the method.\n\n-\tThe method achieves impressive results, e.g., 1.87× speedup on LLaVA-NeXT-13B with 98.7% performance retention, and maintains strong performance even with very few tokens (e.g., 4 tokens on LLaVA-1.5-7B)."}, "weaknesses": {"value": "-\tMy main concern lies in the efficiency part. The authors only provide efficiency test on base version. The agent-based text enrichment is presented as optional and shows mixed results, but its computational overhead and when it is most beneficial are not thoroughly discussed.\n-\tWhat is the efficiency cost under the practical experimental settings? e.g. The experiments in Table 3 with Qwen2.5-VL.\n-\tHow does the time complexity of MMTok scale with the number of vision tokens, especially compared to the baselines?\n-\tThe visualization results are not fully convincing, which weaken the interpretability of the proposed approach.\n-\tThe performance drop is severe under some benchmarks, which yield questions about the availability among real-world scenario."}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uYNJYG8yXX", "forum": "GvPdSWZT31", "replyto": "GvPdSWZT31", "signatures": ["ICLR.cc/2026/Conference/Submission3509/Reviewer_8R44"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3509/Reviewer_8R44"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002486484, "cdate": 1762002486484, "tmdate": 1762916770343, "mdate": 1762916770343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}