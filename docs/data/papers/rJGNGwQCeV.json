{"id": "rJGNGwQCeV", "number": 7189, "cdate": 1758011012498, "mdate": 1759897867750, "content": {"title": "Towards Adapting Vision-Language Models for Semi-Supervised Domain Generalization", "abstract": "Semi-supervised Domain Generalization (SSDG) offers a cost-effective solution for generalizing models to unseen domains with limited labels. While existing SSDG methods, mainly built upon small-scale backbones, struggle to match fully supervised DG performance, large-scale vision-language models like CLIP have shown remarkable generalization through downstream fine-tuning. However, adapting these models to SSDG remains underexplored. In this paper, we identify a critical issue: existing popular fine-tuning methods suffer from under-utilizing unlabeled data in the semi-supervised learning frameworks, thereby overfitting the limited labeled data, leading to training collapse and generalization ability degradation. To address these challenges, we propose two novel components: (1) the De-False-Correlation Adapter (DFC-Adapter), which reduces false correlations to refine visual features and (2) Learnable Multi-granularity Text-guided Embedding Augmentation (LMTEA), which synthesizes semantic-aligned but domain-perturbed augmented visual embedding for consistency regularization through multi-granularity text guidance and learnable style encoding. We establish the first benchmark for CLIP fine-tuning methods in SSDG, conducting extensive experiments across six DG datasets and two ImageNet variants. Our results demonstrate that our method significantly outperforms existing CLIP fine-tuning approaches and achieves performance comparable to fully supervised DG methods in some cases.", "tldr": "", "keywords": ["domain generalization", "semi-supervised learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a27f175176155a027772dad261fcd9534909536b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses semi-supervised domain generalization (SSDG) using vision-language models (VLMs) such as CLIP, highlighting that existing fine-tuning methods (LoRA, VPT, etc.) underutilize unlabeled data, causing overfitting and training collapse. To overcome this, the authors introduce: DFC-Adapter and LMTEA. Experiments across multiple DG datasets (PACS, OfficeHome, VLCS, DomainNet, ImageNet variants) show better performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well organized and easy to follow.\n\n2. The experimental part is comprehensive, test on several DG dataset and ImgeNet."}, "weaknesses": {"value": "1. The motivation of the paper is not very clear to me. Comparing the proposed method only to LoRA may not be entirely fair — it would be more appropriate to compare against a semi-supervised baseline such as FixMatch. The key question is not merely whether unlabeled data are used, but how effectively they are utilized. How is this utilization quantified? For example, if a method applies consistency regularization to all unlabeled samples, does that imply 100% utilization? Clarifying this definition and its measurement would make the motivation and evaluation more convincing.\n\n2. In Tables 8 and 9, it appears that the experiments are conducted under a standard domain generalization (DG) setting rather than a semi-supervised one — is that correct? If so, a more meaningful comparison would be combinations such as CLIP-Adapter + LMTEA, DFC-Adapter + LADS, or TIP-Adapter + TEAM, to better isolate the contribution of each component.\n\n3. Following the above point, the experimental results suggest that the proposed approach performs well not only in semi-supervised settings but also in fully supervised and potentially even unsupervised DG contexts. This raises a concern about the motivation: what specifically motivated the design of the adapter and the augmentation components? Were they primarily introduced to enhance the utilization of unlabeled data, or to improve general fine-tuning performance of VLMs? The paper should clarify how these components are conceptually and practically tied to semi-supervised training, rather than general adaptation."}, "questions": {"value": "1. Could you please provide more details on how the pseudo-label utilization rate is calculated for different methods such as LP or VPT? Specifically, how is “utilization” defined and measured in practice, for instance, does it correspond to the proportion of unlabeled samples that contribute to gradient updates, or some other metric?\n\n2. Could you clarify how the spatial refinement bank and the semantic correlation bank function in practice? It would be helpful to understand their initialization strategy and whether they effectively act as a form of single-layer attention. Additionally, could you visualize or provide examples illustrating how these banks refine the features during training?\n\n3. Table 6 and 7 are the same as Table 8 and Table 9."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pSbYG6f9Ou", "forum": "rJGNGwQCeV", "replyto": "rJGNGwQCeV", "signatures": ["ICLR.cc/2026/Conference/Submission7189/Reviewer_Gx2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7189/Reviewer_Gx2X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868440271, "cdate": 1761868440271, "tmdate": 1762919344653, "mdate": 1762919344653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors discuss the underutilization of unlabeled data in semi-supervised domain generalization. To address, they propose DFC-Adapter to mitigate false correlations of visual features. Meanwhile, the LMTEA module generates domain-perturbed augmented visual embeddings to facilitate consistency regularization."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe DFC-Adapter and LMTEA modules enhance the model’s ability to effectively utilize unlabeled data.\n2.\tThe authors conduct evaluations on a diverse and comprehensive set of datasets."}, "weaknesses": {"value": "1.\tThe paper lacks rigor in several aspects, for example: \n+ Tables 6–9 contain duplicated content and inconsistent sizing.\n+ In Figure 3(b), the term L_diff lacks a formal mathematical definition.\n+ In Equation (5), the first term is missing the ITP component \n\n2.\tThe proposed approach is based on CLIP for Single-Source Domain Generalization (SSDG). Therefore, the experimental comparison should prioritize CLIP-based SSDG or few-shot DG methods to ensure fair evaluation and highlight the effectiveness of the proposed design. \n\n3.\tTable 1 shows that the proposed method yields only limited performance improvement, particularly on the large-scale DomainNet dataset.\n\n4.  Considering that on the ImageNet variants, ImageNet-A and ImageNet-R have smaller category spaces than the source domain ImageNet, it might be more appropriate to include an additional experiment on ImageNet-Sketch, which shares the same category space as the source domain."}, "questions": {"value": "As shown in Figure 4, the PL utilization rate of LoRA remains at zero throughout, which seems abnormal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0FfvBe2VXs", "forum": "rJGNGwQCeV", "replyto": "rJGNGwQCeV", "signatures": ["ICLR.cc/2026/Conference/Submission7189/Reviewer_LoRW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7189/Reviewer_LoRW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878032801, "cdate": 1761878032801, "tmdate": 1762919344077, "mdate": 1762919344077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose two components for utilizing unlabeled data during semi-supervised domain generalization training. Specifically, De-False-Correlation Adapter (DFC-Adapter) tries to reduce false correlations from domain-specific biases and pre-trained knowledge to refine visual features. And Learnable Multi-granularity Text-guided Embedding Augmentation (LMTEA) synthesizes domain-perturbed augmented visual embeddings for consistency regularization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper proposes two components, including DFC-Adapter and LMTEA, to adapt pre-trained VLMs to SSDG without overfitting. \n\n2.\tThe proposed methods improved the utilization rate and accuracy of unlabeled data a lot."}, "weaknesses": {"value": "1.\tThe details of Figure 5 are unclear. What do the different colors represent - domains or categories? Specifically, does each cluster correspond to samples from the same category across different domains, or samples from the same domain across different categories? \n\n2.\tThe manuscript requires more rigorous proofreading and editing. In particular:\n(a) Tables 6 and 8 appear to be identical, and Tables 7 and 9 are also identical.\n(b) Line 136 contains a typographical error: \"underexplored..\" (duplicate punctuation).\n\n3.\tGiven the strong pretrained knowledge of CLIP, it would be more convincing to include its zero-shot performance as a reference in the experimental comparison. For instance, on DomainNet with CLIP ViT-B/16, the reported zero-shot accuracy is around 57.8% [1,2], suggesting that the results in Table 2 may not be directly comparable.\n\n4.\tAdditionally, since the proposed method leverages LLMs to augment domain and attribute prompts, it would be appropriate to include zero-shot CLIP results with the same prompt augmentation as an additional baseline.\n\n5.\tThe paper states that Equation (6) holds \"based on the assumption that the predefined texts would be a well-matched pair for the image features.\" However, in Equation (8), the augmentation follows the same structure, but E(ta) may also not be well-matched to the target image features either. As a result, the augmented embeddings might not be optimal. \n\n[1] PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization.ICCV 23\n\n[2] UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models. NeurIPS 24"}, "questions": {"value": "See Weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KfNxGUxBGb", "forum": "rJGNGwQCeV", "replyto": "rJGNGwQCeV", "signatures": ["ICLR.cc/2026/Conference/Submission7189/Reviewer_JSyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7189/Reviewer_JSyp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883001730, "cdate": 1761883001730, "tmdate": 1762919343505, "mdate": 1762919343505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the Semi-Supervised Domain Generalization (SSDG) using large Vision-Language Models (VLMs) like CLIP. The authors highlight a key limitation in existing VLM fine-tuning methods, i.e., limited utilization of unlabeled data, leading to overfitting on limited labeled data and training collapse. As such, the authors propose a De-False-Correlation Adapter (DFC-Adapter) to refines visual features & reduce spurious correlations, and a Learnable Multi-granularity Text-guided Embedding Augmentation (LMTEA) that generates diverse, semantic-aligned feature augmentations. The method is integrated into a FixMatch SSL framework, evaluated on a self-established benchmark, and achieves consistent performance gain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Study the feature disentanglement in the context of foundation models.\n- Propose a new benchmark.\n- Qualitative visualization is provided."}, "weaknesses": {"value": "- My primary concern is the validity and relevance of the experimental setup. The proposed benchmark, while a contribution, is primarily a recombination of existing 2D recognition datasets. These tasks are relatively simple and may have been part of the model's pre-training data, raising doubts about whether the results demonstrate *real* generalization. Furthermore, the cited literature to justify this setup is outdated, with most references predating 2023. Thus, I am not sure whether the benchmark can connect to contemporary, real-world challenges.\n- The proposed method rely on external models for generating false correlation samples and style encodings, which introduces much complexity and computational overhead. Furthermore, the paper does not analyze the sensitivity of the results to the choice of these external models. In other words, as those images may have been used for many complicated training objective, why should we still consider evaluating them for image classification?\n- The performance gain highlighted in Table 1 is limited compared with the other SOTA methods. \n- Definition of domain-specific bias: A critical issue in domain generalization is to clear define the domain specific and domain agnostic component for the network training. Currently, the pipeline in effect first defines domain-specific elements and then disentangle them to get the domain-agnostic (generalizable) ones, which may not be intuitive and even introduce some noise hurting the effectiveness."}, "questions": {"value": "Please see the comments in weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U0MZHXKnCc", "forum": "rJGNGwQCeV", "replyto": "rJGNGwQCeV", "signatures": ["ICLR.cc/2026/Conference/Submission7189/Reviewer_UAE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7189/Reviewer_UAE2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966871634, "cdate": 1761966871634, "tmdate": 1762919343218, "mdate": 1762919343218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}