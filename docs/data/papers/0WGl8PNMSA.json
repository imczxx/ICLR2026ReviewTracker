{"id": "0WGl8PNMSA", "number": 9253, "cdate": 1758116379626, "mdate": 1759897735095, "content": {"title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval", "abstract": "With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose **Retro\\***, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance **scoring mechanism**, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro\\* also supports **test-time scaling** by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro\\*'s reasoning capabilities, we introduce a novel **reinforcement learning** algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro\\* outperforms existing document retrieval methods with notable advantages, leading to **state-of-the-art** performance on the BRIGHT benchmark.", "tldr": "", "keywords": ["Information Retrieval", "LLM Reasoning", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2060e3e5019c13a781a5d82bbd8f1b576107b1b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper developed Retro*, a reasoning-based re-ranking method to score the relevance of the documents for reasoning-intensive retrieval. By leveraging distillation from a teacher model, then followed by reinforcement learning with composite reward evaluation, Retro* learns to score documents using verbal reasoning from the two-stage training strategy, and works together with test-time scaling strategies (multiple rollouts). As a result, Retro scores significant performance improvement over other re-ranking baselines on BRIGHT reasoning-intensive retrieval benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written with clarity. \n2. Incorporating rubrics as part of the input and having both intra- and inter- document rewards for better learning are reasonable and original ideas. \n3. The empirical results are strong. The ablation studies show that the two-stage training is essential for performance improvements, with most of the gain coming from RL with composite reward. \n4. It’s interesting to see from the test-time scaling experiments that having an ensemble of scores from the same reranking model offers consistent performance improvement."}, "weaknesses": {"value": "1. While the rubrics are an important part of the contribution, having them in the model may restrict the generalizability to other reasoning-intensive tasks that are unseen. \n2. The paper mentioned that the training data is based on BGE-Reasoner-Data, but it is unclear to the reader how BGE-Reasoner-Data is curated. It’s helpful to clarify that in the paper."}, "questions": {"value": "1. The performance on Leetcode degrades significantly after reranking. I wonder if the authors have some understanding of why this is happening?\n2. Why test-time scaling by average can boost the performance by so much since the aggregation is average? I can understand that without averaging, there might be more randomness, but it is a bit surprising to see that this type of ensemble can lead to performance gain. \n3. Are there any potential data leakage in the experiments, especially when using the SFT model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2bSjpENwk5", "forum": "0WGl8PNMSA", "replyto": "0WGl8PNMSA", "signatures": ["ICLR.cc/2026/Conference/Submission9253/Reviewer_t3di"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9253/Reviewer_t3di"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974748034, "cdate": 1761974748034, "tmdate": 1762920904875, "mdate": 1762920904875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Retro* a rubric-based relevance scoring mechanism for reasoning-intensive retrieval. Retro* leverage a two-stage SFT+RL training with composite intra-/inter-document rewards. The method achieves strong nDCG@10 on BRIGHT and shows consistent gains from model scale and test-time sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Retro* uses rubric-based scoring methods, yields interpretable absolute scores rather than only relative ranks.\n2. Retro* propose intra/inter rewards for RL, and the paper ablates each components.\n3. The proposed methods achieve SOTA BRIGHT results."}, "weaknesses": {"value": "1. The model uses two types of rewards (intra- and inter-document), but it remains unclear which dominates training dynamics or contributes more to final gains in run-time.\n2. As rubric-guided scoring is claimed to enhance interpretability, more qualitative or case-based evidence would strengthen this claim.\n3. The reliance on a strong teacher (Qwen3-235B) for SFT trajectory generation raises reproducibility and cost concerns, as smaller or open models may not reproduce the same reward quality."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JJxVUH4A0y", "forum": "0WGl8PNMSA", "replyto": "0WGl8PNMSA", "signatures": ["ICLR.cc/2026/Conference/Submission9253/Reviewer_d5S7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9253/Reviewer_d5S7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016692726, "cdate": 1762016692726, "tmdate": 1762920904160, "mdate": 1762920904160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Retro*, a new method for training rerankers for reasoning intensive queries. Retro* is first trained with both SFT on traces from a large powerful model and then later trained with reinforcement learning. For the later stage, the authors design new reward functions specifically for improving reranking performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the paper are as follows:\n- The RL training rewards proposed in this paper are novel and are distinct from other baseline methods. \n- The parallelism and inference time analysis is interesting."}, "weaknesses": {"value": "The weaknesses of the paper are:\n- Lately, there has been a lot of debate whether reasoning is helpful for reranking. Is the reasoning trajectory actually necessary or can model be trained directly to just produce the score. The authors should ablate whether the reasoning trajectory are helpful and if so how helpful they are. \n- For the retro* results in table 1, the authors use first-stage candidates from BGE-Reasoner-Embed. What is the first stage retriever for the other baseline models in main table? If the first stage retriever is different then it is hard to compare numbers. \n- Along similar line, the authors should clarify that the baseline models are not build from the same backbone. For instance, RankLLaMA and RankZephyr are built from older llama models.\n- The performance gains seem limited when making comparisons in the same setting. For instance, ReasonIR+QwenRerank obtains a score of 36.9 on bright and this is with no additional reranker training (the qwen model instruct model is used as is). From table 2, we see that Retro* obtains a scores of 37.4 after SFT and RL training. \n- ReasonIR is mentioned in the paper but is not used as a baseline. I understand that ReasonIR is a retriever but the ReasonIR paper also presents numbers for ReasonIR+a zero-shot qwen based reranker.\n- Rubric-based relevance scoring is not novel. Table 10 of the Rank1 paper shows that data-specific prompts were used for BEIR and the non-stackoverflow BRIGHT subsets. ReasonIR did something similar as well. Also the name \"rubric-based scoring\" is a little mis-leading because it implied there are multiple rubric elements, but the rubric is just a definition of similarity for each task. \n- Minor issues: There are some typos in the paper. E.g. \"while positive documents are assigned low scores\", \"with zero RL achieves a higher score of 35.1.\""}, "questions": {"value": "Here are some questions:\n- I think there is a typo on line 214. There are four chosen trajectories and not just one, right?\n- For the only-SFT ablation how many training samples were used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6aMLs3sp8j", "forum": "0WGl8PNMSA", "replyto": "0WGl8PNMSA", "signatures": ["ICLR.cc/2026/Conference/Submission9253/Reviewer_9eA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9253/Reviewer_9eA5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131986327, "cdate": 1762131986327, "tmdate": 1762920903748, "mdate": 1762920903748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}