{"id": "jz41Oh9eV1", "number": 17218, "cdate": 1758273589469, "mdate": 1759897190287, "content": {"title": "Robust Preference Alignment via Directional Neighborhood Consensus", "abstract": "Aligning large language models with human preferences is critical for creating\nreliable and controllable AI systems. A human preference can be visualized as a\nhigh-dimentional vector where different directions represent trade-offs between\ndesired attributes (e.g., helpfulness vs. verbosity). Yet, because the training\ndata often reflects dominant, average preferences, LLMs tend to perform well on\ncommon requests but falls short in specific, individual needs. This mismatch creates\na preference coverage gap. Existing methods often address this through costly\nretraining, which may not be generalized to the full spectrum of diverse preferences.\nThis brittleness means that when a user’s request reflects a nuanced preference\ndeviating from the training data’s central tendency, model performance can degrade\nunpredictably. To address this challenge, we introduce Robust Preference Selection\n(RPS), a post-hoc, training-free method by leveraging directional neighborhood\nconsensus. Instead of forcing a model to generate a response from a single, highly\nspecific preference, RPS samples multiple responses from a local neighborhood of\nrelated preferences to create a superior candidate pool. It then selects the response\nthat best aligns with the user’s original intent. We provide a theoretical framework\nshowing our neighborhood generation strategy is provably superior to a strong\nbaseline that also samples multiple candidates. Comprehensive experiments across\nthree distinct alignment paradigms (DPA, DPO, and SFT) demonstrate that RPS\nconsistently improves robustness against this baseline, achieving win rates of up\nto 69% on challenging preferences from under-represented regions of the space\nwithout any model retraining. Our work presents a practical, theoretically-grounded\nsolution for enhancing the reliability of preference-aligned models1.", "tldr": "", "keywords": ["Large Language Models", "Preference Alignment", "Inference-Time Method"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cea4b3963df63c6fb6713b1e6c40acfc7ca2d30f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Having identified what they call a \"preference coverage gap\", where the targeted user preferences might differ significantly from choices made during the training of an LLM, the authors propose to handle it at inference time, introducing a \"Robust Preference Selection,\" or RPS. Instead of asking a model to generate for a target outside of its domain, they first sample a neighborhood of more familiar preferences ; then generate a response for each adapted vector ; and pick the best response according to the original target preferences.\nThe robustness gain of the RPS method is both presented formally and validated experimentally, as they compare the performances of 3 training paradigms (DPA, DPO and SFT) coupled with RPS on 3 datasets (UltraFeedback, HelpSteer and HelpSteer2). Interestingly the experiments confirm its soundness as target preferences go outside of the training distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- this is a pragmatic contribution as it shows a practical way to improve robustness at inference when generating from an LLM outside the preferences learned during its training ;\n- despite the relative simplicity of the approach, the conceptual link with Distributionally Robust Optimization is theoretically and philosophically interesting ;\n- the experimental results do illustrate the validity of the approach ;\n- the paper is well written and solid, both formally and experimentally."}, "weaknesses": {"value": "- the simplicity of the approach (see \"strengths\" above) diminishes the contribution which really boils down to: instead of asking the model to generate outside of its domain it's better to keep it closer to home _and_ then pick the response closer to what the user wanted ;\n- the verbosity vs helpfulness example used here is somewhat intuitive but it is not clear (to me?) how much it is a trade-off, so the single theta controlling both dimensions can be seen as problematic. In other words there could very well be a long, helpful answer ;\n- only the DPA model is trained to use correctly the dimensions in the prompt, the others (DPO, and SFT) might do their best but there's no calibration. Yes, it does seem to work here but would it apply to more complicated cases, than verbosity vs helpfulness?"}, "questions": {"value": "- can you comment on this artificial trade-off you see in verbosity vs helpfulness? And the single theta you use as a control?\n- couldn't we just generate more samples, and use a better scorer?\n- can you comment on the correlation of the weights used in the prompts and the semantic of the generated answers, especially for DPO and SFT ofc?\n- what about using human judges for something as difficult to assess, even in this simple (verbosity, helpfulness) case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p37nKLEcoZ", "forum": "jz41Oh9eV1", "replyto": "jz41Oh9eV1", "signatures": ["ICLR.cc/2026/Conference/Submission17218/Reviewer_Kpri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17218/Reviewer_Kpri"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760975433967, "cdate": 1760975433967, "tmdate": 1762927182188, "mdate": 1762927182188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To align user preferences at inference time, previous studies introduce a preference vector (e.g., helpfulness vs. verbosity) in the prompt to adjust the model’s behavior. However, these approaches often underperform outside their training domains, thus requiring extra training. In this work, the authors propose **Robust Preference Selection (RPS)**, a post-hoc method for improving preference alignment during inference. It samples a set of neighboring vectors from the target one and generates responses with each, then selects the optimal one using a reward model. Experimental results demonstrate that RPS attains a higher win rate compared to naive-sampling baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, the motivation is clear, and the teaser figures are intuitive and easy to follow.\n- The authors present theoretical evidence for their proposed method, showing the effectiveness of RPS under certain assumptions."}, "weaknesses": {"value": "- Assumption 1 appears rather idealized, and the paper provides limited empirical evidence to support it. Although the authors mention that Figure 5 offers some justification, a deeper analysis or additional experiments would help validate it.\n- The paper lacks ablation studies on the choices of $k$ and $\\theta_{\\max}$; without these, it is difficult to assess whether the method is sensitive to hyperparameters.\n- The assumption of a well-calibrated reward model that generalizes to out-of-distribution (OOD) data seems overly strong and may not hold in other domains."}, "questions": {"value": "- It seems that the authors utilize models from prior works. In this case, what is the training distribution of each model? How do the authors ensure that the testing range of $10^\\circ$ to $45^\\circ$ indeed includes out-of-distribution (OOD) cases?\n- The prompt for the LLM judge appears quite simple, raising doubts about whether the evaluation is truly robust in a zero-shot setting. Have the authors tested the robustness of the LLM judge by sampling multiple times?\n- How does the value $v_{\\text{target}}^{\\top} r(x, y)$ compare with the baseline? Does it outperform the baseline across all angles as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K1CtnxbfO9", "forum": "jz41Oh9eV1", "replyto": "jz41Oh9eV1", "signatures": ["ICLR.cc/2026/Conference/Submission17218/Reviewer_6hsD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17218/Reviewer_6hsD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640071545, "cdate": 1761640071545, "tmdate": 1762927181858, "mdate": 1762927181858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the preference coverage gap, a problem where LLMs aligned on dominant, average human preferences perform poorly on specific, out-of-distribution requests. To mitigate this brittleness without costly retraining, the work introduces Robust Preference Selection (RPS), a training-free, post-hoc adjustment method. The work identifies the preference coverage gap, as LLMs tend to perform well on common requests but falls short in specific, individual needs. RPS solve this by generating a candidate pool of responses from a local neighborhood of more \"in-distribution\" preference vectors, rather than directly from the out-of-distribution target preference. The final response is chosen from this pool by selecting the candidate that best aligns with the original target preference. The paper present both theoretical and empirical analysis for RPS framework, showing the validity and soundness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "First, the paper proposed a conceptually novel method, RPS with a clear motivation. Rather than attempting to force a model to directly generate a high-quality response for a difficult, out-of-distribution (OOD) preference, RPS reframes the problem. It hypothesizes that it is more effective to first sample from a neighborhood of related, but easier, preference vectors where the model is inherently more competent. This conceptual shift from direct, constrained generation to a \"generate-then-select\" paradigm and provides new insights into LLM alignment.\n\nSecond, the proposed solution is practical and (potentially) broadly applicable due to its post-hoc, training-free nature. Unlike various methods that require extensive retraining or fine-tuning to the model architecture, this work offers a lightweight option where it can be implemented at inference time on the pre-trained models. The simplicity of the algorithm, generating from slightly perturbed preference vectors and then re-ranking, ensures a low barrier for adoption, making it a highly valuable tool for practitioners seeking to improve model robustness in real-world applications.\n\nThird, the paper provides comprehensive empirical validation to support its claims. The authors go beyond a simple performance comparison by establishing a strong, compute-matched baseline. The consistent win rates of RPS against several baseline across a diverse set of models, including those trained with SFT, DPO, and DPA, demonstrate the method's generalizability. Crucially, the analysis that correlates the performance gain of RPS with the degree of OOD-ness of the preference provides compelling evidence for the paper's core hypothesis. This result shows that the method is effective precisely in the challenging scenarios it was designed to address. Furthermore, the paper gives relatively complete theoretical justification of using RPS for model alignment."}, "weaknesses": {"value": "First, the paper's theoretical claim of being \"provably superior\" rests on a critical yet unformalized logical gap, which will undermine its rigor. The entire argument of Theorem 1 hinges on the \"local consistency\" assumption, stated as v_target^T r(x, y_i) ≈ v_i^T r(x, y_i). This approximation is presented without any formal justification, error bounds, or discussion of the conditions under which it might hold. Consequently, the strong language of \"guarantee\" and \"proof\" is a mischaracterization; the theoretical contribution should be more accurately and detailed framed as a heuristic argument.\n\nSecond, the paper's exclusive reliance on a single automated metric, the judgment of GPT-4o-mini, introduces a potential confounder that is not adequately addressed. While LLM-as-judge is a common practice, it is known to have biases. Without a supporting human evaluation study or an analysis using multiple distinct judge models to check for consensus, it is hard to conclude that the observed win rates reflect a true improvement in response quality rather than an artifact of the specific evaluation protocol."}, "questions": {"value": "1.\tAre results robust to different judges (e.g., GPT-4o, open-source preference models)? Can a small human evaluation be included?\n2.\tThe theoretical argument hinges on the \"local consistency\" assumption. Can this be formalized? For example, under what conditions on the reward model r and the distance between v_i and v_target does this approximation hold with a bounded error? Without this, the claim of a proof seems difficult to justify.\n3.\tWhat happens in higher-dimensional preference spaces (≥3 attributes)? If there is any preliminary results?  \n4.\tIf it is possible to conduct an ablation study on hyperparameters such as k and θ_max?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0ByCKL8m8t", "forum": "jz41Oh9eV1", "replyto": "jz41Oh9eV1", "signatures": ["ICLR.cc/2026/Conference/Submission17218/Reviewer_9caP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17218/Reviewer_9caP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875539183, "cdate": 1761875539183, "tmdate": 1762927181395, "mdate": 1762927181395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Robust Preference Selection (RPS), a three-phase, training-free method designed to address the “preference coverage gap”, where language models fail to align with out-of-distribution user preferences. The authors provide a theoretical framework to justify this approach and present experiments under three preference-learning datasets across three alignment paradigms to demonstrate the superiority of RPS."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The preference selection method is training-free, making it applicable to models trained under different schemes.\n- Theoretical analysis shows that the expected score of the best response selected by RPS is greater than that of the best response selected by the baseline."}, "weaknesses": {"value": "- The method’s generalization to higher-dimensional preference spaces is not empirically validated.\n- RPS assumes that the reward model used in the Consensus Selection phase (Phase 3) is robust when evaluating responses against OOD targets ($v_{\\text{target}}$), potentially shifting the “brittleness” problem from generation to evaluation.\n- The theoretical foundation relies on two key assumptions: Assumption 1 and the local consistency assumption (L203). Assumption 1 requires the neighborhood vector $v_i$ to differ from the OOD target $v_{\\text{target}}$, whereas the local consistency assumption requires them to be sufficiently similar for the final evaluation score to be transferable.\n- The paper lacks user studies or alternative automated reward metrics that could demonstrate RPS’s superiority."}, "questions": {"value": "- Could the authors provide empirical evidence supporting the local consistency assumption, particularly under broader settings such as $\\theta_{\\max}=30^\\circ$?\n- Could the authors provide an ablation study using other reward models to demonstrate that the RPS framework is generalizable?\n- How does the proposed inference-time approach compare against strong training-time optimization baselines such as GRPO [1]?\n\n[1] Shao, Zhihong, et al. \"Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\" arXiv preprint arXiv:2402.03300 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YIry3mYe2k", "forum": "jz41Oh9eV1", "replyto": "jz41Oh9eV1", "signatures": ["ICLR.cc/2026/Conference/Submission17218/Reviewer_fz2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17218/Reviewer_fz2E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993618275, "cdate": 1761993618275, "tmdate": 1762927181119, "mdate": 1762927181119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}