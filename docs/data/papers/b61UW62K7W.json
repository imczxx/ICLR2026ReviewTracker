{"id": "b61UW62K7W", "number": 9240, "cdate": 1758116053693, "mdate": 1763718299563, "content": {"title": "Single-stream Policy Optimization", "abstract": "We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3-8B, SPO improves the average maj@32 by $+3.4\\ \\text{percentage points} (\\mathrm{pp})$ over GRPO, driven by substantial absolute point gains on challenging datasets, including $+7.3\\ \\mathrm{pp}$ on BRUMO 25, $+4.4\\ \\mathrm{pp}$ on AIME 25, $+3.3\\ \\mathrm{pp}$ on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.", "tldr": "", "keywords": ["Single-stream Policy Optimization", "Large Language Models", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/561a7c77af87d8a05380c773514c2c9ad9d40561.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Single-Stream Policy Optimization (SPO), a group-free fine-tuning algorithm that effectively improves the throughput and scalability of group-based methods (e.g., GRPO) in long-horizon or tool-integrated scenarios. The authors also propose a KL-adaptive value tracker and batch-level advantage normalization to provide stable and continuous learning signals. Experimental results show that SPO significantly enhances the model's reasoning accuracy in terms of pass@k."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea and algorithm framework is simple.\n- The motivation is clear.\n- The experimental results are significant."}, "weaknesses": {"value": "- Judging from the title, the core contribution lies in single-stream. Although Section 5.4 discusses the agentic training scenario, I believe plotting runtime versus pass@k is important, as it helps highlight the advantages of single-stream (If the advantage of single-stream mainly lies in distributed training scenarios). The authors mainly analyze a specific example in Figure 4 and Figure 5, but I think providing the corresponding success rate metrics is also necessary.\n- I encourage the authors to plot ablation experiments similar to Figure 2, which would help everyone intuitively and quickly understand the contribution of each component in SPO."}, "questions": {"value": "- I think the value tracker and batch-level advantage normalization could also be directly integrated into GRPO, would this lead to further improvements, would it outperforms SPO?\n- In a standard RLVR scenario, does single-stream result in $\\times G$ times the runtime compared to GRPO (where $G$ is the number of responses in a group)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pJVuS8XXyM", "forum": "b61UW62K7W", "replyto": "b61UW62K7W", "signatures": ["ICLR.cc/2026/Conference/Submission9240/Reviewer_hydy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9240/Reviewer_hydy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722477158, "cdate": 1761722477158, "tmdate": 1762920894697, "mdate": 1762920894697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SPO, a simplified and scalable alternative to group-based methods like GRPO for fine-tuning LLM. It eliminates group synchronization and potential baseline issues by using a KL-adaptive value tracker and global advantage normalization. Experiments with Qwen3-8B show faster convergence and higher accuracy on challenging math reasoning benchmarks, achieving significant gains in maj@32 and pass@k metrics.\n\n\nOverall, the idea is interesting and potentially offers significant advantages. The acceleration in training speed can be substantial; however, the number of experiments conducted does not seem sufficient. Beyond the limited number of datasets and models, the authors introduced several new hyperparameters but did not isolate their individual contributions. The algorithm is always presented as a whole, while it is limited in how each parameter impacts performance. It would also have been valuable to include an ablation study on a smaller, less computationally expensive experimental setup, as I understand that the current configuration may be time- and resource-intensive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Training speed is notably increased through the streaming of a single output.\n- The paper provides a variance analysis of GRPO."}, "weaknesses": {"value": "- Only one model and one dataset were used for training; testing multiple models across different datasets would be necessary to validate the generality of the results. \n- No ablation study was conducted for the D_half parameter.\n- The effect of using the same prompt versus different prompts was not analyzed.\n- No ablation was performed for the w parameter.\n- Since the warm start seems effective, it would be helpful to include an ablation and clarification showing the minimal effective value,  for instance, demonstrating that a relatively small warm start is sufficient to gain its benefits."}, "questions": {"value": "In addition to clarifying the absence of ablation studies, please address the last weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d4IOGcgkRN", "forum": "b61UW62K7W", "replyto": "b61UW62K7W", "signatures": ["ICLR.cc/2026/Conference/Submission9240/Reviewer_GDbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9240/Reviewer_GDbD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731656696, "cdate": 1761731656696, "tmdate": 1762920894251, "mdate": 1762920894251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Recent propular RL methods like GRPO employ group-based value estimate and avoid maintainence and training overhead of a learned value model. However, they also come with some downsides. First, the value calculation in GRPO requires a group of rollouts for each update, which makes it expensive. This becomes especially problematic in agentic workflows when the slowest rollout in the group will slowdown the entire update. Second, there can be degenerate groups (all pass or all fail) which will lead to zero advantage and subsequent wasted computation.\n\nTo fix the above limitations, authors propose Single-stream Policy Optimization (SPO). The key innovations of this methods are as follows:\n1. It uses a KL-adaptive value estimate as the posterior of the a parametrized beta distribution. The alpha and beta values are updated based on sampled reward (binary 0,1) such that effective update rule for value follows an adaptive exponential moving average.\n2. Since each prompt has a unique moving value estimate, they only use single response per prompt and normalize the advantage over the entire training batch. They used PPO's clipped objective for training their policy.\n3. They also leverage the value estimate to construct a prompt sampling weight that puts highest weight for prompt close to 50% win rate and creates an automatic prompt curriculum.\n\nOverall, their method consistently improves over GRPO over a diverse range of cometition math evals. In appendix they also compare with A*PO, another method that uses fixed precomputed V* estimate and takes one sample per prompt during online learning. SPO performs very similar to A*PO while also giving the advantages of automatic prompt weighting and learning free dynamic value estimate."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Clear motivation and a very innovative way of calculating a moving model-free value estimate via on-policy samples\n- Their normalized advantage calculation leads to low variance and stable training\n- Their dynamic value estimate paves way for automatic prompt curriculum"}, "weaknesses": {"value": "- Their method does require some value warmstarting to be effective (similar to the inital cost of A*PO)."}, "questions": {"value": "1. Would have preferred to see an ablation experiment of w/ and w/o prompt curriculum to see the effectiveness of automatic curriculum\n2. I'm curious if the method will very rarely re-visit harder prompts due to the weighted sampling scheme. For example, if a prompt is harder for initial checkpoint and consistently gets 0 reward, its value will drift towards 0 and will be less likely to get resampled in the future.\n\nTypos:\n1. (equation 7) $N_{eff} = \\alpha(x) + \\beta(x)$.. there shouldn't be a +1\n2. (equation 10) Advantage shouldn't have subscript $t$ as its only using terminal reward."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OHhmMv88kZ", "forum": "b61UW62K7W", "replyto": "b61UW62K7W", "signatures": ["ICLR.cc/2026/Conference/Submission9240/Reviewer_uRF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9240/Reviewer_uRF7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884269559, "cdate": 1761884269559, "tmdate": 1762920893211, "mdate": 1762920893211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Single-stream policy optimization (SPO) for RLVR (verifiable rewards) LLM training. It basically compares with GRPO,  and replaces GRPO's baseline with (i) a persistent, KL-adaptive Beta tracker for per-prompt success probability (ii) global advantage normalization, and (iii) prioritized prompt sampling that biases training toward prompts with higher learning potential."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Very clear problem framing. It identifies two GRPO pain points, degenerate groups (no signal) and synchronization overhead and direct addresses both.\n2. The group-free simulation (4.35x faster) is compelling for tool-use/agentic regimes as low-latency is needed."}, "weaknesses": {"value": "1. Prioritized sampling is not so valid and might fail. This is a proxy for Bernoulli uncertainty but ignores the effective sample size $N_{eff} = \\alpha + \\beta$ carried by the Beta tracker. A prompt with $\\hat{v} \\approx 0.5$ but huge $N_{eff}$ should be less prioritized than one with the same $\\hat{v}$ and tiny $N_{eff}$. The prioritized sampling can over-sample well-known, mid-probability prompts.\n2. For $\\rho(x) = 2^{\\frac{-D(x)}{D_{half}}}$, there's no derivation or sensitivity study for $D_{half}$. Can the authors provide more ablation study on that?\n3. The experiment is only on one base model (Qwen3-8B) and binary RLVR reward. Showing another base LLM or a non-binary reward variant would be more beneficial.\n4. Missing baselines. Can the authors provide some non-GRPO baselines? A*-PO, GRESO, RLOO? Why the authors only provide GRPO as baseline?"}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "55IlBujRPC", "forum": "b61UW62K7W", "replyto": "b61UW62K7W", "signatures": ["ICLR.cc/2026/Conference/Submission9240/Reviewer_ss5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9240/Reviewer_ss5K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956852727, "cdate": 1761956852727, "tmdate": 1762920892623, "mdate": 1762920892623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}