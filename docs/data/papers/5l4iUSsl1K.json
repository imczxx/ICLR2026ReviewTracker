{"id": "5l4iUSsl1K", "number": 6616, "cdate": 1757990686261, "mdate": 1759897904897, "content": {"title": "Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning", "abstract": "Reasoning-augmented search agents, such as Search-R1, are trained to reason, search, and generate the final answer iteratively. Nevertheless, due to their limited capabilities in reasoning and search, their performance on multi-hop QA benchmarks remains far from satisfactory. To handle complex or compound queries, we train an LLM-based search agent with the native capability of query expansion through reinforcement learning. In each turn, our search agent proposes several query variants, which are searched simultaneously to cover more relevant information. Meanwhile, given limited post-training data and computing resources, it is very challenging for a search agent to master multiple tasks, including query generation, retrieved information understanding, and answer generation. Therefore, we propose incorporating a pre-trained squeezer model that helps the search agent understand the retrieved documents, allowing the search agent to focus on query generation for high retrieval recall. With the assistance of the squeezer model, we discover that even a small-scale 3B LLM can demonstrate a strong capability of query expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks. To be specific, our experiments across seven question-answering benchmarks demonstrate that our method, named ExpandSearch, achieves an average improvement of 4.4% compared to state-of-the-art baselines, with strong gains on multi-hop reasoning tasks requiring diverse evidence aggregation.", "tldr": "We train LLM-based search agents via reinforcement learning to generate multiple diverse query variants for improved retrieval coverage, then employs a squeezer model to distill the expanded results into reasoning-focused summaries", "keywords": ["Large Language Models", "Agent", "Reinforcement Learning with Verifiable Rewards", "Information Retrieval", "Post Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29e81d7459f12a3661f187f06a756d12481cfb0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ExpandSearch, a search agent trained with reinforcement learning to generate multiple complementary query variants (syntax-/semantics-oriented) and then compress retrieved chunks with a frozen “squeezer” LLM before continuing reasoning. Across seven QA benchmarks (NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MuSiQue, Bamboogle), ExpandSearch outperforms baselines and achieves an average EM of 0.446; ablations show a 34.3% relative gain over Search-R1 and a clear drop without the squeezer, while naïvely adding “expansion+squeeze” without RL yields no improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. End-to-end “expand-then-squeeze” design directly targets single-query semantic brittleness and information overload: parallel query expansion boosts recall, a frozen squeezer condenses evidence, and the agent is trained with PPO within the Search-R1 framework.\n\n2. Broad, consistent gains on seven benchmarks with clear reporting of training/eval splits and baselines; average EM 0.446 and a 34.3% relative improvement over Search-R1 substantiate the claim.\n\n3. Careful experiment design: EM improves as the number of rephrased queries n rises (with diminishing returns), ablations quantify the squeezer’s contribution (avg EM 0.446 → 0.364 when removed), and test-time generalization holds across different squeezers (LLaMA-4-17b vs LLaMA-3.1-8b)."}, "weaknesses": {"value": "1. Efficiency and cost are insufficiently characterized given the added query fan-out and extra squeezer calls; figures analyze accuracy vs. n but omit end-to-end latency, token/compute costs, or cost-accuracy trade-off curves.\n\n2. The squeezer is frozen and not jointly trained with the agent, preventing cross-module credit assignment and co-adaptation; joint or partially joint training is absent and could unlock further gains.\n\n3. No out-of-domain evaluation: models trained/evaluated on Wikipedia are not tested under corpus shift (e.g., biomedical, legal, finance); assessing OOD generalization on domains like PubMed/BioASQ (or comparable corpora) with retrieval quality, answer accuracy, and calibration would strengthen external validity."}, "questions": {"value": "refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ncU64fpjIR", "forum": "5l4iUSsl1K", "replyto": "5l4iUSsl1K", "signatures": ["ICLR.cc/2026/Conference/Submission6616/Reviewer_Mg5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6616/Reviewer_Mg5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769261769, "cdate": 1761769261769, "tmdate": 1762918936935, "mdate": 1762918936935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ExpandSearch, a reinforcement-learning-based methodology that trains the LLM agent to learn how to answer queries by asking multiple good questions to the search engine and perform retrieval-augmented generation. Compared to Search_R1, ExpandSearch improves the diversity of questions by generating multiple variants, which leads to higher recall during the retrieval stage. After this, all documents are summarized by another frozen LLM to ensure the conciseness of information. The approach is conceptually reasonable and provides effective performance gain on a variety of QA datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is generally well-written and easy to follow. \n2. The idea of having a learnable strategy of writing multiple candidates and compressing the information is clean and reasonable. \n3. Empirically, ExpandSearch outperforms many recent baselines on General QA and Multi-Hop QA using a variety of Qwen models.\n4. The ablation study shows that each introduced component leads to ca ertain performance gain."}, "weaknesses": {"value": "1. The method is a straightforward extension of Search-R1 by incorporating multiple queries and hence has limited novelty and insightfulness. \n2. Using another LLM as a squeezer may introduce unwanted computational overhead, or even giving abilities that the small model does not possess. \n3. Though the intuition of learning to write multiple diverse queries is beneficial, there is a lack of sufficient qualitative and quantitative analysis on the advantage brought by this strategy, other than the ablation study."}, "questions": {"value": "1. There is a significant performance drop when the squeezer is removed, as shown in the ablation study. When using another larger LLM from a completely different model family (Llama instead of Qwen) as the squeezer, how to make sure that these models are not using their internal knowledge base to help answer the question, which may potentially cause information leakage?\n2. Why is LLama chosen as the squeezer model instead of Qwen?\n3. It would be beneficial to test the scaling of the number of queries and see when it roughly reaches the plateau, as an increasing number of queries is improving the performance almost linearly."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The paper’s writing is shockingly similar to ParallelSearch by Zhao et al, which is cited in the paper. The similarity in writing, graphics, and algorithms is sufficient to be considered plagiarism or self-plagiarism. It is also hard to clearly distinguish the contribution of this paper compared to ParallelSearch, although there are some expansions, which are unfortunately not highlighted. As a reviewer, I am very confused about the relationship between these two papers. Are they the same work or concurrent works? Regardless, I don’t think it complies with the paper publication standard in academia."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UYV03FcAym", "forum": "5l4iUSsl1K", "replyto": "5l4iUSsl1K", "signatures": ["ICLR.cc/2026/Conference/Submission6616/Reviewer_pzbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6616/Reviewer_pzbi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933710615, "cdate": 1761933710615, "tmdate": 1762918936489, "mdate": 1762918936489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ExpandSearch, a dual-model RL framework that addresses limitations of single-query retrieval in reasoning-augmented LLMs like Search-R1. The expand stage trains an LLM to generate multiple semantically diverse queries (syntactic and semantic expansions) for broader evidence coverage. The squeeze stage uses a frozen summarization model (“squeezer”) to distill the retrieved documents into compact, relevant contexts and removes noise.\n\nThe approach is trained via RL based on Exact Match (EM) reward. Experiments on seven QA benchmarks (including HotpotQA, Musique, 2Wiki, Bamboogle, NQ, TriviaQA, PopQA) show EM gains over Search-R1 and related RL-based retrieval agents (ZeroSearch, ParallelSearch, Router-R1). The authors claim the method enables even a 3B model to outperform 7B baselines by improving query diversity and information precision."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong motivation and clear insight: The key observation that LLM-based search agents struggle with both semantic incompleteness (too narrow queries) and information overload (too much irrelevant retrieved text) is well articulated and supported.\n\n2. The “expand-then-squeeze” decomposition is intuitively appealing and aligns with human search behavior.\n\n3. Decoupling the retrieval (expansion) and summarization (squeeze) components is practical, scalable, and computationally efficient. The modularity allows plug-and-play squeezers without retraining.\n\n4. The paper is easy to follow, with good examples and clear algorithmic description.\n\n5. Results are consistent across datasets and model sizes. The ablation studies (with/without squeezer, with/without syntax or semantic expansion) reveal useful insights about the learned expansion behavior.\n\n6. RL training seems stable"}, "weaknesses": {"value": "1.  The model is trained and evaluated using the same EM metric. This creates a bias, the agent is directly optimizing for the benchmark metric, so reported gains may simply reflect reward overfitting rather than genuine reasoning or retrieval improvements.\n\n2. Other evaluation metrics like F1, LLM-as-judge, or match scores (e.g., Match, F1) are absent. Without those, the evaluation is incomplete.\n\n3. The paper repeatedly claims that ExpandSearch “improves recall” and “balances recall and precision,” but does not provide any retrieval metrics. Without retrieval-level analysis, it’s unclear whether gains come from better retrieval, better summarization, or just reformulations.\n\n4. While the paper includes seven datasets, only 2-4 hop QA benchmarks are used (HotpotQA, 2Wiki, Musique, Bamboogle). These are relatively shallow multi-hop tasks by current standards. \n\n5. The authors omit comparisons with recent strong baselines that report state of the art performance on multi-hop rag benchmarks, including: CoRAG (Wang et al), R1-Searcher (Song et al), FrugalRAG (Java el al), O2-Searcher (Mei et al), etc. The squeeze stage is conceptually similar to RECOMP (Xu et al.), which also compresses retrieved text using a summarizer before answer generation. Similarly, it would be good to make clear distinction with existing methods that use search expansion. ExpandSearch differs in using RL for multi-query expansion, but both share the same underlying goal of mitigating context overload by selective compression.\n\n6. The approach seems conceptually similar to prior works mentioned in the paper like Router-R1, ParallelSearch, which also use multiple queries. A clear distinction would position this work better.\n\n7. The authors claim cross-domain generalization, but nearly all benchmarks (NQ, TriviaQA, Hotpot, etc.) are Wikipedia-derived, sharing similar style and domain distribution. This is a weak claim without including additional, real world datasets (e.g. biology).\n\n8. The paper claims even a small 3B model shows strong performance, but that’s achieved only by offloading summarization to a larger frozen model. The overall system compute footprint is much greater than single-model baselines. A clear comparison of latency, overall model size (including squeezer) is required for a fair comparision"}, "questions": {"value": "Kindly see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N41AewSzNM", "forum": "5l4iUSsl1K", "replyto": "5l4iUSsl1K", "signatures": ["ICLR.cc/2026/Conference/Submission6616/Reviewer_5JKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6616/Reviewer_5JKj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006302377, "cdate": 1762006302377, "tmdate": 1762918936022, "mdate": 1762918936022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an extension to the Search-R1 framework by including a query expansion component for retrieval. The model does multiple query expansions simultaneously and is trained using RL. Comparison against Search-R1 shows about 4% performance gain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "+ The paper focuses on a timely topic and Search-R1 is a great platform to explore this idea.\n+ The paper evaluates on seven datasets.\n+ The approach is simple and the paper is easy to follow."}, "weaknesses": {"value": "- The first main weakness is comparison with weak baselines. Even though the authors extended Search-R1 with query expansion, they should have compared the model with Search-R1 that interacts with a search engine capable of query expansion. Query expansion in retrieval has been studied for decades. There are many methods, such as Relevance Models, Embedding-based Query Expansion techniques, and even LLM-based query expansions that do not need any training. Search-R1 with these query expansion methods should be considered as baselines. Otherwise, it's unclear where the gains come from. Are they coming from the fact that query expansion is useful for retrieval? (which is a known phenomenon) or does the proposed approach for query expansion work compared to existing alternatives?\n\n- The presented results are confusing. How come results for Search-R1 and ExpandSearch in Table 3 don't match with any of the Search-R1 and ExpandSearch results in Table 2? By the way, the ablation study should be done on either all LLM sizes or on the Qwen base 7B where the difference between Search-R1 and ExpandSearch is the smallest.\n\n- No statistical significance tests are done to demonstrate the obtained improvements are meaningful.\n\n- The main contribution of this work is on query expansion, which is basically an approach for improved retrieval (not language modeling). Therefore, the focus on the experiments should be more on various retrieval models and query expansion baselines instead of blindly following the trend on experimenting with different LLM sizes. For example, does this query expansion approach work if they retrieval model is based on term matching, like BM25 or KL divergence? What about other dense retrieval models, like DPR and ColBERT? What about retrieval models with sparse representations, like SPLADE? What about more advanced retrieval models, like Hypencoder?\n\n- Analysis on the impact of the number of expanded tokens is missing. \n\n- Last but not least, the paper lacks sufficient novelty for ICLR. Seems just like a prompt engineering paper that encourages the model to perform query expansion, rather than studying how to design a reward functions for this task, how to optimize this effectively, and so on."}, "questions": {"value": "None. The concerns raised in the Weakness section are so significant that cannot be resolved in a rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7BuShTNeZL", "forum": "5l4iUSsl1K", "replyto": "5l4iUSsl1K", "signatures": ["ICLR.cc/2026/Conference/Submission6616/Reviewer_Qimr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6616/Reviewer_Qimr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177694356, "cdate": 1762177694356, "tmdate": 1762918935582, "mdate": 1762918935582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}