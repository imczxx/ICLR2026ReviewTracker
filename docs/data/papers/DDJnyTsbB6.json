{"id": "DDJnyTsbB6", "number": 22815, "cdate": 1758335842890, "mdate": 1763762732871, "content": {"title": "Visual serial processing deficits explain divergences in human and VLM reasoning", "abstract": "Why do Vision Language Models (VLMs), despite strong benchmark performance, often fail on surprisingly simple visual reasoning tasks? We hypothesize that this gap reflects a deficit in visually-grounded serial processing. To test this hypothesis, we compared human and VLM performance on three domains that systematically vary serial processing load: geometric reasoning (via concept complexity), enumeration (via individuation demands), and mental rotation (via transformation difficulty). In each domain, decreased VLM accuracy was strongly correlated with increased human reaction time (used as a proxy for serial processing load). As tasks require more demanding serial processing---whether composing concepts, enumerating items, or performing mental transformations---the VLM-human performance gap widens reliably. These findings support our hypothesis that limits in serial, visually grounded reasoning form a fundamental bottleneck distinguishing current VLMs from humans.", "tldr": "", "keywords": ["Vision language models", "visual reasoning", "geometric reasoning", "serial processing", "cognitive science"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/69dc78d1f8dec713b514269fd488def5640c6068.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an hypothesis to explain some limitations of Visual Language Models: This is due to a deficit of serial processing on the visual side. In order to assess that hypothesis, several tests are driven on several SOTA VLM, related to geometric reasoning, enumeration and mental rotation. It is indeed observed that errors are more important when the serial aspect of that task was increasing. Note that this increasing serial aspect is measured by an increasing processing time. It is concluded that these observations support the hypothesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The hypothesis is simple and clearly stated. The experimental protocols are carefully described."}, "weaknesses": {"value": "Globally, I found the contribution relatively weak. \n-the unique and central hypothesis of this paper is just stated, with no justification or intuition or elaborated development. It is just given. \n- the main contribution is about the assessment of a rather simple and obvious hypothesis. I agree that it is important  to precisely evaluate it but the impact of the paper is not so important. \n- the fact that the level of complexity of the serial task is directly and uniquely measured by the processing time should be discussed more. I am not convinced it is always true\n-the conclusion corresponds to a generality, whereas the test was only carried out for three kinds of tasks and some VLM. \n-beyond this conclusion, it could have been interesting to discuss more implications or possible adaptations of VLM."}, "questions": {"value": "Are you certain that the level of complexity is directly measured by the processing time ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ajDEHtOypy", "forum": "DDJnyTsbB6", "replyto": "DDJnyTsbB6", "signatures": ["ICLR.cc/2026/Conference/Submission22815/Reviewer_kB3k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22815/Reviewer_kB3k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760634269860, "cdate": 1760634269860, "tmdate": 1762942398559, "mdate": 1762942398559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to explain VLMs failures on simple visual tasks. They argue that this is due to failures in visually grounded serial processing. The authors compare VLMs and humans on tasks where serial processing requirements are manipulated. They find that VLM accuracy is correlated with human reaction times, which they take as a proxy for serial processing load. The authors here accumulate evidence for a hypothesis outlined in previous work [1]. Overall, I find the paper to be very well put together and enjoyable, and while it does not offer an entirely novel account of why VLM visual processing fails (see [1]), it does provide more evidence for this account and therefore I am inclined to accept it."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- *Originality* The paper is well motivated and uses established experiments to investigate a theory explaining problems in VLM visual processing.\n- *Quality* The paper is very well put together, the experiments are sensible and overall it is a good read. \n- *Clarity* The paper is written clearly and the results are presented in a very digestible way.\n- *Significance* The paper accumulates further evidence for the binding problem hypothesis explaining the problems VLMs have with basic visual processing."}, "weaknesses": {"value": "While I really like the paper in general, it does not offer an entirely novel account of why VLM visual processing fails (see [1]). Ideally, this paper would offer a more concrete path towards implementing visual serial processing in vision language models --  I think the different solutions offered in this paper are not worked out well enough to really understand what would fix this issue. In any case, I think the paper still provides novel evidence for the binding problem theory, albeit limited in scope somewhat, and I will recommend acceptance."}, "questions": {"value": "**Main questions**\n- Line 440 \"However, this seems to be a strictly linguistic form of serialization that is difficult to apply to visual tasks like mental rotation\" can you elaborate on this? Does this mean that since reasoning models can only generate new text they can not apply serialized processing to the visual information? I don't think I understand completely.\n- Can you elaborate a bit on how exactly you perform the tool use experiments? I couldn't find a lot of detail on how this was run.\n- One thing that would be nice to add is some data on what happens when you restrict exposure times or reaction times in humans. In theory, if serial processing is the only distinguishing thing between humans and VLMs in these tasks, if you restrict the time humans can engage with a specific stimulus shouldn't you see a similar drop in accuracy as in VLMs? \n\n**Minor comments**\n- I would put Figure 1 b-d on the same y-axis, the axes are close enough already and that would make it more easily comparable.\n- Figure 2 the b for subfigure b seems to be cut off and the legends are not sorted in a coherent way (extremely minor).\n\n\n[1] Campbell, Declan, et al. \"Understanding the limits of vision language models through the lens of the binding problem.\" Advances in Neural Information Processing Systems 37 (2024): 113436-113460."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JYC8RZsXKe", "forum": "DDJnyTsbB6", "replyto": "DDJnyTsbB6", "signatures": ["ICLR.cc/2026/Conference/Submission22815/Reviewer_o54c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22815/Reviewer_o54c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649020843, "cdate": 1761649020843, "tmdate": 1762942398273, "mdate": 1762942398273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key paradox in Vision-Language Models (VLMs): strong performance on complex benchmarks yet frequent failures on simple visual reasoning tasks. The authors propose the serial processing deficit hypothesis, arguing that VLMs lack visually grounded serial processing capabilities that humans rely on for tasks like geometric reasoning, enumeration, and mental rotation. Using controlled cognitive science paradigms, they evaluate four state-of-the-art VLMs (GPT-4o, Claude Sonnet 3.7, Llama 4 Maverick, Qwen2.5 VL) alongside human behavioral data. Human reaction time (RT) serves as a proxy for serial processing load. Across all domains, VLM accuracy inversely correlates with human RT, supporting the hypothesis. Augmentation strategies (Chain-of-Thought, tool use, reasoning training) improve performance but reveal limitations, suggesting the need for architectural changes to integrate serial visual reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The serial processing deficit hypothesis offers a coherent lens for interpreting diverse VLM failures (counting, spatial reasoning, compositionality). \n\n- Experiments are grounded in validated cognitive paradigms with parametric control over complexity (MDL for geometry, overlap/color for enumeration, angular disparity for rotation). \n\n- Using human reaction time (RT) as a continuous predictor of VLM performance bridges cognitive science and AI evaluation, enabling predictive insights beyond static benchmarks. \n\n- The paper is well-written, with relevant figures and comprehensive appendices detailing prompts, stimuli, and per-model results. \n\n- Discussion of architectural directions (visually grounded RL, foveal masking) provides a roadmap for addressing identified limitations."}, "weaknesses": {"value": "- “Serial processing” conflates multiple mechanisms (attentional shifts, compositional reasoning, iterative refinement). It is good to clarify which aspects are hypothesized to be deficient. \n\n- RT is used to define and validate serial processing load, but RT reflects overall task difficulty, not exclusively serial processing. Alternative explanations (binding problem, perceptual ambiguity) are not fully ruled out. \n\n- Augmentation strategies (CoT, tool use) provide task-specific scaffolding rather than true serial visual processing. Improvements in colored overlapping conditions suggest linguistic grounding, not visual seriality. \n\n- The paper treats VLMs as black boxes without probing attention patterns or internal representations. Linking performance to architectural features would strengthen conclusions. \n\n- While architectural improvements are discussed, none are implemented or evaluated. \n\n- Findings are based on synthetic stimuli and four VLMs; applicability to real-world images and newer architectures remains uncertain."}, "questions": {"value": "Check weakness in addition to:\n\n- How do you dissociate serial processing from general task difficulty in the RT–accuracy relationship? \n\n- How is the serial processing deficit distinct from the binding problem? Could these be alternative framings of the same phenomenon? \n\n- Have you analyzed VLM attention maps for evidence of sequential scanning? \n\n- Could failures in mental rotation reflect insufficient training on rotated exemplars rather than architectural limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ATVctsZZB", "forum": "DDJnyTsbB6", "replyto": "DDJnyTsbB6", "signatures": ["ICLR.cc/2026/Conference/Submission22815/Reviewer_Ddut"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22815/Reviewer_Ddut"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739375005, "cdate": 1761739375005, "tmdate": 1762942398045, "mdate": 1762942398045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the \"serial processing deficit hypothesis\" to explain systematic failures of VLM on visual reasoning tasks. The authors test this hypothesis across three cognitive domains (geometric reasoning, numerical estimation, and mental rotation) by systematically varying task complexity. The core finding is a consistent inverse correlation between human reaction time (used as a proxy for serial processing load) and VLM accuracy: as tasks require more sequential processing (indicated by longer human RT), VLM performance degrades significantly while human accuracy remains high. The paper provides causal evidence through augmentation experiments showing that external serial processing mechanisms (CoT, reasoning training, tool use) can improve performance in limited contexts, but VLMs lack intrinsic visual serial reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using human reaction time as a proxy for serial processing load is well-grounded in cognitive science and provides a principled way to manipulate task complexity across diverse domains.\n\n2. Testing across three distinct cognitive domains (geometric reasoning, numerosity, mental rotation) with multiple SOTA VLMs (GPT-4o, Claude Sonnet 3.7, Llama 4 Maverick, Qwen2.5 VL) strengthens the generalizability of findings.\n\n3. The observation that CoT only helps counting when objects have unique colors reveals the fundamental limitation—VLMs can only leverage serial processing when it can be linguistically mediated."}, "weaknesses": {"value": "1. **Missing critical prior work on VLM serial processing limitations**: This problem has been substantially investigated in prior work, particularly in \"How Far Are We from Intelligent Visual Deductive Reasoning?\" (arXiv 2403.04732, March 2024). That paper:\n   - Documented that VLMs fail at visual deductive reasoning tasks requiring multi-hop comparative reasoning and sequential hypothesis verification\n   - Showed that **segmenting RPM problems into individual patterns improved accuracy from 7.4% to 21.2%**—direct causal evidence that breaking tasks into sequential steps helps VLMs\n   - Identified \"compounding errors\" where VLMs make cascading mistakes in autoregressive pattern description, suggesting inability to perform iterative self-correction\n   - Demonstrated VLMs achieve only 10-20% accuracy even when in-context examples are identical to queries, showing failure in sequential visual comparison\n   - Found that VLMs make \"confounding errors\" where they cannot maintain focus on single patterns when multiple similar patterns are present—a clear attention/serial processing deficit\n\n   The current paper claims the serial processing deficit as a key novel finding but fails to cite or discuss this highly relevant prior work that provides converging evidence from a different task domain (abstract geometric pattern reasoning). The segmentation results in particular are compelling causal evidence that the authors should engage with.\n\n2. **Limited architectural analysis**: While the paper identifies the deficit, it provides minimal insight into *why* current VLM architectures fail at serial processing. Is it the single-pass vision encoder? Lack of recurrent processing? Insufficient visual working memory? \n\n3. **Weak tool use baseline**: The tool use augmentation only tests rotation tools for the mental rotation task. Why not test cropping/zooming tools for counting tasks, or tools that isolate geometric elements? The current tool use experiments don't adequately test whether VLMs can leverage external mechanisms for serial visual analysis beyond simple image transformations.\n\n4. **Insufficient analysis of human-VLM differences**: The paper uses human RT as ground truth but doesn't deeply investigate *how* humans perform serial processing on these tasks. Eye-tracking data or think-aloud protocols could provide richer insights into what specific serial mechanisms are missing in VLMs."}, "questions": {"value": "1. How do you reconcile your findings with the RPM paper's segmentation experiments, which provide direct causal evidence for the serial processing deficit?\n\n\n2. Can you provide more granular analysis of where in the processing pipeline VLMs fail? For example, do they fail at individuating objects (segmentation), tracking them (working memory), or transforming representations (mental operations)?\n\n3. The CoT results suggest linguistic mediation is necessary for serial processing in current VLMs. Could multimodal chain-of-thought (with intermediate visual representations) help?\n\n4. Have you tested with few-shot evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qvNk5EDquz", "forum": "DDJnyTsbB6", "replyto": "DDJnyTsbB6", "signatures": ["ICLR.cc/2026/Conference/Submission22815/Reviewer_GX7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22815/Reviewer_GX7P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762215821989, "cdate": 1762215821989, "tmdate": 1762942397758, "mdate": 1762942397758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}