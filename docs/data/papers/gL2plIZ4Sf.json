{"id": "gL2plIZ4Sf", "number": 11579, "cdate": 1758202066888, "mdate": 1759897566470, "content": {"title": "A Hybrid Feature Tree-Based Approach for Explainable LLMs in Domain-Specific Knowledge Management", "abstract": "The \"black-box\" nature of Large Language Models (LLMs) poses a significant barrier to their adoption in high-stakes, regulated domains like finance and healthcare, where verifiable explanations are mandatory. We propose a novel hybrid framework that enhances LLM explainability by generating hierarchical feature trees from individual Question-Answer (Q\\&A) pairs and merging them into a unified, global \"Uber Tree.\" This structure provides both local explanations for specific answers and a global overview of the model's knowledge landscape. Our method combines the semantic understanding of LLMs for tree generation and merging with traditional recursive algorithms for robustness, ensuring scalability. Crucially, we introduce a formal consistency verification step to validate the alignment between individual explanations and the global knowledge structure. Applied to the domain of mortgage compliance using a comprehensive dataset of 1000 Q\\&A pairs, our framework demonstrates high-quality tree generation, effective merging that outperforms purely algorithmic baselines, and strong consistency (95\\%). A human evaluation with domain experts confirms a significant improvement in explainability and auditability over standard Chain-of-Thought explanations. This work offers a practical pathway toward auditable and verifiable AI systems at enterprise scale.", "tldr": "", "keywords": ["Explainable AI", "xAI", "LLM", "feature tree", "uber tree"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f684e9352e9fb54b0b2a7b877dc3e1e8cb55ab23.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for meta-analysis of Q&A systems using different tree-generation and aggregation prompts. The authors aren't completely clear which domain(s)/use cases necessitate such a metadata tree. The authors evaluate their method on a Chain of Trees baseline on a dataset of mortgage compliance Q&A."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Overall the method is well motivated, in this specific domain that attribute trees can be constructed and verified. \n\n2. The authors give some qualitative evaluation, though it would be greatly improved over more domains and a higher number of domain experts."}, "weaknesses": {"value": "1. The paper doesn't clarify their key contributions well. The related work is in 5 areas that the authors don't give sufficient technical depth/differentiation. \n\n2. The authors dont give sufficient technical details on the merging step. On 3.2.1 the authors dont give a clear algorithm or prompt generation. Furthermore, the authors arent clear what a 'good' aggregated tree ought to contain. There is no measure on candidate trees. If it's left to the LLM through prompting, this is really not a suitable contribution. \n\n3. The human evaluation is lacking. 3 expert labelers over 30 instances is pretty light. But again, the authors haven't educated the reader on the tree quality. Please give specific examples of the human evaluation task. Demonstrate the tree differences side-by-side. \n\nFurthermore, the measures aren't even given(!?) in Table 3? What are the value scales for FT, CoT columns? What is the specific p-value test?  \n\n4. The tree evaluation is quite unintuitive and needs more space. In table 2, the authors note 100% success-rate with a heuristic non-LLM fallback. OK? It's not obvious from table 2 that Hybrid tree construction is better. It's sparser, but we can always add sparsity penalties to the other methods. Since there's no(?) tree-level quality scoring, it seems any method could 'game' the statistics on this table."}, "questions": {"value": "1. What is the prompt for tree merging? The specific prompt. \n\n2. Is there an explicit measure for tree quality?\n\n3. Can you visually present the evaluation task? What are all the columns in Table 3 representing, and why is FT qualitatively better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0HVacJTMT8", "forum": "gL2plIZ4Sf", "replyto": "gL2plIZ4Sf", "signatures": ["ICLR.cc/2026/Conference/Submission11579/Reviewer_Wquh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11579/Reviewer_Wquh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687539062, "cdate": 1761687539062, "tmdate": 1762922665412, "mdate": 1762922665412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to explain LLM outputs in regulated domains, e.g. finance, and health. They do so by restructuring the outputs of LLMs in verifiable feature trees that are then merged into a single global \"Uber Tree\" representing the global structure of the model. The method is tested on a Q&A dataset of mortgage applications."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a timely and important challenge: improving explainability of LLMs in sensitive, high-stakes domains. Introducing structural constraints to the model’s outputs to better capture and understand its internal reasoning structure is a well-motivated and sensible approach. The proposed hybrid framework—combining LLM-based semantic abstraction with algorithmic merging—demonstrates technical creativity and practical relevance, especially for regulated areas like mortgage compliance where interpretability and auditability are essential."}, "weaknesses": {"value": "While the use of structured outputs is a sensible way to probe LLM behavior, the approach falls short of what would be required in a real regulated deployment. In such settings, structural constraints should ideally be integrated into the model’s training or architecture, not imposed only through prompt engineering or in-context control.\n\nAlthough the paper claims broad applicability to regulated domains, all experiments are conducted solely on mortgage compliance data. For a study submitted to ICLR, this narrow scope—limited to a single dataset and a single proprietary LLM—restricts the generalizability and scientific contribution of the work.\n\nFinally, reliance on a closed-source, proprietary LLM undermines the paper’s stated goal of developing verifiable and auditable AI systems. Guarantees in regulated environments require transparency about the predictive model’s inner structure, which is not available here."}, "questions": {"value": "Have you considered evaluating the proposed framework on datasets from other regulated domains such as healthcare or legal compliance, to test its generalizability beyond the mortgage domain?\n\nRather than constraining outputs only through prompting, did you explore modifying or fine-tuning LLMs to natively generate structured, hierarchical explanations? If not, what do you see as the main barriers to doing so?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VOKhGZIout", "forum": "gL2plIZ4Sf", "replyto": "gL2plIZ4Sf", "signatures": ["ICLR.cc/2026/Conference/Submission11579/Reviewer_gU3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11579/Reviewer_gU3Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872148653, "cdate": 1761872148653, "tmdate": 1762922665022, "mdate": 1762922665022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a way to improve LLM explainability: generating hierarchical feature trees from individual QA pairs, and merging into a unified global \"Uber Tree\". They chunk the individual feature trees and prompt the LLM to recursively merge them chunk by chunk, removing duplicated information and getting globally semantically related nodes. They use the merged Uber Tree to check if invidiual trees are consistent with it and find that they are mostly very consistent, showing they are consistent in the internal knowledge being used. They validated with human experts and show that these feature trees a re more clear than cot, but not as comprehensive, and maybe similarly easy to verify."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to understand.\n2. The paper gives a comprehensive structural and semantic evaluations of the tree-based method, demonstrating that the merged hierarchy and the individual explanations are internally consistent, having the shared consistent domain knowledge."}, "weaknesses": {"value": "1. In the human evaluation, the authors only compared clarity, comprehensiveness, and ease of verification between feature trees and CoT, but this could simply be because FT is a tree structure. There are other merging methods, and they did not report human evaluation results on those.\n2. The authors mainly validated whether LLMs can generate the required JSON structure (which current LLMs can already do reliably), whether the global knowledge looks coherent and abstract (which makes sense since they use the same Uber Tree), and whether the outputs have high internal consistency. However, they did not evaluate the accuracy of the results against any external ground truth, so a tree could be coherent and self-consistent but still factually incorrect.\n3. It is also unclear whether the explanations themselves align with expert intuition. The human study only evaluated presentation-level properties (clarity, comprehensiveness, ease of verification), but did not assess whether the key factors, logical steps, or abstractions actually match how domain experts reason about these regulations.\n4. In the abstract, they authors claim that the proposed method improved auditability, but the human evaluation in Table 3 shows no statistically significant improvement (p=0.5834) in \"Ease of Verification\" between feature trees and CoT explanations, weakening this claim.\n5. The authors did not evaluate the faithfulness of the feature tree-based explanations. Thus, even if the explanations are clear, we don't know if they can inform the correctness of the answer, and thus we do not know if the proposed method makes the explanation more auditable."}, "questions": {"value": "1. Could you do human evaluation also on the other tree-based baselines?\n2. What are the end accuracy of the answers following each method's explanation?\n3. Do the experts agree on the domain-knowledge being used by the LLM despite thinking the explanations are clear?\n4. How faithful are the explanations and corresponding answers? How often do the answers logically follow the explanations? How logically consistent is each feature tree internally?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oZyx0T2YvO", "forum": "gL2plIZ4Sf", "replyto": "gL2plIZ4Sf", "signatures": ["ICLR.cc/2026/Conference/Submission11579/Reviewer_YEPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11579/Reviewer_YEPR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953301816, "cdate": 1761953301816, "tmdate": 1762922664511, "mdate": 1762922664511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to enhance the explainability of LLMs in high-stake domains. The method uses an LLM to generate structured, hierarchical Feature Tree from an individual Q&A pair. Then merges these local trees into Uber Tree. The paper verifies consistency where an LLM audits the alignment between Uber Tree and local Feature Trees, producing a consistency score. The framework evaluated on the mortgage compliance dataset, achieved high structural consistency and improved clarity over CoT explanation in human expert evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. Consistency verification step is a compelling idea.\n3. The experiment is done comparing with human evaluation with domain expert."}, "weaknesses": {"value": "1. The problem statement in the introduction is very broad. The described challenge is fundamental to almost all xai research.\n2. The main text of the table needs to provide guidance on how to interpret the results in the table (e.g., what the takeaway is).\n3. The proposed Feature Tree seems like a post-hoc structuring of the answer's content generated by a separate LLM call which is a bit different from explanation of LLM's internal reasoning process for generating an answer (like LIME, SHAP as mentioned as limitations in the paper)\n4. The paper does not clearly specify whether for the generation of the Feature Tree from the pair, which is a post-hoc step, is the LLM given only the text of the question and answer of does it have access to the reasoning trace of the model that produced the pair.\n5. The experiment is only done in mortgage compliance domain. I wonder the experiment result of other high-stake domains like medicine.\n6. The experiment relies on one LLM (GPT 4.1) which lacks generalization.\n7. The human evaluation only involves 3 domain experts which lacks statistical power to draw generalizable conclusions. Also need to give specific information about the domain experts."}, "questions": {"value": "Look at the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yUXr4kjNBO", "forum": "gL2plIZ4Sf", "replyto": "gL2plIZ4Sf", "signatures": ["ICLR.cc/2026/Conference/Submission11579/Reviewer_8bZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11579/Reviewer_8bZV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996377355, "cdate": 1761996377355, "tmdate": 1762922663980, "mdate": 1762922663980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}