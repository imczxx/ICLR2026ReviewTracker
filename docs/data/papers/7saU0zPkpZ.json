{"id": "7saU0zPkpZ", "number": 15958, "cdate": 1758257636205, "mdate": 1759897270693, "content": {"title": "AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation", "abstract": "Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term **semantic negligence**. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground–background separation. From an energy perspective, this corresponds to a lower-entropy attention distribution. Motivated by this, we introduce **AlignVid**, a training-free framework with two components: (i) *Attention Scaling Modulation (ASM)*, which directly reweights attention via lightweight Q/K scaling, and (ii) *Guidance Scheduling (GS)*, which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce **OmitI2V** to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity. Code and benchmark will be released.", "tldr": "", "keywords": ["Video generation", "Semantic fidelity", "Attention analysis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60c936e834f374323cd968bd4cfde337e79c364e.pdf", "supplementary_material": "/attachment/e5a59496eafeaa6d2a9d09145f32d57dcf1caf2e.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of \"semantic negligence\" in ti2v models, where prompts that require large changes to an image are often ignored. The authors propose AlignVid, a training-free method that improves prompt-following by directly scaling attention weights during inference, and introduce a new benchmark called OmitI2V to measure this specific failure mode"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a common problem in ti2v. The model's tendency to ignore prompts that require significant edits like adding or removing an object. The initial pilot study showing that blurring an image can improve results is a simple but very effective way to motivate the investigation into attention mechanisms.\n- The proposed method, AlignVid, is simple and practical. It doesn't require any model retraining, making it easy to apply to existing models. \n- While I have some reservations about the general eval (see weaknesses), the creation of the OmitI2V benchmark is a good contribution. It is specifically designed to evaluate the problem of semantic negligence (addition, deletion, modification), which is a gap in existing benchmarks."}, "weaknesses": {"value": "- The theoretical analysis in Section 4 feels generic and not well-connected to the specific problem the paper aims to solve. The theory explains that scaling attention logits is like temperature scaling, which reduces entropy. However, it doesn't explain why this helps with the specific TI2V problem of balancing an input image with a text prompt. The theory is for a general DiT, but the problem is about a conditioned generation task, and the link between the two is not convincingly made.\n- The paper introduces a new benchmark, OmitI2V, but it's not clear why a new, small-scale benchmark was necessary instead of using or extending the widely adopted VBench-I2V. The evaluation for OmitI2V relies on a MLLM model to answer yes/no questions, which can be brittle and may not be that reliable due to randomness of MLLM. Results on VBench are included in the appendix, but improvements are very marginal.\n- The results in Table 2 and the appendix show a clear trade-off: AlignVid improves semantic alignment and motion, but it consistently leads to a small drop in aesthetic quality. This suggests the method isn't strictly making the model \"better,\" but rather shifting its behavior to prioritize prompt-following over visual quality. This is pretty similar to the CFG tradeoff. Higher cfg leads to higher text following, but lower quality."}, "questions": {"value": "The pilot study observation that Gaussian blur \"sharpens foreground-background separation\" in attention maps is the core motivation for the entire paper. However, this is only supported by one qualitative example in Figure 2. Could you provide more quantitative evidence for this claim, for example, by showing averaged attention entropy or contrast metrics across many samples from your benchmark, with and without the blur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eeUBo03gct", "forum": "7saU0zPkpZ", "replyto": "7saU0zPkpZ", "signatures": ["ICLR.cc/2026/Conference/Submission15958/Reviewer_MrsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15958/Reviewer_MrsV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960944391, "cdate": 1761960944391, "tmdate": 1762926168647, "mdate": 1762926168647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and addresses \"semantic negligence\" in text-guided image-to-video (TI2V) generation, a common failure mode where models ignore textual prompts that require substantial edits (e.g., adding, deleting, or modifying objects) to the source image. The authors propose AlignVid, a novel, training-free framework that improves semantic fidelity by modulating the model's attention mechanism. To evaluate their method, the authors also introduce OmitI2V, a new benchmark of 367 samples specifically designed to test for semantic negligence. Experiments on state-of-the-art TI2V models (FramePack and Wan2.1) demonstrate that AlignVid significantly improves semantic alignment with negligible computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly formalizes a critical and prevalent weakness in current TI2V models. The accompanying OmitI2V benchmark is a valuable and necessary tool for the community, as existing benchmarks do not adequately measure this specific failure mode.\n2. The paper provides a solid theoretical motivation for why ASM works. Section 4, which links Q/K scaling to inverse temperature control of the softmax and proves its monotonic effect on attention entropy, elevates the method beyond a simple heuristic.\n3. The ablation studies (Tables 3, 4, 5) systematically validate each design choice of AlignVid (scalar scaling, scaling position, block-level scheduling, and step-level scheduling). The inclusion of a human evaluation (Table 11) further strengthens the paper's claims by correlating the proposed VQA-based metric with human judgment."}, "weaknesses": {"value": "1. The primary metric for semantic fidelity hinges on the performance of a VQA model (Qwen2.5-VL-32B). This introduces a potential point of failure, as the evaluation model may have its own biases or \n2. The Block-level Guidance Scheduling (BGS) requires a one-time calibration step that involves using external models (PCA and SAM2) to identify \"foreground-sensitive\" blocks. This adds a layer of complexity and an external dependency compared to a fully self-contained method. The sensitivity of the method to this specific calibration process is not fully explored.\n3. The human study is encouraging. However, did the authors observe any systematic failure modes in the Qwen2.5-VL-32B evaluator itself (e.g., false positives/negatives for specific edit types) when inspecting results? How confident are the authors that the VQA scores fully capture the nuances of semantic adherence?"}, "questions": {"value": "1. The calibration to find \"foreground-sensitive\" blocks relies on PCA and SAM2. How crucial is this specific pipeline? Have the authors explored simpler, heuristic-based block selection strategies (e.g., modulating only the first 50% of blocks) and how do they compare?\n2. The theoretical analysis posits that ASM works by reducing attention entropy. Have the authors empirically measured the attention entropy during generation (e.g., in the targeted blocks/steps) to confirm that AlignVid is indeed reducing it as predicted, and that this reduction correlates with the improved semantic fidelity scores?\n3. It is interesting that the adaptive \"energy-based modulation\" (Table 3) performed worse than simple scalar scaling. Do the authors have any intuition as to why this more complex, adaptive approach was less effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w0csxGqT3h", "forum": "7saU0zPkpZ", "replyto": "7saU0zPkpZ", "signatures": ["ICLR.cc/2026/Conference/Submission15958/Reviewer_eqQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15958/Reviewer_eqQN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983927143, "cdate": 1761983927143, "tmdate": 1762926168048, "mdate": 1762926168048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to address the issue of poor instruction following in Text-Guided Image-to-Video (TI2V) generation (semantic negligence as noted by authors). This occurs when models frequently fail to incorporate complex semantic changes by the text prompt (e.g., object addition or deletion).\n\nThe core motivation comes from a study showing that applying Gaussian blur to the input image unexpectedly improves instruction following. Through visualizing the attention maps, the authors was able to see clear foreground and background separation. The authors link this effect to achieving a lower-entropy attention distribution (i.e., clearer foreground-background separation) within the model.\n\nTo exploit this finding without sacrificing quality, the authors propose AlignVid, a training-free framework. AlignVid deploys two components (i) Attention Scaling Modulation (ASM) which scales the Q/K to directly modulate the attention and (ii) Guidance Scheduling (GS) which determines where to apply ASM.\n\nFinally, the paper introduces OmitI2V, a new human-annotated benchmark focused specifically on quantitative evaluation of instruction following (semantic adherence) across addition, deletion, and modification scenarios. Results reportedly demonstrate the effectiveness of this approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Theoretically Motivated Approach: The authors provide an excellent theoretical foundation, analyzing attention maps to observe clear foreground–background separation. They effectively link this empirical finding to an energy perspective, correctly identifying the mechanism as corresponding to a desirable lower-entropy attention distribution.\n\n* Excellent Writing and Clarity: The paper is well-written, professional, and easy to follow.\n\n* Extensive Validation Across Baselines: The effectiveness of the method is demonstrated through comprehensive studies across multiple state-of-the-art baselines (specifically FramePack, FramePack F1, and Wan2.1). The consistent and significant improvements shown in the OmitI2V benchmark validate the proposed approach."}, "weaknesses": {"value": "* Lack of Established Evaluation Benchmarks: The proposed AlignVid method is exclusively validated using the proposed OmitI2V benchmark. Including evaluation results on established public benchmarks would significantly strengthen the paper's claims and demonstrate generalizability. For example, the ViCLIP metric [1], which assesses video-text semantic alignment, would be particularly useful to include.\n\n* Lack of Generalization Experiments: The proposed AlignVid approach sounds like a general-purpose method that could also be applicable to standard Text-to-Image (T2I) and Text-to-Video (T2V) generation settings, as it operates on the attention mechanism. It would be valuable to see evaluation results demonstrating the effectiveness of AlignVid within these broader contexts.\n\n[1] Internvid: A large-scale video-text dataset for multimodal understanding and generation, ICLR 2023."}, "questions": {"value": "1. Comparison with Classifier-Free Guidance (CFG): Could the authors provide a comparison between the proposed AlignVid approach and Classifier-Free Guidance (CFG)? Since CFG is the widely adopted technique to enhance controllability in generative models, a thorough analysis of how AlignVid performs (either alone or when combined with CFG) is essential for justifying its contribution.\n\n2. Generalization to Broader Tasks: Given that the AlignVid approach operates by modifying the attention mechanism, it appears to be a generalizable technique. Can the authors discuss and provide experimental results demonstrating the effectiveness of AlignVid when applied to standard T2I and T2V generation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Loxu00gtMg", "forum": "7saU0zPkpZ", "replyto": "7saU0zPkpZ", "signatures": ["ICLR.cc/2026/Conference/Submission15958/Reviewer_o7F5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15958/Reviewer_o7F5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762499402038, "cdate": 1762499402038, "tmdate": 1762926167703, "mdate": 1762926167703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}