{"id": "4irIRIxhs2", "number": 22835, "cdate": 1758336054835, "mdate": 1759896843858, "content": {"title": "Quantum Speedups for Sampling and Non-convex Optimization with Stochastic Zeroth Oracles", "abstract": "We propose quantum algorithms with provable speedups for sampling from probability distributions of the form $\\pi \\propto e^{-f}$, where $f:\\mathbb{R}^d\\mapsto \\mathbb{R}$ is a potential function. In particular, we consider access only to a stochastic evaluation oracle, allowing simultaneous queries of the potential value at two different points under the same stochastic parameter. By introducing novel quantum algorithms for stochastic gradient estimation in this setting, our algorithms improve the evaluation complexities of classical samplers, such as Hamiltonian Monte Carlo (HMC) and Langevin Monte Carlo (LMC) in terms of dimension, precision, and other problem-dependent parameters. Furthermore, we demonstrate that our quantum sampling algorithms can be used to achieve quantum speedups in optimization, particularly for minimizing nonsmooth and approximately convex functions that commonly appear in empirical risk minimization problems.", "tldr": "We propose quantum algorithms to reduce the evaluation query complexities of HMC and LMC for sampling and optimization.", "keywords": ["Gibbs sampling", "MCMC methods", "quantum sampling", "non-convex optimization", "quantum gradient estimation", "LMC", "HMC", "quantum algorithm"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90c1a3aec2fda5e3d29a9c4e372329ad54825a52.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel quantum algorithmic framework for accelerating sampling and non-convex optimization in a stochastic zeroth-order setting. The theoretical contribution is significant, as it cleverly combines quantum gradient estimation, quantum mean estimation, and classical sampling theory to provide a provable quantum speedup under specific conditions. However, the paper suffers from fundamental issues regarding the justification of its core model and its general applicability, which currently limit the solidity of its claims and the breadth of its impact."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work presents a comprehensive theoretical framework that integrates tools from quantum computing (gradient estimation, mean estimation) and classical numerical analysis (MLMC, convergence theory for samplers) to achieve an end-to-end complexity analysis with polynomial speedups."}, "weaknesses": {"value": "There are some major concerns affecting the score of this paper, as follows:\n\n1. Questionable Fairness and Realism of the Core Oracle Model\n\nThe paper's technical approach relies crucially on a strong oracle assumption: the ability to query the stochastic function at two different points using the same random seed (i.e., \"reproducible randomness\"). This is a fundamentally more powerful model than the standard stochastic zeroth-order oracle used by the classical baselines, which typically allows for independent sampling on each query. Demonstrating a quantum speedup against classical algorithms that operate in a weaker, standard model is arguably unfair. The claimed speedup might be a direct consequence of this stronger assumption rather than a pure algorithmic improvement. \n\nFurther, this strong assumption severely restricts the generality of the proposed algorithms. They are primarily applicable to finite-sum problems, where fixing the random seed corresponds to selecting a specific data index. For many important real-world problems (e.g., optimization based on physical experiments, interactions with non-stationary systems), the algorithm is not directly applicable. The paper should more explicitly acknowledge this limitation rather than presenting its results as a general \"stochastic zeroth-order\" acceleration.\n\n2. Insufficient Analysis of Quantum Resource Costs\n\nWhile the focus on query complexity is standard for a theoretical paper, the complete omission of other quantum resource costs may mislead readers about the algorithm's practical feasibility. What is the asymptotic scaling of the number of qubits required to construct the phase oracle (Proposition 2.3) and run the robust estimation framework (Algorithm 1)? Is this scaling polynomial in the dimension and the precision? This information is crucial for assessing practical viability.\n\n3. Lack of Comparison with Relevant Quantum Works\n\nThe paper chooses to compare its performance against classical zeroth-order algorithms. However, it lacks a critical comparison with relevant quantum algorithms. For the most natural application scenario—finite-sum optimization—there exist other quantum-accelerated methods. How does the proposed sampling-based framework compare to these approaches? Is it superior in terms of query complexity, generality, or implementation difficulty?"}, "questions": {"value": "1. The introduction and discussion should clearly state that the work relies on a \"strengthened oracle model with reproducible randomness.\"  Meanwhile, the authors should discuss whether this strong assumption is necessary for achieving the speedup. \n\n2. A fairer comparison would be against a classical algorithm that is also granted the same powerful oracle, or the paper should explicitly frame the speedup as being achieved at the cost of reduced generality.\n\n3. A rough asymptotic analysis of the quantum resource requirements should be provided in the appendix or discussion.\n\n4. A dedicated paragraph in the related work should discuss the anticipated performance of the proposed algorithm against existing quantum optimization/sampling algorithms under the same (finite-sum) setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4OvNlUjbG6", "forum": "4irIRIxhs2", "replyto": "4irIRIxhs2", "signatures": ["ICLR.cc/2026/Conference/Submission22835/Reviewer_aUSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22835/Reviewer_aUSH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733147412, "cdate": 1761733147412, "tmdate": 1762942405831, "mdate": 1762942405831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the potential of quantum algorithms to accelerate the optimization of functions with only access to zeroth oracle, in particular nonsmooth and almost convex functions. The authors demonstrate that using quantum mean estimation and jordan's algorithm, it is possible to achieve quadratic speedups over classical methods on various problems. This work derives an algorithm to efficiently and accurately estimate gradients even in the presence of noise and approximation errors. Additionally, this paper shows that the result can be further generalized to non-smooth scenarios via gradient estimation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The algorithm proposed in the paper is explained very well. The paper has a good presentation where it focuses on not only the technical details but also the intuition behind the algorithm. Besides, showing an elegant algorithm for gradient estimation on nonsmooth functions is interesting."}, "weaknesses": {"value": "No significant weekness"}, "questions": {"value": "- Is it possible to also prove a lower-bound for the optimization scenario considered in this work?\n- Would it be possible to extend the result to some extent to non-convex landscapes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K5dPSMl6DS", "forum": "4irIRIxhs2", "replyto": "4irIRIxhs2", "signatures": ["ICLR.cc/2026/Conference/Submission22835/Reviewer_j1qK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22835/Reviewer_j1qK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905565680, "cdate": 1761905565680, "tmdate": 1762942405589, "mdate": 1762942405589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes quantum algorithms that achieve provable speedups for sampling from Gibbs distributions $\\pi \\propto e^{-f(x)}$ and for optimizing nonconvex objectives, when only stochastic zeroth-order (function value) access to $f$ is available. The authors introduce new quantum stochastic gradient estimation methods that improve classical query complexities from$\\tilde{O}(d^2\\sigma^2/\\varepsilon^2) $ to $\\tilde{O}(d\\sigma/\\varepsilon)$ or even $ \\tilde{O}(d^{1/2}\\sigma/\\varepsilon)$ under additional smoothness assumptions. These estimators are then used to obtain LMC and HMC with reduced oracle complexity, leading to polynomial quantum speedups in dimension, precision, and noise parameters for both sampling and optimization tasks. The paper further extends these results to nonsmooth and approximately convex optimization, showing that quantum sampling techniques can yield faster convergence in empirical risk minimization–type problems. Theoretical guarantees are established for all algorithms, assuming fault-tolerant quantum computation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: Introduces the first quantum algorithms achieving provable polynomial speedups for stochastic zeroth-order sampling and optimization, extending quantum gradient estimation to a realistic noisy-oracle model. Provides rigorous convergence and complexity analyses for quantum variants of LMC and HMC, connecting Jordan’s gradient estimation, quantum mean estimation, and MLMC in a novel way.\n- Breadth of applicability: Framework covers both strongly convex and nonconvex settings, and further applies to nonsmooth approximately convex optimization, showing broad theoretical relevance.\n- Clarity: The paper is well-structured, making the logical flow of ideas easy to follow.\n- Significance: Establishes new theoretical baselines for quantum advantages in sampling and optimization, potentially guiding future algorithm design once fault-tolerant quantum hardware becomes available."}, "weaknesses": {"value": "- Proof missing details: could the author explain in section B.1, proof of theorem 3.2, there seems to be a mismatch of $\\kappa$ and $\\sigma$ in the proof and statement. Could the authors clarify a bit on this?\n- Unclear treatment of bounded gradients: Several proofs rely on a global bound $\\|\\nabla f(x)\\|\\le M$ without establishing or bounding M in terms of problem parameters. Could the authors clarify a bit on this?\n- Novelty of their techniques: much of the techniques of quantum speedups seems to come from quantum gradient estimation and mean estimation. Could the authors explain more about their technical novelty?"}, "questions": {"value": "- See the weaknesses part.\n- Line 1372: \"where the last inequality is due to the fact that tails of $\\pi^{\\beta}$ is upper bounded by a Gaussian with variance $\\Omega(1/\\beta)$.\" could the authors explain more about why this holds, especially relying on what kind of assumptions? It seems not immediately clear to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YjopT7xpuz", "forum": "4irIRIxhs2", "replyto": "4irIRIxhs2", "signatures": ["ICLR.cc/2026/Conference/Submission22835/Reviewer_wZj3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22835/Reviewer_wZj3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994404444, "cdate": 1761994404444, "tmdate": 1762942405107, "mdate": 1762942405107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel quantum algorithm for stochastic gradient estimation under various smoothness assumptions, leading to quadratic speedups for smooth potential functions. By leveraging this new stochastic gradient estimation subroutine in zeroth-order sampling tasks, this paper proposes two new quantum algorithms that achieve polynomial speedups over existing classical methods. The application of this approach to non-smooth and approximately convex optimization has also been discussed in the paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A new quantum gradient estimation subroutine is proposed that overcomes the drawbacks of existing gradient estimation methods. In particular, this paper only requires the expectation value of the Lipschitz constant to be bounded. This is a slightly weaker assumption than many previous results. This is achieved by a careful combination of quantum mean estimation and Jordan's gradient estimation algorithm. \n- This quantum gradient estimation subroutine has been applied to both LMC and HMC, and the convergence is analyzed. \n- Applications to noisy, approximately convex optimization problems are discussed. This is a prominent problem class with important applications in ML, such as empirical risk minimization."}, "weaknesses": {"value": "- The distance metric ($W_2$) used in Theorem 3.2 appears to be weaker than those in Theorem 3.4. Is this because the analysis of the base classical algorithm (HMC) is less explored compared to LMC? Does the quantum algorithm improve the distance metric?\n- The approximate convexity assumption (Assumption 4.1) is very weak in high dimension ($d \\gg 1$). Is it possible to relax this assumption further and still obtain quantum speedups? Will quantum algorithms be more competitive in the more \"noisy\" regime?"}, "questions": {"value": "See comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wx6puUbaM2", "forum": "4irIRIxhs2", "replyto": "4irIRIxhs2", "signatures": ["ICLR.cc/2026/Conference/Submission22835/Reviewer_WWpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22835/Reviewer_WWpT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762718711097, "cdate": 1762718711097, "tmdate": 1762942404843, "mdate": 1762942404843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}