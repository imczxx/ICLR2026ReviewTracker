{"id": "LJdAdCo3BN", "number": 15610, "cdate": 1758253122081, "mdate": 1759897295663, "content": {"title": "Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime", "abstract": "Adam [Kingma & Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\\ell_\\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\\ell_2$-max-margin classifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $\\beta_2\\to 1$ and characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\\ell_\\infty$-max-margin classifier for any batch size. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.", "tldr": "We characterize the implicit bias of mini-batch Adam on separable data, showing that it can deviate from the $\\ell_\\infty$-max-margin classifier, whereas Signum consistently preserves the $\\ell_\\infty$-bias for any batch size.", "keywords": ["Adam", "implicit bias", "separable data", "adaptive algorithms", "mini-batch"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6e2d23c7ce7f6e5452a7d7ec91b0d2fd847170b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors study the implicit bias of Adam under stochastic mini-batch settings. Under certain assumptions, authors propose that stochastic Adam has an implicit bias toward a maximum margin solution w.r.t. a specific margin, determined by a data-dependent diagonal matrix. This result is vastly different from the previous work of implicit bias of full-batch Adam [1], and of great importance to the understanding of Adam.\n\n[1] Zhang et al. The implicit bias of Adam on separable data. NeurIPS 2024"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Considering that the real practice of Adam is usually mini-batch instead of full-batch, studying the implicit bias of stochastic Adam is of great importance. \n2. The conclusions of this paper are fairly complete and abundant, covering the stochastic Adam and Signum."}, "weaknesses": {"value": "1. **Strong technical assumptions**: The critical assumption that $\\beta_2\\to 1$ essentially makes the denominator of Adam's updates remain invariant for each data. From the theoretical perspective, such a simplification directly renders that stochastic Adam has the same updating direction as a specific preconditioned full-batch GD, which appears to be the direct reason for the derived implicit bias. The central question, however, is whether this assumption is justifiable for drawing conclusions about the true implicit bias of stochastic Adam in practical settings. This is particularly controversial because, even under the simple incremental setting, for any fixed $\\beta_2 < 1$, the term $\\beta_2^N$ decays dramatically with a large sample size $N$. Consequently, toy experiments with a minimal $N=10$ are insufficient and fail to substantiate the generalizability of the claimed conclusion. In addition, the assumption that Adamproxy has a convergent direction is also unsatisfactory. To some extent, combined with the connection between Adamproxy and preconditioned GD, the final conclusion is somewhat trivial, following the intuitive idea that only support vectors will contribute to the updates like [2]. I know that a similar assumption is also utilized in [3], but [3] does not study the simple linear classification settings, and hence acceptable.  Furthermore, many assumptions could be improved. For example, the assumption (a) in Theorem 3.3. should be rigorously established. In addition, could the author explain why the theoretical derivation for general data has to be built upon Adamproxy, instead of Adam, given that the remainder will converge to 0? \n\n2. **Lack of convergence rate**: The mathematical derivation and calculations do not provide an exact convergence rate for the directional convergence. \n\n3. **Potential issues in the proof**: In the proof of Proposition 4.3, authors finally conclude that $\\\\|\\nabla L(w_t)\\\\|\\_{P^{-1}(w_t)} \\to 0$ since $\\sum \\eta_t \\\\|\\nabla L(w_t)\\\\|\\_{P^{-1}(w_t)}$ is bounded. However, the boundedness of $\\sum a_n b_n$ and the divergence of $\\sum a_n$ do not, in general, guarantee that $b_n \\to 0$ (There exist many counter-examples such that $\\lim\\sup b_n >0$).  Authors should provide more details on how they derive such a conclusion. \n\n[2]. Soudry et al. The implicit bias of gradient descent on separable data. JMLR. \n\n[3]. Xie and Li. Adam exploits $\\ell_\\infty$-geometry of loss landscape via coordinate-wise adaptivity. ICML 2024."}, "questions": {"value": "1. In fact, both the construction of the GR data and the assumption that $\\beta_2 \\to 0$ for general data ensure a critical point: within each epoch, all updates $w_{t+1} - w_t$ corresponding to different data points share the same adaptive term—namely, the denominator (please correct me if this observation is inaccurate). Based on this property, we can interchange the order of summations so that all data points contribute equally to the total update of one epoch. I am curious whether this property still holds if we consider randomly shuffling the data within each epoch, rather than processing them in a fixed order.\n2. Moreover, I suggest that the authors revise their presentation regarding the implicit bias on the GR data, at least in the abstract. This result heavily depends on the specific construction of the data distribution, and I am somewhat skeptical whether it can be legitimately described as an implicit bias of Adam. Therefore, stating the $\\ell_2$-margin conclusion in the abstract without explicitly mentioning its strong reliance on the particularly trivial data setup could be misleading, especially for readers who do not examine the full details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AwL3J3hJtU", "forum": "LJdAdCo3BN", "replyto": "LJdAdCo3BN", "signatures": ["ICLR.cc/2026/Conference/Submission15610/Reviewer_yBsj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15610/Reviewer_yBsj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604538444, "cdate": 1760604538444, "tmdate": 1762925881511, "mdate": 1762925881511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the implicit bias of Adam in the noisy setting and showed it may not converge to $\\ell_\\infty$-max-margin as deterministic Adam. The converged direction depends on the dataset structure and batch size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper explicitly points out that previous literature only considers the implicit bias of Adam under deterministic setting and suggest that noise can have different influence on Adam and signum. It can expand the study of Adam. \n2. There is one rigorous theoretical example on the Generalized Rademacher data that explicitly shows that Adam will converge $\\ell_2$-max-margin. \n3. There are many synthetic experiments that show the solution that stochastic Adam converges to is closer to $\\ell_2$-max-margin, supporting the theoretical example and more general conjecture. The experiments also show that the converged solution of signum is never close to $\\ell_2$-max-margin. \n4. The experiments also show the key role of batch size in deciding the implicit bias."}, "weaknesses": {"value": "1. The theory for stochastic Adam is not complete on general data. After the stage of theorem 4.8, the fixed point analysis can only be done by simulation of algorithm 3, which is less satisfying. Also the proposed AdamProxy almost reduces to AdaGrad. So it is not sure whether the analysis really suits for Adam, which works with a fixed $\\beta_2<1$. \n2. The assumption 4.4 is too strong, which requires the existence of the converged direction. In comparison, Zhang et al., 2024 doesn’t need to assume the existence but directly shows the convergence. \n3. The example of Generalized Rademacher data is not general enough. I feel the result in section 3 can hold for any dataset such that the stochastic gradient $g_t$ always satisfying $|g_t[k]|=|g_t[l]|$ for any $k,l \\in [d]$. However, this case is also too special in the sense that $v_t$ in Adam always satisfies $|v_t[k]|=|v_t[l]|$. The authors intentionally built the example to eliminate the coordinate-adaptivity of Adam but it also makes me wonder whether it can really help understand Adam in the general case. \n4. It would be better to include batch size or noise scale in the framework. Since the figure 6 already shows that batch size 5 can already make the solution deviate from the incremental Adam, it is better to understand the effect the batch size intuitively through theoretical result."}, "questions": {"value": "1. Do you think it is possible to get similar result without assumption 4.4? What is the necessary role of it if we can't remove it?\n2. I only want to point out some possible experiments to explore, which might make the story more complete. But it won't affect my rating and doesn't need to be finished in the rebuttal process. I feel the batch size decides the balance between noise variance and deterministic gradient in $v_t$. When the batch size is small and noise dominates $v_t$, the update rule of Adam will become more similar with SGD in the sense that the effective learning rate is divided by $\\sigma$. When the batch size is large and deterministic gradient square dominates $v_t$, the update rule will be more similar to deterministic setting. Therefore, the shape of the noise may be as important as batch size. Then it's worth to try the following experiments. Fix deterministic gradient function $g(x)$ and manually inject noise $\\delta(t)$ so that the stochastic gradient is $g(x_t)+\\delta(t)$. $\\delta(t)$ can a random variable from Gaussian distribution and we can adjust the covariance matrix. We can multiply it by large constant or small constant and see how the converged direction of Adam changes under different scenario. For example, for what range of signal to noise ratio will the stochastic Adam converge closer to $\\ell_2$-max-margin or $\\ell_\\infty$-max-margin? We can also try covariance matrix that is different from identity matrix. When the eigenvalues are heterogeneous, will incremental Adam still converge to $\\ell_2$-max-margin?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4ugJChd5pZ", "forum": "LJdAdCo3BN", "replyto": "LJdAdCo3BN", "signatures": ["ICLR.cc/2026/Conference/Submission15610/Reviewer_v5Xn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15610/Reviewer_v5Xn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761102293227, "cdate": 1761102293227, "tmdate": 1762925880199, "mdate": 1762925880199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the implicit bias of Adam for binary classification on linearly separable data, particularly the effect of batch size on the learned solution. Building on prior work that shows that iterates of full-batch Adam directionally converge to the $\\ell_\\infty$-max-margin solution, this work shows empirically, and theoretically, under some conditions, that for batch size 1, this is not the case: incremental Adam (using one sample every step) converges to the $\\ell_2$-max-margin solution for some dataset class, while for general datasets, it characeterizes the convergence of proxy-Adam (limit $\\beta_2\\to 1$) using a fixed-point framework. Additionally, the paper shows that other variants of signGD may not share this behaviour: Signum, in the limit of momentum parameters tending to 1, converges to the $\\ell_\\infty$-max-margin solution for any batch size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow, overall. \n\n2. It studies an important problem (what is the effect of batch size on the implicit bias of Adam?), characterizes the implicit bias of per-sample Adam under some conditions and provides interesting insights about how can be different compared to standard max-margin solutions ."}, "weaknesses": {"value": "There are a few weaknesses that the paper should address:\n\n1. The paper is missing discussion on some related works that study the implicit bias of Adam in neural networks [1-2], and effect of momentum parameter values [3] and rotations [4]. There is also another paper [5] on effect of batch sizes on SGD and Adam that should be cited in Section 7.\n\n2. In Fig. 1 (left), the cosine similarity of Adam iterates with the $\\ell_2$-max-margin solution does not converge to 1. This is later clarified in Fig. 3 (and Section 4). However, it could be useful to either i) add another panel in Fig. 1 (similar to Fig. 3 (right)), or ii) add a pointer to Fig. 3 or Section 4 in Fig. 1 caption. Relatedly, the Introduction mainly discusses how per-sample Adam converges “closer to $\\ell_2$-max-margin solution”, and it could be helpful to further clarify this early on, e.g., adding something similar to line 361 in point 3 of the Contributions. \n\n**References:**\n\n[1] Tsilivis et al., “Flavors of margin: Implicit bias of steepest descent in homogeneous neural networks”, ICLR 2025.\n\n[2] Vasudeva et al., “The Rich and the Simple: On the Implicit Bias of Adam and SGD”, NeurIPS 2025.\n\n[3] Orvieto and Gower, “In Search of Adam’s Secret Sauce”, NeurIPS 2025.\n\n[4] Zhang et al., “Understanding Adam Requires Better Rotation Dependent Assumptions”, NeurIPS 2025.\n\n[5] Marek et al., “Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful”, NeurIPS 2025."}, "questions": {"value": "Some suggestions to further improve clarity are as follows:\n\n1. It should be clarified that for vectors, the division is element-wise (e.g., Eq. 2). Relatedly, it can be mentioned in the proof of Cor. 3.2, that shared elements in the denominator allow moving to common division rather than element-wise.  \n\n2. Omit ‘the’ in line 211.\n\n3. More pointers can be added to the Appendix in the main body. For instance, references to proofs, and the additional experiments would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nIY6BVhoOn", "forum": "LJdAdCo3BN", "replyto": "LJdAdCo3BN", "signatures": ["ICLR.cc/2026/Conference/Submission15610/Reviewer_f1uy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15610/Reviewer_f1uy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769956478, "cdate": 1761769956478, "tmdate": 1762925879367, "mdate": 1762925879367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}