{"id": "oFQN4JrriB", "number": 18748, "cdate": 1758290614594, "mdate": 1759897083435, "content": {"title": "Identifying and Reducing \"Generative Collisions\" in Black-Box Large Language Models", "abstract": "Large language models (LLMs) often return near‑duplicate responses to independent users who issue the same prompt even in tasks that demand creativity and uniqueness, a failure mode we call \\textbf{generative collisions}. Existing \"diverse decoding\" techniques---temperature tuning, nucleus sampling, random prompt paraphrasing, etc.---spread a \\textit{single} user’s samples but do not effectively prevent \"collisions\" \\textit{across} users, sessions, or queries, because every independent sample still draws from the same high‑probability basin of the model’s distribution.  This phenomenon is so pervasive that users frequently complain of an output \"sounding like ChatGPT\", implying a homogenization of writing style and culture.\n\nTo minimize generative collisions, we introduce \\textbf{ORBIT} (\"Orthogonal Randomized Buffer Inference Technique\"), a black‑box algorithm that formalizes and combines two approaches for diverse generation: randomization and orthogonalization. ORBIT first initializes a small buffer of completions that are generated to optimize randomness over any other metric (including quality), and then samples the final output(s) to be as divergent from the existing buffer as possible while also maintaining quality. The buffer is instantiated locally and independently for each user session, requiring no cross-user coordination, and yet still minimizes collisions across sessions. ORBIT does not need access to model internals, and is therefore practical for commercial LLM endpoints.  \nWe evaluate ORBIT on 11 tasks---ranging from regex‑scorable toy problems which don't require subjective evaluations to open‑ended creative writing that truly demands authenticity---and show that it consistently decreases collisions against all other black-box methods tested, lowering empirical collision rates by 1–2 orders of magnitude relative to all baselines. ORBIT's implementation is domain agnostic---it accepts a domain name and the original prompt as input---allowing it to be easily generalized to any domain.", "tldr": "We formalize “generative collisions” and show that ORBIT—a black-box randomize-then-orthogonalize sampler with private buffers—wins 11/11 tasks in terms of avoiding near-duplicate responses to creative prompts", "keywords": ["diversity", "LLMs", "inference", "black-box", "creative"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8cbee2be7f5bb856d6be984f9747c619a3157bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce ORBIT, an algorithm designed to solve so-called: \"generative collisions\" for LLMs. These collisions refer to how LLMs are prone to providing near-duplicate answers to different users, when the same prompt is used.\nThe authors argue that standard diversity techniques fail to prevent these inter-user collisions because all users still sample from the model's most probable outputs.\n\nConveniently, ORBIT is a black-box algorithm that can be applied to an LLM without having access to its internal state. Using this method, the authors demonstrate how ORBIT consistently and significantly reduces generative collisions across 11 tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The black-box nature of ORBIT makes it an appealing and practical tool.\nThe method is also seemingly domain-agnostic, and shows promising empirical results on the tasks defined in the paper."}, "weaknesses": {"value": "**Concern 1:**\n\nMy main concern is the motivation for the task the authors tackle, and how this motivation is presented. I would especially argue for a stronger dichotomy between syntactic and semantic diversity. The paper operates on the premise that generative collisions are inherently bad, rather than setting out to prove they are.\n\nIn particular, I find the provided arguments for the importance of generative collisions to be very brief, and near handwavey. For instance the motivating sentence “sounding like ChatGPT” may leave the reader (and me as the reviewer) questioning if having a consistent style is truly undesirable. \nIndeed, one could make the argument that having a consistent LLM style is what users will come to expect. For example “sounding like ChatGPT”, could be analogous to saying “This text reads like Dostoevsky\", which is not always bad…\n\nAnother example is the claim on line 107 “. . . it may also narrow stylistic or semantic variety”. This seems like a rather weak claim.\n\nAdditionally, line 170: “. . . (users expect personalized variation rather than a single canonical answer)”. This statement could really use some empirical evidence.\n\n\nThe most rigorous argument that the authors bring up is a reference to (Sourati, Ziabari, and Dehgani, 2025) in the introduction, which argues that LLM homogenization may be culturally detrimental. During the related work section they provide only a reference to (Miranda et al 2025), whilst claiming “a growing body of work”. The authors do not provide further argumentation or empirical results to the importance of this issue. Instead, the authors proceed with their method and talk about generative collisions as if it's a crucial issue.\n\nMy suggestion would be to strengthen this motivation part significantly.\nPerhaps dedicating a bigger part of the paper to unpacking some claims such as: “Enabling Plagiarism”, “Undermining Originality and Authenticity” etc.\n\n**Concern 2:**\n\nExplanation of experiments.\n\nI feel that the main paper would benefit from being clearer on the tasks used for the experimental setup. Perhaps include a figure of an actual example input -> good/bad outputs. \n\nFurthermore, going over to the appendix you list the tasks in Appendix D, along with the sentence “exact prompt wordings are available in appendix”. This is confusing, as this already resides in the appendix. But, more importantly I could not actually find the exact wordings anywhere else in the appendix? Without these prompts, it is impossible for a reviewer or future researcher to assess the true nature of the tasks or reproduce the results.\n\nThe usage of an LLM judge is reasonable. However, involving human annotators would provide a way to mitigate the weak claims of concern 1. Meaning, you could actually bolster your claims about the importance of your task. Consider doing this for at least a portion of your experiments.\n\n**Concern 3:**\n\nThe limited scope of the experiments.\n\nGiven that the paper positions ORBIT as a practical solution, I argue for experiments demonstrating properties such as: memory consumption, generation time, compute efficiency etc.\n\n**Concern 4:**\n\nIn my opinion, the writing and overall structuring could use some improvements. This also ties in with my concern 1 & 2.\n\nFor example, the full related work paragraph from line 144 - 149, is speculation and not something that related work has done. Consider moving this into perhaps the discussion section.\n\nThe start of section 3.2 (line 222 - 228) is rather confusing. I suggest revisiting this formulation, and not start explaining a method that does NOT work. At least prepare the reader with something like “Here's a naive idea and why it fails…”"}, "questions": {"value": "What, if any, are the overhead costs of running ORBIT. See concern 3"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C8wMsr1Cyi", "forum": "oFQN4JrriB", "replyto": "oFQN4JrriB", "signatures": ["ICLR.cc/2026/Conference/Submission18748/Reviewer_tvRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18748/Reviewer_tvRF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418040304, "cdate": 1761418040304, "tmdate": 1762928463330, "mdate": 1762928463330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the concept of \"generative collisions,\" where independent users receive near-identical outputs from LLMs for the same prompt (even despite different seed values and assuming normal [i.e. low] temperatures). To mitigate this, they propose ORBIT, a two-stage black-box algorithm. First, it generates a hidden buffer of highly diverse (but potentially low-quality) structured generation samples by conditioning on randomized latent variables. Second, it generates the final user-facing output by prompting the model to be maximally different from the contents of this buffer. Experiments across 11 tasks show that ORBIT dramatically reduces collision rates by 1-2 orders of magnitude compared to standard diversity techniques like high-temperature sampling, MBR, and DPP."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The concept of \"inter-user generative collisions\" is a sharp and useful framing of a widely observed LLM failure mode.\n\nThe method is fully black-box, making it immediately applicable to commercial API-based models, which is a major advantage over most academic work in this area!\n\nThe empirical results are outstanding. A 10-100x reduction in collision probability is a massive improvement over existing methods!\n\nThe two-stage \"randomize-then-orthogonalize\" approach is an elegant way to force exploration of the output space without sacrificing quality catastrophically.\n\nI partially agree that novelty is a global property you can't as easily \"sample your way out of\" locally, while quality is is a key insight that crisply motivates the entire approach."}, "weaknesses": {"value": "The core \"orthogonalization\" step relies on natural language prompting (\"be different from these\") and structured generation rather than a more formal mechanism. This may be sensitive to prompt phrasing and model instruction-following capabilities - especially with stuff like chat templates and schemas adding complexity here. \n\nUsing an LLM to judge quality is a significant limitation. Human evaluation would be necessary to truly confirm that the novelty gains do not come at an unacceptable cost to quality, as hinted at by the Napoleon task failure.\n\nThe DERIVESCHEMA step, which generates the latent variables for the randomization phase, feels under-specified to me \n\nThe related work on sampling techniques could be more comprehensive. For instance, there is no mention of Min-p sampling, which also aims to shape the output distribution for quality and diversity by truncating the low-probability tail. Situating ORBIT relative to methods like Min-p would provide a richer context."}, "questions": {"value": "How robust is the orthogonalization prompt in Phase II? Have you experimented with different phrasings? How does performance change if the model is weaker at following complex negative constraints?\n\nThe Napoleon quiz task showed a catastrophic quality failure. In what other domains does ORBIT's aggressive novelty-seeking break factuality, coherence, or adherence to crucial constraints?\n\nWhat is the practical wall-clock and token cost overhead compared to baselines? While the number of calls is matched, the context for the final generation step in ORBIT seems much larger (containing the buffer), which would increase costs and latency, right? I'd like to see a discussion of performance impacts here. \n\nIncredible work. The connection to classic diverse selection criteria is fascinating. Have you considered a more formal implementation of the orthogonalization step that explicitly models an MMR-like objective, rather than relying on natural language prompting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YTSqhhMLoD", "forum": "oFQN4JrriB", "replyto": "oFQN4JrriB", "signatures": ["ICLR.cc/2026/Conference/Submission18748/Reviewer_rdc9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18748/Reviewer_rdc9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626954334, "cdate": 1761626954334, "tmdate": 1762928462769, "mdate": 1762928462769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies inter-user collisions — two independent users issuing the same prompt and getting near-duplicate responses — and argues this is a distinct form of lack of diversity from the usual intra-user setting. It defines a collision probability over a distance metric and threshold, then proposes ORBIT, a two-phase, black-box procedure: first generate “very random” hidden seeds (possibly low quality), then ask the model to generate outputs maximally far from that buffer while keeping quality. On 11 prompt families ORBIT lower measured collision rates than baselines under a reasonable budget."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Targets a real, increasingly visible phenomenon (different people getting the same LLM answer) and gives it a metricized form. \n- ORBIT is black-box, per-session, no cross-user coordination, so it’s a plausible inference-time patch. \n- Empirically strong on their own metric: ORBIT wins on all tasks and stays better across varying threshold of 0.7–0.9.\n- The paper is honest about threats to validity (τ calibration, LLM-as-judge, short/templated prompts)."}, "weaknesses": {"value": "* The text says “a collision occurs iff $(d(x,x′) \\le τ)$” but the formal definition is ($P_{\\text{coll}} = \\Pr[d(x,x′) \\le 1-τ]$). This mixes distance and similarity in one line and makes later numbers slightly unclear; please fix to one canonical version. \n* Motivation could be sharper. Right now “inter-user collisions” is still modeled as *two i.i.d. draws from the same G(p)*, i.e. it is basically paired-sample diversity; the paper does not fully separate product UX concerns (two students submitting same essay) from distributional mode collapse. More concrete, high-stakes scenarios would help. \n* The line “it is very difficult to increase actual entropy … without an unacceptable decrease in quality” is not established in any way, weakening the motivation. \n* Baselines are disadvantaged. Most compared methods are stateless techniques (high-T, DPP, self-consistency, persona cycling). ORBIT, in contrast, explicitly uses session history. Hence this comparison is not fair.\n* In one place Phase I “optimizes randomness over any other metric (including quality),” elsewhere the paper argues quality is locally recoverable by resampling. \n* Binary quality evaluation is OK but not the best."}, "questions": {"value": "- For the “resampling recovers quality” claim, what is the exact resampling budget (calls/tokens) and how does it compare to simply running the best baseline with the same budget? Right now ORBIT seems to get the nicer budget. \n- Figure 2 is really not readable. \n- The intro, in an effort to motivate the setting, leans on the idea that users can just resample i.i.d. to get quality. Are you claiming the ORBIT’s sampler i.i.d.?\n- Why did you decide on your model choices?\n- Please fix various wording and spacing issues. For example 265 the iterator is a step? line 80.5 has an extra space. All citations are not clickable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZF1QeHFhb1", "forum": "oFQN4JrriB", "replyto": "oFQN4JrriB", "signatures": ["ICLR.cc/2026/Conference/Submission18748/Reviewer_wG3F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18748/Reviewer_wG3F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925634419, "cdate": 1761925634419, "tmdate": 1762928462195, "mdate": 1762928462195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}