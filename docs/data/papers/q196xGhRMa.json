{"id": "q196xGhRMa", "number": 3180, "cdate": 1757352226731, "mdate": 1763263431532, "content": {"title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation", "abstract": "Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). With CodePDE, we present a thorough evaluation on critical capacities of LLM for PDE solving: reasoning, debugging, self-refinement, and test-time scaling. CodePDE shows that, with advanced inference-time algorithms and scaling strategies, LLMs can achieve strong performance across a range of representative PDE problems. We also identify novel insights into LLM-driven solver generation, such as trade-offs between solver reliability and sophistication, design principles for LLM-powered PDE solving agents, and failure modes for LLM on hard tasks. These insights offer guidance for building more capable and reliable LLM-based scientific engines.", "tldr": "", "keywords": ["LLM", "PDE solving"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ca0583ccb02af9c05336b2ceda8e31ef6f2dff02.pdf", "supplementary_material": "/attachment/b1c83ca979a950200fb124dff34e8ac88c008b98.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CodePDE, a inference framework for LLMs to generate code for solving PDEs given a problem described in natural language. This is done by treating PDE solving as a code generation task.\nThe paper incorporates LLM capabilities, chain-of-thought, closed-loop debugging and refinement for this task. The paper does analysis into the LLMs skill-sets and tradeoff between LLM reliability and code complexity. The paper demonstrates how various LLMs can be used as agents for scientific computing given the paper's inference strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* LLM abilities are constantly improving. This paper presents an API to leverage LLMs by framing PDE solving as a code generation task, enabling LLMs to produce solver code directly from natural language.\n\n* CodePDE integrates task specification, code generation, debugging, evaluation, and refinement in a structured pipeline.\n\n* The evaluating is systematic. The paper analyses 16 LLMs across five PDE families using metrics like nRMSE, convergence rate, and execution time. The paper clearly presents experimental results and takeaways when answering their experimental questions.\n\n* self-Refinement with coarse feedback (nRMSE) can improve LLM solvers\n\n* Unlike numerical solvers, LLM generated code is human-readable facilitating error diagnosis and transparency. The paper identifies trade-offs between solver reliability and sophistication."}, "weaknesses": {"value": "> Takeaways. LLMs can improve code for better accuracy using simple performance feedback.\nInterestingly, the best models at generating code are not always the best at refining it, suggesting\nthese are two different skills.\n* (L356 above): Perhaps some percentage improvements in table 1 between \" CodePDE: Reasoning + Debugging (best of 32)\" and \"CodePDE: Reasoning + Debugging + Refinement (best of 12)\" would better help justify this claim, where the reasoning is currently unclear.\n\n* Some mixed results between CodePDE and the neural network and foundation model baselines.\n\n> In general, solution quality generally improves with\nincreasing sample count n, with the most significant gains observed between n=4 and n=16.\nBeyond this point, returns diminish, suggesting that moderate sampling budgets often suffice to reach\nnear-optimal performance.\n* (L364 - 266 above): Is it possible to classify the types of errors that each model makes? Such analysis may explain the performance plateau and how to improve the nRMSE lower bound."}, "questions": {"value": "* For each PDE evaluation, what is the behavior of the LLMs over dataset subsets based on difficulties, initial conditions, or characteristics of the PDEs? For a given PDE, is there a relationship between PDE characteristics and CodePDE LLM performance?\n\n> Interestingly, while advanced reasoning models (e.g., o3 and DeepSeek-R1) typically lead to better\nsolvers in the “reasoning + debugging” stage, they are not necessarily better than standard ones\n(e.g., GPT-4o and DeepSeek-V3) in the refinement stage.\n* ( L352 - 355 above ): Are there additional signals that can be passed to the LLM for self-refinement for improved performance? How much information can be transmitted by nRMSE alone and what effects does this have on the LLM's solutions? \n\n> However, LLMs can occasionally introduce redundant operations or inefficient looping structures,\nwhich leads to slower execution.\n* (L842 above): Understanding the reasoning, debugging or refinement iterations / runtimes is an important step in analyzing the efficiency of an LLM driven framework for solving PDEs. What is the relationship between code structures (inefficient loops, readability, redundant operations) and the abilities of the LLMs to improve the quality of their solvers through iterative refinement / chain-of-thought ?\n\n> Takeaways. A low failure rate alone is not a sufficient measure of a model’s capability, the ability\nto generate diverse, high-order numerical methods is also critical. For challenging PDEs where\nsingle-shot generation is prone to failure, test-time scaling is essential for obtaining\ndiverse, correct, and robust solvers.\n* (L407 above): What is the relationship between an LLM's distribution of numerical method order and its PDE solving performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4fPo2tLqVZ", "forum": "q196xGhRMa", "replyto": "q196xGhRMa", "signatures": ["ICLR.cc/2026/Conference/Submission3180/Reviewer_ABqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3180/Reviewer_ABqD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549671506, "cdate": 1761549671506, "tmdate": 1762916586608, "mdate": 1762916586608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "kukX7n4M7T", "forum": "q196xGhRMa", "replyto": "q196xGhRMa", "signatures": ["ICLR.cc/2026/Conference/Submission3180/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3180/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763263430727, "cdate": 1763263430727, "tmdate": 1763263430727, "mdate": 1763263430727, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CodePDE, an inference framework for solving partial differential equations (PDEs) by framing the problem as a code generation task driven by large language models (LLMs). The CodePDE framework consists of five steps: Task Specification, Code Generation, Debugging, Evaluation, and Solver Refinement. The authors conduct a comprehensive evaluation of 16 different LLMs across 5 representative PDE benchmarks.\n\nThe primary contribution is demonstrating that LLMs, when guided by this structured framework, can generate high-quality PDE solvers. These generated solvers are competitive with specialized neural solvers and outperform manually crafted reference solvers on 4 out of the 5 tasks evaluated. The paper emphasizes the critical importance of inference-time techniques such as automated debugging (which improved the bug-free rate from 41% to 84% ) and self-refinement based on nRMSE feedback. Finally, the work highlights the interpretability of this approach, using a failure case (the Reaction-Diffusion equation ) to show how the human-readable code reveals the LLM's algorithmic reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive & Rigorous Experiments: A major strength is the experimental breadth. The authors test 16 LLMs on 5 PDE benchmarks and compare them comprehensively against numerical solvers, specialized software, multiple neural solvers (FNO, PINN, etc.), and other agentic workflows.\n2. Deep Insight on Interpretability: The failure-mode analysis for the Reaction-Diffusion equation in Section 5.7 is excellent. It perfectly illustrates the core advantage of this method over \"black-box\" neural solvers: the generated code is human-readable. This allows researchers to precisely diagnose the model's reasoning failure (i.e., missing the analytical solution trick for the reaction term ), which is critical for high-stakes scientific applications."}, "weaknesses": {"value": "1. Dimensionality Limitation: The evaluation is focused on 1D and 2D PDEs. The true challenge for PDE solvers (the \"curse of dimensionality\") lies in high-dimensional problems. It is unclear how this framework would scale, as the complexity of the solver code (e.g., 3D FDM stencils) would increase dramatically.\n2. Practicality of Refinement Signal: Step 5 (Solver Refinement) relies on nRMSE as the feedback signal, which requires a ground-truth solution. In real-world scientific discovery, the entire purpose of solving the PDE is that the ground truth is unknown. This seems to be a major limitation for practical application.\n3. High Inference Cost: The framework relies on extensive LLM sampling: \"test-time scaling\" (best-of-n, where n=32), iterative refinement (12 samples), and debugging (up to 4 rounds). The total number of LLM calls to produce one high-quality solver appears very high. The paper measures the final solver's execution time but not the generation cost, which is a key practical barrier.\n4. Weakened Novelty of Contribution: The paper's claim to be the \"first inference framework for generating PDE solvers\" is significantly weakened by existing work. There are already a number of published LLM-based solvers for PDEs, like PINNsAgent[1], which also operate by leveraging LLMs to generate solver code for solving PDEs. The existence of such prior art diminishes the claimed novelty of the paper's primary contribution.\n5. Insufficient Baseline Comparisons: The experimental evaluation lacks comparisons against several key baselines. Notably, foundational neural solvers such as DeepONet[2] are omitted from the comparison in Table 1. Furthermore, the paper does not benchmark its performance against other similar and highly relevant LLM-based PDE solvers, such as PINNsAgent[1].\n\n[1] Wuwu, Qingpo, et al. \"PINNsAgent: Automated PDE Surrogation with Large Language Models.\" arXiv preprint arXiv:2501.12053 (2025).\n\n[2] Lu, Lu, Pengzhan Jin, and George Em Karniadakis. \"Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.\" arXiv preprint arXiv:1910.03193 (2019)."}, "questions": {"value": "1. The self-refinement step (Step 5) uses nRMSE, which requires a ground-truth solution. How is this step intended to work in a real-world scenario where no ground truth is available?\n2. The evaluation focuses on 1D and 2D problems. How do the authors envision this framework scaling to 3D or higher-dimensional PDEs?\n3. The experimental evaluation in Table 1 appears to be missing key comparisons, such as some traditional neural operators (e.g., DeepONet), and LLM-based automatic solvers for PDE (e.g., PINNsAgent).\n4. The framework relies on extensive LLM sampling, including best-of-n scaling ($n=32$) 1, 12 refinement samples 2, and up to 4 rounds of debugging3. This implies a significant \"generation cost\" (e.g., total LLM calls or tokens) to produce a single high-quality solver. While the execution time of the final solver is measured4, could the authors provide an analysis of this generation cost? How does this inference overhead compare to the practical cost of implementing a reference solver or training a neural baseline like FNO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Xin3XDyOE", "forum": "q196xGhRMa", "replyto": "q196xGhRMa", "signatures": ["ICLR.cc/2026/Conference/Submission3180/Reviewer_1Crm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3180/Reviewer_1Crm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960462878, "cdate": 1761960462878, "tmdate": 1762916585909, "mdate": 1762916585909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CodePDE, an inference framework that uses large language models (LLMs) to generate executable numerical solvers for partial differential equations (PDEs). The authors frame PDE solving as a code generation task and evaluate LLM capabilities across reasoning, debugging, self-refinement, and test-time scaling. Through systematic experiments on five representative PDE families, they demonstrate that LLMs equipped with their framework can produce solvers competitive with hand-crafted numerical methods and specialized software. The paper also provides insights into trade-offs between solver reliability and sophistication, design principles for LLM-based scientific agents, and failure modes on challenging problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ **Pioneering Exploration**: The work courageously explores a novel paradigm of using LLMs for numerical code generation, opening up new research directions in AI-enabled scientific computing.\n+ **Comprehensive Evaluation**: Extensive benchmarking across 16 LLMs and 5 PDE families provides valuable data for the community.\n+ **Practical Framework**: The debugging and refinement mechanisms demonstrate real utility for improving code generation reliability in scientific contexts.\n+ **Insightful Analysis**: The findings about test-time scaling, numerical scheme diversity, and the generation-refinement skill dichotomy offer meaningful insights.\n+ **Foundation for Future Work**: The framework and evaluation methodology establish a strong baseline for subsequent research in this direction."}, "weaknesses": {"value": "+ **Limited Forward-Looking Insight**: While empirically thorough, the paper misses an opportunity to deeply discuss how this LLM-driven approach might evolve to complement rather than replace traditional numerical methods, and what unique advantages the fusion might bring.\n+ **Practical Deployment Concerns**: The generated solvers, while accurate, lack the performance optimizations and battle-testing of established numerical libraries, limiting immediate practical utility.\n+ **Motivation Gap**: The paper could better articulate scenarios where generating new solvers is preferable to intelligently configuring existing high-performance solvers."}, "questions": {"value": "1. Looking forward, how do you envision the optimal division of labor between LLM-generated solvers and traditional numerical methods? What unique capabilities might emerge from their combination?\n2. Could your framework be adapted to focus more on high-level solver selection and configuration, while leveraging established libraries for the core numerical computations?\n3. What are the most promising research directions for improving the reliability and performance of LLM-generated scientific code to bridge the gap with hand-optimized implementations?\n4. Beyond the metrics you evaluated, what other aspects should be considered when assessing the practical usefulness of AI-generated solvers in real scientific workflows?\n5. How might the interpretability of LLM-generated solvers (as you note) be leveraged to create hybrid human-AI scientific computing systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "52x4RUl5ZW", "forum": "q196xGhRMa", "replyto": "q196xGhRMa", "signatures": ["ICLR.cc/2026/Conference/Submission3180/Reviewer_HkLY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3180/Reviewer_HkLY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996210580, "cdate": 1761996210580, "tmdate": 1762916585747, "mdate": 1762916585747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}