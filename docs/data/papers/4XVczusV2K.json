{"id": "4XVczusV2K", "number": 22498, "cdate": 1758331912544, "mdate": 1759896862530, "content": {"title": "Diverse and Sparse Mixture-of-Experts for Causal Subgraph–Based Out-of-Distribution Graph Learning", "abstract": "Current state-of-the-art methods for out-of-distribution (OOD) generalization lack the ability to effectively address datasets with heterogeneous causal structures at the instance level. Existing approaches that attempt to handle such heterogeneity either rely on data augmentation, which risks altering label semantics, or impose causal assumptions whose validity in real-world datasets is uncertain. We introduce a novel Mixture-of-Experts (MoE) framework that can model heterogeneous causal structures without relying on restrictive assumptions. Our key idea is to address instance-level heterogeneity by enforcing semantic diversity among experts, each generating a distinct causal subgraph, while a learned gate assigns sparse weights that adaptively focus on the most relevant experts for each input. Our theoretical analysis shows that these two properties jointly reduce OOD error. In practice, our experts are scalable and do not require environment labels. Empirically, our framework achieves strong performance on the GOOD benchmark across both synthetic and real-world structural shifts.", "tldr": "We introduce a Mixture-of-Experts framework with diverse experts and sparse gating for graph OOD generalization, with a derived risk bound linking these properties to reduced OOD error.", "keywords": ["graph neural network", "out-of-distribution learning", "mixture-of-experts"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2462b37d4314dd73b5c2b0b30453f1fb1367257f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Mixture-of-Experts (MoE) framework to address the out-of-distribution (OOD) generalization problem for graphs. The method explicitly models the instance-level causal heterogeneity prevalent in real-world data by forcing experts to extract diverse causal subgraph hypotheses and using a sparse gating network to adaptively select the best explanation for each instance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- S1: The paper is well-motivated, with a clear and easy-to-follow logic.\n\n- S2: The paper provides a clear theoretical framework that decomposes the OOD risk into coverage and selection terms. It directly links the proposed diversity and sparsity mechanisms to the goal of reducing OOD error, effectively supporting the paper's core claims and contributions.\n\n- S3: The authors have conducted extensive experiments on standard benchmarks to demonstrate the effectiveness of their method. Furthermore, the ablation studies validate the necessity of each core component of the framework."}, "weaknesses": {"value": "- W1: The core premises of the paper, namely instance-level heterogeneity and the resulting causal diversity, require more formal and clear definitions and introductions.\n\n- W2: Assumption 3.5 appears to be a critical condition for the method to achieve OOD generalization. It states that at least one expert can achieve satisfactory performance in the test environment. This assumption may be too strong, as it implicitly requires that the causal subgraph structures present in the test data must have been seen during training and learned by at least one expert.\n\n- W3: The method relies on training multiple expert networks, yet it lacks a corresponding analysis of computational efficiency. Although the paper claims the method is scalable, it does not provide a quantitative analysis of the relationship between computational cost (e.g., training time) and the number of experts, K."}, "questions": {"value": "- Q1 (Regarding W1): The concepts of instance-level heterogeneity and causal diversity need further elaboration. What is causal diversity, and is it a concept specific to the instance level? A related issue is the assumption of instance-level heterogeneity, which should be explicitly stated. Does this assumption imply that different instances can have different causal graphs? If so, why would different instances from the same data source follow different causal graph structures, and what are the corresponding real-world scenarios?\n\n- Q2 (Regarding W2): My understanding of the method is as follows: different experts are used to extract different subgraph structures based on a diversity constraint. A selection mechanism then assigns the maximum predictive weight to the expert with the lowest prediction loss, and the subgraph extracted by this expert is considered the causal part. Is this understanding correct? If so, the generalization guarantee provided by Assumption 3.5 is, in essence, a strong constraint on the structure of the test graphs. Could you please discuss this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6Ls0TOGMH6", "forum": "4XVczusV2K", "replyto": "4XVczusV2K", "signatures": ["ICLR.cc/2026/Conference/Submission22498/Reviewer_nffD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22498/Reviewer_nffD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760760392379, "cdate": 1760760392379, "tmdate": 1762942244287, "mdate": 1762942244287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a Mixture-of-Experts (MoE) approach for addressing out-of-distribution graph classification tasks. Intuitively, each expert would learn a (perhaps disjoint) subset of the environment distributions. By softly enforcing diversity and sparseness of the expert distribution, the authors ensure that the resulting method both covers the environment space and selects the correct expert for that environment with high probability. Experimental results highlight the effectiveness of the method for graph classification.\n\nIn summary, the work is nicely written, the theoretical analysis (although simple) provides a clear understanding of the method, empirical results are promising, and notation is clean. Although the manuscript presents minor issues (e.g., $l_{sparse}$ is only described in the appendix - with a different name), I am happy to provide a favorable review for the work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method (although unnamed) is clearly described. \n\n2. The theoretical analysis is to-the-point and provides an intuitive understanding for the chosen design.\n\n3. Empirical results highlight the method’s competitive performance.\n\n4. Ad-hoc terminology (e.g., semantic diversity, mechanism coverage) is rigorously defined."}, "weaknesses": {"value": "1. I believe that the main disadvantage of the introduced design is the introduction of many hyperparameters for model regularization. In practice, how can they be selected? Although there are standard approaches in machine learning for hyperparameter selection, these are often expensive.\n\n2. Also, I am a bit doubtful about the use of the word OOD; usually, this means a difference between training and test datasets. It is unclear how it is represented in the experimental analysis in this section. Could the authors elaborate briefly on this?\n\n3. Diversity and sparsity constraints seem to solely lead to marginal improvements over the baseline method - which also presents competitive performance regardless. Are there datasets for which the imposed constraints lead to more significant performance boosts?\n\n4. Could the authors discuss the runtimes for the presented methods in Table 1? Also, when adding further constraints (e.g., diversity and sparsity), the optimization problem changes; does it significantly affect the convergence of the stochastic gradient algorithm?\n\n5. [Minor] $l_{sparse}$ in line 304 uses a different notation in the appendix. \n\n6. [Minor] I could not find the anonymous repository referred to in the manuscript. Could the authors share the code?"}, "questions": {"value": "Please refer to the questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8yDWLWIR7G", "forum": "4XVczusV2K", "replyto": "4XVczusV2K", "signatures": ["ICLR.cc/2026/Conference/Submission22498/Reviewer_q7vv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22498/Reviewer_q7vv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760811781729, "cdate": 1760811781729, "tmdate": 1762942243880, "mdate": 1762942243880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel MoE framework designed to extract diverse subgraphs and address dataset heterogeneity. The proposed approach theoretically demonstrates that both semantic diversity among experts and sparse expert selection contribute to reducing prediction error. Extensive experiments are conducted to validate the framework’s effectiveness across different datasets, highlighting its potential for improving generalization in heterogeneous graph learning scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a novel MOE framework for extracting diverse subgraphs, aiming to address dataset heterogeneity.\n\n2. A theoretical analysis is provided to demonstrate that semantic diversity among experts, together with sparsity in expert selection, jointly contributes to error reduction.\n\n3. Extensive experiments are conducted to validate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. It is necessary to clarify which components are genuinely novel, and whether the proposed method is merely a combination of existing MoE techniques.\n\n2. The total loss comprises multiple terms, so it would be helpful to explain how their relative influences are balanced. More detailed ablation studies are needed to further validate the design, and to examine whether each component is essential for the final performance.\n\n3. Considering the trade-off between performance and computational cost, how many experts are recommended to be used? A detailed analysis or estimation of computational efficiency would also strengthen the work."}, "questions": {"value": "None, see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wKNeMO9iiF", "forum": "4XVczusV2K", "replyto": "4XVczusV2K", "signatures": ["ICLR.cc/2026/Conference/Submission22498/Reviewer_GS29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22498/Reviewer_GS29"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398643453, "cdate": 1761398643453, "tmdate": 1762942243641, "mdate": 1762942243641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a causal subgraph–based MoE framework for graph OOD generalization. The key idea is to consider instance-level different causal subgraph conceptual experts. The authors give a risk decomposition showing that diversity ensures at least one expert matches the unseen environment, and sparsity ensures the gate actually picks it. Empirically, on the GOOD benchmark, the method achieves the best average rank and clearly improves on strong OOD baselines such as LECI and GALA."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper correctly identifies that many causal-subgraph OOD methods assume one invariant causal subgraph per task/environment, but different instances often rely on different motifs.\n2. The idea of learning different causal subgraphs using MoE is fresh. I will explain from a different perspective, though: different MoE learns different spurious correlations. Given enough experts, there will always be one expert learning no spurious correlations with the diversity constraint.\n3. The experimental results are very strong with useful and complete ablations.\n4. The assumption is lighter, easier to extend."}, "weaknesses": {"value": "1. Given the story, there is few interpretability results in terms of how MoE solves the instance-level heterogeneity problem.\n2. LECI provides several enhanced Motif datasets, CFP-Motif, which represent different causal assumptions. The authors may train the three splits together. If the MoE assumption stands, we should observe that, given different underlying causal assumptions, the gate activation should follow clear patterns. Without this experiment or other alternative, the claim will not be justified enough.\n3. The authors show 1, 4, 8 experts, but in MoE literature there is always a question: does diversity collapse or does routing become unstable when K=16/32 on small datasets?\n4. Lack of visualizations of learned subgraphs per expert."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xJkX22WMXl", "forum": "4XVczusV2K", "replyto": "4XVczusV2K", "signatures": ["ICLR.cc/2026/Conference/Submission22498/Reviewer_4z1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22498/Reviewer_4z1L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971686724, "cdate": 1761971686724, "tmdate": 1762942243332, "mdate": 1762942243332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}