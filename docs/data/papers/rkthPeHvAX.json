{"id": "rkthPeHvAX", "number": 14529, "cdate": 1758238143600, "mdate": 1759897364369, "content": {"title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "abstract": "Humans organize knowledge into compact categories that balance compression with semantic meaning preservation. Large Language Models (LLMs) demonstrate striking linguistic abilities, yet whether they achieve this same balance remains unclear. We apply the Information Bottleneck principle to quantitatively compare how LLMs and humans navigate this compression-meaning trade-off.\nAnalyzing embeddings from 40+ LLMs against classic human categorization benchmarks, we uncover three key findings. First, LLMs broadly align with human categories but miss fine-grained semantic distinctions crucial for human understanding. Second, LLMs demonstrate aggressive statistical compression, achieving ``optimal'' information-theoretic efficiency, while humans prioritize contextual richness and adaptive flexibility. Third, encoder models surprisingly outperform decoder models in human alignment, suggesting that generation and understanding rely on distinct mechanisms in current architectures. In addition, training dynamics analysis reveals that conceptual structure develops in distinct phases: rapid initial formation followed by architectural reorganization, with semantic processing migrating from deeper to mid-network layers as models discover more efficient encoding. These divergent strategies, where LLMs optimize for compression and humans for adaptive utility, reveal fundamental differences between artificial and biological intelligence, guiding development toward more human-aligned AI.", "tldr": "Humans compress information to preserve meaning and communication, sacrificing efficiency; LLMs favor statistical patterns over nuance. Bridging this gap could improve LLMs’ conceptual reasoning and human alignment.", "keywords": ["Compression", "Human and Machine Cognition", "Information Theory", "Concepts"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd615b204b4aaf3316ddccdabd38a03f10e43b84.pdf", "supplementary_material": "/attachment/c6c2d656008fbdf1db4021c586e0d23e18622564.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies how LLMs and humans trade-off compression and meaning in conceptual representations. Building on Rate–Distortion Theory and the Information Bottleneck (IB), the authors propose an objective function to combines information-theoretic compression with geometric coherence. Using digitized, classic human categorization benchmarks and embeddings from many models (encoders and decoders), they report three core findings: (i) LLMs broadly align with human category boundaries, (ii) LLMs capture weak typicality gradients relative to humans, and (iii) LLM-derived clusters achieve lower L (greater compression efficiency) than human categories, suggesting humans preserve more semantic richness at the expense of statistical efficiency. The paper releases the digitized benchmarks and positions the framework as a diagnostic for monitoring compression–meaning balance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality. A clear, unified information-theoretic lens that ties together clustering complexity (compression) and semantic coherence (meaning), then uses it to compare humans and LLMs at scale. The encoder-decoder contrast is especially thought-provoking.\n2. Quality. Careful benchmark curation (classic cognitive datasets), comprehensive model coverage, and multi-angle evaluation (category alignment, typicality, and L-curves); The training-dynamics analysis with 57 OLMo-7B checkpoints is helpful.\n3. Clarity. The paper’s structure is easy to follow; figures that separate boundary alignment vs. typicality vs. L-frontiers help the reader parse what “efficiency” means in practice.\n4. Significance. The finding that humans are less efficient but more semantically rich challenges a common “optimality” narrative and invites new objectives/architectures for more human-aligned understanding."}, "weaknesses": {"value": "1. Many correlations are modest. Please provide bootstrap confidence intervals, multiple-comparison controls, and seed variability for clustering and correlation estimates.\n2. The conclusion that humans are statistically suboptimal depends on the chosen geometry and L weighting. Most results fix $\\beta=1$; the paper should report sensitivity curves over $\\beta$ and discuss how human/LLM rankings vary under plausible trade-off weights. Also clarify how human distortion is computed from typicality/membership data to avoid mismatched metrics.\n3. Encoder-decoder gap interpretation. The architecture result is intriguing, but confounded by training objective differences (MLM vs. autoregressive), tokenizers, and pretraining corpora. A controlled study with matched data/tokenizers and frozen-head probes would strengthen the claim."}, "questions": {"value": "1. How exactly is the Distortion term computed for human categories, from typicality distances, membership uncertainty, or an inferred geometry? Please add a short methodological box that maps human ratings to the variance term and discuss limitations.\n2. If typicality is computed against category names, can you replicate with prototype centroids (learned per category) or descriptive definitions, and do encoder-decoder gaps shrink?\n3. With a common tokenizer/corpus and matched parameter counts, do masked-LM encoders still outperform autoregressive decoders on AMI and typicality?\n4. During the mid-layer migration phase, what happens to the complexity-distortion frontier layer-wise? Does the “efficiency” shift track changes in heads/FFNs (e.g., sparsity, attention concentration)?\n5. Can you test the framework on non-noun categories (events, relations) or multilingual replicas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concern."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HFWc4nEkm4", "forum": "rkthPeHvAX", "replyto": "rkthPeHvAX", "signatures": ["ICLR.cc/2026/Conference/Submission14529/Reviewer_X8mp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14529/Reviewer_X8mp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721064009, "cdate": 1761721064009, "tmdate": 1762924924220, "mdate": 1762924924220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts a rigorous analytical re-examination of the boundaries of “understanding” in large language models (LLMs). The authors demonstrate that current LLMs still struggle to master fine-grained semantic distinctions, which is the critical element shaping human-level comprehension. Besides, Information-theoretic optimality does not equate to human-level understanding. Maximizing compression efficiency diverges from achieving semantic alignment fundamentally. In addition, focusing solely on decoder-based language models is unlikely to improve alignment with human comprehension capabilities. These findings reveal a fundamental divergence in the strategies employed by LLMs and humans for understanding natural language: LLMs rely on statistical compression, while humans depend on semantic richness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "(S1) The paper offers a theoretically motivated and quantitatively explicit framework that fuses Rate–Distortion Theory and the Information Bottleneck to provide a new perspective on the compression–meaning trade-off in LLMs. The use of semantic compactness as an internal proxy for meaning fidelity is original.\n\n(S2) The empirical analysis is well-executed and leverages high-quality, historically grounded human categorization datasets. Their public release greatly facilitates reproducibility and establishes a lasting benchmark for semantic understanding in language models.\n\n(S3) The evaluation spans widely of model architectures (encoder, decoder, and static word embeddings) and scales (from 300M to 70B parameters). The inclusion of both static and contextual embeddings, and the analyses across training checkpoints like OLMo-7B, provides a multi-angle validation of the proposed framework’s generality and interpretive depth.\n\n(S4) The paper is written with exceptional clarity and precision, presenting complex theoretical ideas in an accessible and logically coherent manner, making the idea easy to follow."}, "weaknesses": {"value": "(W1) Although the analysis is thorough, one might consider including token-level efficiency statistics in future works, though this does not affect the validity of the current findings.\n\n(W2) The study could be further enriched by considering computational efficiency (e.g., token-level cost) as an additional axis in the compression–meaning landscape. Doing so may illuminate how efficiency interacts with semantic representation in practice.\n\n(W3) This study only scoped in English, it would be interesting as future work to examine whether similar compression–meaning trade-offs hold across multilingual models, given that linguistic granularity may modulate semantic richness.\n\n(W4) The study currently focuses on conceptual categorization, exploring how it extends to relational or compositional understanding could broaden its applicability and lasting its significance."}, "questions": {"value": "1. Could the authors comment on how the main results might change if the distortion term in the $\\mathcal{L}$ objective were replaced with a more cognitively grounded metric, such as human similarity judgments or human-rated typicality scores?\n2. It would be useful to include a brief sensitivity analysis or supplementary report across several $\\beta$ values to illustrate whether the LLM–human divergence remains consistent under varying compression–meaning trade-offs.\n3. Could the authors clarify whether the proposed $\\mathcal{L}$ is intended to generalize beyond categorical tasks, or whether it should be viewed as specific to concept-formation settings rather than to other forms of understanding such as relational reasoning, compositional generalization, or contextual disambiguation?\n4. Would the authors consider adding a brief discussion on what kinds of inductive biases or representational mechanisms might help future models better align with human conceptual structure?\n5. Although the compression is measured at a bit level in the current formulation, could the authors discuss what theory or empirical understanding about the compression at token-level in language models that this framework could provide?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "11pPiwihHy", "forum": "rkthPeHvAX", "replyto": "rkthPeHvAX", "signatures": ["ICLR.cc/2026/Conference/Submission14529/Reviewer_7Xee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14529/Reviewer_7Xee"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945942362, "cdate": 1761945942362, "tmdate": 1762924923829, "mdate": 1762924923829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the relationship between compression and meaning in human and machine representations, proposing that large language models (LLMs) achieve broad categorical alignment with human judgments but lack fine-grained semantic distinctions. The authors frame this as a compression–meaning tradeoff: LLMs optimize for efficient representation (compression) at the expense of semantic richness, while humans maintain inefficient but more structured representations that support flexible reasoning.\nThe study evaluates a diverse set of LLMs across multiple benchmarks of human conceptual categories, comparing model cluster structures (via token embeddings and k-means) with human category structures. Analyses include rate–distortion metrics and mutual information between model-derived clusters and human-labeled categories. Key findings include that LLMs achieve near-optimal compression–distortion tradeoffs, whereas human representations appear suboptimal by information-theoretic measures. Additionally, encoder-only models better align with human judgments than decoder-only models, suggesting differences between recognition and generation mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The study compares a wide range of LLM architectures and sizes.\n* Compression and concept formation is a deep question at the intersection of cognitive science and machine learning.\n* This study links displines between information theory and the prototype theories of human concept learning."}, "weaknesses": {"value": "* The paper takes prototype theory as a given framework for human concept representation, but this theory has been very controversial compared to Exemplar theory (Medin & Schaffer, 1978) as an alternative explanation. Modern cognitive models are usually hybrid and integrate prototype and exemplar components. The manuscript should acknowledge this longstanding debate and should not take for granted that the cognitive system for humans are prototype-like. \n\n* The paper lacks an introduction or figures about the background and the set up for the cognitive experiments that they compare LLMs with, and how they relate to compression-meaning tradeoff.  A schematic figure illustrating the overall task (compression–meaning tradeoff, human vs. model representation pipeline) is missing, especially considering the importantance to introduce the cognitive experiment clearly for this crowd of audience. \n\n*  The paper is difficult to read and repetitive, I would suggest to remove some of the colorful RQ01 blocks. \n- Fig. 1 (left): The figure has only a single square, so the rest of them are decoder architectures? \n- Fig. 2 (right): I would not term this as categorical success\n\n* Unsupported claims:\n    * Line 421: The statement that “human conceptual systems, though appearing suboptimal, serve distinct cognitive needs such as flexible generalization and causal reasoning” is unsubstantiated within the current analysis.\n    * Line 72: “Challenges the popular assumption that statistical optimality equals understanding” — what is the “popular assumption” here? Please clarify.\n\n* line 90: \"cognitive studies applying information theory to human concept learning without connecting to modern llms\", that is not true, see Wu et al. 2025. about prior cognitive modeling work applying information theory to human and LLM, in the context of rate–distortion tradeoffs.\n\n* fig 1 left: there is just one square\n* line 323: I would not term this as gradients"}, "questions": {"value": "* What precisely are the human categories, and how are they measured?\n* How is mutual information calculated between human and model representations, and between human categories and other human categories?\n* What prompts were used for contextual embedding extraction?\n* What exactly is meant by “compression–meaning tradeoff” in cognitive terms?\n* What are the exact prompts used to elicit contextual embeddings from the LLMs? And how much is it deviating from the classical behavioral experiments. \n\nReference:\n\n_Medin, D. L., & Schaffer, M. M. (1978). Context theory of classification learning. Psychological Review, 85(3), 207–238_\n\n_Wu, S., Thalmann, M., Dayan, P., Akata, Z., & Schulz, E. (2025).  Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences. In The Thirteenth International Conference on Learning Representations (ICLR 2025)_"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F5V49dievw", "forum": "rkthPeHvAX", "replyto": "rkthPeHvAX", "signatures": ["ICLR.cc/2026/Conference/Submission14529/Reviewer_zYSK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14529/Reviewer_zYSK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950553142, "cdate": 1761950553142, "tmdate": 1762924923261, "mdate": 1762924923261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper compares how large language models and humans organize conceptual categories, exploring whether models trade compression for meaning in a similar way to people. It digitizes classic psychology datasets on human categorization and analyzes embeddings from over forty models, both encoders and decoders. Using an information-theoretic framework inspired by rate–distortion theory, it quantifies the balance between information compression and semantic fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- very interesting and important premise \n- Systematic, broad comparison covering 40+ models and also layer-wise comparisons.\n- Formulation/digitization of the datasets from cognitive science is a good contribution\n- Transparent discussion on the limitations"}, "weaknesses": {"value": "- The main question lies in how strongly to rely on the proposed metrics to infer “human-like” learning. The information–compression trade-off captures geometric efficiency in embedding space, but it is not clear whether this translates to human-style conceptual abstraction or reasoning.\n- Dependency on parameters and metrics - The authors themselves acknowledge that \"architectural design and pre-training objectives significantly influence a model's ability to abstract human-like conceptual information.\"\nIs cosine similarity to category names, too narrow to capture the richness of human judgments?\n- robustness to different similarity measures, clustering methods, and parameter settings is not tested.\n- How does current large scale frontier models perform under similar analysis ?"}, "questions": {"value": "- Metrics - Need more information about the Adjusted Mutual Information (AMI), Normalized Mutual Information (NMI), and\nAdjusted Rand Index (ARI) metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cu8W3YhVVp", "forum": "rkthPeHvAX", "replyto": "rkthPeHvAX", "signatures": ["ICLR.cc/2026/Conference/Submission14529/Reviewer_E9Bi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14529/Reviewer_E9Bi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990865047, "cdate": 1761990865047, "tmdate": 1762924922748, "mdate": 1762924922748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}