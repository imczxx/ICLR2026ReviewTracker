{"id": "V5PHmFqsiY", "number": 13035, "cdate": 1758212892270, "mdate": 1759897469740, "content": {"title": "Fine-Tuning of Transformer models with Frames", "abstract": "Fine-tuning large-scale pre-trained models for downstream tasks remains a challenge, particularly as model sizes continue to grow. While Parameter-Efficient Fine-Tuning (PEFT) strategies such as Low-Rank Adaptation (LoRA) have emerged as effective solutions,\ntheir memory requirements scale linearly with the size of the model, $\\mathcal{O}(dr)$, where $d$ is the hidden dimension of the model and $r$ is the rank.\nIn this work, we present FrameFT, a novel PEFT method based on Fusion Frames. We model the parameter update $\\Delta W$ with a sparse coefficient matrix in the Fusion Frame representation space. \nIt turns out that Fusion Frames can be generated algorithmically and shared across model layers, enabling highly efficient updates. \nHence, only the sparse coefficients of the basis expansion are stored/optimized, dramatically reducing the memory footprint and parameter count. \nThe sparse structure of the coefficient matrix in FrameFT, together with the sparsity in the Fusion Frames themselves, provides computational benefits compared to other fine-tuning methods.\nOur technical analysis shows that FrameFT allows obtaining formal convergence results.\nWe evaluate our method across a suite of supervised fine-tuning benchmarks, primarily focusing on Language tasks, but also report applicability to Vision models. Our empirical evaluation demonstrates that FrameFT achieves performance on par with or exceeding that of state-of-the-art PEFT techniques, while requiring much fewer trainable parameters and memory.", "tldr": "We propose a new fine-tuning algorithm that uses Frame Theory for sparse fine-tuning", "keywords": ["PEFT", "LLMs", "fine tuning", "fusion Frames", "frame theory", "finite frame theory"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1abe9995993df30f6cfbef235066a1e6e735af1d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new parameter efficient fine tuning (PEFT) method that seeks to model the adapter weights in a Fusion Frame representation space. More precisely the authors show that by modelling the adapter wights as a sparse matrix that lives in a fusion frame representation space the memory footprint and parameter count can be reduced leading to a more efficient PEFT methodology. In order to show that their method yields good performance the authors showcase their method on a collection of supervised fine tuning benchmarks associated to language tasks as well apply their method to some vision tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:** The idea of using sparse adaptors in a fusion frame representation space as adaptors for fine tuning is original as at least from my knowledge of PEFT methods that are currently used within the literature. However, I don't feel the main theorem, namely theorem 3.1 is very original as it uses lemma 1 which is quite a simple lemma that follows essentially from the definition of a fusion frame. While such theorems are not always seen in papers on PEFT methods they are prevalent in many deep learning paper on optimization. Furthermore, the authors don't actually show Theorem 3.1 for a deep network. This seems to be stated for one layer, unless I have miss read something.\n\n**Clarity:** The paper is written well and was easy for me to follow. In general the authors did a good job of clarifying their methods and their ideas and explaining the results of their experiments."}, "weaknesses": {"value": "**Novelty:** It is my feel that the paper lacks novelty. While their idea of looking at a PEFT adaptors from the point of view of fusion frames is nice and new I can't say that there is anything deep and novel going on. I even found the experimental performances on the language tasks shown in the main paper rather underwhelming. If I compare their approach to FourierFT for each model I found that for 3 out of the 5 models it achieves comparable performance to FourierFT and for Gemma-2-2B and Llama-3.1-8b it achieves better performance but only by about approximately 1.5% on the average. I also found this for the result on the GLUE benchmark shown in Table 1, although in the case of the RoBERTa Base their approach beats FourierFT by 1.1%. Furthermore, I didn't find anything novel in the theory. In Theorem 3.1 you are basically showing that a function that has been adapted using fusion frames can converge linearly under gradient descent. But this is for a function not a deep model. In the case of a deep model you have such fusion frame adaptors in each layer so you would need to model a composition. This means you Theorem 3.1 doesn't really apply. Also, I noticed you didn't have any limitations of your methodology.\n\n**Significance:** I don't think the paper will make a significant impact on the community as it seems to just show how one can take a certain different representation of adaptors with no real theoretical insight into why this works out well. Even from the experiments it does comparable to FourierFT in many cases. The results on the ViT's in the appendix seemed to show that their approach did quite well when compared to the others though the authors only considered a ViT-B and a ViT-L model which themselves are rather outdated."}, "questions": {"value": "1. How is Theorem 3.1 useful in the case that my function is an LLM with adaptors employing your fusion frame approach? I think here what you have proved is useful for a one layer model but you need another theorem that shows how the bound changes when you compose functions as deep models are compositions of the layers. \n\n2. I am guessing your point of Theorem 3.1 is to show that your adaptation method leads to a bound on the gradient that can be used to show the rate of convergence of gradient descent? Did you find that with fusion frame adaptation converged faster than the other methods you tested against in the experiments section. I noticed that the scores when compared to say FourierFT were comparable and so my hypothesis would be that your adaptation method would only lead to marginal improvements in convergence speed. Please clarify this for me. Furthermore, don't you fine tune your models with the Adam optimizer? So is Theorem 3.1 really relevant?\n\n3. Do you apply your fusion frame adaptation method to both the attention matrices and feedforward matrices or just one out of the 2? I could have missed this but if this is explained in the paper could you point me towards where you explain how exactly you apply your method in the actual transformer architecture for the LLM experiments and the ViT experiments in the appendix.\n\n4. I noticed you did not really talk about any limitations of your method or at least position some limitations with respect to what is currently done in the literature. Are there any limitations to your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0KkbEawWlc", "forum": "V5PHmFqsiY", "replyto": "V5PHmFqsiY", "signatures": ["ICLR.cc/2026/Conference/Submission13035/Reviewer_6i8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13035/Reviewer_6i8s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645477514, "cdate": 1761645477514, "tmdate": 1762923769777, "mdate": 1762923769777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient parameter fine-tuning method based on Fusion Frames, FrameFT, for efficiently adapting large-scale Transformer models in language and visual tasks. This method significantly reduces trainable parameters and memory usage while maintaining or even improving performance by representing parameter updates with sparse coefficients in multiple overlapping subspaces. The main contribution of this paper lies in introducing the Frame theory into the model fine-tuning framework, providing proofs of convergence and smoothness, and verifying the universality of the method in instruction fine-tuning, language and visual tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The FrameFT parameters and memory efficiency are significant. The weight update is expressed by the sparse coefficients of the overlapping subspace, reducing the trainable parameters and video memory occupation, while maintaining or improving performance.\n2. It has sufficient cross-modal verification, performs excellently in both language and visual tasks, and has strong universality."}, "weaknesses": {"value": "1. The theoretical analysis of the method in the paper is rather abstract. Although there is a formal proof, it lacks an intuitive mechanism explanation for the FrameFT structure to enhance optimization and generalization performance.\n2. The cross-layer shared sparse model lacks empirical support. It is assumed that this structure can be shared across layers, but sufficient verification has not been provided."}, "questions": {"value": "The author can further provide a theoretical explanation or mathematical analysis of the \"adaptive sparse strategy\" to clarify the core logic of designing sparse positions based on task loss and gradient sensitivity, as well as the theoretical advantages of this strategy over random sparsity. Meanwhile, it is suggested that the performance improvement effect of the adaptive strategy be demonstrated through comparative experiments of complex tasks (such as the detailed classification of FGVC Aircraft) and the visualization of the dynamic adjustment process of sparse positions, so as to further deepen the understanding of the optimization direction of sparse patterns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QDstm869Rd", "forum": "V5PHmFqsiY", "replyto": "V5PHmFqsiY", "signatures": ["ICLR.cc/2026/Conference/Submission13035/Reviewer_AUxt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13035/Reviewer_AUxt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653654585, "cdate": 1761653654585, "tmdate": 1762923769063, "mdate": 1762923769063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FrameFT, a new parameter-efficient fine-tuning method based on Fusion Frames. Fusion Frames are pre-computed by decomposing the input and output spaces into multiple subspaces. Interactions between corresponding input–output subspace pairs are captured using a learnable sparse matrix. The Fusion Frames and sparsity pattern are shared across layers, which provides computational benefits. Experiments on both language and vision tasks demonstrate the effectiveness of FrameFT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally clear and well structured.\n\n- The proposed FrameFT method is a novel application of Fusion Frame theory to PEFT. The experimental evaluation spans both NLP and vision domains, providing supporting evidence of its effectiveness.\n\n- Theoretical analysis is provided regarding convergence and computational efficiency, which strengthens the technical contribution."}, "weaknesses": {"value": "- The proposed method appears closely related to existing PEFT techniques, particularly SVFT [1], which also decomposes model weights and introduces a learnable sparse interaction matrix. SVFT additionally explores off-diagonal parameters to capture richer basis interactions. FrameFT instead uses pre-computed Tight Fusion Frames (TFFs) to partition the input and output spaces into fixed-dimension subspaces, but similarly employs a layer-shared sparse interaction matrix. Given these conceptual overlaps, the novelty of FrameFT would benefit from a more explicit discussion of the differences, both theoretically and empirically.\n\n- A direct comparison with recent and relevant PEFT baselines such as SVFT [1], SMT [2], and VeRA [3] is currently missing. Such comparisons are important to assess whether FrameFT provides meaningful gains over the state-of-the-art.\n\n---\n\nReferences\n\n[1] Lingam et al., SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors, NeurIPS 2024\n\n[2] He et al., SMT: Fine-Tuning Large Language Models with Sparse Matrices, ICLR 2025\n\n[3] Kopiczko et al., VeRA: Vector-Based Random Matrix Adaptation, ICLR 2024"}, "questions": {"value": "1. Lines 368–369 mention that constructing Fusion Frames requires only a finite amount of time. Could the authors provide the actual wall-clock time (in seconds) for the configurations used in the reported experiments?\n\n2. The abstract (lines 18–20) states that Fusion Frames can be shared across layers. Were these frames computed once and reused across all compatible layers, or recomputed per layer? Clarification would help better understand the computational trade-offs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SeljCQz4hs", "forum": "V5PHmFqsiY", "replyto": "V5PHmFqsiY", "signatures": ["ICLR.cc/2026/Conference/Submission13035/Reviewer_Z3X9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13035/Reviewer_Z3X9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777306377, "cdate": 1761777306377, "tmdate": 1762923768661, "mdate": 1762923768661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new PEFT method comparable to LoRA for transformer models. Instead of LoRA's low-rank factorization, they adopt the multiple overlapped subspace projections that are motivated from the fusion frame theory. More specifically they select the input output subspace projection matrices from the spectral tetris algorithm (Casazza et al. 2011), fix them, and only learn the inner full coefficient matrices. The method was tested on several language benchmarks (also on some vision benchmarks in the appendix) while compared with some selected PEFT methods like Fourier-FT, S2FT, DoRA, and LoRA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting that they used the less popular (to my knowledge) theory of fusion frame to the PEFT problem.\n- In some scenarios/settings, they showed improvement in performance over some PEFT methods on various benchmarks."}, "weaknesses": {"value": "- First, it would be nice to provide some reference/book on the frame theory in Sec.2 (background) unless you are proposing it in the paper. You can do it at the beginning of the section, like \"The readers can refer to this citation for the introduction of the theory...\"\n\n- In Sec.3 (main) they proposed to replace the left right singular matrices in LoRA by the fusion frame matrices. It looks to me like a logical leap, and there is not much discussion on intuition and motivation. So, why frame theory? How does frame theory play a role here? The fusion frame matrices, assuming Parseval as you actually did, appear to be nothing but adopting overcomplete sets of orthonormal basis in place of the low-rank left and right singular vectors.\n\n- The proposed method also looks similar to LoRA-XS (https://arxiv.org/pdf/2405.17604) which fixes the singular vectors and trains the full inner matrix only. But because in theory we can stack multiple modules or multiple heads in LoRA-XS, it can potentially be similar to having a set of overcomplete orthonormal bases in the end?\n\nLoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters Klaudia Bałazy, Mohammadreza Banaei, Karl Aberer, Jacek Tabor\n\n- Also, is there any ablation study on how effective the chosen spectral tetris algorithm is? Ie, one can easily think of using randomly generated overcomplete orthonormal basis for Pn and Pm via multiple runs of Gram-Schmidt processes and stacking them. Why spectral tetris?\n\n- Why is Theorem 3.1 (convergence analysis) especially unique for the proposed method? I guess it also holds for LoRA and any other variants (eg, LoRA-XS) if we make some bounded parameter assumptions since most PEFT models are Lipschitz smooth. In this sense, Line 161, where they said LoRA can fail to converge, may be an overstatement?\n\n- Another aspect that is not well discussed in the paper is the memory footprint and the inference efficiency. LoRA's efficient memory use is one of its key benefits: LoRA can only store two low-rank matrices. But the proposed approach needs to store the large overcomplete basis matrices (I guess more than (d x d)) or merge into (d x d) matices, which can be less efficient than LoRA especially for on-device applications where many PEFT methods are targeted for. LoRA can use far less memory footprint during forward pass (run sequentially A@x then B@(A@x)), but the proposed approach uses the overcomplete basis (although fixed during training, but have to be stored as a part of the checkpoints, and loaded at the inference time). The number of trainable parameters may not be a big deal in practice, but the inference time matters more. I guess the proposed method has little benefit in this regard.\n\n- I am a bit skeptical about the fact that there are too many similar papers these days on sparse PEFT methods that extend LoRA in a variety of different ways to structure the learnable parameter space, eg, Fourier, SVD, vector-based, partially fixing some parameters and varying the rest, etc. Since most ideas are essentially tweaking a bit the parameter space of the (already powerful) pre-trained model, and there is always a high chance that the fine-tuned model performs better than the pre-trained one. So it may be hard to judge which parameter structure is better than others in a uniform manner in a principled way. I mean showing the performance improvements on some selected scenarios (hyperparameter settings and chosen benchmarks) could be potentially possible for most reasonable parametrization. This is not to criticize your paper, but I think some rigorous theoretical study on the relationship between parameter space and model expressibility needs to be done."}, "questions": {"value": "See questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7e2DQCPArV", "forum": "V5PHmFqsiY", "replyto": "V5PHmFqsiY", "signatures": ["ICLR.cc/2026/Conference/Submission13035/Reviewer_TDeP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13035/Reviewer_TDeP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923552706, "cdate": 1761923552706, "tmdate": 1762923768331, "mdate": 1762923768331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}