{"id": "ZFZhV7Snf4", "number": 24590, "cdate": 1758358245872, "mdate": 1759896759374, "content": {"title": "Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis", "abstract": "A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. \nWe find that autoregressive language models, \nfrom token prediction to sequence-to-sequence tasks, \nare remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50\\% token corruption). \nBy contrast, under the same levels of data corruption, \nclass-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81\\% relative to baseline), \nwhile classifiers show a moderate impact that diminishes with dataset scale.\nTo explain these discrepancies, \nwe analyze the results through a multi-perspective lens, \nintegrating information theory, \nPAC learning, and gradient dynamics. \nThese analyses establish two fundamental principles governing robustness: \nthe **richness of conditioning information**, which constrains the learning problem, \nand the **absolute information content** of the training data, which allows the signal from correct information to dominate statistical noise.", "tldr": "", "keywords": ["data quality; probabilistic model; multi-perspective analysis;"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/951667a59f6a0c5655ab425b3c0b2603952e36c7.pdf", "supplementary_material": "/attachment/62b5487c118658ca11bae0dc72c5d545b0e020ee.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic experimental analysis of the sensitivity of mainstream probabilistic models—specifically autoregressive, diffusion, and classifier models—to low-quality data. The authors identify two primary factors governing robustness: the \"richness of conditioning information\" and the \"absolute information content\" of the training data. The paper further provides theoretical grounding for these findings from the perspectives of information theory, PAC learning, and gradient dynamics."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper provides a systematic and logically self-consistent experimental investigation that effectively demonstrates the varying tolerances of autoregressive models, diffusion models, and image classifiers to low-quality data.\n2. The two key factors proposed as governing robustness—the \"richness of conditioning information\" and the \"absolute information content\"—are insightful and provide a strong conceptual framework for understanding the observed phenomena.\n3. The explanatory analysis accompanying the experimental results is thorough and convincing, particularly the multi-perspective theoretical support that unifies the empirical findings.\n4. The paper's technical details are clearly presented, and the experimental setup appears to be highly reproducible."}, "weaknesses": {"value": "1. The study's focus on training models \"from scratch\" (as noted in Sec 3.5) overlooks the dominant \"pre-training and fine-tuning\" paradigm used in modern practice. The robustness of a large model pre-trained on massive datasets may differ significantly when fine-tuned on low-quality data. This omission limits the direct applicability of the paper's findings to many real-world scenarios.\n2. The analysis relies almost exclusively on unstructured, random noise (as acknowledged in Appendix B). This is a significant limitation, as real-world data noise is often structured or correlated (e.g., systematic mislabeling). It would substantially strengthen the claims to include even a simple experiment with structured noise. For instance, in the classification task, could the authors investigate a scenario where noise is not uniformly random but fixed to specific incorrect labels (e.g., all instances are mislabeled as class 'B')?"}, "questions": {"value": "1. Following on from weakness #2, the paper's conclusions on robustness, especially the explanation rooted in gradient averaging (Sec 4.3), are derived from experiments using unstructured random noise. How well would these principles generalize to real-world, structured noise patterns where the \"noise\" gradients are coherent, not random, and thus would not average out?\n2. The experimental design in Sec. 3.1 attempts to isolate noise effects by fixing the “equivalent correct sample exposure,”but this approach changes both batch size and iteration count, introducing confounding factors. A control experiment with fixed batch size and varying noise ratio would better isolate the true impact of gradient averaging from training configuration effects.\n3. Section 3.3 argues for the effect of context richness by comparing the WMT 2014 translation task (sparse context) with the CNN/DailyMail summarization task (rich context). As these are fundamentally different tasks, it is difficult to attribute the observed difference in robustness solely to the richness of the conditioning information. A more convincing demonstration would involve comparing robustness within the same task.\n4. I am not an expert in this specific area. Based on the paper's clear writing and well-structured experiments, I am open to adjusting my score pending the feedback from reviewers with deeper expertise."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O3vxX2o6BO", "forum": "ZFZhV7Snf4", "replyto": "ZFZhV7Snf4", "signatures": ["ICLR.cc/2026/Conference/Submission24590/Reviewer_rhZG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24590/Reviewer_rhZG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743630340, "cdate": 1761743630340, "tmdate": 1762943130674, "mdate": 1762943130674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how different models behave when trained with low-quality (corrupted) data and proposes a three-part explanation with information-theoretic, PAC, and gradient-based perspectives. Empirically, the authors report: \n1. Autoregressive LMs remain strong even with heavy token/target corruption\n2. Class-conditional diffusion models collapse as label corruption grows (while FID stays roughly flat, the accuracy of conditional generation drops)\n3. Image classifiers are moderately sensitive on small datasets but surprisingly robust at ImageNet scale. \n\nThen they offer interpretations of these empirical observations based on distinct theoretical frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The empirical observation that GPT-2 and ImageNet-scale classification can tolerate, or even thrive under heavy label/target corruption is interesting and perhaps worth documenting."}, "weaknesses": {"value": "Beyond that, the theoretical analysis/explanation they provide seems weak and superficial. Each of the three perspectives allows for straightforward counter-arguments, and the paper stops short of unifying them into a single, predictive theory. The writing is polished and persuasive on the surface, but the theoretical substance is thin; as a result, the paper takes rather longer to read than it should.\nBelow, I provide my view on each perspective the paper provides.\n\n1) Information-theoretic perspective\n\n- Residual information cannot explain “no degradation” at 50% noise. The relative information loss calculation under uniform noise decays with the number of classes but cannot go below $p_e$, which is its asymptotic limit. Hence, this argument cannot by itself explain why the full-ImageNet classifier exhibits no degradation (and even slight improvement) with ~50% incorrect labels.\n\n- The paper argues that the “absolute information content” drives robustness, but in their experiments (to my understanding) the total exposure to correct samples, i.e., absolute information, is held constant across noise conditions. If so, the framework again does not clarify why the full-ImageNet setting is uniquely robust while smaller-scale settings are not, given that the protocol equalizes clean-signal exposure.\n\n2) PAC perspective\n\n- They merely discuss a classical bound, not a targeted explanation. The appeal to $\\Omega(\\epsilon^{-1}\\log\\delta^{-1} + d\\epsilon^{-1} \\log \\epsilon^{-1})$ lower bounds is standard and doesn’t address the core issue in these experiments. Even when the number of correct samples seen is held fixed, additional corrupted samples still inject competing supervision that makes the signal harder to extract. This is simply ignored and the message of Subsection 4.2 reads as if “making the number of clean samples $m$ very large” is the operative solution. However, it does not clarify why $m$ being large should make the training robust to noisy samples.\n\n- The link from “conditioning richness” to effective VC dimension $d$ is hand-wavy. The diffusion example is particularly strained: conceptually, conditional diffusion model jointly learns (i) the data distribution and (ii) a class-conditional guidance. Even with label corruption, part (i) is achieved (as the authors report, the FID remains stable), and what fails is mainly the alignment between the conditional label and the type of the images generated. This is like an unconditional diffusion model equipped with inaccurately trained classifier guidance. Simply attributing this to the larger VC dimension compared to sparse conditions blurs the comprehensive picture. To properly support such a claim, one would need experiments that vary the number of classes in class-conditional generation or also consider text-to-image diffusion setting (richer conditioning) in parallel, and then compare those outcomes. Direct comparison of conditional diffusion model with image classifier does not seem appropriate to me.\n\n3) Gradient-based perspective\n\n- The authors previously mentioned that they use larger batch sizes to stabilize the optimization process under high-noise conditions. However, the reported standard deviations in Table 4 rather show the noisy model exhibiting lower loss variance than the clean baseline at comparable batch sizes. If the loss variance is intended as a proxy for gradient stability, this observation seems inconsistent with their comment on training instability in high-noise regimes. It is genuinely difficult to see what the authors intend to claim with this analysis.\n\n- Line 457 mentions that incorrect gradients are diverse/orthogonal and thus cancel, which might be true considering the statistics of Table 4. However, the paper does not directly test this (e.g., cosine similarity distributions between per-example gradients from clean vs. corrupted samples, magnitude of gradient sum from clean vs. corrupted samples)."}, "questions": {"value": "See Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lqDL3cAO1d", "forum": "ZFZhV7Snf4", "replyto": "ZFZhV7Snf4", "signatures": ["ICLR.cc/2026/Conference/Submission24590/Reviewer_gCmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24590/Reviewer_gCmf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879084933, "cdate": 1761879084933, "tmdate": 1762943130393, "mdate": 1762943130393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this paper provide a review of generative models' evaluation in the context of corrupted (noisy) data, taking into account several levels of noise injection, and explore their corresponding robustness. They observe that autoregressive language models, are  more resilient to low quality data, compared to diffusion models.\nIn order to explain these discrepancies, they analyze the results through employing tools from information theory, PAC learning, and gradient dynamics. Through their analyses they conclude that robustness is affected by two factors, namely the richness of conditioning and the absolute information content of the training data, where the former is driven by the learning problem and the latter is related to the data per se"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "In this paper the authors provide a comprehensive review of two classes of generative models: autoregressive models for text generation and class-conditional diffusion models, in the presence of noisy/low quality data, for several signal to noise ratios.\nIn order to perform their analysis they employ metrics from a spectrum of perspectives, namely information theory, PAC learning, and gradient dynamics. They conclude that the robustness of the method is dependent on the task (richness of conditioning) and the training data (absolute information content)"}, "weaknesses": {"value": "The authors test two different types of generative models under different tasks, for different types of data (categorical and continuous) using different metrics.\nThey do not provide a single task that test these models against, using the same criteria, so in a sense, the comparison in not objective/very informative.\n\nAlso, the results that they present under no corrupted data do not match the ones in the corresponding literature \nFor example the test accuracy of CIFAR-100 was found to be significantly higher than the one reported here, at [1] and [2], namely 84.3% and 75.22% respectively\n\n\n[1] \"Text-to-Image Diffusion Models are Zero-Shot Classifiers\", Clark et al., NeurIPS 2023, and \n[2] \"Better Diffusion Models Further Improve Adversarial Training\", Wang et al., ICML 2023"}, "questions": {"value": "I would like to ask the authors if they could please :\n\n-- compare coth generative models under same task and same metrics;\n-- employ the SOTA of both generative models whilst analyzing their performance"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IG3MS5unWd", "forum": "ZFZhV7Snf4", "replyto": "ZFZhV7Snf4", "signatures": ["ICLR.cc/2026/Conference/Submission24590/Reviewer_XFTw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24590/Reviewer_XFTw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925973591, "cdate": 1761925973591, "tmdate": 1762943130133, "mdate": 1762943130133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "[Please note that, unlike for the other papers in my batch, I am expert only for a subset of the material in this paper, so my review and score is overall pretty reserved/ weak confidence. I will of course participate in the discussion period, and am amenable to changing my score. I apologize if some of my questions seem basic; answering them will nevertheless help me engage more effectively in the discussion period. I will do my best to give a fair assessment, or recommend that the AC down-weight my review if by the end of the discussion period if I have not met my own (I think high) bar for substantive participation.]\n\nThe paper studies how different classes of probabilistic models (autogressrive LLMs, sequence-to-sequence transformers, class-conditional diffusion models, image classifiers) behave when trained with noisy (low quality) data. The work systematically injects controlled random corruption into inputs or labels and compares how performance changes across these architectures/ training dynamics. \n\nHere is my summary of the setup and contributions (please let me know if you think I missed something important):\n\n**Core setup**\n- Noise model: random uniform corruption of tokens or labels; corruption ratios from 10–100% (effective error rate $\\tfrac{r}{1+r}$\n- 4 types of models, as discussed above \n- 2 types of experiments: (a)  constant clean-signal exposure, scales total compute by $1+r$ to keep the number of correct samples constant; (b) fixed budget, noisy data displacing clean data\n- Overall, finds that AR models are pretty resilient; seq-to-seq models with richer conditioning can be made more robust; diffusion models can degrade a ton; classifiers are more sensitive on smaller datasets; some nice results showing that gradient averaging can help"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper has many strengths:\n\n- I don't think I've ever seen a paper put this extent of models into one controlled framework. I think the design here is very careful, and highlights meaningful differences between these model types and training dynamics\n\n- The theoretical framing is clean. It ties together a variety of ideas (information theory, PAC, gradient perpsective) to try to explain what's going on here, to give more explanation beyond the high-level takeaways (e.g., that the richness of the context matters).\n\n- The experiments are well constructed to support the theoretical analysis; they seem very careful and contained. \n\n- Overall, the writing does not over-lcaim causal mechanisms and is very clear about limitations (e.g., scale). I appreciate this a lot, given the ambitious nature of this paper. I think this is a clear/refreshing strength of the work."}, "weaknesses": {"value": "As stated in my summary, please note I am not an expert on all material in this paper. I've combined observations that I think might be weaknesses with related questions.\n\n**Noise model**\n\nAll injected noise is random and unstructured (uniform token or label replacement). In practical settings, low-quality data are often structured (e.g., correlated, systematically mislabeled). Is it fair to say that the results therefore demonstrate robustness to _stochastic corruption_, not necessarily to _realistic dataset noise_?\n\n**Experimental design**\n\nIn the constant/clean-signal experiments, increasing compute by $1+r$ changes the effective LR, regularization. I think the isolation of robustness, as a result, is not entirely clean. Can you please discuss this furhter? \n\n**Theory vs. experiments**\n\nGradient-averaging explanation is intuitive but the experiments don't go into this, unless I missed something. There's no ablation separating variance reduction from bias.\n\n**A couple of instances of slight over-claims**\n\nE.g., I think it is over-generalized to say things like decoder-only LMs are largely insensitive to low-quality data; the writing in general is very careful, but things like this should be hedged. For that experiment, the study uses one model and random noise, not realistic web contamination or fine-tuning noise.\n\n**Novelty re: theory**\n\nThis isn't a big deal, but wanted to ask about it. My take take is that the theory isn't super novel; the principles elicited restate known relationships between sample complexity, conditioning, SNR. What's nice here is the unification, but I don't think it's fundamentally new."}, "questions": {"value": "I've integrated by questions in the \"weaknesses\" section so that they are directly next to observations I've made. I'm hoping this is clearer than separating them out."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QISwkENRWb", "forum": "ZFZhV7Snf4", "replyto": "ZFZhV7Snf4", "signatures": ["ICLR.cc/2026/Conference/Submission24590/Reviewer_rdHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24590/Reviewer_rdHq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762204750814, "cdate": 1762204750814, "tmdate": 1762943129765, "mdate": 1762943129765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}