{"id": "cliPM6kk9J", "number": 13362, "cdate": 1758216983590, "mdate": 1759897442568, "content": {"title": "QAProt: Enabling Sequence-to-Text Protein Function Learning with a Comprehensive QA Corpus", "abstract": "Inferring protein function from sequence is a grand challenge in genomics, yet progress is bottlenecked by the narrow, template-driven datasets available for training. These datasets, derived from structured databases, fail to leverage the rich diversity of knowledge in scientific literature. To address this gap, we introduce **QAProt**, a large-scale corpus with over 987,000 free-form question-answer pairs mined directly from PubMed abstracts, capturing broader topical and linguistic variability than existing resources. To ensure high fidelity, we developed a rigorous multi-LLM cleaning pipeline that yields a 13 times reduction in estimated hallucination rates. Our analyses reveal that current protein LLMs exhibit a performance collapse when tested on the realistic distribution of taxa and functions found in QAProt, highlighting the complementary nature of our literature-derived data distribution. A single epoch of fine-tuning on our dataset yields remarkable improvements, including an 86% performance gain on previously unseen protein domains. QAProt is a complementary new resource that enables the development of more powerful, generalizable models for protein science. Dataset available anonymously at https://huggingface.co/conferenceacc/QAProt.", "tldr": "We present QAProt, the largest and most diverse protein question–answer dataset mined from scientific literature, enabling improved sequence-to-text modeling and zero-shot functional prediction beyond structural insights.", "keywords": ["Question–answer dataset", "Large-scale protein dataset", "Language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e9e475d5a80f10e768be8fb331b6dd6579b1a3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce QAProt, a large dataset of questions about specific proteins derived from PubMed abstracts. They describe a multi-stage pipeline using LLMs to generate questions, match them with protein sequences, and perform quality control. They show that existing specialized language models for protein Q&A fail on the resulting questions, but also that the same models are substantially improved by minimal finetuning thereon."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The dataset is large, diverse, and well-motivated; work like this could help to bridge the divide between existing multimodal language models, which for reasons of data availability are currently limited to text, images, video, and audio, and biological modalities.\n\nThe authors seem attentive to data quality, and I appreciate the steps taken to remove spurious questions."}, "weaknesses": {"value": "There are a few major weaknesses that need to be resolved before I can recommend accepting this manuscript:\n\n1. Based on the examples provided in the paper and my (albeit brief) examination of the data files, there seems to be a mismatch between the author's motivations and the final product; QAProt is not a true \"sequence-to-text\" dataset. For example, it would be a stretch to say that the answers to the questions in Figure 5 (mostly about phenotypic outcomes) can be derived from the corresponding sequence at all. Likewise, many other questions in the set mostly appear to test knowledge of the biomedical literature and not protein sequence understanding. It's not even clear to me based on the manuscript that the language models under evaluation are presented with the protein sequence at all. I think the dataset needs another split that excludes questions not answerable from the corresponding sequence alone.\n2. The dataset needs deduplicating. I personally found tens of copies of several questions (especially basic ones like \"What is the function of [x protein]?\"), together comprising a large fraction of the corpus.\n3. It does not appear that the authors have taken enough care to minimize leakage between train and validation splits; as far as I can tell, the choice of which proteins to hold out did not take sequence homology into account.\n4. While substantial improvements are observed when the baseline models are fine-tuned on QAProt, it's unclear based on the results in the paper how much of that can simply be attributed to format adaptation. It would be good to include additional baselines where e.g. questions from the original training domains of these models are reformatted in the style of QAProt."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PDm8xqLnw6", "forum": "cliPM6kk9J", "replyto": "cliPM6kk9J", "signatures": ["ICLR.cc/2026/Conference/Submission13362/Reviewer_6GXQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13362/Reviewer_6GXQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761005604010, "cdate": 1761005604010, "tmdate": 1762924007908, "mdate": 1762924007908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors used LMs to extract question-answer pairs about proteins' structure+function from the abstracts of scientific papers. This new dataset is highly diverse and can be used for training and benchmarking predictive models. The paper introduces the extraction method, describes the dataset, and presents some benchmarking results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a new, valuable resource to the community. \n\nThe general direction of extracting semi-structured data from papers is a powerful framework that modern LMs have enabled. This work is a good example of the framework.\n\nThere are a number of well-executed technical details, such as how hallucinations are mitigated.\n\nThe paper is frank about weaknesses of the work and what could be improved."}, "weaknesses": {"value": "The primary contribution of the paper is a new, interesting dataset. However, I didn't find the exposition on the composition of this dataset adequate. The paper should include some examples from it, a distribution of the categories depicted as colors in Fig 2A, and a discussion of question difficulty. \n\nI found the evaluation setup confusing. Is the bleu metric adequate for measuring accuracy of this sort of question answering? I'm concerned that the various models, particularly ones trained on templated data, can output things that are semantically correct but that have high bleu from the target text. I would have found it to be more reliable if you had formulated the questions as multiple choice or fill-in-the blank. Is there a precedent for using bleu in modern QA papers? I would have trusted an LM autorater more."}, "questions": {"value": "See my above question about bleu. Can you provide an argument that this is an adequate metric, particularly when the models tend to provide templated outputs? \n\nI found Table 5 confusing, since it doesn't provide any comparisons. How should I be interpreting this result? \n\nI feel that the experiments are convolving two things: (1) predicting information about proteins and (2) formulating free-text responses. What if you used a model that gives structured outputs, such as ProTrek, and then had an off-the-shelf LM use this structured output + the question to formulate a free-text response?\n\n There is a large range of types of questions, yet there is no analysis of models' performance based on the question type. Some questions, such as mutation effect prediction, are likely significantly more difficult than others. Can you do some analysis where you present per-question-type metrics?\n\nI found this comment far too informal: \"The results show that\nQAProt clearly outperforms the others in both semantic/topic coverage and lexical richness(Figure 2).\" Is there a way to make this more rigorous?\n\nIt was unclear to me how the new data is qualitatively different from prior datasets. I understand the point about templating, but this is a superficial detail that concerns how concepts are presented, not the underlying information. In what sense is your data, when ignoring templating, qualitatively different from prior datasets? If you had used a few source datasets, such as Brenda + Uniprot, could you have obtained a similar diversity of facts?\n\n\"Next, we retain abstracts that specifically discuss proteins and genes using UniProt\nAPI. Given an abstract, we iterate over the words in the abstract, and for every word, we make an API call to UniProt to check whether the word is a protein name or not.\"\nThis seems highly inefficient. Why not feed the abstract into an LM to extract a few candidate names and then call the API just on these?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eadEFOkyWG", "forum": "cliPM6kk9J", "replyto": "cliPM6kk9J", "signatures": ["ICLR.cc/2026/Conference/Submission13362/Reviewer_xVvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13362/Reviewer_xVvB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694877051, "cdate": 1761694877051, "tmdate": 1762924007519, "mdate": 1762924007519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce QAProt, a diverse dataset specifically designed to benchmark protein language models in the context of functional annotation. By sourcing questions from PubMed, the dataset aims to address a broader range of protein-related queries, beyond the narrow focus of traditional datasets like UniProt. The authors demonstrate that fine-tuning on QAProt for at least one epoch improves performance on the task of generating protein-to-text descriptions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a detailed experimental setup, offering quantitative evaluations across various protein language models. The authors also provide insight into the LLM filtering pipeline and the prompting strategiesused to mitigate potential biases in the evaluation process. They show a clear concern for evaluation bias and the influence of question formulation during the fine-tuning process, which adds transparency to the methodology.\n \n* The proposition of QAProt as a protein benchmark that incorporates data from PubMed abstracts, rather than relying solely on curated sources like UniProt, is a contribution. This approach presents an opportunity to move beyond the mainstream Gene Ontology (GO)annotations and expand the scope of functional annotation for proteins."}, "weaknesses": {"value": "* While the authors provide valuable experimental results, the paper can be difficult to follow at times, particularly when referencing tables. There are instances where the discussion jumps between tables without clear transitions or explicit mention of table numbers in the text. The current referencing style may confuse readers and make it challenging to track the progression of the argument.\n \n* A notable limitation is the lack of human expert evaluation. While the authors employ various filtration techniques and model-based assessments, protein language models are still prone to hallucination or generating biologically incorrect information. The absence of statistical validation or human expert assessments, from biologists or domain experts, raises concerns about the biological accuracy and real-world applicability of the model's predictions.\n \n* The experimental setup and comparisons between models could benefit from more depth. While the paper outlines the evaluations and presents the results, the analysis of these results remains relatively superficial."}, "questions": {"value": "* Does the paper include any human expert evaluations to assess the biological relevance and real-world applicability of the model’s predictions? Given the complexity of protein function annotation, would human expert assessments provide valuable qualitative insights that are missing from the current quantitative evaluations (e.g., accuracy, recall)?\n \n* How does the paper ensure that the proteins used to generate functional questions are sufficiently biologically similar or relevant to one another? The paper does not provide a clear methodology for confirming protein similarity when generating these questions, which raises concerns about the biological accuracy of the generated queries. Can the authors provide more transparency about the process used to group or pair proteins for this task?\n \n* How does the paper handle the train-test data split, particularly with regard to sequence similarity between proteins in the training and test sets? Is there any consideration of data leakage, such as if test proteins are too similar to training proteins, which could artificially inflate performance metrics? Could the authors clarify the methods used to ensure that training and test proteins are sufficiently dissimilar, or explain how similarity is managed in this context?\n \n* Are the clusters formed biologically interpretable? Do proteins within each cluster share meaningful functional or structural similarities? How do the authors assess whether the clustering method truly captures biologically relevant structures? What evaluation metrics were used to assess the quality of the protein question clusters formed based on embeddings?\n \n* The paper references Table 4 when discussing the results, but the results are actually presented in Table 8 in the appendix. Could the authors clarify the reference to Table 4 and ensure consistency in the presentation of the results to avoid confusion?\n \n* Given the distribution shift between the training data of the baseline models (such as those trained on UniProt/SwissProt) and the more diverse scope of the QAProt dataset, should it not be expected that the protein LLMs would exhibit reduced performance on QAProt, as indicated in Table 4? Additionally, the paper does not mention whether any sequence similarity between the proteins in the original test sets and those in the QAProt test set was considered to mitigate the distribution shift and ensure a fairer comparison. Was any effort made to match sequence similarity or relevance between the test sets to reduce the gap and make the comparison more equitable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e3eYIHg51S", "forum": "cliPM6kk9J", "replyto": "cliPM6kk9J", "signatures": ["ICLR.cc/2026/Conference/Submission13362/Reviewer_FyPV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13362/Reviewer_FyPV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835305322, "cdate": 1761835305322, "tmdate": 1762924007204, "mdate": 1762924007204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new benchmark QAProt for understanding the relationship between protein sequence and function, an important challenge in biology. Unlike existing datasets the authors mine more open-ended question-answer pairs from the literature thus capturing a broader definition of function than that can be found in structured schemas. \n\nThe authors first gather a set of abstracts that discuss protein/gene names. They then use an LLM to generate question-answer pairs from the abstract. They then apply LLMs to filter out examples with hallucinations. \n\nThe authors then benchmark several models with various automatic metrics in both the zero shot and fine-tuned setting, showing that while zero shot performance is poor, finetuning on the data helps significantly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Connecting protein sequence to function is an important problem and mining data from the literature presents a very promising approach for dataset collection. Resources such as this one would be very valuable to the community and help move beyond structured schemas."}, "weaknesses": {"value": "-It would be great to have a more rigorous human evaluation as BLEU etc is often not well correlated with human judgement. \n\n-Would be great to have some understanding of inter-annotator agreement with humans as above and/or for the LLM-as-judge models. \n\n-Would also be great to have more error analysis (e.g. examples that contain hallucinations but pass the cleaning step and/or model errors on the benchmark)\n\n-There is some concern that since the question/answer pairs were mined from the literature (up to 2020), that they are included in the training data of many LLM models and thus models could potentially perform well at this task due to data contamination. Would be curious to see what the authors thoughts are about this."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W3nIcteaja", "forum": "cliPM6kk9J", "replyto": "cliPM6kk9J", "signatures": ["ICLR.cc/2026/Conference/Submission13362/Reviewer_MKnM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13362/Reviewer_MKnM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762228876907, "cdate": 1762228876907, "tmdate": 1762924006873, "mdate": 1762924006873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}