{"id": "r8e7hBhWSG", "number": 3655, "cdate": 1757491992755, "mdate": 1763683836561, "content": {"title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "abstract": "Vision-Language Model (VLM) driving agents promise explainable end-to-end autonomy by first producing natural-language reasoning and then predicting trajectory planning. However, whether planning is **causally** driven by this reasoning remains a critical but unverified assumption. To investigate this, we build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan. Our data generation process converts sensors and annotations into structured inputs and, crucially, separates priors from to-be-reasoned signals, enabling clean information ablations. Using DriveMind, we train representative VLM agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) and evaluate them with nuPlan’s metrics. Our results, unfortunately, indicate a consistent **causal disconnect** in reasoning-planning: removing ego/navigation priors causes large drops in planning scores, whereas removing CoT produces only minor changes. Attention analysis further shows that planning primarily focuses on priors rather than the CoT. Based on this evidence, we propose the Reasoning-Planning Decoupling Hypothesis, positing that the training-yielded reasoning is an ancillary byproduct rather than a causal mediator. To enable efficient diagnosis, we also introduce a novel, training-free probe that measures an agent's reliance on priors by evaluating its planning robustness against minor input perturbations. In summary, we provide the community with a new dataset and a diagnostic tool to evaluate the **causal fidelity** of future models.", "tldr": "We propose and validate the Reasoning-Planning Decoupling Hypothesis for VLM-based driving models, positing that the training-yielded reasoning is an ancillary byproduct rather than a causal mediator to planning.", "keywords": ["Vision language model", "Autonomous driving", "Planning", "Chain-of-thought"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1a8500641fdfc8e1f97a0001aa3d84d13fe0ff5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the causal link between reasoning and planning in VLM driving agents. It introduces the DriveMind dataset and a \"Causal Probe\" to test this. The findings suggest a \"reasoning-planning disconnect,\" where agents use textual priors as shortcuts and reasoning serves as a non-causal byproduct."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper has clear significance, as it addresses the important assumption of causality in reasoning-based VLM agents. The quality of the work is solid, highlighted by the creation of the DriveMind dataset for causal analysis and a structured experimental plan . The \"Causal Probe\" is a novel contribution for diagnostics. The paper is clearly written, and the \"Reasoning-Planning Decoupling Hypothesis\" is supported by the ablation and attention analyses. This work raises valuable questions about the nature of interpretability in these models."}, "weaknesses": {"value": "The paper's setup for CoT generation and training is worth discussing. The CoT is generated by GPT-4.1 from ground truth and then used as a supervision target alongside the plan (Input $\\rightarrow$ [CoT, Plan]). This joint SFT objective might allow the model to learn two separate tasks, both of which could be predicted from textual priors. It would be helpful for the authors to discuss if this disconnect might be an artifact of this joint training. Exploring a sequential setup (e.g., Input $\\rightarrow$ CoT, then [Input, Model's_Own_CoT] $\\rightarrow$ Plan) could provide further insight."}, "questions": {"value": "1. Could the joint SFT objective for both CoT and plan tokens contribute to the observed disconnect? I am curious if the authors considered or tested a sequential training setup (e.g., training a planning module on the model's own generated reasoning) and whether that might change the causal link.\n2. Regarding the \"Causal Probe\" results in Figure 2 , how frequently was the direct contradiction (where the CoT's stated logic mismatches the plan's action) observed across the full test set?\n3. The GRPO results showing reinforced shortcutting are interesting. Could the authors elaborate on the choice of the \"format\" reward and whether they think a different reward (perhaps one that scores the alignment of the CoT with visual evidence) might lead to a different outcome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CV9gCANAd0", "forum": "r8e7hBhWSG", "replyto": "r8e7hBhWSG", "signatures": ["ICLR.cc/2026/Conference/Submission3655/Reviewer_xNnf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3655/Reviewer_xNnf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700315854, "cdate": 1761700315854, "tmdate": 1762916900143, "mdate": 1762916900143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a key assumption in recent VLM-based driving agents: whether CoT reasoning meaningfully contributes to downstream planning decisions. To address this question, the authors introduce DriveMind, a dataset comprising approximately 50K VQA-style samples derived from nuPlan, with GPT-4.1-generated CoT annotations and verified through human validation. The authors fine-tune three VLM models on DriveMind using both SFT and GRPO, conducting systematic ablations that remove CoT, visual inputs, or textual priors.\n\nThe empirical findings reveal a interesting pattern: planning agents without visual information perform comparably to their vision-enabled counterparts, while removing textual priors significantly degrades performance. This suggests that existing VLM driving agents may exhibit shortcut learning by over-relying on textual priors rather than engaging in genuine visual reasoning.\n\nTo mechanistically probe this phenomenon, the paper introduces a sequence-level attention analysis demonstrating that during planning, attention mass concentrates predominantly on textual priors rather than on preceding CoT tokens or image features. Additionally, the authors propose a training-free causal probe to evaluate model robustness under perturbed prior conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. well‑motivated question with a clear negative result.: The paper addresses a fundamental assumption in VLM-based driving: whether CoT reasoning causally contributes to planning. The ablation results are striking: vision-deprived agents (Plan_NoV) achieve performance comparable to full vision+CoT agents on nuPlan metrics, while removing textual priors causes catastrophic degradation. This demonstrates a clear reasoning-planning disconnect and represents an important cautionary finding for the field.\n\n2. Rigorous diagnostics: The paper uses complementary diagnostic approaches: (a) sequence-level attention analysis quantitatively shows that planning attends primarily to textual priors rather than CoT or images, while reasoning generation shows appropriate visual attention; (b) training-free causal probes via perturbations qualitatively expose agent brittleness to prior modifications, revealing contradictions between stated reasoning and actual planning. This multi-faceted evidence strengthens the central claims.\n\n3. Reproducibility: All the training settings and nuPlan metric definitions are carefully documented in Appendix, which is valuable for replication and for the communities to do follow-up analysis."}, "weaknesses": {"value": "1. Limited contributions: While the paper compellingly demonstrates the reasoning-planning disconnect through attention analysis and perturbation studies, it does not consolidate these insights into a standardized, reusable evaluation metric or benchmark. The causal probe remains qualitative (e.g., visual inspection of trajectory contradictions) rather than providing a quantitative robustness score that could be systematically computed and compared across models.\n\n2. Insufficient justification of dataset novelty and quality: The paper positions DriveMind as addressing semantic gaps in nuScenes-based datasets and sim-to-real gaps in CARLA-based datasets, but lacks direct comparative analysis. Specifically: (a) missing comparison table showing how DriveMind differs from existing datasets (DriveLM, DriveVLM, DriveCoT, OmniDrive) in terms of semantic richness, CoT quality, or modular structure; (b) Potential data leakage: CoT generation explicitly provides GPT-4.1 with future expert trajectories, which may introduce subtle leakage despite stylistic constraints; (c) Missing ablation on dataset quality: It's unclear whether the observed disconnect is specific due to DriveMind's distributional bias and data quality, or would generalize to other datasets.\n\n3. Limited experimental scope and generalizability: The primary experiments focus on 7B-parameter models (Qwen2.5-VL, LLaVA-1.6) and an OmniDrive reimplementation. This raises several concerns: (a) Model scale: The findings may not generalize to larger VLMs (like 32B+), which may exhibit different reasoning capabilities and attention patterns. Testing at least one larger model would strengthen generalizability claims; (b) Proprietary models: The absence of closed-source model evaluation (e.g., GPT-4o as a driving agent) makes it unsure whether this is an artifact of open-source VLM training or a broader issue."}, "questions": {"value": "1. Is the reasoning-planning disconnect specific to this model scale and Drivemind dataset, or does it generalize to larger models and other training recipes? Can the authors test larger open-source models or evaluate closed-source models in zero-shot settings to demonstrate broader applicability?\n\n2. Given that textual priors enable shortcut learning, did the authors experiment with alternative input designs or tasks that reduce prior specificity—such as coarse-grained navigation, noisy ego states, or prompts that require explicit visual grounding? This would clarify whether the issue is fundamental or can be mitigated through careful task design.\n\n3. The causal probe perturbs textual priors. Have the authors tried perturbing visual inputs (e.g., masking, noise, scene replacement)? If planning truly relies on textual shortcuts, visual perturbations should have minimal impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ox4x1zGAkw", "forum": "r8e7hBhWSG", "replyto": "r8e7hBhWSG", "signatures": ["ICLR.cc/2026/Conference/Submission3655/Reviewer_Head"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3655/Reviewer_Head"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967316898, "cdate": 1761967316898, "tmdate": 1762916899947, "mdate": 1762916899947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the causal relationship between reasoning and planning in VLM based autonomous driving agents. To facilitate rigorous analysis, the authors introduce the DriveMind dataset, which is specifically designed for causal investigation. The work applies a variety of diagnostic methods—including systematic ablations, perturbation-based causal probes, and sequence-level attention analysis—to comprehensively study how current VLMs achieve driving performance. Across multiple experiments and baselines, the authors find strong evidence for a 'reasoning-planning disconnect,' where agents predominantly rely on textual priors for planning and CoT reasoning is often non-causal."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and empirically validates a central issue in the development of autonomous driving agents: the disconnect between reasoning and planning in VLM-based models.\n\n2. The paper introduces DriveMind, a large-scale nuPlan-based dataset with plan-aligned CoT and modular inputs, enabling rigorous causal analysis and systematic ablations for vision-language driving models.\n\n3. The proposed Causal Probe offers a novel, training-free diagnostic method to identify shortcut reliance, providing a practical tool for evaluating the causal robustness of driving agents."}, "weaknesses": {"value": "1. COT seems to contain many dimensions of scene information (Traffic Light Analysis, Weather Analysis, etc.), but it is unclear whether every case analyzes the same set of dimensions, or whether the CoT content is truly focused on those features most strongly related to the subsequent driving action. The potential redundancy in CoT construction might reduce the relevance and specificity of training signals.\n\n2. In Section 3.1, it is mentioned that “GPT-4.1 is tasked with explaining the causal logic behind the expert trajectory,” but Figure 5 in the appendix shows a prompt where the model is required to generate both the CoT and the trajectory sequence simultaneously. This could lead to confusion about the source of trajectory ground truth, and clarification from the authors would be appreciated.\n\n3. In Section 4.2, the text notes that “the Plan NoV agent, effectively ‘driving blind’ without any visual input, performs on par with the CoT agent.” However, such results could arise from ordinary driving scenarios and basic behaviors. The paper does not provide a detailed distribution of driving scenarios or behaviors within the DriveMind dataset, nor a case study for particularly challenging situations. Supplementary statistics or case studies on scenario diversity and difficulty would help address concerns about evaluation rigor.\n\n4. In Section 4.4, the experiments on the causal probe present only a small set of illustrative cases, which limits the persuasiveness of the findings. It is recommended to include quantitative metrics and analysis to demonstrate that the conclusions hold across a broad range of scenarios."}, "questions": {"value": "1. COT seems to contain many dimensions of scene information (Traffic Light Analysis, Weather Analysis, etc.), but it is unclear whether every case analyzes the same set of dimensions, or whether the CoT content is truly focused on those features most strongly related to the subsequent driving action. The potential redundancy in CoT construction might reduce the relevance and specificity of training signals.\n\n2. In Section 3.1, it is mentioned that “GPT-4.1 is tasked with explaining the causal logic behind the expert trajectory,” but Figure 5 in the appendix shows a prompt where the model is required to generate both the CoT and the trajectory sequence simultaneously. This could lead to confusion about the source of trajectory ground truth, and clarification from the authors would be appreciated.\n\n3. In Section 4.2, the text notes that “the Plan NoV agent, effectively ‘driving blind’ without any visual input, performs on par with the CoT agent.” However, such results could arise from ordinary driving scenarios and basic behaviors. The paper does not provide a detailed distribution of driving scenarios or behaviors within the DriveMind dataset, nor a case study for particularly challenging situations. Supplementary statistics or case studies on scenario diversity and difficulty would help address concerns about evaluation rigor.\n\n4. In Section 4.4, the experiments on the causal probe present only a small set of illustrative cases, which limits the persuasiveness of the findings. It is recommended to include quantitative metrics and analysis to demonstrate that the conclusions hold across a broad range of scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yq6Nk7nGWo", "forum": "r8e7hBhWSG", "replyto": "r8e7hBhWSG", "signatures": ["ICLR.cc/2026/Conference/Submission3655/Reviewer_Q5kf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3655/Reviewer_Q5kf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990385562, "cdate": 1761990385562, "tmdate": 1762916899730, "mdate": 1762916899730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether the reasoning produced by Vision-Language driving agents truly causes their planning behavior. The authors build DriveMind, a nuPlan-based dataset with plan-aligned Chain-of-Thought reasoning, and perform systematic ablation and perturbation studies on several VLM backbones. The main finding is striking: removing reasoning or vision barely affects planning, but removing textual priors causes a sharp drop—suggesting that current models rely heavily on shortcuts rather than genuine reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First large-scale causal analysis explicitly targeting the reasoning-planning link in VLM-driving agents.\n\n2. Multiple baselines (SFT, GRPO, OmniDrive, Llava) and clear ablation logic (with/without CoT, priors, vision).\n\n3. Sequence-level attention analysis and causal probe provide intuitive visual evidence of shortcut learning.\n\n4. Findings question a core assumption in explainable autonomous-driving research and open a new evaluation perspective."}, "weaknesses": {"value": "1. The “disconnect” remains correlational; causal claims rely on ablation heuristics rather than formal interventions or counterfactual reasoning frameworks.\n\n2. All tests are in simulation (nuPlan) with no real-world validation or human-driving baselines.\n\n3. Other explanations (data bias, network capacity, token-level co-training) are not empirically ruled out."}, "questions": {"value": "1. How consistent are these findings across different scene complexities (e.g., intersections vs highways)? Stratify evaluation by scenario complexity to verify whether shortcut reliance strengthens under challenging conditions will be a good bonus.\n\n2. Could token-level leakage between reasoning and planning outputs (e.g., implicit trajectory hints in CoT) explain part of the observed independence?\n\n3. How sensitive are the causal-probe results to the perturbation magnitude δ? It's better to see some experiments between the results and the perturbation level."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WIMdXmRsGt", "forum": "r8e7hBhWSG", "replyto": "r8e7hBhWSG", "signatures": ["ICLR.cc/2026/Conference/Submission3655/Reviewer_zZua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3655/Reviewer_zZua"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213387571, "cdate": 1762213387571, "tmdate": 1762916899480, "mdate": 1762916899480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}