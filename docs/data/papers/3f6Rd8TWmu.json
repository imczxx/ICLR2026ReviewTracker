{"id": "3f6Rd8TWmu", "number": 18069, "cdate": 1758283480394, "mdate": 1759897135053, "content": {"title": "FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits", "abstract": "The performance of large language models (LLMs) is highly sensitive to the input prompt, making prompt optimization a critical task. However, real-world application is hindered by three major challenges: (1) the black-box nature of powerful proprietary LLMs, (2) the need for high sample efficiency due to query costs, and (3) the desire for privacy-preserving collaboration among multiple users. To address these challenges simultaneously, we introduce a novel framework for sample-efficient federated prompt optimization based on multi-armed bandits (MABs). The MAB framework is uniquely suited for this problem as it is (1) inherently a black-box optimization method, (2) practically sample-efficient, and (3) enables collaborative learning with theoretically guaranteed benefit from more participating agents. We first propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a federated variant of the Linear UCB algorithm, where agents collaborate by sharing model parameters instead of raw data. We then extend our approach to the practical setting of comparative user feedback by introducing FedPOB with Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated dueling bandits. Extensive experiments demonstrate that both FedPOB and FedPOB-Pref significantly outperform existing baselines and that their performance consistently improves as more agents participate in the collaboration, validating the effectiveness of our federated approach.", "tldr": "We propose federated prompt optimization algorithms based on multi-armed bandits to efficiently optimize the prompts for black-box LLMs, considering both score and preference feedback.", "keywords": ["Prompt Optimization", "Neural Bandits", "Dueling Bandits"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a4ced28a97f33ebb09cca5f2548d57189c5b600.pdf", "supplementary_material": "/attachment/7e44df93adad5e74a4750d050a55579f2d7b29c3.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenges of large language model (LLM) prompt optimization, particularly the challenges imposed by black-box access, the requirement for sample efficiency, and the necessity of privacy-preserving collaboration. To resolve these issues simultaneously, the authors introduce Federated Prompt Optimization via Bandits (FedPOB) which formalises the problem as a Multi-Arm Bandit problem and apply federated algorithms. The problem settings spans both scalar and comparative preference feedbac. Their experiments confirm that both the FedPOB algorithm and its comparative extension, FedPOB-Pref, significantly surpass existing baselines, with performance reliably increasing as more agents participate in the federated collaboration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem setting the authors consider is well-formulated and clear. Accordingly the paper is well-written and easy-to-follow.\n2. The FedPOB algorithm has a solid theoretical foundation from the standard Multi-Armed Bandits literature that offers convergence guarantees and shown to improve with more collaborating agents.\n3. The authors extend the problem to the setting with preference feedback that is common for subjective tasks with LLMs such as creative writing.\n4. Evaluations span many model families and standard benchmark tasks to provide reliable results."}, "weaknesses": {"value": "1. The paper relies heavily on the linearity assumption which is not tested. Empirical evidence that support this assumption could further solidify the results of this work.\n2. There are no theoretical guarantees for the preference feedback setting in Section 4.2. While the authors motivate FedPOB-Pref with inspirations from duelling bandits and federated learning, only experimental evidence is provided for its performance.\n3. The preference feedback setting is constrained to the BTL model which has known limitations. Formulation where the pairwise comparison $P(p_{t,1} \\succ p_{t,2})$ is modelled directly would make the approach more applicable and general."}, "questions": {"value": "1. Could the authors provide concrete examples to the motivation of federated learning for prompt optimisation? While the motivation for prompt optimisation is clear and why the bandit formulation can resolve the challenges of LLMs being black-boxes and costly to query, I struggle to find a practical example for federated prompt optimisation.\n2. Does FedPOB work asynchronously? Is there any point when an agent has to wait for a weight update from the Central Server? For example, let's assume that there are two agents. Agent 1 accumulates information quickly and send updates to the central server every round while Agent 2 accumulates information slowly and only reaches the threshold in Line 8 Algo 1 every 10 rounds. Can multiple updates happen from Agent 1 without any from Agent 2? How would such an extreme setting affect performance?\n3. In line 243 the authors claim that in many practical applications only preference feedback is available or it is more applicable. What are exactly these applications?\n4. How does the acquisition logic (Line 3-4 in Algo 3) relate to previous works on Duelling Bandits? For example, it is similar to BAI version of the algorithm presented in [1]. Would the algorithm work with other approaches from the literature?\n5. In Section 4, does the number of samples observed and used for learning increases with the number of agents? Is the batch size fixed per agent or per iteration? For a consistent evaluation, it would be valuable to show performance as a function of number of samples seen by all the agents.\n\n[1]: Sample-Efficient Alignment for LLMs, Liu et al. (2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4TSnnrgYXm", "forum": "3f6Rd8TWmu", "replyto": "3f6Rd8TWmu", "signatures": ["ICLR.cc/2026/Conference/Submission18069/Reviewer_uKNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18069/Reviewer_uKNR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737933628, "cdate": 1761737933628, "tmdate": 1762927855622, "mdate": 1762927855622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FedPOB, a sample-efficient, privacy-preserving framework for black-box federated prompt optimization built on multi-armed bandits, addressing three deployment constraints at once: no gradient/parameter access, high query cost, and multi-user collaboration. It proposes two algorithms: FedPOB (a federated LinUCB that shares/aggregates linear bandit statistics instead of raw data) and FedPOB-Pref (a federated dueling bandit variant that aggregates parameters with dynamic regularization to handle noisy preference feedback efficiently). Experiments on Instruction Induction and BBH with API LLMs show consistent gains over baselines and monotonic improvements as more agents participate, while exposing a controllable performance-vs-communication trade-off via a thresholded sync rule."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem this paper study is interesting and important. It provides a score based methods and a preference based method.\n2. Extensive experiments show that the proposed method outperforms baselines."}, "weaknesses": {"value": "1. Why the rewards are decreasing as the communication rounds increase is Figure 4. \n2. The reviewer is curious which experiment shows and compares the sample efficiency as this is a main assertion from the paper. \n3. What is the unique contribution of the paper or the unique challenge that this solves in the FL scenarios. It looks like the paper is making a sophisticated algorithm to its FL version."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iuvhsdzy47", "forum": "3f6Rd8TWmu", "replyto": "3f6Rd8TWmu", "signatures": ["ICLR.cc/2026/Conference/Submission18069/Reviewer_4GM4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18069/Reviewer_4GM4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864110391, "cdate": 1761864110391, "tmdate": 1762927854953, "mdate": 1762927854953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting algorithm, FedPOB, to allow distributed agents to collaborate on prompt optimization. The setting is that all the agents share the same task they want to solve but may have different prompt sets, and they want to collaborate together to help each of them find their optimal prompts. The algorithm works by having each agent use a linear model of an embedded prompt to predict the score of the solution, and training the linear model in a federated averaging-like manner. The prompts in each iteration for each agent are chosen to maximize the linear model's UCB to balance exploration and exploitation. The paper demonstrates natural scaling law such as improved performance with more agents and improved empirical performance compared to prior prompt optimization methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper looks at prompt optimization in the setting where there are distributed agents. This is a very timely paper which looks at a problem that I think is going to become more important.\n- The methods are simple and easy to understand; I can imagine them working in a wide variety of settings\n- Thorough experimental evaluation. Experiments across GPT and Qwen show that the conclusions apply to different models. Experiments across tasks and datasets show wide applicability across tasks.\n- Follows the typical FL protocol, which makes it seem realistic in setting and sounds productionizable\n- The extension to the preference learning setting is good for improving the applicability of the method."}, "weaknesses": {"value": "- The setting is strange. Heterogeneity is only in the prompt set but not in the tasks? I would actually rather the setting be flipped: heterogeneity in tasks (x,y pairs) but prompt set is the same. Distributed agents will probably encounter different sets of prompts, while their prompt sets would probably be the same.\n- It may be better to add some ablations using nonlinear critics. There's no real reason to limit ourselves to linear models practically is there?\n- Should probably discuss the Private Evolution line of work. It has a similar idea: use scores to iterate on black box LLM prompts. Including a discussion of this work and how it is related is likely best.\n\nLin, Zinan, et al. \"Differentially private synthetic data via foundation model apis 1: Images.\" arXiv preprint arXiv:2305.15560 (2023).\n\nXie, Chulin, et al. \"Differentially private synthetic data via foundation model apis 2: Text.\" arXiv preprint arXiv:2403.01749 (2024).\n\nLin, Zinan, et al. \"Differentially private synthetic data via apis 3: Using simulators instead of foundation model.\" arXiv preprint arXiv:2502.05505 (2025).\n\nSome follow ups of Private Evolution also look at the federated setting.\n\nHou, Charlie, et al. \"Private federated learning using preference-optimized synthetic data.\" arXiv preprint arXiv:2504.16438 (2025)."}, "questions": {"value": "- I think my main concern is with the setting of this paper. Why would we have heterogeneous prompts instead of heterogeneous task datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RAtYcZL8dp", "forum": "3f6Rd8TWmu", "replyto": "3f6Rd8TWmu", "signatures": ["ICLR.cc/2026/Conference/Submission18069/Reviewer_CFze"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18069/Reviewer_CFze"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031541524, "cdate": 1762031541524, "tmdate": 1762927854083, "mdate": 1762927854083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a federated framework (FedPOB) for prompt optimization for agents in decentralized settings by casting prompt selection as a linear contextual bandit over fixed prompt embeddings. The core idea of this framework is that Multiple agents collaborate by exchanging only lightweight sufficient statistics instead of raw data. The framework supports both numeric-score feedback (FedPOB, a federated LinUCB) and pairwise-preference feedback (FedPOB-Pref, a federated dueling bandit with reduced communication). This method is theoretically justified and with bandwidth efficiency (message overhead of $O(d^2)$), where the synchronization is event-triggered once the threshold D is reached; in each round, agents upload their increments and receive aggregated globals. Empirical results show that this method is sample-efficient and performance-effective over recent baselines on diverse tasks and could scale to a medium to large number of agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This method is effective with closed-weight/API LLMs, where we only needs scalar scores or pairwise preferences. No gradient information is required, and there is no raw data shared across agents.\n\n2. The linear-bandit pooling lets multiple agents discover good prompts in a fast manner; the framework also has an event-triggered synchronization system that balances the communication frequency and updates with a threshold D, allowing many local trials between rounds, and performance improves as agent number increases.\n\n3. The design is simple and can work with models in any scale with low-bandwidth requirements. The message size is $O(d^2)$, where d is usually no more than 1e4 scale."}, "weaknesses": {"value": "1. The experiments use GPT-3.5-turbo, GPT-4o-mini, and Qwen3-235B on Instruction Induction and BBHâ€”both are LLMs, which are primarily generation-style evaluations. There's no encoder-only (e.g., RoBERTa/GLUE) study, so applicability to pure encoder tasks isn't demonstrated. If this is by-design short-handed, the author should acknowledge this limitation.\n\n2. Limited evaluation on tasks with mixture of models in diverse scales. For instance, what will the performance look like if some models are significantly smaller (thus less capable) than others (i.e., DeepSeek-R1-671b with OPT-125m)?\n\n3. FedPOB models expected reward as linear in a fixed prompt embedding. The paper does not test what happens if the model is mis-specified (i.e., non-linear reward surfaces) nor compare against non-linear bandits (e.g., kernelized UCB, neural bandits). As a result, it's unclear how performance degrades when the embedding is misspecified or interactions are non-linear.\n\n4. While the bandwidth cost is low, the framework is communication-heavy, with each trigger requiring two communications across many agents. The main content of the paper focuses primarily on task performance rather than efficiency. The author should provide experiments with end-to-end wall-clock latency/throughput."}, "questions": {"value": "1. The author claims in lines 80-81 that, with more agents involved, performance improves. Is there any scaling law/ patterns?\n\n2. Can this method scale to more than 10 agents, and does this method assume the ability of the agents is similar (as questioned also in weakness 2)?\n\n3. The author assumes the preference feedback is generated by the Bradley-Terry-Luce (BTL) model, which is strong and arbitrary. Does this assumption actually hold under a multi-agent setting? What happens if this assumption is violated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YgzlnWJEnB", "forum": "3f6Rd8TWmu", "replyto": "3f6Rd8TWmu", "signatures": ["ICLR.cc/2026/Conference/Submission18069/Reviewer_2qph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18069/Reviewer_2qph"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137840872, "cdate": 1762137840872, "tmdate": 1762927853704, "mdate": 1762927853704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}