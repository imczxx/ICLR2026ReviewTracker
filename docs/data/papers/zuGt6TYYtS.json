{"id": "zuGt6TYYtS", "number": 4459, "cdate": 1757683625298, "mdate": 1759898031371, "content": {"title": "FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning", "abstract": "Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at https://anonymous.4open.science/r/GRPO_speculative-2CD3.", "tldr": "", "keywords": ["large language models", "group relative policy optimization", "speculative decoding", "acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a69c2724c980c7e4d0eb14910627a5a658890e2d.pdf", "supplementary_material": "/attachment/5e9cda4b68723ae42914abf0681e4f8e7fc630df.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a method to incorporate speculative decoding into GRPO to accelerate the rollout process in reinforcement learning. The authors argue that directly applying speculative decoding is inefficient because the original algorithm does not handle high concurrency. To address this, they propose FastGRPO, which improves concurrency efficiency, and further suggest updating the draft model online. Experiments show clear speedup gains, and the paper is generally well written with comprehensive coverage of datasets and models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a clear and important bottleneck in RL training. The motivation is sound, the method is technically reasonable, and the presentation is clear. The acceleration results are convincing, and the experimental setup is thorough and well-organized."}, "weaknesses": {"value": "The main concern is that the paper does not analyze how the proposed approach affects RL performance. Both the method and the experiments focus solely on acceleration metrics, without considering the impact on learning quality. Rollout is a sampling process that benefits from high temperature to maintain data diversity, which is crucial for effective policy learning. It is unclear whether the proposed acceleration changes the diversity or quality of the collected trajectories. The authors should include analyses or experiments evaluating these aspects, such as reward comparisons or diversity statistics.\n\nIn addition, the experiments do not compare with other state-of-the-art RL acceleration methods, such as PipelineRL or similar parallel rollout techniques, which limits the strength of the empirical validation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "T7em7bZb29", "forum": "zuGt6TYYtS", "replyto": "zuGt6TYYtS", "signatures": ["ICLR.cc/2026/Conference/Submission4459/Reviewer_Hu9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4459/Reviewer_Hu9Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966216970, "cdate": 1761966216970, "tmdate": 1762917379173, "mdate": 1762917379173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the severe inference-time bottleneck in Group Relative Policy Optimization (GRPO) for large language models (LLMs), where autoregressive generation dominates training time. The authors introduce FastGRPO, a concurrency-aware speculative decoding framework with online draft model learning specifically tailored for high-concurrency RL training. By dynamically tuning drafting/verification parameters based on concurrency and continuously adapting the draft model to the evolving target, the method achieves substantial end-to-end speedups (2.35x–2.72x) over standard and state-of-the-art baselines on various LLMs and mathematical reasoning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work pinpoints practical, under-discussed causes of inefficiency in LLM RL training—especially the interplay between sequence length variance, dynamic concurrency, and memory/compute bottlenecks. This is well exemplified in Figure 1, where the generation phase is shown to consume up to 98% of GRPO training time, emphasizing the criticality of accelerating this stage.\n\nThe concurrency-aware speculative decoding scheme is well-motivated and nontrivial, adapting speculative tree depth and verification in response to real-time batch concurrency (see Algorithm 1 and the design in Section 4.1). The operational intensity formulation (Equation in Section 4.1) is technically thoughtful, and the methodology is justified theoretically and empirically.\n\nThe online draft learning mechanism shows astute recognition of distributional drift, which is convincingly quantified in Figure 4 (not provided in this snippet, but referenced), and ablation results (Table 3) indicate a robust solution.\n\nThe experiments are broad and carefully chosen. The acceleration is confirmed across multiple models (Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, DeepSeek-R1-Distill-Qwen-7B, etc.) and three mathematical datasets of varying complexity."}, "weaknesses": {"value": "Lack of direct comparison with very recent parallel or hardware-aware decoding approaches: Methods such as SpecExec (Svirschevski et al., 2024) and TurboAttention (Kang et al., 2025) are not discussed or compared, though they address related efficiency targets under high concurrency. This omission reduces clarity on FastGRPO’s relative standing among recent innovations.\n\nEmpirical results lack coverage beyond math/reasoning tasks: While Section 5.3 briefly covers a few general-purpose benchmarks (e.g., MT-Bench, HumanEval), the primary evaluations remain dominated by mathematical datasets. The method’s generality for non-mathematical generative RL tasks (e.g., dialogue, instruction following) is only partially tested and reported in a superficial manner.\n\nThe adaptivity logic for tuning $N_{\\text{verify}}$, $K_{\\text{draft}}$, and $L_{\\text{draft}}$ is somewhat heuristic and under-explored for edge cases: Although the operational intensity framework is principled, the parameters depend on empirically-set hardware constants and logarithmic scaling (see Section 4.1 and Appendix B), whose practical calibration details are not fully fleshed out for heterogeneous or less conventional hardware platforms.\n\nMathematical clarity, though good, could be improved: For example, the formula for $K_{\\text{draft}}$ in Section 4.1 is not explicitly linked to actual tree coverage efficiency, and the effect of $\\alpha$ is described more narratively than analytically. The stepwise impacts of speculative tree over-expansion (e.g., wasted computation) are mostly argued intuitively.\n\nExposition of experimental protocols needs crisper detail: For instance, Table 1 and Table 3 report impressive speedup, but the precise wall-clock measurement setup (system load, parallel jobs, handling of inter-job contention) is only mentioned for some cases and not consistently. The “draft learning is free” claim would be more credible with breakdowns of GPU utilization and timeline plots. Similarly, Appendix B details the measurement of $C_{\\text{peak}}$, but system variance or sensitivity to other workloads is not discussed.\n\nLimited discussion of failure or suboptimal regimes: The work reports major speedups, but does not adequately address situations where speculative decoding might degrade (e.g., for models with unstable or highly creative output distributions, overfitting of the draft, or pathological reward landscapes).\n\nAblation details and datasets could be better visualized: Table 3 is comprehensive, but for a work whose core is acceleration, per-epoch training time curves, acceptance lengths, or variance plots (vs. batch size or sequence diversity) would add depth. Figures focus mainly on speedup ratios or batch size, avoiding more granular runtime or alignment dynamics.\n\nSome related methods and baselines are not explicitly cited or compared. Notable omissions include adaptive parallel decoding (Israel et al., 2025) and task-specific alternatives for cache-aware masking or split-parallelism (Federici et al., 2025; Polisetty et al., 2025).\n\n\n\nNo explicit statistical reporting of significance/testing: Performance gains are substantial but confidence intervals, run-to-run variance, or significance tests are absent, which would be crucial in justifying claims of “consistent” or “robust” improvement.\n\nSpecific references to figures/tables as required:\n\nFigure 1 illustrates the overwhelming dominance of generation time across datasets and models (91–98%), which strongly motivates the acceleration focus of FastGRPO.\nFigure 3: Clearly demonstrates that naive speculative decoding fails to provide speedup at high batch sizes (with curves “below 1.0x”), while the proposed adaptive tuning (“Optimal”) yields robust, sustained acceleration as concurrency grows—a direct validation of the core claim.\nTable 1: Quantifies the end-to-end and generation-phase speedup over state-of-the-art speculative decoding methods (EAGLE-2/3, HASS), confirming FastGRPO’s efficacy in both overall and granular metrics across a set of LLMs and datasets.\nTable 3: Ablates online draft learning and concurrency-aware tuning, showing granular Gen SR/E2E SR ratios for different setups and revealing that both components are crucial for the full speedup.\nAppendix Figure 6: (noted in methodology) empirically motivates the critical batch size $C_{\\text{peak}}$ and provides a hardware-specific context for the adaptive scaling formulas introduced in Section 4.1."}, "questions": {"value": "The concurrency scaling formulas rely on empirically determined hardware constants (e.g., $C_{\\text{peak}}$ via the “knee” in Figure 6). How robust are these to fluctuating system loads, mixed-precision, and multi-GPU setups? Could you provide stricter sensitivity analysis or calibration guides?\n\nWould FastGRPO’s approach generalize to autoregressive tasks with less structural output (e.g., open-ended dialogue, story generation) or with non-standard reward distributions? Are there tasks where adaptation of the draft model actually hurts acceptance (e.g., under creative instruction-following), and how is that detected or mitigated?\n\nCan the authors report statistical variances or confidence intervals for the speedup/acceptance results in Tables 1, 3, and 4? Is the acceleration robust across multiple random seeds and different training restarts?\n\nFor the “draft model pretraining may be unnecessary” claim (Section 5.3 and Figure 5), could you clarify if there are scenarios (e.g., pathological/outlier domains) where failing to pretrain leads to persistent misalignment or poor acceptance rates?\n\nAre there any practical trade-offs or user-visible impacts on final model quality (e.g., accuracy, diversity, reward coverage) due to the continuous adaptation of the draft model vs. a static approach? Are there empirical results measuring possible “overspecialization” or drift toward the target policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jerFk8ZaRb", "forum": "zuGt6TYYtS", "replyto": "zuGt6TYYtS", "signatures": ["ICLR.cc/2026/Conference/Submission4459/Reviewer_1Sgp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4459/Reviewer_1Sgp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762057098603, "cdate": 1762057098603, "tmdate": 1762917378404, "mdate": 1762917378404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose to use speculative decoing in the RL training process, especially to acccelerate the rollout process of GRPO. It then identifies two issues: 1) speculative decoding may achieve limited speedup when the batch size is large; 2) the accept length decline in the mid and later stage of RL training if we donot update the draft module. The authors propose to adjust draft strategies to build the draft path/tree based on the concurrency and add the draft module updates in the actor update process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper address a key issue in RL training especially GRPO-like algorithm, and attempts to use speculative decoding to accelerate the major bottleneck-the rollout stage. Specifically,\n\n1. The paper propose a concurrency-aware way to adjust the N_draft, draft branching and draft depth, to balance the compute and memory, achieving the sweet spot and avoiding compute/memory bound.\n\n2. It add an online distillation for the draft module that uses KL divergence to update draft and actor accordingly.\n\n3. Good ablation study. It includes results w/ and w/o concurrency adjustment, online learning, and the need for draft pretraining."}, "weaknesses": {"value": "1. Performance experiments missing. It is unclear whether using speculative decoding can leads to reduced accuracy in final performance. Even speculative decoding align with the original sample probability, but the difference between rollout/training may worsen and other issue may appear.\n\n2. The accept rate doesn't seem to improce significantly w/ and w/o online training shown in Table 2?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IEXMsJiOvl", "forum": "zuGt6TYYtS", "replyto": "zuGt6TYYtS", "signatures": ["ICLR.cc/2026/Conference/Submission4459/Reviewer_1RsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4459/Reviewer_1RsX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762229211761, "cdate": 1762229211761, "tmdate": 1762917377495, "mdate": 1762917377495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}