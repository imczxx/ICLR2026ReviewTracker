{"id": "Ds8IunNp2a", "number": 2655, "cdate": 1757182601311, "mdate": 1763116772703, "content": {"title": "Can we trust the attribution method?", "abstract": "Attribution methods are essential for interpreting deep learning models, helping to align model decisions with human understanding. However, their trustworthiness remains uncertain. Previous work has highlighted several design flaws in attribution methods such as the choice of reference points and the selection of attribution paths, but we argue that even a theoretically perfect attribution method—one that provides the true ground truth—cannot fully resolve the trust issue. For the first time, we summarize the specific manifestations of such issue: Two samples with infinitely close distances but different classification results share the same important feature attention region.  We rigorously derive this phenomenon and construct scenarios demonstrating that attribution trust issues persist even under ideal conditions. Our findings provide a new benchmark for evaluating attribution methods and highlight the need for cautious application in real-world scenarios. Our code is available at: https://anonymous.4open.science/r/Distrust-8677/", "tldr": "", "keywords": ["Attribution", "Interpretability", "XAI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ee8612a761d759d6ffb7b14b6168755fcb6089cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies whether feature attributions are trustworthy. It highlights a near-boundary phenomenon that two almost identical inputs with opposite predictions can yield very similar saliency, and proposes a “ground-truth attribution” defined by decomposing the output change between the two inputs along the input-space path. Using this construction, it compares multiple attribution methods and argues that satisfying axioms does not guarantee reliability."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The trustworthiness of attribution methods is well-motivated and directly relevant to ongoing debates in explainability.\n\n2. The problem setup and the central question are clearly articulated."}, "weaknesses": {"value": "1. This work has a faulty “ground truth” premise. The paper assumes the output change should be fully or primarily attributed to the pixels that differ between the two images. In nonlinear networks, small pixel perturbations can redistribute importance globally. Therefore, input difference is not a faithful surrogate for measuring the reliability of attributions. The completeness axiom constrains attribution to sum to output change, not to mirror input interpolation. Hence, the proposed “ground truth” cannot validly measure attribution fidelity, making the subsequent experiments and conclusions problematic.\n\n2. The claim that “satisfying axioms does not ensure reliability” adds limited novelty. Attribution axioms are widely regarded as necessary but not sufficient.\n\n3. Robustness and reliability of explanations have been examined in many existing works[1,2,3]. In contrast, this work brings few new conclusions. \n\n[1] Rudin, Cynthia. \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.\" Nature machine intelligence 2019.\n\n[2] Zhou, Yilun, et al. \"Do feature attribution methods correctly attribute features?.\" AAAI 2022.\n\n[3] Dombrowski, Ann-Kathrin, et al. \"Explanations can be manipulated and geometry is to blame.\" Advances in neural information processing systems 2019."}, "questions": {"value": "1. How do you justify that the output change should be fully (or primarily) attributed to the perturbed pixels, despite nonlinear cross-feature interactions?\n2. Why must two near-boundary images necessarily have different attributions? If you cannot establish this necessity, on what basis do you reject the reliability of existing methods rather than concluding that visual similarity alone is inconclusive?\n3. What new conclusions or insights does your evaluation add beyond existing studies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ouwtn6KQZW", "forum": "Ds8IunNp2a", "replyto": "Ds8IunNp2a", "signatures": ["ICLR.cc/2026/Conference/Submission2655/Reviewer_HM8k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2655/Reviewer_HM8k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731251859, "cdate": 1761731251859, "tmdate": 1762916321066, "mdate": 1762916321066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "lznU3oEbDw", "forum": "Ds8IunNp2a", "replyto": "Ds8IunNp2a", "signatures": ["ICLR.cc/2026/Conference/Submission2655/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2655/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763116771959, "cdate": 1763116771959, "tmdate": 1763116771959, "mdate": 1763116771959, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an intrinsic trust limitation of feature attribution methods, arguing that even a theoretically perfect algorithm that yields the “ground-truth attribution” can still be untrustworthy: for two samples that are infinitesimally close yet classified into different labels, their attributions may highlight the same “important” region, thus failing to capture what truly distinguishes the decisions.\n\nHowever, the paper suffers from a fundamental conceptual flaw. The authors conflate classification explanation with contrastive attribution. By construction, the attributions of two nearby samples near the decision boundary are expected to be similar under the local smoothness assumption. In such cases, meaningful analysis should focus on the differential change ($\\Delta$ attribution) that explains the class transition, rather than interpreting attribution similarity as a failure of trustworthiness. Consequently, the core argument misinterprets the nature of attribution and the semantics of decision explanation.\n\nMy suggestion is to reframe the comparison to $\\Delta$ attribution and see if the trustworthiness are violated in that setup."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper raises an important and underexplored question: whether attribution methods can ever be fully trustworthy even when theoretically correct. \n2. The authors provide a mathematically grounded setup for analyzing infinitesimally close sample pairs across decision boundaries."}, "weaknesses": {"value": "1. The proposed “ground-truth attribution” derived from the residual–gradient coupling fundamentally attributes class transition rather than class membership. In other words, it measures why the prediction changes from class A to class B, not why the model predicts class A. This conflates attribution with adversarial sensitivity, departing from the standard semantics of attribution methods such as IG or SHAP, which are designed to decompose a single class score into feature contributions.\n\n2. Because the ground truth is inherently contrastive (A-->B), comparing it against non-contrastive attributions (e.g., IG(x)) creates an unfair benchmark. To evaluate the behavior of traditional attribution methods under this setup, one should instead compare their attribution differentials, such as $\\Delta IG = IG(x) − IG(x′)$, rather than the absolute maps themselves. Otherwise, the observed “similarity” between two attributions merely reflects local smoothness rather than a genuine failure of trustworthiness.\n\nreference: Pan, Deng, Xin Li, and Dongxiao Zhu. \"Explaining deep neural network models with adversarial gradient integration.\" Thirtieth International Joint Conference on Artificial Intelligence (IJCAI). 2021."}, "questions": {"value": "Your “ground-truth attribution” is defined through two similar samples belonging to different classes. Does this formulation aim to explain why the model predicts class A or why the prediction changes from A to B? Please clarify whether you regard this as a classification or contrastive explanation, and how it aligns with standard attribution semantics (e.g., IG, SHAP)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V7N2otk5c2", "forum": "Ds8IunNp2a", "replyto": "Ds8IunNp2a", "signatures": ["ICLR.cc/2026/Conference/Submission2655/Reviewer_oFag"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2655/Reviewer_oFag"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935817161, "cdate": 1761935817161, "tmdate": 1762916320608, "mdate": 1762916320608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a fundamental limitation in attribution. The authors demonstrate those method gives misleading trust of attribution through scenarios where two nearly identical inputs yield different classifications but share the same attribution. This shows that attribution methods may fail to distinguish decisive features in classification shifts. The authors offer theoretical foundation for this phenomenon and propose a benchmark for evaluating attribution reliability. They caution that attribution results should not be blindly trusted, even when the method is technically correct."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow. \n- The paper points out an interesting question regarding the trust of attribution, and provides rigorous analysis to the question."}, "weaknesses": {"value": "1. Can we indeed construct such $x_1$ and $x_2$? According to my understanding, the existence of $x_1$ and $x_2$ is based on the fact that they lies closely to the decision boundary, such that $\\forall\\varphi>0$, $\\exists x_{1}, x_{2}\\in[\\gamma(t)\\mid t \\in (0, 1)]$, satisfying $C(x_1) \\neq C(x_2)$ and $\\|x_1 - x_2\\| < \\varphi$. However, is there any guarantee that those samples are semantically meaningful images?\n2. $\\alpha(t)$ is not defined. Actually, what is the purpose of $\\alpha(t)$.\n3. I have concerns that if it is intrinsically supposed to be the phenomenon that nearly identical inputs could have same attributions? For example, if we consider two images with all black but one pixel different (e.g., one red and one blue). The classifier classifies them differently (red and blue). And the attribution should be able to identify that pixel and the attribution region should be the same."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5o1XHSJSfY", "forum": "Ds8IunNp2a", "replyto": "Ds8IunNp2a", "signatures": ["ICLR.cc/2026/Conference/Submission2655/Reviewer_wofT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2655/Reviewer_wofT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990150306, "cdate": 1761990150306, "tmdate": 1762916320320, "mdate": 1762916320320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper raises a trust issue in feature attribution--two samples with infinitesimal differences but different predicted labels can have the same attribution map. A theoretical result is developed to demonstrate this issue. Empirical results on image classification with ResNet show that this theoretical issue can occur in practice. Overall, this paper challenges the assumption that axiom-justified attribution methods can always be trusted."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes an original issue related to trustworthiness in feature attribution. Previous work such as [R1] and [R2] typically shows the issue that similar samples can have different explanations, while this work shows that similar samples with different predicted labels can have the same explanation.\n\nReferences\n\n[R1] Dombrowski et al. (2019) - Explanations can be manipulated and geometry is to blame\n\n[R2] Slack et al. (2021) - Feature Attributions and Counterfactual Explanations Can Be Manipulated"}, "weaknesses": {"value": "- In the paper, there are two ways of evaluating the trustworthiness of an attribution method. (1) A trustworthy attribution method should capture the ground truth about the explained model's decision-making process. This is the evaluation criterion in Section 4.5. (2) A trustworthy attribution method should have different attribution maps for two similar samples with different predicted labels. This is the evaluation criterion introduced by the paper. In the setting considered in this paper, (1) and (2) contradict each other. It then becomes unclear why (2) is more relevant to user trust than (1). Can't we argue that an attribution method that satisfies (1) but fails (2) is trustworthy because the method reveals counterintuitive aspects about a model's decision-making process, highlighting that the model itself or the samples are not trustworthy?\n\n- The proof for Theorem 1 seems incomplete. Specifically, the property $\\lVert x_1 - x_2 \\rVert \\le \\phi$ is not (directly) proven.\n\n- In line 231, it is stated that \"as $\\epsilon \\rightarrow 0, \\lVert x_1 - x_2 \\rVert$ approaches 0.\" This does not hold directly from the continuity of a neural network (as stated in line 226), where the implication is in the opposite direction. I think the authors can remove this statement as currently it is not required to prove line 232.\n\n- In lines 269-270, it is argued that $r = x_2 - x_1$ transitions the model's output from class A to class B, and that is the basis for treating Equation 3 as the ground truth attribution. However, since $x_2$ is not unique (as mentioned in lines 234-244), it's possible to have another adversarial sample $x_3$ such that $r' = x_3 - x_1$ is small and transitions the model's output from class A to class B. Since $r' \\neq r$, applying Equation 3 gives us two ground truths. It then becomes unclear what the notion of ground truth is."}, "questions": {"value": "- Based on the exposition in the paper, an attribution method that captures the ground truth is not trustworthy because it produces identical attribution maps for similar samples that have different predicted labels. Can't we argue that the attribution method itself is trustworthy because it reveals counterintuitive aspects about the explained model's decision-making process, highlighting that the model itself or the samples are not trustworthy.\n\n- It seems like the scenario where two similar samples have different predicted labels arise only from adversarial samples. In this case, can't we argue an attribution method that fails the trustworthiness criterion proposed in lines 60-61 is actually trustworthy, because the method reveals counterintuitive aspects about the adversarial samples?\n\n- Related to the second point, is it possible to construct samples $x_1$ and $x_2$ without adversarial attack? In other words, can $x_1$ and $x_2$ be sampled from $P(x)$ directly? What assumptions about $P(x)$ might be necessary, and are they reasonable?\n\n- Currently, the last property $\\lVert x_1 - x_2 \\rVert \\le \\phi$ in Theorem 1 is not (directly) proven. Can you provide the completed proof?\n\n- Since there can be multiple adversarial samples that flip the predicted label in the same direction, are there multiple ground truth attribution maps?\n\n- Section 4.4 only shows qualitative visual examples. Can you provide quantitative comparisons between different methods? Is there a tradeoff between the trustworthiness criterion in Section 4.4 vs. the faithfulness criterion in Section 4.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vlWwT34km5", "forum": "Ds8IunNp2a", "replyto": "Ds8IunNp2a", "signatures": ["ICLR.cc/2026/Conference/Submission2655/Reviewer_BhXD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2655/Reviewer_BhXD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199108720, "cdate": 1762199108720, "tmdate": 1762916320033, "mdate": 1762916320033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}