{"id": "DVfaLBUc2s", "number": 7960, "cdate": 1758046184809, "mdate": 1759897819731, "content": {"title": "Dynamic Compression in Distributed Communications for Reduction of Transmission Losses", "abstract": "Recent trends in machine learning demonstrate an increasing demand for computational resources, compelling the ML community to leverage multiple devices for training. This concept is realized through distributed and federated learning approaches. However, despite the numerous advantages offered by these paradigms, they suffer from a significant limitation: the necessity of frequent information exchange between devices. A common solution to this issue involves compression. Existing operator definitions account only for second-moment deviation, which does not fully reveal the changes. Also, it is not assumed to vary during the learning process, but the forwarded information may have patterns, for example decreasing from iteration number. To address this limitation, we propose several novel classes of compression operators. Additionally, we introduce dynamic data types that adapt to the nature of the transmitted data. Our comprehensive theoretical analysis demonstrates the efficiency of this approach when applied to state-of-the-art algorithms, such as EF21, DIANA and DASHA. Experimental results further validate that the reduction in compression error and accelerates the convergence.", "tldr": "", "keywords": ["optimization", "distributed optimization", "communication compression"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b114f45ec3ee3cf3ea92204345c1b56b09778f9c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a limitation in existing theoretical frameworks for compression operators when applying to low-bit quantization. To address this, the authors propose new operator classes ($\\mathbb{U}(\\omega, \\epsilon)$ and $\\mathbb{B}^3(\\alpha, \\epsilon)$) that explicitly include this absolute error term, $\\epsilon$. The paper then introduces a \"Dynamic Data Type\" method, which adaptively refines the quantization grid to make $\\epsilon$ diminish during training, thereby enabling convergence to an exact solution instead of a residual error neighborhood. The theoretical analysis is applied to state-of-the-art algorithms like EF21, DIANA, and DASHA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a practical limitation in the standard theoretical framework for compression operators. The observation that fixed-grid, low-bit quantization introduces a non-diminishing absolute error $\\epsilon$, which is ignored by traditional relative-error definitions, is a valid and inspiring contribution. Besides, the theoretical analysis appears to be solid."}, "weaknesses": {"value": "W1, The paper suffers from significant readability issues that hinder understanding. The main experimental plots on page 8 lack proper \"Figure\" numbers and captions. Crucial new notation, specifically the core concepts of the $\\mathbb{B}^3(\\alpha, \\epsilon)$ and $\\mathbb{U}(\\omega, \\epsilon)$ operator classes, are introduced implicitly rather than through formal definitions.\n\nW2, The experimental validation feels thin and, in some cases, confusing. The theory is developed for EF21, DIANA, and DASHA, but the experiments appear to focus only on EF21. The analysis of the results is brief, and some of the experimental plots are confusing and seem to contradict the paper's own claims (as detailed in Question 2)."}, "questions": {"value": "Q1 Regarding the main convergence results, could the authors elaborate on how the absolute error term $\\epsilon$ precisely impacts the convergence guarantees? Furthermore, does this new theoretical framework gracefully cover traditional (non-low-bit) biased and unbiased compressors?\n\nQ2 In the ResNet gradient norm plots (page 8, right column), there are two major concerns. First, why do the 'dynamic' (top row) and 'static' (bottom row) experiments have different Y-axis starting points (e.g., $10^3$ vs. $approx 10^1$)? Besides, the plots seem to show that the static baseline (bottom row) actually converges to a *lower* gradient norm (approx. $10^{-4}$) than the proposed dynamic method (approx. $10^{-2}$). Can the authors please explain this discrepancy?\n    \nQ3 How does the proposed dynamic quantization grid algorithm perform when applied to the other compressors analyzed in the theory, such as DIANA and DASHA? The current experiments seem limited to EF21."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HMugaF6260", "forum": "DVfaLBUc2s", "replyto": "DVfaLBUc2s", "signatures": ["ICLR.cc/2026/Conference/Submission7960/Reviewer_WzCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7960/Reviewer_WzCT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569525952, "cdate": 1761569525952, "tmdate": 1762919976229, "mdate": 1762919976229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the communication bottleneck in distributed and federated learning by introducing a novel framework for dynamic compression. Traditional compression operators rely on fixed variance assumptions and do not adapt as gradients shrink during training, leading to inefficiencies. To overcome this, the authors propose new definitions for biased and unbiased compression operators that incorporate finite-grid rounding errors, and introduce dynamic data types that adjust compression precision throughout the optimization process. The approach is theoretically analyzed and integrated into state-of-the-art distributed optimization algorithms such as EF21, DIANA, and DASHA, with convergence guarantees under both nonconvex and PL conditions. Experimental results on logistic regression and ResNet tasks demonstrate that dynamic data types significantly reduce communication costs while maintaining or improving convergence speed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe idea of dynamic compression (adjusting accuracy during the training phase) is of practical significance and solves the problem that static quantization cannot accurately control errors.\n2.\tThe authors derive a new class of compression operators and rigorously prove their compatibility and convergence with existing operators."}, "weaknesses": {"value": "1.\tLimited Experimental Scale: The experiments primarily focused on logistic regression and small- to medium-scale ResNet image classification tasks. They were not validated in scenarios such as large-scale distributed training (e.g., ImageNet and GPT-like tasks), heterogeneous networks with varying bandwidths, or large numbers of nodes.\n2.\tAlthough the paper compares the three frameworks of EF21, DIANA, and DASHA, these frameworks all come from the same research school and lack comparison with other dynamic compression algorithms in recent years.\n3.\tTheoretical convergence analysis relies heavily on assumptions and lacks rigorous proofs for non-convex complex models."}, "questions": {"value": "1.\tThere are problems with the structure of the article. It is best not to have definitions and subsections in the Introduction. You can consider putting section 1.1 in Preliminaries. Meanwhile, there are grammatical and typographical issues. It also misses Conclusion chapter and the caption of the figures.\n2.\tAblation experiments are missing: The paper does not systematically study: the individual effects of different ùëù on the convergence rate; the performance of dynamic compression under different gradient distributions (sparse/dense); and whether it can be used in conjunction with Top-k sparsification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jBAbH5Zx2r", "forum": "DVfaLBUc2s", "replyto": "DVfaLBUc2s", "signatures": ["ICLR.cc/2026/Conference/Submission7960/Reviewer_yMD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7960/Reviewer_yMD1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829546183, "cdate": 1761829546183, "tmdate": 1762919975870, "mdate": 1762919975870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies compression operators for distributed learning, where communication between clients and the central server is often the main bottleneck. The proposed operators compress transmitted values (such as gradients) by rounding each coordinate to a finite grid. Two variants are considered: an unbiased one that uses probabilistic rounding based on proximity to grid points, and a biased one that rounds to the nearest grid point.\nThe key idea is the use of a dynamic grid that adapts over time. As the communicated values (e.g., gradients) become smaller during training, the grid range is reduced, improving precision and communication efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea of using a dynamic grid for quantization is reasonable and intuitive, though I am not sure whether it is novel or has been studied before."}, "weaknesses": {"value": "1. **Poor writing quality**  \n   The paper is poorly written, with many grammatical errors and unclear sentences. Several key terms are never defined, e.g., $\\mathbb{B}^3(\\alpha, \\varepsilon)$ and $\\mathbb{U}(w, \\varepsilon)$, even though they seem central to the main results. Other notations, such as $\\varepsilon(\\mathcal{A})$, also appear without explanation. Overall, the presentation feels rushed and unpolished.\n\n2. **Unclear implementation of the dynamic grid**  \n   The update of the grid depends on a condition called *Degree\\_cond*, which is said to rely on some threshold of the communicated values. However, the paper does not explain how this threshold should be chosen.\n\n3. **Lack of theoretical analysis for the dynamic scheme**  \n   There is no theoretical justification for how the dynamically changing quantization set should be updated or how change strategies affect convergence. \n\n4. **Limited practical relevance**  \n   In deep learning tasks, the gradient norm often does not converge to zero, so the proposed dynamic-grid approach may not be practical in real-world training scenarios."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4c2H68TPLI", "forum": "DVfaLBUc2s", "replyto": "DVfaLBUc2s", "signatures": ["ICLR.cc/2026/Conference/Submission7960/Reviewer_fp2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7960/Reviewer_fp2V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868243484, "cdate": 1761868243484, "tmdate": 1762919974773, "mdate": 1762919974773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dynamic Compression in Distributed Communications ‚Äî a framework that introduces adaptive (dynamic) compression operators and ‚Äúdynamic data types‚Äù for distributed optimization. The main idea is to adjust the quantization or rounding grid during training so that compression precision increases as gradients become smaller, thereby reducing transmission costs without losing convergence guarantees. The authors formalize new definitions for biased and unbiased compressors that consider finite quantization grids and additive error terms, analyze their convergence under smoothness and PL conditions, and integrate them into existing algorithms such as EF21, DIANA, and DASHA. Experiments on logistic regression (LibSVM datasets) and ResNet image classification show that dynamic compression improves communication efficiency compared to static compression."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1 The theoretical sections are detailed and mathematically rigorous, with convergence analyses for several classes of compressors.\n\nS2 The proposed ‚Äúdynamic data type‚Äù idea is conceptually simple and appealing ‚Äî gradually increasing precision as training progresses aligns with observed gradient magnitude decay.\n\nS3 The proposed approach is applied to several algorithms."}, "weaknesses": {"value": "W1 Insufficient experimental evaluation. The experiments are very limited ‚Äî mostly small-scale logistic regression (mushrooms, a9a, w8a) and a single ResNet test. There are no large-scale distributed benchmarks, no results on real multi-node setups, and no comparisons with recent adaptive or learned compression methods (e.g., natural compression, adaptive quantization, or low-bit dynamic quantizers).\n\nW2 Lack of ablation studies. There is no analysis on how parameters like grid update frequency, percentile choice, or rounding base \nùëù affect convergence or communication cost. The ‚Äúdynamic data type‚Äù mechanism is treated as a black box, without studying its sensitivity or stability.\n\nW3 Missing important comparisons. There's are many related methods in the literature but the paper does not have any comparison with other methods.\n\nW4 The paper is math-heavy with unclear separation between motivation, theoretical contribution, and implementation. The experimental section is particularly brief and lacks reproducibility details (e.g., hardware, network topology, communication budget)."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dBCvXgzlvY", "forum": "DVfaLBUc2s", "replyto": "DVfaLBUc2s", "signatures": ["ICLR.cc/2026/Conference/Submission7960/Reviewer_2QdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7960/Reviewer_2QdW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898541379, "cdate": 1761898541379, "tmdate": 1762919974177, "mdate": 1762919974177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes dynamic data type‚Äìbased compressors for distributed/federated optimization. Instead of assuming infinite-precision lattices, the authors formalize finite-grid rounding (both unbiased and biased variants) and integrate them into EF21, DIANA, and DASHA, with convergence analyses that exploit geometric grid refinement. Experiments on logistic regression (LibSVM) and ResNet show reduced communication for comparable accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper identifies a gap in classic compressor analyses (infinite grids/second-moment focus) and introduces finite-grid rounding operators with additive-error terms and precise definitions for unbiased/biased rounding.\n\n+ With a geometric refinement schedule, the analysis achieves standard O(1/K) nonconvex rates without residual and a linear rate under PL-type conditions, directly tying accuracy to grid tightening.\n\n+ The experiments show practical integration and positive results."}, "weaknesses": {"value": "- No conclusion part is involved in this paper.\n\n- Dynamic grids imply control-plane costs. However, these implementation/latency costs aren‚Äôt quantified, so real-world gains may be overstated versus ‚Äúbits on paper.‚Äù\n\n- There‚Äôs no ablation on alternative schedules or robustness of the trigger, nor prescriptive guidance for combining with other biased/unbiased compressors beyond basic closure statements."}, "questions": {"value": "+ Can you follow the ICLR format with providing a conclusion in this paper?\n\n+ How sensitive are results to the refinement factor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bXbXMQsJ2v", "forum": "DVfaLBUc2s", "replyto": "DVfaLBUc2s", "signatures": ["ICLR.cc/2026/Conference/Submission7960/Reviewer_6MJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7960/Reviewer_6MJr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990383455, "cdate": 1761990383455, "tmdate": 1762919973544, "mdate": 1762919973544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}