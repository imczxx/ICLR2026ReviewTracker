{"id": "mLLAc1FhWV", "number": 20840, "cdate": 1758310745937, "mdate": 1759896956070, "content": {"title": "Exact Certification of Neural Networks and Partition Aggregation Ensembles against Label Poisoning", "abstract": "Label-flipping attacks, which corrupt training labels to induce misclassifications at inference, remain a major threat to supervised learning models. This drives the need for robustness certificates that provide formal guarantees about a model's robustness under adversarially corrupted labels. Existing certification frameworks rely on ensemble techniques such as smoothing or partition aggregation, but treat the corresponding base classifiers as black boxes—yielding overly conservative guarantees. We introduce EnsembleCert, the first certification framework for partition aggregation ensembles that utilizes white-box knowledge of the base classifiers. Concretely, EnsembleCert yields tighter guarantees than black-box approaches by aggregating per-partition white-box certificates to compute ensemble-level guarantees in polynomial time.  To extract white-box knowledge from the base classifiers efficiently, we develop ScaLabelCert, a method that leverages the equivalence between sufficiently wide neural networks and kernel methods using the Neural Tangent Kernel. ScaLabelCert yields the first exact, polynomial-time calculable certificate for neural networks against label-flipping attacks. EnsembleCert is either on par, or significantly outperforms the existing partition-based black-box certificate. Exemplary, on CIFAR-10, our method can certify upto $\\mathbf{+26.5\\\\%}$ more label flips in median over the test set compared to the existing black-box approach while requiring $\\mathbf{100 \\times}$  fewer partitions, thus challenging the prevailing notion that heavy partitioning is a necessity for strong certified robustness.", "tldr": "We develop the first white-box informed robustness certificate for partition-based ensembles and the first polynomial-time exact certificate for sufficiently wide neural networks against label-flipping poisoning attacks.", "keywords": ["Certified Robustness", "Provable Robustness", "Certificates", "Neural Tangent Kernel", "Partition Aggregation", "Label-flipping", "Label Poisoning", "Kernel SVM", "Kernel Regression", "Integer Program", "Multiple Choice Knapsack Problem"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e670e22c8d6e8eb469ee9a6537dcc9eccdc3619.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces EnsembleCert and ScaLabelCert, the methods to provide white-box certificates for partition-aggregation ensembles against label-flipping attacks. The methods utilize the white-box information about the base classifiers the target ensembles consists of. The protocols provide polynomial-time computable solutions for IP formulation for certification problem. Please use sparingly!"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work provides a novel theoretically grounded approach to certify the classification ensembles to label flipping attacks. The authors claim that a white-box knowledge of base classifiers can be used to significantly tighter ensemble-level certificates. The polynomial time solutions for relaxed IP formulations are provided, making the certificates computable at least for simple base classifiers."}, "weaknesses": {"value": "No information about the computation overhead needed for the certification of an ensemble is provided. The authors indicate that the complexity of the relaxed IP problem for ensemble-wise certification scales quadratically with the number of dataset partitions, potentially making approach infeasible for large datasets and non-trivial base models. \n\nThe improvement of an existing certification protocol (LabelCert) to provide sound certificates for infinitely-wide neural networks does not seem to be applicable in the experimental setup: the conditions up to which theoretical grounds of ScaLabelCert hold in terms of the base models' width are not studied. \n\nThe effect of small constant C for base SVMs in ScaLabelCert remains understudied: at least empirical effect of the \"smallness\" of C on the performance of the classifiers as the function of dimensionality of the input samples and the cardinality of subdatasets used for training has to be studied."}, "questions": {"value": "Please comment on the weaknesses above. I am willing to increase my score if the weaknesses are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CxUv0d4ivl", "forum": "mLLAc1FhWV", "replyto": "mLLAc1FhWV", "signatures": ["ICLR.cc/2026/Conference/Submission20840/Reviewer_qz3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20840/Reviewer_qz3B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567864881, "cdate": 1761567864881, "tmdate": 1762936332338, "mdate": 1762936332338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors attempt to distinguish between poisoning defences that hold white box knowledge, versus those that don't, and attempt to demonstrate improved defensive utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "On the surface, this paper is well written, is built upon interesting ideas, and works within an important space. The devil, however, very much is in the details (which I'll discuss in the following section). But I find the idea of trying to articulate and interrogate the intrinsic limitations of other approaches to be an interesting. \n\nThe authors have also contextualised their work in the context of both deep proofs, and through references to more classical problems (including MCKP)."}, "weaknesses": {"value": "Okay, so, the aforementioned devil. To me, the primary problem is a lack of specificity (somewhat ironic, given the length of the appendices). For example, consider the white-box nature of the system. This is a crucial part of the overall conceptual landscape of this paper. Yet there are 25 different references to white-box information and 3 pages before the white-box information involved in the paper is defined in any way, which is on line 168. \n\nThe white box nature of the paper also gives me pause on a logical level as well. For I'm not certain as to how realistic it is to be able to construct this for any problem of interest. There are black-box points of comparison, yet the important questions (to me) do not receive the level of attention that they deserve. And I don't mean my personal research interests - I mean questions on how this would scale, how this would be used, and what the real drawbacks of this sort of approach would be. Yes, the authors provide information on the P/NP complexity, and discuss polynomial time scaling, but this doesn't, to me, cover the actual practical realities of how this would actually behaved computationally, for systems of interest. \n\nFundamentally, if this is not something that would be able to be realistically applied to problems of interest (due to the scaling of cost), I don't think the authors have appropriately contextualised what someone would get out of it. \n\n(Also L28 is not the correct use of the word exemplary)"}, "questions": {"value": "I'd appreciate answers to the following questions\n\n1. Can you contextualise how \"sufficiently small C\" would behave across different problems of interest?\n2. The focus upon looking at the absoute number of flips, rather than the proportion of flips within the dataset seems odd to me. Or, more ot the point, the authors do not make a clear case as to why the absolute number of flips is more important than how many flips three are relative to the size of the overall dataset. Could you comment upon this?\n3. How would this scale? Yes, it may be polynomial-time calculable, but is it reasonable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QECHjE8X8L", "forum": "mLLAc1FhWV", "replyto": "mLLAc1FhWV", "signatures": ["ICLR.cc/2026/Conference/Submission20840/Reviewer_ZEFw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20840/Reviewer_ZEFw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907453432, "cdate": 1761907453432, "tmdate": 1762936331795, "mdate": 1762936331795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the first practical white-box certification framework for partition-aggregation ensembles (EnsembleCert) and a polynomial-time exact NTK-based label-flip certificate for wide neural networks (ScaLabelCert) againast label poisoning attack . Compared to prior aggregation defenses—which all assume each poisoned point flips one voter—and prior NTK certificates—which are exact but NP-hard and tiny-scale—this work meaningfully improves both computational tractability and certificate tightness, and reveals a nontrivial empirical insight: strong base classifiers often lose robustness when partitioned. Despite the restriction to infinite-width NTK models, the analytic contribution is solid and the empirical message is important, but weaknesses still exists in terms of experiments and claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clean reduction of ensemble-level certification to MCKP with a proven equivalence (P1=P2), enabling tractable white-box aggregation not present in DPA/ROE/BagFlip.\n\n-  NTK “small-C” simplification collapses LabelCert’s bilevel MILP into a polynomial certificate—genuinely improving scalability while keeping exactness.\n\n-  Empirical finding that heavy partitioning often reduces certified robustness is novel and challenges a core assumption in existing ensemble defenses.\n\n-  Technically coherent integration bridging black-box aggregation work and exact NTK-based poisoning certificates."}, "weaknesses": {"value": "- Applicable only to infinite-width NTK models with specific regularization; finite-width neural networks remain uncertified, limiting practical impact.\n\n- Claims of “first exact certificate for neural networks” are too broad—true only in this NTK regime and should better acknowledge LabelCert’s precedence.\n\n- Comparisons to other white-box poisoning frameworks (e.g., gradient-based relaxations, other NTK poisoning work) are conceptual rather than empirical. The paper also lacks some accuracy–robustness trade-off.\n\n- Experimental comparisons with other white-box poisoning certificates are missing. Without evaluating against existing white-box approaches, it is hard to contextualize how much tighter, faster, or more scalable the proposed method actually is."}, "questions": {"value": "- The paper certifies exact robustness only for infinite-width NTK models. How does this transfer to real finite-width neural networks? There is no empirical evidence showing the method remains meaningful for practical models. \n\n- The claim that deep partition aggregation is weak may simply reflect that the chosen base model degrades significantly when trained on small partitions. Is the observed phenomenon inherent to partitioning, or just a consequence of weak base learners?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0zJ6KgViq0", "forum": "mLLAc1FhWV", "replyto": "mLLAc1FhWV", "signatures": ["ICLR.cc/2026/Conference/Submission20840/Reviewer_C1z2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20840/Reviewer_C1z2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969435518, "cdate": 1761969435518, "tmdate": 1763000400259, "mdate": 1763000400259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates robustness certifications for partition-aggregation ensembles under label-flipping poisoning attacks for infinite-width neural networks.\nThe proposed method, EnsembleCert, provides ensemble-level certificates by aggregating white-box robustness certifications of the base classifiers.\nThe white-box knowledge to build certifications is extracted from base classifiers by means of ScaLabelCert, a method that relies on neural tangent kernels to obtain exact, polynomial-time certificates against label-flipping attacks.\nThe approach circumvents the overly conservative guarantees of other black-box techniques, and performs on-par or better than them, while being more efficient than existing Mixed Integer Linear Program approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Clear theoretical contribution.**\nThe paper presents a clean and rigorous formulation of ensemble-level certification under label-flip attacks under the NTK assumption.\nThe reduction of the certification problem to a multiple-choice knapsack formulation, solvable via dynamic programming, is elegant and allows for tractable, provably tight ensemble certificates under the white-box setting.\nThe theoretical development is, as far as I can judge, internally consistent and sound.\n\n* **Novel use of NTK-based exact certification.**\nScaLabelCert provides an exact, closed-form certification method for models under the NTK assumption.\nLeveraging the NTK equivalence to derive analytical bounds on robustness is an interesting and, as far as I can judge, original direction compared to existing heuristic or black-box defences.\n\n* **Tighter bounds and interpretability.**\nWithin its stated assumptions, EnsembleCert yields significantly tighter certified radii than prior black-box ensemble approaches, and the paper empirically demonstrates this with well-documented experiments.\nThe decomposition of ensemble robustness into per-partition contributions provides interpretability and insight into how partitioning affects robustness."}, "weaknesses": {"value": "* **Finite width and NTK.**\nFor ScaLabelCert you rely on the assumption that the Neural Tangent Kernel (NTK) limit holds.\nWhile I appreciate the clean mathematical treatment that NTK's dynamic allow for, I feel like a discussion of the applicability of your approach is not clearly presented.\nIn the abstract as well as throughout the text, for instance, you mention \"sufficiently wide neural networks\", alluding at the fact that, for these networks, you can provide exact certifications.\nIf I am not misunderstanding, though, your certification holds in the (exact) NTK limit, as you do not provide any description of a large but finite network width.\nThis gap between your theoretical advancements and standard neural network architectures should be more thoroughly discussed in my opinion.\nIn particular, for finite architectures, can the certificates you obtain be, in principle, arbitrarily wrong?\n\n* **White-box inputs.**\nBuilding on my previous comments, EnsembleCert relies on obtaining the smallest number of flips needed to change the prediction for one of the models in a partition.\nThis step too, relies on the NTK assumption, limiting the practical applicability of your approach.\nWhile I agree that black-box estimates are often overly conservative, and agree that relying on white-box information can improve upon this, it seems to me that in a realistic setting what you can provide is heuristic statements, rather than guarantees.\nTherefore, I think it may not be fair to compare against existing approaches that, while maybe overly conservative or prohibitively expensive, provide quantifiable _guarantees_ for realistic settings.\nThis to me is a crucial gap in the paper, and I look forward to any clarification on this point you may have.\n\n* **Experiments with neural networks.**\nIn your experimental section you do not experiment with any neural network, nor validate the NTK approximation.\nAs you rely on soft-margin SVMs, which can be framed as convex optimization problems.\nIt remains unclear whether the performance boost of your approach stems from this fact mainly.\nThis setting seems to be far from the more realistic non-convex, finite-width neural network setting.\nTherefore, I would say that your argument for scalability to larger neural networks is unsubstantiated.\nIf I missed some key parts of your argument, I am happy to discuss this point further."}, "questions": {"value": "* Is it possible that the certificates produced under your NTK assumption are arbitrarily inaccurate for a finite network?\n* How can you argue for your approach in a realistic setting where the NTK assumption may not be satisfied?\n* How can you quantify how good your guarantees are if the NTK assumption does not hold exactly?\n* Can you provide an heuristic/empirical or theoretical criterion for when a network is \"sufficiently wide\" for your guarantees to meaningfully apply?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mvm63bo84G", "forum": "mLLAc1FhWV", "replyto": "mLLAc1FhWV", "signatures": ["ICLR.cc/2026/Conference/Submission20840/Reviewer_NDxz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20840/Reviewer_NDxz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983355696, "cdate": 1761983355696, "tmdate": 1762936330412, "mdate": 1762936330412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}