{"id": "JEkzgeYwIk", "number": 3979, "cdate": 1757578855324, "mdate": 1759898059938, "content": {"title": "Safeguarding Visual Privacy in Dataset Distillation: Robust Initialization via Augmentation", "abstract": "Dataset distillation synthesizes small datasets that enable models to achieve accuracy comparable to training on the original full data, yielding substantial training efficiency gains. \nIn addition, distilled data have been used for privacy-preserving applications, especially to mitigate membership inference attacks (MIA), where adversaries query a model to decide whether a sample was in its training set. \nHowever, we are the first to show that state-of-the-art dataset distillation leaks visual privacy. Distilled images can be visually consistent with private originals, as measured by LPIPS, thereby leaking sensitive information. \nWe theoretically trace this risk to the common practice of initializing distilled images with original samples. \nTo counter this, we propose Kaleidoscopic Transformation (KT), a plug-and-play module that applies aggregated, strong yet semantics-preserving perturbations to selected original images at initialization. \nExtensive experiments demonstrate that KT consistently strengthens resistance to MIA and improves visual privacy, while maintaining competitive downstream accuracy. Our code will be publicly available.", "tldr": "", "keywords": ["Dataset Distillation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3157d85026df1acf55bacd48e6b199285d71a8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the concept of visual privacy leakage in the context of dataset distillation. The authors quantify such leakage by measuring the minimum dissimilarity between the original dataset and the distilled dataset. They further demonstrate that applying averaged randomized data augmentation can mitigate visual privacy leakage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-motivated research direction** The paper tackles an important problem, visual privacy leakage in dataset distillation. This direction bridges the gap between privacy protection and data-efficient learning, and is highly relevant to real-world applications.\n- **Practical and easily adoptable methodology** The authors employ the LPIPS metric to quantify visual privacy leakage and introduce an averaged randomized data augmentation strategy to mitigate it. Both techniques are practical and can be easily integrated into existing dataset distillation methods.\n- **Theoretical analysis inspired by differential privacy** The paper provides a theoretical analysis inspired by differential privacy, offering some insights on how to improve the membership privacy."}, "weaknesses": {"value": "1. Although the paper references differential privacy, it does not provide a formal $(\\epsilon,\\delta)$-DP guarantee or privacy accounting. Theoretical discussions are based on simplified assumptions about random sampling rather than a complete privacy mechanism, making the DP connection mostly conceptual. This weakens the claimed theoretical contribution.\n\n2. The core method, averaged randomized data augmentation, is conceptually simple and lacks algorithmic novality. While it empirically reduces visual similarity, the mechanism does not offer clear theoretical grounding. The synthesized images shown in the paper, especially Figure 9,  appear unrealistic or semantically inconsistent, raising concerns about whether the method truly mitigates privacy leakage or merely produces degraded samples.\n\n3. The evaluation is limited to CIFAR-10, CIFAR-100, and Tiny-ImageNet, which are relatively small and clean benchmarks. These datasets do not adequately capture realistic privacy risks in visual data distillation. Incorporating datasets such as CelebA, as done in [a, b], would provide a more convincing and representative demonstration of visual privacy leakage. Moreover, the experiments only used ConvNet backbones. Extending the evaluation to more diverse architectures would strengthen the generality and robustness of the conclusions."}, "questions": {"value": "1. Could the authors elaborate further on Equation (4)? It is unclear what the function $\\xi$ specifically represents. \n\n2. Could the authors clarify their experimental settings and differences from [a, b]? The reported baseline performance in this paper appears to deviate from the results presented in those prior works.\n\n[a] Dong, Tian, et al. \"Privacy for free: How does dataset condensation help privacy?\" ICML, 2022.\n\n[b] Harder, Fredrik, et al. \"Pre-trained perceptual features improve differentially private image generation.\" TMLR, 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q8hoNTRSbO", "forum": "JEkzgeYwIk", "replyto": "JEkzgeYwIk", "signatures": ["ICLR.cc/2026/Conference/Submission3979/Reviewer_MuaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3979/Reviewer_MuaC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499840017, "cdate": 1761499840017, "tmdate": 1762917123168, "mdate": 1762917123168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a previously underexplored risk in dataset distillation: visual privacy leakage when the distilled dataset itself is released. The authors argue and empirically show that state-of-the-art distillation (e.g., DATM (Guo et al., 2024), RDED (Sun et al., 2024)) at high images-per-class (IPC) can produce synthetic images that closely resemble originals, measurable via low nearest-neighbor LPIPS and leading to potential exposure of sensitive content. They trace the failure primarily to initialization that copies real images and propose Kaleidoscopic Transformation (KT), a plug-and-play, strong-yet-semantics-preserving augmentation strategy applied at initialization. KT averages multiple stochastic differentiable augmentations to produce an initialization image per original, then the chosen distillation method proceeds unchanged.\n\n  The paper provides: (a) a formalization of “visual privacy” as minimum perceptual distance between any distilled and any original image; (b) DP-style analyses splitting initialization vs. matching optimization, yielding bounds for standard initialization (Theorem 1) and improved bounds with KT (Theorem 2); and (c) empirical evaluation on CIFAR-10/100 and Tiny-ImageNet with ConvNet backbones, reporting membership privacy via LiRA TPR@0.1% FPR (Carlini et al., IEEE S&P 2022), visual privacy via Min LPIPS (Zhang et al., CVPR 2018), plus utility (test accuracy). Across DM/DSA/MTT/DATM/RDED, KT consistently increases minimum LPIPS (often markedly at IPC=50), reduces LiRA success, with limited accuracy drop (e.g., on CIFAR-100, DATM at IPC=50: Min LPIPS 0.01→0.29, TPR 3.2→0.6, accuracy 55.0%→49.2%). Appendix reports SSCD/DreamSim corroboration and a sensitive-domain case (COVID-19 CXR)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Important problem framing: distinguishes model-release MIA settings from data-release visual privacy, and defines a concrete minimum-distance visual privacy criterion; shows high-IPC distillation can visually leak originals.\n- Clear causal story: ties leakage to common real-image initialization, supported by analysis splitting initialization vs. matching, and experiments where KT at initialization mitigates both LiRA and nearest-neighbor similarity.\n- Practical mitigation: KT is simple, plug-and-play, and broadly applicable across distillation families (DM/DSA/MTT/DATM/RDED); empirical wins are consistent across datasets and IPC values, especially at high IPC.\n- Evaluation breadth: uses LiRA (Carlini et al., 2022) for low-FPR attack evaluation; Min LPIPS for perceptual nearest-neighbor leakage; complementary SSCD/DreamSim; includes a sensitive medical example.\n- Relevance to current SOTA: evaluates with DATM (Guo et al., 2024), RDED (Sun et al., 2024), and classic baselines (DSA/DM/MTT), aligning with the contemporary distillation landscape."}, "weaknesses": {"value": "- Visual privacy metric scope: Min LPIPS/SSCD/DreamSim measure perceptual similarity but do not directly capture identity leakage (e.g., face identity) or task-specific semantics. A user study or identity-aware metrics (e.g., face recognition similarity) on privacy-sensitive domains would strengthen claims.\n  - Utility trade-offs at high IPC: While KT improves privacy substantially, several tables show non-trivial accuracy drops at IPC=50 (e.g., DATM on CIFAR-100: −5.8 pp). A more systematic analysis of where utility loss concentrates (classes, frequencies) and whether stronger initialization augmentations hurt rare or fine-grained categories would help.\n- Threat model simplification: Treating the entire training set as “members” for distilled data deviates from standard MIA threat modeling. Please justify and also evaluate a setting where only a subset contributes to distillation or where the adversary targets initialization samples specifically (some analysis in Appendix I helps; consider expanding in main text).\n- Theory-to-metric link: The DP-style bounds (Theorems 1–2) motivate KT but are not directly validated against the operational privacy metrics (LiRA/LPIPS). Adding experiments showing empirical correlations between the derived quantities (e.g., Σ, added perturbation variance 1/n Σϵ) and observed LiRA/LPIPS would strengthen the theoretical bridge.\n- Generality beyond small ConvNets: Results are with Conv-3/4 on CIFAR/Tiny-ImageNet. It would help to see at least one modern backbone (e.g., small ViT or ResNet-18) to demonstrate transferability.\n- Details on “semantics-preserving”: The paper asserts KT is semantics-preserving; it would be useful to quantify class-conditional label stability under KT-only initializations to support this claim, and to catalog which augmentations are used with what probabilities."}, "questions": {"value": "- Identity-level leakage: Can you evaluate KT on a face dataset with an identity recognizer to quantify identity leakage reduction vs. LPIPS? Similarly for medical imaging, use domain-relevant feature extractors.\n- Initialization-targeted attacks: Beyond LiRA, what is the success of a nearest-neighbor retrieval attack that directly searches T for the most similar original to each distilled image (at varying IPC), and how does KT change that distribution?\n- Augmentation ablations: Which transformation families (geometric, color, frequency) drive the best privacy–utility trade-off? Are there transformations that hurt utility disproportionately and should be excluded?\n- Backbones and scales: Do the privacy and utility trends hold with ResNet or ViT backbones, and at larger resolution datasets (e.g., ImageNet-100)?\n- Parameter n selection: You recommend n=3; how sensitive are results to n across distillation methods? Is there an adaptive criterion (e.g., stopping when Min LPIPS exceeds a threshold) to set n per class?\n- Formal DP: While you avoid the incorrect assumption in prior work (Carlini et al., 2022b critique), do you envision a path to concrete (ε,δ)-DP guarantees under KT initialization (perhaps with per-sample clipping/calibrated noise), and how would that interact with utility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nwRoK0d2lT", "forum": "JEkzgeYwIk", "replyto": "JEkzgeYwIk", "signatures": ["ICLR.cc/2026/Conference/Submission3979/Reviewer_CvU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3979/Reviewer_CvU6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627953712, "cdate": 1761627953712, "tmdate": 1762917122898, "mdate": 1762917122898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies privacy risks when releasing distilled datasets (as opposed to only releasing models). It shows that state-of-the-art dataset distillation can visually leak private images—especially at higher images-per-class (IPC)—because many methods initialize distilled images from real samples. The authors formalize visual privacy (min LPIPS between distilled and original images) and analyze privacy across two phases: (1) initialization and (2) matching optimization. They argue that initialization with real images creates a major leakage path and propose Kaleidoscopic Transformation (KT), a plug-and-play initialization module that applies multiple strong, differentiable augmentations and averages them to create a more randomized initialization sample. Experiments on CIFAR-10/100 and Tiny-ImageNet show lower MIA success (TPR@0.1% FPR) and higher Min-LPIPS with KT, with modest utility drops; they claim up to 25× LPIPS increase with only 3.4% accuracy loss in some settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Formalization & insight: The two-phase analysis (Proposition 1; Theorem 1) makes a persuasive case that init from real data is the main culprit; Theorem 2 shows KT tightens KL-based bounds by adding bounded random perturbations. \n- Consistent privacy gains: Across datasets/IPC, KT reduces MIA success and raises Min-LPIPS, with visualizations showing nearest-neighbor leakage essentially removed."}, "weaknesses": {"value": "- Theory scope: connection between visual privacy and differential privacy."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "JMY6qSVKzU", "forum": "JEkzgeYwIk", "replyto": "JEkzgeYwIk", "signatures": ["ICLR.cc/2026/Conference/Submission3979/Reviewer_fGHY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3979/Reviewer_fGHY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933605222, "cdate": 1761933605222, "tmdate": 1762917122600, "mdate": 1762917122600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical privacy vulnerability in state-of-the-art dataset distillation methods: visual privacy leakage, where distilled images can be visually similar to original training samples, particularly at high compression ratios (high IPC - images per class). The authors  analyze this risk in data-release scenarios, where attackers have direct access to distilled datasets rather than just model queries. First, the paper demonstrates that modern dataset distillation methods (like DATM, MTT, RDED) produce distilled images that strongly resemble original data at high IPC values (e.g., IPC=50), creating severe visual privacy risks. This goes beyond traditional membership inference attack (MIA) concerns to expose actual visual content. Second, through theoretical analysis (Proposition 1, Theorem 1), the authors trace this leakage to the common initialization practice of using original training samples as starting points for distilled images. While the subsequent matching optimization phase provides some privacy protection by limiting individual sample influence (bounded by 2B|S|/|T| · λmax(Σ^-1)), insufficient perturbation during initialization leaves distilled samples visually aligned with their initialization counterparts. Third, \nthe authors introduce Kaleidoscopic Transformation (KT), a plug-and-play module that applies aggregated, strong data augmentations to selected original images during initialization (before distillation begins). By averaging multiple augmented versions of each sample, KT introduces additional randomness that strengthens both visual privacy and resistance to MIAs while maintaining competitive downstream accuracy. Finally, they present experimental results on CIFAR-10/100 and Tiny-ImageNet with various distillation methods to show KT \nconsistently increases visual consistently increases visual dissimilarity (LPIPS) by up to 25× while maintaining reasonable utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Important Problem: Identifies an imp challenge of visual privacy leakage in dataset distillation, demonstrating that state-of-the-art methods (DATM, RDED) produce distilled images visually similar to originals at high IPC, creating severe risks in data-release scenarios—a practically important threat for sensitive domains. Good first principled analysis.\n\n2. Good Empirical Results: KT consistently improves privacy across all methods and datasets: reduces MIA success from 17.3% to baseline levels at IPC=50, increases visual dissimilarity (LPIPS) by up to 25× (from 0.01 to 0.29 for DATM on CIFAR-100), while maintaining reasonable accuracy (49.2% vs 55.0% for DATM IPC=50 CIFAR-100, ~6% drop).\n\n3. Comprehensive Experimental Analysis: Extensive evaluation across multiple datasets (CIFAR-10/100, Tiny-ImageNet, COVID-19), five distillation methods (DM, DSA, MTT, DATM, RDED), three IPC values (1, 10, 50), and three perceptual metrics (LPIPS, SSCD, DreamSim). Results consistently show KT's effectiveness with fair comparison framework.\n\n4. Practical Deployability: KT is a simple plug-and-play module requiring no modification to existing distillation algorithms, making it immediately adoptable by practitioners using any distillation method."}, "weaknesses": {"value": "1. Unrealistic Gaussian Assumptions in Theory: Theorems 1-2 assume distilled samples follow normal distributions N(μ, Σ) with fixed covariance, which is highly unrealistic for DNN-based distillation. The proofs don't actually handle DNNs in the loop—they analyze simplified mathematical models that don't capture complex, non-convex neural network optimization dynamics. The bounds depend on λmax(Σ^-1), which is never characterized or computed in practice.\n\n2. No Formal Privacy Guarantees: Despite theoretical analysis, the paper provides no rigorous differential privacy bounds. Proposition 1's δ=|S|/|T| is too large for formal DP, and Theorem 2 only shows KT improves this under Gaussian assumptions. There's no formal connection between LPIPS thresholds and actual privacy protection levels—what value of τ in Definition 2 is provably safe remains unanswered.\n\n3. Hand-Wavy Treatment of Matching Optimization: Lemma 1 (from Dong et al. 2022) connecting final distilled data to initialization assumes distribution matching converges to a specific linear relationship (Equation 5), but this doesn't hold for trajectory matching methods (MTT, DATM) which are the state-of-the-art. The stochasticity model for matching optimization oversimplifies complex iterative updates across thousands of gradient steps."}, "questions": {"value": "1. Gaussian Assumption Justification: Can you provide empirical evidence that distilled samples actually follow approximately Gaussian distributions? Please show histograms or normality tests for distilled data distributions. How do you compute or estimate λmax(Σ^-1) in practice, and what are typical values observed in your experiments?\n\n2. Applicability to Trajectory Matching: Your theoretical analysis relies on Lemma 1 which assumes distribution matching. How does this analysis apply to trajectory matching methods (MTT, DATM) which are your main baselines and don't satisfy the span(T) relationship in Equation 5? Can you clarify whether Theorems 1-2 actually hold for these methods?\n\n3. Privacy Threshold and Adversarial Robustness: What LPIPS threshold τ in Definition 2 constitutes \"safe\" visual privacy, and how was this determined? Have you evaluated against stronger adversaries who might use optimization-based reconstruction attacks or fine-tune models on distilled data to extract original information, rather than just visual inspection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rOY6nFPKcj", "forum": "JEkzgeYwIk", "replyto": "JEkzgeYwIk", "signatures": ["ICLR.cc/2026/Conference/Submission3979/Reviewer_gR5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3979/Reviewer_gR5J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107515142, "cdate": 1762107515142, "tmdate": 1762917122321, "mdate": 1762917122321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}