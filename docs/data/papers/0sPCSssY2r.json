{"id": "0sPCSssY2r", "number": 10721, "cdate": 1758180379442, "mdate": 1763548432994, "content": {"title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?", "abstract": "Model Context Protocol (MCP) has become a key infrastructure for connecting LLMs with external tools, scaling to 10,000+ MCP servers with diverse tools. Unfortunately, there is still a large gap between real-world MCP usage and current evaluation: they typically assume single-server settings and directly inject tools into the model’s context, bypassing the challenges of large-scale retrieval and multi-tool composition. To bridge this gap, we propose **LiveMCPBench**, which evaluates 95 real-world daily tasks explicitly constructed to stress diverse tools and scaled multi-server routing. The benchmark includes a ready-to-deploy tool suite of 70 servers with 527 tools, ensuring reproducibility without scattered API configuration. We further introduce an LLM-as-a-Judge evaluation framework that directly verifies task outcomes, handling dynamic data sources and multiple valid solution paths. We benchmark 10 state-of-the-art LLMs and observe a substantial performance gap: while Claude-Sonnet-4 reaches 78.95% task success, most models achieve only 30–50%. Our analysis reveals that active tool composition strongly correlates with task success, whereas retrieval errors account for nearly half of all failures-highlighting retrieval as the dominant bottleneck. Together, these results provide the first large-scale, reproducible diagnosis of MCP agent capabilities and point towards future research on improving retrieval robustness and encouraging effective tool composition. Code and data will be released upon publication.", "tldr": "We introduce LiveMCPBench, a benchmark of 95 real-world daily tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse tools.", "keywords": ["Model Context Protocol", "MCP-use", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/569fec7d95b1e85073f3d367c8542b0d62c78862.pdf", "supplementary_material": "/attachment/16dfeb73f1fef653ec0c55ff75165551cad396cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LiveMCPBench, a benchmark made up of 95 real-life multi-tool tasks such as “Check next Monday’s Beijing–Shanghai high-speed train tickets,” “Summarize today’s news into a PDF,” and “Find a 3-day stay in Paris.” These cover six domains — office work, daily life, entertainment, finance, travel, and shopping — and each task requires combining multiple tools and accessing real-time information (e.g., tickets, news). This design effectively tests a model’s real abilities in tool discovery and tool orchestration.\n\nThe team also developed LiveMCPTool, which includes 70 servers and 527 tools. It works right out of the box — no complex API keys or permission setups required — ensuring reproducibility across different users (so results aren’t skewed by unavailable tools).\n\nFinally, they proposed LiveMCPEval, an automatic evaluation system using another LLM (e.g., DeepSeek-V3) as a referee to directly judge task completion rather than strict tool usage. For example, in the “generate news PDF” task, the model passes as long as it produces a correct PDF — regardless of the specific tool order. The system also handles dynamic information (like changing weather) automatically, removing the need for manual checking.\n\nI really like this paper — the dataset is valuable and fills an important gap. I’d recommend acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The dataset is highly practical and valuable for real-world LLM evaluation."}, "weaknesses": {"value": "1. Relying on LLM-based evaluation may introduce bias.\n\n2. The framework doesn’t account for hallucinated tool calls or factually wrong but plausible outputs."}, "questions": {"value": "1. Although the team validated scoring accuracy with human labels (e.g., DeepSeek-V3 achieved 78.95% agreement with humans), LLM judges can still have systematic biases — for instance, misjudging an empty PDF as a valid “completed” output. While the authors analyzed some error cases, they didn’t fully address or mitigate these inherent biases, which may cause certain models’ true capabilities to be over- or underestimated.\n\n2. Tool-use hallucinations may produce seemingly correct but actually invalid results (e.g., fabricated tool responses or outdated data). The paper doesn’t discuss how such hallucinations are detected or penalized in the evaluation, which could limit the reliability of the benchmark results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0qq1HrLsq6", "forum": "0sPCSssY2r", "replyto": "0sPCSssY2r", "signatures": ["ICLR.cc/2026/Conference/Submission10721/Reviewer_kNq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10721/Reviewer_kNq6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761103709742, "cdate": 1761103709742, "tmdate": 1762921951928, "mdate": 1762921951928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiveMCPBench, a benchmark made up of 95 real-life multi-tool tasks such as “Check next Monday’s Beijing–Shanghai high-speed train tickets,” “Summarize today’s news into a PDF,” and “Find a 3-day stay in Paris.” These cover six domains — office work, daily life, entertainment, finance, travel, and shopping — and each task requires combining multiple tools and accessing real-time information (e.g., tickets, news). This design effectively tests a model’s real abilities in tool discovery and tool orchestration.\n\nThe team also developed LiveMCPTool, which includes 70 servers and 527 tools. It works right out of the box — no complex API keys or permission setups required — ensuring reproducibility across different users (so results aren’t skewed by unavailable tools).\n\nFinally, they proposed LiveMCPEval, an automatic evaluation system using another LLM (e.g., DeepSeek-V3) as a referee to directly judge task completion rather than strict tool usage. For example, in the “generate news PDF” task, the model passes as long as it produces a correct PDF — regardless of the specific tool order. The system also handles dynamic information (like changing weather) automatically, removing the need for manual checking.\n\nI really like this paper — the dataset is valuable and fills an important gap. I’d recommend acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The dataset is highly practical and valuable for real-world LLM evaluation."}, "weaknesses": {"value": "1. Relying on LLM-based evaluation may introduce bias.\n\n2. The framework doesn’t account for hallucinated tool calls or factually wrong but plausible outputs."}, "questions": {"value": "1. Although the team validated scoring accuracy with human labels (e.g., DeepSeek-V3 achieved 78.95% agreement with humans), LLM judges can still have systematic biases — for instance, misjudging an empty PDF as a valid “completed” output. While the authors analyzed some error cases, they didn’t fully address or mitigate these inherent biases, which may cause certain models’ true capabilities to be over- or underestimated.\n\n2. Tool-use hallucinations may produce seemingly correct but actually invalid results (e.g., fabricated tool responses or outdated data). The paper doesn’t discuss how such hallucinations are detected or penalized in the evaluation, which could limit the reliability of the benchmark results.\n\n------------------\n## After Rebuttal\n**Because my main concerns remain unaddressed and directly affect the validity of the benchmark, I regret that I must lower my score from 8 to 4.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0qq1HrLsq6", "forum": "0sPCSssY2r", "replyto": "0sPCSssY2r", "signatures": ["ICLR.cc/2026/Conference/Submission10721/Reviewer_kNq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10721/Reviewer_kNq6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761103709742, "cdate": 1761103709742, "tmdate": 1763718320335, "mdate": 1763718320335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduced LiveMCPBench, a benchmark consisting of a “plug‑and‑play” tool suite of 70 MCP servers with 527 tools and 95 multi‑step daily tasks, plus an LLM‑as‑a‑Judge evaluator (LiveMCPEval) to handle dynamic data and multiple valid solution paths. It aims evaluating LLM agents’ ability to retrieve and compose tools across a realistic, large‑scale MCP servers with low effort (e.g., no API access keys)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a real gap in MCP evaluation. Unlike prior MCP benchmarks (e.g., MCPBench, MCP‑RADAR, MCPEval), LiveMCPBench evaluates large‑scale retrieval and multi‑tool composition over 70 servers / 527 tools and 95 tasks.\n2. The authors curate a key‑free, ready‑to‑deploy MCP tool suite that removes scattered API configuration barriers, improving reproducibility relative to API‑centric benchmarks where many APIs require access keys or have become unavailable.\n3. Breaks down failure root causes into tool composition and retrieval errors."}, "weaknesses": {"value": "I appreciate the authors’ effort in creating a curated collection of MCP tools that work plug-and-play. Their main motivation is clear: existing MCP benchmarks are either too small or too difficult to use. Many tools in those benchmarks require access keys or are no longer valid. In contrast, LiveMCPBench includes key-less tools, making them easy to use. However, the benchmark itself is not large. It covers only 95 tasks, which is modest for evaluating general-purpose agentic reasoning. The number of MCP servers and tools is also limited—just 70 servers and 527 tools. This makes it unsuitable for testing agents designed for much larger MCP ecosystems with thousands of servers (something that the authors mention in the beginning of the paper).\n\nThere is another concern. Like other large benchmarks, LiveMCPBench tools may become unusable if their owners stop maintaining them. Although the toolset is packaged for reproducibility, the benchmark depends on dynamic online sources. It does not address long-term sustainability issues such as server versioning, security, or maintenance (see Limitations §A). Over time, servers may change, become unavailable, or behave differently. This can harm the reproducibility and reliability of benchmark results. The paper does not propose concrete solutions, such as Dockerized server snapshots, health checks, or semantic versioning. Without these, future evaluations may not be consistent or valid. This technical gap puts the benchmark’s long-term value for the research community at risk.\n\nA key requirement of dealing with an 'ocean of MCP tools' is a good retrieval system that can reliably select a small superset of  tools relevant to a given task. Authors use a embedding-based retrieval subsystem which is technically rigid: it uses a single embedding model (Qwen3-Embedding-0.6B) and a single summary generator (Qwen2.5-72B-Instruct) to process server and tool descriptions, with a fixed top-k=5 retrieval (§G.2). This design choice means that all agentic models are evaluated with the same retrieval backbone, potentially conflating agent reasoning ability with retriever recall. If the retriever fails to surface relevant tools, even a strong agent may be unable to succeed. The paper’s own analysis shows that retrieval errors account for ~50% of failures (Fig. 6), but does not explore how varying the retriever, k-value, or embedding model might affect outcomes. This limits the generalizability of the findings and may overstate the impact of “retrieval errors” attributable to the chosen pipeline.\n\nAs mentioned before, LiveMCPBench includes only 95 tasks, which is modest for a benchmark intended to evaluate general-purpose agentic reasoning. Furthermore, the task distribution is skewed: 32.6% are Office tasks (Fig. 7), and annotators ultimately used only 150 out of 527 tools (28.46%; §E.1). This means that a large portion of the curated toolset remains untested, and the benchmark may over-represent office-suite workflows while under-testing other domains such as location, code, or finance. The limited and imbalanced task set restricts the breadth of agentic behaviors that can be meaningfully evaluated and may bias results toward models that excel in office-related scenarios.\n\nThe paper depends on an LLM-based automatic evaluator (LiveMCPEval) to judge task success. While this approach enables scalable, open-ended evaluation, the reported agreement with human annotators is low (see Fig. 10 and Appx. H). This means a significant fraction of judgments can diverge from human consensus, especially for long or complex trajectories. Such residual disagreement undermines the reliability of leaderboard rankings and may allow models to “game” the judge or be unfairly penalized for nuanced outputs. The paper acknowledges these limitations but does not propose robust adjudication mechanisms (e.g., dual judges, confidence calibration, or selective human audits) to mitigate the risk."}, "questions": {"value": "1. Why did you opt for a binary success metric instead of more granular measures like retrieval recall, parameter correctness, or stability across reruns?\n2. How do you ensure that the fixed retrieval pipeline (embedding model, summarizer, top-k=5) does not bias the benchmark results?\n3. Have you conducted ablations with alternative retrievers, varying k-values, or hybrid retrieval methods to confirm that retrieval errors are not artifacts of your chosen pipeline?\n4. Given that LiveMCPEval achieves only ~81% agreement with human judgments, what steps can you take to improve evaluator reliability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rk5msSi3wv", "forum": "0sPCSssY2r", "replyto": "0sPCSssY2r", "signatures": ["ICLR.cc/2026/Conference/Submission10721/Reviewer_d7CQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10721/Reviewer_d7CQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860703141, "cdate": 1761860703141, "tmdate": 1762921951560, "mdate": 1762921951560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiveMCPBench, a novel and challenging benchmark for evaluating LLM-powered tool-use agents within a large-scale, real-world Model Context Protocol (MCP) ecosystem.1 The authors argue that existing benchmarks are insufficient because they rely on simulated or small-scale APIs and often bypass the critical challenges of large-scale retrieval and multi-tool composition.\n\nThe key contributions include:\n1. LiveMCPBench: A benchmark of 95 multi-step, dynamic \"daily tasks\" across six practical domains (Office, Travel, Finance, etc.).\n2. LiveMCPTool: A reproducible, dependency-free tool suite comprising 70 MCP servers and 527 tools, packaged for immediate use.\n3. LiveMCPEval: An automated LLM-as-a-Judge framework utilizing human-aligned key points to verify success, even with dynamic data and multiple valid solution paths.\n\nEmpirical evaluation of 10 frontier LLMs reveals a substantial performance gap (Claude-Sonnet-4 achieves 78.95% success, while most others are 30–50%). Error analysis identifies tool retrieval as the dominant bottleneck (accounting for $\\sim 50\\%$ of failures), and confirms a strong correlation between active tool composition and task success."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper correctly identifies and addresses the fundamental shift from small, unstable API sets to large-scale, hierarchical Model Context Protocol (MCP) ecosystems. The focus on retrieval over $500+$ tools and multi-tool composition in daily, dynamic tasks makes this benchmark uniquely challenging and representative of real-world use.\n2. The curated, dependency-free toolset (LiveMCPTool) is a major engineering strength. By eliminating the reliance on scattered, proprietary API keys, the paper ensures that the benchmark is reliable and accessible, overcoming the known instability issues of prior works.\n3. The empirical results are sharp and actionable. The finding that retrieval errors account for nearly 50% of failures and the positive correlation between active tool composition (measured by tool count and execution attempts) and success are clear diagnostic insights for future research on the planning and execution modules of agents."}, "weaknesses": {"value": "1. Limited Agent Architecture Analysis: The paper uses a single baseline, the MCP Copilot Agent (a ReACT agent). While the diagnosis is excellent, the paper doesn't explore how the retrieval error issue might be mitigated by a more advanced planning or memory architecture (e.g., Tree-of-Thought, specialized memory/reflection, or workflow memory). \n\n2. While the dependency-free nature of LiveMCPTool is a strength, the underlying data source for the 70 servers (e.g., news, weather, stock data) is only broadly mentioned as being derived from the \"MCP Marketplace\" and \"daily scenarios.\" Clarifying whether these servers utilize pre-cached static data or truly access live/dynamic public APIs (even if locally hosted) would solidify the claim of dynamism.\n\n3. The observation that the LLM-as-a-Judge effectiveness depends on the model (e.g., DeepSeek-V3 performs well, but Claude-Opus-4 performs poorly) is important. This causes a slight concern about the evaluation stability if the chosen judge model were to change or degrade. A brief discussion on judge model robustness or alternative verification protocols would strengthen this section."}, "questions": {"value": "1. Could the authors elaborate on the implementation of the LiveMCPTool servers? Specifically, are the 70 servers accessing live, real-time public APIs (e.g., a real stock market ticker, a real weather service), or do they simulate dynamism using static but complex datasets? \n\n2. Given that Retrieve Error is the dominant bottleneck ($\\sim 50\\%$), the ReACT agent used only has a single retrieval step before attempting execution. What minimal architectural modification (e.g., a Retrieval Reflection step, where the agent critiques the $k=5$ tools before acting) could be incorporated into the MCP Copilot Agent to address this weakness, and why was this approach not chosen for the baseline?\n\n3. Cost of Composition: Table 3 shows Claude-Sonnet-4 uses $2.71$ tools and $5.59$ executions on average, demonstrating active composition. Could the authors provide a small analysis (e.g., in the Appendix) breaking down the token cost or dialogue turns associated with this active composition versus simple single-tool calls?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OVuuFiAIgk", "forum": "0sPCSssY2r", "replyto": "0sPCSssY2r", "signatures": ["ICLR.cc/2026/Conference/Submission10721/Reviewer_spBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10721/Reviewer_spBt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905541594, "cdate": 1761905541594, "tmdate": 1762921951140, "mdate": 1762921951140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiveMCPBench, a benchmark for evaluating LLM agents in large-scale MCP environments. It comprises (1) 95 real-world tasks, (2) LiveMCPTool with 70 servers and 527 tools, (3) LiveMCPEval achieving 81% human agreement for automated evaluation, and (4) MCP Copilot Agent as baseline. Testing 10 frontier models reveals Claude-Sonnet-4 reaches 78.95% success rate, while most models struggle with retrieval errors (50%) and meta-tool-learning in multi-tool scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Realism and scale: LiveMCPBench captures the complexity of real MCP ecosystems with 527 tools across 70 servers, moving beyond prior single-server or simulated tool-use settings.\n2. Well-rounded design: The integration of LiveMCPTool, LiveMCPEval, and MCP Copilot Agent provides a coherent, end-to-end evaluation framework that is both scalable and reproducible.\n3. Empirical insight: The benchmark yields meaningful diagnostic findings—retrieval errors dominate failure modes, and tool composition correlates strongly with success.\n4. Promised reproducibility: The work offers a ready-to-deploy tool suite and automated evaluation protocol, enabling consistent, and the authors intend to release code and data."}, "weaknesses": {"value": "1. Limited model coverage: Only ten models are tested, excluding several widely used or newly released systems (e.g., GPT-5, Grok-4, GLM-4.5). This undermines the generalizability of the reported findings.\n2. Volatile metric design: The experiment employs a single-pass success metric. Realistic deployments often depend on reliability across multiple attempts. Using metrics such as pass@k (success in k runs) and pass^k (all k runs successful) would better capture stability and practical robustness.\n3. Evaluation methodology limitations: While LiveMCPEval enables scalable automatic assessment via the LLM-as-a-Judge paradigm, up to a 10-point discrepancy from human annotations indicates that purely LLM-based evaluation may be insufficient for structured MCP interactions. Integrating rule-based or code-verification modules might improve transparency, interpretability, and consistency with MCP's deterministic design.\n4. Missing annotation and cost detailed demonstration: As a benchmark paper, it lacks concrete examples demonstrating the annotation process and a detailed report of API token usage and computational cost, which weakens transparency and reproducibility.\n5. Typo in paper writing: In Figure 11, \"Evaulation Error\" should be corrected to \"Evaluation Error.\""}, "questions": {"value": "1. How were the ten evaluated models selected, and are there plans to include more recent systems such as GPT-5, Grok-4, or GLM-4.5 to improve completeness and ensure broader coverage across model families?\n2. Why did the evaluation rely solely on single-run success rates instead of including reliability metrics such as pass@k or pass^k that better capture model stability under repeated execution? Could such metrics be incorporated in future updates of the benchmark?\n3. Can you provide a concrete annotation example illustrating how the annotation principles are applied in practice, along with a detailed summary of API token usage of LiveMCPBench?\n4. What criteria were used to define task difficulty across the 95 daily tasks, and how well do these categories align with real-world MCP usage complexity?\n5. Could the benchmark be extended to support multi-agent collaboration scenarios, where multiple LLMs coordinate across MCP servers? If so, what challenges or modifications would be required in the current setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fJ8bvc8Guw", "forum": "0sPCSssY2r", "replyto": "0sPCSssY2r", "signatures": ["ICLR.cc/2026/Conference/Submission10721/Reviewer_i9Xk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10721/Reviewer_i9Xk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938922235, "cdate": 1761938922235, "tmdate": 1762921950687, "mdate": 1762921950687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}