{"id": "2iMSDChf21", "number": 12272, "cdate": 1758206776035, "mdate": 1759897520926, "content": {"title": "Subspace Node Pruning: Revealing Structured Importance via Orthogonal Subspace Transformations", "abstract": "Improving the efficiency of neural network inference is undeniably important in a time where commercial use of AI models increases daily. Node pruning is the art of removing computational units such as neurons, filters, attention heads, or even entire layers to significantly reduce inference time while retaining network performance. In this work, we propose the projection of unit activations to an orthogonal subspace in which there is no redundant activity and within which we may prune nodes while simultaneously recovering the impact of lost units via linear least squares. We furthermore show that the order in which units are orthogonalized can be optimized to maximally rank units by their redundancy. Finally, we leverage these orthogonal subspaces to automatically determine layer- wise pruning ratios based upon the relative scale of node activations in our subspace, equivalent to cumulative variance. Our method matches or exceeds state-of-the-art pruning results on ImageNet-trained VGG-16, ResNet-50 and DeiT models while simultaneously having up to 24×lower computational cost than alternative methods. We also demonstrate that this method can be applied in a one-shot manner to OPT LLM models, again outperforming competing methods.", "tldr": "We describe the projection of neural activities to a subspace in which they are orthogonal but from which they can still be (node) pruned. This subspace enables: pruning only non-redundant information, sensible ranking of nodes, network-wide pruning.", "keywords": ["Node Pruning", "Subspace", "Efficient AI"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9df1fa3241716459a86daed05cdebfcb6494916.pdf", "supplementary_material": "/attachment/b4da32ff442395bbf4f6d91add1aee0093759a4f.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Subspace Node Pruning (SNP), a structured pruning framework that projects layer inputs into a lower-triangular orthogonal subspace (via LDL/Gram-Schmidt), prunes the least informative latent units, and automatically reconstructs the dropped contributions through a linear least-squares–equivalent reparameterization of the weights. It complements this with (i) an unnormalized-ZCA–based importance score that ranks units by their non-redundant (post-orthogonalization) variance, and (ii) a global cumulative-variance cutoff that automatically allocates layer-wise pruning ratios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper is well-motivated. The core idea of orthogonalization-based node pruning comes with a clear, coherent technical narrative.\n\n(2) Global calibration of pruning across layers via a single cutoff/ordering is an attractive and practical design choice.\n\n(3) Empirical results are competitive on standard CNNs (VGG, ResNet); the pre-ordering idea shows benefits in the reported settings."}, "weaknesses": {"value": "(1) Experimental breadth is still limited for a pruning paper: more diverse architectures (e.g., mobile/dense variants) and tasks/datasets (e.g., CIFAR-10/100, generative models) would improve generality; reporting actual inference-time/throughput speed ups on unified hardware is also important.\n\n(2) Missing or incomplete baselines: SVD/low-rank pruning is a natural comparison given the conceptual proximity; related modern structured-pruning baselines could be covered more exhaustively.\n\n(3) Improvements over strong baselines are often marginal, and in some ResNet settings the method is not SOTA. Please contextualize gains (effect sizes, significance) and discuss trade-offs.\n\n(4) Presentation issues: some formatting/typos and unclear definitions detract from readability. For example, “howver” in line 367."}, "questions": {"value": "(1) Can you report end-to-end inference latency and throughput (same hardware, batch size, backend) and relate them to FLOPs reductions? This would substantiate practical speedups.\n\n(2) Please clarify grouped/joint importance computation for ResNets (exact procedure/algorithmic steps) and discuss computational cost when moving from simple feed-forward layers to CNN/ResNet blocks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mxIwTtv87F", "forum": "2iMSDChf21", "replyto": "2iMSDChf21", "signatures": ["ICLR.cc/2026/Conference/Submission12272/Reviewer_ARux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12272/Reviewer_ARux"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704080721, "cdate": 1761704080721, "tmdate": 1762923207374, "mdate": 1762923207374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Subspace Node Pruning (SNP), a novel structured pruning method. The approach works by projecting neural network activations onto an orthogonal subspace using a lower-triangular transformation matrix. This unique transformation allows for pruning nodes in the subspace (which corresponds to pruning the original input nodes) while also automatically reconstructing lost, non-redundant information, via a proven equivalence to linear least squares (LLS).\n\nThe method includes two key complementary contributions:\n- An \"unnormalized-ZCA\" scoring method to optimally reorder units based on their redundancy before pruning.\n- A \"cumulative variance\" metric that acts as a global, automated mechanism for selecting non-uniform, layer-wise pruning ratios from a single, universal threshold.\n\nExperiments on VGG-16, ResNet-50, DeiT, and OPT demonstrate that SNP achieves highly competitive performance against SOTA methods, while being significantly more computationally efficient (e.g., up to 24x faster than alternative pruning algorithms)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength is the method's 24x speedup in the pruning step compared to SOTA iterative methods like TPP.\n- The \"cumulative variance\" heuristic is an effective method for automatic, non-uniform, layer-wise ratio selection, without requires expert tuning.\n- The LLS-equivalent reconstruction provides excellent pre-retraining accuracy (Fig 4 left, Fig 5), which means strong results on one-shot LLM pruning (Table 6).\n- The method proposed in the paper has a wide range of application scenarios, from CNNs (VGG), ResNets, to Transformers (DeiT, OPT)."}, "weaknesses": {"value": "- The reconstruction provides a \"rather small impact\" after full retraining, could means that the LLS algorithm is less critical to the final result.\n- The results are highly competitive but do not universally \"match or exceed\" SOTA. Its strength is the trade-off, not setting new accuracy records.\n- The global variance heuristic is effective but limited, as it ignores downstream layer sensitivity, treating all variance as equally important.\n- This method relies on computing and decomposing the Gram matrix $C_l = X_l X_l^T$, which has dimensions $n_l \\times n_l$ (where $n_l$ is the number of units/filters), the $O(n_l^3)$ cost of decomposition (and $O(n_l^2)$ for storage) could become a bottleneck for layers with an extremely large number of units.\n- Retraining is still necessary. Therefore, it is not a perfect one-shot solution.\n- While the method automates layer-wise ratios, it still relies on the global variance cutoff percentage hyperparameter, which needs to be set manually."}, "questions": {"value": "- Have the authors studied the computational/memory cost for very large FFN layers (e.g., $n_l > 10000$)? Is there a point where the cost of LDL decomposition itself becomes a practical bottleneck, and have you considered approximate decomposition methods?\n- Given the very similar final performance between ZCA and SAW ordering, what is the practical justification for using the more complex ZCA ordering over the simple SAW baseline?\n- Have the authors considered a simple hybrid global metric, such as weighting the \"cumulative variance\" by a first-order Taylor-expansion score, to create a more robust, sensitivity-aware pruning heuristic?\n- How did you determine the optimal global variance percentage for your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LYjNyYHuXP", "forum": "2iMSDChf21", "replyto": "2iMSDChf21", "signatures": ["ICLR.cc/2026/Conference/Submission12272/Reviewer_Rnnj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12272/Reviewer_Rnnj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754161686, "cdate": 1761754161686, "tmdate": 1762923207063, "mdate": 1762923207063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method of pruning that focuses on pruning away the nodes whose variance can largely be explained by others. They do so via a lower-triangular orthogonalization that implies each subsequent node's \"unique\" variance residue outlines how much of the node's values can be linearly mapped using the previous ones. As this process is order dependent the authors naturally have to propose a way of optimizing the ordering. This they do via avoiding the permutative computational burden and instead just computing the importance score for each node via its orthogonalization w.r.t remaining nodes. Subsequently, as one still needs to find a consistent strategy for pruning nodes across layers, the authors subsequently appropriately normalize the LDL diagonal elements by computing the cumulative percentage of variance lost if pruned at a certain node. Results are shown on Imagenet, with VGG and Resnet models, and subsequently using Transformer networks and some final no-retraining results for language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors test their approach on a variety of settings overall, and look at various aspects of the pruning problem such as FLOP and pruning cost. \n- Both CNN and Transformer models are investigated.\n- Pruning cost is low, as the authors avoid the permutative blow-up of possibilities via fixing the ordering via Importance scoring"}, "weaknesses": {"value": "There are some significant issues that come to mind that may need addressing (in no order of importance):\n\n- Although it makes a bit of sense to me why this approach would outperform \"without\" retraining as the weights are re-parameterized accordingly after pruning away the redundant nodes, as the authors themselves acknowledge re-training is still critical to obtaining good performance. As the main objective of pruning is to reduce compute cost while preserving performance, I don't see the significance of the no-retraining results \n- Following up with the point above, I also don't see the immediate significance of the proposed approach having low pruning cost. Ultimately performance matters while reducing the final compute, and the authors are doing re-training to get to the best performance anyway (which adds 10+ hours), so given that much additional time I don't immediately see the utility of low pruning cost in this pipeline. Apart from this, overall, I didn't get the feeling from the results that the improvements, flop and accuracy wise are that significant. Furthermore, I think the comparisons would definitely benefit from more baselines, especially for Table 2. \n- A general issue I have with node based pruning is that without the actual impact of each node on the rest of the architecture, it is not clear whether one should prune it. To give an example, consider simple variance based pruning for each node, by ordering the nodes from lowest to highest variance (and then subsequently adapting the layer's bias to account for the node removal). This approach initially seems quite reasonable as nodes with lower variance should have a lower impact on the overall network's decision, after having adjusted for bias. However, it is definitely possible that the network weights have adapted to the lower variance nodes by increasing in magnitude so that the \"effective variance\" of the low variance nodes may not be low. So in that case, pruning away the lowest variance nodes is not a guarantee that one is actually removing the important ones. And my observation is also that the authors' proposed approach should not be that dissimilar from a simple variance based pruning strategy, as they use \"unnormalized-ZCA\" ordering, which would have a significant bias towards giving low scores to units with lower variances. So I'm also not sure how different the orderings/pruned nodes would be when compared to simple variance/energy based pruning methods."}, "questions": {"value": "Same as weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E9SQokIkjL", "forum": "2iMSDChf21", "replyto": "2iMSDChf21", "signatures": ["ICLR.cc/2026/Conference/Submission12272/Reviewer_MsgY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12272/Reviewer_MsgY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805018697, "cdate": 1761805018697, "tmdate": 1762923206678, "mdate": 1762923206678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Subspace Node Pruning, a method that projects neural activations into an orthogonal subspace to identify and remove redundant units. Pruned nodes are approximately reconstructed through linear least squares, avoiding full retraining. The authors introduce an unnormalized-ZCA-based importance metric and a variance-based global pruning ratio to automate layer-wise sparsity. Experiments on CNNs, Transformers, and OPT language models show comparable or slightly better accuracy than prior work, with lower computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear and coherent technical idea. Using orthogonal subspaces for pruning is a reasonable formulation, and the derivation based on LDL or Gram-Schmidt decomposition is easy to follow. The proposed global variance criterion provides a practical way to automatically balance pruning ratios across layers without manual tuning. The experimental coverage is fairly good, including CNNs, Transformers, and language models, and the results are generally consistent, indicating that the method is sound and well-implemented."}, "weaknesses": {"value": "(1) The empirical gains over strong baselines appear relatively modest. Improvements are generally small and sometimes not consistent across models or pruning ratios. The results are solid but may not convincingly demonstrate a clear advantage over prior work.\n\n(2) The presentation could be further polished. Some definitions and notations could be clarified, and a few minor typographical or formatting issues slightly affect readability. In addition, the overall structure of the paper is somewhat unconventional, which makes it harder to follow the logical flow and locate key methodological details."}, "questions": {"value": "(1) The use of orthogonalization for pruning appears related to prior work on redundancy-based and subspace reconstruction methods, and it would be helpful to more clearly articulate what is fundamentally new in this formulation and how it differs conceptually from existing techniques.\n\n(2) The claimed efficiency of the method might come with certain trade-offs. In Table 4, even when the amount of data used during pruning is increased, the improvement remains marginal compared with the baselines. The authors are encouraged to clarify whether the reported speed and simplicity come at the cost of performance or data efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1nMWeKJf8h", "forum": "2iMSDChf21", "replyto": "2iMSDChf21", "signatures": ["ICLR.cc/2026/Conference/Submission12272/Reviewer_2DnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12272/Reviewer_2DnV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817527275, "cdate": 1761817527275, "tmdate": 1762923206209, "mdate": 1762923206209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}