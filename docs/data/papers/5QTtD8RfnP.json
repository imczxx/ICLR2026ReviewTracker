{"id": "5QTtD8RfnP", "number": 21755, "cdate": 1758321355393, "mdate": 1759896904758, "content": {"title": "GNPA-DIL: Unveiling the Vulnerability Genome Through Semantic Graph Distillation and Invariant Neural Reasoning", "abstract": "Software vulnerabilities constitute an escalating security crisis with over 25,000 new CVEs documented annually, demanding detection models capable of identifying complex vulnerability patterns across evolving codebases. Contemporary vulnerability detection models exhibit catastrophic brittleness when deployed beyond controlled benchmarks, failing to maintain accuracy on rigorously-validated samples and collapsing entirely when confronted with routine syntactic variations or cross-function vulnerability patterns. The GNPA-DIL model overcomes these limitations through a neural architecture trained on vulnerability-centric program slices extracted via Code Property Graphs, learning domain-invariant representations that capture fundamental vulnerability semantics rather than superficial code patterns. By learning to process dramatically compressed program representations, the GNPA-DIL model transcends the context limitations plaguing existing architectures while preserving the critical information flows that characterize actual vulnerabilities. This fundamental advance in vulnerability representation learning enables the model to generalize beyond its training distribution, detecting previously unseen vulnerability types with 63.48\\% accuracy on Emerging-Post-Vulnerability CVEs. On the SVEN benchmark, GNPA-DIL achieves 73.58\\% F1-score compared to the best baseline's 54\\%, representing a 36\\% relative improvement, while maintaining 67.63\\% accuracy on cross-function vulnerabilities despite being trained only on function-level data.", "tldr": "", "keywords": ["code security", "transfer learning", "reinforcement learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/763e98346dea4f9920bf17b2104a5a0beb4219d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a graph-guided neural program analysis framework (GNPA-DIL) that effectively bridges symbolic static analysis and neural learning. By combining Code Property Graph (CPG) analysis with domain-invariant learning, the model captures the semantics of vulnerabilities rather than superficial syntax patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is well sounded and has significant practical value\n\nThe manuscript makes an attempt to introduce a rigorous mathematical, formal definitions on the vulnerability detection task, a rare and commendable feature in vulnerability detection research."}, "weaknesses": {"value": "The manuscript forms a structured and sounded motivations, but the paper’s formality may obscure accessibility. Theoretical sections (e.g., Theorems 1–6, Definitions 1–4) dominate the presentation but lack intuitive explanation or ablation to confirm practical contribution of each formal component.\n\nThe architectural novelty, while well-motivated, could be viewed as an incremental synthesis of prior CPG + invariant-learning approaches rather than a fundamentally new paradigm.\n\nMinor issue: Duplicate references (line 538, 541)"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RngHRsyZf8", "forum": "5QTtD8RfnP", "replyto": "5QTtD8RfnP", "signatures": ["ICLR.cc/2026/Conference/Submission21755/Reviewer_16E8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21755/Reviewer_16E8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894437036, "cdate": 1761894437036, "tmdate": 1762941920119, "mdate": 1762941920119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed GNPA-DIL model is trained on vulnerability-centric program slices extracted using Code Property Graphs, enabling it to learn domain-invariant representations that capture fundamental vulnerability semantics rather than superficial code patterns. The robustness analysis is conducted thoroughly."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is rigorously evaluated across multiple benchmarks, demonstrating consistent performance gains."}, "weaknesses": {"value": "1. This paper is hard to follow as the mathematical formalisms in Section 3 are highly dense and presented without sufficient intuitive explanation. \n\n2. Minor inconsistencies in citation formatting are present."}, "questions": {"value": "How is the Wasserstein constraint (Theorem 5) actually enforced during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uXL0MgSasu", "forum": "5QTtD8RfnP", "replyto": "5QTtD8RfnP", "signatures": ["ICLR.cc/2026/Conference/Submission21755/Reviewer_9Y1D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21755/Reviewer_9Y1D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898734605, "cdate": 1761898734605, "tmdate": 1762941919784, "mdate": 1762941919784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a neural network approach for vulnerability detection. The GNPA-DIL model presents a neural architecture trained on vulnerability-centric program slices extracted via Code Property Graphs, learning domain-invariant representations. The model achieves accuracy of 73.58% on SVEN."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The use of neural networks for vulnerability detection is an active area of research. The paper contributes in that space."}, "weaknesses": {"value": "The paper os hard to read. It consists of introduction, related work and a set of theorems with limited explanations. The authors appear to have made heavy use of LLMs when writing the paper (which they acknowledged0.\n\nExperiments are not convincing as accuracy seems low."}, "questions": {"value": "I could not understand the high level approach-- please descrine."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JSG4owZ8o2", "forum": "5QTtD8RfnP", "replyto": "5QTtD8RfnP", "signatures": ["ICLR.cc/2026/Conference/Submission21755/Reviewer_B5Uy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21755/Reviewer_B5Uy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960508427, "cdate": 1761960508427, "tmdate": 1762941919346, "mdate": 1762941919346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GNPA-DIL, a vulnerability detection model that combines CPG with domain-invariant neural learning. The approach extracts vulnerability-centric program slices from CPGs and trains neural networks with domain-invariant constraints to achieve robustness against semantic-preserving transformations. The authors claim significant improvements over baselines on multiple benchmarks including SVEN  and demonstrate cross-function generalization capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Relevant Problem**: Addressing robustness of vulnerability detectors to semantic-preserving transformations is important and timely\n\n**Cross-Function Generalization**: The ability to detect project-level vulnerabilities despite function-level training (67.63% on ReposVul) is potentially valuable if validated properly\n\n**Multi-Benchmark Evaluation**: Testing across diverse datasets with different characteristics (synthetic vs. real-world, function-level vs. project-level) is valuable"}, "weaknesses": {"value": "Soundness:\n\n1. Mathematical Rigor vs. Practical Implementation Gap: The paper presents extensive mathematical formalism (Sections 3.1-3.7) involving Riemannian manifolds, variational methods, Wasserstein distances, and wavelet decompositions. However, there is a complete disconnect between this theoretical framework and the actual implementation. The paper did not explain:\n\n- How the \"vulnerability manifold\" (Eq. 6) is constructed in practice\n- How the Bellman operator (Eq. 15) is computed\n- Whether the wavelet decomposition (Eq. 25) is actually used in the model\n\nThis suggests the mathematical framework may be decorative rather than functional.\n\nMissing Architecture Details: Despite the heavy mathematical notation, basic implementation details are absent:\n\n- What neural architecture is used? (GNN? Transformer? RNN?)\n- How are CPG slices encoded as neural network inputs?\n- What is the model size and computational complexity?\n- How is domain-invariant learning actually implemented in the training procedure?\n\nDataset Quality Concerns: The paper reduces FormAI from 331,000 to 8,259 samples (97.5% reduction) and PrimeVul from 235,768 to 2,096 samples (99.1% reduction) through a \"three-phase refinement\". This extreme filtering raises concerns:\n\n- Is the model learning from a representative sample or cherry-picked easy cases?\n- How do baselines perform when trained on the same filtered dataset?\n- The paper doesn't provide fair comparisons with baselines on identical training data\n\nIncomplete Experimental Validation:\n\n- No ablation study showing the contribution of CPG slicing vs. domain-invariant learning vs. other components\n- No comparison on identical training data with baselines\n- No analysis of failure cases or error types\n\nContribution:\n\nThe claimed contribution “unveiling the vulnerability genome” is ambitious, but the delivered contribution is a flawed experiment, an unverified slicing method, and a confused methodology.\n\n- **“Vulnerability genome” :**\n\n  This metaphor is exaggerated and unscientific. On the expert-validated PrimeVul benchmark, the recall is only about 40%, directly disproving the claim that the model has “unveiled the genome.” At best, it captures some invariant features in some cases. The overstated framing hurts the paper’s credibility.\n\n- **Actual potential contribution:**\n\n  The true potential lies in cross-granularity generalization. However, this value is undermined by other flaws, most notably the following contradiction:\n\n  The paper reports opposite behaviors on PrimeVul (high-precision / low-recall) and SVEN (high-recall / medium-precision) without any explanation, revealing a serious inconsistency that undermines the validity of its experimental results.\n\nPresentation:\n\n1. **Excessive Mathematics Without Justification:**\n\n   Theoretical complexity (Riemannian geometry, measure theory, functional analysis) is introduced without showing why it is necessary. For example:\n\n   - Why view vulnerabilities as a *“Riemannian substructure”* (Def. 2)?\n   - How is the Hausdorff distance to the *“vulnerability manifold”* (Eq. 14) computed?\n\n2. **Unclear Architecture Figure:**\n\n   Figure 1 shows pipeline stages but lacks detail on what each component actually performs.\n\n3. **Weak Related Work:**\n\n   The related-work section is thin and does not adequately situate this work within prior literature.\n\n4. **Missing or Redundant Figures/Tables:**\n\n   Figures 3–5 are redundant, and several “Tables” are referenced but not actually present in the paper."}, "questions": {"value": "1. Are the mathematical definitions and theorems actually implemented, or are they just conceptual analogies? Please clarify their connection to the implementation.\n2. Could you include ablations isolating the impact of:\n\n- CPG slicing (vs. full code input),\n- domain invariance (with/without Wasserstein regularizer),\n- and the graph neural architecture choice?\n\n3. Can you explain the opposite behaviors on PrimeVul (high-precision/low-recall) and SVEN (high-recall/medium-precision)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ljlRzPtKsC", "forum": "5QTtD8RfnP", "replyto": "5QTtD8RfnP", "signatures": ["ICLR.cc/2026/Conference/Submission21755/Reviewer_EZox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21755/Reviewer_EZox"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093684065, "cdate": 1762093684065, "tmdate": 1762941918974, "mdate": 1762941918974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}