{"id": "0eEtTsnmyo", "number": 12686, "cdate": 1758209496628, "mdate": 1759897493882, "content": {"title": "On Uniformly Scaling Flows: A Density-Aligned Approach to Deep One-Class Classification", "abstract": "Unsupervised anomaly detection is often framed around two widely studied paradigms. Deep one-class classification, exemplified by Deep SVDD, learns compact latent representations of normality, while density estimators realized by normalizing flows directly model the likelihood of nominal data. In this work, we show that uniformly scaling flows (USFs), normalizing flows with a constant Jacobian determinant, precisely connect these approaches. Specifically, we prove how training a USF via maximum-likelihood reduces to a Deep SVDD objective with a unique regularization that inherently prevents representational collapse. This theoretical bridge implies that USFs inherit both the density faithfulness of flows and the distance-based reasoning of one-class methods. We further demonstrate that USFs induce a tighter alignment between negative log-likelihood and latent norm than either Deep SVDD or non-USFs, and how recent hybrid approaches combining one-class objectives with VAEs can be naturally extended to USFs. Consequently, we advocate using USFs as a drop-in replacement for non-USFs in modern anomaly detection architectures. Empirically, this substitution yields consistent performance gains and substantially improved training stability across multiple benchmarks and model backbones for both image-level and pixel-level detection. These results unify two major anomaly detection paradigms, advancing both theoretical understanding and practical performance.", "tldr": "We uncover a theoretical bridge between deep one-class classification (e.g., Deep SVDD) and a class of normalizing flows called Uniformly Scaling Flows (USFs).", "keywords": ["anomaly detection", "normalizing flows", "one-class classification", "representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b03d81395f56c2a32058f5b0608882b2d879d582.pdf", "supplementary_material": "/attachment/92ce3625c47d78b40a7e0972236bd98e1c8db377.zip"}, "replies": [{"content": {"summary": {"value": "The paper connects Deep SVDD and normalizing flows for anomaly detection by introducing a subclass called Uniformly Scaling Flows (USFs), where the Jacobian determinant is constant across inputs. Under this setup, the maximum-likelihood objective of a flow becomes equivalent to the Deep SVDD loss, offering a simple link between one-class classification and flow-based modeling. The authors argue that this keeps the stability and invertibility benefits of flows while avoiding the collapse issues of Deep SVDD. In experiments, they replace affine coupling layers with additive USF layers in existing flow-based detectors (FastFlow, CFlow, U-Flow) using frozen pretrained backbones. Results on MVTec AD and VisA show that the USF variants reach similar or slightly better accuracy, with improved training stability and less sensitivity to initialization. The paper positions USFs as a lightweight and theoretically grounded alternative to standard flow-based anomaly detectors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper formalizes a clear theoretical link between Deep SVDD and a subclass of normalizing flows with constant Jacobian determinants, unifying two separate paradigms of anomaly detection through an analytical derivation.\n- The paper reinforces previous observations that the log-determinant term can dominate likelihood in flow-based detectors, and systematically validates that removing input-dependent volume terms leads to more stable training.\n- The paper includes a range of controlled experiments and ablations that clearly isolate the effect of the proposed change, making the empirical findings easy to interpret.\n- The paper is generally easy to follow, with clean derivations and consistent notation. The theory-to-experiments flow is coherent, and the ablation results are presented in a readable and well-organized manner."}, "weaknesses": {"value": "- The paper builds its main theoretical motivation on Deep SVDD, a 2018 one-class method that has largely fallen out of use in both image and tabular anomaly detection. Modern benchmarks (e.g., ADBench, MVTec, VisA) consistently show that simple feature-based or transformer-based approaches, contrastive learning, and diffusion-based detectors far outperform Deep SVDD, which is now mostly cited for historical context. As a result, grounding the paper’s core theory around an outdated baseline places it in an awkward position. The conceptual bridge it draws may be elegant, but its practical impact is limited unless validated against current methods.\n-  The paper frames Uniformly Scaling Flows (USFs) as a key innovation for removing input-dependent volume effects, but constant-Jacobian / volume-preserving flows have long existed in the literature (e.g., NICE; FlowSVDD; OneFlow), and have already been applied specifically to anomaly detection to avoid SVDD-style collapse. As such, the only substantial novelty lies in the formal SVDD–USF MLE equivalence and the controlled additive-vs-affine ablations, not in the idea of using constant-determinant flows itself. \n- The empirical section mostly replaces affine coupling with additive coupling in pre-existing architectures (FastFlow, CFlow, U-Flow) using pretrained frozen features. The practical contribution is thus modest. It is more of an architectural ablation than a fundamentally new detection framework. The theoretical bridge to Deep SVDD does not clearly lead to a new practical algorithm beyond this substitution.\n- Each baseline uses a different pretrained backbone (ResNet-18, WRN-50-2, CaiT), so absolute numbers are not directly comparable across architectures; only within-architecture swaps are fair. A shared backbone control would strengthen the empirical claims.\n- In the main text, in Proposition 1, $F_\\alpha(x)=x/(\\alpha||x||)$ gives $||F_\\alpha(x)||\\equiv 1/\\alpha$ for all $x\\neq0$, so the claimed monotonicity $||x||\\uparrow\\Rightarrow||F_\\alpha(x)||\\downarrow$ is false; yet the loss is then (incorrectly) written as $\\alpha^{-2}\\mathbb{E}||x||^{-2}$. In Appendix B.4, $F_\\alpha$ is redefined as the scalar $1/(\\alpha|x|)$ but treated as vector-valued, creating a type mismatch with the Deep-SVDD loss $\\mathbb{E}||F(X)-c||^2$. A minimal fix is the vector-valued radial inversion $F_\\alpha(x)=x/(\\alpha||x||^2)$ with $F_\\alpha(0)=0$, which yields the intended ordering and $L=\\alpha^{-2}\\mathbb{E}\\big[1/||x||^2\\big]=1/\\big(\\alpha^2(d-2)\\big)$ for $d>2$.\n- The citations should be in parentheses using \\citep."}, "questions": {"value": "- How sensitive are the reported gains to the choice of pretrained backbone? Would the same improvements hold if all methods shared an identical architecture or were trained from scratch?\n- How does the proposed USF formulation differ from prior work such as FlowSVDD, which also uses constant-Jacobian (volume-preserving) flows for anomaly detection and explicitly argues that this avoids hypersphere collapse?\n- How does the improved flow baselines compared to state-of-the-art non-flow baselines?\n- The experiments rely on MVTec AD and VisA, both image datasets, even though all features come from pretrained ImageNet backbones. Why restrict the evaluation to images? If the method is architecture-agnostic, testing on tabular anomaly detection (e.g. ADBench) could better show its generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uB4iYlR9fV", "forum": "0eEtTsnmyo", "replyto": "0eEtTsnmyo", "signatures": ["ICLR.cc/2026/Conference/Submission12686/Reviewer_U6Ni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12686/Reviewer_U6Ni"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760738535375, "cdate": 1760738535375, "tmdate": 1762923523501, "mdate": 1762923523501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. We appreciate the recognition of our theoretical contributions and empirical validation. We address the common concerns raised across reviews below, with detailed responses to individual reviewers following.\n\n### On Practical Value and Scope of Experiments\n\nSeveral reviewers noted the limited experimental scope (image-only datasets) and requested broader validation. We acknowledge this limitation and are actively extending our experiments to address these concerns. Our survey of current flow-based anomaly detection libraries reveals that USF architectures are largely absent from mainstream implementations:\n\n- **GANF** (Graph AD with NFs): Only non-USFs\n- **STG-NF**: Contains both USF and non-USF variants\n- **Anomalib**: Only non-USFs  \n- **SanFlow**: Only non-USFs\n\nThis gap underscores the practical value of our investigation. Since the fundamental differences in latent space alignment and their consequences for anomaly detection are currently under-investigated, our experiments clearly indicate that USF architectures have the potential to improve a large number of highly performant AD methods in a drop-in fashion. A well-grounded theory that connects density estimation and latent norm-based reasoning allows practitioners to make informed decisions during AD algorithm design, which can have substantial influence as our experiments demonstrate.\n\nWe note that recent theoretical work has analyzed conditions under which volume-preserving flows may be suboptimal, particularly for multi-modal distributions (Draxler et al., ICML 2024, \"On the Universality of Volume-Preserving and Coupling-Based Normalizing Flows\"). Interestingly, their analysis reveals that when the latent distribution is rotationally symmetric with density decreasing from the origin—exactly our setup with isotropic Gaussian bases—volume-preserving flows learn the same transport map as more expressive alternatives. This theoretical insight supports our empirical finding that USF substitution improves performance in the anomaly detection setting, where the base distribution satisfies these conditions.\n\n### On Novelty Beyond Prior Work (FlowSVDD, OneFlow)\n\nMultiple reviewers questioned the novelty relative to FlowSVDD and OneFlow. We emphasize that while these works employ volume-preserving flows, they fundamentally differ from our approach:\n\n1. **Different Loss Functions**: Both FlowSVDD and OneFlow develop custom loss functions rather than using maximum likelihood estimation. FlowSVDD jointly learns a decision radius, breaking the direct connection to density estimation. OneFlow uses a custom loss based on Bernstein polynomial quantile estimates and volume minimizations.\n\n2. **Different Theoretical Guarantees**: Their theoretical analyses focus solely on collapse prevention through bijectivity. Our argument relies on both bijectivity *and* the direct connection to maximum likelihood training—i.e., minimization of the reverse KL-divergence to a Gaussian in latent space—which provides provably non-degenerate optima (Section 4.2).\n\n3. **Latent Space Alignment Analysis**: Neither work analyzes the monotonic alignment between data density and latent norm that we establish in Section 4.3. This property is crucial for understanding why USFs provide superior anomaly ranking and is not merely a consequence of avoiding collapse.\n\n4. **Unified Perspective**: Our contribution lies in revealing that standard MLE training of USFs *already implements* an effective Deep SVDD variant with implicit regularization, unifying density estimation and one-class classification principles. This theoretical bridge has not been previously established in the literature.\n\n### On Experimental Design and Future Work\n\nWe designed our experiments to isolate the causal effect of the USF substitution through controlled within-architecture comparisons. While this means absolute numbers may be lower than current leaderboards (as we did not perform model-specific tuning), it ensures that observed improvements are attributable to the architectural change rather than hyperparameter optimization artifacts. We are extending our evaluation to include additional datasets and modalities in the revised manuscript."}}, "id": "VLyO4VirnE", "forum": "0eEtTsnmyo", "replyto": "0eEtTsnmyo", "signatures": ["ICLR.cc/2026/Conference/Submission12686/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12686/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12686/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718064134, "cdate": 1763718064134, "tmdate": 1763718064134, "mdate": 1763718064134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Uniformly Scaling Flows (USFs), a restricted class of normalizing flows where the Jacobian determinant is constant across the input space. The authors show that maximum-likelihood training of such a flow can be reformulated as a Deep SVDD-style objective with an implicit regularization term preventing collapse. Experiments on replacing standard coupling layers in FastFlow, CFlow, and U-Flow with USF variants showed moderate AUROC improvements and notably reduced run-to-run variance on MVTec AD and VisA benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clean mathematical connection between one-class classification and flow-based objectives.\n2. This work empirically shows improved training stability across several flow architectures.\n3. The idea of exploring Jacobian regularization may inspire follow-up work on stable density modeling."}, "weaknesses": {"value": "1. The theoretical equivalence is mathematically valid but largely follows from simplifying the log-det term in standard flows. It does not lead to a new learning principle.\n2. The main idea is an extension of the prior hybrid flow–SVDD and One-Flow works. The difference from these prior works is a mild variation rather than a substantive theoretical or algorithmic innovation.\n3. Only modest empirical improvements are shown on many classes in the experiments.\n4. The evaluation is limited to two industrial anomaly detection datasets, and the experimental comparisons are limited to flow-based methods. They didn't include the latest SOTA methods into the experimental comparisons."}, "questions": {"value": "1. The derivations are internally consistent and reproduce known properties of flow-based AD models. However, the equivalence theorem is straightforward once the constant determinant assumption is applied, and the empirical validation does not seem to justify the claimed unification.\n2. Section 5 on experiments reads like an afterthought and does not deeply analyze why stability is improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nbrtiZPC8h", "forum": "0eEtTsnmyo", "replyto": "0eEtTsnmyo", "signatures": ["ICLR.cc/2026/Conference/Submission12686/Reviewer_b6fW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12686/Reviewer_b6fW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974461711, "cdate": 1761974461711, "tmdate": 1762923523117, "mdate": 1762923523117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The theory is clear and careful: training a uniformly scaling flow (USF) with MLE reduces to a Deep-SVDD-style objective with an implicit weight regularizer, which explains why collapse is avoided. The constant-Jacobian view cleanly links density level sets to latent norms. Empirically, the drop-in USF swap improves stability strongly and often accuracy across MVTec AD and VisA on CFlow and U-Flow (e.g., ~72.5→90.8 and ~92.7→94.5), while FastFlow is roughly neutral. The scope is mainly image AD; dimensionality-reduction needs the proposed VAE hybrid. Overall, technically solid."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear theoretical link between USFs and Deep SVDD that explains behavior and avoids collapse.\n2. Practical “drop-in” recipe that substantially reduces run-to-run variance; accuracy gains on CFlow/U-Flow are convincing.\n3. Broad evaluation across datasets, metrics, and architectures; ablations isolate the US modification.\n4. Writing is professional; related work coverage is adequate."}, "weaknesses": {"value": "1. Architectural novelty is incremental (additive/volume-preserving flows are known); the novelty depends mostly on the connection and analysis.\n2. Expressivity trade-off vs. affine coupling is under-analyzed; FastFlow gains are limited.\n3. Evaluation scope is visual AD; no results on tabular/time-series or non-vision AD.\n4, VAE-USF section is promising but under-evaluated (few settings, limited ablations).\n5. Baselines emphasize flow variants; adding non-flow one-class/reconstruction baselines would strengthen the broader claim."}, "questions": {"value": "1. Beyond isotropic Gaussian bases, do heavy-tailed or mixture bases preserve the same alignment benefits in USFs?\n2. For FastFlow, why are gains small? Capacity, optimization, or interaction with 2D conv flows?\n3. Did you actually apply the log-normal prior on det(J) during training, and how sensitive are results to its hyperparameters?\n4. In VAE-USF, how did you set the reconstruction vs. likelihood weighting, and did varying latent dimension change the story?\n5. Any path to dimension reduction without the VAE (e.g., relaxed invertibility or partial flows)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iFNzLzX8LO", "forum": "0eEtTsnmyo", "replyto": "0eEtTsnmyo", "signatures": ["ICLR.cc/2026/Conference/Submission12686/Reviewer_cPS3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12686/Reviewer_cPS3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994669613, "cdate": 1761994669613, "tmdate": 1762923522688, "mdate": 1762923522688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}