{"id": "dpVhlf13tk", "number": 14174, "cdate": 1758229681745, "mdate": 1763721531208, "content": {"title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks", "abstract": "Code-capable large language model (LLM) agents are increasingly embedded into software engineering workflows where they can read, write, and execute code, raising the stakes of safety-bypass (“jailbreak”) attacks beyond text-only settings. Prior evaluations emphasize refusal or harmful-text detection, leaving open whether agents actually compile and run malicious programs. We present **CodeAgentJail**, a benchmark spanning three escalating workspace regimes that mirror attacker capability: empty (CAJ-0), single-file (CAJ-1), and multi-file (CAJ-M). We pair this with a hierarchical, executable-aware **Judge Framework** that tests (i) compliance, (ii) attack success, (iii) syntactic correctness, and (iv) runtime executability, moving beyond refusal to measure deployable harm. Using seven LLMs from five families as backends, we find that under prompt-only conditions in CAJ-0, code agents accept 61\\% of attacks on average; 58\\% are harmful, 52\\% parse, and 27\\% run end-to-end. Moving to single-file regime in CAJ-1 drives compliance to $\\sim$100\\% for capable models and yields a mean ASR (*Attack Success Rate*) $\\approx$71\\%; the multi-file regime (CAJ-M) raises mean ASR to $\\approx$75\\%, with 32\\% being instantly deployable attack code. Across models, wrapping an LLM in an agent substantially increases vulnerability -- ASR raises  by 1.6$\\times$ -- by frequently overturning initial refusals during planning/tool-use steps. We further observe similar jailbreak trends when replacing OpenHands with SWE-Agent and OpenAI Codex, suggesting that CodeAgentJail is agent-agnostic.\nCategory-level analyses identify which attack classes are most vulnerable and most readily deployable, while others exhibit large execution gaps. These findings motivate execution-aware defenses, code-contextual safety filters, and mechanisms that preserve refusal decisions throughout the agent’s multi-step reasoning and tool use.", "tldr": "We introduce CodeAgentJail-Bench -- spanning empty, single-file, and multi-file workspaces -- together with a hierarchical, executable-aware judge pipeline, showing code agents often compile and run malicious code.", "keywords": ["ai", "agent", "code", "jailbreak", "security", "attack", "judge"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff73444ef73aad5c435f66bca0f729d3303071d3.pdf", "supplementary_material": "/attachment/493b36e238220c822cecdd9cf054b2b018052a4f.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents CodeAgentJail, a benchmark covering three workspace regimes: empty (CAJ-0), single-file (CAJ-1), and multi-file (CAJ-M). It is paired with a hierarchical, executable-aware Judge Framework that evaluates (i) compliance, (ii) attack success, (iii) syntactic correctness, and (iv) runtime executability. The authors conduct experiments on different models built upon OpenHands and find that agent frameworks tend to be more vulnerable than base models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The multi-file setting is interesting and realistic."}, "weaknesses": {"value": "W1. Limited novelty in benchmark construction design\n\nThe benchmark contribution seems limited. Levels 0 and 1 are largely inherited from RMCBench, while level 2/M is generated by prompting an uncensored model to produce detailed code. However, it is unclear whether the generated code truly aligns with malicious purposes and is both practical and executable as intended.\n\nW2. Overlap with prior executable-aware benchmarks\n\nThe paper claims:\n\n> “Prior evaluations emphasize refusal or harmful-text detection, leaving open whether agents actually compile and run malicious programs. To our knowledge, this is the first executable-aware benchmark for code agents across all three workspace settings.”\n\nHowever, previous work (e.g., RedCode) also emphasizes executable-aware evaluation of risky and malicious program generation and execution.\n\nW3. Reliability of LLM-based executability judges\n\nThe proposed executability judge is itself an LLM-based agent. This raises concerns about consistency and reliability—e.g., a “attack evaluation judge” may consider a sample malicious, while the “executability judge” might not.\n\nMoreover, the claim that the judge “never edits or overwrites existing files” is unconvincing, as the agent may still inadvertently modify the workspace. Additionally, agents could refuse to execute potentially malicious code, which complicates runtime evaluation."}, "questions": {"value": "Q1: What are the specific design innovations in the benchmark construction compared to prior work?\n\nQ2: What additional significant contributions does this work provide beyond RedCode [1]?\n\nQ3: How do the authors ensure that the LLM-based runtime evaluation judge remains reliable, especially given the malicious or unsafe nature of the evaluated tasks?\n\n\n[1] RedCode: Risky Code Execution and Generation Benchmark for Code Agents (NeurIPS 2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Rg048EGI1k", "forum": "dpVhlf13tk", "replyto": "dpVhlf13tk", "signatures": ["ICLR.cc/2026/Conference/Submission14174/Reviewer_XHKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14174/Reviewer_XHKK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439728033, "cdate": 1761439728033, "tmdate": 1762924632418, "mdate": 1762924632418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark to evaluate if a coding agent actually compile and run malicious programs generated by itself in response to jailbreaking queries. The benchmark covers three regimes - starting with an empty repo, completing a single-file codebase, and working in a multi-file codebase. The paper also proposes a judge framework testing compliance with the harmful request and attack success, along with syntactic correctness and executability. 7 LLMs are benchmarked, and show significant potential for misuse. As more context is provided in the codebase, the attack success rate increases. LLMs are shown to be more vulnerable than their base forms when they are wrapped in the OpenHands agentic framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A three-tiered benchmark is provided for evaluating the jailbreak susceptibility of coding agents, covering different stages of completion of the codebase.\n- An intuitive framework is provided for judging the success rates of the proposed attacks, with a 4-stage evaluation.\n- Some interesting analysis is provided to explain why LLMs are more vulnerable to jailbreak attacks when wrapped in the OpenHands agentic framework, involving the overturning of refusal in latter stages of planning.\n- Detailed analysis is provided to determine the most common classes of vulnerabilities present in contemporary coding agents."}, "weaknesses": {"value": "- 2 splits of the codebase is derived from existing work, and no analysis is provided for the quality of the CAJ-M split, which was created by this work. Therefore, the proposed benchmark largely builds on existing work. How does CAJ compare with other coding jailbreak benchmarks in terms of scale?\n- No analysis is provided to show whether the refusal judge and attack evaluation judge results in human-aligned evaluation of ASRs.\n- While this work focuses on executability of generated code, it does not consider how easily the generated code can be debugged via an agentic framework to be made executable, as long as it encodes the malicious intent. This somewhat limits the practicality of this benchmark, in my opinion.\n- While the paper claims that LLMs are more vulnerable when wrapped in an agentic framework, the only framework used in the paper is the OpenHands framework. An agentic framework with explicit checks for malicious intent would be more resilient to attacks than the base LLM."}, "questions": {"value": "- Can this benchmark be used to evaluate Claude Code/Codex-style agentic frameworks?\n- Does the executability judge approach introduced in this work generalize to other coding jailbreak benchmarks?\n- In CAJ-1/CAJ-M, are checks done to ensure that the part of the file that the agent is tasked with filling in also involves malicious content?\n- Why are separate syntax-error and runtime-error judges proposed in the evaluation pipeline? Most syntax errors can be easily corrected by a coding agent with a linter as one of its tools.\n- The “Problem Definition” section of the paper feels somewhat overly lengthy, since most of the formalisms and terminology introduced is not used elsewhere in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xsn0SGhUWq", "forum": "dpVhlf13tk", "replyto": "dpVhlf13tk", "signatures": ["ICLR.cc/2026/Conference/Submission14174/Reviewer_z1ru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14174/Reviewer_z1ru"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773210758, "cdate": 1761773210758, "tmdate": 1762924631853, "mdate": 1762924631853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CODEAGENTJAIL, a benchmark and evaluation framework for assessing the security vulnerabilities of code-capable LLM agents under systematic jailbreak attacks. Unlike prior jailbreak studies focused on textual refusal, this work measures whether code agents actually generate and execute malicious code within realistic workspaces. The authors define three escalating attack regimes: CAJ-0 (empty workspace, prompt-only), CAJ-1 (single-file partial code), and CAJ-M (multi-file repository), and design a four-phase judge pipeline that evaluates compliance, harmfulness, syntactic correctness, and runtime executability. Experiments on seven major LLM families within the OpenHands agent framework show that (1) code agents are substantially more vulnerable than base LLMs (≈1.6× ASR increase), (2) compliance approaches 100% in seeded workspaces, and (3) up to 32% of multi-file attacks produce deployable malicious programs. The study highlights critical weaknesses in current safety mechanisms and calls for execution-aware and refusal-persistent defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark is methodologically innovative, bridging safety evaluation and systems engineering. Its multi-stage structure cleanly separates intent from operational harm, offering a more realistic safety metric. The experiments are large-scale, multi-model, and reproducible under open-source infrastructure. The ablation contrasting base LLMs and agentic wrappers compellingly isolates the cause of vulnerability escalation. The paper also contributes thoughtful discussions and future research directions on refusal persistence and execution-aware safety."}, "weaknesses": {"value": "While the paper is technically rigorous and methodologically sophisticated, several limitations constrain its novelty claims and generalizability.\n\n1. Overstated novelty of the “code-format vulnerability” finding.\nThe paper highlights that malicious requests written in code or tool-like formats are more likely to be executed than refused. However, this phenomenon has already been systematically demonstrated in prior literature, most notably in Drop the Guardrails: Tool-Primed Prompt Pairing and Refusal Behavior in GPT-OSS (Kevin Power, 2025; Kaggle Write-up, OpenAI Red-Teaming Hackathon Winner). That study quantified 15–42 percentage-point reductions in refusal rates when harmful requests were framed as code or tool calls, providing direct empirical support for the same effect discussed here.\n\n2. Limited language and environment diversity.\nAll evaluations are conducted in Python-based Dockerized workspaces using OpenHands agents. While this ensures reproducibility, it limits ecological validity since real-world code agents frequently operate in polyglot settings (e.g., JavaScript, SQL, shell scripting).\n\n3. Interpretability of “execution success.”\nTreating runtime executability as the primary indicator of harm can overstate actual exploitability. Many successful runs may simply produce syntactically valid but non-dangerous programs. Incorporating static vulnerability scanning (e.g., CodeQL or Semgrep) or privilege-aware evaluation would provide a more nuanced view of operational risk."}, "questions": {"value": "1. How exactly does this work extend Power (2025)’s “tool/code” findings? Is the main novelty the runtime/executable evaluation or something else?\n2. Can you show an example trace where an initial refusal is overturned during the agent’s planning/tool steps? \n3. How are Docker sandboxes configured to prevent real harm (network egress, privileges)?\n4. Did you try any simple defenses (e.g., persist refusal state across steps, pre-execution safety gate)? \n5. How does “runtime-error-free” map to real-world risk? Do you plan to add static checks (CodeQL/Semgrep) or human verification to filter false positives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYXoCDW8Uw", "forum": "dpVhlf13tk", "replyto": "dpVhlf13tk", "signatures": ["ICLR.cc/2026/Conference/Submission14174/Reviewer_rYaq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14174/Reviewer_rYaq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021061939, "cdate": 1762021061939, "tmdate": 1762924631395, "mdate": 1762924631395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CODEAGENT JAIL-Bench, a benchmark designed to assess the security of code-capable LLM agents against jailbreaking attacks across three increasingly realistic workspace regimes: empty (CAJ-0), single-file (CAJ-1), and multi-file (CAJ-M). The authors introduce a hierarchical, four-phase judge framework that evaluates not only policy compliance and attack success but also the syntactic correctness and runtime executability of the generated code, thereby measuring deployable harm rather than just textual refusal. Evaluations of seven LLMs reveal that wrapping models in an agentic framework increases attack success rates by an average of 1.6x, as multi-step planning often overturns initial refusals. The study finds that while single-file contexts drive compliance to nearly 100% for capable models, the multi-file regime is the most dangerous, yielding 32% instantly deployable attack code."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The benchmark introduces three escalating regimes (CAJ-0, CAJ-1, CAJ-M) that mirror real-world attacker capabilities, ranging from naive prompt-only attacks to single-file to multi-file repository manipulation.\n- This framework includes LLM judges to test if the malicious code actually parses, builds, and runs to completion, providing a strong measure of executable harm.\n- The paper provides empirical evidence that code agents are more vulnerable than their base LLMs as iterative reasoning and tool feedback loops can erode initial safety decisions."}, "weaknesses": {"value": "- Technical Novelty in Jailbreaking: Despite using \"jailbreak\" in the title, the paper does not any automated jailbreaking algorithms such as gradient-based or genetic search methods. It relies instead on direct prompting strategies and curated malicious codebases.\n- Security Scope: The evaluation primarily focuses on malware generation across categories like spyware and ransomware, without considering other critical security issues such as Common Weakness Enumeration (CWE) vulnerabilities.\n-  Dataset Contribution & Size: The CAJ-0 dataset is derived from existing benchmarks (RMCBench), limiting its novelty. The datasets are also relatively small: CAJ-0 contains only 182 prompts , CAJ-1 has 100 samples , and CAJ-M consists of 180 repositories\n- Evaluation:\n  - Deployable Harm: While the \"Runtime-Error Judge\" checks if code executes, it is unclear if the code is genuinely harmful or capable of causing real-world attacks. \n  - Reliance on LLM Judges: Attack success is determined by an LLM judge (specifically Claude-3.7-Sonnet ), which may suffer from inaccuracies compared to ground-truth security assessments. The authors could consider utilizing external/commercial validation tools like VirusTotal to confirm the detectability or functional maliciousness of generated code.\n  - Compared to Prior Work: The conclusion that agents are more harmful than base LLMs has been previously explored in works like RedCode (Guo et al., 2024a), raising questions about the extent of new insights provided by this specific experimental setup.\n\n- Agent Framework and Model Diversity: \n  - The evaluations only use the OpenHands agent framework, overlooking other relevant scaffolds like SWE-agent. \n  - High syntax and runtime error rates (e.g., only 4.33% average runtime success in CAJ-1 ) might stem from the base models' limited coding capabilities rather than effective safety guardrails. The evaluated models appear somewhat weak, and the inclusion of more state-of-the-art models (e.g., closed-source models) could be considered."}, "questions": {"value": "See weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JryElskzJu", "forum": "dpVhlf13tk", "replyto": "dpVhlf13tk", "signatures": ["ICLR.cc/2026/Conference/Submission14174/Reviewer_UA7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14174/Reviewer_UA7N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181099095, "cdate": 1762181099095, "tmdate": 1762924630908, "mdate": 1762924630908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}