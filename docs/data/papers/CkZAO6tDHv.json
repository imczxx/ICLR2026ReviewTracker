{"id": "CkZAO6tDHv", "number": 22088, "cdate": 1758325787877, "mdate": 1763753326828, "content": {"title": "Echoing: Identity Failures when LLM Agents Talk to Each Other", "abstract": "As large language model~(LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, *echoing*, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $66$ AxA configurations, $4$ domains (3 transactional, 1 advisory), and $2500+$ conversations (over $250{,}000$ LLM inferences), we show that echoing occurs across major LLM providers, with echoing rates as high as $70\\\\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\\\\%$) that are not reduced by reasoning efforts. We analyze prompt, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ agent turns) and is not merely an artifact of sub-optimal experiment design. Finally, we introduce a protocol-level mitigation where targeted use of structured response reduces echoing to $9\\\\%$.", "tldr": "", "keywords": ["Multi-Agent Systems", "Large Language Models", "Agent Alignment", "AI Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/caba05dc44c1dcbcf04c3c062d69a81309a351ef.pdf", "supplementary_material": "/attachment/c9ef5e15cda3abdb6e1419d75537300c7f67be16.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates a failure mode in agent-vs-agent (AxA) dialogues, termed echoing, in which an agent abandons its assigned identity and adopts that of its counterpart. The authors formalize AxA interactions and conduct experiments across different LLM backbones, prompts, and domains (e.g., hotel booking, car sales, and supply chain). Experimental results show that echoing is prevalent (occurring in approximately 5%–70% of cases), even in “reasoning” models (averaging about 32.8%), and often emerges in the later stages of dialogue (typically after seven turns), frequently masked by standard completion metrics. Furthermore, the authors propose a protocol-level scheme that mitigates echoing through structured responses, which reduces but does not completely eliminate it."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) This paper formally defines echoing as a distinct AxA failure, distinguishing it from generic errors or hallucinations as well as from prior MAS settings, and clearly elucidates the AxA framework.\n\n(2) This paper presents extensive experiments across multiple LLMs and domains, providing a solid empirical foundation for echoing research. Furthermore, the study analyzes failed cases and extracts relevant principles (prevention, non-intrusion, seamlessness, and architectural integration).\n\n(3) This study demonstrates that echoing persists even in stronger “reasoning” models and that standard evaluation metrics often mask this phenomenon. These findings prompt a rethinking of evaluation methods for AxA systems."}, "weaknesses": {"value": "(1) The text in the third section of Figure 1 is too small to be easily read.\n\n(2) The three domains used in the experiments are all transactional and relatively structured. It is recommended to add at least one less-structured domain to examine whether echoing is a common AxA phenomenon beyond task-oriented setups."}, "questions": {"value": "Please See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HBPASWINnI", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_f5W6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_f5W6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876554619, "cdate": 1761876554619, "tmdate": 1762942061998, "mdate": 1762942061998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment (part 1)"}, "comment": {"value": "As AxA systems move toward production with actively developing frameworks, such as A2A, IBM (Agent Communication Protocol), and Cisco (AGNTCY.org), the community needs systematic understanding of the failure modes unique to autonomous agent-agent interactions. A study of these particular failures is largely absent in research/industry. As such this work addresses three critical gaps:\n\n1. We **identify** and formalize **echoing**, a behavioral failure where agents abandon their assigned roles and mirror their conversational partners. Unlike coordination failures in multi-agent systems or human facing agentic systems, echoing emerges specifically from autonomous AxA dynamics without human oversight. This failure cannot be detected through single agent evaluations.  \n     \n2. We quantify echoing's **prevalence** with focus in enterprise transactional scenarios (a real-world immediate scenario actively pursued). Across multiple conversations with various configurations, we find echoing occurs in 5-70% of interactions depending on model and domain. Critically, 93% of echoing conversations still complete their tasks successfully, i.e., a booking made, car sold, contract agreed. **Task completion metrics mask this behavioral failure**. \n     \n3. We note existing benchmarks overlook this problem. Recent work has noticed persona inconsistencies but treated them as minor simulator imperfections. For example, CRMArena-Pro observes in their appendix that their LLM user simulator may \"occasionally produce responses that may be inconsistent, subtly deviate from the persona,\" attributing this to \"an inherent limitation\" of LLM-based simulators solvable by \"more advanced foundation models.\" Our work demonstrates these are not incidental implementation details but **fundamental and prevalent failures** that:  \n    \n    - Persist even in advanced reasoning models (32.8% average)  \n    - Occur across diverse model families and parameter scales  \n    - Stem from alignment procedures optimized for human-facing rather than AxA interactions\n\nAs one reviewer recognized: *\"This paper formally defines echoing as a distinct AxA failure, distinguishing it from generic errors or hallucinations... demonstrates that echoing persists even in stronger 'reasoning' models and that standard evaluation metrics often mask this phenomenon\"* (Reviewer f5W6). Another noted: *\"The paper addresses a timely and important problem in emerging agent–agent (AxA) LLM systems... provides a comprehensive empirical study\"* (Reviewer aRQ4).\n\nBeyond identification and quantification, we provide **insights into factors of influence**: model architecture, reasoning capabilities (Section 4.2.1), prompt design (Section 4.2.2), domain characteristics (Section 4.2.3), and conversation dynamics (Section 4.3). We show that **these problems cannot be solved by focusing on single-agent performance**, they emerge from interaction dynamics between agents.\n\nThrough the factors of influence, we also show that the **mitigation approaches** that one might reflexively suggest are insufficient: prompt engineering reduces but does not eliminate echoing (similar to hallucination) (section 4.2.2), structured responses requiring explicit role **assertion** by the LLM reduce echoing from 29% to 9% (section 4.4), while other role **insertion** approaches fail by breaking the conversation flow (Appendix).\n\nComment on mitigation: Our focus in this work was to identify, validate the problem, and explore first-reflex mitigation strategies (prompting, reasoning, protocol-level interventions). We demonstrate that while these attenuate echoing, they cannot eliminate it consistent with the hypothesis that echoing stems from fundamental mismatch in models rather than suboptimal deployment choice. We actually **do not** expect there to be a general mitigation strategy at the prompt/protocol level that completely removes echoing.\n\n## Revision Summary\n\n**New experiments**:\n\n- Cross-model judge ablation (4 models, 100 conversations from each domain)  \n- Llama 3.1 open-weight models (6 additional AxA configurations with 8B and 70B)  \n- Medical consultation domain (non-transactional, advisory AxA setting)\n\n**Clarifications**:\n\n- Human validation methodology expanded (Appendix A)  \n- Task success criteria and relationship to role fidelity (Section 3\\)  \n- Identity Boundary prompts as explicit anti-drift baseline (Section 4.2.2)  \n- Customer vs. seller variation rationale and asymmetric patterns  \n- Figure aggregation scope (Figure 3: all configs, Figure 4: reasoning-only)\n\n**Paper Improvements**:\n\n- Dedicated Related Work section covering MAS, role consistency, multi-turn safety, LLM benchmarks added in Appendix.  \n- Figure 1 text enlarged for readability  \n- Typo corrections\n\n**Artifacts Included**: Code, agent configurations, conversation logs, evaluation scripts in supplementary materials."}}, "id": "N0PIlkuuFI", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754314747, "cdate": 1763754314747, "tmdate": 1763754340073, "mdate": 1763754340073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores a failure mode in multi-agent LLM systems called echoing, where one agent abandons its role and mirrors its conversational partner. Across 2,000+ agent–agent conversations spanning 60 configurations, 3 domains, and multiple model providers, the authors find echoing occurs in 5–70% of interactions and persists even in advanced reasoning models. They show that neither stronger reasoning nor improved prompting fully prevents echoing, though structured, role-reinforcing responses can reduce it to around 9%. The study argues that these identity drifts are unique to agent–agent settings and call for new evaluation and mitigation strategies beyond single-agent benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses a timely and important problem in emerging agent–agent (AxA) LLM systems, highlighting identity drift as a realistic and underexplored failure mode.\n\n- It provides a comprehensive empirical study across reasoning and non-reasoning models, multiple domains, and major LLM providers, revealing that echoing persists even in advanced models.\n\n- It proposes a practical mitigation strategy, namely structured, role-reinforcing responses, that significantly reduces echoing, offering a concrete first step toward improving AxA reliability."}, "weaknesses": {"value": "- The study focuses solely on proprietary, closed-weight models, leaving it unclear whether the findings generalize to open-source or smaller-scale models.\n\n- There is no simple baseline where agents are explicitly instructed not to mirror or drift mid-conversation, which would help isolate whether echoing persists even under explicit anti-drift training.\n\n- The task evaluation setup and success criteria are not clearly defined, making it difficult to assess whether role drift meaningfully affects overall task quality or utility outcomes.\n\n- The proposed mitigation, while promising, is protocol-level only and may not scale or generalize to more complex multi-party or tool-using AxA scenarios. Can the authors elaborate on extension of their framework to such scenarios?\n\n\nOverall, this paper raises an important and timely issue in multi-agent LLM research and offers valuable empirical insights into identity drift in agent–agent interactions. However, the experimental design and evaluation leave key questions unanswered. While the study is thought-provoking and well-motivated, it would benefit from a stronger methodological foundation and clearer evaluation framework before being ready for inclusion at ICLR."}, "questions": {"value": "- In section 3 you mention \"successful evaluation\", but it's unclear to me what metric you use and how you compite. Can you provide some clarifications?\n\n- Have you tried adding something like “Do not impersonate anyone else; stick to your assigned persona” to the LLM prompt? I think that’s an important baseline rule that should have been tested.\n\n- You mention your \"focus on customer–seller interactions, treating the customer agent as the primary variable with fewer seller variations to isolate echoing susceptibility\" but it's unclear to me why that would isolate echoing. I imagine there might be a power/role effect on echoing but I don't see a clear reason on why you wouldn't want to experiment with seller variations.\n\n\n----------------\n\nTypo\n\n- ln 204 seller agents is -> seller agents are"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2NLz9KtHDo", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_aRQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_aRQ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994244410, "cdate": 1761994244410, "tmdate": 1762942061135, "mdate": 1762942061135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies echoing in agent-to-agent (AxA) interactions which is that one LLM abandons its assigned role and starts mirroring its counterpart. The authors run AxA conversations across 60 configurations and three negotiation-style domains such as customer–seller settings like car sales, hotel booking, supply chain. They analyze prompt styles and conversation dynamics, finding echoing tends to emerge later in the dialogue and propose a simple protocol-style mitigation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Positions AxA vs multi-agent systems (MAS) as an interesting contrast where success is not the same as role fidelity.\n2. Echoing is practically relevant for AxA deployments. It might also be relevant to simulation benchmarks using LLMs.\n3. The results showing that even when reasoning modes are enabled, role drift persists suggest a further direction  into analyzing reasoning models.\n4. Methodology and design choices are explained in detail.\n5. It's an interesting idea to simulate conversations of two LLMs under the customer-seller setting."}, "weaknesses": {"value": "1. All three domains are variations of customer–seller negotiation. This limits external validity. Consider collaborative planning, tool-use workflows, safety-critical settings, multilingual, or non-negotiation AxA tasks, and evaluate on existing LLM simulation benchmarks to situate results.\n2. The phenomenon is largely context-overwriting (early system/role prompts diluted by long histories).\n3. System-prompt location differs by provider (e.g., some put it only at the very beginning). If the defense is “include role each turn,” test that explicitly across APIs.\n4. Try including the role in every message—should be a primary baseline.\n5. Numbers across Figure 3 vs Figure 4 appear to differ.\n6. Some statements (noted around lines ~285–290) read stronger than the evidence shown.\n7. The intro hints at related work, but an explicit related work section comparing to MAS role-consistency, persona retention, and multi-turn safety would help."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "baIE0mGG0S", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_U7gL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_U7gL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034632443, "cdate": 1762034632443, "tmdate": 1762942060771, "mdate": 1762942060771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the echoing failure mode in agent-to-agent conversations, where LLM agents drift from their assigned identities and start mirroring their conversational partners. The authors introduce an LLM-based evaluation metric (EchoEvalLM), and conduct an empirical study across 60 configurations, 3 domains, and ~2000 conversations in total. Results show that echoing is prevalent (5–70%), persistent in reasoning models (32.8%). They further show that structured responses reduce echoing to 9%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies the echoing failure mode in agent-to-agent conversations, where LLM agents drift from their assigned identities and start mirroring their conversational partners.\n\n2. Results show that echoing is prevalent (5–70%), persistent in reasoning models (32.8%). They further show that structured response can reduce echoing to 9%.\n\n3. Overall, the paper is written in a clear and easy-to-understand manner, although some details need clarification."}, "weaknesses": {"value": "1. Evaluation: The use of GPT-4o as both subject and judge introduces potential circularity and bias. Having an ablation study on different evaluators can further strengthen the model. \n2. Evaluation: Many critical  details regrading human validation are missing, e.g, the total number of human evaluated data, the review qualification, the agreement rate between humans/annotators, etc.\n3. Only one potential mitigation method is presented:  The simple structured response can reduce echoing rate from 29% to 9%. In the corresponding subsection 4.4, it lacks details on what model is evaluated for mitigation strategy investigation. \n4. Why this echoing failure mode can happen, is there any inherent reason? An in-depth analysis and principled solution would be appreciated.\n5. Dataset: Line 221: \"Results are obtained on at least 10 independent runs per configuration, yielding approximately 2000 conversations for validation and analysis.\" -> The number of total evaluated data points is still quite limited in my opinion.\n6. Code is not available, there is a reproducibility risk until code release."}, "questions": {"value": "See the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QFLYgjwvUS", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_kiYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_kiYu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042006143, "cdate": 1762042006143, "tmdate": 1762942060527, "mdate": 1762942060527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}