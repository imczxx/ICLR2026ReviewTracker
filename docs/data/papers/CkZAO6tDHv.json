{"id": "CkZAO6tDHv", "number": 22088, "cdate": 1758325787877, "mdate": 1759896887224, "content": {"title": "Echoing: Identity Failures when LLM Agents Talk to Each Other", "abstract": "As large language model~(LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human–agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, *echoing*, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $60$ AxA configurations, $3$ domains, and $2000+$ conversations, we demonstrate that echoing occurs across three major LLM providers, with echoing rates from $5\\\\%$ to $70\\\\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\\\\%$) that are not reduced by increased reasoning efforts. We analyze prompt impacts, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ turns in experiments) and is not merely an artifact of sub-optimal prompting. Finally, we introduce a protocol-level mitigation in which targeted use of structured responses reduces echoing to $9\\\\%$.", "tldr": "", "keywords": ["Multi-Agent Systems", "Large Language Models", "Agent Alignment", "AI Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8badf198dc8c4f92a40421f55400adb3a0f5cb33.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates a failure mode in agent-vs-agent (AxA) dialogues, termed echoing, in which an agent abandons its assigned identity and adopts that of its counterpart. The authors formalize AxA interactions and conduct experiments across different LLM backbones, prompts, and domains (e.g., hotel booking, car sales, and supply chain). Experimental results show that echoing is prevalent (occurring in approximately 5%–70% of cases), even in “reasoning” models (averaging about 32.8%), and often emerges in the later stages of dialogue (typically after seven turns), frequently masked by standard completion metrics. Furthermore, the authors propose a protocol-level scheme that mitigates echoing through structured responses, which reduces but does not completely eliminate it."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) This paper formally defines echoing as a distinct AxA failure, distinguishing it from generic errors or hallucinations as well as from prior MAS settings, and clearly elucidates the AxA framework.\n\n(2) This paper presents extensive experiments across multiple LLMs and domains, providing a solid empirical foundation for echoing research. Furthermore, the study analyzes failed cases and extracts relevant principles (prevention, non-intrusion, seamlessness, and architectural integration).\n\n(3) This study demonstrates that echoing persists even in stronger “reasoning” models and that standard evaluation metrics often mask this phenomenon. These findings prompt a rethinking of evaluation methods for AxA systems."}, "weaknesses": {"value": "(1) The text in the third section of Figure 1 is too small to be easily read.\n\n(2) The three domains used in the experiments are all transactional and relatively structured. It is recommended to add at least one less-structured domain to examine whether echoing is a common AxA phenomenon beyond task-oriented setups."}, "questions": {"value": "Please See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HBPASWINnI", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_f5W6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_f5W6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876554619, "cdate": 1761876554619, "tmdate": 1762942061998, "mdate": 1762942061998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores a failure mode in multi-agent LLM systems called echoing, where one agent abandons its role and mirrors its conversational partner. Across 2,000+ agent–agent conversations spanning 60 configurations, 3 domains, and multiple model providers, the authors find echoing occurs in 5–70% of interactions and persists even in advanced reasoning models. They show that neither stronger reasoning nor improved prompting fully prevents echoing, though structured, role-reinforcing responses can reduce it to around 9%. The study argues that these identity drifts are unique to agent–agent settings and call for new evaluation and mitigation strategies beyond single-agent benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses a timely and important problem in emerging agent–agent (AxA) LLM systems, highlighting identity drift as a realistic and underexplored failure mode.\n\n- It provides a comprehensive empirical study across reasoning and non-reasoning models, multiple domains, and major LLM providers, revealing that echoing persists even in advanced models.\n\n- It proposes a practical mitigation strategy, namely structured, role-reinforcing responses, that significantly reduces echoing, offering a concrete first step toward improving AxA reliability."}, "weaknesses": {"value": "- The study focuses solely on proprietary, closed-weight models, leaving it unclear whether the findings generalize to open-source or smaller-scale models.\n\n- There is no simple baseline where agents are explicitly instructed not to mirror or drift mid-conversation, which would help isolate whether echoing persists even under explicit anti-drift training.\n\n- The task evaluation setup and success criteria are not clearly defined, making it difficult to assess whether role drift meaningfully affects overall task quality or utility outcomes.\n\n- The proposed mitigation, while promising, is protocol-level only and may not scale or generalize to more complex multi-party or tool-using AxA scenarios. Can the authors elaborate on extension of their framework to such scenarios?\n\n\nOverall, this paper raises an important and timely issue in multi-agent LLM research and offers valuable empirical insights into identity drift in agent–agent interactions. However, the experimental design and evaluation leave key questions unanswered. While the study is thought-provoking and well-motivated, it would benefit from a stronger methodological foundation and clearer evaluation framework before being ready for inclusion at ICLR."}, "questions": {"value": "- In section 3 you mention \"successful evaluation\", but it's unclear to me what metric you use and how you compite. Can you provide some clarifications?\n\n- Have you tried adding something like “Do not impersonate anyone else; stick to your assigned persona” to the LLM prompt? I think that’s an important baseline rule that should have been tested.\n\n- You mention your \"focus on customer–seller interactions, treating the customer agent as the primary variable with fewer seller variations to isolate echoing susceptibility\" but it's unclear to me why that would isolate echoing. I imagine there might be a power/role effect on echoing but I don't see a clear reason on why you wouldn't want to experiment with seller variations.\n\n\n----------------\n\nTypo\n\n- ln 204 seller agents is -> seller agents are"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2NLz9KtHDo", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_aRQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_aRQ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994244410, "cdate": 1761994244410, "tmdate": 1762942061135, "mdate": 1762942061135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies echoing in agent-to-agent (AxA) interactions which is that one LLM abandons its assigned role and starts mirroring its counterpart. The authors run AxA conversations across 60 configurations and three negotiation-style domains such as customer–seller settings like car sales, hotel booking, supply chain. They analyze prompt styles and conversation dynamics, finding echoing tends to emerge later in the dialogue and propose a simple protocol-style mitigation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Positions AxA vs multi-agent systems (MAS) as an interesting contrast where success is not the same as role fidelity.\n2. Echoing is practically relevant for AxA deployments. It might also be relevant to simulation benchmarks using LLMs.\n3. The results showing that even when reasoning modes are enabled, role drift persists suggest a further direction  into analyzing reasoning models.\n4. Methodology and design choices are explained in detail.\n5. It's an interesting idea to simulate conversations of two LLMs under the customer-seller setting."}, "weaknesses": {"value": "1. All three domains are variations of customer–seller negotiation. This limits external validity. Consider collaborative planning, tool-use workflows, safety-critical settings, multilingual, or non-negotiation AxA tasks, and evaluate on existing LLM simulation benchmarks to situate results.\n2. The phenomenon is largely context-overwriting (early system/role prompts diluted by long histories).\n3. System-prompt location differs by provider (e.g., some put it only at the very beginning). If the defense is “include role each turn,” test that explicitly across APIs.\n4. Try including the role in every message—should be a primary baseline.\n5. Numbers across Figure 3 vs Figure 4 appear to differ.\n6. Some statements (noted around lines ~285–290) read stronger than the evidence shown.\n7. The intro hints at related work, but an explicit related work section comparing to MAS role-consistency, persona retention, and multi-turn safety would help."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "baIE0mGG0S", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_U7gL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_U7gL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034632443, "cdate": 1762034632443, "tmdate": 1762942060771, "mdate": 1762942060771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the echoing failure mode in agent-to-agent conversations, where LLM agents drift from their assigned identities and start mirroring their conversational partners. The authors introduce an LLM-based evaluation metric (EchoEvalLM), and conduct an empirical study across 60 configurations, 3 domains, and ~2000 conversations in total. Results show that echoing is prevalent (5–70%), persistent in reasoning models (32.8%). They further show that structured responses reduce echoing to 9%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies the echoing failure mode in agent-to-agent conversations, where LLM agents drift from their assigned identities and start mirroring their conversational partners.\n\n2. Results show that echoing is prevalent (5–70%), persistent in reasoning models (32.8%). They further show that structured response can reduce echoing to 9%.\n\n3. Overall, the paper is written in a clear and easy-to-understand manner, although some details need clarification."}, "weaknesses": {"value": "1. Evaluation: The use of GPT-4o as both subject and judge introduces potential circularity and bias. Having an ablation study on different evaluators can further strengthen the model. \n2. Evaluation: Many critical  details regrading human validation are missing, e.g, the total number of human evaluated data, the review qualification, the agreement rate between humans/annotators, etc.\n3. Only one potential mitigation method is presented:  The simple structured response can reduce echoing rate from 29% to 9%. In the corresponding subsection 4.4, it lacks details on what model is evaluated for mitigation strategy investigation. \n4. Why this echoing failure mode can happen, is there any inherent reason? An in-depth analysis and principled solution would be appreciated.\n5. Dataset: Line 221: \"Results are obtained on at least 10 independent runs per configuration, yielding approximately 2000 conversations for validation and analysis.\" -> The number of total evaluated data points is still quite limited in my opinion.\n6. Code is not available, there is a reproducibility risk until code release."}, "questions": {"value": "See the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QFLYgjwvUS", "forum": "CkZAO6tDHv", "replyto": "CkZAO6tDHv", "signatures": ["ICLR.cc/2026/Conference/Submission22088/Reviewer_kiYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22088/Reviewer_kiYu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042006143, "cdate": 1762042006143, "tmdate": 1762942060527, "mdate": 1762942060527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}