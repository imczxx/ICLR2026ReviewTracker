{"id": "QUVcCwWbFb", "number": 22781, "cdate": 1758335363518, "mdate": 1759896846506, "content": {"title": "On the Theory of Neural Network Surjectivity: Can You Elicit Any Behavior from Your Model?", "abstract": "Given a trained neural network, can any specified output be generated by some input? Equivalently, does the network correspond to a function that is surjective? In generative modeling, surjectivity implies that any output, including harmful or undesirable ones, can in principle be generated by the networks, raising concerns about model safety and jailbreaks. In this paper, we prove that many fundamental building blocks of modern neural architectures, such as networks with pre-layer normalization and linear-attention modules, are almost always surjective. As corollaries, widely used generative frameworks, including GPT-style transformers and flow-matching models, admit inverse mappings for arbitrary outputs. By studying surjectivity of these modern and commonly used neural architectures, we contribute a formalism that sheds light on their vulnerability to a broad class of adversarial attacks.", "tldr": "", "keywords": ["Theory", "Safety", "Neural Network", "Generative Model"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ae0f3995c7fda5317845c762a5c5adf27dfc414.pdf", "supplementary_material": "/attachment/02d52cf5b8d1c38b14384af4d0b711139fb7d0fe.pdf"}, "replies": [{"content": {"summary": {"value": "The paper views generative models as functions and asks whether they are surjective - that is, whether every possible output can be realized by some input, with major implications on AI safety: it means that there exists some prompt for any possible model output, including harmful or jailbreaking ones.\nUsing tools from differential topology (homotopy, Brouwer degree), the authors prove that several modern architectures such as LeakyReLU MLPs, linear attention layers, and Pre-LayerNorm residual blocks—are almost always surjective.\nThey argue that this implies GPT-style Transformer blocks are surjective (in embedding space), raising concerns about the fundamental vulnerability surface of modern LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Framing surjectivity as a lens on model safety is novel and surprisingly elegant.\nThe use of degree theory and homotopy to prove surjectivity of nonlinear neural blocks is rigorous and instructive.\nThe connection between expressivity, topology, and AI safety is thought-provoking and could open new directions in theoretical AI safety research."}, "weaknesses": {"value": "The discussion that the surjectivity theorems are proved only “in the embedding space under the assumption of direct control over the input embeddings” (p. 7) should be made much more prominent. To my understanding, this is a major conceptual limitation that belongs early in the Introduction, not as a short remark. The paper should clearly articulate the difference between embedding-level surjectivity and token-level generation in real autoregressive models. It would also be very helpful if the authors could comment on whether this assumption is merely an artifact of the mathematical analysis (which is my guess), or whether it (hopefully) represents a more fundamental limitation.\n\nThe triangle inequality seems to be used in the wrong direction in the derivation of R in Theorem 3.1. I believe a correct bound can be obtained by the other side of triangle inequality and by using the Lipschitz property of LeakyReLU and the minimal singular values of W1 and W2. Please verify this?\n\nThe link between the non-surjectivity of softmax attention and the surjectivity of GPT-style transformers could be explained more clearly. As I understand it, the Pre-LayerNorm residual connection makes any continuous sublayer surjective, which would reconcile why softmax attention alone is non-surjective but GPT (including softmax) blocks are. Could the authors make this reasoning explicit in the paper? I believe this is a CRUCIAL point.\n\n“Smoothness” motivation is misleading: the claim that “since almost all networks are trained via backpropagation, they are usually smooth” (p. 4 line 190) is confusing, as in ML/optimization theory, “smooth” typically means Lipschitz gradient (C1 with bounded derivative). For the ICLR crowd, the standard notion of smooth function might not be the first thing coming to mind. ReLU networks are only piecewise-linear (C0, not C1). LeakyReLU is C1 but not C2. This is about presentation and not soundness, but I think is an important point.\n\nThe introduction of homotopy (Def. 6) feels abrupt. It should be motivated as the key technical tool allowing analyzing simple functions (linear) as a basis for more complicated ones (nonlinearities).\nI suggest to also add after the definition a concrete example (for instance, the alpha-path from LeakyReLU to identity) to make the notion more tangible."}, "questions": {"value": "You show that surjectivity holds in continuous embedding space, and even demonstrate it empirically by optimizing GPT-2 embeddings to reproduce arbitrary sentences.\nHowever, could this theory be extended to embeddings induced by discrete token lookups?\nFor example:\nAre there conditions on the embedding matrix or token distribution that preserve (approximate) surjectivity? Clarifying this would make the safety implications much stronger.\n\nI will be happy to consider raising my score if these concerns are addressed, particularly if the embedding-space limitation is presented clearly in the Introduction and its possible relaxation is discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The GPT-2 demo involves optimizing embeddings to reconstruct a publicly available sentence for illustration purposes. It does not constitute a harmful or privacy-related experiment."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HfbLLZiFtU", "forum": "QUVcCwWbFb", "replyto": "QUVcCwWbFb", "signatures": ["ICLR.cc/2026/Conference/Submission22781/Reviewer_8qdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22781/Reviewer_8qdq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760499620138, "cdate": 1760499620138, "tmdate": 1762942385211, "mdate": 1762942385211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether different deep learning architectures are surjective by using tools from differentiable topology to prove this. The authors provide detailed theoretical results as well as an in depth discussion of the implications of these surjectivity results for AI safety. The authors argue that surjectivity poses a fundamental challenge to creating jailbreak-proof models since there will almost always exist an input that can lead a transformer model to produce any output."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written with clear explanations and intuitive introductions for all the relevant concepts.\n- Theoretical results are clear and leverage interesting connections to differential topology.\n- The proofs seem sound although some of them are outside of my area of expertise.\n- The discussion is detailed and insightful"}, "weaknesses": {"value": "My key concern with this work is the following:\n\nThe authors claim that the surjectivity of certain DL models like transformers imply that they will always be susceptible to jailbreaks, however, as ankowledged by the authors, they do not account for discrete tokens or the autoregresive aspect of transformers. The latter seems particularly relevant since without autoregression, I struggle to think of outputs that are intrinsicly harmful and am therefore not convinced that surjectivity is a problem in itself. \n\nIf we are not looking at a non-autoregressive model like a classifier, then presumably we want the model to be able to predict any of the classes for some given inputs and the matter is more wether the some of the inputs cause the model to predict a class that does not match the human-assigned label (as in adversarial examples). In this case surjectivity is not the problem but the fact that the pre-image of a class contains samples that a human would assign to a different class. If we do not want the model to be able to predict a given class for any possible input, that class can be removed at an architectural level.\n\nSimilarly, for the drone case described in this work, any action in the action space of the AI controlled drone should be appropriate in some contexts. The problem would come from adversaries being able to induce this action in the wrong context, not if the adversaries create the right context for this action to be taken. Again I would say that surjectivity is not a problem in itself.\n\nIn the case of an autoregressive transformer based language model, for any token in the vocabulary there should be a sequence of inputs for which that token is the correct continuation. Again the problem comes from the kind of input that elicited the response and it is hard for me to imagine a single token response that could be intrinsically harmful. It is only when generating several tokens in a row or in relation to a specific prompt that I can think of intrinsically harmful or undesirable responses. For example if a language model responds \"yes\" to a user asking if they should do something illegal, that seems harmful. However, the fact that there exist prompts like \"Say yes\" for which the model will answer \"yes\" does not seem intrinsically problematic. Instructions on how to do something illegal seem like they would always be undesirable, no matter what they are in response to, however, as acknowledged by the authors, these multi-token generations where the adversary doesn't know the output they want to generate is not covered in this work."}, "questions": {"value": "Am I missing something in the main weakness outlined above? Is surjectivity intrinsicly unsafe or are outputs only harmful in relation to a context?\n\nWhat is prompt \"a\" found by your algorithm in Appendix D to elicit GPT2 to write a part of a New York Times article? Does the prompt include the target \"b\"?\n\nWhy do you claim that transformer blocks are surjective if you show that the attention mechanism is not surjective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hJEzMjBwo5", "forum": "QUVcCwWbFb", "replyto": "QUVcCwWbFb", "signatures": ["ICLR.cc/2026/Conference/Submission22781/Reviewer_piGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22781/Reviewer_piGz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578341341, "cdate": 1761578341341, "tmdate": 1762942384842, "mdate": 1762942384842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the surjectivity property of several models, including MLP with several activations, attention layers, and the effect of LayerNorm. The proof methods use tools from differential and algebraic topology, and show that several of the architectures are indeed surjective. An experiment is given in the appendix supporting the claim."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The use of differential topology and degree theory is an interesting approach that I have not seen before in the literature of ML.\n\n- The connection between surjectivity and safety is interesting, and could be further developed and also related to optimization rather than only expressiveness."}, "weaknesses": {"value": "**W1 - Non-rigorous and hand-wavy proofs**\n\nThe paper contains many places with either non-rigorous proofs or hand-wavy arguments. This is a major problem, especially in a theoretical work. To give some examples:\n\n- There is no formal proof for why MLP with ReLU is not surjective, and the informal intuition at the end of section 3.2  is very cryptic and hand-wavy. Table 1 shows the MLP with ReLU case as a result, which is clearly not the case here.\n\n- In the proof of Theorem D.2, instead of providing a rigorous argument, it is said that the model “Cannot reach a large chunk of the output space “. This argument is very informal and not appropriate for a rigorous mathematical proof. To prove non-surjectivity formally, it is needed to show a specific output value that cannot be reached.\n\n- Layer norm is defined incorrectly in this paper. The denominator should be the variance (plus a small $\\epsilon$ term), so there is a $\\frac{1}{\\sqrt{d}}$ term missing. This also changes the proof of Theorem 3.1, and shows that the bound depends on the input dimension.\n\nThese are just some examples I found, I suggest going over the proofs and making sure every argument is justified.\n\n**W2 - The motivation for studying surjectivity is lacking**\n\n- The authors try to connect surjectivity to safety, which I think is interesting, but not properly aligned in the setting of this paper. For example, a linear predictor is also almost always surjective, so is it also not safe? The reason for surjectivity is the condition that $d_1 > d$, and so the linear part of the model is already surjective, and it is only left to show that the non-linearity doesn’t “ruin” this property, as in the case of ReLU. Since linear predictors are clearly not the focus of today’s safety issues, I believe the scope of the surjectivity property studies in this paper is very limited. \n\n- Another point is that the condition of $d_1 > d$ is unrealistic in most of today’s models. For example, the input dimension of images is in the millions, while it is unrealistic to have a hidden dimension of the same size.\n\n- The authors claim that the models are “almost always” surjective, excluding a zero measure set of low-rank matrices. However, recent results about the implicit bias of such models suggest that there is a tendency to converge to low-rank matrices, see e.g. Implicit Regularization in Deep Matrix Factorization, 2019, Arora et al., and follow-up works. This suggests that the zero-measure set is what models actually converge to; this should be at least mentioned in the paper.\n\n**W3 - The experiment is lacking**\n\nThere is an experiment in the appendix that supports the surjectivity claim. However, it is very lacking and currently only hurts the surjectivity claim rather than supports it. First, the results are not clearly stated; it is said, “We run the algorithm many times for different sentences and it almost never fails. “ (line 1382). I don’t believe this is a good way to present the results in a scientific paper. Second, only a single sentence is presented, which is in standard natural language. An experiment should be done on several inputs, in a proper way, describing the results. Also, a more interesting result would be if the model could output non-realistic sentences or even gibberish, which should be guaranteed to be possible due to the surjectivity property.\n\n\nA minor comment, I personally prefer putting the related works section in the main text and not in the appendix, as I view it as an integral part of the paper. It is better to move the proofs to the appendix if there are space issues."}, "questions": {"value": "- Does surjectivity provide a practical mean to attack a model (and thus pose a safety issue), rather than a hypothetical possibility for an attack?\n\n- In the more realistic setting where $d_1 << d$, is the model always not surjective? In this case, can you describe the space of outputs?\n\n- Empirically, does the surjectivity property hold also for sentences not in natural languages? E.g., for random outputs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "65h6AaQakT", "forum": "QUVcCwWbFb", "replyto": "QUVcCwWbFb", "signatures": ["ICLR.cc/2026/Conference/Submission22781/Reviewer_NQcw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22781/Reviewer_NQcw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725371683, "cdate": 1761725371683, "tmdate": 1762942383897, "mdate": 1762942383897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines the surjectivity of commonly used neural network layers, and networks composed of them - with potential implications for safety esp. in relation to jailbreaking and adversarial attacks. They do so by invoking various results from algebraic topology, e.g. Brouwer's fixed point theorem (Theorem 3.1) or using homotopy and Brouwer degree (Theorem 3.3). They classify various layer types as surjective or otherwise, and connect these results to commonly used architectures in ML. They then discuss the implications of their results for various domains and tasks.\n\nI recommend the acceptance of the paper because it tackles an important, complementary aspect of safety discussion, and demonstrate the usefulness of a not commonly utilized mathematical toolkit in such analyses, potentially helping the field to advance. I do not recommend a strong acceptance because the paper itself does not make a strong connection between their results and practical safety concerns, nor does it suggest a convincing pathway to doing so."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Examining the expressivity of the model architectures in question is an important and complementary aspect of ML safety. Although it certainly cannot be the only approach to safety, the authors' introduction sounds promising in terms of future research making more concrete connections between the surjectivity of function classes in question and achievability of certain adversarial outcomes.\n- The analyses cover a good variety of modern neural network layers.\n- The paper is written well and the main ideas are communicated clearly.\n- The paper is clear about the limitations of its approach to safety, and extensively discusses how it relates to alternatives at a high level."}, "weaknesses": {"value": "- Theoretical achievability of an outcome says little about its practical relevance. A dramatic example of this is adversarial attacks being by construction $\\epsilon$-bounded around natural images. Although it is good that the authors are transparent about this fact, the fundamental problem still remains. The paper does not outline a convincing roadmap towards closing this gap.\n- The paper's theoretical exposition also have some gaps in terms of the architectures they cover. For example, the MLP in Theorem 3.3 requires input and output dimensions to be equal, which is not unreasonable per se, but also leaves out important use cases for such architectures. It also ignores potential outcomes of optimization: e.g. explicit or implicit regularization not uncommonly leads to low-rank layer matrices."}, "questions": {"value": "Here I add some more minor questions and comments:\n- Page-long citations exist in the references, please fix.\n- L034: Citation paragraph typo\n- L125: That the current analyses address MLPs with $\\mathbb{R}^d \\to \\mathbb{R}^d$ should be explicitly acknowledged and discussed, either here or later in the paper.\n- L127: $b_2$ -> $\\lambda_2$\n- L196: Reference?\n- L216: Would it make more sense to present Def. 7 and Lemma 1 closer to Theorem 3.3? I leave this up to the authors\n- L216: Double parantheses. Also applies to Lemma 1.\n- L283: What is this a warm-up to?\n- L441: I had a hard time following the arguments in this paragraph; it would benefit from being rewritten more clearly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1LUSwUCJRR", "forum": "QUVcCwWbFb", "replyto": "QUVcCwWbFb", "signatures": ["ICLR.cc/2026/Conference/Submission22781/Reviewer_cHvE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22781/Reviewer_cHvE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013401891, "cdate": 1762013401891, "tmdate": 1762942383598, "mdate": 1762942383598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}