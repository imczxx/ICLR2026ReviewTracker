{"id": "qxsSiIIGfj", "number": 13517, "cdate": 1758218842699, "mdate": 1759897431547, "content": {"title": "MatSciBench: Benchmarking the Reasoning Ability of LLM in Materials Science", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in scientific reasoning, yet their reasoning capabilities in materials science remain underexplored.\nTo fill this gap, we introduce MatSciBench, a comprehensive college-level benchmark comprising 1,340 problems that span the essential subdisciplines of materials science.\nMatSciBench features a structured and fine-grained taxonomy that categorizes materials science questions into 6 primary fields and 31 sub-fields, and includes a three-tier difficulty classification based on the reasoning length required to solve each question.\nMatSciBench provides detailed reference solutions enabling precise error analysis and incorporates multimodal reasoning through visual contexts in numerous questions.\nEvaluations of leading models reveal that even the highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on college-level materials science questions, highlighting the complexity of MatSciBench. \nOur systematic analysis of different reasoning strategies—basic chain-of-thought, tool augmentation, and self-correction—demonstrates that no single method consistently excels across all scenarios. \nWe further analyze performance by difficulty level, examine trade-offs between efficiency and accuracy, highlight the challenges inherent in multimodal reasoning tasks, analyze failure modes across LLMs and reasoning methods, and evaluate the influence of retrieval-augmented generation.\nMatSciBench thus establishes a comprehensive and solid benchmark for assessing and driving improvements in the scientific reasoning capabilities of LLMs within the materials science domain.", "tldr": "", "keywords": ["LLM", "Reasoning", "Material Science", "AI for Science"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/caf111e9db4409ece03e61c265795357d0c46be7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new benchmark, MatSciBench, which is specifically designed for the subject of material science. The benchmark consists of 1340 questions, extracted from undergraduate and graduate textbook.\nThe questions are further curated by human experts for better quality. Some questions involve images, which means requiring multimodal reasoning ability.\nThe paper benchmark the performance of latest LLMs with different reasoning paradigms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- A carefully curated benchmark is a good contribution to the community, especially for the under-explored domain of material science.\n\n- The benchmark contains multimodal questions.\n\n- Good data size.\n\n- Good analysis on the stats of the dataset."}, "weaknesses": {"value": "- More information needed for: \n  - Line 175, the parsing algorithm of identifying the problems and solutions.\n  - the details on background of Human experts who reviewed the extracted questions. Is there any cross validation?"}, "questions": {"value": "Can you have a paragraph elaborating how the questions in material science different from other domains, such as math, physics and chemistry."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HWSoNq74hx", "forum": "qxsSiIIGfj", "replyto": "qxsSiIIGfj", "signatures": ["ICLR.cc/2026/Conference/Submission13517/Reviewer_vfWw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13517/Reviewer_vfWw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741450885, "cdate": 1761741450885, "tmdate": 1762924125866, "mdate": 1762924125866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **MatSciBench**, a new benchmark designed to evaluate the reasoning capabilities of Large Language Models (LLMs) in the domain of materials science. The benchmark consists of 1,340 college-level problems sourced from 10 textbooks, covering 6 primary fields and 31 sub-fields of materials science. Key features of MatSciBench include a three-tier difficulty classification based on reasoning length, detailed reference solutions for error analysis, and the inclusion of 315 multimodal questions with visual contexts. The authors conduct an extensive evaluation of several \"thinking\" and \"non-thinking\" models, analyzing the efficacy of different reasoning strategies like Chain-of-Thought (CoT), tool-augmentation, and self-correction. The primary finding is that even the best-performing model, \"Gemini-2.5-Pro,\" scores below 80%, indicating the benchmark's difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*   **Significance:** The work addresses a clear and important gap. While numerous STEM benchmarks exist, materials science—a critical, interdisciplinary field at the intersection of physics, chemistry, and engineering—has been underexplored. Creating a dedicated, high-quality benchmark for this domain is a significant step towards understanding and improving LLM capabilities for real-world scientific applications.\n\n*   **Quality:** \n    *   **Data Sourcing:** The problems are curated from established college-level textbooks, not synthetically generated by LLMs. This ensures the questions are authentic, relevant, and representative of the knowledge required in the field.\n    *   **Rigorous Curation:** The inclusion of expert review, detailed reference solutions for over 900 questions, and a structured, hierarchical taxonomy demonstrates a high level of rigor. \n\n*   **Originality:** While the concept of a benchmark is not new, MatSciBench is original in its specific focus, depth, and quality for the materials science domain."}, "weaknesses": {"value": "*   **Limited Scope of RAG Case Study:** The RAG analysis is presented as a \"case study\" but is too limited to support generalizable conclusions. It primarily uses one model (DeepSeek-V3) and one RAG implementation (general web search). The finding that RAG improves comprehension but not knowledge accuracy is fascinating and counter-intuitive, but it could be an artifact of the specific model or the noisy nature of web search for specialized topics. A more robust analysis would involve multiple models and, crucially, a domain-specific retrieval corpus (e.g., the source textbooks themselves).\n\n*   **Difficulty Classification Metric:** Using response length as a proxy for difficulty is a pragmatic and clever approach, and the authors do well to validate it by showing a negative correlation with accuracy. However, this metric could be noisy. For some models, increased length might simply reflect higher verbosity or a tendency to \"show work\" on all problems, not necessarily an increase in reasoning complexity. A small-scale validation against human expert ratings of difficulty would have made this classification unassailable."}, "questions": {"value": "1.  **Regarding the RAG Analysis:**\n    *   The finding that RAG can increase hallucination is important. Do you believe this is a general risk, or is it specific to using a broad web search for highly technical queries where precision is low?\n    *   Have you considered conducting a more controlled RAG experiment where the retrieval corpus is composed of the digitized textbooks from which the questions were sourced? This would test the model's ability to \"use a reference\" more directly and might yield different results regarding knowledge-based errors.\n\n2.  **Regarding Multimodal Reasoning:**\n    *   The analysis identifies spatial reasoning and precise data extraction as key challenges. Could you include one or two specific examples in the appendix showing a model failing to interpret a crystal lattice diagram or misreading values from a phase diagram? This would powerfully illustrate the nature of the current limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ybdf1ogOF9", "forum": "qxsSiIIGfj", "replyto": "qxsSiIIGfj", "signatures": ["ICLR.cc/2026/Conference/Submission13517/Reviewer_nUjU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13517/Reviewer_nUjU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899468465, "cdate": 1761899468465, "tmdate": 1762924124990, "mdate": 1762924124990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MatSciBench, a benchmark of 1,340 expert-curated, college-level materials science problems aimed at evaluating large language models' (LLMs) reasoning abilities in an underexplored yet crucial scientific domain. MatSciBench spans 6 primary fields and 31 sub-fields, classifies difficulty on a three-tier scale, and incorporates detailed reference solutions and over 300 multimodal (image-based) questions. The paper systematically evaluates a suite of SOTA LLMs—distinguishing between “thinking” and “non-thinking” models—across prompting strategies (chain-of-thought, tool-augmentation, self-correction), analyzes performance along multiple dimensions (difficulty, efficiency, multimodal, error type), and provides nuanced insights into LLMs’ scientific reasoning in materials science."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-  The benchmark offers one of the most comprehensive and fine-grained materials science evaluations, comprising 1,340 real questions derived from a wide spectrum of authoritative university-level textbooks.\n\n- The inclusion of 315 image-based questions establishes a new standard for assessing multimodal reasoning in materials science, backed by dedicated quantitative analysis.\n\n-  The experimental design is notably comprehensive, including extensive evaluation of SOTA LLMs across multiple prompting strategies and a detailed analysis of error types, which provides meaningful diagnostic insights."}, "weaknesses": {"value": "- The motivation for this study requires further elaboration. Given the existence of numerous established scientific reasoning benchmarks (e.g., MMMU-PRO for multimodal reasoning, GPQA/SuperGPQA for graduate-level reasoning, SciEval, SciBench for scientific evaluation), the authors must clearly contrast and articulate the unique advantages and research necessity of MatSciBench against these established contemporaries.\n- The evaluation framework is not fully documented. Specifically, the proportional distribution of questions across different data subcategories/subfields needs clearer articulation. Furthermore, the description of the evaluation metrics used is vague; a precise clarification of the scoring mechanism (e.g., per-question calculation and overall aggregation) is required for reproducibility.\n- While the study provides diverse question categorizations, it lacks deep insights into performance disparities across different model types (i.e., how model families handle specific sub-fields). Moreover, the multimodal analysis is insufficient, notably lacking evaluations of powerful open-source Vision-Language (VL) models such as Qwen3-VL and InternVL-3."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Az5ybfe4Qv", "forum": "qxsSiIIGfj", "replyto": "qxsSiIIGfj", "signatures": ["ICLR.cc/2026/Conference/Submission13517/Reviewer_b6nC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13517/Reviewer_b6nC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995061148, "cdate": 1761995061148, "tmdate": 1762924124431, "mdate": 1762924124431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a well-designed benchmark to fill an important gap in evaluating LLMs in materials science. It offers strong dataset organization, rigorous evaluation, and demonstrates that current LLMs still lack deep scientific reasoning capacity in this domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces first comprehensive benchmark focused specifically on materials science reasoning for LLMs.\n\n2. This benchmark covers 6 fields and 31 subfields, with a well-organized taxonomy and difficulty levels.\n\n3. This benchmark also contains visual reasoning tasks using diagrams, figures, and context images."}, "weaknesses": {"value": "1. This benchmark lacks comparison with human performance, making it unclear how difficult the benchmark truly is or whether it reaches \"college-level\" difficulty.\n\n2. The paper claims that RAG fails to reduce knowledge errors and may induce hallucinations. However, prior work[1] shows that when using domain-specific retrieval and query refinement, RAG can significantly improve factual accuracy. I suggest the authors soften this claim and clarify that the negative result stems from their specific RAG setup (general web search, no scientific databases or retrieval filtering), rather than RAG being ineffective in scientific domains overall. \n\n3. This paper indicates a substantial drop in performance on image-based questions.  The decline could also stem from dataset design (e.g., 3D structural images, low-resolution diagrams), mismatched image-question alignment, or higher intrinsic difficulty of visual questions. The authors should conduct more detailed analysis on this section. \n\n[1] Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WeVru9gioS", "forum": "qxsSiIIGfj", "replyto": "qxsSiIIGfj", "signatures": ["ICLR.cc/2026/Conference/Submission13517/Reviewer_WJ8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13517/Reviewer_WJ8b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061516171, "cdate": 1762061516171, "tmdate": 1762924124046, "mdate": 1762924124046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}