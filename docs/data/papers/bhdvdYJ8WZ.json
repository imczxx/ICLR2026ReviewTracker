{"id": "bhdvdYJ8WZ", "number": 12161, "cdate": 1758206059924, "mdate": 1763688398989, "content": {"title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation", "abstract": "The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to constrain the divergence between the generator and the fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep denoising importance from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Together with a scaled VFM-based discriminator, our final model, dubbed **SenseFlow**, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX.1 dev. The source code will be released.", "tldr": "We scale distribution match", "keywords": ["text-to-image generation", "diffusion distillation", "distribution matching distillation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7280cecaeba066e601864d70a6514af77641a510.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper improves Distribution Matching Distillation (DMD) for large text to image models like SD 3.5 and FLUX by introducing Implicit Distribution Alignment (IDA) to handle divergence issues, Intra Segment Guidance (ISG) to improve timestep wise denoising and VFM-based discriminator for effective training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(i) This paper makes contributions by scaling DMD to large flow-based models, such as SD3.5 and FLUX\n\n(ii) The authors propose Implicit Distribution Alignment (IDA) is a novel method that helps stabilize training\n\n(iii) The approach also shows strong empirical performance across multiple challenging benchmarks."}, "weaknesses": {"value": "(i) The theoretical justification provided in Proposition 3.1 lacks clarity. Specifically, the definition of the conditional drift term $\\hat{v}_{\\theta}(X_t, t) = X_1 - X_0$ in line 674 is problematic. Since this quantity is conditioned on $X_1$, it should be treated as a random variable dependent on $X_1$. However, the authors appear to use it deterministically in the derivation of error bounds, which raises concerns about the correctness of the results.\n\n(ii) The error term $\\Delta_k = \\mathbb{E}||v_{\\phi_k} - v_{\\theta_k}||$ is not well justified. According to the authors’ own note in line 667 about definition of $v_{\\theta_k}$: “where the velocity field is the output of the generator network at each timestep t”, $v_{\\theta}$ is only defined at anchor timesteps. In contrast, $v_{\\phi_k}$ is defined at all timesteps. Since the expectation is taken over $t$, this mismatch makes the definition of $\\Delta_k$ unclear.\n\n(iii) My main theoretical concern is with Proposition A.7. As far as I understand, the proof does not include any updates related to IDA, since there are no updates over $k$ shown in the argument. Despite this, the authors make conclusions about the behavior of the procedure. It is possible that I may be missing some detail, but this seems to be mainly due to unclear definitions discussed earlier.\n\n(iv) The proposed Intra-Segment Guidance (ISG) loss (Equation 12) appears closely related to the loss function used in Consistency Trajectory Models (CTM) [1]. If we set $s = \\tau_{i-1}, u = t_{\\text{mid}}, \\tau_i = t$, ISG becomes a specific case of the CTM loss. If this connection is correct, the novelty of ISG should be further clarified and better distinguished from prior work.\n\n(v) Equation 4 seems to contain a conceptual error. At $t = 0$, the forward process should converge to a delta distribution centered at $X_0$, yet the formula implies convergence to a standard Gaussian. This contradicts the standard behavior of the forward process. Moreover, in the appendix, the authors denote $X_1$ as the data variable, which adds further confusion regarding the consistency of notation.\n\n[1] https://arxiv.org/abs/2310.02279 Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion"}, "questions": {"value": "(i) In Figure 3, I noticed that TTUR(10) appears to result in more unstable behavior compared to TTUR(5). Intuitively, a larger update ratio should allow for better optimization of the inner problem, potentially leading to more stable training. Maybe authors have some intuitions about that and can provide  an intuitive explanation for this behavior?\n\n(ii) There seems to be a typo in line 726: \"Eq. equation\". Please correct it.\n\n(iii) In the proof of Lemma A.3, could the authors explain why the inequality $\\mathbb{E}||v_{\\phi_{k+1}} - v_{\\theta_k}|| \\leq L ||\\phi_{k+1} - \\theta_k||$ holds under Assumptions A.1? \n\n(iv) While I recognize that most reported metrics reflect human preferences to some extent, human evaluation remains an important standard in large-scale generative model assessments. Could the authors clarify why no human evaluation was included, and whether such comparisons might be provided in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Dt9bpl0OyU", "forum": "bhdvdYJ8WZ", "replyto": "bhdvdYJ8WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12161/Reviewer_iTJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12161/Reviewer_iTJr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760833185656, "cdate": 1760833185656, "tmdate": 1762923116238, "mdate": 1762923116238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of Distribution Matching Distillation (DMD and DMD2)—a state-of-the-art timestep distillation framework for diffusion models—when applied to large-scale flow-based text-to-image models such as SD3.5 and FLUX. The authors first analyze the performance of vanilla DMD on these large models and identify key challenges in scaling. To overcome them, they propose three improvements: Implicit Distribution Alignment (IDA), which enforces consistency between the weights of the fake model and the student model; Intra-Segment Guidance (ISG), which redistributes timestep denoising importance learned from the teacher model; and a VFM-based discriminator, which leverages the rich prior knowledge encoded in large foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and well-structured, presenting a clear and compelling motivation for the study.\n2. The proposed approach is thoughtfully designed, addressing the common challenges of stability and scalability in distribution-matching distillation.\n3. The experiments are comprehensive, supported by detailed analysis and clear explanations."}, "weaknesses": {"value": "1. The effect of ISG is not very clear. Although Table 2 shows slight improvements in quality and human-preference metrics, no qualitative comparisons are provided for this component. Including an additional visualization—similar to Figure 3—or a reconstruction loss curve could better illustrate ISG’s impact on training stability and convergence speed.\n2. The design of the VFM discriminator also requires further clarification, particularly regarding the rationale for incorporating another reference image feature as a conditioning input. Moreover, additional visual comparisons would strengthen the paper, especially since you claim a trade-off between distribution coverage (FID-T) and human-preference scores when using this discriminator."}, "questions": {"value": "1. If possible, please provide additional visualizations illustrating the effect of ISG on the training process.\n2. The use of strong foundation models such as DINOv2 and CLIP for the discriminator is reasonable. However, the incorporation of the reference image feature as an additional conditioning input deviates from the vanilla DMD design. Could the authors clarify the motivation for this choice and discuss its impact on image quality and diversity?\n3. How many images were used during training?\n4. In Table 1, it is strongly recommended to include DMD2 results for SD3.5 and FLUX to demonstrate the proposed method’s effectiveness over DMD2 when applied to flow-based models, as claimed.\n5. Table 2 is not comprehensive and should include more ablation variants that isolate or combine each proposed component."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1iVkRVwKyV", "forum": "bhdvdYJ8WZ", "replyto": "bhdvdYJ8WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12161/Reviewer_j3tb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12161/Reviewer_j3tb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749919090, "cdate": 1761749919090, "tmdate": 1762923115559, "mdate": 1762923115559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses instability in Distribution Matching Distillation (DMD/DMD2) when scaling to large flow-based text-to-image (T2I) teachers (e.g., SD-3.5 Large, FLUX.1-dev), identifying poor fake-network tracking and sparse timestep supervision as key issues. It introduces $\\textbf{Implicit Distribution Alignment (IDA)}$, which stabilizes training by progressively aligning the fake network with the generator ($\\phi \\leftarrow \\lambda \\phi + (1-\\lambda)\\theta$), and $\\textbf{Intra-Segment Guidance (ISG)}$, which samples intermediate denoising steps to better cover under-trained timesteps. A VFM-based discriminator (frozen DINOv2 + CLIP) provides semantic guidance. On SDXL, SD-3.5 Large, and FLUX.1-dev, SenseFlow trains 4-step generators that nearly match 80-step teacher quality (e.g., GenEval 0.7098 vs. 0.7140) with minimal overhead ($\\approx$3--6\\% for ISG, 0.6--4\\% for IDA)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ IDA directly addresses the brittle inner loop in DMD via an explicit, cheap parameter interpolation; Appendix A formalizes that this bounds the generator–fake field gap. The “Training‑hours vs. FID” plot (Fig. 3) shows markedly smoother convergence with IDA on SD‑3.5 Large.\n\n+ Experiments span three strong teachers (SDXL 2.6B, SD‑3.5 Large 8B, FLUX.1‑dev 12B) with contemporary baselines (LCM/PCM/Lightning/Hyper, SD‑3.5‑Turbo, FLUX‑Turbo/‑schnell). Results cover COCO‑5K, GenEval, and T2I‑CompBench; for SD‑3.5 the 4‑step Euler variant is best or second‑best across metrics and best in 5/6 CompBench categories.\n\n+ Tables/figures ablate the contributions of IDA, ISG, and VFM‑disc (e.g., removing both tanks SD‑3.5 quality from FID‑T 13.38 → 43.84), and they quantify overheads (ISG ≈3–6%; IDA ≈0.6–4%). Appendix A.5’s step‑consistency visuals support the assumption behind IDA."}, "weaknesses": {"value": "+ The paper notes that adding the VFM discriminator improves human‑preference proxies while slightly raising FID‑T (interpreted as reduced diversity). However, there is no explicit diversity study (e.g., user study, precision–recall curves over seeds). Given the important of this trade‑off in distillation, a quantitative diversity analysis would strengthen the claims.\n\n+ The authors seem to omit IDA on smaller models SDXL but claims generality. A small ablation whether IDA helps small-scale models or not can clarify further the importance of IDA. Timestep settings differ per model (Appendix B.1) but the impact of these choices isn’t studied.\n\n+ All the results are 4‑step; but the prposed method should be able to handle 1-2 step(s) sampling. Showing at least 2‑step results or failure modes would calibrate how far SenseFlow goes in the low‑NFE regime."}, "questions": {"value": "+ Please add benchmarks about diversity as I mentioned above\n+ Does IDA hurt/help in that small-scale models setting (SD1.5 and SDXL)? Also, can you discuss how anchor selection (Appendix B.1) affects results?\n+ Can SenseFlow handle 1-2 step(s) sampling ? If yes then please show comparison and benchmarks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0JMA90c2x2", "forum": "bhdvdYJ8WZ", "replyto": "bhdvdYJ8WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12161/Reviewer_xdRf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12161/Reviewer_xdRf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830846364, "cdate": 1761830846364, "tmdate": 1762923114889, "mdate": 1762923114889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved version of DMD (VSD). The idea is to set the parameters of the fake score model as a linear combination of the fake model and the student model. The paper also proposes to reallocate timestep importance to help convergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies the problems of DMD and proposes a fix. The proposed method achieves good results for finetuning Flux and SD3.5. \n\n2. The paper proposes good ablation experiments to help readers understand the contribution of each component. \n\n3. The proposed method is faster compared with DMD."}, "weaknesses": {"value": "1. Although I recognize IDA as a meaningful contribution, the proposed method IDA only works when the student model and fake model have the same network structure. For example, VSD (DMD) is originally designed for text-to-3D. Or more broadly speaking, the distribution matching does not require the student model to be same as the teacher model (real model here). In these general settings, the IDA does not work. \n\n2.  The images in Fig5, Ours-SD3.5 seem too bright. Same thing for Fig 10. What could be the cause?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nBAmLlSFjl", "forum": "bhdvdYJ8WZ", "replyto": "bhdvdYJ8WZ", "signatures": ["ICLR.cc/2026/Conference/Submission12161/Reviewer_qo4N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12161/Reviewer_qo4N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906893168, "cdate": 1761906893168, "tmdate": 1762923114019, "mdate": 1762923114019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}