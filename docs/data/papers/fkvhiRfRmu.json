{"id": "fkvhiRfRmu", "number": 3951, "cdate": 1757572644518, "mdate": 1763350661828, "content": {"title": "CUES: Bottom-Up Exploration and Top-Down Guidance for Agentic Data Synthesis", "abstract": "Training LLM-based agents with reinforcement learning (RL) in complex environments requires high-quality, environment-specific data. However, generating tasks that are semantically coherent, behaviorally valid, and executable is prohibitively expensive, making the scarcity of such data a fundamental bottleneck for scaling capable agents. Existing synthesis methods struggle to balance high-level intent with environmental grounding, often producing either unexecutable instructions or aimless, low-quality trajectories. To address this dilemma, we propose \\textbf{CuES}, a \\textbf{Cu}riosity-driven and \\textbf{E}nvironment-grounded framework for agentic data \\textbf{S}ynthesis that operates without predefined queries. CuES first uses curiosity-driven exploration to uncover a foundation of fundamentally solvable interaction patterns, ensuring executability by design. Concurrently, top-down guidance expand exploration and task diversity while keeping generated tasks aligned with user intentions. Experiments on AppWorld, BFCL, and WebShop show that CuES generates diverse, executable, and high-quality training tasks, achieving or surpassing the diversity and effectiveness of manually curated datasets and delivering strong downstream RL performance, which makes it possible to train environment-specific agents cost-effectively and efficiently. The code is available at \\url{https://github.com/Anonymize-Author/CuES}.", "tldr": "", "keywords": ["Agentic RL; Data Synthesis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f456b127d332e8f56a5f07d654bb1fa18785ef8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "CuES is a framework for generating training data for LLM agents that combines bottom-up exploration with minimal top-down guidance. It explores an environment to identify solvable interaction patterns, converts them into tasks, verifies executability, and rewrites queries to adjust difficulty—without requiring seed queries. Two simple controls—an Environment Memory and Concept Pools—keep exploration grounded and aligned with user goals. On AppWorld, BFCL, and WebShop, CuES produces diverse, executable tasks and improves downstream RL performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. CuES blends bottom-up task discovery with two light top-down controls. Environment Memory helps revisit useful states, and Concept Pools steer curiosity toward goals without scripting solutions. This keeps the system grounded yet focused.\n\n2. The pipeline does not rely on seed queries or external corpora, and it validates each candidate by checking goal satisfaction and trajectory faithfulness before keeping it. Only verified successes are logged for later stages.\n\n3. On AppWorld, WebShop, and BFCL, CuES yields data that matches or exceeds curated sets and improves downstream training.\n\n4. The framework is designed to produce executable, varied tasks, and analyses show broad coverage while maintaining distance from validation sets. Dataset statistics also indicate a substantial synthesized scale across environments."}, "weaknesses": {"value": "1. The setup fixes substantial budgets (e.g., 500 rollouts, up to 30 steps; long token limits) but does not report synthesis cost per accepted task (tokens, wall-clock) or rejection rates beyond PR. Please include cost-per-kept-query, attempts-per-accept, and an efficiency curve (PR vs. budget) so others can plan deployments.\n\n2. The paper relies on SR@k and relative energy distance computed in embedding space plus t-SNE visuals; these capture distribution shape but can miss behavioral coverage (skills, tools, preconditions). Add behavior-level measures like API/tool coverage, unique action schemas, success-path novelty, and lexical overlap checks against eval sets to rule out near-duplicates.\n\n3. The table toggles the concept pool and shows a trade-off (PR rises but ED increases on WebShop), yet there’s no analogous on/off ablation for the Environment Memory to quantify its contribution. Add a memory ablation and a sweep over pool strength/selection size to map the diversity–executability frontier, and report downstream RL sensitivity to these knobs. This would clarify how much each top-down cue drives gains vs. side effects.\n\n4. Acceptance hinges on a judge verifying goal satisfaction and path faithfulness, with a binary reward=1.0 criterion. It would help to stress-test this with adversarial or ambiguous tasks (e.g., partial success, detours) and to report false-accept/false-reject rates, perhaps via a small human audit or secondary checker."}, "questions": {"value": "1. When I click the anonymous GitHub link, it shows \"Page not found.\" Is there an issue?\n\n2. How did you do the RL training? Can you provide more details?\n\n3. How sensitive are your gains to the strength and composition of the Concept Pool and the mini-pool sampling during exploration? Could you share ablations sweeping pool size/filters and the Environment Memory settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cjmmmgdc6s", "forum": "fkvhiRfRmu", "replyto": "fkvhiRfRmu", "signatures": ["ICLR.cc/2026/Conference/Submission3951/Reviewer_GfAy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3951/Reviewer_GfAy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761280536939, "cdate": 1761280536939, "tmdate": 1762917109722, "mdate": 1762917109722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CuES, a query-free framework for synthesizing high-quality, executable training tasks for LLM-based agents by unifying curiosity-driven bottom-up exploration with lightweight top-down guidance. CuES runs in five stages—requirement confirmation, curious exploration, task abstraction, quality control, and query rewriting—using a concept pool and environment memory to steer exploration toward valid affordances, and explicit execution/judging to ensure tasks are solvable. It generates diverse, environment-grounded tasks without manual seed queries, then adjusts difficulty via progressive hint injection. Experiments on AppWorld, WebShop, and BFCL v3 show CuES’s synthesized data match or surpass original datasets and yield strong downstream performance, outperforming larger or closed-source baselines (e.g., 64.1% greedy on WebShop and ~45% on AppWorld/BFCL). Ablations demonstrate controllable trade-offs between executability, diversity, and alignment via confidence thresholds, batch size, rollout depth, and concept pools. Overall, CuES bridges top-down goal clarity with bottom-up grounding, providing a scalable recipe for cost-effective agent training in complex environments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method has some design in task abstraction and quality control in data synthesis.\n2. The results can surpass strong proprietary models like GPT-5, but more training details need to be confirmed."}, "weaknesses": {"value": "1. Some experiment settings are not clear: (a). Do you use the user need and seed query to synthesize data? Section 3.1 only says they are optional, but I would like to know if you use them in experiments. (b). The training configuration is missing: Do you mix the data synthesized across benchmarks to train a model or train one model for each benchmark? Training steps, batch size, learning rate, loss, accuracy across checkpoints and other dynamics are also missing. (c). The RL setup, algorithm, rewards, etc. are also missing.\n2. From the Figure 3, the synthesized data looks similar to the validation set (maybe more difficult). Training on such data poses the risk of overfitting."}, "questions": {"value": "1. typo in line 057: ... feasible in situ\n2. typo in line 060: ... only targets -> only target\n3. There is a missing comparison with Learn-by-interact (https://arxiv.org/pdf/2501.10893). This work uses backward construction to align with goals, synthesizes subtasks that could be applied in different scenarios, and requires LLMs to follow instructions in exploration in the data synthesis to improve efficiency. This contradicts the descriptions in line 056-061.\n4. Why there is no thinking in Table 1 with open-sourced models, but there is thinking in Table 2 with proprietary models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OqSOZjdgb3", "forum": "fkvhiRfRmu", "replyto": "fkvhiRfRmu", "signatures": ["ICLR.cc/2026/Conference/Submission3951/Reviewer_gNE7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3951/Reviewer_gNE7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761437229201, "cdate": 1761437229201, "tmdate": 1762917109515, "mdate": 1762917109515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CuES (Curiosity-driven and Environment-grounded data Synthesis), a framework that removes reliance on manual seed queries, discovers tasks bottom-up from interaction traces, and then explicitly executes and judges candidates before query rewriting. Two lightweight top-down controls—Requirement Confirmation and Concept Pools—shape coverage and reduce wasted exploration without prescribing exact tasks; an Environment Memory stores compact state–action sketches to revisit informative frontiers and avoid redundant loops. The pipeline has five stages: requirement confirmation, curiosity-guided exploration, task abstraction, quality control, and query rewriting. The authors evaluate CuES on AppWorld, BFCL, and WebShop, reporting synthesized data that match or surpass original datasets and lead to strong downstream training performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear. The paper contrasts top-down imitation (seed queries/LLM expansion) with bottom-up exploration (derive tasks from interactions), arguing CuES integrates the clarity of the former with the executability of the latter.\n2. The proposed framework is lightweight and efficient. Requirement Confirmation and Concept Pools “soft-aim” exploration without scripting solutions; Environment Memory mitigates redundancy by revisiting salient states. This balances diversity with usable coverage.\n3. This paper uses a series of experiments to confirm the performance. On AppWorld/BFCL/WebShop, synthesized data reportedly match or exceed original datasets, with >30-point gains over strong baselines on avg@8/greedy metrics."}, "weaknesses": {"value": "1. Evidence granularity & ablation transparency. The main text repeatedly claims surpassing original datasets and >30-point gains, but the snippet provides no per-task breakdowns, significance tests, or variance across seeds, which are crucial given exploratory randomness.\n2. Top-down controls may implicitly bias discovery. While “lightweight,” Requirement Confirmation/Concept Pools could bias to familiar concepts, potentially missing novel behaviors.\n3. Operational cost and failure modes of exploration. Prior work notes drift/inefficiency in bottom-up pipelines; CuES introduces memory and guidance, but we need exploration cost curves, yield vs. steps, and failure analyses (invalid/partial trajectories) to ensure practicality."}, "questions": {"value": "1. How robust is CuES to requirement mis-specification? If Requirement Confirmation is noisy or misleading, how does performance degrade?\n2. Cross-domain transfer. Can a concept pool learned in one domain help another (e.g., from AppWorld to WebShop)? I would like to see more domain transfer experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gab4iRGJLL", "forum": "fkvhiRfRmu", "replyto": "fkvhiRfRmu", "signatures": ["ICLR.cc/2026/Conference/Submission3951/Reviewer_Y3MX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3951/Reviewer_Y3MX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905678968, "cdate": 1761905678968, "tmdate": 1762917109277, "mdate": 1762917109277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}