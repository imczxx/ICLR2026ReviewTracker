{"id": "gZIcyx1tQY", "number": 22229, "cdate": 1758328044276, "mdate": 1763700742061, "content": {"title": "Probability Distributions Computed by Hard-Attention Transformers", "abstract": "Most expressivity results for transformers treat them as language recognizers (which accept or reject strings), and not as they are used in practice, as language models (which generate strings autoregressively and probabilistically). We characterize the probability distributions that transformer language models can express. We show that making transformer language recognizers autoregressive can sometimes increase their expressivity, and that making them probabilistic can break equivalences that hold in the non-probabilistic case. Our overall contribution is to tease apart what functions transformers are capable of expressing, in their most common use-case as language models.", "tldr": "We exactly characterize the probability distributions expressed by transformers when viewed as language recognizers and autoregressive models.", "keywords": ["expressivity", "weighted automata", "language models", "transformers", "linear temporal logic"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03eb48711f6fe9bda0751bbc9f6413ff3fd65281.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper advances the theoretical study of the expressivity of transformer language models beyond Boolean recognizers, by characterizing their expressivity as probabilistic, autoregressors. The paper focuses on Unique-Hard Attention Transformers (UHATs) and connects them with Linear Temporal Logic (LTL) and counter-free Deterministic Finite Automata (cfDFA) paradigms. Overall, the paper proposes several expressivity results for these paradigms, characterizing when different modeling choices (classifier vs. autoregressive paradigms, Boolean vs. real semirings) do or do not change the set of weighted languages a model can represent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical results are meaningful and timely, which advance the theoretical understanding of the expressivity of transformer-based models. \n\n\n2. In my view, the main strength of the paper is its focus on the probabilistic autoregressive regime, which is more aligned with how language models are used in practice, and thus represents a more meaningful case of study in comparison to previous works.  \n\n\n3. The paper characterizes equivalences in terms of language recognition of UHATs with known classic models (LTL, DFA…). Furthermore, describe different levels of expressivity under different scenarios or configurations."}, "weaknesses": {"value": "1. These results hold for unique-hard attention with no position embeddings. This diverges from common configurations of language models (which include soft attention and positional encodings). Furthermore, previous works such as (Li and Cotterell, 2025) focus on soft attention, resulting in a less-idealized setup (closer to real-world transformers). I believe the paper would significantly benefit from a discussion on which results will hold or not (and why) under these assumptions.\n\n\n2. Some results are somewhat incremental, based mostly on the results of (Yang et al., 2024) or (Li and Cotterell, 2025). While I value the theoretical results, I believe that this limits the contribution of this particular work. \n\n\n3. While the paper adequately describes its goals in Section 1, its novelty and the main contributions are unclear or hard to follow. I would recommend the authors to reinforce this part, in order to emphasize which are the novel results / extensions of this particular work (with respect to previous related works), and the relevance of its contributions.\n\n\n4. The paper lacks a discussion of potential future research directions building on these findings."}, "questions": {"value": "1. Which results extend to soft attention? Similarly, which are the implications of positional embeddings? I would recommend a discussion on these matters.\n\n\n2. Related to my previous point, recent works show that hard attention can be simulated using soft attention through temperature scaling, e.g., see (Yang et al, 2024). Based on this, will some of your results hold under soft attention? Can these findings bridge the results from this paper to soft-attention regimes?\n\n3. The Related Work section seems shallow in its current state. This is mostly because most of the closest works are described in Section 1. I would encourage the authors to reorganize Sections 1 and 2.\n\n\n4. Furthermore, some recent works are missing in the literature review (e.g., see the list below). Some of these missing works address similar scenarios or goals. For example, (Yang et al, 2024) also examines several subclasses of languages recognized by hard-attention transformers, which can be defined in variants of linear temporal logic. Please clarify the similarities or differences with these works.\n \n- Yang, A., Strobl, L., Chiang, D., & Angluin, D. (2024). Simulating hard attention using soft attention. arXiv preprint arXiv:2412.09925.\n\n\n- Hao, Y., Angluin, D., & Frank, R. (2022). Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10, 800-810.\n\n- Barceló, P., Kozachinskiy, A., Lin, A. W., & Podolskii, V. (2023). Logical languages accepted by transformer encoders with hard attention. ICLR 2024.\n\n\n\n\n5. Please define the acronym UHAT as “unique-hard attention transformers” in Page 1, for the sake of clarity to non-familiar readers.\n\n\n6. Relevant results from these works focus on counter-free automata. I wonder whether this limits impact on certain cases that exhibit periodicity. A brief discussion of what is left outside the counter-free setting and whether partial extensions or approximations are possible would be helpful.\n\n\n7. While the paper focuses on expressivity results, can something be said about “learnability” (i.e., sample complexity, efficiency…)?  \n\n\n8. Sections 6.2 and 6.3 lack explicit UHAT results. I believe that the paper might benefit from a more clear discussion on how those results contribute to the study of UHAT expressivity.\n\n\n9. Please include a forward-looking discussion of open problems and next steps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kUqIHeBukk", "forum": "gZIcyx1tQY", "replyto": "gZIcyx1tQY", "signatures": ["ICLR.cc/2026/Conference/Submission22229/Reviewer_hwiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22229/Reviewer_hwiM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833838838, "cdate": 1761833838838, "tmdate": 1762942127171, "mdate": 1762942127171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the expressive power of transformers as probabilistic autoregressive models in the context of formal language theory. Existing theoretical work mostly focuses on transformers as language recognizers (Boolean setting). Here, the authors analyze the probability distributions computed by hard-attention transformers when used as language models, providing equivalence results and separation results between Boolean classifiers, probabilistic classifiers, and probabilistic autoregressors. The study also connects these models with temporal logics and weighted automata, clarifying how expressivity shifts across these settings.\nThe paper is rigorous, well-structured, and contributes to closing a gap in theoretical understanding of transformer expressivity in autoregressive scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Tightly written theoretical work with clear formal contributions.\n* Addresses a meaningful gap in theory: expressive power of transformers as generative language models.\n* Strong formal rigor, with proofs and precise definitions throughout the paper."}, "weaknesses": {"value": "* **Purely theoretical:** While the theoretical contributions are solid, there are no experimental results nor concrete applied examples to illustrate relevance for real-world transformer LMs. Given the conference venue, this limits perceived impact.\n* **Accessibility:** The paper assumes familiarity with temporal logics, weighted automata, and semirings. This is appropriate for a specialized logic or theoretical CS audience, but is demanding for the general machine-learning readership at ICLR.\n\nMinor Comments:\n* There are small repetitions in early sections (e.g., “we then” in lines ~77-82, “language” in lines ~90-93) that make the text slightly repetitive.\n* The notion of “state” is only clarified around Section 4-5. Since “state encoder” is central, I suggest introducing more intuitively what a “state” represents earlier in the introduction.\n* Typo in Section 6.1: $\\tau_1$ should be $\\tau_2$."}, "questions": {"value": "I do not have any particular questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HwfZoTbKBE", "forum": "gZIcyx1tQY", "replyto": "gZIcyx1tQY", "signatures": ["ICLR.cc/2026/Conference/Submission22229/Reviewer_DAYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22229/Reviewer_DAYT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840490894, "cdate": 1761840490894, "tmdate": 1762942126882, "mdate": 1762942126882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the expressive capacity of Transformers when used as probabilistic language models rather than classifiers. To do so, the authors analyze models along 2 axes: 1) Classifiers vs Autoregressors 2) Boolean weights vs (positive) Real-valued weights. The authors show the following:\n- UHATs, LTL and cfDFAs have equivalent state encoders (Thm 6.1) They use this to show that UHATs, LTLs and cfDFAs as classifiers (or autoregressors) define the same weighted language (Corollary 6.1)\n- LTL classifiers can only output finitely many distinct weight values (Prop 6.1). This implies that LTL classifiers cannot express the language $(\\frac{1}{2} a)^*$, which can be expressed by autoregressors (Corollary 6.2).\n- LTL classifiers and autoregressors are equivalent (for the right subsets of operators) (Thm 6.2)\n- In the case of autoregressors, cfNFAs are more expressive than cfDFAs (Prop 6.2)\n- Boolean autoregressors are more expressive than classifiers when key operators are missing (Prop. 6.3)\n- The expressivity gain of autoregression is limited; $(aab)^*$ is not definable by an LTL regressor (Prop 6.4 & Thm 6.3)\n\nI think this is a good paper with solid theoretical contributions. However, I find the style in which it is written makes it hard to quickly extract the main insights and results. I have given a 6 and will increase my score to an 8 if the authors address points 3, 4 and 7 in the Questions/Comments section."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Angle is novel and interesting. I agree with the authors that there is a lack of literature on the topic of language *modelling* with Transformers.\n- This work improves our understanding of the interplay of expressive power between i) Boolean and Real-valued models and ii) classifier and autoregressive models. Given the equivalencies drawn by the authors, the theoretical results have deep implications about many families of models.\n- The authors provide extremely rigorous proofs and reductions. From a technical perspective. The theoretical approach taken is non-trivial and is in itself a significant contribution to theoretical research. Although I am knowledgeable about the Transformer expressivity literature, I am not well-versed in temporal logic, thus I was not able to fully check all the proofs pertaining to this in detail."}, "weaknesses": {"value": "- Although complete and precise in its writing style, I find the paper is very dense and not written in a way where key insights are easy to find/extract. See comments for actionable feedback.\n- The paper has no experimental validation of theoretical claims it makes. It would be nice to at least have minimal experiments to support the results.\n- This work makes few connections to practical settings, such as how their claims might account for empirical shortcomings of LLMs, and it does not discuss the implications of their results for well-known algorithmic tasks. Stronger statements of the form “UHAT Autoregressors belong to class X and therefore cannot perform task Y from a broader class” could improve the paper."}, "questions": {"value": "1. [1] and [2] are highly relevant and are not cited in the related works. They investigate related topics namely how Transformers can express weighted/probabilistic automata/grammars. The latter paper also works on a notion quite similar to what you define as \"State Encoders\" through a notion they define as \"Simulation\".\n2. Do the authors see any relationship between their work and work on \"Generation in the limit\"[3]? This could equally be an interesting direction for discussion.\n3. Could the authors put a section \"Contributions\" with bullet points or something similar in the introduction? It was hard to parse what was done in the paper vs previous work when reading.\n4. It would be beneficial to clarity to add brief proof sketches in the main text for (at least) the most technical theorems instead of simply deferring to the appendix.\n5. I quite like Figure 1 and think it does a good job summarizing the results. However, the upwards arrows are hard to parse in terms of direction of inclusion, it was not immediately obvious for me what it meant. I feel putting a $\\subseteq$ or similar might be clearer.\n6. I think it could also be helpful to have a table summarizing the main results based on assumptions made, e.g. with columns \"Thm/Prop number\" \"Semiring\" \"Model Type\" \"Main Finding\"\n7. Could the authors add a section discussing implications of their results to practice and to specific task families (as mentioned in the \"Weaknesses\" section)?\n\n\n[1] Zhao, H., Panigrahi, A., Ge, R., & Arora, S. (2023). Do transformers parse while predicting the masked word?. arXiv preprint arXiv:2303.08117.\n\n[2] Rizvi-Martel, M., Lizaire, M., Lacroce, C., & Rabusseau, G. (2024, April). Simulating weighted automata over sequences and trees with transformers. In International Conference on Artificial Intelligence and Statistics (pp. 2368-2376). PMLR.\n\n[3] Kleinberg, J., & Mullainathan, S. (2024). Language generation in the limit. Advances in Neural Information Processing Systems, 37, 66058-66079."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YRZwADWByv", "forum": "gZIcyx1tQY", "replyto": "gZIcyx1tQY", "signatures": ["ICLR.cc/2026/Conference/Submission22229/Reviewer_A2Nv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22229/Reviewer_A2Nv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938631813, "cdate": 1761938631813, "tmdate": 1762942126600, "mdate": 1762942126600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a number of theoretical results concerning the expressivity of Transformer models, in particular in relation with counter-free, weighted Deterministic Finite Automata (DFA) and Nondeterministic Finite Automata (NFA). The central innovation consists in considering a setup that is closer to real-world usage of transformer models. In fact, while the results are still largely limited to Unique Hard Attention Transformers (UHAT), but the authors consider their use a autoregressive token generator (language models), rather than just a Boolean classifier (for language recognition). The proof techniques rely on establishing a mapping between UHAT and Linear Temporal Logic, and then proving results for LTL. The paper shows that, while some equivalences established for the Boolean classifier setting extend to the autoregressive one, other equivalences instead break."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I should start by cautioning that my expertise with the topics discussed here is limited, though I agree that understanding the expressivity of transformer models is an important research direction, given their role in powering LLMs and other modern AI advancements, even close to a decade after their initial introduction.\n\nIn my opinion, the strongest merit of the work is that of showing how results obtained in the Boolean classifier setup do not necessarily port to the autoregressive setup, which is indeed closer to how transformers are used, at least in LLMs. A natural next step would consist in applying the same treatment to SoftMax Attention Transformers."}, "weaknesses": {"value": "While the main result is in my opinion that of showing a discrepancy between the Boolean classifier and autoregressive setup, most of the paper is devoted to proving that many equivalence results hold in both setting, thus somewhat reducing the novelty of most contributions in the work. The broke equivalence also seems to apply rather peculiar configurations (subsets of LTL).\n\nAdditionally, while considering the autoregressive setup is a step toward making the analysis more practically relevant, the work still introduces several simplifications over transformers as they are actually employed in the real world.\n\nA few minor points:\n\n* Some acronyms are introduced considerably later than when they are used, which decreases readability\n* At lines 119-124, it would be worth point out the connection between normalized weighted languages and discrete probability distributions\n* While definition 4.2 considers multiple possible suffixes, several sections in the paper appear to focus on estimating just the next token distribution, which creates confusion while reading"}, "questions": {"value": "* How challenging would it be, in your estimate, to adapt your analysis to SMATs?\n* The subsets of LTL that cause the equivalence to break appear to be linked to specific operators: does that provide insights in terms of either expressiveness of complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fpKZ0TOj6r", "forum": "gZIcyx1tQY", "replyto": "gZIcyx1tQY", "signatures": ["ICLR.cc/2026/Conference/Submission22229/Reviewer_uL8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22229/Reviewer_uL8H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008308844, "cdate": 1762008308844, "tmdate": 1762942126076, "mdate": 1762942126076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "List of Changes"}, "comment": {"value": "Dear all,\n\nThank you very much for your reviews. Find below a list of all changes made to the draft. In addition, we have highlighted all changes made using red text in the PDF. \n\n# List of changes\n\n- Since the paper deals with soft-attention transformers as well as hard-attention transformers, we have retitled the paper to \"Probability Distributions Computed by Transformer Language Models\".\n- Refocused the introduction to clarify the narrative of the paper and its results.\n- Expanded the Related Work section and moved it to the end of the paper (Section 6).\n- Section 5 (Expressivity Results) has been reorganized. Whereas before we discussed classifiers, then autoregressors, then various other formalisms, now we discuss the real semiring (5.2), then the Boolean semiring (5.3). This mirrors Figure 1 more closely.\n- Corrected Cor 5.4 and Figure 1 to say that the formalisms in question are incomparable.\n- Theorem 5.6 (formerly Theorem 6.2) is now stated for the Boolean semiring only, not a general semiring. This eliminates the awkward condition that the translations in this theorem preserve the support of the weighted language. We think the new theorem is cleaner and supports the message of the paper better (that it is autoregression, as much as weighting, that makes the language modeling setup different from language recognition).\n- Additionally, the former Lemmas 6.1 and 6.2, which served as the proof sketch for Theorem 5.6 (formerly Theorem 6.2), have been relegated to the appendix (as Lemmas B.1 and B.2) and replaced with a more readable sketch.\n- Proposition 5.7 (formerly Corollary 6.3) has been rewritten to reflect the fragments of temporal logic we use, and a short motivation has been added above it.\n- The proof of Proposition 5.9 (in Appendix D) has been rewritten without using semigroup theory.\n- Replaced the Conclusion with a Discussion (Section 7) addressing implications for applications and future work.\n- Many small clarifications and corrections."}}, "id": "akXqgWdqQs", "forum": "gZIcyx1tQY", "replyto": "gZIcyx1tQY", "signatures": ["ICLR.cc/2026/Conference/Submission22229/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22229/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission22229/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763702293039, "cdate": 1763702293039, "tmdate": 1763702293039, "mdate": 1763702293039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}