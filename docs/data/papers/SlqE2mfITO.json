{"id": "SlqE2mfITO", "number": 8474, "cdate": 1758085539225, "mdate": 1763022458958, "content": {"title": "LightMotion: A Light and Tuning-free Method for Simulating Camera Motion in Video Generation", "abstract": "Existing camera-controlled video generation methods face computational bottlenecks, either due to significant fine-tuning overhead or heavy inference processes. In this paper, we proposes LightMotion, a light and tuning-free method for simulating camera motion in video generation. Operating in the latent space, it eliminates additional fine-tuning, inpainting, and depth estimation, making it more streamlined than existing methods. The endeavors of this paper comprise: (i) The latent space permutation operation simulates three basic camera motions: panning, zooming, and rotation, whose combinations cover almost all real-world movements. (ii) The latent space resampling strategy combines background-aware sampling with cross-frame alignment, accurately filling new perspectives while maintaining coherence across frames. (iii) Our analysis reveals that the tuning-free permutation and resampling will cause an SNR shift in latent space, leading to poor-quality generation. To address this, we propose the latent space correction scheme, which mitigates the shift and consequently improves video quality. Extensive experiments validate the superiority of LightMotion over other baselines.", "tldr": "A Light and Tuning-free Method for Simulating Camera Motion in Video Generation", "keywords": ["Tuning-free", "Lightweight", "Video Generation", "Camera Simulation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2d50a0b06c0d34b9fc61fb973d7c6e295787cbf0.pdf", "supplementary_material": "/attachment/ba475af02391979101d978384c2e8919899be94f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces \"LightMotion,\" which is a simple, \"tuning-free\" way to add camera motion (like panning, zooming, rotating) to generated videos.\n\nIt works in the latent space, mostly by shuffling things around (permutation operations). It also has a \"background-aware\" resampling trick and cross-frame alignment to keep the video from looking weird or artifact-y.\n\nA key part of the paper is that the authors found that these latent manipulations mess up the Signal-to-Noise Ratio (SNR), which hurts video quality. They figured out why this happens and proposed a fix for it. They test their method against a bunch of others with both metrics and visual comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **It's Simple and Cheap:** I really like that this method is \"tuning-free.\" You don't need to fine-tune a massive model, and it doesn't rely on extra, heavy models for things like depth or inpainting. It just works in the latent space, which makes it super accessible and compute-friendly.\n* **Smart Resampling:** The background-aware sampling and cross-frame alignment seem to really work. Figure 3 shows it avoids those weird repeating objects and temporal artifacts that have plagued other tuning-free methods.\n* **Finding (and Fixing) the SNR Shift:** This was a really cool insight. Spotting that the latent operations were causing an SNR shift—and then actually fixing it—is a solid contribution. Figure 4 does a good job showing how much this fix improves the final video quality."}, "weaknesses": {"value": "1.  **Needs a Better Failure Analysis:** The \"Limitations\" section is pretty vague. It just says performance drops with \"rapid, complex\" motion. But what does that *mean*? Is there a specific speed or angle where it just breaks? The paper really needs a proper analysis of its failure cases. Figure 9 (ablation) hints at this, but it's not detailed enough.\n2.  **The User Study is a Bit \"Meh\":** The user studies (Table 1, Appendix) and the GPT-4o evaluation feel a bit weak. You don't give enough detail on how they were run. Was it randomized? What did the interface look like? Were the results even statistically significant? Using GPT-4o scores just makes it feel even less transparent than just asking people.\n3.  **How \"General\" is the Motion?** You *claim* it supports non-linear camera movements (like in Appendix E), but you're super sparse on the details. How do you actually parameterize or implement a complex, curvy camera path? The paper doesn't convincingly show that it can generalize beyond the simple pan/zoom/rotate."}, "questions": {"value": "1.  Can you show some quantitative results or just a clearer failure analysis for extreme camera movements (like huge perspective shifts or super-fast, jerky paths)?\n2.  For the user/GPT-4o study: What did you do to prevent bias (like randomization)? And did you run any statistical significance tests on the results?\n3. Can you provide more experiments on more SOTA models like WAN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JCuppptRQ3", "forum": "SlqE2mfITO", "replyto": "SlqE2mfITO", "signatures": ["ICLR.cc/2026/Conference/Submission8474/Reviewer_yJek"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8474/Reviewer_yJek"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793468760, "cdate": 1761793468760, "tmdate": 1762920354336, "mdate": 1762920354336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7vP9PapUoT", "forum": "SlqE2mfITO", "replyto": "SlqE2mfITO", "signatures": ["ICLR.cc/2026/Conference/Submission8474/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8474/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763022458260, "cdate": 1763022458260, "tmdate": 1763022458260, "mdate": 1763022458260, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LightMotion, a lightweight and tuning-free method for simulating camera motion in video generation. Operating in the latent space, it eliminates the need for fine-tuning, inpainting, or depth estimation by introducing three components: latent space permutation for modeling panning, zooming, and rotation; latent space resampling for background-aware and temporally consistent frame synthesis; and latent space correction to mitigate SNR shifts and enhance visual quality. Experiments demonstrate that LightMotion achieves better video quality and camera controllability compared to some existing methods while maintaining high efficiency and full compatibility with several U-Net-based video diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and structured, making it accessible to readers with varying levels of expertise in the field.\n2. Visual results seem promising compared to vanilla results yielded by AnimateDiff and some baseline methods.\n3. The paper provides thorough experimental evaluations, including ablation studies on key components."}, "weaknesses": {"value": "1. The authors do not adapt their method to (or even discuss their method on) advanced DiT-based video models such as HunyuanVideo and the WAN series. Moreover, their approach relies on cross-attention maps, which may be difficult to transfer to modern architectures that predominantly employ full 3D attention mechanisms. In addition, I believe the proposed Cross-Frame Alignment strategy would also be challenging to apply to more advanced video generation models, since these models often adopt the causal VAE, where multiple frames in pixel space are encoded into a single latent frame, making frame-by-frame region replication infeasible. These limitations raise concerns about the generalization ability of LightMotion.\n2. The author should discuss (or compare with, if possible) more existing outstanding works on motion customization (both U-Net-based ones and DiT-based ones), including but not limited to:\n    1. MOFT: Video Diffusion Models are Training-free Motion Interpreter and Controller\n    2. MotionClone: Training-Free Motion Cloning for Controllable Video Generation\n    3. VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models\n    4. VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control\n3. This paper does not specify or thoroughly discuss the evaluation dataset since it is a self-collected one, which may not provide a comprehensive view of LightMotion’s effectiveness. Releasing more details of the evaluation dataset or incorporating videos from publicly available benchmarks would greatly enhance the credibility of the paper."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ItWNHB2nm5", "forum": "SlqE2mfITO", "replyto": "SlqE2mfITO", "signatures": ["ICLR.cc/2026/Conference/Submission8474/Reviewer_rUmJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8474/Reviewer_rUmJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834348867, "cdate": 1761834348867, "tmdate": 1762920353943, "mdate": 1762920353943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a camera control algorithm for video generation models to enable controllable camera motion during generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method is training-free, making it easily integrable into any existing model."}, "weaknesses": {"value": "The paper should include comparative experiments against other baseline models, such as the camera-control variant of Wan2.1-Fun: https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera .\n\nThe method modifies the model’s forward computation, which may introduce a potential train-inference inconsistency. The authors should provide additional experiments to verify that this modification does not degrade the model’s original generative capabilities.\n\nThe generation results are not adequately demonstrated. The authors are encouraged to submit supplementary materials in video format."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IL3QRz2a7A", "forum": "SlqE2mfITO", "replyto": "SlqE2mfITO", "signatures": ["ICLR.cc/2026/Conference/Submission8474/Reviewer_EWhG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8474/Reviewer_EWhG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912935512, "cdate": 1761912935512, "tmdate": 1762920353368, "mdate": 1762920353368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LightMotion, a lightweight and tuning-free method for simulating camera motion in video generation. The method operates directly in the latent space of pre-trained video diffusion models and requires no fine-tuning. Its core contributions are threefold: 1) Latent space permutatio which simulates fundamental camera motions (pan, zoom, rotate) through geometric transformations; 2) Latent space resampling, which uses background-aware sampling and cross-frame alignment to fill new regions coherently and avoid artifacts; 3) Latent space correction, a proposed mechanism to mitigate an identified SNR shift caused by the previous operations, thereby improving output quality. Extensive experiments show that LightMotion achieves performance comparable to tuning-based methods while being more efficient."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The method is **well-motivated**, tackling a clear gap in the literature between expensive tuning-based methods and limited tuning-free alternatives. The three-component design is logical and addresses key challenges like artifact avoidance and quality preservation.\n- The **experimental validation is comprehensive**, demonstrating that LightMotion, as a training-free method, achieves performance comparable to training-based methods across multiple metrics (FVD, CLIP scores, control errors) and in user studies."}, "weaknesses": {"value": "- The **experiments are based on older U-Net-based video generation models** (e.g., AnimateDiff). The method's effectiveness has not been validated on current mainstream DiT-based video diffusion models, which limits the perception of its contemporary relevance.\n- The explanation of the **SNR shift is insufficient**. The authors identify that permutation/resampling operations cause a quality drop and link it to an SNR shift, based on a correlation between predicted noise variance and theoretical SNR. However, this correlation is established for the standard denoising process. It is unclear if this premise holds after the non-standard latent manipulations performed by LightMotion. The core mechanism of the SNR shift lacks a solid theoretical explanation or an intuitive justification."}, "questions": {"value": "- As raised in the weaknesses, could the authors comment on or demonstrate **LightMotion's compatibility and performance with modern DiT-based video diffusion models**?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NC7YMxhm7w", "forum": "SlqE2mfITO", "replyto": "SlqE2mfITO", "signatures": ["ICLR.cc/2026/Conference/Submission8474/Reviewer_TMGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8474/Reviewer_TMGm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012367355, "cdate": 1762012367355, "tmdate": 1762920353023, "mdate": 1762920353023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}