{"id": "SqQYMnfyLS", "number": 14087, "cdate": 1758228278221, "mdate": 1759897390749, "content": {"title": "Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets", "abstract": "Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. Yet most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and privacy. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries like healthcare, fraud detection, and predictive maintenance where even small gains on minority cases can be critical.", "tldr": "Generating useful, faithful and diverse data with a focus on minority samples in imbalanced settings.", "keywords": ["synthetic tabular data", "class imbalance", "conditional generation", "transformer-based models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e71ce6b62b6c9a4b38c9dc8754e6469e8f1ec685.pdf", "supplementary_material": "/attachment/0ac7a08cdedcc43e25e6e5801bf658090147d57b.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes CTTVAE+TBS which is a transformer-based tabular VAE that can perform synthetic data generation for imbalanced tabular data. It has 2 key components: CTTVAE (class-aware triplet loss) and TBS (training by sampling). The authors find on the 6 real world datasets that was tested, the proposed method is able to yield a higher F1 score (high utility) while maintaining fidelity (measured by Wasserstein and JSD) and also preserve privacy (measured by DCR and NNDR)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed architecture combined with upsampling has not been done before\n2. The results look great in terms of all 3 aspects of tabular synthetic data generated\n3. The paper is well written and the proposed concepts are presented clearly"}, "weaknesses": {"value": "1. Only 6 datasets are used. It is lot fewer than other past works, which makes the claim less strong.\n2. Diffusion method such as TabDDPM and tabular foundation model based methods such as TabPFGen and TabEBM are not present in the results"}, "questions": {"value": "1. It would be great to show an intuitive 2D dataset and show how the proposed method is able to outperform other baseline methods"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tEhywTAtjK", "forum": "SqQYMnfyLS", "replyto": "SqQYMnfyLS", "signatures": ["ICLR.cc/2026/Conference/Submission14087/Reviewer_ZBoB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14087/Reviewer_ZBoB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593570593, "cdate": 1761593570593, "tmdate": 1762924565884, "mdate": 1762924565884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CTTVAE+TBS, a conditional transformer-based tabular variational autoencoder designed to generate high-quality synthetic tabular data in the presence of severe class imbalance. CTTVAE combines a class-aware triplet margin loss (which enforces compact and separable latent representations) with a training-by-sampling (TBS) strategy that adaptively increases exposure to underrepresented classes. This dual mechanism enables the model to produce synthetic data that better supports downstream tasks, particularly for minority categories, without sacrificing fidelity or privacy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Generating synthetic datasets under severe class imbalance is a practically important problem with broad real-world relevance. The paper is well written and clearly organized, with careful optimization of baseline models and comprehensive details of the experimental setup and results provided in the appendix."}, "weaknesses": {"value": "My main concern with the paper relate to its empirical evaluation. The paper compares CTTVAE only against GAN- and VAE-based models, which are now relatively weak baselines compared to modern diffusion-based approaches. The paper justify this by stating that diffusion models such as TabDDPM require significantly greater computational resources and are typically reported at the dataset level, making alignment with their evaluation setup impractical. However, this reasoning is not fully convincing. In practice, TabDDPM is not substantially more computationally demanding than models like TVAE or CTGAN. Indeed, when Wang and Nguyen (2025) introduced TTVAE, they included direct comparisons with both TabDDPM and TabSyn. It should therefore be feasible to run diffusion-based baselines on the same train/test splits, using the same number of replications, and to evaluate them with the same utility, fidelity, and privacy metrics—ensuring that hyperparameters are appropriately tuned (e.g., via Optuna). Including these stronger baselines would make the empirical analysis much more compelling.\n\nAdditionally, while line 56 indicates that CTTVAE is compared against two classical oversampling baselines, and Figure 4 in the Appendix lists both SMOTE and SMOTENC, the paper only reports results for SMOTE. The paper needs to clarify this point.  \n\nOther aspects of the paper that warrant improvement include:\n\n1. The experiments are replicated only three times. Increasing the number of replications (e.g., to ten) would improve the statistical robustness of the results.\n\n2. The paper evaluates only six datasets in its baseline comparisons. Expanding the number of benchmark datasets would strengthen the empirical evidence—for example, the TTVAE paper includes sixteen benchmarks.\n\n3. In line 58, the paper claims to compare CTTVAE with five state-of-the-art baselines, but this statement should be moderated since models such as TVAE and CTGAN are no longer considered state-of-the-art.\n\n4. Tables 3 and 4 should include standard deviations, and the main text should explicitly reference that per-dataset results are available in Tables 8–13 of the Appendix, which should likewise report standard deviations.\n\nMinor issues:\n\nThe caption of Table 2 states that the table report results for both majority and minority groups, but only the minority group results are reported. (The paper might also want to point out that the results for both minority and majority classes are presented in Table 7 in the Appendix).\n\nIn line 123, the paper should cite Wang and Nguyen (2025) rather than Badaro et al. (2023).\n\nThe captions of Tables 3, 4, and 5 should define the abbreviations “Maj.” and “Min.” for clarity. \n\n##########################################\n\nOverall, this is paper that could be of relevance to the ICLR community. However, in its current form, I am inclined to recommend rejection due to significant limitations in the experimental evaluation—particularly the absence of comparisons with diffusion-based models. That said, I would be open to revisiting my assessment if the paper address these issues and provide evidence that the advantages of CTTVAE persist when evaluated against stronger diffusion-based baselines."}, "questions": {"value": "Could you clarify why aligning the evaluations of diffusion models with the paper’s experimental setup is considered impractical?\n\nCan you clarify why the paper did not pursue comparisons against SMOTENC?\n\nThe paper notes that DCR and NNDR are reported at the 5th percentile following Zhao et al. (2021), but the computation procedure is unclear. Is the 5th percentile taken over the distances between each synthetic record and all real records, or is DCR first computed per synthetic record and then the 5th percentile reported across all synthetic samples?\n\nThe paper omits details about the transformer architecture. Does this imply that the same configuration as in the TTVAE reference is used?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The README file accompanying the submitted code states under the Project Status section that the paper has been submitted to AAAI 2026. This is most likely a simple oversight—perhaps the authors initially planned to submit to AAAI and forgot to update the README. However, I wanted to flag this here in case it indicates a potential double submission."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mQgT8j17D8", "forum": "SqQYMnfyLS", "replyto": "SqQYMnfyLS", "signatures": ["ICLR.cc/2026/Conference/Submission14087/Reviewer_yrzf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14087/Reviewer_yrzf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984442853, "cdate": 1761984442853, "tmdate": 1762924564395, "mdate": 1762924564395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CTTVAE+TBS, a conditional transformer-based variational autoencoder framework designed for tabular data generation under severe class imbalance. The model introduces two main components: a triplet-margin loss that structures the latent space to enforce class separability, and a training-by-sampling strategy that probabilistically adjusts batch composition to mitigate class frequency bias during training. Together, these mechanisms aim to improve minority-class representation and enhance the downstream utility of generated data while maintaining fidelity and privacy. Experimental results across multiple real-world datasets show that CTTVAE+TBS achieves stronger minority-class F1 scores and better privacy metrics than existing tabular data generators such as SMOTE, CTGAN, and TTVAE, though its overall utility gains are sometimes inconsistent."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and clearly structured, making the overall methodology and experimental design easy to follow."}, "weaknesses": {"value": "1. The paper lacks a clear explanation of how 'balanced datasets' are constructed in experiments, and it is also unclear how many minority samples were actually added or generated to compose the balanced dataset used for experiments.\n\n\n2. In several datasets, models trained on the original imbalanced data outperform those trained on oversampled data, raising questions about the true benefit of the proposed oversampling process. If the primary motivation is to generate standalone synthetic data rather than to improve classifier performance, the paper should include a direct evaluation of the quality, representativeness, and practical utility of the generated samples themselves.\n\n\n3. There exists diffusion-based oversampling methods, such as Sos (Score-based Oversampling for Tabular Data) [1], tackle the same imbalance problem through score-based generative modeling. The paper would benefit from a comparison or discussion that clarifies how CTTVAE+TBS differs from or complements these diffusion-based approaches.\n\n\n[1] Kim, J., Lee, C., Shin, Y., Park, S., Kim, M., Park, N., & Cho, J. (2022, August). Sos: Score-based oversampling for tabular data. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining (pp. 762-772)."}, "questions": {"value": "1. What do you think might be the reason why oversampling sometimes performs worse than using the original data?\n\n2. Could the authors clarify how the balanced dataset used in the experiments was composed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethic concerns"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jaZVE0tMJS", "forum": "SqQYMnfyLS", "replyto": "SqQYMnfyLS", "signatures": ["ICLR.cc/2026/Conference/Submission14087/Reviewer_xVLk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14087/Reviewer_xVLk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008898750, "cdate": 1762008898750, "tmdate": 1762924563846, "mdate": 1762924563846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of the paper aim to tackle the problem of conditional generation of imbalanced data within tabular data, regime. They introduce a Conditional Transformer-based Tabular Variational Autoencoder (CTTVAE), an extension of TTVAE augmented by an additional loss, namely a class-aware triplet margin loss and a training-by-sampling (TBS) strategy that adaptively increases exposure to underrepresented groups. Through this adaptation they intend to restructure the latent space so that it better encapsulates the intra-class compactness and inter-class separation and to mitigate representation bias  when categorical features exhibit strong imbalance.\nAt the experiments section they explore the performance of their method against state of the arts methods, across six real-world benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors of the paper introduce CTTVAE+TBS through adding a triplet loss to TTVAE and TBS. By doing this, they aim to conditionally generate imbalanced data, a task that is frequently overseen. They add a triplet margin loss to the TTVAEs loss function (that replaces KL divergence term, with Maximum Mean Discrepancy (MMD) penalty between the aggregated posterior q(z) and the Gaussian prior p(z)) in order to encourage embeddings of the same class to lie closer together than those of different classes. \nMoreover they use a variant of the TBS concept, at which sampling is guided solely by a categorical variable as opposed to sampling over all discrete columns."}, "weaknesses": {"value": "The method, albeit interesting, does not seem to significantly outperform existing methods. Actually SMOTE outperforms CTTVAE+TBS across all distance metrics. \n\nthree soft comments: \n(1) there is a sum at equation (4) that is not explained (why, over what);\n(2) at equation 6, it (most likely) should be $\\hat{z}_i$;\n(3) at Figure 1., above the circled set with the blue and red dots, it should probably be (z,h) instead of just z;"}, "questions": {"value": "I would like to ask the authors if they could please :\n\n1) comment on the sensitivity of the method to hyperparemeters $\\alpha$ and $\\beta$ (Loss CTTVAE, page 4);\n2) please explain what is the role of $u_r \\sim \\mathcal{U}(0,1)$, in equation 6.;\n3) comment on the sensitivity and performance of the method with respect to the number of classes;\n4) comment on the outperformance of SMOTE across all distance metrics (Table 3);\n5) add the absolute difference between correlation matrices of SMOTE as well (Figure 3);\n6) compare their results with [1], a diffusion model based method that also generates (and imputes) tabular data and also employ conditional generation\n\n\n[1] Jolicoeur-Martineau, Alexia, Kilian Fatras, and Tal Kachman. \"Generating and imputing tabular data via diffusion and flow-based gradient-boosted trees.\" International conference on artificial intelligence and statistics. PMLR, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XMPX7blZB8", "forum": "SqQYMnfyLS", "replyto": "SqQYMnfyLS", "signatures": ["ICLR.cc/2026/Conference/Submission14087/Reviewer_fUEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14087/Reviewer_fUEk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762370979107, "cdate": 1762370979107, "tmdate": 1762924563463, "mdate": 1762924563463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}