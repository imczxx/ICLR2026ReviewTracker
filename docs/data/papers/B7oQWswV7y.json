{"id": "B7oQWswV7y", "number": 12495, "cdate": 1758208206774, "mdate": 1759897506013, "content": {"title": "Automatic Instance Selection with Genetic Updating for Few-shot LLM Jailbreak", "abstract": "This paper studies the problem of few-shot large language model (LLM) jailbreak, which aims to trigger unsafe outputs of LLMs using only a handful of adversarial examples. However, the effectiveness of the current few-shot jailbreak attacks is limited by the challenge of systematically selecting the most potent instances, with existing methods often resorting to inefficient manual or random selection. In this paper, we propose a novel approach named Automatic Instance Selection with Genetic Updating (ACCEPT) for few-shot LLM jailbreak. The core of our ACCEPT is to utilize textual gradient and fitness scores to guide the optimization process automatically. In particular, our ACCEPT designs a loss objective prioritizing successful jailbreaks, which can further guide the selection of instances via textual gradient. Furthermore, we construct a pool with meaningless marks, and consider the injection operators as chromosomes following the genetic algorithm. A fitness function is then defined in jailbreak scenarios, which helps the iterations across generations for proper prompts. Extensive experiments across several benchmark datasets can validate the effectiveness of the proposed ACCEPT in comparison with extensive baselines.", "tldr": "", "keywords": ["Large Language Model", "Jailbreak attack", "Few-shot", "text gradient", "genetic algorithm"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/091c800299b1e5e9256855f3e80d0b7c88ff256e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a framework named ACCEPT for few-shot LLM jailbreaking. The method leverages textual gradients to automatically select the most effective instances and uses a genetic algorithm to enhance the harmful prompt through non-semantic perturbations. Experiments show that the proposed approach achieves superior results on both open-source and closed-source models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach is highly innovative and logically sound; the concept of leveraging an LLM to generate \"textual gradients\" is particularly novel.\n2. The experimental evaluation is thorough, and the accompanying analysis is detailed.\n3. All figures and tables are aesthetically pleasing and easy to understand."}, "weaknesses": {"value": "1. While the paper's primary focus is on offensive jailbreak techniques, the manuscript would be strengthened by briefly discussing the defensive implications of this work. For instance, adding a short elaboration in the conclusion on how these findings can inform the development of more robust defense mechanisms would be a valuable addition.\n2. It would benefit from more specific details in the methodology sections. In particular, the descriptions of the TextGrad in Section 3.2 and the GA in Section 3.3 could be expanded with more concrete implementation details to improve clarity."}, "questions": {"value": "1. In Section 2.1, the distinction between the two methods, \"Adversarial Prompting and In-Context Learning,\" could be further detailed.\n2. When the harmful request is combined with the selected instances for a jailbreak attack, how is it handled if the attack is not effective?\n3. How are the emojis inserted into the harmful request, specifically the ones in Figure 7, selected? Inappropriate emojis could have a negative effect.\n4. What does \"fitness\" mean in Figure 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XqYveYfeKt", "forum": "B7oQWswV7y", "replyto": "B7oQWswV7y", "signatures": ["ICLR.cc/2026/Conference/Submission12495/Reviewer_7z1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12495/Reviewer_7z1H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712215099, "cdate": 1761712215099, "tmdate": 1762923368543, "mdate": 1762923368543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the in-context learning-based few-shot jailbreak problem and proposes a unified framework named ACCEPT. The framework introduces two complementary components. For instance selection, ACCEPT employs a TextGrad mechanism that reformulates the discrete selection of few-shot examples into an optimization process guided by textual feedback from LLMs. For prompt design, it integrates a genetic algorithm to search for effective injection strategies, enhancing the stealthiness and adaptability of jailbreak prompts. Comprehensive experiments conducted on multiple open-source and closed-source language models, across various datasets, demonstrate that ACCEPT achieves SOTA attack performance, significantly surpassing existing jailbreak baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ACCEPT tackles the few-shot jailbreak problem, which is both practically significant and highly relevant to the security of current LLM systems.\n2. The use of a genetic algorithm for prompt injection provides a heuristic yet interpretable optimization process. This design allows researchers to better understand how perturbations influence jailbreak success, improving both transparency and controllability.\n3. The TextGrad mechanism leverages LLM feedback to guide instance selection, transforming a discrete combinatorial problem into a continuous optimization-like process.\n4. Extensive experiments across multiple open-source and closed-source models show consistent and substantial performance improvements, validating the effectiveness and robustness of the proposed framework."}, "weaknesses": {"value": "1. Although the TextGrad mechanism achieves promising optimization results through LLM-based feedback, the paper lacks a solid theoretical justification. Its convergence properties and robustness are not formally analyzed or discussed.\n2. The study primarily relies on the ASR-GPT metric, using an LLM to judge attack success. This single-metric setup may introduce bias from the evaluator model itself.\n3. The proposed framework integrates multiple heuristic search processes (e.g., the genetic algorithm and iterative LLM optimization), which are likely to incur significant computational overhead. However, the paper provides no discussion or analysis of efficiency or resource consumption."}, "questions": {"value": "Please refer to my comments on weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U9hjaUoKnN", "forum": "B7oQWswV7y", "replyto": "B7oQWswV7y", "signatures": ["ICLR.cc/2026/Conference/Submission12495/Reviewer_hs9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12495/Reviewer_hs9X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718073991, "cdate": 1761718073991, "tmdate": 1762923367732, "mdate": 1762923367732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ACCEPT, an automated attack framework designed to address the challenge of instance selection in few-shot LLM jailbreak attacks. It achieves a superior jailbreak success rate through the synergistic cooperation of two components: a semantic-level module that selects optimal instances and a non-semantic-level module that enhances the attack prompt."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The overall framework is novel, enhancing both the selected instances and the malicious request from two synergistic perspectives: semantic and non-semantic.\n2. The paper is well-written with a clear and coherent logical flow.\n3. The experiments are extensive and sound."}, "weaknesses": {"value": "1. The implementation of TextGrad relies on an LLM, so the quality of the LLM determines the final jailbreak success rate. It is necessary to elaborate on this part in the conclusion or appendix.\n2. The paper should provide a clearer case study to illustrate how the \"problem diagnosis\" and \"contribution assessment\" parts of the textual gradient are generated and how they guide instance selection."}, "questions": {"value": "1. In \"Gradient-Guided Candidate Sampling\", why select a candidate subset first instead of directly selecting from the entire sample pool C?\n2. What does the single-point in the crossover operation mean?\n3. In Algorithm 1, should the final output be E_{best}? I think that E_{T_{grad}} is not necessarily the best.\n4. In the \"Fitness Function,\" when calculating the probability for refusal terms, is it the sum of the refusal probabilities for each word, or is the refusal probability calculated for the entire sentence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KatQSxWISZ", "forum": "B7oQWswV7y", "replyto": "B7oQWswV7y", "signatures": ["ICLR.cc/2026/Conference/Submission12495/Reviewer_nnRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12495/Reviewer_nnRX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843858837, "cdate": 1761843858837, "tmdate": 1762923367460, "mdate": 1762923367460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper devises a few-shot jailbreak method called ACCEPT. The goal is to provide the target LLM with some instances together with a harmful question and leverage the in-context learning to make it output a harmful response. The method starts with a set of candidate instances and utilizes an auxiliary LLM to provide \"textual gradient\" to narrow down the candidate pool and select new instances. To reduce the refusal rate, it also includes a genetic algorithm to inject special non-semantic tokens. The instance selection and the genetic algorithm run alternately. Experiments use two datasets, AdvBench and HarmBench, and six models against five baselines. Results show the proposed method can achieve best ASRs and is more robust against defenses. Ablation studies show that both the instance selection and the genetic algorithm are necessary."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It studies a critical issue of few-short jailbreaks.\n\n2. The idea to combine the textual gradient and the genetic algorithm is interesting.\n\n3. The experimental results show its effectiveness and robustness."}, "weaknesses": {"value": "1. It's unclear how the fitness score is computed. Although the paper lists some refuse phrases in the appendix, it's not clear how they are used to compute the probability. For example, does it generate a set of samples and compute the ratio of the matched strings, or use some token distribution? If the token distribution is used, how can we attack the closed-source model or service that doesn't return the distribution?\n\n2. It's unclear how the quality of the candidate pool will affect the performance. Because TextGrad only selects the instances from the pool without creating any new instances, and only one candidate pool was used. For example, if the algorithm starts with some random pool instead of the malicious ManyHarm, it may not succeed.\n\n3. It's unclear what the time cost is or the queries needed to conduct this attack. Similarly, it's suggested to compare the efficiency with baselines.\n\n4. Regarding the robustness, since the injected non-semantic tokens include some special tokens, paraphrasing or input filtering may easily remove the adversarial effect. Also, it's unclear what the baseline is in Figure 2."}, "questions": {"value": "1. How is the fitness score computed?\n\n2. Will a different candidate pool affect the performance significantly?\n\n3. What is the time cost and the query budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Scy4MaVDLz", "forum": "B7oQWswV7y", "replyto": "B7oQWswV7y", "signatures": ["ICLR.cc/2026/Conference/Submission12495/Reviewer_Jt2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12495/Reviewer_Jt2U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884236947, "cdate": 1761884236947, "tmdate": 1762923367152, "mdate": 1762923367152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}