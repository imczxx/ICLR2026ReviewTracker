{"id": "jCkmwIN9kz", "number": 22451, "cdate": 1758331208618, "mdate": 1759896865351, "content": {"title": "Fighting Fire with Fire: Assessing Test Set Contamination Through Deliberate Training on Test Data", "abstract": "Test set contamination poses a serious threat to reliable model evaluation. Whether inadvertent or deliberate, contamination may lead to misrepresenting model capabilities to both researchers and the public: this spurious performance augmentation may in turn cause harm when these models are deployed in real-world applications. In this work, we propose a novel test set contamination detection method that relies solely on analyzing loss trajectories during deliberate fine-tuning on target benchmarks. Our key insight is that models exhibit quantifiably different learning dynamics when exposed to previously encountered versus novel data. Concretely, we systematically fine-tune models on test data mixed within decontaminated data at varying proportions to simulate contamination scenarios, and fine-tune on decontaminated data only to simulate the clean counterparts.  We show that clustering methods using as few as 200 data points can distinguish clean from contaminated scenarios with +95\\% accuracy.  Our method also demonstrates superior robustness in detecting contamination of paraphrased evaluation data compared to membership inference attack baselines, which operate at the individual sample level and typically target verbatim matches. Critically, our approach represents a paradigm shift from static detection metrics to dynamic training-based assessment: observing how models react to controlled fine-tuning on target data rather than analyzing fixed outputs or input manipulations. We posit that this intervention-based methodology offers inherently higher resistance to detection evasion, as the metrics cannot be directly optimized as reward signals during model development, providing a more robust foundation for maintaining evaluation integrity.", "tldr": "", "keywords": ["test set contamination", "loss trajectory analysis", "learning dynamics"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34efaf2fd792214310372daf58fa175537971343.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a novel detection method for the test set contamination problem, by distinguishing the training dynamic curves on test data of clean and cheated models."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed detection method is novel in the sense that it depends on the dynamics of further training on targeted data, rather than on static metrics.\n2. The detection accuracy is high.\n3. It's robust to paraphrasing. \n4. It derives a performance gain prediction tool."}, "weaknesses": {"value": "1. The largest model evaluated is of 4B size. If the proposed method is not able to scale up to larger models, the contribution would be limited.\n2. The authors should clarify the knowledge and capability of the contamination detector via a formal threat model. For example, in your setting, the detector has access to the model weights, which is a strong assumption. I think only open-sourced models give this while-box access to the detector. Moreover, the detector also knows the training algorithm (SFT and its details), which is not practical in real-world scenarios.\n3. In practice, different models usually use different training algorithms and hyperparameters. Thus, the ideal results on cross-model generalization cannot support real-world practice: training a classifier with a fixed model and using it to discriminate any new model.\n4. The experimental details are so ambiguous that I can not confidently identify the pros and cons of the reported results. Especially, the authors need to clarify which curves are used for training the classifier in each figure and table.\n    \n    - For example, in Figure 2 (a), at the dot of (Qwen 2.5-3B, 0.16), what does the curve set for the classifier contain? All curves with various model archs and various poisoning ratios? Or only curves with  Qwen 2.5-3B, and clean and 0.16 poisoned cases?\n    - Similarly, what are the training and evaluation settings for Table 1? Lines 214-215 say that it's about cross-model generalization. But it seems that the model used in training is the same one for evaluation.\n    - Similarly, in Table 2, for each row, how do you collect curves and train the classifier? For each model arch, do you collect curves with various poisoning ratios and train only one classifier? Or do you train different classifiers for different evaluation ratios?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CQPVxLSOaY", "forum": "jCkmwIN9kz", "replyto": "jCkmwIN9kz", "signatures": ["ICLR.cc/2026/Conference/Submission22451/Reviewer_tHiN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22451/Reviewer_tHiN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536353429, "cdate": 1761536353429, "tmdate": 1762942223518, "mdate": 1762942223518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the critical problem of test set contamination in LLMs and proposes an intervention-based contamination detection method that leverages the analysis of loss trajectories from deliberately fine-tuning models on the test set. By comparing the dynamics of loss decay, the method distinguishes between models trained with and without access to benchmark data. The dynamic, training-based audit is shown to outperform the MIA baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a compelling departure from static, output-based contamination detection by focusing instead on loss trajectory dynamics induced by deliberate fine-tuning, which is interesting to me.\n2. Accurate contamination detection can be achieved using only a fraction of the loss trajectory, enabling early identification with as few as 256 samples or the first 16 data points (Fig. 3b)."}, "weaknesses": {"value": "1. Firstly, I have to say the novelty is limited, as some of the previous works have already discussed using the loss trajectory for MIA, despite being cited. The conceptual novelty is therefore slightly less pronounced outside the context of LLMs, and the authors would strengthen their case by discussing distinct challenges in the LLM versus vision or other modalities.\n2. The proposed Dynamic Time Warping (DTW) distance between loss trajectories seems to be intuitive and lacks formal justification or theoretical analysis connecting observed loss dynamics with contamination levels, and no bounds or convergence results are discussed regarding the reliability of such signals.\n3. The manuscript’s writing and presentation could benefit from further refinement. For instance, the full names and abbreviations of several terms (e.g., DTW and MIA) are repeated multiple times; the related-work section is not always expressed in a formal academic tone; a few equations are typeset in a way that may strike readers as informal; several sentences still carry noticeable traces of LLM generation; and there are minor typographical issues (e.g., line 184: “We use OLMo-2-0425-1B (?)”; line 193: “See App A.”).\n4. Figure 1 may be confusing because the trend sketched in the diagram does not fully align with the textual description. Replacing the schematic illustration with the actual experimental curves could clarify the point.\n5. The evaluation covers only a small set of datasets and models, and the chosen models are neither very recent nor fully representative of the current state of the art.\n6. The paper does not include ablation studies that would explicitly demonstrate the effectiveness of the proposed method.\n7. I am somewhat surprised to see the paper end with a “Discussion\" and \"Limitation” section rather than a “Conclusion.” A concise concluding summary would help readers better appreciate the key takeaways."}, "questions": {"value": "1. How does the proposed contamination detection method generalize to benchmarks involving free-form generation, regression tasks, or highly unbalanced labels? \n2. Is there empirical evidence or theory to support how post-training stages might affect the detectability of contamination in loss trajectories, especially in practice?\n3. What are the computational costs of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lMkSu6K0gd", "forum": "jCkmwIN9kz", "replyto": "jCkmwIN9kz", "signatures": ["ICLR.cc/2026/Conference/Submission22451/Reviewer_Bx44"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22451/Reviewer_Bx44"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928276889, "cdate": 1761928276889, "tmdate": 1762942223326, "mdate": 1762942223326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an intervention-based audit for test-set contamination: deliberate fine-tune a model on the target benchmark itself. The method is implemented through recording the loss trajectory during fine-tuning, and classify whether the model had prior exposure to the benchmark by comparing the shape of that trajectory (via DTW + kNN/k-medoids). The study spans three model families (OLMo-2, Qwen2.5, Qwen3) and benchmarks. For the experimental results, the authors report >95% accuracy for clean vs. contaminated detection, and early detection performance. They also show the ability to estimate the magnitude of spurious gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Simple but effective, intervention-centric approach.** Different from prior methods, shifting the signal from static outputs to training dynamics is refreshing. The focus on trajectory shape instead of absolute loss value fits this motivation.\n2. **Broad quantitative sweep.** The experimental grid is reasonably large (three model families; dozens of benchmarks) with consistent gains in detection acc comparing with those using static outputs signals. \n3. **Early-signal detection.** The experimetns show that ≈256 samples already enough for high accuracy makes the method more practical."}, "weaknesses": {"value": "1. **Realism & scope of contamination.** The paper simulates contamination by SFT on mixed data with poisoning ratios p rather than during pre-training. It’s plausible that pre-training or RLHF leaves different fingerprints than SFT exposure. The authors acknowledge some threats (e.g., RLHF altering dynamics), but the empirical settings are still simplifying too much.\n2. **Practicality for third-party models auditing.** The approach requires fine-tuning the suspect model on the test benchmark to collect loss trajectories. That’s feasible for open models, but many real audits target closed models that either cannot access parameters or cannot be trained for IP/policy reasons. Even for open models, the paper’s approach still implies non-trivial compute.\n3. **Sensitivity to training recipe.** The classifier is keyed to optimization traces: optimizer, batch size, lr, number of steps , and the exact SFT objective. The paper varies LR modestly, a deeper robustness slice may help.\n4.  **Baselines feel underpowered. ** The adapted baselines are per-sample MIA scores aggregated and then fed to 8-NNclassifiers. This may undersell stronger MIA pipelines (e.g., LiRA[1]/RMIA-like[2] likelihood ratio testing). Stronger baseline choices would strengthen claims.\n\n[1] Carlini, Nicholas, et al. \"Membership inference attacks from first principles.\" SP, 2022.\n[2] Zarifzadeh, Sajjad, Philippe Liu, and Reza Shokri. \"Low-cost high-power membership inference attacks.\" ICML, 2024."}, "questions": {"value": "1. **Closed-model audit.** If the suspect model cannot be fine-tuned (typical for frontier APIs), what’s your recommended adaptation to this  scenarios?\n2. **Training-recipe invariance.** How sensitive is detection to optimizer type choice, batch size, LR schedule (cosine vs. constant), fine tuning steps?\n3. **Compute & time overhead.** What is the end-to-end cost per audited model/benchmark (wall-clock, GPU hours, memory)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iLj23bJzPx", "forum": "jCkmwIN9kz", "replyto": "jCkmwIN9kz", "signatures": ["ICLR.cc/2026/Conference/Submission22451/Reviewer_4auC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22451/Reviewer_4auC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941311847, "cdate": 1761941311847, "tmdate": 1762942223154, "mdate": 1762942223154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}