{"id": "Cvwu959bOs", "number": 24155, "cdate": 1758353386577, "mdate": 1759896779632, "content": {"title": "Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction", "abstract": "This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65\\% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46\\% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54\\% (relative).", "tldr": "We propose Cognitive-Structured RE, a framework that improves relation extraction accuracy and explainability through a novel Hit@Dict RL reward.", "keywords": ["Relation Extraction", "Large Language Models", "Explainable AI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/539a99f3df183c6c14d827c24fe7a1e9bff10241.pdf", "supplementary_material": "/attachment/6cca201f3f6be302ea4d0d52017f4a727d786ffd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework called COGRE for relation extraction (RE) tasks, aiming to improve both accuracy and explainability. The framework combines cognitive-structured reasoning and reinforcement learning (RL) to optimize RE models. A key component of the method is the HIT@DICT reward, which uses a relationship keywords dictionary to evaluate the quality of explanations generated by the model. The authors claim that this approach improves RE performance, especially for one-shot learning, and provides better human-readable explanations. The framework is evaluated on TACRED and NYT29 datasets, showing improved F1 scores and human evaluation results compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel approach by combining cognitive-structured reasoning with reinforcement learning for relation extraction tasks. This integration allows the model to improve both the accuracy and explainability of its predictions, which is a significant contribution to the field of explainable AI. The experimental setup is well-designed, and the results, particularly in terms of F1 score and human evaluation, show that the proposed HIT@DICT reward improves the model’s ability to generate better explanations."}, "weaknesses": {"value": "The HIT@DICT reward relies heavily on a predefined keywords dictionary, which restricts the model’s ability to generalize beyond the fixed set of keywords, potentially causing bias and limiting performance in more complex or novel relations. This reliance on keywords could also lead to overfitting to specific patterns, rather than allowing the model to capture the full complexity of relational semantics. Second, the lack of comparison with current mainstream large models (like GPT-4o，GPT-5，Gemini) is a major drawback, as it makes it difficult to assess how this approach stacks up against state-of-the-art methods. Without this comparison, it's unclear whether the proposed framework brings substantial improvement over existing models, which have demonstrated strong performance in similar tasks."}, "questions": {"value": "Can the authors provide more details on how the keywords dictionary is built, and what the limitations of this approach might be in real-world applications? How would the model perform when faced with novel relations that do not have clear keyword representations in the dictionary?\n\nWould it be possible to expand the experiments to include comparisons with mainstream LLMs? How does COGRE compare with these models in terms of both accuracy and explanation quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6GnwOXPZ4e", "forum": "Cvwu959bOs", "replyto": "Cvwu959bOs", "signatures": ["ICLR.cc/2026/Conference/Submission24155/Reviewer_Pg7E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24155/Reviewer_Pg7E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468943515, "cdate": 1761468943515, "tmdate": 1762942961700, "mdate": 1762942961700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COGRE, a Relation Matching framework improving accuracy and explainability using a cognitive-inspired, three-step reasoning prompt (chunking, anchoring, reasoning). This reasoning is then optimized via RL using a novel HIT@DICT reward. This reward combines task accuracy with an explanation score, which is measured by matching model output against a \"relational keywords dictionary\" automatically constructed (using an LLM) from the model's own correct explanations . Experiments show this RL optimization provides significant gains, such as a +23.46% absolute F1 improvement on One-shot NYT29 and a 54% relative increase in human-rated explanation quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides strong empirical support through rich experimentation. Beyond baseline F1 comparisons, the authors include detailed ablation studies to validate their framework's components, an analysis of training dynamics to show the reward's behavior.\n2. The HIT@DICT reward is a novel approach to supervising explanation quality without costly human-annotated data. The method of automatically constructing a \"credit dictionary\" by using an LLM to extract keywords from the model's own correct outputs is a clever, data-efficient way to create a fine-grained reward signal for jointly optimizing accuracy and explainability."}, "weaknesses": {"value": "1. The task definition is unconventional. While framed as Relation Extraction (RE), the task is more accurately described as \"Relation Matching\"—a binary classification on whether entity pairs in two sentences share a relation. This setup is considerably simpler than traditional multi-class Relation Classification.\n\n2. The authors appear to overlook highly relevant related work [1][2] concerning Relation Extraction using RLVR. A discussion of these papers seems necessary to properly position this work.\n\n3. The performance attributed to HIT@DICT is debatable. The authors use a much stronger model (GPT-4o, per the paper) to analyze the student model's reasoning and create the high-quality keyword dictionary. This introduces an external dependency on a powerful model, creating an unfair advantage that is not available to the baseline methods.\n\n4. The authors should consider adding a simpler, crucial baseline: SFT using only the final binary classification label (Yes/No), without any CoT generation. This is a more direct and lightweight approach, and its performance would provide a clearer context for the benefits of the proposed RL method.\n\n[1]Li, Ran, et al. \"Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced (R2) GRPO.\" [2]Dai, Runpeng, et al. \"R1-re: Cross-domain relation extraction with rlvr.\""}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pc41FRRdz6", "forum": "Cvwu959bOs", "replyto": "Cvwu959bOs", "signatures": ["ICLR.cc/2026/Conference/Submission24155/Reviewer_GtZe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24155/Reviewer_GtZe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877926652, "cdate": 1761877926652, "tmdate": 1762942961408, "mdate": 1762942961408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes COGRE, a three-step, 'cognitively inspired; framework for one-shot relation extraction (RE): proposition chunking, keyword anchoring, and integrative reasoning. It adds an RL objective that sums an accuracy reward with HIT@DICT, a rule-based explanation reward built from a relational-keywords dictionary distilled by GPT-4o from true-positive LLM explanations (Alg. 1). On one-shot TACRED/NYT29, COGRE improves F1 over prompting baselines; adding RL with HIT@DICT further boosts F1 and yields shorter, human-preferred explanations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1) A clear lear, modular framework that operationalizes explainable RE with readable traces. \n\nS2) I think this design is reasonably simple, and reproducible tht avoids costly LLM-as-a-judge at training time; formulas here are explicit. \n\nS3) Solid empirical picture wuith consistent improvements across two LLMs and datasets."}, "weaknesses": {"value": "W1) I believe the reward here may have a fatal flaw in that it maybe prioritizing label/keyword copying. If I understand the framework ((Alg 1)) correctly, RHIT@DICT explicitly uses ground truth labels for both sentences and includes decomposed label tokens in the dictionary, risking explanation gaming (keyword stuffing). \n\nW2) Limited baselines -- Comparisons used by authors omit stronger LLM RE set-ups (e.g., richer in-context label descriptions or recent IE-aligned LLMs) beyond SUMASK and a neuro-symbolic classifier; scope could be broader.\n\nW3) No variance over episode sampling. Authors sample 1,000 episodes per partition but report only point estimates (no CIs/std over different samplings/seeds), so stability is unclear. \n\nW4) Hyperparameters fixed without sensitivity (L310-316). Reward weights w_entity=0.4, w_relation=1.0, N=5 and imbalanced-classification weights in Eq. (6) are “heuristically chosen,” but no ablation is provided. The authors should clarify how these values came about. \n\nI think in totality this is good work, but evaluation could be made strengthened with stronger baselines, variance reporting, and reward-design robustness checks."}, "questions": {"value": "Q1) Beyond length normalization in Eq. (5), how do you prevent models from inflating RHIT@DICT by repeating label tokens/synonyms? Any anti-duplication filters or hit caps? \n\nQ2) Since you sample 1,000 episodes per split, can you report mean±std over ≥3 random episode samplings (and seeds) for Table 1?\n\nQ3) How do results change if the dictionary is built with an open model (e.g., Phi) instead of GPT-4o? I think this analysis would be stronger if you included a drop-in replacement analysis. \n\nQ4) For eq 8, what is θ_ref exactly (frozen backbone or SFT’d model)? Please report sensitivity to β."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EV8GX7ScM4", "forum": "Cvwu959bOs", "replyto": "Cvwu959bOs", "signatures": ["ICLR.cc/2026/Conference/Submission24155/Reviewer_kcwt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24155/Reviewer_kcwt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933624617, "cdate": 1761933624617, "tmdate": 1762942961085, "mdate": 1762942961085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called COGRE, aiming to improve task accuracy and explainability for one-shot RE. Its core components include a three-step reasoning mechanism mimicking cognitive psychology (semantic chunking, keyword anchoring, and integrative reasoning) and an optimization process based on RL. The latter employs an innovative HIT@DICT reward function, which combines accuracy rewards with an explanation quality reward based on an automatically constructed relation keyword dictionary using LLM. As the result, COGRE is validated on the one-shot TACRED and NYT29 datasets, using Qwen2.5-15B-Instruct and Phi-4. It improves accuracy with balanced precision and recall and with the RL+HIT@DICT reward, the F1 score improves by an absolute 23.46%. Human evaluation shows that the relation keywords generated by the best model are highly aligned with the golden labels, resulting in a relative 54% improvement in explanation quality score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper innovatively frames one-shot RE as a reasoning and explanation task, by designing a reasoning mechanism integrating cognitive psychology and the HIT@DICT reward function with a LLM-constructed dictionary, which simultaneously boost accuracy and explanation quality and can be transferred to other LLM reasoning tasks. \n2. COGRE addresses two major issues in the RE field, including the lack of data in low-resource scenarios and supervision in language-based explanations. COGRE does not rely on the number of labeled samples, requiring only one supporting sentence/relation. Through a three-step reasoning mechanism, it generates traceable explanations, solving the trust problem of the black-box problem.\n3. The paper details the experimental setup, clearly explaining the basic model, hyperparameters, hardware environment, and training time. It also provides training and testing data, as well as prompt templates, which will help researchers in related fields to further expand upon this."}, "weaknesses": {"value": "1. The paper contains inconsistencies in describing prior work and lacks clarity in theoretical formalizations. In the Abstract, the paper frame its contribution as addressing the \"lack of supervision for language-based explanations in traditional RE\" that acknowledges prior work may have attempted language-based explanations but lacked supervision for them. However, in the Related Work, it overstate this by claiming \"these approaches have limited explainability due to the lack of language-based explanations\", which contradicts the actual state of the field.\n2. The experimental setup undermines the paper’s claims of state-of-the-art (SOTA) performance and real-world applicability. The paper only compares against a 2023 prompt-based baseline(e.g., SUMASK (Li et al., 2023)). However, RE with LLMs is a rapidly evolving field these two years. Without comparing to recent works, the paper’s claim of being \"SOTA\" is unsubstantiated.\n3. The paper’s evaluation of language-based explainability is limited to surface-level metrics, failing to validate the fidelity of explanations (whether explanations truly reflect the model’s reasoning)."}, "questions": {"value": "1. In Section 3.1, the paper defines s_1 and s_2 as \"two input sentences\", which is scattered across the text (not explicitly tied to Equation (1)) and lacks explicit labeling. For example, \" s_1 denotes the support sentence and s_2 denotes the test sentence\"), leading to confusion for readers. \n2. The theoretical introduction of the three-step reasoning (chunking → anchoring → integration) is overly brief. For example, it should explain how chunking aligns with Miller’s 1956 theory beyond a passing citation or how each step mitigates LLM hallucinations with mechanistic details.\n3. The paper’s core motivation is to address explainability in \"high-stakes domains (healthcare, law, finance)\" (Section 1). However, it only evaluates on general-domain datasets (one-shot TACRED and NYT29). This misalignment raises critical questions about real-world applicability. For example, can COGRE accurately chunk domain-specific terms in healthcare, or generate meaningful explanations for professional relations? Without testing on domain-specific datasets, the paper fails to validate its intended use case.\n4. The paper uses fixed hyperparameters for the HIT@DICT reward and GRPO optimization but provides no sensitivity analysis. For instance, how do changes in ω_entity and ω_relation affect F1 and explanation quality? \n5. The core problem the paper aims to solve is \"the lack of supervision for language-based explanations\", which implicitly requires ensuring that explanations are causally linked to the model’s decisions (not just textually aligned with gold labels). However, the paper only evaluates explanations via human subjective scores (3-point Likert scale) and keyword matching rates with the dictionary. If keywords are removed from the matching dictionary, can the model still correctly identify the relations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cgiqquw1ag", "forum": "Cvwu959bOs", "replyto": "Cvwu959bOs", "signatures": ["ICLR.cc/2026/Conference/Submission24155/Reviewer_L1ac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24155/Reviewer_L1ac"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967868634, "cdate": 1761967868634, "tmdate": 1762942960643, "mdate": 1762942960643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}