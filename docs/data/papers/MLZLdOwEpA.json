{"id": "MLZLdOwEpA", "number": 25286, "cdate": 1758366209375, "mdate": 1759896726617, "content": {"title": "AI Alignment with Provable Protection of Human Judgements", "abstract": "Reinforcement learning from human preference rankings forms the basis for training language models to be helpful and value-aligned. As these powerful AI systems are trained for increasingly high-stakes tasks, the risk of leaking sensitive human training data increases. However, the problem of protecting human preference data is complicated by the fact that reinforcement learning from human feedback is a multistage pipeline involving learning a reward function from human preferences, and subsequently training a language model policy from the learned rewards. To address these issues, we design algorithms for the task of alignment from preference feedback that provably avoid leaking human preference data in both the Bradley-Terry and Plackett-Luce models. Our algorithms satisfy $\\epsilon$-DP while matching the minimax optimal sample complexity for the task of aligning a policy to human preference rankings. These results demonstrate that there is no inherent tradeoff between protecting the privacy of human preferences and efficient alignment with human values.", "tldr": "", "keywords": ["Alignment", "RLHF", "performance guarantees", "asymptotic match"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92a748e9119bf34cfc22f518404542aef9271b9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studied the RLHF under the central differential privacy model. The authors investigated the problem of private RLHF on both pairwise and K-wise comparisons for the human preference model in RLHF. And they also consider RLHF via the contextual bandits view and the Markov decision process view. They designed a private pessimistic MLE algorithm for the reward model by achieving DP via the objective perturbation and output perturbation. And they provided suboptimality bounds for their algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studied an interesting and important problem of private RLHF.\n2. The paper presented algorithms and derived theoretical guarantees for their design."}, "weaknesses": {"value": "1. The statement for writing in the paper is unclear. For example, in the abstract, the authors stated they aim to protect the privacy of human preferences. However, the algorithm design is not privacy-preserving for preference information.\n2. The algorithm and technical parts are incremental. They are mainly building on [1], combining with existing DP techniques.\n3. The theoretical results are missing some important factors. For example, it is well-known that the exponential term of $e^B$ is unavoidable due to the BT model, but I didn't find it in their final bounds.\n4. The literature review is not comprehensive. Please see [2-4] for related work.\n5. Lack some empirical results.\n\n[1]. Zhu, Banghua, Michael Jordan, and Jiantao Jiao. \"Principled reinforcement learning with human feedback from pairwise or k-wise comparisons.\" International Conference on Machine Learning. PMLR, 2023.\n\n[2]. Zhan, Wenhao, et al. \"Provable offline preference-based reinforcement learning.\" arXiv preprint arXiv:2305.14816 (2023).\n\n[3].Yuan, Hongyi, et al. \"Rrhf: Rank responses to align language models with human feedback.\" Advances in Neural Information Processing Systems 36 (2023): 10935-10950.\n\n[4].Song, Feifan, et al. \"Preference ranking optimization for human alignment.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 17. 2024."}, "questions": {"value": "1. In your algorithm 1, by using the objective function, refer to section 5.2 of [1], this already achieves DP for parameter estimation. Why do you still need output perturbation in step 6?\n2. Compare your results with the results in [1] and discuss them. And what is the advantage of your methods and results compared with theirs?\n3. You claim your results are minimax optimal sample complexity. What is the evidence for minimax optimal such as lower bounds?\n\n[1].Chowdhury, Sayak Ray, Xingyu Zhou, and Nagarajan Natarajan. \"Differentially private reward estimation with preference feedback.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0naaMDzl6x", "forum": "MLZLdOwEpA", "replyto": "MLZLdOwEpA", "signatures": ["ICLR.cc/2026/Conference/Submission25286/Reviewer_u7tJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25286/Reviewer_u7tJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761075985784, "cdate": 1761075985784, "tmdate": 1762943387906, "mdate": 1762943387906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the leakage of human preference data in Reinforcement Learning from Human Feedback (RLHF). The authors design algorithms that satisfy $\\epsilon$-DP while still aligning to the preference data."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly states the assumptions, which are reasonable.\n2. The paper considers a extensive set of settings with both Contextual Bandits and general MDPs both in the pairwise and K-wise comparison setting.\n3. The findings of this paper are quite strong, demonstrating that the proposed algorithms satisfy $\\epsilon$-DP, while having a small sub-optimality gap."}, "weaknesses": {"value": "1. The writing of the paper needs some improvement. The introduction and conclusion section seem to bring up unrelated examples such as the generation of harmful content. It is not fully clear how these examples relate to the topic of the paper.\n2. While I like the theoretical results of the paper, it does not include any empirical results. While I understand that the focus of the paper are the theoretical results,  I believe given the practical implications of the results, at least a small toy-box experiment would be highly beneficial.\n3. I’m not convinced by the relevance of the considered topic. Preference data doesn’t seem to sensitive data and I do find the examples given in the introduction to be quite artificial. I do believe the authors should include more practical examples of why the leakage of preference data would be a problem."}, "questions": {"value": "1. Is there any research on whether it is possible to extract preference data?\n2. Why would the leakage of preference data be problematic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SQJMB3Y1jG", "forum": "MLZLdOwEpA", "replyto": "MLZLdOwEpA", "signatures": ["ICLR.cc/2026/Conference/Submission25286/Reviewer_uQnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25286/Reviewer_uQnB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844685086, "cdate": 1761844685086, "tmdate": 1762943387659, "mdate": 1762943387659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for performing Reinforcement Learning from Human Feedback (RLHF) with formal differential privacy guarantees, ensuring that human preference data remains confidential while maintaining statistical efficiency. The authors design algorithms for learning from preference rankings under the Bradley–Terry and Plackett–Luce models that achieve differential privacy and match the minimax-optimal sample complexity of non-private RLHF. They provide theoretical results for both contextual bandit and general MDP settings, demonstrating that private versions of maximum-likelihood estimation and pessimistic policy optimization yield near-optimal suboptimality bounds $O(\\sqrt{d/n})$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Ensuring data privacy is a crucial aspect of reinforcement learning, and the authors present an algorithm that achieves differential privacy guarantees without sacrificing performance.\n2. The manuscript is well-organized, clearly presented, and easy to understand.\n3. This paper addresses both the contextual bandit scenario and the general MDP setting."}, "weaknesses": {"value": "1. The level of technical difficulty and originality. Algorithm 1, 2 and 3 seem to be derived straightforward using the gaussian mechanism.\n2. The paper only provides upper bounds without corresponding lower bounds to demonstrate their tightness.\n3. The paper lacks experimental results. While this is acceptable for a theoretical study, RLHF’s primary objective in the context of LLMs is practical alignment in real-world applications. Without empirical validation, the contribution of the paper appears weaker and less convincing in demonstrating its real-world relevance.\n4. Typo: in page 2 the definition of $\\sum_{D}$, the inner summation should start from $k=j+1$."}, "questions": {"value": "How does the result of this paper compare with those in other DP-RHLF studies? For example, in [Chowdhury et al., 2024], a similar bound is achieved on a randomized response strategy.\n\n[Chowdhury et al., 2024] Chowdhury, Sayak Ray, Xingyu Zhou, and Nagarajan Natarajan. \"Differentially private reward estimation with preference feedback.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DPnRXruTwJ", "forum": "MLZLdOwEpA", "replyto": "MLZLdOwEpA", "signatures": ["ICLR.cc/2026/Conference/Submission25286/Reviewer_q4Kd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25286/Reviewer_q4Kd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893697955, "cdate": 1761893697955, "tmdate": 1762943387287, "mdate": 1762943387287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the privacy risks in Reinforcement Learning from Human Feedback (RLHF). While RLHF effectively teaches models to follow human-preferred behaviors, it relies on human preference data—potentially revealing sensitive information from human data. To mitigate this, the authors propose differentially private algorithms for aligning policies using human preference rankings under Bradley–Terry and Plackett–Luce models. Their key contribution is proving that privacy-preserving RLHF can match the minimax optimal sample complexity of non-private algorithms for the leading terms, meaning there is no inherent tradeoff between privacy and statistical efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper’s originality lies in integrating differential privacy (DP) directly into the theory of RLHF.  Prior work on privacy in RL assumes rewards are private but known, while this work uniquely handles the learning of rewards from human rankings.\n2. The theoretical development is rigorous, well-grounded in statistical learning and DP theory, and connects clearly to recent results on RLHF sample complexity (Zhu et al. 2023)."}, "weaknesses": {"value": "1. While the theoretical contributions are solid, the paper lacks any empirical or simulated demonstration of the proposed algorithms. Even small-scale synthetic experiments (e.g., in 2D linear bandits) could illustrate the impact of privacy noise on performance and validate the asymptotic analysis.\n2. The results hinge on the assumption that rewards are linearly parameterized with known feature maps. This assumption, while standard in theoretical RL, limits applicability to real-world LLM alignment tasks where reward models are deep neural networks. \n3. The paper does not provide or discuss any lower bound in the context of DP-RLHF itself. To me, the techniques to derive the upper bounds in this paper are relatively direct given the literatures in both RLHF and DP.\n4. Since this is a theory driven paper, it is necessary to discuss more on existing literatures on the theory of RLHF. References include [1,2,3,4,5], etc.\n\n**References:** \n\n[1] Zhan, Wenhao, Masatoshi Uehara, Nathan Kallus, Jason D. Lee, and Wen Sun. \"Provable Offline Preference-Based Reinforcement Learning.\" In The Twelfth International Conference on Learning Representations.\n\n[2] Liu, Zhihan, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, and Zhaoran Wang. \"Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer.\" Advances in Neural Information Processing Systems 37 (2024): 138663-138697.\n\n[3] Cen, Shicong, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, and Bo Dai. \"Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF.\" In The Thirteenth International Conference on Learning Representations.\n\n[4] Xie, Tengyang, Dylan J. Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Hassan Awadallah, and Alexander Rakhlin. \"Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF.\" In The Thirteenth International Conference on Learning Representations.\n\n[5] Huang, Audrey, Wenhao Zhan, Tengyang Xie, Jason D. Lee, Wen Sun, Akshay Krishnamurthy, and Dylan J. Foster. \"Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization.\" In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. Could the authors provide even a small-scale empirical study (e.g., synthetic linear bandit or toy MDP) to illustrate the tradeoff between privacy level $\\epsilon$ and suboptimality? This would make the paper more accessible to the broader ICLR audience.\n2. Can the method extend beyond linear setups to general function approximations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FgTsvU6Xkj", "forum": "MLZLdOwEpA", "replyto": "MLZLdOwEpA", "signatures": ["ICLR.cc/2026/Conference/Submission25286/Reviewer_5hVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25286/Reviewer_5hVq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762894368987, "cdate": 1762894368987, "tmdate": 1762943387121, "mdate": 1762943387121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}