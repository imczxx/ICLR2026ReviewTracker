{"id": "c8oAL6i2yR", "number": 13663, "cdate": 1758220614916, "mdate": 1759897421488, "content": {"title": "A Probabilistic Basis for Low-Rank Matrix Learning", "abstract": "Low rank inference on matrices is widely conducted by optimizing a cost function augmented with a penalty proportional to the nuclear norm $\\Vert \\cdot \\Vert_*$.\nHowever, despite the assortment of computational methods for such problems, there is a surprising lack of understanding of the underlying probability distributions being referred to.\nIn this article, we study the distribution with density $f(X)\\propto e^{-\\lambda\\Vert X\\Vert_*}$,  finding many of its fundamental attributes to be analytically tractable via differential geometry.\nWe use these facts to design an improved MCMC algorithm for low rank Bayesian inference as well as to learn the penalty parameter $\\lambda$, obviating the need for hyperparameter tuning when this is difficult or impossible. \nFinally, we deploy these to improve the accuracy and efficiency of low rank Bayesian matrix denoising and completion algorithms in numerical experiments.", "tldr": "We study the matrix probability distribution with negative log-density given by the nuclear norm.", "keywords": ["Nuclear Norm", "Low Rank", "Matrix Distribution", "Nonsmooth"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27a8c8bc2260211030dca8caca62bcc8575fc296.pdf", "supplementary_material": "/attachment/19c5e1c744f28210b6832fcfd58ea6ffe2e0bc1d.zip"}, "replies": [{"content": {"summary": {"value": "This paper approaches conventional matrix completion with nuclear norm regularization from a Bayesian perspective by studying the fundamental distribution $f(X) \\propto \\exp(-\\lambda \\|X\\|_*)$. Building on this viewpoint, the authors propose an MCMC algorithm tailored for low-rank Bayesian inference and explore strategies for estimating the tuning parameter $\\lambda$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper seems to be the first to rigorously study the properties of the matrix-valued distribution with density proportional to $ \\exp(-\\lambda \\|X\\|_*) $. It presents an explicit formula for the normalizing constant and provides an approximation for the stochastic representation that is both computationally and analytically tractable."}, "weaknesses": {"value": "It appears that the proposed method can assist in estimating the optimal tuning parameter $\\lambda$ without relying on a grid search, which is commonly used in practice. While some modifications have been proposed to boost the MCMC algorithm, but it is still relatively slow compared to the well-established soft-impute algorithm. I am curious whether there are additional benefits to adopting a Bayesian approach for matrix learning. Although I am not an expert in Bayesian methods, one notable advantage is the ability to quantify uncertainty through the posterior distribution. In contrast, traditional matrix completion methods often face challenges in conducting inference. It would strengthen the paper if the authors could further illustrate—both theoretically and numerically—the inference capabilities of their estimator for the target matrix."}, "questions": {"value": "Before equation (3), it says the constant $C$ is independent of $X$. I am curious, isn't $C$ will depend on the dimension of $X$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qw2B4dfb25", "forum": "c8oAL6i2yR", "replyto": "c8oAL6i2yR", "signatures": ["ICLR.cc/2026/Conference/Submission13663/Reviewer_Dn8r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13663/Reviewer_Dn8r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679351633, "cdate": 1761679351633, "tmdate": 1762924232162, "mdate": 1762924232162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper primarily elucidates probability distributions related to the nuclear norm. By studying the distribution with density, the authors develop an improved MCMC algorithm for low-rank Bayesian inference and propose a principled approach to learn the penalty parameter , thereby eliminating the need for hyperparameter tuning. Research indicates that leveraging fundamental properties of the nuclear norm distribution can enhance the accuracy and efficiency of low-rank Bayesian matrix denoising and completion algorithms.\nPaper is mostly theory with some lightweight experiments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents rigorous theoretical derivations with strong conceptual significance, offering a novel probabilistic perspective that bridges convex optimization and Bayesian inference. In addition, it systematically investigates the probabilistic foundations underlying the nuclear norm regularization term, providing an explicit analytical form of the nuclear norm distribution, along with its normalizing constant, symmetry properties, and the distribution of singular values.\n\n2. The motivation behind the problem formulation and algorithm design is clearly articulated, and the numerical experiments cover a wide range of scenarios."}, "weaknesses": {"value": "1. The authors claim to have designed an improved MCMC algorithm for low-rank Bayesian inference, as described in Section 4.1 and the corresponding appendix. However, the paper does not clearly specify how the proposed MCMC algorithm differs from existing ones, either theoretically or empirically. It is recommended that Section 4.1 be expanded to include a clearer theoretical analysis and experimental comparison to substantiate the claimed improvement.\n\n2. In Section 5.2, the numerical experiments evaluate the effective sample size (ESS) of the proximal and SVD-Langevin MCMC methods only on synthetic rank-1 data. Conducting additional experiments on non-synthetic benchmark datasets—particularly under varying rank settings—would strengthen the empirical evidence and enhance the overall persuasiveness of the paper.\n\n3. The design purposes of Figures 2 and 3 are unclear and somewhat confusing. In addition, Figure 4 lacks quantitative evaluation metrics. A more detailed analysis of how the parameter  affects the algorithm’s performance, along with a broader set of experiments, would significantly strengthen the credibility and impact of the study."}, "questions": {"value": "1. I would like to see applications of the proposed algorithm in more practical scenarios, such as robust PCA or related real-world low-rank inference tasks. Furthermore, a performance comparison with other representative methods—such as those based on the weighted nuclear norm—would provide a more comprehensive evaluation and highlight the advantages of the proposed approach.\n\n2. How does the computational complexity of the proposed improved MCMC method scale as the matrix dimension increases? Has the convergence rate or computational cost been quantitatively analyzed either theoretically or empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fLT9cNHJHt", "forum": "c8oAL6i2yR", "replyto": "c8oAL6i2yR", "signatures": ["ICLR.cc/2026/Conference/Submission13663/Reviewer_L6sh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13663/Reviewer_L6sh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806662273, "cdate": 1761806662273, "tmdate": 1762924231784, "mdate": 1762924231784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the underlying probability distribution associated with the nuclear norm, a widely used regularizer in low-rank matrix learning. Using tools from differential geometry, the authors analyze its key properties, including the normalization constant, the distribution of the nuclear norm itself, and the joint distribution of its singular values, while also revealing its close relationship with the Normal Product (NP) distribution.\nBased on these theoretical results, the paper designs an improved MCMC algorithms.Even when tuning the parameters is difficult or impossible, it can adaptively adjust the hyperparameter λ, leading to better performance in Bayesian low-rank matrix denoising and completion experiments."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This article fills a theoretical gap. Although the nuclear norm is extensively used in optimization, its probabilistic characterization has rarely been studied in depth. The paper provides a clear and valuable theoretical treatment of this topic.\n\n2. The proposed Normal Product (NP) approximation and corresponding sampling procedures bridge theory and practice. The adaptive λ estimation process is well motivated and empirically validated through experiments on matrix completion and denoising tasks.\n\nOverall, this paper presents reliable and meaningful theoretical results, and the experimental section also demonstrates the benefits of this theoretical analysis, such as the estimation of lambda."}, "weaknesses": {"value": "The authors mention that several researchers in variational Bayesian inference have previously noted the close connection between the “Normal Product” distribution and the nuclear-norm distribution. It would strengthen the contribution if the paper could clarify the extent of prior work—both theoretical and applied—by these authors, to better situate the novelty of this study within the existing literature."}, "questions": {"value": "1. Bayesian estimation of the regularization parameter λ is a good subject of study, but it remains unclear how the proposed samplers or adaptive λ estimation scale to high-dimensional matrices. Providing computational complexity indicators, runtime, or empirical timing results would make the work’s practical relevance more convincing.\n\n2. Regarding the approximation of NP to NND, what are the specific assumptions behind this approximation (e.g., the singular value condition, etc.)? Although some experimental results later in the paper show that the two are in agreement, theoretical error analysis or limits (even approximate ones or some discussion) will help to clarify the accuracy and applicability of this approximation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0D9AQUqacH", "forum": "c8oAL6i2yR", "replyto": "c8oAL6i2yR", "signatures": ["ICLR.cc/2026/Conference/Submission13663/Reviewer_NPQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13663/Reviewer_NPQk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762412066216, "cdate": 1762412066216, "tmdate": 1762924231368, "mdate": 1762924231368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main result of the paper is the exact computation of the normalizing constant for the nuclear norm distribution. The knowledge of this normalizing constant makes it possible to refine and improve a number of sampling algorithms (prior or posterior based)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper definitely provides an answer to an interesting question."}, "weaknesses": {"value": "The difficulty I have has to do with the how dense it is. I guess it coud be better organized to fit into the ICLR size limit. But at this stage I feel like the authors just rapidly extracted part of their important work to fit into the first 13 pages and put everything else into the appendix. The paper is good but it should be rewritten around a main clear narrative. In particular I would suggest making a clean introduction to the sampling schemes (with perhaps a quick description along with the definition of the distribution so that it is clear your result is applicable in practice). Then explain when you replace the use of the NND by the NPD. Because in the current version of the paper this is not very clear. You introduce then NPD as a simplification of the NND but when you introduce the sampling algorithms, you go back to the NND and seem to sample directly from the distribution given by Proposition 3.4. \n\nSection 4.1. which should be central to the paper as it deals with the practical use of the nuclear norm distribution appears sketchy and some of the Figures (I think of Figure 6 in particular are ambiguous). To be fair the authors should also be more straightforward regarding the fact that sampling can be done without knowing the normalizing constant of the distribution (this only appears in the numerical experiments if I’m not wrong the authors indicate that some earlier work by Pereyra conduct simulations from the NND without the need for this constant). You also really need to better format your references. Some of them appear in the middle of the text. \n\n====================================================================================\n\n\n- Proposition 3.2. I would detail a bit more. In particular, I guess the \\lambda^{-nm} comes from the determinant of the Jacobian. I would add a word\n- Proposition 3.3. I would recall the definition of the surface measure. Also to use the surface measure, don’t you need some regularity assumptions to define the surface measure. How do you know that the set {X|\\|X\\|_*\\leq 1} is sufficiently regular?\n- In proposition 3.4. If X = USV^T and U, V and S are defined as in Proposition 3.4. How do you ensure the dimensions match? To me, unless there is something I don’t see, U is of size n \\times n, V is of size m\\times but then S I assume is of size n\\times n ? In this case, \n- I would recall the definition of the Stiefel manifold\n- In the proof of Proposition 3.4. you mention the book of Zeitouni, Anderson and Guionnet. First you should correct the reference. Anderson and Zeitouni are two different researchers. Second, when you cite the book, it does not make sense to refer to Anderson alone (there should at least be an “et al.“) but this might be related to the way you encoded the reference. Finally [[Check Proposition 4.1.3.]]\n- lines 180-181, I don’t understand why you say that it is undefined on the set of measure 0 corresponding to repeated singular values. If I look at the statement of Proposition 3.4. I understand that the probability of having a singular value with multiplicity > 1 is zero (although I don’t really understand why) but I don’t see why the pdf would be undefined on matrices with repeated singular values. \n- line 171, what do you mean by “an order constraint” on the singular values ? you mean you assume the singular values are sorted in your notations?\n- In Figure 1, the motivation for the display of the Gamma(7,1) diagram is unclear\n- line 189, Theorem 3.4. —> Proposition 3.4. ?\n- The whole paragraph 182-189 should be rewritten. I would for example remove the comment on the Pereyra paper. This paper seems to be on a completely different topic\n- line 226 I would remove the sentence “This is well known Gaunt (2022)” by the simpler “X_1X_2 has the following density (see for example Gaunt 2022)”\n- Can you provide a reference for lines 230-234?\n- The equality in (11) (i.e. rightmost) is not clear to me. I understand you get something that is proportional to $\\frac{1}{\\sqrt{z}}e^{-Cz}$ but why do you get rid of the constants (in particular the one in the exponential) ?\n- “of the modified Bessel function of second kind Johnsson & Kotz..” —> “of the modified Bessel function of second kind (see e.g. Johnsson & Kotz)”?\n- line 261, is that the function on line 1039 ? Then why not include it in the statement of the Theorem?\n- Line 264-265 : “it is necessary to restrict the dimensionality” —> What do you mean? I would either remove or give a clearer explanation. Lines 267 to 272 are interesting as they motivate the 4/3 factor but you need to better explain where the 3/4 comes from.\n- Sometimes you use NP sometimes you use NPD for the nuclear product (distribution) prior. I recommend homogenizing.  \n\nSection 4.1\n\nGenerally speaking, there is too much information in section 4.1. and 4.2\n\n- line 292 “relies on the smoothness of the posterior, structure unavailable ..” --> the sentence does not make sense to me.\n- line 295 : “and then proposing..” —> do you mean “sampling” ?\n- In section 4.1. you use many lines to expand on how how some sampling scheme can or cannot be used on the nuclear norm distribution. Why not just keep the exposition simple and focus on the sampling scheme that are best. Then give the steps associated to those clearly. You should clearly recall what you mean by posterior and prior sampling. Do you mean (from what I understand from line 309-310) that you have Y = F(X) and you are interested in P(X|Y) \\propto p(Y|X)p(X) where p(X) \\sim NND? I think you should clarify that somewhere. From the same lines, sampling from the prior is then X \\sim NND(\\lambda)\n\nSection 4.2.\n\n- In the statement of Theorem 4.1. I guess you mean Y|X  \\sim N(X, \\gamma^2 I) ?\n- What is the point of taking gamma random ? I think you should specify P_{\\gamma^2} in section 4.2. \n- line 322 : “We now consider how a Gibbs sampler can be constructed for this posterior” —> which posterior ? P(X, \\gamma^2|Y, \\lambda) ? I would then clearly say “from the posterior P(X, \\gamma^2|Y,..)”\n- There is a mistake in the second appearance of the posterior P(X, \\gamm^2|Y) in the statement of Theorem 4.1. If I’m not mistaken, it should be P(X, \\gamma^2|Y, \\lambda) and not P(X, \\gamma^2|X, \\lambda) \n- I don’t understand how you go from the distributions in Theorem 3.4. to those on lines 324 - 326"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8xHwnWdeXv", "forum": "c8oAL6i2yR", "replyto": "c8oAL6i2yR", "signatures": ["ICLR.cc/2026/Conference/Submission13663/Reviewer_qvqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13663/Reviewer_qvqW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762463241153, "cdate": 1762463241153, "tmdate": 1762924231026, "mdate": 1762924231026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}