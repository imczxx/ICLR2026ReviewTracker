{"id": "X74KnsoYEM", "number": 15210, "cdate": 1758249006986, "mdate": 1759897321110, "content": {"title": "Label Smoothing Improves Machine Unlearning", "abstract": "The objective of machine unlearning (MU) is to eliminate previously learned data from a model. However, it can be challenging to strike a balance between computation cost and performance when using existing MU techniques. Taking inspiration from the influence of label smoothing on model confidence and differential privacy, we propose a simple gradient-based MU approach that uses an inverse process of label smoothing. This work introduces UGradSL, a simple, plug-and-play MU approach that uses smoothed labels. We provide theoretical analyses demonstrating why properly introducing label smoothing improves MU performance. We conducted extensive experiments on several datasets of various sizes and different modalities, demonstrating the effectiveness and robustness of our proposed method. UGradSL also shows close connection to improve the local differential privacy. The consistent improvement in MU performance is only at a marginal cost of additional computations. For instance, UGradSL improves over the gradient ascent MU baseline constantly on different unlearning tasks without sacrificing unlearning efficiency. A self-adaptive UGradSL is also given for simple parameter selection.", "tldr": "", "keywords": ["Machine unlearning", "Label smoothing", "Differential privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e9eefd284fb235c5f1e323356dd44e9b1438e30.pdf", "supplementary_material": "/attachment/fe2aac2b3bc1ad96b20094a2aa37cc7e310779b7.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a gradient-based machine unlearning (MU) method, the UGradSL, that integrates label smoothing (LS) into the unlearning in the ways of fine-tuning and gradient accent.  \nThe key idea is to perform gradient ascent with generalized label smoothing (GLS) on the forgetting set $D_f$, and gradient descent on the retaining set $D_r$, forming a mixed-gradient optimization.  \nThe authors show a theoretical condition under which NLS improves gradient ascent.\nExperiments across multiple datasets and model architectures (ResNet18, ViT, BERT) show improved unlearning–retaining trade-offs with small computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Easily pluggable into existing GA/FT unlearning pipelines without retraining.\n2. Broad Empirical Evaluation. Tested on six datasets and multiple unlearning types (class, random, group), consistently outperforming baselines.\n3. Provided theoretical intuitions. Explained why NLS can guide ascent toward the equivalent retrained solution."}, "weaknesses": {"value": "1. The condition of the inner product of $\\Delta\\theta$'s in Theorem 2 is critical but only verified empirically on one dataset (CelebA, Figure 4 in the Appendix). This lacks theoretical justification and broader empirical validation.\n\n2. Theorem 3's connection to LDP is interesting but the practical meaning is unclear - it provides label-level privacy for the forgetting set, but doesn't directly translate to model-level unlearning guarantees.\n\n3. Hyperparameter sensitivity needs ablation studies. A demonstration about the smoothing rate $\\alpha$'s influence on performance is expected. While an adaptive version is proposed, the distance threshold $\\beta$ introduces another parameter."}, "questions": {"value": "Besides the weaknesses:\n1. In Algorithm 1, the distance computation $d(z^r_i, z^f_i)$ is not clearly defined. What distance metric? In which space (feature/parameter)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RvhEcVngQb", "forum": "X74KnsoYEM", "replyto": "X74KnsoYEM", "signatures": ["ICLR.cc/2026/Conference/Submission15210/Reviewer_Zyp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15210/Reviewer_Zyp9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451951653, "cdate": 1761451951653, "tmdate": 1762925513008, "mdate": 1762925513008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper integrates label smoothing into the unlearning loss to reduce time cost and mitigate drops in remaining and test accuracies. It proves that gradient ascent can achieve exact unlearning, and generalized label smoothing tightens errors. Building on this, it introduces UGradSL and UGradSL+, which iterate over retained and forgetting datasets, and show gains on class, random, and sub-class unlearning versus baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Label smoothing combined with a gradient-based method is interesting for handling unlearning\n\n2. Both theoretical and experimental evidence support the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The presentation of the technical sections is not clear, for example, why there exists an $\\approx$ symbol in the condition of Theorem 1 and why $\\epsilon$ in the conclusion of $\\epsilon$-Label-LDP relies on the weights $\\gamma_1$ and $\\gamma_2$.\n\n2. The proposed method requires calculating the distance with samples in the minibatch, which will lead to a large computation cost.\n\n3. The proposed UGradSL does not work better in performance, while UGradSL+ requires a longer time than other baselines.\n\n4. This paper does not contain essential abolition studies for the GA ratio and the optional smoothing ratio"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XfLvbGto5Q", "forum": "X74KnsoYEM", "replyto": "X74KnsoYEM", "signatures": ["ICLR.cc/2026/Conference/Submission15210/Reviewer_Xe6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15210/Reviewer_Xe6w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654721067, "cdate": 1761654721067, "tmdate": 1762925512476, "mdate": 1762925512476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper concentrates on the low-computation and high-efficiency machine unlearning in the field of computer vision. The author proposed a novel method UGradSL and UGradSL+ (finetune based) combine the GA (gradient ascent) with the NLS (negative label smoothing) and apply mix-gradient strategy which perform GD (gradient descent) on the retain data and GA with NLS on the forget data. Moreover, the stronger variant self-adaptive UGradSL supports the automatic selection of the smooth rate. The paper provides the theoretical analysis and abundant experiments to prove the effectiveness and robustness of method. In addition, the study also explored the label smoothing and local differential privacy (LDP)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ This paper is clear, logical and easy to understand. The tables and figure are clear and detailed, with good instructions.\n+ The proposed method is a simple and plug-and-play tool, which can directly integrate into the existing gradient-based unlearning methods (such as GA and finetune) and improve their performance.\n+ This paper provides the mathematical and theory proof to explain the limitation of existing GA methods and the effectiveness of NLS in specific circumstances. Moreover, the authors innovatively establish the connection between label smoothing and LDP."}, "weaknesses": {"value": "- Although the baseline methods are representative, the experiments lack comparison with the latest schemes between 2024 and 2025.\n- Some symbols and formulas lack precise definitions and explanations or exist clerical, such as “distance d()” in the Algorithm 1 where different distance calculation methods can lead to the difference between computational overhead and performance.\n- The performance of the method is sensitive to the hyperparameters settings (such as p and α in Eq.8), which may result in the risk of overfitting to the retain classes.\n- Since the time cost is only a part of the computational overhead, the analysis of computational resource overhead is incomplete. And the automatic parameter selection in self-adaptive version also may lead to the additional time cost and computational complexity."}, "questions": {"value": "* Could the authors do more comparative experiment with new methods, such as [1],[2] and etc.\n* The authors should detailly check the symbols in the article and conduct standardized descriptions and definitions.\n* The authors should analyze and explain the generalization gap between the RA and TA, where the increase of RA and decrease of TA reflect the overfitting and memory to the retain data rather than learning the generalization features.\n* Since the efficiency is the core advantage of the proposed method, the authors should compare the memory cost during the unlearning process (such as peak GPU memory usage and etc.), which may help other researchers make decision with limited resources.\n* Could the authors do another ablation experiment to the distance computation and automatic parameter selection in Algorithm 1, which helps the observation of whether the improvement of self-adaptive UGradSL comes at the cost of disproportionate calculation time.\n\nReference:\n[1] Certified Unlearning for Neural Networks\n[2] Towards Efficient Machine Unlearning with Data Augmentation: Guided Loss-Increasing (GLI) to Prevent the Catastrophic Model Utility Drop"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JTwgOXPemv", "forum": "X74KnsoYEM", "replyto": "X74KnsoYEM", "signatures": ["ICLR.cc/2026/Conference/Submission15210/Reviewer_dpJc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15210/Reviewer_dpJc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899694833, "cdate": 1761899694833, "tmdate": 1762925511975, "mdate": 1762925511975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UGradSL, which is a gradient-based unlearning methods that integrate label smoothing. The approach performs gradient ascent on forget data and gradient descent on retain data, using smoothed labels to balance forgetting and retaining.\nTheoretical analyses explain when NLS improves gradient ascent and link the method to label-local differential privacy (Label-LDP) guarantees. Experiments on several datasets demonstrate the effectiveness of the proposed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has clear problem framing and the motivation for introducing label smoothing to stablilize gradient ascent is sound.\n2. The theoretical analysis part is clear and provides useful insights.\n3. The proposed UGradSL is simple yet effective.\n4. The experiments cover class-level, random, and group unlearning and sufficient datasets and baselines."}, "weaknesses": {"value": "1. The forget-set size: The forget-set size appears fixed for each experiment. Varying forget-set sizes is important for assessing the effectiveness of the method. This would reveal whether the proposed method scales well when more data need to be forgotten.\n2. Lack of ablation study/sensitivity analysis: The paper lacks a discussion on the contribution of each term in the mixed gradient objective. For example, results for different p should be provided, since it is an important factor used to balance GD and GA.\n3. Smoothing rate sensitivity: the effect of the label smoothing rate should be analyzed, and a discussion on how much the observed benefits arise from smoothing is worth adding in."}, "questions": {"value": "1. About forget-set size: have the authors evaluated the method under different forget-set sizes or compositions? Since forget-set size can significantly affect both unlearning and retention, understanding how the method scales with it would help assess its general applicability.\n2. In Eq (8), p controls the balance between gradient ascent (forgetting) and gradient descent (retaining). Could the authors provide experimental results for different p values? This would clarify how sensitive the method is to this balance.\n3. How was the smoothing rate chosen? It would be valuable to include sensitivity results or justification for the chosen range.\n4. How the density is computed in UGradSL+ and how it interacts with the theoretical framework?\n5. The paper suggests that UGradSL can be easily integrated into other unlearning frameworks. Have the authors tested this claim empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MhWTIkbMtR", "forum": "X74KnsoYEM", "replyto": "X74KnsoYEM", "signatures": ["ICLR.cc/2026/Conference/Submission15210/Reviewer_be9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15210/Reviewer_be9Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984506802, "cdate": 1761984506802, "tmdate": 1762925511434, "mdate": 1762925511434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}