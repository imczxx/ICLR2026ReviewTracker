{"id": "B2HF0p2ZaT", "number": 14537, "cdate": 1758238379242, "mdate": 1759897363969, "content": {"title": "QRad: Enhancing Radiology Report Generation by Captioning-to-VQA Reframing", "abstract": "Radiology Report Generation using AI has demonstrated significant potential in modern clinical workflows. However, existing approaches have limited clinical utility due to a lack of interactive capabilities and compromised factual reliability because linguistic variations are prevalent in the training data and lead to overfitting. We introduce QRad, a novel approach which reframes radiology report generation from image captioning to a self-directed visual question-answering (VQA) process. Specifically, we convert radiology reports into question-answer pairs and train our model to first generate a chain of questions and then respond with answers. The answers are concatenated to form the radiology report. Our approach offers three advantages: First, quality is considerably improved (by 10.5% in RadGraph-F1) because linguistic variations (such as the omission or ordering of medical topics) is removed from the answer generation's criterion, allowing the model to focus on factual accuracy rather than presentation style. Second, the model provides an intrinsic VQA capability that enables physicians to interact with the model for details that may have been omitted in the initial output. Third, QRad derives confidence scores from token probabilities through its ability to answer template questions about specific medical conditions, a capability unavailable in previous models, enabling Receiver Operating Characteristic (ROC) based evaluation to facilitate regulatory approvals. Experiments show that QRad outperforms state-of-the-art models with only 13% of their size, offering a promising path for clinical adoption and regulatory validation in real-world settings.", "tldr": "", "keywords": ["Radiology Report Generation", "Image Captioning", "Medical Imaging"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c50574f2d78299004130eb1a1d2e1d649dd291ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "They frame radiology report generation as a QA process, where the final report consists of a set of chained answers. For training, QA pairs are generated by splitting the report into a set of answers and letting GPT-4 generate a corresponding question for each answer. The results show modest improvements with the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Formulating free-text report generation as VQA task is novel and targets very relevant problems in report generation, such as ambiguous validation and linguistic ambiguity.  \n- The architecture is simple but efficient and effective.  \n- The formulation allows for an interactive mode additionally to just writing the report.  \n- Ablation study for validating the effectiveness of different components  \n- Clear reporting of all variants of CheXbert score  \n- Extensive comparison to prior work"}, "weaknesses": {"value": "- I am missing an architecture / flow figure showing what is the input, what is generated from that, how does the final output look etc.  \n- The language modeling loss during training relies on teacher forcing, so actually C,A,B is not penalized a lot compared to A,B,C if the sentences describing each of these findings remain correct as only the very beginning of the sentence is penalized. The formulations within A,B,C are penalized the same in Q-Rad given that for the training of answer generation again language modeling loss is used. The motivation section should be adapted to be technically accurate about this topic.  \n- A ROC curve is not the same as a confidence score, the third paragraph in the introduction completely mixes these things up.  \n- The concepts proposed in this paper are related to structured report generation, which should therefore be discussed in the related work section.  \n- RGRG and RaDialog should be discussed as a related works using a two-step approach. RGRG proposes a 2-level approach of first selecting abnormal regions and then specifying the findings, and RaDialog first classifies findings on a high level (using the same labels as the proposed image classification QA) before generating a final report."}, "questions": {"value": "- How can you assure that these two artifacts are actually aligned, so that the scores from the binary answers can be used for judging the quality of the generated report?  \n- Why are many models from Table 2 excluded in Table 1, even if they were evaluated on MIMIC-CXR report generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Jz4EsNE7ug", "forum": "B2HF0p2ZaT", "replyto": "B2HF0p2ZaT", "signatures": ["ICLR.cc/2026/Conference/Submission14537/Reviewer_xLL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14537/Reviewer_xLL4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760965488471, "cdate": 1760965488471, "tmdate": 1762924928915, "mdate": 1762924928915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes QRad, a novel framework that reframes radiology report generation from a traditional image captioning problem into a self-directed visual question answering (VQA) process. The model first generates a chain of questions relevant to the X-ray image (capturing the structure of the report) and then answers each question, concatenating the results into a coherent report. This reformulation mitigates overfitting to linguistic variations, improves factual accuracy, supports interactive querying, and allows for ROC-based evaluation—a step toward clinical validation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Comparing with the tradition method to generate the radiology report, it has much improvement."}, "weaknesses": {"value": "1. The contribution of this work appears limited in scope for the broader research community. It does not provide substantial technical or theoretical advancements in the field of machine learning or deep learning. The study mainly builds upon previous question–answering research [1,2,3,4] and extends it by incorporating image inputs. \n\n2. The paper claims to present \"a novel approach that reframes long text generation\"; however, the generation results appear to be highly dependent on the underlying LLM, which raises concerns about the novelty and contribution of the proposed method.\n\n3. In Table 2, the reported improvement is not significant, which raises questions about the effectiveness and robustness of the proposed approach.\n\n4. Limited Generalization Beyond X-rays Although the paper claims extensibility to other domains, all experiments are confined to chest X-rays (MIMIC-CXR, IU-Xray). There is no evidence that the approach generalizes to CT, MRI, or multi-view imaging, which are common in clinical workflows.\n\n[1] Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts\n\n[2] SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering\n\n[3] PromptCap: Prompt-Guided Task-Aware Image Captioning\n\n[4] All You May Need for VQA are Image Captions"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VhV4zKxRxR", "forum": "B2HF0p2ZaT", "replyto": "B2HF0p2ZaT", "signatures": ["ICLR.cc/2026/Conference/Submission14537/Reviewer_qxPY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14537/Reviewer_qxPY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760993982069, "cdate": 1760993982069, "tmdate": 1762924928445, "mdate": 1762924928445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel method called QRad, which aims to reframe the task of radiology report generation. While traditional approaches typically follow a direct image-to-text path (i.e., image captioning), QRad adopts a two-stage, self-directed Visual Question-Answering (VQA) process.\n\nThis process comprises two core modules:\n\n1. A Question Generation Module ($f_Q$) that generates a chain of clinically relevant questions based on the input radiographic image.\n\n2. An Answer Generation Module ($f_A$) that provides answers to these questions.\n\nThe final radiology report is constructed by sequentially concatenating these answers.\n\nThe authors claim the method offers three major advantages: (1) It improves the report's factual accuracy by reducing overfitting to linguistic style variations.(2) The model has an intrinsic interactive capability, allowing clinicians to ask follow-up questions to obtain more information.(3) It is capable of extracting numerical confidence scores to support Receiver Operating Characteristic (ROC) curve-based evaluation, which is critical for regulatory approval."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper shows strong potential across four key areas: clinical relevance, evaluation, design, and efficiency.\n\n- It tackles the critical clinical need for automated, reliable radiology report generation. This addresses the goal of reducing radiologists' workload and improving diagnostic efficiency, promising a significant clinical impact.\n\n- The paper proposes a novel and practical evaluation methodology. Extracting numerical confidence scores via template-based VQA is a major contribution. This solves the generative models' critical bottleneck—the lack of quantifiable confidence—which is essential for regulatory validation (like ROC analysis for FDA approval). This provides a viable path for deployment beyond typical NLP metrics.\n\n- The design features conceptual elegance. The idea of decomposing the task—separating factual content generation (answers) from report structure planning (questions)—is ingenious. It offers a principled solution to the \"loss-metric mismatch\" problem, allowing the model to focus purely on factual accuracy instead of stylistic variations.\n\n- The model demonstrates impressive efficiency and performance. QRad, with a much smaller size (0.9B parameters), outperforms larger SOTA models (often >7B parameters) on both linguistic and clinical metrics. This high parameter efficiency is highly valuable for real-world applications and deployment."}, "weaknesses": {"value": "Despite its merits, the paper presents several significant deficiencies that impact the evaluation of its contribution, making it difficult to meet the acceptance standards of a top-tier conference.\n\n- **Major Concerns Regarding Novelty and Literature Context:** The paper's claim of core innovation—reframing report generation as a VQA process—is unsubstantiated. Multiple prior works have already explored VQA-based report generation[1] and interactive report generation[2]. Furthermore, the paper's two-stage \"Question-Answer\" process is conceptually similar to the RG-AG pipeline[3]. QRad's failure to acknowledge these precedents presents its contribution as a paradigm shift rather than the more accurate incremental contribution (e.g., from structured VQA to free-text VQA). The authors must clearly define their specific, narrower contribution.\n\n- **Insufficient Methodological Elaboration:** The paper lacks a detailed flowchart to explain the data flow and architectural specifics of the two modules, particularly regarding weight sharing and training strategies when both modules use the same Transformer architecture. Given that the core concept is not entirely new, the method's technical contribution appears to be incremental, with innovation stemming primarily from data preparation rather than the model architecture itself.\n\n- **Lack of Qualitative and Clinical Validation:** A critical omission is the absence of a complete generated report example. The paper only shows answers to individual questions but never the final report constructed by concatenating these answers. This simple concatenation may introduce problems with coherence, repetition, and logical flow, making it impossible to evaluate the report's true clinical utility and readability. Evaluating component-level accuracy is an insufficient substitute for assessing the quality of the final, integrated product (i.e., the complete report).\n\n[1] Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting\n\n[2] RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance\n\n[3] Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports"}, "questions": {"value": "Based on the analysis above, the authors are requested to clarify the following points:\n\nThe paper’s core innovation claim is reframing report generation as a VQA process. However, prior work such as Rad-ReStruct [1] already modeled structured reporting as a hierarchical VQA task. Given this, could the authors explicitly articulate the novelty of their contribution relative to these existing VQA-based reporting frameworks and specifically delineate how their approach differs?\n\nThe paper presents interactivity as a key advantage. The RaDialog[2] is an end-to-end conversational VLM specifically designed for interactive report generation and correction. How does QRad’s emergent interactivity compare to the specifically engineered dialogue capabilities of models like RaDialog? What are the relative pros and cons of the two methods?\n\nThe two-stage, Chain-of-Thought-inspired decomposition is central to the method. The RG-AG pipeline [3] also proposes a similar two-stage decomposition, but uses a generated draft report as the intermediate context, rather than questions. Could the authors comment on this alternative and justify the choice of using \"questions\" as the intermediate \"thought\" artifact, as opposed to other potential textual representations (such as a draft report)?\n\nThe methodological description would be significantly improved by a more detailed architectural diagram. Could the authors provide a flowchart that clearly details the precise data flow, including tensor dimensions, the interaction between the shared visual encoder and the two separate decoder modules, and any attention masking mechanisms used for efficient training?\n\nA critical piece of evidence is missing: the final, end-to-end generated report. Could the authors provide several examples of complete reports (stitched together from the answers), covering both normal and abnormal cases, juxtaposed with ground-truth reports, to allow for the assessment of their narrative coherence, readability, and overall clinical soundness?\n\n[1] Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting\n\n[2] RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance\n\n[3] Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PbR4oCvK0d", "forum": "B2HF0p2ZaT", "replyto": "B2HF0p2ZaT", "signatures": ["ICLR.cc/2026/Conference/Submission14537/Reviewer_E7Na"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14537/Reviewer_E7Na"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497184640, "cdate": 1761497184640, "tmdate": 1762924927928, "mdate": 1762924927928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces QRad, which reframes radiology report generation from direct image captioning to a two-stage, self-directed VQA pipeline: (1) a Question Generator produces a chain of clinically relevant questions conditioned on the image; (2) an Answer Generator outputs sentence-level answers that are concatenated into the final report. The authors also introduce template yes/no queries for predefined findings to derive per-class probabilities from token logits, enabling ROC/AUC evaluation. Using an MI2-based vision encoder with a small text decoder (~0.9B params; 4B variant also shown), QRad achieves strong gains on clinical metrics (CheXbert, RadGraph) on MIMIC-CXR (and ReXrank results), often matching or surpassing larger (≥7B) models, while adding interactivity and probability outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Problem reformulation with clear clinical motivation: isolates stylistic variability (omission/order) into question planning, focusing supervision on factual content in answers.\n* Clinically useful features: interactive VQA for follow-up questions; per-finding probabilities for ROC/AUC.\nParameter efficiency: competitive results with ~13% of typical 7B models.\n* Thorough evaluation: multiple metrics (lexical + clinical), ablations (captioning→VQA, classification-QA mix), bootstrap CIs, and ReXrank results.\n* Practical data recipe: report→QA conversion plus closed-vocab classification QA improves supervision quality."}, "weaknesses": {"value": "* Synthetic supervision dependency: heavy reliance on GPT-4-based report→QA parsing may introduce biases; robustness to prompt/model changes is not quantified.\n* Probability quality: using [yes]/[no] token logits as “probabilities” lacks calibration analysis (ECE/Brier), threshold stability, and domain-shift robustness; regulatory claims feel premature without prospective validation.\n* Coverage failures: end-to-end safety depends on the Question Generator not missing clinically important topics; failure modes and safeguards are underexplored.\n* Scope: primarily frontal CXR on public datasets; generalization to multi-view/portable/other modalities remains open.\n* Comparability: some baselines (e.g., multi-image MAIRA-2) are not strictly comparable; more detail on data overlap/leakage controls would help."}, "questions": {"value": "* Calibration: Do you apply post-hoc calibration (e.g., temperature scaling) to yes/no logits? Please report ECE/Brier and calibration under class imbalance.\n* Coverage auditing: What fraction of ground-truth findings are never queried by the Question Generator? Any human audit or auto-check using RadGraph/CheXbert to detect omissions?\n* Robustness to QA parsing: Sensitivity to different LLMs/prompts/rule-based splitters for report→QA? Can you release multiple converted variants to quantify variance?\n* Interactive reconciliation: If follow-up VQA contradicts initial answers, how is the final report reconciled (priority rules, uncertainty tagging, versioning)?\n* Domain shift: Any results for portable CXR, ICU cohorts, or non-English reports?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iPaajSVOy3", "forum": "B2HF0p2ZaT", "replyto": "B2HF0p2ZaT", "signatures": ["ICLR.cc/2026/Conference/Submission14537/Reviewer_fVMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14537/Reviewer_fVMS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762284640175, "cdate": 1762284640175, "tmdate": 1762924927492, "mdate": 1762924927492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}