{"id": "iaoAKDRAJQ", "number": 22735, "cdate": 1758334871757, "mdate": 1763701454527, "content": {"title": "A Tale of Two Smoothness Notions: Adaptive Optimizers and Non-Euclidean Descent", "abstract": "Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction in their analyses, however, lies in the smoothness assumptions they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum, a guarantee unattainable under standard smoothness. We further develop an analogous comparison for stochastic optimization by introducing adaptive variance, which parallels adaptive smoothness and leads to qualitatively stronger guarantees than the standard variance.", "tldr": "", "keywords": ["adaptive optimizer", "steepest descent", "loss geometry", "convergence rate"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5658424cd47e8c8867dc6c75bba6f9a3ac44b7b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper relaxes the convexity assumption in the analysis of adaptive optimizers. It also introduces new assumptions in the analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main contribution is relaxing convexity assumption accompanied by the technical results.\n\nOther contributions consider different assumptions in the analysis.\nFor acceleration the authors use the recently introduced smoothness in Kovalev’s analysis, but convexity is here. Despite the assumption used in other papers I do not consider its incorporation as a significant contribution – Kovalev’s assumptions are standard and more widely used in contrast to the considered assumption which was introduced very recently. But still it enables new rates."}, "weaknesses": {"value": "- New variance assumption improves dimension dependence. But no surprise since the assumption is different. I mean the improvements are not because of improved analysis or new techniques. The discussion is given in Lines 450-454 and states that the dependence is unavoidable in the worst case. \n\nDetailed discussion is needed. How does the adaptive variance assumption relate to the standard one? Does it hold in practice?\n\n- Some theorems do not properly refer to the assumptions used.\n\n- Line 465: 1/T^-3.5. \\varepsilon?"}, "questions": {"value": "Line 053: the adaptive smoothness (cf. Definition 2.2), introduced by Xie et al. (2025b) \nit not called adaptive smoothness in the origin (at least I could not find). It is confusing. Usually the algorithms are adaptive to smoothness. Please, clarify the naming. It is assumption, does it adapt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lgvZR5wJUr", "forum": "iaoAKDRAJQ", "replyto": "iaoAKDRAJQ", "signatures": ["ICLR.cc/2026/Conference/Submission22735/Reviewer_aLQH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22735/Reviewer_aLQH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727550823, "cdate": 1761727550823, "tmdate": 1762942363356, "mdate": 1762942363356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper compares standard non-Euclidean smoothness (governing NSD/Lion/Muon-style methods) with a stronger adaptive smoothness that underpins Adam/Adagrad/Shampoo-type optimizers. It extends adaptive smoothness to the nonconvex setting (unified analysis for well-structured preconditioners), proves accelerated $O(\\tilde{T}^{-2})$ convex rates for adaptive methods (unattainable under standard $\\ell_{\\infty}$ smoothness), and introduces adaptive variance that yields dimension-free NSD rates in stochastic nonconvex optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Two genuinely different smoothness/variance regimes with algorithmic consequences. The paper formalizes why “adaptive optimizers is approximately NSD with a norm” is not the full story: adaptive methods converge under adaptive smoothness $\\Lambda_{\\mathcal H}(f)$, which is always $\\ge$ the standard smoothness used by NSD (Prop. 2.3). Under adaptive smoothness, accelerated $O(\\tilde T^{-2})$ convergence with Nesterov momentum is achieved (Thm. 4.4), while standard $\\ell_\\infty$-smoothness faces a $\\Omega(T^{-1}/\\log T)$ barrier (via prior lower bounds cited by the authors). The analogous adaptive variance notion similarly strengthens stochastic guarantees (Def. 4.1), yielding dimension-free NSD rates (Thm. 4.6) that are provably unattainable under only $\\|\\cdot\\|_{\\mathcal H,\\ast}$-variance (Thm. 4.7). This idea is interesting and well substantiated by aligned theorems.\n- Unified nonconvex analysis for adaptive preconditioners beyond diagonal cases. Theorem 3.2 (and its corollaries) covers weighted/EMA/cumulative updates over well-structured preconditioner sets $\\mathcal H$ (matrix subalgebras), extending nonconvex analysis beyond diagonal preconditioners that prior works largely required. The matrix inequality (Lemma 3.4) controls  via  $\\mathrm{Tr}(H)\\,\\|S_T\\|_{\\mathrm{op}}$ and explicates where extra $\\log d$ factors enter for non-commutative $\\mathcal H$. This step makes unified treatment possible and should be of independent interest.\n-  Results are parameterized in terms of the geometry $\\mathcal H$ (e.g., diagonal vs. general PSD cones), update style (weighted/EMA/cumulative), and budgeted noise control (minimizing $\\mathrm{Tr}(P_{\\mathcal H}(\\Sigma))$ or $\\sigma_{\\mathcal H}(f)$). The two-phase behavior (deterministic $\\tilde O((\\Delta_0\\Lambda_{\\mathcal H}/T)^{1/2})$ vs. stochastic $\\tilde O(T^{-1/4})$) and explicit \\emph{log-factors} can be useful for practitioners anticipate when acceleration/normalization helps and when geometry-induced constants dominate."}, "weaknesses": {"value": "- The main guarantees hinge on adaptive quantities—$\\Lambda_{\\mathcal H}(f)$ (minimizing trace bounds over a preconditioner set) and $\\sigma_{\\mathcal H}(f)$ (sup over $x,t$ with minimization over $H\\in\\mathcal H$). These are hard to estimate or upper-bound in realistic deep-learning settings. The paper discusses relationships to bounded covariance (e.g., via $\\mathrm{Tr}(P_{\\mathcal H}(\\Sigma))$) but stops short of estimators/diagnostics practitioners could compute to check assumptions or guide geometry choice. Without empirical proxies (e.g., online estimates of $P_{\\mathcal H}(\\widehat\\Sigma)$, curvature sketches, or trace surrogates), it’s difficult to translate the theory into optimizer selection.\n- While Theorem 4.4 achieves an appealing $O(\\tilde T^{-2})$ term, the bound carries \\emph{$\\log d$ penalties} and depends on a radius $D=\\max_t\\|x_t-x^\\ast\\|_{\\mathcal H}$ (mitigated by a projected variant). The paper provides a projection-based algorithm with the same rate, but the \\emph{constants and projection effects} (e.g., clipping bias, implementation overhead) are not explored. Similarly, non-commutative $\\mathcal H$ introduces unavoidable $\\log d$ blow up via Lemma 3.4; it would help to quantify when these factors remain small (e.g., block-diagonal or Kronecker-structured $\\mathcal H$) or to show empirical results indicating the log-factors are not rate-limiting in practice."}, "questions": {"value": "-  The paper compares to NSD’s standard-smoothness rates and highlights separations, but the discussion of near-optimality (w.r.t. $d$, $\\tilde T$, and geometry) is mostly qualitative. For instance, are the nonconvex \\emph{$\\tilde O(T^{-1/4})$} stochastic adaptive rates minimax-optimal under the proposed adaptive variance? Are the $\\log d$ factors necessary for broad non-commutative $\\mathcal H$, or artifacts of proof technique?\n- Could you specify practical projection oracles for common H (diagonal, Kronecker, spectral)? \n- Can you propose online, low-cost estimators (e.g., streaming sketches of gradient covariances projected onto H or trace estimators for $P_H(\\Sigma)$? to upper-bound $\\Lambda_H(f)$ and $\\sigma_H(f)$ during training? A brief “estimation recipe” would make the theory far more deployable and allow users to select H adaptively."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "N33Gs3Hmlx", "forum": "iaoAKDRAJQ", "replyto": "iaoAKDRAJQ", "signatures": ["ICLR.cc/2026/Conference/Submission22735/Reviewer_MwWV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22735/Reviewer_MwWV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943768213, "cdate": 1761943768213, "tmdate": 1762942362967, "mdate": 1762942362967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes a class of adaptive preconditioning algorithms for convex and non-convex, stochastic optimization under an assumption on the objective function called adaptive smoothness. The non-convex section essentially extends the analysis of Xie at el (2025a) from the convex to the non-convex setting (Theorems 3.2 and 3.3), which requires a lot of technical work for the case of non-diagonal preconditioners. The convex section extends the accelerated $1/T^2$ result of Kovalev (2025a) from diagonal preconditioners to a more general class of preconditioners (Theorem 4.4), analyzes normalized steepest descent (NSD) under an adaptive noise assumption (Theorem 4.6), and provides a lower bound of sign descent (i.e. NSD w.r.t. $\\ell_\\infty) with a standard, non-adaptive noise assumption."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and contributes to an important problem, namely understanding the improved performance of preconditioning based optimizers.\n2. There are a lot of theoretical results, and from what I can tell, these results are not simple repetitions of known ideas. In particular, the results in the non-convex setting rely on matrix inequalities that involve a good bit of technical work. I checked the proofs of these matrix inequalities (Lemma 3.4 and all lemmas it relies on), and they appear correct.\n3. Several of the results demonstrate a clear separation between regimes: the accelerated $1/T^2$ rate under adaptive smoothness (Theorem 4.4) improves on the known $1/T$ lower bound under $\\ell_\\infty$ smoothness. Also, the dimension-free rate of NSD under adaptive noise (Theorem 4.6) improves on the dimension-dependent lower bound under non-adaptive noise (Theorem 4.7)."}, "weaknesses": {"value": "1. There are several examples of exaggerated or inaccurate language related to the contribution of this work. I feel that the paper would be stronger if the writing was more direct, transparent, and objective about the contributions and the relationship to previous work. Below are some examples.\n\n    1a. The paragraph on lines 64-71 seems to misrepresent the authors' contribution. The paragraph states that the contribution is to show the accelerated $1/T^2$ rate for preconditioned algorithms, but really the contribution is to extend the results of Kovalev (2025a) from diagonal pre-conditioners to a more general class. I think Kovalev (2025a) should definitely be cited here, and it should be clarified that the current paper extends the proof techniques of Kovalev (2025a).\n\n    1b. Definition 2.2 should cite Xie et al (2025b). Under the current presentation, it is easy for the reader to mistakenly believe that the adaptive smoothness from Definition 2.2 is a novel concept introduced by the authors, and indeed I got this impression after reading the introduction and first few pages. I recommend that the authors clearly point out when they are building on previous work. This is especially important during the high-level discussion about the power of different smoothness notions: it is easy for the reader to mistakenly get the idea that all of these high-level concepts were introduced by the authors, when really (in my opinion) the main contribution of this work is technical (e.g. matrix inequalities to handle non-commutative preconditioner classes), not conceptual.\n\n    1c. Lines 83-86 state that \"our results demonstrate that adaptive smoothness and adaptive variance are not simply stronger versions of their standard counterparts but profoundly different conditions that unlock qualitatively stronger guarantees\". How is $1/T^2$ qualitatively stronger than $1/T$ and not just quantitatively stronger? To me, this kind of language comes off as over-selling the work, when really the technical contributions speak for themselves. I encourage the authors to spend less time in the draft discussing the \"profound\" implications of their work and more time on direct, transparent discussion.\n\n    1d. It is stated that Lemma 3.4 may be of independent interest. Given that Lemma 3.4 is a bound on terms specific to the algorithm considered in this paper, I don't see how this result could possibly be of interest outside the context of this algorithm.\n\n2. Assumption 3.1 seems to require almost surely bounded noise, which is stronger than previous work. Is this a typo in the statement of Assumption 3.1, or does the proof actually require almost surely bounded noise? If it is the latter, then there needs to be some discussion of this fact and point out that stronger results are only achieved under stronger assumptions. I would also appreciate if the authors discuss why this stronger assumption is necessary in the proof."}, "questions": {"value": "Questions:\n1. Do the results in stochastic setting actually require almost surely bounded noise, as in Assumption 3.1? (see Weakness #2).\n\nSmall suggestions:\n- It should be added as a condition to Lemma E.2 that $X$ and $A$ are diagonalizable, since the proof uses an eigendecomposition for both of these matrices. Similar conditions apply in Lemma E.1 for $X$ and $X-Y$. This does not affect the proof, since the analysis only applies these lemmas for symmetric matrices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZgBpWBh9RG", "forum": "iaoAKDRAJQ", "replyto": "iaoAKDRAJQ", "signatures": ["ICLR.cc/2026/Conference/Submission22735/Reviewer_E5SU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22735/Reviewer_E5SU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762212016001, "cdate": 1762212016001, "tmdate": 1762942362754, "mdate": 1762942362754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a rigorous theoretical separation between adaptive optimizers (like Adam) and normalized steepest descent (NSD) (like SignGD). The authors' central thesis is that these two algorithm families utilize different assumptions, even when operating in the same non-Euclidean geometry. The paper shows that NSD's convergence is governed by standard smoothness ($L\\_{||\\cdot||\\_{\\mathcal{H}}}(f)$), while adaptive optimizers are governed by a provably stronger (more restrictive) \"adaptive smoothness\" ($\\Lambda\\_{\\mathcal{H}}(f)$). The authors justify this distinction by showing that this stronger assumption is what allows adaptive optimizers to achieve Nesterov acceleration, a feat that is provably impossible under the standard smoothness assumption alone. This \"tale of two notions\" is extended to the stochastic setting with the introduction of \"adaptive variance\" (a more precise, structure-aware noise measure) and is complemented by a new unified convergence proof for adaptive methods in the non-convex setting."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- By identifying the two distinct smoothness notions and linking the stronger \"adaptive smoothness\" to the ability to achieve acceleration (which is provably impossible for the weaker, standard smoothness), the paper provides a clear separation.\n- The authors' parallel \"adaptive variance\" argument is well-supported, as they provide not just a pessimistic, dimension-dependent upper bound for standard variance (Prop D.4) but also a corresponding lower bound (Thm 4.7) to prove that this $d$-dependence is unavoidable.\n- Beyond its main thesis, the paper provides a strong technical contribution by delivering the first unified non-convex convergence analysis for adaptive optimizers using general well-structured preconditioner sets."}, "weaknesses": {"value": "- I am not fully clear about the distinction between adaptive methods and NSD, as the author claims they exploit different notions of smoothness. For instance, under the adaptive smoothness assumption, NSD could potentially also achieve the same rate as in Theorem 3.3, and when combined with the acceleration technique in Eq. (4), could possibly also attain an $O(1/T^2)$ rate for convex functions. Conversely, adaptive methods could also match the rate of NSD under $L\\_{\\\\|\\cdot\\\\|\\_{\\mathcal{H}}}(f)$ smoothness.\n- Regarding the theoretical results, Assumption 3.1 appears stronger than the standard boundedness-in-expectation condition. It requires that the noise be bounded for every realization of randomness. Additionally, Theorem 3.2 does not appear to guarantee convergence; could the authors clarify why this is the case?\n- Finally, the cited lower bound for non-acceleration applies to $L\\_{\\\\|\\cdot\\\\|\\_{\\infty}}(f)$. It remains unclear whether this separation holds for a general $\\mathcal{H}$."}, "questions": {"value": "Please refer to the weaknesses section.\n\nIn addition:\n- Is there a function that is not adaptively smooth but still has a finite $L\\_{\\\\|\\cdot\\\\|\\_{\\mathcal{H}}}(f)$? Otherwise, the two assumptions would characterize the same class of functions, differing only by constants.\n- Regarding the practical implications, it seems that full-matrix AdaGrad should outperform Adam, since its corresponding $\\mathcal{H}$ is a superset of that for Adam, leading to smaller adaptive smoothness. Is there any empirical evidence supporting this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OclJ5UEKGe", "forum": "iaoAKDRAJQ", "replyto": "iaoAKDRAJQ", "signatures": ["ICLR.cc/2026/Conference/Submission22735/Reviewer_7CSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22735/Reviewer_7CSx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762357707273, "cdate": 1762357707273, "tmdate": 1762942362559, "mdate": 1762942362559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}