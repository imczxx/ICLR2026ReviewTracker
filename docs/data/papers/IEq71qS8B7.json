{"id": "IEq71qS8B7", "number": 18298, "cdate": 1758286126481, "mdate": 1763739682144, "content": {"title": "Learning Brain Representation with Hierachical Visual Embeddings", "abstract": "Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain–image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system.\nIn this paper, we propose a brain–image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multiscale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.", "tldr": "", "keywords": ["Visual Decoding", "Brain-Computer Interface", "EEG", "Contrastive Learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3283f297b9394713a71a59925a781e4eb999c7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an EEG-image alignment strategy using multiple pre-trained visual encoders with various inductive biases to capture hierarchical and multiscale visual representations. Contrastive learning is used for the alignment between EEG and visual embeddings. A Fusion Prior is introduced to learn a  mapping on large-scale visual data that matches EEG features to this pre-trained prior so to enhance\ndistributional consistency across modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The presented brain-to-image framework builds on contrastive learning and pretrained vision priors. Multi-level fusion of CLIP and\nVAE features are integrated to balance brain-image alignment and  high-fidelity image reconstruction. Paper is clearly written and the approach well motivated. The idea of addressing the limitations of CLIP and related encoders in capturing local and fine-grained information by incorporating low-level visual features modeled by a Variational Autoencoder  into the fused representation is interesting and gives improved results."}, "weaknesses": {"value": "The idea of  integrating multiple pre-trained encoders to construct multiscale visual representations, is not original in broader terms. The paper lacks a balanced conclusion rating limitations of the proposed  methodology. Apart from the semantic similarity, I am unsure how to deal with the aspect of similarity of generated result."}, "questions": {"value": "In Figure 4 there seem to be still challenges in terms of generation. Is subject 8 chosen for a particular reason?\n\nIt is stated that the fused configuration that integrates complementary semantic encoders with a VAE latent, produces a well-balanced system across retrieval and reconstruction. Can you pls describe in more detail what is considered by a well-balanced system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pZ5gIcw4Wk", "forum": "IEq71qS8B7", "replyto": "IEq71qS8B7", "signatures": ["ICLR.cc/2026/Conference/Submission18298/Reviewer_DCJ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18298/Reviewer_DCJ4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765001550, "cdate": 1761765001550, "tmdate": 1762928022299, "mdate": 1762928022299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for aligning EEG/MEG and image representations to enable visual retrieval and reconstruction from brain signals. The proposed model fuses visual embeddings from multiple vision encoders and uses a contrastive loss to align the combined image representations with EEG representations. For image reconstruction, the framework employs a conditioning adapter to match brain-derived features with the expected distribution of an image generative model. The authors demonstrate substantial performance gains over state-of-the-art baselines in both image retrieval and reconstruction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors contribute a framework for both image retrieval and image reconstruction from brain signals. The idea of using multiple image encoders, fuse their representations and apply a symmetrical self-supervised loss is sound and interesting. The results of their approach also show a significant increase in performance against the previous state-of-the-art, and across several modalities (EEG and MEG). While I have some questions regarding the novelty of the components of the framework (see below), the final performance of the method is undeniable and to be commended.\n\n- Finally, the paper is very well written (with no apparent typos) and well structured. All figures and tables are of high-quality and easily interpretable. The authors also present all necessary details to understand the methodology and the evaluation setup.\n\n- I believe this paper can be of some significance to the community. The superior performance of this method can lead to similar advances  in cross-modal capabilities in regards to other modalities (such as sound, video)."}, "weaknesses": {"value": "- While the model outperforms existing baselines, this improvement might be partly expected due to the increased image information provided by combining multiple encoders. Prior studies (e.g., [1], [2]) have already shown that the choice of image encoder significantly influences decoding performance. Can the authors comment on this?\n\n- The authors claim that the superior performance of their model is due to involving different levels of image information (high-level from CLIP and low-level from VAE). However, they do not provide quantitative evidence for how each encoder contributes to the final results or why they exclusively attribute high-level features to CLIP and low-level to VAE. Could the authors include a quantitative or attribution analysis showing the relative contribution of each encoder to the model’s performance? \n\n- The so-called fusion prior seems to combine two existing ideas: using multiple image encoders (separately mentioned as a contribution) and employing the IP adapter (which is present in previous studies such as in [3]). The full novelty of this contribution is therefore somewhat unclear: can the authors clarify the novelty aspect of their work?\n\n- Ablations are quite limited: (1) while the authors have a different brain and image encoder compared to the previous studies, they do not isolate the contribution of each of these encoders by keeping one part similar to the baseline and changing the other part (e.g., replacing only the EEG encoder by NICE, ATM, etc.).  (2) The image encoder ablation is limited: the authors only test subsets of the three chosen encoders (CLIP-ViT-B32, CLIP-ResNet, and VAE) without exploring alternatives (see [2] for more alternatives). My belief is that selecting a more powerful image encoder would lead to the same performance without a need to fuse image embeddings. \n\n- Several baselines are trained on full-channel EEG data, while the proposed model uses only a subset of 17 occipital and temporal channels. This discrepancy may affect the fairness of the comparison. How do the authors justify comparing models trained on different EEG channel subsets, and can they provide results using the same number of channels for a fairer comparison?\n\n- I also found it disappointing that the main paper does not include a dedicated section to discuss the ethical considerations of this work. While the use of this technology in the future can have benefits for certain populations, it's also obvious that it can be used for nefarious purposes, and this should be discussed in the context of the paper.\n\n**References**:\n- [1] Song, Yonghao, et al. \"Decoding natural images from eeg for object recognition.\" arXiv preprint arXiv:2308.13234 (2023).\n- [2] Rajabi, Nona, et al. \"Human-Aligned Image Models Improve Visual Decoding from the Brain.\" Forty-second International Conference on Machine Learning. 2025.\n- [3] Li, Dongyang, et al. \"Visual decoding and reconstruction via EEG embeddings with guided diffusion.\" Proceedings of the 38th International Conference on Neural Information Processing Systems. 2024."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZyYIfj4wKj", "forum": "IEq71qS8B7", "replyto": "IEq71qS8B7", "signatures": ["ICLR.cc/2026/Conference/Submission18298/Reviewer_uw2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18298/Reviewer_uw2d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831478136, "cdate": 1761831478136, "tmdate": 1762928021361, "mdate": 1762928021361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a well-designed framework for decoding visual information from brain signals, covering both image retrieval and reconstruction tasks. The method performs very well across various benchmarks, especially when generalizing across different individuals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation behind the approach is clear, and the results are encouraging."}, "weaknesses": {"value": "Pls see questions."}, "questions": {"value": "The system mainly relies on a two-stage training process and a strategy that merges information in a layered way. However, the technical contribution is somewhat limited. The key tools used, like CLIP, VAE, contrastive learning, and IP-Adapter, are all well-known existing techniques. What’s new is how they’re combined and applied to this particular problem.\n\nOne concern is that the method is resource-heavy. It needs large-scale pretraining on ImageNet and depends on models like SDXL, which may not be practical for researchers with limited computing power. The contrastive learning step is also expensive, using large batch sizes.\n\nThe paper treats the EEG brain signals as fixed blocks of spatial and temporal data but doesn’t explore how specific time intervals or frequency bands relate to different visual features. Exploring this could give useful insights into how the brain processes complex vs. simple visuals.\n\nThe paper mentions a gap between brain signals and visual data in the contrastive learning space, but this isn’t explored in depth. Adding visualizations, like t-SNE plots could help show how brain embeddings from different people relate to the corresponding image embeddings and shed light on what drives successful generalization.\n\nAnother limitation is that the Fusion Prior is trained without any text input, which may miss out on the benefits of SDXL’s strong understanding of visual semantics. It might be worth trying weak text supervision, such as using automatically generated captions, to see if it helps improve semantic alignment in the reconstructed images without hurting the model’s ability to capture low-level details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P92wtR80Ti", "forum": "IEq71qS8B7", "replyto": "IEq71qS8B7", "signatures": ["ICLR.cc/2026/Conference/Submission18298/Reviewer_rjbm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18298/Reviewer_rjbm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870690247, "cdate": 1761870690247, "tmdate": 1762928019705, "mdate": 1762928019705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Area Chairs and Reviewers,\n\nWe sincerely thank you for the thoughtful and detailed feedback on our manuscript. We would like to emphasize the core contributions and innovations of this paper in three aspects: technical framework, scientific findings, and empirical performance/community impact. In the revised manuscript, we have highlighted these core contributions and innovations in green for clarity.\n\n**1. Technical framework: fusion-based brain–vision interface**. \n\nWe propose a fusion-based alignment strategy that maps brain embeddings to a fused visual representation constructed from multiple pretrained encoders, including CLIP for semantic information and a VAE for pixel-level details. To the best of our knowledge, this is the **first brain-to-image decoding framework** that learns brain representations by leveraging the complementary information across hierarchical visual encoders. Furthermore, we introduce a visual fusion prior, trained on large-scale visual data to provide a robust and general visual representation. Aligning brain embeddings with this prior enables faithful image reconstruction and serves as an effective bridge for a brain-to-image decoding framework.\n\n**2. Scientific findings: joint semantic and low-level alignment in brain signals**. \n\nUsing this fusion framework, we provide systematic evidence that brain recordings carry information that **simultaneously aligns with high-level semantic features and low-level pixel features**. In the revision, as Reviewer uw2d suggested, the supplemented Fig. 8 and Tab. 14 show that adding VAE to single-encoder settings consistently improves decoding performance on THINGS-EEG (e.g., **SynCLR: +10.5 top-1 / +4.6 top-5; DINO: +26.6 top-1 / +29.0 top-5**), and that neither semantics-only nor pixels-only settings can recover the same joint brain–vision structure. This goes beyond proposing a new module and offers concrete empirical insight into how brain signals relate to different levels of visual representation.\n\n**3. Performance, plug-and-play generalization, and community resource**. \n\nUnder a fixed fusion-based training scheme (same mixed visual encoders and alignment objective), we directly reuse brain encoders from prior EEG decoding work: simply swapping our brain encoder for their backbones, while keeping the fusion pipeline unchanged, already yields large decoding gains over their original single-encoder settings (see Tab. 13 in the revision), showing that our interface is **model-agnostic and plug-and-play** across diverse brain backbones. Overall, our method delivers strong improvements in both retrieval and reconstruction quality, and we will release the pretrained Fusion Prior and code so that future brain-to-image decoding work can directly build on this interface."}}, "id": "bR97JRcDAg", "forum": "IEq71qS8B7", "replyto": "IEq71qS8B7", "signatures": ["ICLR.cc/2026/Conference/Submission18298/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18298/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18298/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726223096, "cdate": 1763726223096, "tmdate": 1763728302937, "mdate": 1763728302937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Area Chairs and Reviewers,\n\nWe sincerely thank you for the thoughtful and detailed feedback on our manuscript. We would like to emphasize the core contributions and innovations of this paper in three aspects: technical framework, scientific findings, and empirical performance/community impact. In the revised manuscript, we have highlighted these core contributions and innovations in green for clarity.\n\n**1. Technical framework: fusion-based brain–vision interface**. \n\nWe propose a fusion-based alignment strategy that maps brain embeddings to a fused visual representation constructed from multiple pretrained encoders, including CLIP for semantic information and a VAE for pixel-level details. To the best of our knowledge, this is the **first brain-to-image decoding framework** that learns brain representations by leveraging the complementary information across hierarchical visual encoders. Furthermore, we introduce a visual fusion prior, trained on large-scale visual data to provide a robust and general visual representation. Aligning brain embeddings with this prior enables faithful image reconstruction and serves as an effective bridge for a brain-to-image decoding framework.\n\n**2. Scientific findings: joint semantic and low-level alignment in brain signals**. \n\nUsing this fusion framework, we provide systematic evidence that brain recordings carry information that **simultaneously aligns with high-level semantic features and low-level pixel features**. In the main text, Tab. 3 and in the revision, the supplemented Fig. 8 and Tab. 14 jointly show that adding the VAE to single-encoder settings consistently improves decoding performance on THINGS-EEG (e.g., **DINO: +26.6 top-1 / +29.0 top-5; CLIP ViT-B/32: +21.4 top-1 / + 11.0 top-5; RN50: +17.7 top-1 / +10.0 top-5 ; SynCLR: +10.5 top-1 / +4.6 top-5**), and that neither semantics-only nor pixels-only settings can recover the same joint brain–vision structure. This goes beyond proposing a new module and offers concrete empirical insight into how brain signals relate to different levels of visual representation.\n\n**3. Performance, plug-and-play generalization, and community resource**. \n\nUnder a fixed fusion-based training scheme (same mixed visual encoders and alignment objective), we directly reuse brain encoders from prior EEG decoding work: simply swapping our brain encoder for their backbones, while keeping the fusion pipeline unchanged, already yields large decoding gains over their original single-encoder settings (see Tab. 5 in the revision), showing that our interface is **model-agnostic and plug-and-play** across various EEG encoder backbones. Overall, our method delivers strong improvements in both retrieval and reconstruction quality, and we will release the pretrained Fusion Prior and code so that future brain-to-image decoding work can directly build on this interface."}}, "id": "bR97JRcDAg", "forum": "IEq71qS8B7", "replyto": "IEq71qS8B7", "signatures": ["ICLR.cc/2026/Conference/Submission18298/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18298/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18298/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726223096, "cdate": 1763726223096, "tmdate": 1763740130265, "mdate": 1763740130265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for decoding and reconstructing visual stimuli from EEG and MEG signals. It leverages multiple pretrained visual encoders to capture hierarchical, multi-scale representations and introduces a Fusion Prior trained on large-scale visual data to stabilize cross-modal alignment, improving both retrieval and reconstruction performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a timely and relevant problem—decoding visual information from brain signals—using an interesting combination of methods. It combines multiple pretrained visual encoders to capture hierarchical visual representations and introduces a Fusion Prior to improve stability and cross-modal consistency. While the individual components build on existing ideas, their integration is potentially novel and thoughtfully motivated. The experiments indicate improvements over other methods"}, "weaknesses": {"value": "While the paper addresses an important problem, its main novelty lies in combining known components rather than introducing a fundamentally new mechanism. The Fusion Prior is interesting but I didn't find the motivation very clear, and its contribution relative to the pretrained encoders is not clearly disentangled. The approach relies heavily on VAEs for low-level reconstruction, but alternative reconstruction strategies—such as diffusion-based or adversarial priors directly aligned with EEG features—are not explored or compared. Evaluation is limited to the THINGS datasets with averaged EEG signals, raising questions about generalization and robustness in more naturalistic settings."}, "questions": {"value": "1. How critical is the Fusion Prior to the observed improvements—could similar gains be achieved by joint training or fine-tuning the visual encoders?\n2. Why was the VAE chosen as the only source of low-level features? Have you considered comparing it with alternative reconstruction models such as diffusion or GAN-based priors?\n3. How does the model handle single-trial EEG or higher-noise conditions, given that the experiments rely on averaged signals?\n4. Can the approach generalize beyond the THINGS datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qs8J05btUI", "forum": "IEq71qS8B7", "replyto": "IEq71qS8B7", "signatures": ["ICLR.cc/2026/Conference/Submission18298/Reviewer_wbLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18298/Reviewer_wbLE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963766790, "cdate": 1761963766790, "tmdate": 1762928018946, "mdate": 1762928018946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}