{"id": "ngwbYGykrW", "number": 488, "cdate": 1756742345660, "mdate": 1759898257864, "content": {"title": "FG-CLIP 2: A Bilingual Fine-grained Vision-language Alignment Model", "abstract": "Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. Our approach leverages rich fine-grained supervision, including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages. We will release the model, code, and benchmark to facilitate future research on bilingual fine-grained alignment.", "tldr": "", "keywords": ["Vision-Language Model; Chinese Multimodal Benchmark"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93b5fbb8dbba789f1aff3bfe5dabb9c1216d24c6.pdf", "supplementary_material": "/attachment/56e4739099b176af160c32ca890f79dd5b3b8af9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a bilingual fine-grained vision-language alignment model for English and Chinese. It extends CLIP-style architectures by integrating fine-grained supervision signals and introducing a new Textual Intra-modal Contrastive (TIC) loss. The training follows a two-stage paradigm: global alignment with long and short captions, and fine-grained regional alignment using multiple discriminative objectives. Authors curate large-scale bilingual datasets and construct several Chinese benchmarks to evaluate fine-grained and bilingual understanding. Experiments on 29 datasets and 8 tasks show consistent improvements over existing methods in retrieval, detection, segmentation, and as a vision encoder in large multimodal models (LMMs)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a comprehensive bilingual fine-grained vision-language alignment model for English and Chinese. The two-stage training design systematically transitions from global to fine-grained alignment, combining both caption lengths and region-level signals. Explicit bilingual integration with curated English and Chinese datasets totaling over ~2.4 billion pairs supports multilingual robustness. The approach bridges the gap between previously disjoint English fine-grained and Chinese global models, enhancing cross-lingual generalization. The resulted model FG-CLIP2 verified on 29 datasets and 8 tasks shows consistent improvements over existing methods"}, "weaknesses": {"value": "- The presentation in this paper could be improved, especially in the Approach section. For example, Section 3.2 does not introduce the $L_{FGV}$ and $L_{FGT}$ loss. If these losses are defined in FG-CLIP (or elsewhere), they need be mentioned. In Section 3.3 on training data,  which is the key factor enabling the model’s bilingual capability, the paper lacks essential visualizations and analysis, such as dataset construction details, sample examples, and composition distribution. Moreover, there appears to be no experimental analysis of how different data components contribute to bilingual performance improvement.For example, is there evidence that the inclusion of Chinese training data improves the model’s English alignment capability, given the larger overall training corpus?\n- As one of the core contributions, the Textual Intra-modal Contrastive (TIC) mechanism lacks sufficient experimental validation. For instance, no visualization or embedding analysis is provided to support the claimed improvement in semantic separability.\n- Similarly, the two-stage training strategy from coarse to fine alignment is not thoroughly evaluated. All results are reported only for the final model, without distinguishing between training stages. Therefore, the claim that the second stage effectively enhances fine-grained capability is insufficiently verified.\n- Finally, the comparison with previous works should include the amount of training data used for each model."}, "questions": {"value": "Section 4.1 mentions that the first-stage training uses ASCEND 910B NPUs, while the second stage uses H800 GPUs. How long did each stage cost, and why was the second-stage training not continued on NPUs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B9JW0K2oqR", "forum": "ngwbYGykrW", "replyto": "ngwbYGykrW", "signatures": ["ICLR.cc/2026/Conference/Submission488/Reviewer_HrUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission488/Reviewer_HrUG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718046670, "cdate": 1761718046670, "tmdate": 1762915529231, "mdate": 1762915529231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper designs a bilingual vision-language model to enhance fine-grained alignment between visual content and linguistic descriptions for both English and Chinese. It adopts a two-stage training paradigm with multiple discriminative objectives and is trained on large-scale bilingual datasets, outperforming existing models across 29 datasets and 8 tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The proposed bilingual pre-training framework and the design of loss objectives exhibit certain innovativeness.\n\n3. Experimental results fully demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. FG-CLIP 2 has achieved impressive performance improvements compared to FG-CLIP. I wonder which part of the design contributes the most significantly to the performance improvement? Does it come from the gain brought by using SigLIP 2 for initialization? Does the simultaneous use of bilingual data have a mutually promoting effect?\n\n2. Five losses used in the paper are assigned different weights, yet there seems to be no ablation experiment to justify why such weight settings are adopted. In addition, the results of the ablation experiment in Table 7 are not very significant.\n\n3. Some related papers (e.g., [1][]2) should be discussed and compared.\n\n[1] UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding. ECCV 2024.\n\n[2] Contrastive Localized Language-Image Pre-Training. ICML 2025."}, "questions": {"value": "Please refer to the 'weaknesses' part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ED4ExuVZWK", "forum": "ngwbYGykrW", "replyto": "ngwbYGykrW", "signatures": ["ICLR.cc/2026/Conference/Submission488/Reviewer_XDPh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission488/Reviewer_XDPh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736916769, "cdate": 1761736916769, "tmdate": 1762915529089, "mdate": 1762915529089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FG-CLIP2, a bilingual fine-grained vision-language model, designed to advance fine-grained alignment for both English and Chinese. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves SOTA performance. Additionally, a new benchmark for Chinese multimodal understanding is contributed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses the need for English-Chinese bilingual fine-grained vision-language understanding, catering to non-English scenario demands.\n2. A new benchmark suite to advance evaluation in Chinese multimodal understanding is contributed.\n3. FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages."}, "weaknesses": {"value": "1. The paper’s focus is confusing. For bilingual capability, it only mentions adding English-Chinese mixed data without other innovations. The differences between FG-CLIP2 and previous fine-grained vision-language models are not clarified.\n2. There is no analysis of how previous methods would perform when trained on this dataset, affecting the fairness of experimental comparisons.\n3. The training objectives involve too many hyperparameters, but the paper does not explain how these hyperparameters are determined, raising questions about the reliability of the training process."}, "questions": {"value": "refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K5KBOQwPqz", "forum": "ngwbYGykrW", "replyto": "ngwbYGykrW", "signatures": ["ICLR.cc/2026/Conference/Submission488/Reviewer_ipHB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission488/Reviewer_ipHB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795707374, "cdate": 1761795707374, "tmdate": 1762915528962, "mdate": 1762915528962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}