{"id": "GKGme4vZ6O", "number": 19543, "cdate": 1758297099670, "mdate": 1759897033547, "content": {"title": "Scales++: Compute Efficient Evaluation Subset selection with Cognitive Scales Embeddings", "abstract": "The prohibitive cost of evaluating large language models (LLMs) on comprehensive benchmarks necessitates the creation of small yet representative data subsets (i.e., tiny benchmarks) that enable efficient assessment while retaining predictive fidelity. Current methods for this task operate under a model-centric paradigm, selecting benchmarking items based on the collective performance of existing models. Such approaches are limited by large upfront costs, an inability to immediately handle new benchmarks (`cold-start'), and the fragile assumption that future models will share the failure patterns of their predecessors. In this work, we challenge this paradigm and propose a item-centric approach to benchmark subset selection, arguing that selection should be based on the intrinsic properties of the task items themselves, rather than on model-specific failure patterns. We instantiate this item-centric efficient benchmarking approach via a novel method, \\textsc{Scales++}, where data selection is based on the cognitive demands of the benchmark samples. Empirically, we show \\textsc{Scales++} reduces the upfront selection cost by over 18$\\times$ while achieving competitive predictive fidelity. On the Open LLM Leaderboard, using just a 0.5\\% data subset, we predict full benchmark scores with a 2.9\\% mean absolute error. We demonstrate that this item-centric approach enables more efficient model evaluation without significant fidelity degradation, while also providing better cold-start performance and more interpretable benchmarking.", "tldr": "", "keywords": ["LLM evaluation", "efficient benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20c27a0c6cabf3f3fd0093a48e37e9398284c29b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method, Scales++, to predict benchmark performance, based on evaluating LLMs on a limited number of items. The method uses GPT-4 to embedd benchmark items in a low-dimensional space, reflecting different cognitive capabilities necessary to solve the item. Items are then clustered in this space, and evaluation focuses on a number of items that are representative for each cluster. The authors claim that their method predicts benchmark performance similarly well as existing methods, while not requiring historical evaluation data of other LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Using item characteristics to predict benchmark performance is a promising direction\n- The empirical performance of the proposed method appears to be strong"}, "weaknesses": {"value": "- It is quite difficult to understand the experimental setup for most of the experiments in section 5, making it difficult to evaluate (or reproduce) these experiments. \n- I suspect that the evaluation of the random baseline in figure 2 is bugged: MAE should approximately decrease as 1/sqrt(n) in the sample size n, but essentially stays constant going from 0.5% to 2% of items. It is unclear, whether the cause of this could affect the evaluation of the more complicated alternatives as well. \n- There is no meaningful discussion of limitations. Given that the method does not come with any guarantees (and seems to severely underperform alternatives in some settings, according to the appendix), disclaimers about how to responsibly use this should be included in the main text. \n    - In light of the brittleness of the proposed frameworks performance gains, the main text's focus on one large test set, aggregating multiple benchmarks, also feels a bit misleading. \n    - It also seems worth noting, that the computational gains of the proposed method only manifest for new benchmarks, as comprehensive evaluations of a large number of models are publically available for most existing benchmarks. \n- Beyond that, the paper would strongly benefit from a thorough proofread. Examples:\n   - Table 8 seems to be missing entries for IRT++\n   - The writing refers to a 10% subset setting multiple times, but the results only seem to report up to 2%\n   - \"Our SCALES++ LITE annotates the entire in under 20 minutes\" missing word \n    - \"Their approach successfully created Arena-Hard-Auto, a curated 500-item benchmark that capable of robustly recovering LLM relative rankings across multiple large benchmarks\" missing word, also \"successfully created\" is slightly strange phrasing\n    - \"This work introduces a paradigm shift\" seems overly grandiose\n   - \"Our work demonstrates that focusing on intrinsic task properties rather than historical model behav-ior offers a more robust and efficient path forward for comprehensive LLM assessment.\" The robustness claim does not seem to be substantiated anywhere in the paper"}, "questions": {"value": "- What is the setting with 16 initial LLM calls for IRT reported in Table 2? Also, why are these results not reported in Figure 2 (IRT seems to outperform the proposed method for >0.5% data points here)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2NHt8VLtoE", "forum": "GKGme4vZ6O", "replyto": "GKGme4vZ6O", "signatures": ["ICLR.cc/2026/Conference/Submission19543/Reviewer_4sK9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19543/Reviewer_4sK9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761309894543, "cdate": 1761309894543, "tmdate": 1762931429573, "mdate": 1762931429573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new perspective to efficient evaluation of language models - using item-centric view instead of the model-centric view. \nIn this view, the proposed method first rate each evaluation example using 16 criteria (first introduced in a prior work (Zhou et al. 2025)), and use these scores as the representation (\"embedding\") of the example. This is more efficient than relying on performance of prior models as the representation, which could involve 300+ models.\nThe paper further present a more efficient variant of the method, that further reduce the 16 LM calls for the 16 criteria, and uses a GNN trained with auxiliary data instead.\nEmpirically the method is comparable with model-centric approaches while being more computationally efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Taking on an example-centric view is under-explored for this problem and this work innovatively studies this. The method is also shown to be effective.\n* The paper also did a great job in introducing prior works and contrasting them with the proposed method."}, "weaknesses": {"value": "* Several remaining confusions about methods and experiment settings. See questions below."}, "questions": {"value": "* Can you elaborate more on some design choices you made? Why is the UMAP dimension reduction needed? Why is Graph neural network preferred over other possible networks for classification?\n* Can you elaborate more on the auxiliary training data you used? It was currently first mentioned in Line 316 without much introduction. In line 343 you further mention a validation split of auxiliary data. Is the full TULU-SFT set used as auxiliary data? How large is that?\n* Can you explain in figure 2, what are included in compute cost for each method? Is \"initial LLM calls\" equal to 16 for scales++ and 300 for IRT methods?\n* I'm quite confused at section 5 where some cross-architecture and cross-model-size experiments were done. In my understanding scales does not require any training on \"source models\", so what does \"cross-architecture\" refer to here?\n\nMinor:\n* Line 113: The citation of BIG-bench needs to be corrected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LETPRgQq0J", "forum": "GKGme4vZ6O", "replyto": "GKGme4vZ6O", "signatures": ["ICLR.cc/2026/Conference/Submission19543/Reviewer_pZpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19543/Reviewer_pZpu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933936274, "cdate": 1761933936274, "tmdate": 1762931428983, "mdate": 1762931428983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an item-centric approach for benchmarking subset selection to enable efficient evaluation. The authors argue that previous methods suffer from high upfront costs, cold-start issues, and potential generalization problems. To address these limitations, they cluster data points using the \"General Scale,\" which leverages GPT-4o to annotate 16 cognitively grounded properties per data point. To reduce cost further, they train a GNN to predict these 16 dimensions. Experimental results demonstrate the methodâ€™s efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well presented and easy to follow.\n- It highlights important issues with current efficient evaluation techniques, some of which make sense to me."}, "weaknesses": {"value": "-  The contribution is incremental. The approach basically ensembles AnchorPoints, TinyBenchmarks, and GeneralScale. AnchorPoints already evaluated an embedding-based baseline (see \"Pretrained\" in Table 2). This paper builds on that idea by replacing pretrained embeddings with GPT-4o annotations (General Scales). Following TinyBenchmarks, it also uses a weighted average between two estimators.\n- The proposed method does not necessarily solve the generalization problem. Previous approaches can indeed fail to transfer when behavior learned from one model family does not generalize to another. Similarly, the proposed approach relies on GPT-4o to annotate the 16 dimensions, and those annotations may also fail to generalize across different model families. The results in Table 2 and 3 also don't support the better generalization of the proposed method. \n-  The IRT implementation differs from the original TinyBenchmarks paper. TinyBenchmarks Section 3.2 states that data points are clustered by the scores of existing LLMs. This paper, however, implements IRT by clustering IRT parameters, as stated in line 252.\n-  The comparison is incomplete. For methods like AnchorPoints and IRT, one could reduce the number of training models instead of using 300 models; that comparison is missing. TinyBenchmarks introduces P-IRT and GP-IRT, both of which should be compared. The baseline MetaBench is mentioned but not compared."}, "questions": {"value": "- Why does the IRT++ method become worse as the subset size increases?  This is inconsistent with the results in the TinyBenchmark paper, e.g., Figure 5.\n- The Random baseline error looks unusually high. How was the Random baseline implemented? Since the OpenLLM leaderboard is multi-task, one should randomly sample data points from each task, compute the estimated score per task, and then average the scores across tasks. Otherwise, tasks with more data will dominate the sampled points, and the result will be misleading."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DoTxonZEiX", "forum": "GKGme4vZ6O", "replyto": "GKGme4vZ6O", "signatures": ["ICLR.cc/2026/Conference/Submission19543/Reviewer_Gonq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19543/Reviewer_Gonq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941769590, "cdate": 1761941769590, "tmdate": 1762931428456, "mdate": 1762931428456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}