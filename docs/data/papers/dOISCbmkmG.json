{"id": "dOISCbmkmG", "number": 19423, "cdate": 1758296110458, "mdate": 1763767048337, "content": {"title": "MapQA: A Map-Question-Answering Benchmark for Visual Language Model Reasoning", "abstract": "Maps are central to how humans make sense of the world, from navigation and environmental monitoring to military planning and historical interpretation. Yet despite rapid progress in large multimodal models (LMMs), these systems continue to struggle with interpreting maps--an essential skill for visual reasoning that goes beyond pattern recognition and text extraction. To close this gap, we introduce MapQA, the first large-scale benchmark specifically designed to evaluate LMMs on map understanding. MapQA contains over 4,200 carefully curated, open-ended question–answer pairs spanning diverse map types, each constructed to require reasoning directly from the map rather than relying on memorized world knowledge. Benchmark questions are generated through a scalable human-in-the-loop process to ensure quality, and evaluated using an LLM-as-a-judge protocol aligned with human judgments. Our experiments show that while humans answer over 91% of questions correctly, state-of-the-art proprietary models achieve barely half that performance, with open-source models typically below 30%. These findings highlight a substantial gap between human and machine map understanding, underscoring the need for benchmarks like MapQA to guide future progress in multimodal reasoning.", "tldr": "We present a novel dataset for vision language model reasoning that humans can answer but the best VLMs struggle.", "keywords": ["Benchmarks", "Datasets", "Vision Language Model Benchmarks", "Multimodal Model Benchmarks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc9ddb1cd135b041347011309854afbb6a02d65c.pdf", "supplementary_material": "/attachment/8a2d7489d5e070770d332cf3944a7f0b4b444c3f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MapQA, a 4,234-item benchmark for evaluating the cartographic reasoning of Large Multimodal Models (LMMs) across diverse map types. The dataset was created using a mix of expert hand-curation and a scalable human-in-the-loop (HITL) pipeline where questions were generated by GPT-4o-mini. The authors report a significant performance gap, with humans (91.4%) far outperforming the best SOTA LMM (52.5%), concluding that map understanding is a major open challenge. While the problem is well-motivated, the benchmark's contribution is undermined by critical methodological flaws in its curation, evaluation, and analysis."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Identifies a clear and important weakness in LMMs - true cartographic and spatial reasoning, distinct from simple VQA.\n2. The top-line finding of a large human-LMM performance gap is stark and effectively highlights the problem's difficulty."}, "weaknesses": {"value": "1. Using GPT-4o-mini to generate 60% of the benchmark questions introduces a massive, systemic bias, likely testing for LLM-specific patterns rather than diverse human reasoning.\n2. The benchmark improperly mixes two different data distributions (1,647 hand-curated vs. 2,587 LLM-generated) without any analysis of the bias or performance differences between them.\n3. Using only two expert annotators for consensus is a low bar; the statistical justification is self-referential as it relies on the authors' own definition of \"expert\".\n4. The difficulty ranking is based on \"length of time required to respond\", a poor proxy that is heavily confounded by individual annotator speed and skill.\n5. A critical conflict of interest exists by using GPT-4o as the judge while simultaneously evaluating the top-performing OpenAI model family, fatally biasing the results.\n6. The paper fails to describe how the LLM-as-a-judge was calibrated (i.e., $\\alpha$ and $\\beta$ derivation), making the entire scoring mechanism opaque and irreproducible.\n7. Critical decoding parameters for generative evaluation, such as temperature and top-p, are omitted, making the results impossible to reproduce.\n8. Glaring, unprofessional errors, like using \"GPT-4o-mini\" , \"GPTo4-mini\" , and \"o4-mini\" interchangeably, create confusion about which model was actually tested.\n9. The system prompt mentions \"image tiles\" , implying a complex, unexplained pre-processing strategy for handling high-resolution images that is a key missing detail.\n10. The analysis section only highlights expected findings, such as the non-novel positive correlation between token usage and performance. Additionally, the section is also incomplete, stopping short of investigating its own \"unexpected outcome\" (in Fig. 7) by simply stating \"More analysis is needed\"."}, "questions": {"value": "1. How can the benchmark's validity be trusted when 60% of it was generated by an LLM, and what analysis proves the two mixed data distributions (hand-curated vs. HITL) are comparable?\n2. Can the authors justify using GPT-4o as both judge and participant, and can they provide the exact, reproducible methodology for calibrating the judge's $\\alpha$ and $\\beta$ coefficients?\n3. Given that 'response time' is a confounded proxy and the 'Medium' difficulty results were anomalous, what is the actual qualitative difference in the reasoning required for each difficulty tier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K7Qmni6bnO", "forum": "dOISCbmkmG", "replyto": "dOISCbmkmG", "signatures": ["ICLR.cc/2026/Conference/Submission19423/Reviewer_wS2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19423/Reviewer_wS2F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527618987, "cdate": 1761527618987, "tmdate": 1762931349423, "mdate": 1762931349423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MapQA, a novel large-scale benchmark designed to evaluate multimodal large language models on cartographic reasoning, an underexplored but important aspect of visual–spatial understanding. The dataset comprises 4,234 open-ended QA pairs across four map domains and 24 subcategories, combining hand-curated and LLM-generated examples in a human-in-the-loop workflow. The authors introduce a calibrated LLM-as-a-Judge approach (GPT-4o) for evaluation and report a substantial human–model performance gap, demonstrating that map reasoning remains a significant challenge for current models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a timely and original contribution by introducing MapQA, a large-scale benchmark designed to evaluate multimodal large language models (MLMMs) for genuine cartographic reasoning. This focus on map interpretation and spatial reasoning is novel within the broader vision–language research landscape, as prior benchmarks primarily emphasize charts, infographics, or natural images rather than structured geospatial data. The dataset’s design, explicitly aiming to prevent shortcuts such as OCR or world-knowledge retrieval, represents a thoughtful and important step toward evaluating grounded visual reasoning.\n\nFrom a methodological standpoint, the combination of hand-curated and human-in-the-loop LLM-generated QA pairs is innovative, demonstrating a clear effort toward scalability while maintaining human oversight. The attempt to calibrate an LLM-as-a-Judge (GPT-4o) against human accuracy through regression analysis and multi-round validation reflects a serious effort to build a reliable automated evaluation framework—an increasingly relevant topic in LMM research.\n\nThe experimental design and results also provide useful empirical insights. The study highlights a substantial and well-documented performance gap between humans ( 91%) and current models (best ≈ 52%), highlighting the challenges of spatial understanding for today’s multimodal systems. Including both proprietary and open-source models enables a fair comparative baseline, illustrating the persistent limitations of existing architectures across model families."}, "weaknesses": {"value": "While the paper makes a valuable contribution, several aspects limit its clarity, rigor, and interpretability. First, the LLM-as-a-Judge calibration (Section 3.1) is insufficiently explained. The authors report fitting GPT-4o judgments to a linear model y = α + βx with α = 0.035 and β = 0.921, but do not define what x and y represent or how this fit supports the claim that GPT-4o is a “strong indicator” of ground-truth accuracy. Without a clear interpretation of these coefficients or confidence intervals, readers cannot fully assess the reliability of the automatic scoring.\n\nSecond, the description of human evaluation (Section 4.1) lacks transparency. It is unclear whether the “human baseline” results were obtained from the same expert annotators who created the dataset or from an independent participant pool. If the former, the comparison may overstate human performance; if the latter, additional details about sampling, instructions, and quality control are needed to validate the results. Third, there is a potential data contamination issue: GPT-4o-mini was used to generate QA pairs during dataset construction but is also evaluated as a test-time model. Since the same model family produced portions of the benchmark data, its strong performance (ranking highest among all evaluated models) may not reflect genuine generalization. The authors should clarify whether held-out subsets or unseen maps were used to prevent such leakage. Fourth, while the paper repeatedly mentions 24 subcategories of maps, no detailed taxonomy or examples of these subcategories are provided. Including even a summarized list in an appendix would improve interpretability and allow others to build on this benchmark.\n\nFinally, some methodological and analytical aspects need more discussion: for example, correlations (e.g., token count vs. accuracy) are reported without deeper causal interpretation, and the discussion section does not explore why specific models fail (i.e., failure mode analysis). Addressing these limitations would make the paper’s insights more actionable for model development."}, "questions": {"value": "1. LLM-as-a-Judge calibration (Section 3.1): Could the authors clarify what variables x and y represent in the fitted linear model? How exactly does this calibration demonstrate that GPT-4o judgments closely align with human accuracy? It would be helpful to include an interpretation of α and β, as well as any measures of variance or confidence intervals.\n\n2. Human evaluation and baselines (Section 4.1): Were the “human” results reported in Table 2 obtained from the same annotators who created the ground-truth answers, or from a separate group of participants? Please clarify how these human baselines were generated and how participant independence was ensured.\n\n3. Potential data leakage through GPT-4o-mini: Since GPT-4o-mini was used in the question–answer generation process and also evaluated as a model, how did the authors ensure that the evaluation data was unseen by the model? Were any safeguards, such as held-out subsets or map-level separation, implemented to prevent data contamination?\n\n4. Curation process transparency (Section 3.2): While the paper outlines the human-in-the-loop curation pipeline, several methodological aspects remain unclear. Could the authors provide a more detailed quantitative and procedural breakdown of this process? In particular:\n• How many total QA candidates were produced by GPT-4o-mini, and what proportion were accepted, rejected, or manually revised?\n• What exact criteria were used for culling or acceptance at each stage (e.g., when “accuracy falls below 3.5%,” how is that threshold operationalized)? \n• How consistent were the two “expert” annotators—was any interannotator agreement metric (e.g., Cohen’s κ or percent agreement)\ncomputed?\n\n5. Subcategory taxonomy: The paper mentions 24 subcategories under the four primary map types, but does not describe or list them. Could the authors include a summary or appendix table specifying these subcategories and how they were defined?\n\n6. Evaluation scope and reliability: How consistent are GPT-4o’s judgments across different map domains (e.g., Military vs. Urban)? Was any domain-specific calibration performed to confirm that GPT-4o’s reliability as a judge is stable across these categories?\n\n7. Depth of analysis and category-level breakdown: Given that MapQA is explicitly organized into four major map categories (Military, Natural World, Urban, and Aviation) and 24 subcategories, it would be valuable to see category-wise performance results or at least a discussion of domain-specific trends. Did certain map types or subcategories systematically challenge models more than others? Could you also provide some discussion around failure model analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UVA3Iv2NpW", "forum": "dOISCbmkmG", "replyto": "dOISCbmkmG", "signatures": ["ICLR.cc/2026/Conference/Submission19423/Reviewer_D3df"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19423/Reviewer_D3df"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587964985, "cdate": 1761587964985, "tmdate": 1762931348815, "mdate": 1762931348815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MapQA, a large-scale benchmark (4,234 open-answer QA pairs over 794 unique images) focused on map understanding across four domains (Military, Natural World, Urban, Aviation; 24 subcategories). Questions are designed to be answerable only from the map, minimizing reliance on world knowledge or pure OCR. Evaluation uses an LLM-as-a-Judge protocol calibrated to human judgments; humans achieve ~91.4% accuracy, while the best proprietary models reach ~52.5% and open-source models ~26%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear, important problem: map literacy is under-evaluated despite practical impact (navigation, planning, geoscience). \n\nWell-scoped dataset: strong effort to ensure questions require visual/spatial reasoning from the map; explicit down-weighting of pure OCR or memorized trivia. Although many prior work is not acknowledged and ignored.\n\nHuman baselines & stats: Table 1 comprehensively reports scale and splits; human accuracy ~91% provides a meaningful upper bound. (Table 1 on p.4; human scores in Table 2 on p.6.). Testing 18 different models (both proprietary and open-source). The analysis of token usage, model size, and difficulty levels adds useful insights beyond simple accuracy numbers.\n\nTransparent evaluation: clearly specified judge prompts (Fig. 3), binary verdict with numerical slack; calibration to >2k human-rated items.  The alignment of GPT-4o judge scores with human judgments (showing α = 0.035 and β = 0.921) demonstrates that the evaluation methodology is reliable.  \n\nThe questions are carefully constructed to ensure they genuinely test map reasoning rather than text extraction or memorized knowledge. The example in Figure 1 demonstrates how models can fail by relying on historical knowledge instead of actually reading the map.\n\nInsightful findings: substantial human–model gap; weak size-vs-accuracy relationship; positive correlation with total tokens, useful signals for future model design. (Figs. 4–6.)"}, "weaknesses": {"value": "A very limited novelty is there. Doesn't compare with vast literature of prior work on MapQA[1], MapWise[2], MapIQ [3], MapBench[4], CartoMark[5], MapQA (GQA)[6] and many more. A proper comparison should be there with prior works to show how this is different.\n\nWhile calibrated, the judge is still a single proprietary model (GPT-4o) which can bias outcomes (known position/self-enhancement biases in LLM judges). Cross-judge triangulation is limited.  \n\nThe paper highlights categories of difficulty (orientation, relational context, etc.), but qualitative/error taxonomies tied to specific visual/map constructs could be deeper. \n\nImages are not directly included due to size; code and images are promised “upon acceptance,” which may hinder immediate verification. Four top-level types are helpful, but the current 24 subcategories may still under-represent thematic maps (e.g., meteorological/time-varying, cadastral/parcel, transit schematics), limiting coverage. \n\nThe paper reports dataset statistics, split sizes, device configs, and promises to release evaluation scripts and images in a repo after acceptance; good but not yet fully turnkey. \n\nWhile the paper shows models struggle, it doesn't deeply analyze what types of errors they make.  Do they fail at spatial reasoning?\nColor interpretation?  Scale understanding?  The introduction mentions nine failure categories from prior work, but i don't see a breakdown of which categories cause the most problems in MapQA.\n\nThe paper notes that some models couldn't be tested because map images are too large for GPU memory (mentioning they excluded models like Pixtral-124B and Qwen2.5-VL-72B).  This seems like an important limitation - if real-world map understanding requires processing large, detailed images, then smaller context windows might be a fundamental bottleneck.  The paper doesn't discuss whether image size correlates with question difficulty or error rates.\n\nReferences\n[1] https://arxiv.org/abs/2211.08545\n[2] https://arxiv.org/pdf/2409.00255v1\n[3] https://arxiv.org/abs/2507.11625\n[4] https://arxiv.org/pdf/2503.14607\n[5] https://www.nature.com/articles/s41597-024-04057-7\n[6] https://arxiv.org/pdf/2503.07871"}, "questions": {"value": "How do models perform across the four main map types (Military, Natural World, Urban, Aviation)? Are some types consistently harder than others? \n\nThe statistics show 19.2% of questions involve multiple images, but how does this affect accuracy? Do models struggle more when they need to compare or integrate information across maps?\n\nIt is mentioned minimizing OCR-only questions, but what percentage of questions require text reading as part of the reasoning? How do models perform on questions that require both text and visual understanding versus purely visual reasoning?\n\nWhy does Qwen2.5-VL-2B (the smallest model) outperform much larger open-source models? This seems counterintuitive - like is it architecture, training data, or something else?\n\nThe difficulty levels are based on human response time, but there's a puzzling anomaly in Figure 7 where \"Medium\" difficulty questions show better model performance than expected. \n\nThe paper mentions annotators need 95% accuracy to be considered \"experts,\" but how was the initial hand-curated set validated? Who determined those ground truth answers?\n\nAverage questions are 12.7 words - quite short. Are there any longer, more complex questions that require multi-step reasoning? How does question complexity relate to accuracy?\n\nGiven the wide range of image sizes (546×393 to 6000×6000), does resolution affect model performance? Do models do better on smaller, simpler maps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "23HmTtxORx", "forum": "dOISCbmkmG", "replyto": "dOISCbmkmG", "signatures": ["ICLR.cc/2026/Conference/Submission19423/Reviewer_yx5S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19423/Reviewer_yx5S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942211297, "cdate": 1761942211297, "tmdate": 1762931348285, "mdate": 1762931348285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MapQA, a  dataset to evaluate LLMs on Visual Language Model Reasoning over map images. MapQA contains 4234 question-answer pairs over 794 images. The dataset annotation combine human annotation and human-in-the-loop to achieve a scalable annotation with high accuracy. The benchmark evaluates comprehensive open-source and proprietary models. Results show that humans achieve 91.4 % accuracy, while the best models (GPT-4o-mini, GPT-5) reach around 50 %."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark addresses the need for a diverse map question-answering dataset to evaluate LLMs’ reasoning capabilities on maps.\n2. The combination of manual curation, expert annotator validation, and automated human-in-the-loop expansion provides both scalability and high data quality."}, "weaknesses": {"value": "1.The paper claims that all questions are unanswerable without the aid of a map, for example, “Which airport is closest to Chicago O’Hare?”. However, Figure 1 includes a question that a language model can answer without looking at the map, regardless whether the answer is incorrect. It is unclear what standard was used to construct the question set. The paper should clarify what instructions were given to annotators and LLMs during question construction, and report model accuracy when the image is not provided.\n\n2.The paper reports that humans achieve over 90 % accuracy while the best models reach around 50 %. However, it is unclear how human annotation was conducted—what information was presented to the annotators and models, and whether any contextual hints were given. It should also be clarified whether answers could be inferred directly from the provided context rather than the map itself.\n\n3. Although the dataset is designed to evaluate reasoning capabilities of LLMs over maps, some examples (such as in Figure 1) seem to require reasoning beyond the map image. It is difficult to answer such questions without external background knowledge. If the dataset requires reasoning or factual knowledge from outside the map, reasoning model or retrieval-augmented models (e.g. deep-research models) should also be evaluated. \n\n4.The question in Figure 1 “Are the Confederates abandoning Fort Huger during the Battle of Roanoke Island?” is ambiguous. I asked GPT-5 this question, and it answers “yes” because the Confederates did abandon Fort Huger during the battle after the map. But when asked “based on the image, have the Confederates abandoned Fort Huger?”, GPT-5 correctly answers that they had not yet abandoned it at the time represented in the map. This raises concerns about the clarity and quality of the questions.\n\n5.The paper does not provide the distribution of map categories or their sources. And the standard for determining question difficulty is unclear."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aoyUMT4Z7i", "forum": "dOISCbmkmG", "replyto": "dOISCbmkmG", "signatures": ["ICLR.cc/2026/Conference/Submission19423/Reviewer_n98q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19423/Reviewer_n98q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762381192106, "cdate": 1762381192106, "tmdate": 1762931346674, "mdate": 1762931346674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for the time, depth of analysis, and care they invested in evaluating our submission. Your detailed comments substantially improved the clarity and rigor of the paper, and we have addressed each point with the goal of making the dataset, methodology, and evaluation pipeline as transparent as possible. We hope that the revisions and clarifications provided in this response meaningfully resolve the concerns raised. If you find that we have adequately addressed your questions and strengthened the work accordingly, we would be grateful if you would consider raising your score to reflect the improved paper. Your thoughtful feedback has directly contributed to a clearer and more robust version, and we sincerely appreciate your effort and expertise."}}, "id": "QOk0cBjqSX", "forum": "dOISCbmkmG", "replyto": "dOISCbmkmG", "signatures": ["ICLR.cc/2026/Conference/Submission19423/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19423/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission19423/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763758673568, "cdate": 1763758673568, "tmdate": 1763758673568, "mdate": 1763758673568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}