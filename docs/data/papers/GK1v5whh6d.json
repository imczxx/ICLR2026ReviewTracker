{"id": "GK1v5whh6d", "number": 1881, "cdate": 1756956755904, "mdate": 1763017487375, "content": {"title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models", "abstract": "Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics.", "tldr": "Diffusion-SDPO adds a simple winner-preserving scaling to DPO for diffusion models that keeps the preferred loss non-increasing, consistently improves human-preference scores and training stability.", "keywords": ["Direct Preference Optimization", "Preference Alignment", "Text-to-Image Generation", "Diffusion Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/28baf459869a64c376e78a6f9c9397f412e7f8c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors identify a failure mode of the classical DPO objective: both winner and loser losses tend to increase during training, which is highly counterintuitive. Using a first-order Taylor expansion, they derive a simple condition that ensures the winner’s loss does not increase. Under the assumption that the product of the winner’s and loser’s Jacobians is approximately an identity matrix, they introduce a dynamic scaling coefficient that adaptively balances the loser’s contribution, guaranteeing (under the linearization and Jacobian assumptions) non-increasing winner loss. Empirically, the method achieves strong results across standard synthetic reward models, consistently outperforming baseline alignment approaches with a simple, plug-and-play modification."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is simple, elegant, and practical — a few-line modification that can be readily integrated into existing DPO pipelines.\n\n2. The paper identifies and addresses a highly counterintuitive failure mode of classical DPO, providing clear analytical insight and an effective solution.\n\n3. The theoretical assumptions are reasonable, and the mathematical analysis is clear and easy."}, "weaknesses": {"value": "1. While the assumptions are generally realistic, the analysis relies heavily on specific properties of U-Nets. This limits the generality of the results, as U-Nets are already completely replaced by more modern architectures such as Diffusion Transformers (DiTs). The paper would be significantly stronger if the authors discussed how their assumptions extend to DiTs or at least provided quantitative results on such architectures.\n\n2. The paper lacks an analysis of how the dynamic λ coefficients evolve during training. Visualizations or comparisons with fixed λ baselines would help illustrate the mechanism’s behavior and further validate the claimed improvements."}, "questions": {"value": "1. Can the proposed approach be supported by weaker or more general assumptions about the Jacobians? Alternatively, could you provide results or analysis using Diffusion Transformers (DiTs) to demonstrate that the method generalizes beyond U-Nets? Any clarification or quantitative evidence in this direction would be very helpful. \n\n2. Given the practical nature of this work, additional ablations would substantially strengthen the paper—for example, comparing against constant λ values or visualizing how λ evolves during training.\n\nI would be happy to raise my score if the authors address these concerns, as the paper is already practical, easy to follow, and offers a genuinely few-line, plug-and-play improvement. I understand the tight rebuttal timeline and would appreciate any additional clarification the authors can provide."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IjRyh4LaAt", "forum": "GK1v5whh6d", "replyto": "GK1v5whh6d", "signatures": ["ICLR.cc/2026/Conference/Submission1881/Reviewer_Ad82"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1881/Reviewer_Ad82"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656934401, "cdate": 1761656934401, "tmdate": 1762915925803, "mdate": 1762915925803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "fgXnpLFfmr", "forum": "GK1v5whh6d", "replyto": "GK1v5whh6d", "signatures": ["ICLR.cc/2026/Conference/Submission1881/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1881/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763017486477, "cdate": 1763017486477, "tmdate": 1763017486477, "mdate": 1763017486477, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify a limitation in conventional DPO training that updates can increase the reconstruction error of both the winner and loser branches. To address this, they propose Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser’s gradient according to its alignment with the winner’s gradient."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear analysis of the shortcomings of standard DPO and argues that SDPO enables more stable optimization during preference training.\n2. The mathematical derivations are detailed and carefully presented, supporting the approximate solution procedure for Diffusion-SDPO."}, "weaknesses": {"value": "1. The paper lacks a discussion of the computational overhead of computing $\\lambda_{safe}$. For example, there is no comparison with baselines in terms of training memory usage or per-step backward-pass time.\n2. All experiments are trained for only 2,000 steps. It is unclear whether performance has reached the peak for each baseline or whether results are reported before full convergence. Longer-horizon training results would help clarify this.\n3. From Figure 3, the original DPO variant does not appear to exhibit the property described in the paper (“the objective can increase the reconstruction error of both winner and loser branches,” lines 15–16). The paper does not discuss how SDPO improves performance in this case."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZxDVXI3AIr", "forum": "GK1v5whh6d", "replyto": "GK1v5whh6d", "signatures": ["ICLR.cc/2026/Conference/Submission1881/Reviewer_Gmbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1881/Reviewer_Gmbd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849286376, "cdate": 1761849286376, "tmdate": 1762915925333, "mdate": 1762915925333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the reviewers and the area chairs for their valuable time and constructive feedback. Their comments have provided us with important insights that will help us further improve the quality and clarity of our work. After careful consideration, we have decided to withdraw the current submission in order to refine our experiments, strengthen the analysis, and better address the raised concerns. We deeply appreciate the reviewers and area chairs’ efforts."}}, "id": "hbbHEN2Fib", "forum": "GK1v5whh6d", "replyto": "GK1v5whh6d", "signatures": ["ICLR.cc/2026/Conference/Submission1881/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1881/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1881/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763017472822, "cdate": 1763017472822, "tmdate": 1763017472822, "mdate": 1763017472822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Aligning Text-to-image diffusion models with human aesthetic and preference judgments remains difficult. Existing Direct Preference Optimization (DPO) methods have been adapted to diffusion models to improve preference alignment. However, the authors identify a key flaw in this approach: increasing the preference margin (between “winner” and “loser” samples) does not necessarily improve overall image quality.\nIn practice, standard Diffusion-DPO often degrades both winner and loser reconstructions, implying that alignment gains come from worsening the loser rather than improving the winner. This leads to unstable training and even collapse in some cases. To address this, the paper introduces Diffusion-SDPO (Safeguarded Direct Preference Optimization) a modification to DPO that preserves the winner sample’s quality during optimization. The method introduces an adaptive scaling of the loser gradient, determined by how aligned it is with the winner gradient. If the loser’s gradient direction conflicts with the winner’s, its contribution is downweighted."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Insightful Diagnosis of DPO Behavior : The paper makes a clear and valuable observation that simply increasing the preference margin in diffusion-based DPO does not guarantee improved image quality. By identifying that both winner and loser losses can rise during training, the authors uncover a subtle but important failure mode in current preference optimization methods.\n2. Effective Solution : The proposed Diffusion-SDPO introduces a simple modification, adaptive scaling of the loser gradient, that yields substantial empirical gains. Despite its conceptual simplicity and minimal computational cost, the method demonstrates consistent improvements across models and benchmarks, highlighting its strong practical impact."}, "weaknesses": {"value": "The theoretical analysis includes several nontrivial leaps that are not fully justified. \nIn particular, the assumption that $J_w^{\\top} J_l = I$ (identity matrix) appears unrealistic and lacks both empirical and conceptual grounding in the context of diffusion model optimization. \nAssumptions A and B are introduced without sufficient explanation or validation, making it difficult to assess their plausibility.  For example, in assumption A, \"For a fixed t, the noised latents $x_t^w$ and $x_t^l$\n lie in a small neighborhood in latent space\". I do not agree with this. In assumption B, \"They are trained to predict isotropic Gaussian noise at each step. These design choices constrain the singular values of J to stay close to one.\" Predicting isotropic noise means that the outputs  have similar variance in all components. I believe it says nothing about how sensitively each output dimension depends on the parameters."}, "questions": {"value": "1. Could you please include a small empirical study (or a clearer derivation) to justify the near-isometry assumption $J_w J_l^{\\top} \\approx I$. For a toy UNet or a narrow block, estimate Jacobian and report, such as (i) the spectrum of $J J^{\\top}$ and (ii) $\\|J_w - J_l\\|$ when $x_t^w$ and $x_t^l$\nare nearby across several timesteps $t$ and prompts. Even low-rank probes would clarify the frequency and extent to which the assumption holds.\n\n2. If the authors’ claim holds, a compelling validation would be to compare SDPO with baseline DPO methods trained for a larger number of steps more than 2000. In such extended training, baseline methods are expected to continue widening the preference margin but often at the cost of visual fidelity, producing “ugly” or degraded images as shown in Figure 1 despite improvements in preference scores.\nIn contrast, if SDPO effectively constrains the increase of $L_w$, the preferred-image quality should remain stable even with prolonged training. I suggest including both quantitative and qualitative evidence for this effect. Quantitatively, report trends of aesthetic or NR-IQA metrics over extended training steps. Qualitatively, show fixed-seed generations at different checkpoints to visualize whether baseline methods degrade while SDPO maintains perceptual quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RHgOzUGxBN", "forum": "GK1v5whh6d", "replyto": "GK1v5whh6d", "signatures": ["ICLR.cc/2026/Conference/Submission1881/Reviewer_eiGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1881/Reviewer_eiGm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860080435, "cdate": 1761860080435, "tmdate": 1762915925054, "mdate": 1762915925054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses an critical issue in Diffusion-DPO, where the model can widen the preference gap by making both outputs worse, including the preferred one. It adds a simple winner-preserving update that checks how the winner and loser gradients align and scales down the loser’s influence when it would hurt the winner. The method plugs into existing DPO-style training with minimal changes and compute. In experiments on popular text-to-image models, it stabilizes training and delivers consistent gains in preference, aesthetic quality, and prompt alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper identifies and tackles a critical problem in Diffusion-DPO that the optimization can hack the objective by making the lose samples worse than winning samples.\n\n2.\tThe paper proposes an easy rescaling method based on first-order analysis to maintain the win samples quality.\n\n3.\tThe proposed method is validated on various models on different benchmarks."}, "weaknesses": {"value": "1.\tThe key assumption on the near-isometry is fragile. It relies on both near-isometry of self-Jacobian and closeness of Jacobians in two branches, which can break easily. Current analysis is also based on U-Net, which does not include other architectures like DiT.\n\n2.\tA more formal analysis on simplifying the DPO objective to a linear version. It’s not quite clear that the current analysis will still hold in putting back to the sigmoid DPO loss.\n\n3.\tThe performance gain on SDXL over benchmark methods is minimal."}, "questions": {"value": "1.\tFrom SDXL results in Table 6, it can be seen that the proposed method plugged into DMPO consistently achieves a larger increase than plugging into Diff.-DPO and DSPO, why is it this case?\n\n2.\tHow do the authors apply the proposed method to DSPO, where the L^l and L^w is not directly clear in the their objective.\n\n3.\tHow does $\\mu$ collaborate with $\\beta$ in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X4rlxvIoQz", "forum": "GK1v5whh6d", "replyto": "GK1v5whh6d", "signatures": ["ICLR.cc/2026/Conference/Submission1881/Reviewer_wzME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1881/Reviewer_wzME"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970476853, "cdate": 1761970476853, "tmdate": 1762915924914, "mdate": 1762915924914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}