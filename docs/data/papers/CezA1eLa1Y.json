{"id": "CezA1eLa1Y", "number": 22642, "cdate": 1758333872014, "mdate": 1759896855328, "content": {"title": "IncVGGT: Incremental VGGT for Memory-Bounded Long-Range 3D Reconstruction", "abstract": "We present IncVGGT, a training-free incremental variant of VGGT that makes transformer-based 3D reconstruction feasible for long sequences in real-world applications. Vanilla VGGT relies on dense global attention, which causes memory to grow quadratically and requires excessive computation, making it impractical for long-sequence scenarios. Even evolved streaming variants, such as StreamVGGT, still suffer from rapidly growing cache and latency. IncVGGT addresses these challenges from two orthogonal directions: (1) register and fuse overlapping frames into composite views, reducing duplicate tokens, and (2) history-side pruning retains only the top-$k$ most relevant/maximum slots together with the most recent one, bounding cache growth. This incremental and memory-efficient design minimizes computation and memory occupation across arbitrarily long sequences. Compared to StreamVGGT, IncVGGT sustains arbitrarily long sequences with large efficiency gains (e.g., on 500-frame sequences, 58.5$\\times$ fewer operators, 9$\\times$ lower memory, 25.7$\\times$ less energy, and 4.9$\\times$ faster inference) while maintaining comparable accuracy. More importantly, unlike existing baselines that directly run out of memory beyond 300 (VGGT)–500 (StreamVGGT) frames, IncVGGT continues to operate smoothly even on 10k-frame inputs under an 80GB GPU, showing that our design truly scales to ultra-long sequences without hitting memory limits. These results highlight IncVGGT’s potential for deployment in resource-constrained edge devices for long-range 3D scenarios.", "tldr": "", "keywords": ["3D Reconstruction", "memory efficient", "long range", "point cloud"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed785002bd9d8318b9e5abb58b59147e806660a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents IncVGGT, a training-free, streaming variant of VGGT for very long videos under tight memory/compute budgets. It improves scalability via (1) input-side registration & composition, which aligns short windows and fuses overlapping frames into composite views, and (2) history-side cache pruning, retaining only the top-k most relevant cached slots plus the most recent one to avoid key–value growth. Built from StreamVGGT pretrained weights, IncVGGT has substantial latency, memory, and energy reductions compared to StreamVGGT/VGGT. On camera pose and multi-view reconstruction, it performs on par with StreamVGGT but remains below VGGT in accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Scales to very long videos**: Handles sequences up to thousands of frames under tight memory/compute, enabling long-range reconstruction without retraining.\n* **Simple approach**: (i) Registration + composite views cut redundant tokens before attention; (ii) Top-k + recent cache pruning bounds KV growth."}, "weaknesses": {"value": "* **Strong assumptions for input-side composition**. The composition step implicitly requires frames to be in temporal order and to have large inter-frame overlap; if a long sequence contains only partial overlaps (or out-of-order frames), composites cannot be formed and the method effectively falls back to using all frames (akin to StreamVGGT).\n* **Unspecified “un-stitching.”** After composing views, the paper does not explain how to map predictions back to original frames—e.g., recovering per-pixel depth and per-frame camera poses in the native image coordinates—which is critical for practical use.\n* **Lack of long-sequence accuracy benchmarks**. While the paper shows large latency/memory reductions on long sequences, it reports quantitative accuracy only on short-sequence settings; there is no long-range accuracy evaluation (camera pose, video depth estimation), leaving the real effectiveness on very long videos unclear."}, "questions": {"value": "*  After composite-view inference, how do you recover per-frame camera poses and per-pixel depths in each original image? \n* What happens when inputs are out-of-order or have limited overlap so composites can’t be formed—do you fall back to StreamVGGT, and with what cost/accuracy?\n* Could authors provide long-sequence accuracy benchmarks (not just efficiency) to show performance at the scales your method targets (1K, 10K frames)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VDNoAEVyhA", "forum": "CezA1eLa1Y", "replyto": "CezA1eLa1Y", "signatures": ["ICLR.cc/2026/Conference/Submission22642/Reviewer_iPad"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22642/Reviewer_iPad"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528992328, "cdate": 1761528992328, "tmdate": 1762942317452, "mdate": 1762942317452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses VGGT's quadratic memory growth that limits it to 300-500 frames for transformer-based 3D reconstruction. IncVGGT proposes a training-free solution with two strategies: (1) input-side registration fuses overlapping frames into composite views using homography-based alignment before tokenization, and (2) history-side pruning retains only top-k relevant cached slots plus the most recent frame. At 500 frames, this achieves 58.5× fewer operators, 9× lower memory, 4.9× faster inference, and scales to 10k frames with only 1-4% accuracy degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses memory scalability in transformer-based 3D reconstruction, a genuine problem that limits the deployment of foundation models in real-world applications (VR/AR, robotics, autonomous systems).\n- The quantitative gains are substantial and well-documented across multiple dimensions: 58.5× fewer operators, 9× memory reduction, 25.7× energy savings, and demonstrated scalability to 10k frames.\n- The key insight that video redundancy exists at both input (overlapping pixels) and history (cached attention) levels is conceptually clean."}, "weaknesses": {"value": "- Both core components rely on well-established techniques, with little innovation. Input-side registration uses standard homography-based panorama stitching (Brown & Lowe, 2007), while history-side pruning applies straightforward top-k selection to KV caches, similar to existing work in LLMs (H2O, StreamingLLM, and Keyformer).\n- The homography-based registration fundamentally assumes planar scenes or negligible parallax, yet this critical limitation receives only brief acknowledgment (line 190: \"cannot fully account for strong 3D parallax\") without rigorous analysis. The paper provides no quantitative evaluation of when/how this assumption breaks down, no testing on datasets with significant depth variation or non-planar geometry, and no failure case examples or degradation analysis.\n- Tables 4-5 show 1-4% accuracy drops that are dismissed as \"minimal\" without investigating root causes or conditions under which degradation occurs. The paper lacks ablation studies on critical hyperparameters, including span threshold \\lambda, cache size k (only k=5 tested), and window size K, making it impossible to understand the sensitivity to design choices or to optimize accuracy-efficiency trade-offs. Most critically, there is no analysis of error accumulation over the claimed 10k-frame sequences. Do homography estimation errors compound over time, and how does cache pruning affect long-term geometric consistency?"}, "questions": {"value": "- Can you provide ablations showing accuracy vs. efficiency tradeoffs for different cache sizes k ∈ {1, 3, 5, 10, 20}, sensitivity to span threshold \\lambda, and contribution of each component (registration alone vs. pruning alone vs. combined)?\n- You cite VGGT-Long as explicitly addressing kilometer-scale sequences but provide no quantitative comparison. Can you directly compare on the same datasets/sequences and explain why your streaming approach is preferable to their chunk-based method?\n- For 10k-frame sequences, can you provide accuracy metrics plotted against sequence length (e.g., at 1k, 2k, 5k, 10k frames) to demonstrate that errors don't compound over time? How do homography drift and cache pruning affect long-term geometric consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tVtMMT0a0t", "forum": "CezA1eLa1Y", "replyto": "CezA1eLa1Y", "signatures": ["ICLR.cc/2026/Conference/Submission22642/Reviewer_dTAF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22642/Reviewer_dTAF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623724027, "cdate": 1761623724027, "tmdate": 1762942317023, "mdate": 1762942317023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IncVGGT, an incremental and training-free inference framework for theVGGT to enable memory-bounded, long-range 3D reconstruction. The core contribution is a novel two-pronged approach to mitigate the quadratic complexity and ever-growing memory footprint of standard and streaming Transformers. First, an input-side registration and composition module merges overlapping frames into compact composite views, significantly reducing the number of input tokens. Second, a history-side pruning mechanism for the KV cache retains only a fixed-size set of the top-k most relevant historical slots plus the most recent one, effectively converting a linearly growing cache into a constant-size one. \nExperimental results demonstrate that IncVGGT achieves order-of-magnitude improvements in efficiency over state-of-the-art baselines like StreamVGGT, while maintaining comparable accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well writen and intuitive.\n2. Inc-VGGT proposes an elegant two-pronged solution combining input-side frame composition and history-side KV cache pruning.\n3. Inc-VGGT achieves massive efficiency gains with only negligible degradation in accuracy, maintaining performance comparable to the state-of-the-art streaming baseline."}, "weaknesses": {"value": "1. Lack of Qualitative Visualizations: The paper relies solely on quantitative metrics to evaluate reconstruction accuracy. For a vision task like 3D reconstruction, this is insufficient as metrics can sometimes fail to capture important visual artifacts. Providing qualitative visualizations comparing the reconstructed scenes against baselines and ground truth would be essential for a comprehensive assessment of the model's precision.\n\n2. Absence of a Limitations Analysis: The paper does not include a dedicated section or discussion on the limitations of the proposed method. A thorough analysis of potential failure modes—such as scenes with significant 3D parallax where homography is fragile, or low-texture environments where feature matching may fail—would make the paper more complete and provide a clearer picture of its applicability boundaries."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NyZrZUlvGD", "forum": "CezA1eLa1Y", "replyto": "CezA1eLa1Y", "signatures": ["ICLR.cc/2026/Conference/Submission22642/Reviewer_bfom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22642/Reviewer_bfom"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727229291, "cdate": 1761727229291, "tmdate": 1762942316682, "mdate": 1762942316682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents IncVGGT, a training-free incremental variant of VGGT designed to address the quadratic memory growth and excessive computation issues in large-scale visual geometry transformers. The method tackles redundancy from both the input and history sides: on the input side, it performs registration and composition to merge short temporal windows into compact composite views; on the history side, it retains only the top-k historically important slots together with the most recent one for the next step. Experimental results demonstrate notable improvements in both inference time and memory efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow, with clear figures and publicly released code.\n\n- It proposes a simple yet intuitive solution with solid technical contributions.\n\n- The method offers practical value by effectively reducing memory and computation overhead.\n\n- IncVGGT operates in a training-free manner, making it efficient and convenient for deployment."}, "weaknesses": {"value": "- The registration-based redundancy reduction component has been widely applied in the community (e.g., [1]). Moreover, introducing a separate registration-and-composition preprocessing module somewhat compromises the clean and unified design philosophy of VGGT, which originally aimed to eliminate all explicit priors.\nA more elegant approach would be to integrate the registration and composition mechanism directly into VGGT’s internal architecture, preserving its end-to-end structure.\n\n- Limited Novelty: While the contributions toward improving memory and speed are meaningful for industrial and practical applications, the registration-based redundancy reduction and global-local cache pruning techniques are incremental relative to VGGT. The authors themselves describe IncVGGT as an “incremental variant” of VGGT, suggesting that it contributes less conceptual novelty to academic research, even though it provides clear engineering value.\n\n[1] Xiaoshui Huang, Guofeng Mei, Jian Zhang, and Rana Abbas. A Comprehensive Survey on Point Cloud Registration."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YjeeRb9Axg", "forum": "CezA1eLa1Y", "replyto": "CezA1eLa1Y", "signatures": ["ICLR.cc/2026/Conference/Submission22642/Reviewer_EqRK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22642/Reviewer_EqRK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812942691, "cdate": 1761812942691, "tmdate": 1762942316442, "mdate": 1762942316442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IncVGGT, a training-free, incremental variant of VGGT designed for memory-bounded, long-range 3D reconstruction. The method tackles the severe memory and computational scaling issues of existing transformers through a dual strategy: (1) it registers and fuses overlapping input frames into \"composite views\" to reduce input token redundancy , and (2) it implements a global-local cache pruning rule, retaining only the top-k most relevant slots plus the most recent one to bound history cache growth. This approach enables the model to process arbitrarily long sequences (e.g., 10k frames) where baselines fail due to out-of-memory errors , achieving substantial gains in efficiency (memory, speed, and energy) while maintaining comparable accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength of this work is that it directly addresses the most significant barrier to using transformers for real-world 3D reconstruction: prohibitive memory and computational cost on long sequences. The ability to run on 10k-frame sequences while state-of-the-art baselines fail at 300-500 frames is a practical achievement.\n\n- The efficiency improvements are not marginal but substantial, with orders-of-magnitude reductions in operator count, memory usage, and energy consumption. The paper provides thorough experimental validation for these claims, including latency, memory, computation, and energy metrics , which strongly support the method's value.\n\n- The approach is \"training-free\", allowing it to be applied directly to existing models like StreamVGGT. This significantly lowers the barrier to adoption. Crucially, the paper demonstrates that these massive efficiency gains are achieved while \"maintaining comparable accuracy\", with only minimal performance drops in the reconstruction tasks."}, "weaknesses": {"value": "-  The method introduces two critical, hard-coded hyperparameters: the span gate threshold $\\lambda$ and, more importantly, the cache size $k=5$. The entire efficiency-accuracy trade-off hinges on $k$. The paper provides no ablation study to justify $k=5$ or to show the sensitivity of the model's accuracy and memory footprint to this parameter.\n- While the efficiency gains are dramatic, the accuracy tables show a consistent, albeit small, degradation in quality compared to the StreamVGGT baseline. The paper presents this as \"comparable\", but it is a clear trade-off. This accuracy drop is likely a direct result of the \"training-free\" approach, as the model was never trained to be robust to such aggressive input and cache pruning."}, "questions": {"value": "- The global-local cache pruning retains a very small, fixed-size cache ($k=5$). While this bounds memory, how does it affect the model's ability to handle long-range dependencies, such as loop closure or revisiting a previously mapped area? Does this small $k$ effectively limit the model to only local, short-term consistency?\n- The method is applied as a training-free modification to StreamVGGT weights. Have the authors investigated whether fine-tuning the model *with* the proposed input-merging and cache-pruning mechanisms could recover the accuracy lost to this aggressive, non-differentiable pruning? It seems plausible that this would yield a model that is both efficient and as accurate as the baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WwbSAMW0Zp", "forum": "CezA1eLa1Y", "replyto": "CezA1eLa1Y", "signatures": ["ICLR.cc/2026/Conference/Submission22642/Reviewer_AxtP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22642/Reviewer_AxtP"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995222319, "cdate": 1761995222319, "tmdate": 1762942316165, "mdate": 1762942316165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}