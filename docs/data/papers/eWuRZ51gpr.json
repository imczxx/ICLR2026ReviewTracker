{"id": "eWuRZ51gpr", "number": 24831, "cdate": 1758360835200, "mdate": 1759896746353, "content": {"title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression", "abstract": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60\\% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.", "tldr": "We developed SWAN, a new method to shrink the large memory footprint of the LLM KV-cache during inference with small performance loss.", "keywords": ["Transformers", "KV-Cache", "Compression", "Dimensionality Reduction", "LLM Inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1b28ca8c1fe20ada2f76c7b9b73625e4f6965ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to improve memory footprint of the KV-cache, by using an offline orthogonal matrix to rotate and prune the KV-cache. SWAN (Sparse Winnowed Attention) is introduced to perform attention directly on a compressed, sparse KV-cache without (conventional) reconstruction, called \"decompression-free\" as indicated in the paper title. Being free of decompression/reconstruction leads to simultaneous memory and compute savings -- this claim and contribution sound promising!"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. SWAN can achieve up to 50-60% memory savings due to the improvement in memory footprint of KV-cache. Being free of decompression/reconstruction leads to simultaneous memory and compute savings. In other words, unlike other existing decomposition-based KV-cache compression methods, SWAN incurs no (or minimal) computation overhead for reconstruction, and thus save compute as well."}, "weaknesses": {"value": "1. Being free of decompression/reconstruction leads to simultaneous memory and compute savings -- this claim and contribution sound promising! However, if the authors claim to save compute, then they should also demonstrate results of FLOPs (# of floating-point operations), LLM runtime (end-to-end latency), and speed-up (percentage of latency improvement); but these are not thoroughly discussed, nor experimentally analyzed in the paper.\n2. Direct comparisons (with experiments/results) against other related works, especially those requiring reconstruction, should be made. My (the reviewer's) point of view is: LLM inference is memory-bound and a certain degree of computation overhead for reconstruction is not harmful and may even be beneficial for runtime/latency because the fact behind computation overhead for reconstruction is the significant relieve/improvement in memory footprint, and the improvement in memory footprint speeds up LLM inference despite computation overhead for reconstruction.\n3. The authors did not talk much about the prefilling stage. It is not clear whether the prefilling stage may need to adapt to SWAN."}, "questions": {"value": "My questions and suggestions are basically from \"Weaknesses\" as aforementioned.\n1. From Weakness 1: Please demonstrate results of FLOPs (# of floating-point operations), LLM runtime (end-to-end latency), and speed-up (percentage of latency improvement).\n2. From Weakness 2: Please experimentally compare SWAN against other related works, especially those requiring reconstruction.\n3. From Weakness 3: Please address my concern about the adaptation, if any, of the prefilling stage owing to SWAN."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "t6s9yt3JUl", "forum": "eWuRZ51gpr", "replyto": "eWuRZ51gpr", "signatures": ["ICLR.cc/2026/Conference/Submission24831/Reviewer_rx4F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24831/Reviewer_rx4F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863420376, "cdate": 1761863420376, "tmdate": 1762943212452, "mdate": 1762943212452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SWAN (Sparse Winnowed Attention), a fine-tuning-free framework for compressing the KV cache during autoregressive decoding. SWAN first applies an orthogonal rotation (derived from offline SVD on joint Q–K and V–O subspaces) to concentrate information, then prunes each token’s key and value vectors by top-k magnitude, storing them in a sparse format. Attention is computed directly on this hybrid cache (dense recent buffer + sparse history) without decompression. The paper provides a space and compute analysis, including a break-even sequence length for speedups, and evaluates on Llama-3.1-8B-Instruct and OLMoE-1B-7B across GSM8K, MMLU/ARC, and LongBench. Results show that SWAN can maintain full-precision model qu"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ **Decompression-free design:** SWAN allows attention to run directly on a sparse cache, removing the need for reconstruction or merging operations that typically introduce overhead in low-rank or codec approaches.\n+ **Clear, implementable mechanism:** Algorithm 1 precisely specifies runtime steps (project, buffer, prune-to-top-k, append to sparse cache, then hybrid attention), and Fig. 1 clarifies the data path."}, "weaknesses": {"value": "+ **No latency or throughput evaluation:**\nAlthough a theoretical efficiency analysis is provided, no empirical runtime measurements are presented. Wall-clock latency, throughput, or per-step breakdowns (prefill vs. decode) are missing, making it unclear how much real-world speedup SWAN achieves.\n\n+ **No any baseline comparisons:**\nThe paper does not compare against any prior baseline approach. In particular, recent hidden-dimension compression methods such as Palu (low-rank) and EigenAttention (low-rank) are missing. Post-RoPE activations are commonly higher rank; I recommend comparing with these works to assess trade-offs (e.g., reconstruction cost, accuracy drops, memory savings). Meanwhile, ThinK (channel pruning) is also worth discussing.\n\n+ **Limited evaluation scope (long-context tasks):**\nDespite its stated motivation for long-sequence memory efficiency, experiments are largely restricted to short-context benchmarks (e.g., GSM8K, MMLU, ARC). The only long-context coverage is a selected subset of LongBench (e.g., summarization). More comprehensive long-context evaluations (e.g., RULER), ideally with runtime measurements, would better demonstrate SWAN’s effectiveness in its intended regime."}, "questions": {"value": "+ **Handling irregular sparsity:**\nSince each token drops a different subset of channels, how does your implementation manage this irregular sparsity during attention? Do you use per-token CSR-like indexing, and how is it parallelized efficiently on a GPU?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qJXePPST22", "forum": "eWuRZ51gpr", "replyto": "eWuRZ51gpr", "signatures": ["ICLR.cc/2026/Conference/Submission24831/Reviewer_Z7Mp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24831/Reviewer_Z7Mp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967414440, "cdate": 1761967414440, "tmdate": 1762943211321, "mdate": 1762943211321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SWAN (Sparse Winnowed Attention), a decompression-free framework for KV-cache compression in large language model inference.\nSWAN constructs orthogonal rotation matrices offline using singular value decomposition (SVD) over model activations to project Key and Value tensors into a subspace where information is more concentrated. During inference, it maintains a hybrid cache consisting of a sparse, pruned cache for older tokens and a small dense buffer for recent tokens to preserve accuracy.\nThe authors provide theoretical analyses of both space and computational complexity, showing that SWAN can yield significant savings in memory and FLOPs once the sequence length exceeds a predictable threshold.\nEmpirically, they evaluate the approach across mathematical reasoning (GSM8K), commonsense and knowledge benchmarks (MMLU, ARC-Challenge), and long-context understanding (LongBench), demonstrating that SWAN achieves up to 50–60% KV-cache memory reduction with minimal performance degradation."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is clearly written and well-structured, making it easy to follow.\n\n* It introduces an interesting compression approach that converts the KV-cache into a hybrid sparse–dense representation and performs attention computations directly on the compressed cache without decompression.\n\n* The accuracy evaluation is comprehensive, covering a diverse range of tasks—from mathematical reasoning to commonsense understanding and long-context processing—demonstrating the method’s generality."}, "weaknesses": {"value": "* The paper lacks a solid system-level implementation to substantiate its claimed efficiency. The computational savings are analyzed only theoretically, without validation through real runtime measurements. Since the method depends on storing pruned tensors in a sparse (CSR) format, which is typically inefficient unless sparsity is extremely high (>99%), it is unclear whether the reported compression ratios (30–50%)—where accuracy is largely preserved—actually yield any practical speedup.\n\n* The effectiveness of the proposed method appears limited. Most of the retained accuracy comes from the 128-token dense buffer, which merely preserves the most recent tokens. Without this buffer, performance degrades sharply and even collapses on long-context benchmarks. This diminishes the overall contribution, as long-context scenarios are precisely where KV-cache bottlenecks are most critical. Furthermore, the paper omits an important baseline comparison against a pure 128-token sliding window attention, which would clarify how much of the gain comes from SWAN itself versus the buffer.\n\n* The runtime overhead of the proposed approach is not thoroughly analyzed. While the paper presents a limited complexity discussion, it overlooks several potential sources of cost, including (1) applying the projection matrix to Key vectors at each decoding step, and (2) performing the top-k pruning required to build the sparse cache. These steps could introduce non-trivial runtime overhead, yet no empirical evidence is provided to demonstrate that these costs are negligible."}, "questions": {"value": "* What is the baseline accuracy when using only the 128 most recent tokens (i.e., without the proposed sparse cache)?\n\n* How is the compression ratio computed in cases that include a 128-token buffer? To match the overall compression rate of the non-buffered setting, does the method apply more aggressive compression to the older, pruned tokens?\n\n* What is the typical runtime latency introduced by the on-the-fly Key projection and the top-k pruning operations for evicted tokens during inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BSzue1kZIK", "forum": "eWuRZ51gpr", "replyto": "eWuRZ51gpr", "signatures": ["ICLR.cc/2026/Conference/Submission24831/Reviewer_ZvMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24831/Reviewer_ZvMH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977115701, "cdate": 1761977115701, "tmdate": 1762943210772, "mdate": 1762943210772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SWAN introduces a decompression free KV cache compression framework for large language model inference. It uses offline SVD based orthogonal rotations to concentrate important information into fewer dimensions and prunes less important components, allowing attention to operate directly on a hybrid cache composed of a sparse historical cache and a small dense buffer for recent tokens. This approach eliminates reconstruction overhead, reduces both memory and computation costs, and supports runtime adjustable compression levels. Experiments on Llama and OLMoE models show that SWAN maintains near baseline accuracy on reasoning and long context benchmarks while achieving around 50 to 60 percent KV cache memory savings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- eliminates reconstruction overhead by performing attention directly on compressed KV caches.\n- combines a sparse historical cache with a small dense buffer for recent tokens, effectively preserving accuracy."}, "weaknesses": {"value": "- while theoretical compute savings are analyzed, the paper does not provide concrete wall-clock latency or throughput comparisons on modern GPU kernels (e.g., FlashAttention or Triton baselines), leaving practical efficiency uncertain.\n- the claimed compute benefits rely on sparse-dense matvec operations, but these are often inefficient on current GPU hardware; implementation feasibility and actual speedups are not validated.\n- applying the orthogonal projection to queries and keys at each decoding step introduces extra matrix multiplications, and the paper lacks quantitative analysis of this overhead."}, "questions": {"value": "- Can the authors provide actual wall-clock latency and throughput measurements on modern GPUs (e.g., A100, H100) comparing SWAN with dense attention and existing compression methods like KVQuant or GEAR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TWmgKdqVax", "forum": "eWuRZ51gpr", "replyto": "eWuRZ51gpr", "signatures": ["ICLR.cc/2026/Conference/Submission24831/Reviewer_sfm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24831/Reviewer_sfm1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24831/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998248372, "cdate": 1761998248372, "tmdate": 1762943210540, "mdate": 1762943210540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}