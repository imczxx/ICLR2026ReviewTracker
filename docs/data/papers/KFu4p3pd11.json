{"id": "KFu4p3pd11", "number": 6226, "cdate": 1757959470765, "mdate": 1759897928397, "content": {"title": "Masked Generative Policy for Robotic Control", "abstract": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9\\% across 150 tasks while cutting per-sequence inference time by up to 35×. It further improves the average success rate by 60\\% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail. Further results and videos are available at: https://anonymous.4open.science/r/masked_generative_policy-8BC6.", "tldr": "", "keywords": ["Imitation Learning", "Masked Generative Transformer", "Generative Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba89b2a84e9733674e7e6c1395e5f8466d6d0734.pdf", "supplementary_material": "/attachment/65e99b50581d181bb78a8dc15e7f57a187207ce3.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Masked Generative Policy, which is a new framework for visuomotor imitation learning that models robot actions as discrete tokens and leverages masked generative transformers to efficiently generate and refine action sequences. Unlike autoregressive or diffusion generative policies, MGP tries to generate globally coherent future plans and refine them online. It combines MaskGIT-style generation with robotic action modeling. The experimental results demonstrate state-of-the-art performance on Markovian and non-Markovian control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It reframes the policy generation problem as masked generative modeling is new and practical, especially given the latency and horizon challenges in robotics. The tokenization of actions is smart to allow transformer modeling of full sequences.\n- The global coherence maintains long-horizon consistency through token memory. The parallel sampling and selective refinement drastically cut latency, leading to high inference efficiency.\n- The experimental results are comprehensive and demonstrate the effectiveness of the proposed method across simulations and tasks. While diffusion models model smooth distributions and autoregressive models enforce causality, MGP smartly bridges them using mask-and-refine semantics, achieving both speed and robustness."}, "weaknesses": {"value": "- The system design and two-stage training are complex. The VQ-VAE and MGT pipeline introduces extra overhead and possible distribution shift between discrete tokens and true continuous actions.\n- When predicting all tokens at once, it loses the explicit notion of conditioning the next tokens on the current action. In dynamic control, this can lead to physically inconsistent predictions.\n- The model must have enough context to predict consistent future tokens without sequential conditioning. It could work in structured simulation, but may fail with partial observability or noisy real-world sensors where causality exists."}, "questions": {"value": "- Is the pipeline easy to smoothly transfer to real-world tasks? The robustness to sensor noise, delays, or physical contact uncertainty remains a question, especially when it requires a strong encoder and global context.\n- There were few visual rollouts or per-task failure analyses. How token refinement behaves in specific dynamic scenes could be more illustrative.\n- A discussion section on the potential domain mismatch and increased complexity of the proposed two-stage training is helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "40tklppfnP", "forum": "KFu4p3pd11", "replyto": "KFu4p3pd11", "signatures": ["ICLR.cc/2026/Conference/Submission6226/Reviewer_wnrp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6226/Reviewer_wnrp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446191408, "cdate": 1761446191408, "tmdate": 1762918557789, "mdate": 1762918557789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Masked Generative Policy (MGP), a new visuomotor imitation learning framework that models robot control as a masked token-generation problem.\nMGP first discretizes continuous actions with a VQ-VAE tokenizer, then trains a masked generative transformer (MGT) to reconstruct full action sequences from partially masked tokens conditioned on current observations.\n\nTwo inference paradigms are proposed:\n\n- MGP-Short for Markovian, short-horizon tasks: parallel token generation with one or two score-based refinement steps.\n\n- MGP-Long for non-Markovian, long-horizon tasks: predicts the entire trajectory in one pass and adaptively refines uncertain future tokens through posterior-confidence estimation (PCE) as new observations arrive.\n\nExtensive experiments on Meta-World and LIBERO benchmarks show strong gains—up to 35× faster inference and higher success rates (+9% overall, +60% in dynamic or missing-observation settings).\nAblations (MGP-FullSeq, MGP-w/o-SM) validate that PCE-based selective refinement is critical for efficiency and global coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Original idea: creatively transfers masked-generation paradigms (MaskGIT/MUSE) to robotic action synthesis.\n\nTechnical soundness: clearly defined VQ-VAE tokenizer, transformer conditioning, and confidence-guided refinement loop.\n\nEmpirical rigor: evaluated on 150+ tasks across difficulty levels; includes robustness tests (dynamic, missing-observation, non-Markovian).\n\nFair comparison: benchmarks against continuous-action (diffusion/flow) and discrete-token baselines under identical encoders and demos.\n\nAblation insight: MGP-w/o-SM (without score-based masking) confirms that selective refinement improves both efficiency and success rate.\n\nRelevance: unifies the advantages of diffusion (sample quality) and autoregressive (temporal coherence) methods in a parallelizable design."}, "weaknesses": {"value": "Limited analysis of tokenizer sensitivity: performance may depend on the VQ-VAE codebook design, but this is not explored.\n\nHyperparameter transparency: the exact confidence-masking threshold and its effect on refinement stability are not analyzed.\n\nPotential complexity: the two-stage training (tokenizer + policy) increases implementation effort; joint end-to-end training would strengthen the approach."}, "questions": {"value": "How is the confidence-based masking threshold determined? Fixed ratio or adaptive per step?\n\nDoes the posterior-confidence estimation ever over-mask or destabilize refinement when confidence calibration drifts?\n\nHow sensitive is performance to the tokenizer’s codebook size and discretization granularity?\n\nWould an end-to-end jointly trained transformer + VQ-VAE outperform the current two-stage pipeline?\n\nDiscrete tokens normally introduce information loss—what do the authors believe enables MGP’s discrete representation to outperform continuous-action models like Diffusion Policy? Is it the global trajectory modeling, masked refinement dynamics, or some property of the VQ-VAE discretization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DGhm307ov9", "forum": "KFu4p3pd11", "replyto": "KFu4p3pd11", "signatures": ["ICLR.cc/2026/Conference/Submission6226/Reviewer_4ZDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6226/Reviewer_4ZDe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677085043, "cdate": 1761677085043, "tmdate": 1762918557281, "mdate": 1762918557281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a novel imitation learning framework for learning visuomotor policy parameterized by masked generative transformer (MGT), which enables high inference efficiency for closed-loop control while maintaining robustness in long-horizon and non-Markovian tasks. Specifically, two sampling strategies are designed: (1) MGP-Short performs short-horizon sampling and refines action tokens with few iterations for the best performance-efficiency trade-off in Markovian tasks; and (2) MGP-Long samples the full trajectory and adaptively refines tokens with updated observations from the environment to retain global coherence. Experiments demonstrate the strong performance of the proposed methods in Markovian and more challenging tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Unlike diffusion-based policy, which might require external distillation for fast inference speed, MGP puts less stress on iterative sampling for obtaining clean actions, and has high flexibility of test-time adjustment with proposed sampling strategies.\n- MGP-Long iteratively refines the action tokens using the executed actions along with the updated observation to improve trajectory-level coherence, which achieves strong performance in Non-Markovian and dynamic environments, and remains robust to missing observations"}, "weaknesses": {"value": "- Baselines such as diffusion-based policies (e.g. ) as well as VQ-BeT stand out when learning multimodal action distributions, while MGP is also built on top of vector quantization, it is not yet clear how the proposed sampling methods work on tasks with explicit multimodality\n- As all tokens are predicted in parallel, the refinement process can be affected if there are low-quality actions predicted initially with high confidence, causing error accumulation throughout the following iterations. Furthermore, it would be helpful to extend the first ablation studies to investigate how many performance gains can be obtained from more refinement steps, especially in more challenging environments.\n- Please include standard deviations in the table for thoroughness if multiple seeds are used to aggregate the result."}, "questions": {"value": "- Typo: “blcoks” -> “blocks” in line 191\n- In Figure 3, should the unexecuted token “52” at the bottom left be “53” before Posterior-Confidence Estimation\n- In line 269, the authors mentioned four ablation studies were conducted, but in section 4.5, only three of them are elaborated.\n- How many actions are encoded into one discrete token? And would that hyperparameter affect performance on different tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DXBjFrzljW", "forum": "KFu4p3pd11", "replyto": "KFu4p3pd11", "signatures": ["ICLR.cc/2026/Conference/Submission6226/Reviewer_UA2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6226/Reviewer_UA2E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698548305, "cdate": 1761698548305, "tmdate": 1762918556805, "mdate": 1762918556805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Masked Generative Policy (MGP), a novel framework for robot imitation learning that eliminates the inference bottlenecks of diffusion models and the sequential constraints of autoregressive models.\nMGP-Short is specifically designed for Markovian tasks, adapting the masked generative transformer for short-horizon sampling. It demonstrates improved success rates on standard benchmarks while significantly reducing inference time.\nMGP-Long allows for globally coherent predictions over long horizons, enabling dynamic adaptation, robust execution under partial observability, and efficient, flexible execution. It achieves state-of-the-art results in dynamic, observation-missing, and non-Markovian long-duration environments.\nThe authors validated the effectiveness of MGP in multiple simulated environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors conducted a thorough analysis of current action generation methods and proposed MGP to address the latency issues inherent in diffusion-style or autoregressive-style action generation. The paper is clearly articulated and easy to follow. The concept of using MGP to re-predict tokens with low confidence while maintaining those with high confidence is intriguing. Theoretically, this approach could indeed reduce the time consumed in predicting actions."}, "weaknesses": {"value": "1. I acknowledge that the results in the simulated environment are impressive. However, due to the sim-to-real gap, it is often necessary to demonstrate effectiveness in real-world settings within this field.\n\n2. Regarding the confidence score. Could you analyze the situations that might lead to a lower confidence score? Additionally, how can we ensure the accuracy of the confidence score itself?\n\n3. About the MGP-Long settings. In long sequences, certain objects may cause environmental changes due to previous actions. At this point, the predictions may no longer remain globally coherent, and we would need to generate a new action sequence based on the changed objects."}, "questions": {"value": "1. The results in the simulated environment are impressive. However, due to the sim-to-real gap, it is often necessary to demonstrate effectiveness in real-world settings within this field. \n2. How can we ensure the accuracy of the confidence score itself?\n3. Regarding the MGP-Long settings: In lengthy sequences, some objects may lead to environmental changes as a result of prior actions. When this occurs, the predictions may lose their overall coherence, necessitating the generation of a new action sequence that takes into account the modified objects."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JLM2dqBtYF", "forum": "KFu4p3pd11", "replyto": "KFu4p3pd11", "signatures": ["ICLR.cc/2026/Conference/Submission6226/Reviewer_x3HB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6226/Reviewer_x3HB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055382111, "cdate": 1762055382111, "tmdate": 1762918556362, "mdate": 1762918556362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}