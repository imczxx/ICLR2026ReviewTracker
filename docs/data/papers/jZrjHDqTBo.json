{"id": "jZrjHDqTBo", "number": 11473, "cdate": 1758199985002, "mdate": 1759897573359, "content": {"title": "MLLMCLIP: Feature-Level Distillation of MLLM for Robust Vision-Language Representations", "abstract": "Large-scale pretrained vision–language models, such as CLIP, have become the backbone of modern zero-shot recognition.\nDespite their strong generalization ability, these models often struggle with compositionality, particularly in understanding attribute-object combinations and relational structures.\nRecent studies mitigate this issue by augmenting training with synthetic hard negatives generated by large language models and text-to-image models.\nYet, this strategy relies on separate expert models, introducing a sequential generation pipeline with quality-control overhead and resulting in a disjointed source of multimodal understanding.\nTo overcome these limitations, we propose MLLMCLIP, a feature-level distillation framework that bypasses synthetic data generation by directly transferring multimodal knowledge from Multimodal Large Language Model~(MLLM).\nOur framework addresses the key challenges of cross-architecture distillation with three core contributions:\n(1) a question-answering-based protocol to select the teacher MLLM, \n(2) an attention-based method to identify salient teacher tokens, and\n(3) the successful adaptation of Centered Kernel Alignment for stable knowledge transfer. \nMLLMCLIP achieves state-of-the-art performance on 9 out of 11 compositionality benchmarks, while also yielding significant improvements in general-purpose tasks, such as zero-shot classification and image-text retrieval.", "tldr": "", "keywords": ["MLLM", "CLIP", "Distillation", "Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc0c356122b2c5628ea96e4dc424bd7b5784eb19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper distills an MLLM’s compositional reasoning ability into a CLIP encoder by selecting the most attended decoder token from each teacher layer and aligning student features with CKA, avoiding synthetic negatives while preserving dual-encoder efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem framing and practical motivation: improve compositionality without a synthetic data pipeline, and keep an efficient encoder at inference.\n2. This method achieves SOTA on multiple compositionality benchmarks and consistent gains in zero-shot classification and retrieval; ablations highlight attention-based token selection and CKA as key to stable cross-architecture distillation."}, "weaknesses": {"value": "1. The method is a careful assembly of known parts (KD + attention heuristics + CKA), rather than introducing a fundamentally new learning objective or architecture. The combination feels unsurprising with limited insigh.\n2. The attention-based “pick one token per layer” strategy is plausible but quite heuristic. No qualitative analysis shows which tokens are chosen or whether they align with compositional elements. \n2. Qwen2.5-VL-3B is chosen for performance/efficiency, but do not provide a systematic “teacher quality vs student skill” study (e.g., 3B vs 7B vs 8B). Would better teachers yield proportionally better students?\n3. Claiming to “bypass generation pipeline inefficiency” is fair, but per-sample teacher forward passes during training are still expensive. A training-time compute comparison vs synthetic-negative pipelines would clarify the practicality."}, "questions": {"value": "1, Why restrict to a single token per layer? Have you tried:\nTop-k tokens by attention, attention-weighted pooling, or alternative saliency signals (e.g., gradient times input)?\nSelecting separate tokens for image-specific vs text-specific supervision at each layer?\n2. How sensitive is CKA distillation to batch size and distribution? Can you stabilize CKA for smaller batches via cross-batch memory or running-estimate Gram matrices?\n3. How robust are your results across different student backbones (e.g., RN50, ViT-B/16, ViT-L/14) and larger pretraining corpora?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8TgcdEKBoE", "forum": "jZrjHDqTBo", "replyto": "jZrjHDqTBo", "signatures": ["ICLR.cc/2026/Conference/Submission11473/Reviewer_Xxw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11473/Reviewer_Xxw5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743788171, "cdate": 1761743788171, "tmdate": 1762922578317, "mdate": 1762922578317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MLLMCLIP, a CLIP model that distills knowledge from a much larger MLLM (Qwen2.5-VL-3B). Qwen2.5-VL-3B is selected among other multimodal architectures as it performs the best in a modified version of the SugarCrepe benchmark. Direct and relational similarity losses are explored for distillation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper shows that some of the features in MLLMs are beneficial for simpler models as CLIP."}, "weaknesses": {"value": "Minor Flaws\n\n1. Use Self-contained captions: captions for figures and tables should provide an outline of the most relevant information in the figure or the table.  Figure 2 and tables 1, 2, 3, 5 and 6  provide little information.\n2. Figure 2 is hard to parse and several of its elements are not explained in the caption and the text. For example what exactly is the color code, and what is the overall flow of information in this figure. Aux^{t} and Aux^{I} seem to be relevant elements in MLLMCLIPm, but they are not shown in the figure. The same happens with z^{I} and z^{T}\n3. Line 187 what final-layer representations do the authors refer to? CLIP layers? MLLM layers? a specific  component layer?\n4. Equation 2 and line 192 define R^{D} and R^{d} . I can't tell if these are two different dimensions or this is just a typo.\n5. Revisit the notation in line 260 and 261, a variable name with 2 super index and two subindexes is overly complicated and hard to recall \n6. The assertion in 221 must be toned down, as outlined later in section 4.4 not every token contains multi-modal information.\n\nRelated work\n\n7. Related work should be re-writtent to directly address the most related world (i.e. those works who augment CLIP by means of distillation) and to briefly discuss the differences or advantages of MLLMCLIP. \n8. The assertion “Our framework is agnostic to these architectural choices, capable of leveraging the multimodal understanding from both encoder-based and encoder-free MLLMs as a teacher.” is unsubstantiated, MLLMCLIP was only validated with Qwen2.5-VL-3B, and there is no empirical evidence that it can work across multiple designs of MLLMs.\n\nTeacher model selection\n\n9. Could the authors provide additional details on the experimental setup for the model selection, for example total number of questions, random baselines. Section 3,1 seems to imply that the entire SugarCrepe benchmark was used for this process.\n\n10. Finally, the Qwen2.5-VL-3B is selected as it has the best performance in the SugarCrepe benchmark. It is expected that a distillation of this model should also have high performance on it. For fairness the SugarCrepe benchmark should be removed from the empirical evaluation.\n\nFlawed Empirical evaluation:\n\n11. This paper does not provide a direct and fair comparison against the state of the art. While LaCLIP, NegCLIP, FSC-CLIP and TripletCLIP retrain or fine-tune from augmented data, MLLMCLIP distills the feature representation from a much larger MLLM. These two tasks are fundamentally different.\n\n12. At training time MLLMCLIP benefits from the learned feature representation of a 3 Billion model that's been trained over 4.1 trillions of language tokens, and a large set of multi-modal data comprising captions and instructional data, a significant portion of this data undergoes several stages of curation and clean-up. The selected baselines fine-tune over a fraction of the original CLIP training data without the presence of another model and most of the curation and extension is done automatically. When compared to MLLMCLIP, every baseline is at a significant disadvantage regarding the number of training data points, the size of the architectures involved at training time, and the level of data curation.\nGiven the significant mismatch in the training of the models, I don't think any of the results in tables 1,2 and 3 offers a fair comparison.\nIf the authors focus on distillation, there are far more suitable baselines, for example, [A], [B] and [C] already propose to distill larger CLIP models and text to image models.\n\n[A] Yang, Chuanguang, et al. \"Clip-kd: An empirical study of clip model distillation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024\n\n[B] Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP (Basu et al., EMNLP 2024)\n\n[C]Chen, Yifan, et al. \"Comkd-clip: Comprehensive knowledge distillation for contrastive language-image pre-traning model.\" arXiv preprint arXiv:2408.04145 (2024).\n\nPotential Data Leakage\n\n13. The training set of Qwen2.5-VL-3B is not publicly available, it is possible that the validation data contained in the selected benchmarks was previously seen by Qwen2.5-VL-3B at training time (specially datasets with captioned images as COCO and Flickr-30K). As the authors resort to feature distillation, MLLMCLIP has potentially distilled features obtained from the datasets used in tables 1, 2 and 3. This is yet another unfair advantage for MLLMCLIP.\n\nLimited Novelty\n\n14. Finally, I don't see a clear contribution or innovation from this paper. None of the selected distillation strategies is novel, and the model selection seems to be a rather simple adaptation of the SugarCrepe benchmark. The improved results in the benchmarks are better attributed to the large scale of the distilled model and its training data, the unfair baseline selection, and the possible data leakage on the evaluation benchmarks."}, "questions": {"value": "1. I would like to know how the authors can find corresponding language tokens between the LLM in Qwen2.5-VL-3B and the language model in CLIP. My understanding is that Qwen2.5-VL-3B has about 151k unique tokens while CLIP has about 50K. I’ll be surprised if an individual sentence has the exact same amount of tokens in both models. \n2. In a similar spirit does the patch size in the visual embedding of Qwen2.5-VL-3B and the patch size in CLIP visual embedding match? Do the authors perform some upsampling to obtain matching visual patches?\n3. Line 353, Could the authors explain why all the baselines (including CLIP) must be re-implemented? What design choice had to be aligned to better compare with  MLLMCLIP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AqIzspfNFf", "forum": "jZrjHDqTBo", "replyto": "jZrjHDqTBo", "signatures": ["ICLR.cc/2026/Conference/Submission11473/Reviewer_LKhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11473/Reviewer_LKhn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938886843, "cdate": 1761938886843, "tmdate": 1762922577865, "mdate": 1762922577865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a student-teacher distillation framework, MLLMCLIP, where CLIP is considered a student and features are distilled from an MLLM to improve CLIP's performance on compositionality and zero-shot retrieval tasks. The paper shows experiments on different benchmarks and provides a variety of ablations to support their claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to understand.\n2. The authors provide results over a variety of compositionality benchmarks and perform ablation to strengthen the design choices in MLLMCLIP.\n3. The idea to directly use MLLM features as knowledge from the teacher model is novel."}, "weaknesses": {"value": "1. It’s a bit unclear why performing distillation from synthetic data generated using an MLLM is inefficient. Please provide more results or an explanation to support this claim.\n2. For token selection, it’s a bit unclear why only the maximum attention token would be suitable as a teacher token. Can you show what happens when you increase the number of teacher tokens by also incorporating lesser attention tokens?\n3. It's also unclear if the performance of the student model really depends on how the teacher model performs on the multimodal prompt? Please provide results for MLLMCLIP when other teacher models is used\n4. No qualitative examples comparing MLLMCLIP with other methods.\n5. Are the “Image End” and “Text End” taken from assumed to be part of MLLM’s vocabulary or added separately? If added separately, does the MLLM require some fine-tuning for these new tokens?\n6. For MLLMs like LLaVA-v1.5, where the vision encoder is already a CLIP model, would such an MLLM be of any benefit as a teacher for the same CLIP-based student? If not, please mention what kind of MLLMs the MLLMCLIP distillation would be helpful for?\n7. Does performing distillation with MLLMCLIP require more computation than other synthetic data generation-based works?"}, "questions": {"value": "Please refer to the weaknesses for the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LYJ6hjrZFm", "forum": "jZrjHDqTBo", "replyto": "jZrjHDqTBo", "signatures": ["ICLR.cc/2026/Conference/Submission11473/Reviewer_gDAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11473/Reviewer_gDAq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997706600, "cdate": 1761997706600, "tmdate": 1762922577494, "mdate": 1762922577494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MLLMCLIP, a feature-level distillation framework that enhances vision–language models like CLIP by directly transferring multimodal knowledge from a Multimodal Large Language Model (MLLM). The method maps representations of different layers of the MLLM to layer representation of the CLIP models and optimizes their alignment with a Centered Kernel Alignment for knowledge transfer. MLLMCLIP achieves good results on 9 out of 11 compositionality benchmarks and improving general zero-shot tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Direct distillation of representations from MLLMs into more efficient CLIP models is an important research direction as it circumvents expensive synthetic data generation. This is a generally underexplored direction.\n- The chosen architecture and loss function are reasonable. The CKA loss is shown to provide benefits over other options, such as MSE, Cosine, KL/JS divergence.\n- The results demonstrate an improvement over alternative approaches for equal training and architecture settings."}, "weaknesses": {"value": "- There is missing literature that is relevant to this line of research.\n  * There are CLIP variants that focus on more fine-grained understanding such as DreamLIP [A], or FLAIR [B]. Both use synthetic data which could serve as a comparison to this alternative direction.\n  * There is an increasing amount of work that tries to turn MLLMs into embedding models, such as VLM2Vec [C], or LamRA [D]. It would be beneficial to position the paper in contrast to this approach as both directions try to achieve the same goal.\n\n- Some method design choices could be better justified.\n  * It is not obvious why any of the tested layer selection strategies would be optimal for distillation. It was not tested if all CLIP layers require alignment, or if the same MLLM layer could be chosen instead of a set of sequential/strided layers. One sensible alternative would be to simple distill one MLLM layer representation to one (last or penultimate) layer of CLIP.\n  * The texts describes the teacher embedding for the image $h^{\\text{Teach},I}$ separately from the text one $h^{\\text{Teach},T}$. It is not explained how they are chosen in practice. Are they simply identical? If so, why make this distinction?\n\n- The absolute performance of the presented results is quite poor.\n  * All presented results on the different tasks lack behind severely the state-of-the-art. For example for Flickr-30K R@1 retrieval, MLLMCLIP reports 18.9 (T2I) and 26.5 (I2T), which is significantly lower than the results from FLAIR (81.1, 94.7) and VLM2Vec (88.1, 97.6). It is also not clear why the base CLIP performance is so low when OpenCLIP much better performance (71.9, 87.5, taken from [B]).\n  * It is not clear whether the paper reproduces all the results of the baselines or if existing checkpoints were used.\n  * There is no reasoning given for the chosen model size of ViT-B/32. Typically ViT-B/16 is much more competitive for the same number of parameters. In general, the paper could have much higher impact if it tried to get closer to SOTA results, although I can understand that resources can be a limiting factor.\n\n- The teacher token selection ablation (Tab. 4) raises questions.\n  * How is the teacher and text token chosen for \"Attention-based Selection\"? (related to earlier remark on missing clarity)\n  * When both image and text teacher is \"None\", does it mean that no distillation is performed and only the CLIP loss is used? In this case, the performance improvement, while present, is rather small for the compositionality benchmarks, although this was a core motivation for this approach.\n\nMinor:\n- Fig. 1 and Sec. 3.1 would better fit into the start of the experiments section as they have little to do with the method. This will become increasingly irrelevant as MLLMs become stronger.\n\n[A] Zheng et al., DreamLIP: Language-Image Pre-training with Long Captions, ECCV 2024  \n[B] Xiao et al., FLAIR: VLM with Fine-grained Language-informed Image Representations, CVPR 2025  \n[C] Jiang et al., VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks, ICLR 2025  \n[D] Liu et al., LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant, CVPR 2025"}, "questions": {"value": "Please refer to the weaknesses section. Questions of particular interest are:\n- How do you position MLLMCLIP with respect to [A,B,C,D]?\n- What is the reasoning behind having a distillation loss on all CLIP layers? Is there any empirical evidence that suggests this is the best option? \n- How do you explain the performance discrepancy between your presented results and results from the literature?\n- Did you reproduce the baselines or use model checkpoints?\n- Why was ViT-B/32 chosen? What prevents you from scaling to more competitive sizes/architectures (e.g. ViT-B/16)?\n- How do you explain that the core distillation contribution does not improve compositionality results as much as classification and retrieval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kqMQOr5Sh0", "forum": "jZrjHDqTBo", "replyto": "jZrjHDqTBo", "signatures": ["ICLR.cc/2026/Conference/Submission11473/Reviewer_aah4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11473/Reviewer_aah4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084885209, "cdate": 1762084885209, "tmdate": 1762922577182, "mdate": 1762922577182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}