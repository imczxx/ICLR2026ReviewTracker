{"id": "0fNQCOWKc1", "number": 4064, "cdate": 1757594291015, "mdate": 1759898054936, "content": {"title": "UniVA: Universal Video Agents towards Next-Generation Video Intelligence", "abstract": "Recent breakthroughs in visual AI have largely treated video tasks in isolation, with specialized models excelling at generation, editing, segmentation, or understanding individually. We introduce \\textbf{UniVA}, a multi-agent framework for universal video intelligence that unifies video understanding, segmentation, editing, and generation in complex workflows. UniVA employs a Plan-and-Act dual-agent architecture: a planner agent decomposes high-level user requests into a sequence of video-processing steps, and executor agents carry out these steps using specialized modular tool servers (for video analysis, generation, editing, object tracking, \\textit{etc.}). Through a multi-level memory design (global knowledge, task context, and user-specific memory), UniVA supports long-horizon reasoning and inter-agent communication while maintaining full traceability of each action. \nThis design enables iterative and composite video workflows (\\textit{e.g.}, image $\\rightarrow$ video generation $\\rightarrow$ video editing $\\rightarrow$ object segmentation $\\rightarrow$ content composition) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are open-sourced to the community, with the aim of catalyzing next-generation video intelligence research.", "tldr": "", "keywords": ["agent", "video intelligence", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30f5405c35d81fe67ee2f85b1ed906a3932f1885.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a system called UniVA to help with complex video tasks. UniVA is designed to combine many different video tasks, such as understanding, cutting, editing, and creating videos, into one workflow. UniVA uses two types of AI agents to work. A planner agent receives a high-level request from a user and breaks it down into smaller, manageable steps. Then, executor agents take over to complete each of these individual tasks using various specialized tools. UniVA is built with a three-level memory design to handle long and complex tasks. The authors also created UniVA-Bench, a new tool for evaluating how well AI systems can handle complex, multi-step video assignments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to understand. The motivation is clear.\n2. The proposed system is complete. This needs substantial engineering efforts."}, "weaknesses": {"value": "I am not an expert in designing video agent benchmarks, and I do not check the appendix. My criticism will focus on the technical parts.\n1. This paper contains many over-claimed points, for example, next-generation video intelligence. I do not fully agree with the definition of next-generation video intelligence proposed in this paper.\n2. Roughly speaking, the proposed agent works in a ReAct pattern. No significant planning contribution is presented. As for the memory part, the designed working memory is trivial and brings limited insights to the community.\n3. As for the competitors, I am only familiar with the understanding part. Important baselines, such as VideoAgent and VideoAgent2, are missing. Only simple MLLMs are used for comparison, which is very unfair.\n4. So, my point is that I cannot find truly new things brought to the video agent community with substantial experiments to prove them.\n\nTo some extent, HuggingGPT with a ReAct pattern could achieve the same effect as the proposed framework with some engineering efforts. I do appreciate the engineering efforts the authors have made, but the new insights are not enough for ICLR."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ipdSJ7q4lJ", "forum": "0fNQCOWKc1", "replyto": "0fNQCOWKc1", "signatures": ["ICLR.cc/2026/Conference/Submission4064/Reviewer_cun9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4064/Reviewer_cun9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760835339915, "cdate": 1760835339915, "tmdate": 1762917161520, "mdate": 1762917161520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UniVA, a multi-agent framework for unified video intelligence that integrates understanding, segmentation, editing, and generation. UniVA employs a Plan–Act dual-agent architecture, where a planner decomposes user goals into subtasks, and an executor carry out these steps using specialized modular tool servers. Besides, UniVA supports long-horizon reasoning and inter-agent communication while maintaining traceability of each action through a three-level memory system (global, task, user). To evaluate the framework, the authors introduce UniVA-Bench, a benchmark designed for multi-step video tasks spanning understanding, editing, segmentation, and generation. Experiments show that UniVA demonstrates competitive performance across a wide array of video tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is well organized with clear diagrams and easy to follow.\n\n2.\tUniVA-Bench provides a systematic evaluation suite with new “agentic metrics” (wPED, DepCov, ReplanQ), filling a gap for multi-agent video systems.\n\n3.\tThe experiments and visualizations are reasonable and well done."}, "weaknesses": {"value": "1.\tWhile the integration is strong, most modules (e.g., MCP protocol, planning agents, tool servers) are adaptations of existing frameworks rather than new technical inventions.\n\n2.\tWhile the ablation in Figures 6 and 7 shows improvements when incorporating user and task memory, the gains are relatively small. \n\n3.\tMore ablations on user and task memory and comparisons with simpler orchestration baselines could be expanded to strengthen claims.\n\n4.\tThe paper does not analyze latency, scalability, or hardware requirements, which are important for practical deployment."}, "questions": {"value": "1.\tHow does the Planner dynamically adjust its plan when tool outputs deviate from expectations? Is there an explicit feedback or self-correction mechanism?\n\n2.\tHow scalable is UniVA to hundreds of concurrent video tools or longer-than-minute sequences？Does the MCP protocol become a bottleneck?\n\n3.\tCould the authors provide more details on the computational efficiency of the end-to-end UniVA pipeline in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FG21xfMCM3", "forum": "0fNQCOWKc1", "replyto": "0fNQCOWKc1", "signatures": ["ICLR.cc/2026/Conference/Submission4064/Reviewer_YubW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4064/Reviewer_YubW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769619402, "cdate": 1761769619402, "tmdate": 1762917161163, "mdate": 1762917161163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on the goal of creating a universal agent with broad-based video understanding and generation capabilities, sufficient for video creation workflow. To this end, the authors propose UniVA, a unified video agent system that incorporates separate planning and executing agents that interact with tool servers. The system also incorporates various forms of memory to ensure proper context. The authors evaluate their approach on various video understanding tasks, and they also present a novel benchmark named UniVA-Bench for future research."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's goals are ambitious: A universal system for general video understanding and synthesis. They demonstrate a plausible system with promising results on some tasks.\n\n2. The paper is well written and easy to understand. \n\n3. The benchmark suite (UniVA-Bench) can be used for further research in the community."}, "weaknesses": {"value": "1. The approach is very complex, and while some results on tasks are promising, there is still room for better performance given the high complexity of the approach.\n\n2. It is unclear if there are any novel methods or models presented in the paper. This reads almost like a systems architecture paper that combines a number of prior models into one system. \n\n3. More qualitative examples would be helpful in understanding the performance of the approach."}, "questions": {"value": "1. Could the authors please elaborate on the contributions of the paper? Is this more of a systems architecture paper?\n\n2. Would it be possible to include more qualitative examples in the paper? \n\n3. Are there any ways the systems architecture could be simplified without sacrificing performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KV4GUKD9jH", "forum": "0fNQCOWKc1", "replyto": "0fNQCOWKc1", "signatures": ["ICLR.cc/2026/Conference/Submission4064/Reviewer_5atz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4064/Reviewer_5atz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859954813, "cdate": 1761859954813, "tmdate": 1762917160448, "mdate": 1762917160448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniVA, a unified multi-agent framework designed to handle complex video tasks—including generation, understanding, editing, and segmentation—through a Plan-Act architecture enhanced with multi-level memory and modular tools integrated via the Model Context Protocol (MCP). The authors also introduce UniVA-Bench, a dedicated benchmark for evaluating multi-step video workflows."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Built on the Model Context Protocol (MCP), UniVA supports a wide range of video, non-video, and non-AI tools in a modular and plug-and-play manner, enabling flexible and extensible task execution.  \n- The framework demonstrates comprehensive capabilities across diverse video-related tasks, integrating multiple functionalities into a unified pipeline."}, "weaknesses": {"value": "- The paper's core technical components—the Plan-Act architecture and multi-level memory mechanism—are based on well-established paradigms in the agent literature, while the Planner itself is implemented using an existing LLM framework, thus providing limited novel technical insight specifically tailored to video intelligence.  \n- Evaluations in the benchmark primarily compare against non-agentic models, failing to adequately highlight UniVA’s advantages over recent video-specific agent systems.  \n- The small scale of UniVA-Bench—using only 10 videos per task—may undermine the generalizability and robustness of the experimental conclusions.  \n- There is a lack of in-depth analysis on failure modes, such as planning errors, tool invocation conflicts, or memory retrieval issues."}, "questions": {"value": "- How does UniVA perform under real-time or low-resource conditions, and what are the computational requirements for each module?  \n- How does the Planner handle ambiguous or underspecified user instructions, and what mechanisms are in place for recovery?  \n- Can UniVA support interactive video tasks, and if so, how is dynamic user input incorporated during execution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vsMcSHUYEa", "forum": "0fNQCOWKc1", "replyto": "0fNQCOWKc1", "signatures": ["ICLR.cc/2026/Conference/Submission4064/Reviewer_27tr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4064/Reviewer_27tr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964225210, "cdate": 1761964225210, "tmdate": 1762917160147, "mdate": 1762917160147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}