{"id": "3V559xWIWc", "number": 16563, "cdate": 1758266140271, "mdate": 1759897232541, "content": {"title": "SALF & TALF: Optimized Loss Function and Drafting for Tree-based Speculative Decoding", "abstract": "Speculative decoding (SpD) has emerged as a promising approach to accelerate the slow autoregressive inference of large language models (LLMs).\nSpD leverages a lightweight draft model to propose candidate tokens, which are then verified in parallel by the target LLM.\nRecent advances in tree-based SpD significantly improve efficiency by drafting token trees, enabling the verification of multiple sequences at once.\nGiven its strong empirical performance reported across numerous studies, tree-based SpD is rapidly becoming dominant.\nHowever, existing draft model training methods overlook the tree structure when defining the training objectives, causing their training and inference distributions to become misaligned.\nWe address this limitation with a tree-aware loss function (TALF) that explicitly incorporates the tree structure into draft model training.\nUsing trees generated by the target LLM, TALF aligns the draft model’s predictions with the target across all branches, mitigating the misalignment.\nFurther, we improve the tree construction process in drafting with stopping at low further gains (SALF).\nAs drafting iterations search for potential high-probability tokens to add to the tree, \nwe estimate aggregate probability gains.\nThis estimate guides the stopping criterion for drafting, enabling us to balance computational cost against draft quality for maximum performance.\nTogether, SALF \\& TALF deliver 15.6--39.4\\% and 6.5--24.4\\% end-to-end speedups over state-of-the-art SpD methods, EAGLE-2 and HASS, without altering the draft model architecture.", "tldr": "", "keywords": ["speculative decoding", "large language model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1a3614930889648b144442b94582b0d2af75c5a.pdf", "supplementary_material": "/attachment/027ff95cad4710dd2e0b312f3e92f61dc0a61638.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on proposing a tree-aware loss function that explicitly incorporates the tree structure into draft model training to improve the speedup of current SPD. TALF aligns the draft model’s predictions with the target across all branches, mitigating the misalignment. They also improve the tree construction process in drafting with stopping at low further gains. The results show that they can deliver speedup over Eagle2 and HASS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is technically sound and easy to understand.\n\nThe experimental results show the effectiveness of the proposed method."}, "weaknesses": {"value": "The paper lacks comparison to state-of-the-art methods such as Eagle3.\n\nThe paper focuses on generating tree structure and improve the overall MAT to speedup the large language model. However, they only conduct experiments with HuggingFace Transformers framework. Here comes a problem that the method may not have such speedup on the popular inference framework such as vLLM. In fact, HuggingFace Transformers framework does not optimize the speed of LLMs very well, which makes the ratio of the latency of tree generation process smaller. When using vLLM framework where the operations in LLMs are optimized very well, the tree generation process will take more time and reduce the speedup. The author should verify their method on such inference frameworks to show that their method is actually useful in reality."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3jgmIuOHVY", "forum": "3V559xWIWc", "replyto": "3V559xWIWc", "signatures": ["ICLR.cc/2026/Conference/Submission16563/Reviewer_6X3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16563/Reviewer_6X3v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604141489, "cdate": 1760604141489, "tmdate": 1762926644480, "mdate": 1762926644480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a strong contribution to speculative decoding by addressing a clear training-inference mismatch in tree-based methods. The proposed TALF and SALF are novel, well-motivated, and demonstrate significant empirical improvements over state-of-the-art baselines. The work is timely, well-executed, and merits acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Well-Motivated Problem Formulation:​​ The paper convincingly identifies a critical yet overlooked issue: the misalignment between sequence-based training and tree-based inference in speculative decoding. The motivation is powerfully supported by empirical evidence showing the poor calibration of existing draft models on lower-ranked tokens.\n2. ​Effective and Orthogonal Solutions:​​ The two contributions, TALF (training) and SALF (inference), address distinct parts of the pipeline and are shown to be complementary. \n3. ​Extensive and Convincing Empirical Validation:​​ The experiments are thorough, evaluating multiple models (Llama2-7B, Llama3-8B, DeepSeek-R1), diverse tasks, and different sampling temperatures."}, "weaknesses": {"value": "1. Inadequate Justification for Omitting Feature Loss:​​ The decision to remove the feature regression loss in TALF, a key component of EAGLE and HASS designed to prevent feature drift, is not sufficiently justified. \n\n2. Limited Discussion on TALF Precomputation and Generalization:​​ The paper lacks details on the computational cost of precomputing draft trees with the target LLM for TALF training. Furthermore, it should be discussed whether fixing the tree structures from a static dataset (ShareGPT) might limit the draft model's ability to generalize to unseen prompt distributions or dynamic tree-building strategies during inference.\n\n3. Ambiguity in SALF Threshold Interpretation:​​ While the SALF threshold is shown to be effective, its intuitive meaning is somewhat ambiguous. A more detailed interpretation of what the threshold value represents in terms of expected probability gain would aid in understanding and practical tuning."}, "questions": {"value": "1. TALF Generalization:​​ The trees used for TALF training are precomputed on a specific dataset. How does the performance of a TALF-trained model generalize to prompts or domains significantly different from its training data? Is there a risk of overfitting to the specific tree structures generated from ShareGPT?\n\n2. ​Feature Alignment Evidence:​​ Can the authors provide quantitative evidence to demonstrate that a TALF-trained model maintains feature alignment with the target LLM despite the removal of the explicit feature regression loss ?\n\n3. Adaptive SALF Threshold:​​ The SALF threshold is a fixed hyperparameter. Have the authors explored making it adaptive based on runtime statistics to dynamically balance quality and overhead across different stages of generation or tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CDZX649OZi", "forum": "3V559xWIWc", "replyto": "3V559xWIWc", "signatures": ["ICLR.cc/2026/Conference/Submission16563/Reviewer_QFQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16563/Reviewer_QFQX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761146305778, "cdate": 1761146305778, "tmdate": 1762926644113, "mdate": 1762926644113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TALF, a tree-aware training loss that aligns a draft model’s predictions with the target LLM across all nodes of a dynamically generated tree, and SALF, an inference-time tree-construction algorithm that stops expanding branches once the expected probability gain falls below a threshold; together they eliminate training-inference misalignment and cut drafting overhead, yielding 15–39 % end-to-end speedups over state-of-the-art speculative decoders without harming output quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed TALF incorporates tree structure into training, improving alignment across all branches, especially low-probability ones. \n2. SALF reduces drafting overhead by early stopping, improving end-to-end latency without significantly hurting acceptance length. \n3. Together, the TALF and SALF work well with different LLMs (e.g., Llama-2, Llama-3, DeepSeek) and tasks (e.g., MT-bench, HumanEval, GSM8K)."}, "weaknesses": {"value": "1. As mentioned in Line.245, the target model is employed to fix the tree structure in advance. However, the draft model is used to generate the tree structure in SALF, which will incur the inconsistence between training and inference. The detailed computational cost of SALF is expected to provide and what about updating the tree structure after the training is stable (such as half of the total epochs) if the computational cost is acceptable.\n2. SALF uses a manually set threshold (th=0.6 by default), which may need tuning for different models or tasks. In Sec. 4.4, th=0.5 is best for Deepseek-R1-Distill-Llama-8B. If th=0.6 is claimed to be better, proper experiments should be presented."}, "questions": {"value": "Could the proposed method compared to Eagle-3 or combined with Eagle-3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pouVsuj3s5", "forum": "3V559xWIWc", "replyto": "3V559xWIWc", "signatures": ["ICLR.cc/2026/Conference/Submission16563/Reviewer_1He7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16563/Reviewer_1He7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618893325, "cdate": 1761618893325, "tmdate": 1762926643712, "mdate": 1762926643712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an inference-aware drafter training method by supervision on the tree-decoded tokens of a drafter to align with the target model. Specifically, authors propose a new loss function (TALF) which is a cross-entropy sum for all tree nodes of the drfater model and the new tree-construction mechanism (SALF) which imposes conditional stopping-criterion for reducing the drafter overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces inference-aware training of the drafter, which is novel and makes sense in SD literature given that latest method often depends on tree-decoding of the drafter. \n* Experiments are conducted with multiple models and datasets and results are solid.\n* Presentation is clear and ablation are properly studied."}, "weaknesses": {"value": "* **More advanced baselines** : Authors compare the TALF & SALF with EAGLE-2 and HASS. However, more recent methods like EAGLE-3 [1] improves the performance of EALGE-2 by a large margin, so the proposed method should be compared or combined with [1] ([1] also removes feature alignment loss which alignes with the argument in ln 254).  \n\n* **Experiment details** : Some of the experiment setting is unclear or not fair. In ln 352, why taking different approaches for llama series and. Moreover, performance improvement along trained token numbers is lacking. \n\n* **Hyper-parameter sensitivity** : While the authors conducted some ablations on hyper-parameters, naive grid search for SALF threshold (Table 4) and choosing N, B in inference stage weakens the practicality of the algorithm in real serving scenario."}, "questions": {"value": "* What's the size of the drafter models for the experiments? Can author provides the effects of size of the drafter?\n\n* Can author show the scaling effect of the new training algorithm as in EAGLE-3 [1]?\n\n* Can author provide experiment results on other GPU type if possible?\n\n* How does the hyper-parameters N, B, $\\tau$ are selected?\n\n[1] (Li et al.) EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hCGyUbNpJC", "forum": "3V559xWIWc", "replyto": "3V559xWIWc", "signatures": ["ICLR.cc/2026/Conference/Submission16563/Reviewer_Qx4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16563/Reviewer_Qx4H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187592317, "cdate": 1762187592317, "tmdate": 1762926643175, "mdate": 1762926643175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}