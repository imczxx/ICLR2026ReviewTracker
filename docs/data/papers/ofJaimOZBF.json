{"id": "ofJaimOZBF", "number": 9442, "cdate": 1758122741601, "mdate": 1759897724486, "content": {"title": "From ``Sure\" to ``Sorry\": Detecting Jailbreak in Large Vision Language Model via JailNeurons", "abstract": "Large Vision-Language Models (LVLMs) are vulnerable to jailbreak attacks that can generate harmful content. Existing detection methods are either limited to detecting specific attack types or are too time-consuming, making them impractical for real-world deployment. To address these challenges, we propose \\textbf{JDJN} (\\textbf{J}ailbreak \\textbf{D}etection via \\textbf{J}ail\\textbf{N}eurons), a novel jailbreak detection method for LVLMs. Specifically, we focus on \\textbf{JailNeurons}, which are key neurons related to jailbreak at each model layer. Unlike the ``SafeNeurons\", which explain why aligned models can reject ordinary harmful queries, JailNeurons capture how jailbreak prompts circumvent safety mechanisms. They provide an important and previously underexplored complement to existing safety research. We design a neuron localization algorithm to detect these JailNeurons and then aggregate them across layers to train a generalizable detector. Experimental results demonstrate that our method effectively extracts jailbreak-related information from high-dimensional hidden states. As a result, our approach achieves the highest detection success rate with exceptionally low false positive rates. Furthermore, the detector exhibits strong generalizability, maintaining high detection success rates across unseen benign datasets and attack types. Finally, our method is computationally efficient, with low training costs and fast inference speeds, highlighting its potential for real-world deployment.", "tldr": "", "keywords": ["Large Vision Language Model", "Jailbreak Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a66c59e8d73eefcdafff4f33806a7378b23f0aa.pdf", "supplementary_material": "/attachment/64bf7acc750738fe4a5498b832223add90dc8ce1.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a novel method to locate \"jailneurons\" in MLLMs so that the use of a classifier (in this paper, the authors adopt SVM) could handle the jailbreak detection. In detail, to finish the detection, they first locate related neurons by an optimization process and concatenate these neurons for SVM training. Experiments on several models, as well as a few jailbreak methods, demonstrate the performance of such a method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear writing. The pipeline, mathematical details, and experimental setup are clear, which is easy to follow.\n-  Novel way to locate the neurons. The method to build an optimization framework for neuron selection is interesting."}, "weaknesses": {"value": "- My biggest concern is about the experimental results. Take the evaluation on FigStep as an example. ECSO got 90.3% on the OCR subset of MM-safetybench (according to the original paper), which is a similar jailbreaking dataset using typography, so it seems a little weird that ECSO only got 0.596 on FigStep. Besides, the performance in the HiddenDetect paper is 0.846, where it also shows the performance of CIDER on Figstep is 0.713. The numbers reported in Table 1 have a huge gap with the original papers (or other replications), which requires further explanation.\n- From the perspective of storytelling, it does not clearly state the difference between previous neuron-digging methods and this script. Such detection could indeed be facilitated via a simple classifier, which is more efficient than using a guardrail model, but it is the advantage of all similar methods, such as SNIP, HiddenDetect, etc. The focus should be on the disadvantages of the previous safety-neuron picking method, or their suboptimal layer-picking method. More comparisons on this line of work are required[1][2][3] to prove that previous works could only handle normal harmful requests, other than jailbreaking requests.\n- The figures are not clear. More information should be included in the caption or related text parts. For example, what is the value in Table 1? I finish my review with the hypothesis that it is the detection rate (successfully detected/all jailbreak samples)\n\n[1]The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?\n\n[2]Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications.\n\n[3] On the Role of Attention Heads in Large Language Model Safety"}, "questions": {"value": "- What is the detailed experimental setup of baselines?\n- Could you explain more about the difference between previous neuron-digging methods and this script?\n- Will this method be over-sensitive, i.e., classifying benign prompts as jailbreaking? Experiments on or-bench or XSTest (or other MLLM datasets, if any) would be better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vt88eGYeea", "forum": "ofJaimOZBF", "replyto": "ofJaimOZBF", "signatures": ["ICLR.cc/2026/Conference/Submission9442/Reviewer_pMai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9442/Reviewer_pMai"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552852373, "cdate": 1761552852373, "tmdate": 1762921041364, "mdate": 1762921041364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces JDJN (Jailbreak Detection via JailNeurons), a novel method for detecting jailbreak attacks in Large Vision-Language Models (LVLMs). JDJN introduces the concept of \"JailNeurons\" - specific neurons that are activated during jailbreak attempts\nThese neurons are distinct from previously studied \"SafeNeurons\" which explain standard safety mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces JDJN, a novel method for detecting jailbreak attacks in Large Vision-Language Models by identifying and leveraging \"JailNeurons\". The approach demonstrates strong originality in its conceptualization of JailNeurons and its creative \"sure-to-sorry\" localization procedure. The quality of the work is evidenced by comprehensive empirical validation across multiple models and attack types, achieving impressive detection rates while maintaining computational efficiency. The significance of this research is substantial, addressing a critical security challenge in LVLMs with a practical, generalizable solution that could have immediate real-world impact on improving AI system safety."}, "weaknesses": {"value": "1. Section 4.2.1 introduces a mask which is the key to this work. I am wondering whether this mask is neccessary. If the jailbreak information is in the neuron, why we cannot learn a classifier directly? If there is need of filtering out unrelated neurons, you have different options like regularizations and etc.\n\n2. You are missing some baselines in Table1. For example, AdaShield and JailDAM and Gradsafe and etc.\n\n3. For different dataset, how will the mask changing? It will be interesting to know how this changes. If the mask is different for different dataset, how do you explain the neuron you find?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mXYHkGPKYs", "forum": "ofJaimOZBF", "replyto": "ofJaimOZBF", "signatures": ["ICLR.cc/2026/Conference/Submission9442/Reviewer_GpcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9442/Reviewer_GpcJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755847214, "cdate": 1761755847214, "tmdate": 1762921041112, "mdate": 1762921041112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces JDJN (Jailbreak Detection via JailNeurons), a framework for detecting jailbreak attacks in LVLMs by identifying and aggregating neuron activations that are responsible for jailbreak behavior. The method 1. localizes a sparse set of “JailNeurons” through a learned masking optimization 2. trains a lightweight classifier (linear SVM) over these neurons across selected layers"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Experiments on four LVLMs and three attack types show high true-positive rates.\n2. The proposed method seems to have strong generalization ability."}, "weaknesses": {"value": "1. The “causal” interpretation of Eq. (2–3) is asserted but not formally proven. Only intervention -> change is shown but no contrastive group of neurons is shown to not affect the output.\n2. Performance drops on certain benign datasets (e.g., FPR 0.768 on Janus-pro/Normal in Table 3). The parameter settings (JDJN3 in this case) seem to impact the generalization of the proposed approach.\n3 . Efficiency analysis is not very comprehensive. The authors claim that JDJN requires only a single forward pass but the details on the mask localization and training of the classifier is missing.\n4. Potential overfitting to limited benchmarks."}, "questions": {"value": "See weakness.\n1. It would help if the authors can provide more rigorous experiment/ analysis on the chosen jailneurons.\n2. Can the authors provide more discussion on the failure cases.\n3. Can the authors provide a more comprehensive analysis on the proposed method? For example, what would be the mask localization cost and classifier training cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p16u272L6Z", "forum": "ofJaimOZBF", "replyto": "ofJaimOZBF", "signatures": ["ICLR.cc/2026/Conference/Submission9442/Reviewer_dJcj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9442/Reviewer_dJcj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858514552, "cdate": 1761858514552, "tmdate": 1762921040619, "mdate": 1762921040619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies jailbreak detection for Large Vision-Language Models (LVLMs) by identifying a small set of neurons that strongly relate to jailbreak behavior. The authors first show that jailbreak and benign samples trigger different internal activations but that naive linear detection does not generalize well across attack types and benign sources. They then introduce JDJN, which locates “JailNeurons” through a mask-based optimization that forces harmful outputs to switch to refusal responses, and aggregates activations from selected layers to train a lightweight detector. Experiments across several LVLMs, three attack types, and multiple benign datasets indicate that JDJN yields high true-positive rates with very low false-positive rates, and generalizes to out-of-distribution attacks and unseen benign distributions. The method is efficient and works without modifying the base LVLM, making deployment friendly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper frames a clear and specific safety question: whether jailbreak activity concentrates in a sparse set of neurons and whether such neurons can support consistent, low-cost detection. This goes beyond generic claims like “performance limits” and targets internal mechanisms of LVLM jailbreak behavior. The method is well-aligned with that motivation, since the mask-training step is crafted to pinpoint neurons whose ablation flips harmful responses to safe refusals. Both single-layer and multi-layer analyses are thorough, and the adversarial evaluation with adaptive attacks adds credibility. The empirical results demonstrate strong evidence rather than speculation, including cross-model tests and ablation studies that examine critical components such as layer selection, mask threshold, regularization, and detector choice. Overall, the study links motivation, method, and experiments coherently, and offers a useful tool for practical safety settings."}, "weaknesses": {"value": "The causal interpretation of “JailNeurons” could be more rigorous; while the mask-based optimization offers a constructive handle, the paper does not fully rule out the possibility that the selected neurons encode surface-level shortcuts tied to particular phrasing or datasets. \n\nAlthough the authors run OOD tests, the scope of benign distributions is still somewhat narrow, and the stability of neuron sets across architectures and scaling regimes could benefit from deeper analysis. \n\nThere is also limited exploration of joint vision-language pathways; the focus is on text-side activations, so multimodal interplay is not fully dissected.\n\nThe white-box assumption may restrict real-world deployment scenarios, and a discussion on extending the method toward limited-access or proxy-signal settings would strengthen the broader impact."}, "questions": {"value": "You report that JailNeurons are sparse and effective. Do these neurons remain consistent under model finetuning, safety tuning, or instruction-following updates? How stable are they across different checkpoints of the same LVLM architecture?\n\nThe paper mainly analyzes decoder-side neurons. Do convolutional / vision transformer layers contain JailNeurons as well? If so, is the jailbreak signal similarly sparse? If not, what does that imply about multimodal interaction during jailbreak?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KwKZi8giVW", "forum": "ofJaimOZBF", "replyto": "ofJaimOZBF", "signatures": ["ICLR.cc/2026/Conference/Submission9442/Reviewer_5qV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9442/Reviewer_5qV5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941171768, "cdate": 1761941171768, "tmdate": 1762921040344, "mdate": 1762921040344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thorough reviews and constructive feedback. We are encouraged that the reviewers recognize the innovations in our work, particularly the originality in conceptualization of **JailNeurons**, and the high efficiency and generalization of **JDJN**.\n\n**Key Improvements and Clarifications**\n1. Richer baselines and benchmarks. \n- Adding three jailbreak defense methods; \n- Adding three neuron-digging methods;\n- Adding two benign datasets for LVLM; \n- Adding two datasets for testing the phenomenon of model over-rejection.\n\n2. A deeper study of the properties of JailNeurons.\n- Causal relationship between JailNeurons and jailbreak behavior; \n- Transferability of JailNeurons across different training datasets; \n- Transferability of JailNeurons across different model architectures and checkpoints; \n- The necessity of JailNeurons.\n\n3. Clearer experimental settings. \n- Specific configurations of baselines; \n- Evaluation metrics; \n- Criteria for determining the success of adversarial sample attacks.\n\n4. More discussion and analysis of the experiments.\n- Cost analysis of training the detector;\n- Analysis of failure cases;\n- Extension to access-limited scenarios.\n- Probing the role of JailNeurons in vision modules.\n\nPlease see our detailed responses below each review. We appreciate your feedback and welcome any additional questions."}}, "id": "j3nChkum4c", "forum": "ofJaimOZBF", "replyto": "ofJaimOZBF", "signatures": ["ICLR.cc/2026/Conference/Submission9442/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9442/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission9442/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763733308840, "cdate": 1763733308840, "tmdate": 1763733308840, "mdate": 1763733308840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}