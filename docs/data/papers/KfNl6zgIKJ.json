{"id": "KfNl6zgIKJ", "number": 7361, "cdate": 1758017776940, "mdate": 1759897857424, "content": {"title": "Towards Uniformity and Alignment for Multimodal Representation Learning", "abstract": "Multimodal representation learning aims to construct a shared embedding space in which heterogeneous modalities are semantically aligned. Despite strong empirical results, InfoNCE-based objectives introduce inherent conflicts that yield distribution gaps across modalities. We identify and formally analyze two conflicts in the multimodal regime, both exacerbated as the number of modalities \\(M\\) increases: (i) an alignment–uniformity conflict, whereby uniform repulsion undermines positive-pair alignment, and (ii) an intra-alignment conflict stemming from the non-collinearity of multi-way positives. To address these issues, we propose a principled decoupling of alignment and uniformity. We then demonstrate a theoretical guarantee that our method mitigates the distribution gap by introducing a global Hölder divergence over multiple modality distributions. We show that our decoupled losses act as efficient proxies for minimizing this cross-modal divergence. Extensive experiments on retrieval and UnCLIP-style generation demonstrate consistent gains. Overall, this work provides a conflict-free recipe and theoretical guidance for multimodal learning that simultaneously supports discriminative and generative use cases without task-specific modules.", "tldr": "", "keywords": ["Multimodal representation Learning", "CLIP", "Alignment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ff7fee7b8df30d39ce5d79cdd2be22481fb346d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a novel model for multimodal learning that promotes alignment of matching pairs and uniformity over the hypersphere for more modalities through the volume loss from the GRAM paper.\nResults are convincing and the idea is interesting as it improves some limitations of previous methods.\nAlso, the idea of testing the model in the generative task sounds and is convincing."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea sounds and wisely mixes two crucial topics that are scaling multimodal learning to mroe modalities and reduce the modality gap between modality representations.\n\nI appreciate the colored summary box at page 4, as it helps fix the main concepts.\n\nThe results in the generation task are convincing and interesting.\n\nOverall, the paper is interesting!"}, "weaknesses": {"value": "W1) In the box and throughout the paper, the authors say that it is crucial to have intra-modality uniformity and conflict-free alignment. Subsequently, they introduce the uniformity loss U(Z) and the L_align. However, they further add to the total loss the centroid uniformity, why? In this way, if I understood correctly, the total loss has two uniformity terms (U(Z) and (U(C)), plus the align term and the gram volume. I am afraid that the contribution of the uniformity losses may be too strong and disrupt the alignment, but the authors provide no further information for this.\n\n- Also, why is U(C) necessary?\n\n- I know that asking for ablation studies is somewhat boring and standard, but I am really curious to understand the contribution of each of the losses. Moreover, the authors provided simple ablations only on U(C) and L_vol, but a deeper understanding on the contribution of each of the loss is crucial.\n\n- More theoretical explanations on the reason why adding two uniformity terms would be appreciated.\n\nW2) How the proposed losses combination is justified by the theoretical analysis from the divergence perspective? If I'm not wrong (and I might be by the way), the divergence proves the claims regarding the uniformity and the alignment term, not for the combinations of the four losses proposed.\n\nW3) (minor) The authors should revise the notation as the theoretical part of the paper is a bit heavy. Moreover, sometimes the modality index is between brackets and on top, sometimes is without brackets at the bottom. This makes the paper a bit hard to read and confuse the notation.\n\nMinor comments:\n- the references of LanguageBind and GRAM are wrong (still arxiv), please update them with the correct citation.\n- line 131, InfoNEC instead of InfoNCE.\n\nOverall, even though my initial score is not so high, I can lean towards increasing the score if the authors provide theoretical and/or empirical evidences in response to my comments."}, "questions": {"value": "Q1) Can the authors provide ablation studies for each of the losses from scratch on MSRVTT?\n\nQ2) Why the \\lambda_vol for the generation is set to 0.1? the authors say that it is to have more emphasis on anchor-based alignment, but the volume definition is based on the anchor as well.\n\nQ3) Can the authors compute the cosine similarity between true pairs to further strengthen the plot with the tsne?\n\nQ4) can the authors share the link for their implementation? I would be curious to see losses implementation.\n\nQ5) I assume that the results in Table 1 are achieved via the multimodal encoder that both VAST and GRAM employ. Can the authors provide retrieval results before feeding the embeddings to the multimodal encoder (which, in the case of both vast and gram is the text encoder)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YKwLXekDaZ", "forum": "KfNl6zgIKJ", "replyto": "KfNl6zgIKJ", "signatures": ["ICLR.cc/2026/Conference/Submission7361/Reviewer_ZwMh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7361/Reviewer_ZwMh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760630229130, "cdate": 1760630229130, "tmdate": 1762919495369, "mdate": 1762919495369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the inherent limitations of multimodal contrastive learning when using the InfoNCE objective. It identifies two sources of conflict that worsen as the number of modalities $M$ increases: the **alignment–uniformity conflict**, where uniformity forces oppose cross-modal attraction, and the **intra-alignment conflict**, caused by non-collinearity among multi-way positives. To address these issues, the authors propose **UniAlign**, which decouples intra-modality uniformity and cross-modality alignment via an anchor-based loss and a “volume” regularizer. They also introduce a new theoretical formulation — the *global Hölder divergence* — to interpret their objective as minimizing a cross-modal distribution gap. Experiments on retrieval and UnCLIP-style generation show modest but consistent improvements over GRAM and VAST baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a relevant and well-known problem — the modality gap in multimodal contrastive learning — and extends previous analyses from the bimodal to the general multimodal case.  \n2. The proposed framework is straightforward to implement, requiring no architectural modifications or additional modules.  \n3. The empirical results are encouraging, showing that the proposed method consistently outperforms existing alternatives."}, "weaknesses": {"value": "## Major  \n1. **Theoretical clarity.** The theoretical section is difficult to follow, and several key definitions are vague or insufficiently motivated. In particular:  \n   - In Eq. (3), it is unclear why $V_a$ represents the alignment force and $\\Phi_a$ the uniformity force — this connection should be explained more carefully.  \n   - Assumption 1 is introduced without justification; it is not obvious why it should hold or be meaningful in practice.  \n   - In the boxed text of Section 3.1, the statement *“promote uniform coverage within that modality only”* is ambiguous and should be clarified.  \n   - The same box refers to *“consensus magnitude”* as if it were a standard concept, but it is never defined or explained.  \n\n2. **Idealized assumptions.** The theoretical analysis relies on simplified and somewhat unrealistic conditions (e.g., independence and isotropy of negatives, uniform temperature across modalities). A discussion of the robustness of the results under more practical conditions would improve the paper.  \n\n3. **Loose theory–practice connection.** The derivation of the Hölder divergence (Eq. 16) is mathematically elegant but remains disconnected from the practical loss in Eq. (13). The claim that UniAlign effectively minimizes this divergence is heuristic, relying on a kernel-density approximation that is never empirically verified.  \n\n## Minor  \n1. Corollaries should follow from a theorem or proposition. Please revise the naming (e.g., use *Theorem*, *Lemma*, or *Proposition*).  \n2. The method introduces several hyperparameters whose influence is not systematically analyzed. An ablation or sensitivity study would strengthen the experimental section."}, "questions": {"value": "1. Could similar improvements be obtained by simply re-weighting InfoNCE gradients rather than introducing new loss terms?  \n2. Can you empirically estimate the proposed Hölder divergence during training to verify that it indeed decreases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8bjhSDqrKJ", "forum": "KfNl6zgIKJ", "replyto": "KfNl6zgIKJ", "signatures": ["ICLR.cc/2026/Conference/Submission7361/Reviewer_BVa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7361/Reviewer_BVa6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737562590, "cdate": 1761737562590, "tmdate": 1762919494882, "mdate": 1762919494882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies two fundamental conflicts that hinder multimodal contrastive learning, the alignment–uniformity conflict and the intra-alignment conflict, and proposes a principled decoupling framework with a theoretical guarantee via a global Hölder divergence to achieve conflict free multimodal representation learning that improves both discriminative and generative performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is clearly written, presenting the motivation, conflicts, and proposed solution in a well-structured manner.\n\n2..The proposed method is theoretically grounded and experimentally validated, demonstrating robust performance across multimodal retrieval and generation tasks."}, "weaknesses": {"value": "1.There are some writing errors, such as “InfoNEC” in Section 2.2.\n2.Although anchor-based alignment eliminates cross-modal rejection, it introduces modal bias. I have a question: Could the selection of different anchor modalities lead to representation imbalance?\n3.Your approach employs intra-modal consistency to prevent representation collapse. Was modal collapse considered during training?\n4.The proposed global Hölder divergence is defined over multiple modality distributions. Is this divergence sensitive to the curse of dimensionality in high-dimensional embedding spaces?"}, "questions": {"value": "Specific issues can be found in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KqyVhSqJpS", "forum": "KfNl6zgIKJ", "replyto": "KfNl6zgIKJ", "signatures": ["ICLR.cc/2026/Conference/Submission7361/Reviewer_EfDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7361/Reviewer_EfDF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831284539, "cdate": 1761831284539, "tmdate": 1762919494420, "mdate": 1762919494420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim that standard contrastive learning over more than two modalities is limited by a conflict between the alignment and uniformity criteria. To this end, they propose an approach, UniAlign, which aims to resolve this conflict. They theoretically analyze the identified form of conflict and empirically evaluate their approach on generative and retrieval tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses a potential roadblock in contrastive learning, i.e., the possibility that the alignment of positive tuples being interfered with by the uniformity criterion in contrastive learning.\n2. The authors do a good job in conveying the intuition behind the problem that they are tackling.\n3. For the kind of alignment-uniformity conflict presented in their Assumption 1, the authors theoretically prove that it increases as the number of modalities grow."}, "weaknesses": {"value": "1. Although the authors argue for the existence of a cross-modality uniformity conflict which oppose alignment, there is no clear evidence for this, either in the existing literature or in this work.\nFor instance, in Line 104, the authors refer to learning, Yin et al. (2025), mentioning that \"clearly demonstrate that uniformity across modalities (“inter-uniformity”) conflicts with the alignment term\". However, I found no results as such upon going through that work. The claim can be described as: for each positive pair, the alignment term can be cancelled out by the uniformity forces when aggregated across modalities. However, at least intuitively, the probability of this happening seems extremely low, because (i) otherwise most multimodal contrastive methods just would not work; and (ii) it is well known that there exists clusters of similar positive pairs (even in the unsupervised case) which are spontaneously grouped together under contrastive learning [a, b], meaning that the probability that alignment of positive pairs would be reinforced by other similar positive pairs is much higher than the probability of interference from unrelated / negative pairs. Having said that, it is possible that the cancellations could get stronger as the number of modalities increase as the authors argue, however, to establish this fact there needs to be significantly more theoretical and empirical analyses, since there is plenty of very strong evidence pointing to the contrary [c, d].\n\n2. No ablation studies are presented in this paper. Without them, it is difficult to evaluate exactly what is contributing to the performance improvements reported in Table 1.\n\n3. The purpose of the volume-based complement loss is not clear. It seems that it encourages samples that are grouped together from multiple modalities into a tuple a dispersed. However, this would mean that samples that share the same semantic information would be pushed apart, which goes against the desired objective.\n\n4. The L_align term does not seem to be any different from a standard contrastive learning objective, where one modality is considered as an anchor. However, there does seem to be a downside, which comes from imposing only the uniformity objective on the anchor modality. It would imply that samples that are semantically similar in the anchor modality will not be brought together, and consequently, neither would the samples from the other modalities, since such is the nature of the anchor to which they are aligned.\n\nReferences:\n\n[a] Parulekar et al., \"InfoNCE Loss Provably Learns Cluster-Preserving Representations\", COLT 2023. \\\n[b] Lu et al., \"f-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning\", TMLR 2023. \\\n[c] Girdhar et al., \"IMAGEBIND: One Embedding Space To Bind Them All\", CVPR 2023. \\\n[d] Wang et al., \"Image as a Foreign Language: BEIT Pretraining forVision and Vision-Language Tasks\", CVPR 2023."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FVy7Fc5QFj", "forum": "KfNl6zgIKJ", "replyto": "KfNl6zgIKJ", "signatures": ["ICLR.cc/2026/Conference/Submission7361/Reviewer_WFru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7361/Reviewer_WFru"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912046593, "cdate": 1761912046593, "tmdate": 1762919493727, "mdate": 1762919493727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}