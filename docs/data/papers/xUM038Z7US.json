{"id": "xUM038Z7US", "number": 18597, "cdate": 1758289352537, "mdate": 1759897093293, "content": {"title": "MDL-Pool: Adaptive Multilevel Graph Pooling Based on Minimum Description Length", "abstract": "Graph pooling compresses graphs and summarises their topological properties and features in a vectorial representation. \nIt is an essential part of deep graph representation learning for graph-level tasks like classification or regression.\nCurrent approaches pool hierarchical structures in graphs by iteratively applying shallow pooling operators up to a fixed depth.\nHowever, they disregard the interdependencies between structures at different hierarchical levels and do not adapt to datasets that contain graphs with different sizes that may require pooling with various depths.\nTo address these issues, we propose MDL-Pool, a pooling operator based on the minimum description length (MDL) principle, whose loss formulation explicitly models the interdependencies between different hierarchical levels and facilitates a direct comparison between multiple pooling alternatives with different depths.\nMDL-Pool builds on the map equation, an information-theoretic objective function for community detection, which naturally implements Occam's razor and balances between model complexity and goodness-of-fit via the MDL.\nWe demonstrate MDL-Pool's competitive performance in an empirical evaluation against various baselines across standard graph classification datasets.", "tldr": "We develop MDL-Pool an adaptive multilevel pooling operator based on the minimum description length (MDL) principle that automatically selects the optimal number of clusters and pooling depth for graph classification tasks.", "keywords": ["Deep Graph Pooling", "Graph Classification", "Minimum Description Length", "Parameter-free"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f19767286795498c1e67fea66cb727c831d9b716.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MDL-Pool, a differentiable graph pooling method that automatically selects the optimal pooling depth for each graph by minimizing the description length—computed via the map equation—of hierarchical cluster assignments.\nThe method adaptively compresses graph structures and demonstrates effectiveness on both community detection and graph classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Explicit modeling of interdependencies across different hierarchical levels, as opposed to stacking independent pooling layers."}, "weaknesses": {"value": "- Limited performance improvement\nThe authors note that MDL-Pool achieves state-of-the-art results only “in two of the eleven scenarios” and explicitly acknowledge that “we do not find a clear winner for graph classification.” This suggests that the overall empirical gain is modest.\n- Lack of dataset-specific performance analysis\nThe paper does not explore which dataset characteristics (e.g., average graph size) affect the method’s relative performance.\nA correlation study between these properties and MDL-Pool’s performance would strengthen the empirical section.\n- Unclear practical advantage of adaptive depth\nAdaptive depth is a central contribution, yet the paper reports that “the maximum chosen depth was two” and that “graphs in the classification datasets are small enough that two layers suffice”.\nFor single-graph tasks (community detection) or datasets where most graphs use depth 0 or 1, the advantage over fixed-depth configurations remains unclear, especially given that the model performance is comparative to other fixed-depth baselines.\n \n- Computational cost and parameter-free claim\nThe paper emphasizes that MDL-Pool is “parameter-free,” yet it is also “limited to at most two pooling operations”\nThe actual computational savings compared to conventional hyperparameter tuning are therefore uncertain, given that MDL\nReporting runtime or complexity comparisons with fixed-depth GNN baselines would make the claim more convincing."}, "questions": {"value": "- Could you provide a quantitative analysis correlating dataset characteristics (e.g., average node/edge count, modularity) with the relative performance of MDL-Pool?\n- Do the graph-pooling results align with domain knowledge? For example, in the D&D dataset, do amino acids belonging to the same secondary structure fall within the same cluster?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UKcwdSFIlP", "forum": "xUM038Z7US", "replyto": "xUM038Z7US", "signatures": ["ICLR.cc/2026/Conference/Submission18597/Reviewer_5Qqa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18597/Reviewer_5Qqa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634680380, "cdate": 1761634680380, "tmdate": 1762928314018, "mdate": 1762928314018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MDL-Pool, a new adaptive, multilevel graph pooling operator grounded in information theory. It applies the map equation to jointly optimize cluster assignments across all hierarchical levels, explicitly modeling interdependencies between them. Unlike existing hierarchical pooling methods that fix the number of pooling layers, MDL-Pool dynamically selects the optimal pooling depth per graph using the MDL principle."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The integration of the MDL principle and map equation into deep graph pooling is well-motivated. It provides a principled way to address overfitting and model complexity while enhancing interpretability. \n- The proposed multilevel loss seamlessly integrates hierarchical information, overcoming optimization issues caused by layer-wise independence in stacked pooling.\n- The MDL framework naturally implements Occam’s razor, removing the need for hyperparameter tuning for cluster count or levels."}, "weaknesses": {"value": "- The MDL-based loss focuses on topological structure and does not fully leverage node features in evaluating community quality, which might reduce performance on feature-dominant tasks.\n- Experiments show most graphs select only one or two pooling levels; it remains unclear whether MDL-Pool is beneficial in tasks with truly deep hierarchies.\n- The computation of multilevel flow matrices has quadratic cost in graph size, which may hinder scalability to very large graphs. No experiments on large-scale datasets are shown."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qeAO9yd6Pc", "forum": "xUM038Z7US", "replyto": "xUM038Z7US", "signatures": ["ICLR.cc/2026/Conference/Submission18597/Reviewer_iTTL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18597/Reviewer_iTTL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699248384, "cdate": 1761699248384, "tmdate": 1762928313604, "mdate": 1762928313604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a minimum-description-based graph pooling model to learn strong graph representations for different-sized networks. With the mapping function, the model encodes the vertices' interdependencies. in different hierarchical levels that help cluster the network according to its volume and make graph learning effective."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The map equation is beneficial for observing the overall networks and for relevant clustering. \nThe clustering helps for balanced training to fit the model in downstream graph analytics tasks. \nMDL is beneficial because it detects the depth of the input graph, which assists in effective hierarchical graph learning. \nThe comprehensive result is better than other baselines"}, "weaknesses": {"value": "In the experiment, the authors did not mention the hyperparameter's impact on the model. \nThe manuscript does not provide runtime details. Is minimum description length feasible on large volume datasets? \nThe optimization of map equations involves nested matrix operations, which can result in a computationally heavy model. Please check the model's runtime with respect to simpler pooling operations like Top-kPool and SAGPool. \nIn the case of community detection, the datasets are very sparse. Is the model suitable to cluster denser graphs (like Amazon Photo and Physics)? In this case, how does it perform over the other baselines?"}, "questions": {"value": "Is minimum description length feasible on large volume datasets?  Please observe the community detection on Amazon Photos, Physics.\nHow much more efficient is the model compared to simpler pooling operations like Top-kPool, SAGPool, GMT, etc.? Does MDLPool outperform these models?  \nHow does the technique provide expressivity in the model ? could you please show some formal reasoning or visualization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HQmwT1lE74", "forum": "xUM038Z7US", "replyto": "xUM038Z7US", "signatures": ["ICLR.cc/2026/Conference/Submission18597/Reviewer_GEGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18597/Reviewer_GEGj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135155226, "cdate": 1762135155226, "tmdate": 1762928313244, "mdate": 1762928313244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MDL-Pool, an adaptive multilevel graph pooling operator based on the Minimum Description Length (MDL) principle. The method integrates the map equation into graph neural networks (GNNs) to model hierarchical dependencies between pooling levels. Unlike conventional stacked pooling approaches that fix depth and ignore inter-level dependencies, MDL-Pool formulates a joint loss to optimize clusters across all hierarchical levels and automatically selects the optimal depth per graph. The authors evaluate MDL-Pool on both community detection and graph classification benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method automatically determines the optimal pooling depth per graph instance, addressing a long-standing hyperparameter issue in hierarchical pooling.\n2. The paper provides experiments on both synthetic and real-world datasets, including ablations on architecture variants and pooling depths."}, "weaknesses": {"value": "Limited performance gain: In Tables 2 and 3, MDL-Pool does not consistently outperform baselines. For community detection, results are comparable or even worse than baselines on several datasets. Similarly, in graph classification, MDL-Pool’s average accuracy is not higher than several baselines, indicating limited empirical advantage.\n\nInsufficient justification of benefits: While the motivation is sound, the claimed benefits (interdependency modeling and adaptive depth) are not strongly supported by quantitative evidence. The paper should include ablation or visualization explicitly demonstrating that modeling interdependencies leads to measurable improvement.\n\nUnclear parameter selection: Section 4.1 mentions “up to l levels,” but the criterion for selecting the number of levels is not clearly described. How the model avoids overfitting or underfitting different depths needs more elaboration.\n\nChoice of cmax = 50: In Table 2, the authors fix the number of clusters to 50, which seems far from the ground-truth number of communities (e.g., 3–7). This may bias the results. The paper should report results when cmax is closer to the true number (e.g., 10) to assess robustness.\n\nLack of comparative summary metrics: For Table 3, it would be informative to include an overall metric, such as the average rank or mean relative improvement across datasets, to better illustrate general trends rather than per-dataset fluctuations.\n\nQuestionable realization of motivation: The introduction claims that previous works ignore interdependencies between hierarchical structures, but it remains unclear whether MDL-Pool effectively learns such interdependencies rather than simply aggregating multi-level losses. Experimental evidence (e.g., hierarchical attention visualization or gradient correlation analysis) is lacking."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dtfaOjf4Pc", "forum": "xUM038Z7US", "replyto": "xUM038Z7US", "signatures": ["ICLR.cc/2026/Conference/Submission18597/Reviewer_8QYD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18597/Reviewer_8QYD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762207613613, "cdate": 1762207613613, "tmdate": 1762928312858, "mdate": 1762928312858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}