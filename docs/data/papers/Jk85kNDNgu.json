{"id": "Jk85kNDNgu", "number": 6781, "cdate": 1757995424940, "mdate": 1759897894438, "content": {"title": "MuRA: Multi-Rank Adaptation for Efficient and Effective Test-Time Vision-Language Generalization", "abstract": "Vision-language models (VLMs) have demonstrated remarkable zero-shot capabilities, but their performance degrades significantly when encountering distribution shifts. Recently, test-time adaptation (TTA) methods have been introduced to enhance VLMs' generalization ability. Among these methods, knowledge-adaptive approaches that incorporate Low-Rank Adaptation (LoRA) into vision models show relatively limited improvement compared to other TTA strategies. Our investigation reveals that the fundamental limitation stems from LoRA's static rank configuration, as visual inputs with varying information densities inherently require different ranks for optimal adaptation. To address this challenge, we propose Multi-Rank Adaptation (MuRA), a dynamic rank selection mechanism that adapts to varying data distributions. MuRA achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks. By restricting adaptation to only the deepest layer, MuRA shortens the gradient backpropagation path, thereby significantly reducing both computational and memory overhead. Our method represents an efficient and effective approach to test-time vision-language generalization. Our code will be publicly released to benefit the research community.", "tldr": "", "keywords": ["Test-Time Adaptation", "Zero-Shot Generalization", "Vision-Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5097c72c03afdab1637aedf766eb179c0b401f75.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper targets knowledge-adaptive TTA for VLMs and argues that a static LoRA rank is suboptimal because different inputs/datasets require different adaptation capacity. Empirically, the optimal rank varies widely across domains and correlates strongly with image entropy. The proposed MuRA prepares multiple rank-specific LoRA modules via Multi-Rank Orthogonal Decomposition (MROD) and soft, token-level routing (UCF) with Continuous Router Updating (CRU). MuRA delivers state-of-the-art average accuracy on ImageNet OOD and cross-domain suites with attractive accuracy–efficiency trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Clear diagnosis & evidence. The paper convincingly shows rank sensitivity across datasets and its linear relation to image entropy, motivating dynamic rank selection.\n\n- Well-designed, cohesive method. MROD: principled SVD-based init yielding orthogonal residuals; improves stability of one-step TTA. UCF (token-level soft MoE) + CRU: learns token-wise rank preferences over time; soft routing > hard, token-level > instance-level (especially with CRU)."}, "weaknesses": {"value": "- Varying data distributions. Although the paper claims strength under varying data distributions, current experiments use a single test distribution per benchmark. Please evaluate sequential distribution shifts (CL-style streams) to show that CRU adapts appropriately over time and visualize how CRU’s rank routing evolves as the distribution changes. Framing the method explicitly as strong for CL + TTA would also sharpen the contribution.\n\n- Academic formatting quality. The manuscript’s presentation needs polishing (e.g., oversized figures, occasional image blurring/pixelation, inconsistent layout). Please standardize figure sizes/resolution, ensure vector graphics where possible for professional readability."}, "questions": {"value": "- Please visualize the evolution of CRU’s rank routing as the distribution changes (e.g., rank-utilization entropy over the stream, per-domain routing profiles) --- See Weakness 1.\n- Please improve the paper formatting quality --- See Weakness 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OJaGwe0G1a", "forum": "Jk85kNDNgu", "replyto": "Jk85kNDNgu", "signatures": ["ICLR.cc/2026/Conference/Submission6781/Reviewer_nRHo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6781/Reviewer_nRHo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761031398245, "cdate": 1761031398245, "tmdate": 1762919056465, "mdate": 1762919056465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MuRA, a test-time adaptation method for VLMs that fuses multiple LoRA modules of different ranks via a softmax-weighted router and initializes the rank components through an SVD-based “Multi-Rank Orthogonal Decomposition” (MROD). A “Continuous Router Updating” (CRU) strategy is claimed to retain routing knowledge across samples. The method adapts only the deepest visual layer and uses 63 augmentations per test image with entropy-weighted selection. Experiments report gains on ImageNet variants and cross-domain datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical, lightweight design: restricting adaptation to the deepest layer is reasonable for efficiency and simplicity.\n2. Competitive results on multiple benchmarks, with ablations that partially justify the components (MROD, UCF, CRU).\n3. Clear framing of rank sensitivity and correlation with image entropy; the motivation for dynamic rank choice is intuitive."}, "weaknesses": {"value": "1. Limited technical novelty: the core idea—combining multiple LoRA ranks with a softmax router—is a straightforward mixture-of-experts/gating over adapters and feels incremental relative to existing PEFT/TTA adapter ensembles.\n2. SVD vs. random partition: the paper does not convincingly demonstrate why SVD-based rank construction is superior to simpler alternatives (e.g., random splits, fixed-rank LoRA, or PCA variants). Please add controlled comparisons (same parameter budget) and report effect sizes.\n3. CRU under-specified: the Continuous Router Updating component (around Line 300) is described in one sentence without algorithmic detail. How exactly is the router state retained/reset across samples/batches? What regularization, optimizer, learning rate schedule, and stability safeguards are used? Provide pseudo-code and a failure/sensitivity analysis.\n4. Efficiency concerns: generating 63 augmentations per test image can be costly. The paper should include a clear time and memory complexity analysis (asymptotics and wall-clock),  and throughput vs. accuracy trade-offs (e.g., 8/16/32/63 views). Report results with fewer views to show robustness."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MhQIUywNOH", "forum": "Jk85kNDNgu", "replyto": "Jk85kNDNgu", "signatures": ["ICLR.cc/2026/Conference/Submission6781/Reviewer_iFPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6781/Reviewer_iFPH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773288873, "cdate": 1761773288873, "tmdate": 1762919056047, "mdate": 1762919056047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MuRA, a test-time adaptation (TTA) method for CLIP that addresses the limitations of static rank configurations in prior knowledge adaptation paradigms. The core idea is that visual inputs with varying information density may require different adaptation capacities. To this end, the method dynamically selects and fuses multiple low-rank adaptation components to achieve efficient adaptation across diverse image types. Extensive experiments on 15 datasets demonstrate competitive performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to understand, with clear explanations of the method and experimental results.\n- The knowledge adaptation paradigm represents an interesting form of TTA."}, "weaknesses": {"value": "- Intrinsic limitation of the knowledge adaptation paradigm.   The proposed method appears tightly coupled with a specific architecture and may not generalize well to other baselines such as CLIP with a ResNet-50 backbone.\n- Risk of overconfidence from entropy minimization. The use of entropy minimization loss may cause over-confident predictions during test-time adaptation, which could negatively affect model calibration.\n- The manuscript lacks comparisons with the state-of-the-art VLM TTA methods, such as MCP[1], GS-Bias[2], and TT-RAA[3].\n\n[1] Multi-Cache enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models. ICCV 2025\n\n[2] GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models. ICML 2025\n\n[3] Test-Time Retrieval-Augmented Adaptation for Vision-Language Models. ICCV 2025"}, "questions": {"value": "- In Table 3, the performance improvement of Unified Component Fusion (UCF) after MROD appears marginal.   Does this suggest that the contribution of UCF is limited?\n- What theoretical justification supports the design of the Continuous Router Updating (CRU) strategy? I am concerned that continuously updating the router across test samples may lead to the accumulation of adaptation errors or drift over time.\n- The paper lacks visualization or interpretability studies (e.g., t-SNE feature visualizations) that could provide insights into how the proposed adaptation influences the representation space."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W5gbl78GXC", "forum": "Jk85kNDNgu", "replyto": "Jk85kNDNgu", "signatures": ["ICLR.cc/2026/Conference/Submission6781/Reviewer_2MGn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6781/Reviewer_2MGn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806144781, "cdate": 1761806144781, "tmdate": 1762919055455, "mdate": 1762919055455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MuRA (Multi-Rank Adaptation), a test-time adaptation (TTA) framework designed for Vision-Language Models (VLMs) such as CLIP. MuRA adapts both visual and textual embeddings across multiple ranks in a unified optimization objective, yielding more flexible and robust test-time updates. Experiments on benchmark datasets show improved accuracy and robustness compared to single-rank or fixed adapter baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. **Comprehensive Experiments.** \nThe paper adheres to the standard evaluation protocols established in the VLM TTA community and shows consistent performance gains over prior baselines across multiple benchmark datasets.\n\nS2. **Balanced technical depth and clarity.**\nThe paper provides clear algorithmic exposition, ablation studies on rank configurations, and qualitative analyses supporting the motivation in Appendix."}, "weaknesses": {"value": "W1. **Missing comparison against multiple same-rank adapters.**\nWhile the idea of employing multiple ranks for adaptation is interesting, the paper does not convincingly show that the improvement comes from rank diversity itself rather than from simply using multiple adapters. The ablation study compares only single-rank vs. multi-rank configurations, but there is **no baseline that uses several adapters of the same rank. Without this comparison, it remains unclear whether MuRA’s advantage originates from heterogeneous rank composition or just increased model capacity. Including such an experiment would make the contribution much more convincing.\n\nW2. **Unfair comparison due to unmatched adaptation capacity.** \nSeveral reported gains may stem from larger trainable capacity rather than the proposed multi-rank design. In many tables (Table 1&2), MuRA appears to use more total trainable parameters than baselines that use a single adapter of a fixed rank or adapt only one branch. Without capacity-controlled baselines, the comparison is confounded. To ensure the fairness, the paper should report the total trainable parameters, further match total trainable parameters. \n\nW3. **Inference Overhead.** \nUnlike prior single-LoRA approaches, MuRA introduces a gating module that determines which LoRA branch to activate during inference.\nIn practice, this requires computing the forward pass of both the base model and a LoRA module selected by gating module, followed by their weighted summation. This design inevitably incurs additional inference overhead compared to a single LoRA model, where the LoRA weights can be merged into the base weights, resulting in identical forward cost. Therefore, the authors should report and analyze the inference-time computational cost of MuRA, including latency, FLOPs, and throughput, to clarify the trade-off between performance gain and efficiency.\n\nW4. **Incremental performance gains.**\nAs shown in Tables 2–4, the proposed method achieves approximately 1.0–2.5% improvement over single-rank baselines. While the gains are consistent, they are relatively modest given the additional complexity introduced by the multi-rank design. Considering that MuRA requires multiple adapters and a gating mechanism at inference, the improvement-to-overhead ratio appears limited. A more detailed efficiency analysis or scenarios where MuRA provides significantly larger benefits (e.g., under extreme domain shifts) would help justify the practical value of the approach."}, "questions": {"value": "I wrote all my concerns in Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EbS3Qc6jn6", "forum": "Jk85kNDNgu", "replyto": "Jk85kNDNgu", "signatures": ["ICLR.cc/2026/Conference/Submission6781/Reviewer_AXZk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6781/Reviewer_AXZk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970312268, "cdate": 1761970312268, "tmdate": 1762919054803, "mdate": 1762919054803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}