{"id": "4qHj8FkgLq", "number": 501, "cdate": 1756742840734, "mdate": 1763719489515, "content": {"title": "Provably Learning Representations under Generalized Dependency Structure", "abstract": "The presence of noise that depends on the latent variable poses a significant identifiability challenge. Addressing this issue, the standard solution in the literature assumes that the observational data satisfy the conditional independence property given the latent variables. However, this assumption might not be valid in practice. This work relaxes this foundational constraint. Specifically, we consider a *generalized dependency structure* in which the observations may exhibit arbitrary dependencies conditional on the latents. To establish identifiability guarantees, we introduce a two-step theoretical framework. First, we formulate the problem as a factor analysis model use perturbation theory to establish the subspace identifiability of the latent variables. Second, assuming the structural sparsity on the mixing function, or sufficient variability constraint in the latent space, we establish component-wise identifiability of each individual latent factor. Using these identifiability results, we develop an unsupervised approach that reliably uncovers the latent representations. Experiments on synthetic and real data verify our theoretical claims.", "tldr": "", "keywords": ["unsupervised learning", "representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f01c061acb151831a0ad39ad7644b430f1594846.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper discusses the identifiability problem of latent variables with the existence of unknown dependent noise. Theoretical results are given that under some assumptions, latent variable z is subspace-identifiable and can be component-wise identifiable if sparsity condition is satisfied or auxiliary domain variable is further available. An identification algorithm is proposed by combining beta-VAE and a sparsity regularization term."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper focuses on latent variable identifiability, which is an important problem in representation learning. \n\n2. This paper provides thorough results on this problem, including subspace/component-wise identifiability conditions and identification algorithm."}, "weaknesses": {"value": "1.The problem setting of this paper is questionable. The existence of “generalized dependency structure” is the main challenge that this paper aims to address, i.e., the noise $\\epsilon$ depends on latent variable $z$. However under other assumptions proposed by the author (g, e are injective, $\\eta$ is independent), this problem is equivalent to one with independent noise: rewrite Eq. (1) as $x=g(z,e(z,\\eta))\\triangleq f(z,\\eta)$, view $\\eta$ as the noise (which is independent of $z$) instead of $\\epsilon$. This observation undermines the main claim of this paper.\n\n2.There is some clarity problem regarding Thm. 1. In assumption vi, I find no definition of the symbol $\\tilde{h}$, so do the following symbols $\\tilde{\\mathcal{H}}$ and $\\mathcal{T}$. This hinders me from further checking the proof. In addition, please give more explanation about why the uniqueness of $L_{x_b|z}$ implies the existence of $\\tilde{h}$ as stated in Line 190-192, as well as in Line 820-822. \n\n3.The assumptions in Thm 1. lack discussion. It is non-trivial to split $x$ into three parts $x_A, x_B, x_C$, what’s the difference among three parts or how do they function differently? Can the assumptions in Thm 1. be easily satisfied? Did you check whether they are satisfied in your synthetic experiments? Since Thm 1. is the core of this work, such discussions should be included.\n\n4.The novelty of component-wise identifiability results (Thm. 2/3) is limited. In my point of view, these 2 theorems assume the conditions in Thm. 1 are satisfied, i.e., latent variable z is already subspace-identified. In this case, the component-wise identifiability problem of z becomes exactly the setting in the work of Kong et al. (2022) and Zhang et al. (2024), therefore the provided results are direct corollary of previous theorems. The introduced dependency structure does not bring novel challenge to the proof. Please correct me if there are misunderstandings."}, "questions": {"value": "See weaknesses for main questions. Here are some minor questions:\n\n1.In Def. 2, what is a “permutation”? Is it an index permutation on the dimensions of z? If this is the case, what does $\\pi(z^n)$ mean? $z^n$ has only one dimension.\n\n2.Since Thm.1 originates from the work of Hu (2008), can you provide some intuition about the main difficulty caused by the noise dependency for the proof, and your key idea for the mitigation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eIJnpK5M9Y", "forum": "4qHj8FkgLq", "replyto": "4qHj8FkgLq", "signatures": ["ICLR.cc/2026/Conference/Submission501/Reviewer_TTFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission501/Reviewer_TTFZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036699829, "cdate": 1761036699829, "tmdate": 1762915532875, "mdate": 1762915532875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the identifiability when observations remain dependent conditioning on the latent factors. It proposes a generalized dependency framework with theoretical guarantees for both subspace and component-wise identifiability, supported by perturbation-based analysis and sparsity assumptions. A corresponding variational inference model is developed and shown to outperform prior methods on synthetic and real-world datasets, demonstrating promising theoretical and empirical results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In my view, the main contribution of this paper lies in its theoretical advancement, which allows dependencies among observations given latent variables with novel assumptions. The experimental results on the synthetic dataset are also promising. The paper is clearly written, and I appreciate the well-organized presentation and thorough background provided by the authors."}, "weaknesses": {"value": "I think one shortcoming of the paper is the lack of discussion of Li et al. (2025) [https://www.arxiv.org/pdf/2510.18281]. Specifically, Li et al. also build their identifiability results upon Hu (2008) and consider a generalized dependency structure among observations. Since Li’s work was released earlier, it would be important to include a detailed discussion clarifying the differences and the unique contributions of the current paper relative to theirs.\n\nAnother notable concern is the mismatch between the reported baseline accuracies and those reported in the original papers for the real-world dataset. I cross-checked the results of AGW (Ye et al., 2021), TransReID (He et al., 2021), and CLIP-ReID (Li et al., 2023a). The Rank-1 (R1) accuracies reported in the original works are considerably lower than those presented in this submission — for example, the AGW paper reports an R1 of 68.3, whereas this paper lists 85.5. I recommend the authors clarify how these baseline results were obtained, ensuring that all comparisons are fair and consistent with the original implementations (an anonymous link containing the code implementation will be appreciated)."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LqYpgYnCs6", "forum": "4qHj8FkgLq", "replyto": "4qHj8FkgLq", "signatures": ["ICLR.cc/2026/Conference/Submission501/Reviewer_Dt9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission501/Reviewer_Dt9i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757185773, "cdate": 1761757185773, "tmdate": 1762915532750, "mdate": 1762915532750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the identifiability of latent variables under the condition that the noise is dependent on the latent variables. Under some assumptions, the authors prove the subspace and component-wise identifiability. These results motivate the design of the VAE structure and the learning objective. Using their model, the authors show that the proposed method gets better identifiability on the synthetic data, and better accuracy on a downstreaming classification task on a real data set, which indicates better latent variables."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work extends the previous setting of independent noise to dependent noise. This is a practical setting, and renders more flexibility to the model.\n\n2. Based on the theoretical analysis, there are several improvements to traditional VAE, such as the conditional noise distribution and sparsity regularization. Some ablations show the effectiveness of these improvements."}, "weaknesses": {"value": "1. The authors add the regularization on the Jacobian to meet the sparsity assumption. This may be inefficient for high dimensional data.\n\n2. There are three regularization terms, hence three hyperparameters for their weights. There is no sensitivity analysis of them.\n\n3. For real data analysis, there is only one data set. Furthermore, it consists of images. I am afraid the MLP architecture may not be optimal for images."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7bMPw1X0aJ", "forum": "4qHj8FkgLq", "replyto": "4qHj8FkgLq", "signatures": ["ICLR.cc/2026/Conference/Submission501/Reviewer_Mn5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission501/Reviewer_Mn5G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810670734, "cdate": 1761810670734, "tmdate": 1762915532601, "mdate": 1762915532601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies identifiability of latent variables when (i) observations remain dependent given latents and (ii) the noise depends on the latents. It offers a two‑step theory: subspace identifiability from several measurement and component‑wise identifiability from structural sprsity/sufficient changes. An unsupervised VAE-style model is proposed to model the process. Experiments on synthetic dataset and real-world dataset on MSMT17 supports the claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Most prior identifiability results assume conditional independence, yet this paper allows dependence, which is significant.\n2. Theory and identifibility results are clear and easy to follow.\n3. The model matches the theory well."}, "weaknesses": {"value": "1. The assumptions need some illustration to strengthen the readability. Is it possible to provide some illstrative explaination about the assumptions about under what circumstance then can be assured?\n2. The baseline results reported in the table 1 are higher than those in the original papers. Is there any difference on the setting or it is Top-5 accuracy instead?\n3. The theory is not new, i.e., Theorem 1 is mentioned in [1],  Theorem 2 is mentioned in [2], and Theorem 3 in [3]. The difference and similarity should be mentioned in the paper.\n\n[1] Identification of Nonparametric Dynamic Causal Structure and Latent Process in Climate System\n\n[2] On the identifiability of nonlinear ica: Sparsity and beyond.\n\n[3] Partial Identifiability for Domain Adaptation."}, "questions": {"value": "1. The identifiability is proven on $z$ only. Does $\\epsilon$ identifiabile as well? \n2. Theorem 2 and theorem 3 seem to be parallel. Each of them can be used independently to achieve component-wise identifiability. Can them be combined together to achieve stronger identifiability results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YG3gQn1muL", "forum": "4qHj8FkgLq", "replyto": "4qHj8FkgLq", "signatures": ["ICLR.cc/2026/Conference/Submission501/Reviewer_5L1z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission501/Reviewer_5L1z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762264304282, "cdate": 1762264304282, "tmdate": 1762915532473, "mdate": 1762915532473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}