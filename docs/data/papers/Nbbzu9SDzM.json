{"id": "Nbbzu9SDzM", "number": 18341, "cdate": 1758286640873, "mdate": 1763657454664, "content": {"title": "Recasting Transformer Layers as Energy Models", "abstract": "Foundation models rely on sequence-to-sequence mappings parameterized by neural networks, and the design space of these layers continues to expand. Transformer layers remain the dominant choice due to their strong performance and high parallelism, though many design decisions are still empirically based. We introduce causal energy minimization (CEM), a framework that interprets each transformer layer as an algorithm for solving an energy minimization problem with causal structure. This perspective separates the mathematical interpretation of a layer from its numerical realization, offering a unifying lens for layer design and motivating principled architectural innovations. Within CEM, multi-head attention emerges as a gradient step on an interaction energy under the weights sharing constraint, while gated MLP correspond to element-wise energies. The form of transformer components within CEM suggests a weight-sharing scheme in both attention and MLP blocks: we show that this yields parameter-efficient layers with negligible performance loss. Further, the CEM interpretation suggests appealing extensions to the transformer architecture: pre-conditioner-matrices for residual connections, diagonal matrices for inter-token-distances in attention, and multiple gradient-steps (a form of layer re-use) for both attention and MLP blocks. We show that these ideas that occur naturally in CEM lead to improvements on language modelling tasks, positioning CEM as a blueprint for principled and extensible architecture design.", "tldr": "", "keywords": ["transformers", "energy-based models", "layer design", "language modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1b34f485897146e0f749af31fce123cb857b54e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reinterprets Transformer layers from an optimization perspective. It shows that both multi-head attention (MHA) and gated MLP blocks can be expressed as single gradient descent steps on simple energy functions: a log-sum-exp interaction energy for attention and an elementwise energy for MLP. This provides a unified view of Transformer blocks as implicit optimizers. Based on this view, the authors propose architectural variants such as diagonal + low-rank parameterization, lightweight preconditioners, and multiple inner steps (T > 1), achieving improved parameter efficiency and perplexity on language modeling benchmarks. The framework is closely related to Hopfield network interpretations but extends them by covering both attention and MLP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Unified Framework**: Provides a single energy-based optimization perspective that unifies MHA and MLP within the Transformer block.\n\n**Actionable Insights**: Goes beyond theoretical interpretation by proposing architectural modifications directly derived from the framework (diagonal + low-rank parameterization, preconditioners, T-step updates).\n\n**Empirical Gains**: Demonstrates improved parameter efficiency and lower perplexity on language modeling benchmarks."}, "weaknesses": {"value": "**Overlap with prior work**: The paper’s framing is conceptually close to Hopfield-based interpretations of attention. The novelty relies heavily on unifying MHA and MLP.\n\n**Lack of formal guarantees**: There is no analysis of convergence, stability, or approximation error when T > 1. The theory remains at an intuitive level.\n\n**Experimental scale limitations**: Experiments are restricted to mid-sized models. No large-scale results (e.g., 7B+ parameters) are provided. It is unclear how the method scales.\n\n**Reproducibility**: No implementation or code is provided, which makes it difficult to fully verify the empirical claims."}, "questions": {"value": "Minor Errors\n\nLine 25:  pre-conditioner ->  preconditioner\n\nLine 26: inter-token-distances -> inter-token distances\n\nLine 20: weights sharing -> weight-sharing\n\nLine 39: transformers continues -> transformers continue\n\nLine 177: Comparing equations equation 6 and equation 10 -> Comparing Equation (6) and Equation (10)\n\nLine 188: Diagonal-plus-low-rank parameterisation In Section 2.1 -> Diagonal-plus-low-rank parameterisation. In Section 2.1\n\nLine 269: netowrks -> networks\n\nLine 742: to that the energy -> so that the energy\n\nLine 755: components.Table 1 -> components. Table 1\n\nThe term Transformer is inconsistently capitalized throughout the manuscript (e.g., “Transformer” vs. “transformer”).\n\nLine 253: \\mathbf{k}_{1:i} -> \\mathbf{k}^k_{1:i}, \\mathbf{v}_{1:i} -> \\mathbf{v}^k_{1:i}"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gwzuvLIMVY", "forum": "Nbbzu9SDzM", "replyto": "Nbbzu9SDzM", "signatures": ["ICLR.cc/2026/Conference/Submission18341/Reviewer_5RHo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18341/Reviewer_5RHo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226643785, "cdate": 1761226643785, "tmdate": 1762928052297, "mdate": 1762928052297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework called Causal Energy Minimization (CEM), which reinterprets each component of Transformer layers (such as multi-head attention mechanism MHA and gated multilayer perceptron gated MLP) as an optimization algorithm for solving energy minimization problems with causal structure. This framework separates the mathematical semantics of the layer (the defined energy function) from its numerical realization (the optimization algorithm used to minimize the energy), thereby providing a unified perspective for understanding and designing Transformer layers. The paper reexamines the Transformer through the lens of energy models, offering a principled and extensible viewpoint."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work demonstrates originality in its unique framework that combines energy-based models (EBMs) with Transformer layers. The framework innovatively formalizes each Transformer layer as a causal energy minimization problem and derives MHA and gated MLP as single-step gradient updates on specific energy functions.\n2. The work is well-structured, with overall writing that is fluent and suitable for AI/ML readers.\n3. The work holds significance by providing a principled framework for Transformer architecture, potentially guiding future innovations. In the LLM era, architecture design still relies on empirical approaches (such as Llama-style), and CEM bridges optimization theory with sequence modeling, which may inspire more efficient and interpretable models."}, "weaknesses": {"value": "The main weaknesses of the paper lie in the insufficient scale and breadth of the experiments.\n\n1. The experiments are limited to small-scale models (with a maximum of 160M parameters) and a single dataset (SlimPajama), lacking validation on larger scales (such as 1B+ parameters) or diverse tasks. For example, Figure 2 shows that CEM performs effectively on small models, but in large models, weight sharing may lead to insufficient expressiveness, as larger models rely on more parameters to capture complex patterns."}, "questions": {"value": "Can larger models (such as 1B+ parameters) be included? If the authors can provide preliminary results showing CEM's performance on larger scales, this would greatly enhance my confidence, as small-model results may not represent large-model behavior."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "G4x71mRvK0", "forum": "Nbbzu9SDzM", "replyto": "Nbbzu9SDzM", "signatures": ["ICLR.cc/2026/Conference/Submission18341/Reviewer_i5ES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18341/Reviewer_i5ES"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747191006, "cdate": 1761747191006, "tmdate": 1762928051930, "mdate": 1762928051930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Causal Energy Minimization (CEM), a novel framework that reinterprets Transformer layers as algorithms for solving energy minimization problems with causal structure. The key insight is that multi-head attention (MHA) emerges naturally as a gradient descent step on an interaction energy function, while gated MLPs correspond to gradient updates on element-wise energies. This energy-based perspective reveals that standard Transformer components can be derived under weight-sharing constraints: specifically, $W_K = W_V$ and $W_Q = W_O$ in attention, and $W^{d⊤} = W^u = V$ in MLPs. \n\nCEM provides a principled framework for understanding why current architectures work and offers systematic guidance for designing new components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Building on the energy optimization perspective, the authors propose three principled enhancements: \n- (1) Diagonal-plus-low-rank parameterization for attention matrices ( $A_k = \\text{diag}(d_k) + W_k^{Q⊤}W_k^K$ ) to capture richer token interactions.\n- (2) Learned lightweight preconditioners in diagonal-plus-low-rank form to accelerate convergence. \n- (3) Within-layer recursive updates (multiple gradient steps, T=2) that improve optimization without adding parameters. \n\nExperiments on SlimPajama with models ranging from 86M to 162M parameters demonstrate that CEM layers serve as effective parameter-efficient replacements for standard Transformer components: CEM attention uses approximately half the parameters with minimal performance loss, while recursive updates (T=2) with preconditioners can even outperform Llama baselines. Full end-to-end CEM Transformers achieve slightly better perplexity at optimal learning rates while using considerably fewer parameters. The framework positions CEM as a blueprint for principled architecture design, though the authors acknowledge that custom kernels and hardware co-design remain important future work to fully realize efficiency gains in practice."}, "weaknesses": {"value": "One of my primary concerns is that while the energy model framework elegantly echoes the mathematical form of attention in Transformer architectures, the connection to language modeling tasks appears fundamentally superficial. I acknowledge that numerous prior works have reinterpreted attention through various theoretical lenses—such as kernel regression, implicit gradient descent, or associative memory retrieval—but the question remains: **why should we view Transformers specifically as energy models for language modeling?** The paper lacks language-modeling-specific analysis that would justify this perspective. For instance, there is no investigation into how the energy minimization interpretation improves training stability, gradient flow dynamics, loss landscape geometry, or generalization properties in the context of next-token prediction. Without such task-relevant analysis, the energy model theory risks being merely a post-hoc mathematical reformulation rather than providing actionable insights. Does minimizing the proposed interaction energy $\\epsilon(x_i|h_{1:i})$ correspond to any meaningful linguistic or semantic objective? How does this relate to the cross-entropy loss used in language model training? These connections are not established.\n\nFurthermore, I have significant concerns regarding the practical viability of the recursive looping mechanism in CEM-enhanced multi-head attention. The paper reports that recursion becomes unstable for $T \\geq 3$ (mentioned on page 11: \"we only study recursive steps until T=2 because T≥3 has unstable performance and leads to OOM for larger model sizes\"). This suggests the approach has inherent scalability limitations. **What is the fundamental cause of this instability?** Is it due to gradient explosion/vanishing through the recursive loops, insufficient regularization, or the energy landscape becoming pathological with deeper unrolling? The lack of ablation studies on this instability is concerning. \n\nAdditionally, the authors do not compare their within-layer recursion to simpler alternatives like adding more Transformer layers with the same parameter budget, which would clarify whether the recursion provides unique benefits beyond simply increasing model depth. The trade-off between the theoretical appeal of iterative energy minimization and the practical constraint of T≤2 undermines the motivation for this design choice."}, "questions": {"value": "Could you provide empirical evidence demonstrating that the energy minimization perspective offers practical benefits specific to language modeling tasks?*\n\nFor example:\n- How does the energy landscape correlate with language modeling loss during training? \n- Does lower energy at intermediate layers correspond to better next-token prediction accuracy?\n- Can you show that models trained with explicit energy minimization objectives (e.g., auxiliary losses based on $\\epsilon(x_i|h_{1:i})$) improve perplexity, training stability, or sample efficiency compared to standard cross-entropy training?\n\nWithout such task-specific validation, it remains unclear whether the energy model framework is more than a mathematical reinterpretation that happens to fit the Transformer equations.\n\nIs the instability due to gradient pathologies (vanishing/exploding), optimization challenges in the energy landscape, or memory constraints?\n\nHave you tried stabilization techniques such as gradient clipping per recursive step, layer normalization within the loop, adaptive step sizes $\\eta^{(t)}$, or energy-based early stopping criteria?\n\nCan you provide gradient norm statistics, loss curves, or energy trajectories that reveal when and why the recursion diverges?\n\nTest perplexity at matched FLOPs? and Training throughput and memory usage?\n\np.s. overall I think this paper is quite interesting in terms of the parameter reduction and looping mechanism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z41GxTwZZt", "forum": "Nbbzu9SDzM", "replyto": "Nbbzu9SDzM", "signatures": ["ICLR.cc/2026/Conference/Submission18341/Reviewer_jswD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18341/Reviewer_jswD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816640587, "cdate": 1761816640587, "tmdate": 1762928051452, "mdate": 1762928051452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study utilizes a connection between Transformer and energy-based models. In particular, multi-head attention and gated MLP are two such components that can be viewed through the lens of energy-based models. This type of modeling yields a principled framework that formulates Transformer layer as solving an energy minimization problem, which also promotes interpretability. The proposed CEM is different from previous approaches in a sense that it takes account of input sequence history (i.e. causality). And weights are tied between projection operations, which leads to parameter efficient design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Providing a new framework to view Transformer in more principled way. This can help interpretability and further optimization for popular Transformer architectures."}, "weaknesses": {"value": "1. Reframing Transformer architecture with energy models will bring benefit such as weight sharing and thus parameter efficiency. However, this also takes away the freedom of standard Transformer architecture, where one can independently learn different weights.\n\n2. Eq. in 116, if we look at the equation closely, it appears that in Fig. 1 top left, W_O and W_V should be switched.\n\n3. 119, 'concatenated' and 116 eq. summation operator doesn’t match.\n\n4. 180, The integration formulation here doesn't look correct. I believe one should define the integration from -\\infty. (cf. cumulative distributions function and probability density function formulation) Also, GELU and SiLU both are non-zero for their negative domain. So I doubt this kind of construction is mathematically sound.\n\n5. Eq. 7, Why should this thing be called energy term? This looks far-fetched to be called the energy term.\n\n6. 188 paragraph, Authors empirically found that adding the diag term is more effective. However, A_k is learnable anyway. And I don't get the idea of combining both low-rankness and diagonality. If low-rankness doesn't help much and adding the diag term actually improved the performance, then it means imposing low-rankness from the first place is wrong.\n\n7. 203, Similarly to #6, where's justification? Why UV and VU added together and why multiplying this preconditioned to the existing gradient enhances gradient descent? Plus, if enforcing semipositive definite is important, why not starting from Eigenvalue decomposition form with a diagnoal matrix in the middle (three matrices)?\n\n8. 229-231, Future Work, Is it fair to not to include those looped Transformers in the experiments, as they are similar to CEM? Those references are published in 2019, 2023, and 2024 and authors should' taken them into account as well.\n\n9. Fig. 1 bottom right, There's a recursion for CEM MLP. However, there's no related explanation in the text.\n\n10. Alg. 1, MHA subroutine, last line of the inner for loop is problematic since it’s already in the for loop of k. In addition, the minus sign should be changed to the plus. See Sect. 2.1.\n\n11. Alg. 1, Main Blk has RMSNorm inside the 2nd for loop. This is unnecessary since it is already inside the MLP subroutine. Also in the MLP subroutine, having RMSNorm inside the for loop is wrong, since it will normalize every iteration with a new t.\n\n12. Similarly to #10, Alg. 1, MLP subroutine, the minus sign for the gradient descent step is wrong. See Sect. 2.2.\n\n\n[editorial comments]\n1. 461, recursion,can -> recursion, can\n2. 473, yield -> yields\n3. 'weights sharing' and 'weight sharing' are interchangeably used throughout the manuscipt. => be consistent\n4. parameterize/paramerise => be consistent\n5. Fig 1, weights such as W_Q => subscript to superscript change (i.e. W^Q) needed for consistency with later equations\n6. 129, needs a curly brace pair for the set notation for A_k\n7. 129, D_h feature dimension should've been appeared earlier after 120, along with a definition for D_r\n8. Energy notation E mismatch throughout the manuscript: 60, Eq. 1, including \\eta^\\epsilon in Eq. 3\n9. 719, Isn't -(1 / \\tau) supposed to be -\\tau? Cf. Eq. 1\n10. 177, equations equation 6 and equation 10 -> equations 6 and 10\n11. Eq. 12, RHS inside softmax, need the set notation cf. Eq. 4\n12. Eq. 12, triangle should change to nabla, unless you want to define a new thing with \\triangle\n13. Eq. 12, Superscript \\epsilon actually should be the name of the function\n14. Same problem for Eq. 13\n15. 215, could be trained provide -> could be trained to provide\n16. 222, 227, and Alg. 1 have \\eta's with subscripts to denote different energy function name, however Eqs. 3 and 9 have \\eta's with superscripts.\n17. Alg. 1, MHA subroutine, Delete the superscript k from the 1st line. This is because in the main block, MHA is only invoked after all K iterations.\n18. 269, netowrks -> networks"}, "questions": {"value": "Please see my major comments above. I could've given a higher score if I hadn't found that many issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ivStY9lc3Q", "forum": "Nbbzu9SDzM", "replyto": "Nbbzu9SDzM", "signatures": ["ICLR.cc/2026/Conference/Submission18341/Reviewer_j463"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18341/Reviewer_j463"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035237066, "cdate": 1762035237066, "tmdate": 1762928051109, "mdate": 1762928051109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}