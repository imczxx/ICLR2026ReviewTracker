{"id": "MQmrcX5jnk", "number": 20106, "cdate": 1758302544493, "mdate": 1759897001255, "content": {"title": "Learning Boltzmann Generators via Constrained Mass Transport", "abstract": "Efficient sampling from high-dimensional and multimodal unnormalized probability distributions is a central challenge in many areas of science and machine learning. We focus on Boltzmann generators (BGs) that aim to sample the Boltzmann distribution of physical systems, such as molecules, at a given temperature. Classical variational approaches that minimize the reverse Kullback–Leibler divergence are prone to mode collapse, while annealing-based methods, commonly using geometric schedules, can suffer from mass teleportation and rely heavily on schedule tuning. We introduce *Constrained Mass Transport* (CMT), a variational framework that generates intermediate distributions under constraints on both the KL divergence and the entropy decay between successive steps. These constraints enhance distributional overlap, mitigate mass teleportation, and counteract premature convergence. Across standard BG benchmarks and the here introduced *ELIL tetrapeptide*, the largest system studied to date without access to samples from molecular dynamics, CMT consistently surpasses state-of-the-art variational methods, achieving more than 2.5× higher effective sample size while avoiding mode collapse.", "tldr": "Constrained Mass Transport (CMT) is a variational framework for Boltzmann generators that improves high-dimensional multimodal sampling by constraining KL divergence and entropy decay, reducing mode collapse, and outperforming SOTA methods.", "keywords": ["sampling", "Boltzmann generators", "annealing"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/022b0e75b2964fd21da1269eb9af22ffee236f80.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Constrained Mass Transport (CMT) for learning Boltzmann Generators via a sequence of constrained variational updates. The method proposes to minimize $\\mathrm{KL}(q||p)$ subject to (1) a trust-region constraint $\\mathrm{KL}(q||q_i)≤\\varepsilon_{\\text{tr}}$ and/or (2) a bound on entropy decay $H(q_i)−H(q)≤ \\varepsilon_{\\text{ent}}$. The authors derive closed-form optimal intermediate densities and show these induce geometric, tempered, or geometric-tempered annealing paths when (1), (2), or both are active, respectively. The authors demonstrate gains in performance under matched target evaluation budgets of the method on conformational sampling for Alanine peptide systems (dipeptide, tetrapeptide, and hexapeptide) and a new ELIL tetrapeptide system. CMT improves EUBO, ESS, and Ramachandran TV under matched target-evaluation budgets. Ablations indicate both constraints are needed to avoid mode collapse and stabilize ESS across steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to understand. \n\n2. The method is conceptually clean. The paper leverages known existing ideas in reinforcement learning and cleverly adapt them to the sampling problem. The constrained objectives yield analytic intermediates and a unified view of annealing paths.\n\n3. Strong empirical results across peptide systems and metrics under fixed energy budgets.\n\n4. Useful ablations highlighting the complementary roles of the trust-region and entropy constraints."}, "weaknesses": {"value": "1. It would strengthen the paper to report Energy-W2 and TICA-W2 alongside the current metrics. Rama-TV (and Torus-W2 / TICA-W2) are informative at the macro level, but a model can score well on such macro metrics while still missing fine-grained details e.g., bond-length, that Energy-W2 would better capture. TICA-W2 adds a complementary macro view tied to slow collective modes. Including these would provide a more complete picture of both global conformational coverage and local physical accuracy. [1] \n2. Providing non-ML baselines (MD and SMC) for comparison under the same energy evaluation budget would better contextualize the contribution.\n3. Authors provide an analysis for $\\varepsilon_{\\text{tr}}$ vs (ESS lower bounds) but not for $\\varepsilon_{\\text{ent}}​$. It would be interesting to see this.\n\n[1] Tan, Charlie B., et al. \"Scalable equilibrium sampling with sequential boltzmann generators.\" arXiv preprint arXiv:2502.18462 (2025)."}, "questions": {"value": "1. Have you tried Cartesian coordinates? Does CMT’s path construction help maintain overlap in higher dimensions and improve stability in training?\n2. It would be interesting to see how this approach compares to sequential boltzmann generators that are trained on MD data to see if these energy-only methods are approached data-driven. [1]\n3. Any special initialization for $q_0$​? How often do you observe divergence or mode collapse? \n4. For alanine dipeptide and tripeptide, how does this compare to PITA (which is run on all-atom coordinates) [2]\n\n[1] Tan, Charlie B., et al. \"Scalable equilibrium sampling with sequential boltzmann generators.\" arXiv preprint arXiv:2502.18462 (2025).\n[2] Akhound-Sadegh, Tara, et al. \"Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities.\" arXiv preprint arXiv:2506.16471 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sZpLG1NDUf", "forum": "MQmrcX5jnk", "replyto": "MQmrcX5jnk", "signatures": ["ICLR.cc/2026/Conference/Submission20106/Reviewer_dNiK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20106/Reviewer_dNiK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751208494, "cdate": 1761751208494, "tmdate": 1762933002748, "mdate": 1762933002748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a mathematically elegant approach for imposing both trust-region and entropy constraints on a mass transport problem.  This results in improved learning of the target distribution."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written and motivated. The model is trained using energy evaluations, not data samples, which is a challenging and largely (for useful systems) unsolved problem.  The evaluations are sound and target larger than typical systems for this field (including a more complex peptide data set that will be contributed). Meaningful ablations are performed."}, "weaknesses": {"value": "If there is a discussion of the computational efficiency of training and inference I can't find it - especially compared to other approaches."}, "questions": {"value": "How does CMT balance exploration and convergence?\n\nWill there be issues with exploration as systems scale up to proteins with rugged energy landscapes? Or will a uniform distribution on internal coordinates be sufficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RY2rtBv5CA", "forum": "MQmrcX5jnk", "replyto": "MQmrcX5jnk", "signatures": ["ICLR.cc/2026/Conference/Submission20106/Reviewer_QZLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20106/Reviewer_QZLu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974105931, "cdate": 1761974105931, "tmdate": 1762933002148, "mdate": 1762933002148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles sampling by learning normalizing flows along an annealing path defined by a sequence of constrained variational problems. At each step, it minimizes $\\text{KL}(q\\|p)$ to the target $p$ subject to two constraints: (i) a trust-region bound on $\\text{KL}(q\\|q_{\\text{prev}})$ and (ii) a limit on entropy decay. The first constraint helps preserve overlap between iterates and the second helps avoid mass teleportation. They evaluate the approach on several high-dimensional molecular systems, including the ELIL tetrapeptide, which they introduce, and show consistent gains, particularly in effective sample size, over recent variational baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is clearly written and easy to follow. The motivation for combining trust-region and entropy constraints is well explained, and the authors do a good job connecting these ideas to the practical issues of maintaining overlap and avoiding mode collapse during annealing. While the individual components are known, their combination in this specific setting is original and well justified. The experimental section is strong and demonstrates clear empirical gains. Introducing the ELIL tetrapeptide as a new benchmark is also a nice contribution that could help future work in this area."}, "weaknesses": {"value": "From a technical standpoint, the contribution is somewhat incremental. The main novelty is combining two existing constraints (trust-region and entropy regularization) within a single variational framework, rather than introducing a new theoretical ingredient. This does not detract from the paper given the strong empirical results and clear exposition."}, "questions": {"value": "1) Can the authors clarify or provide intuition for why the entropy constraint mitigates mass teleportation? Is there a theoretical justification?\n\n2) How robust is performance to the optimization of the Lagrange multipliers? Normalizing constant estimation can be tricky, which suggests possible issues, but one could also imagine that errors just \"shift the annealing path\" without hurting final performance.\n\n3) How expensive is the constrained optimization relative to the rest of the training pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aqUrHw2qYd", "forum": "MQmrcX5jnk", "replyto": "MQmrcX5jnk", "signatures": ["ICLR.cc/2026/Conference/Submission20106/Reviewer_jMV6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20106/Reviewer_jMV6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096977150, "cdate": 1762096977150, "tmdate": 1762933001621, "mdate": 1762933001621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work “Learning Boltzmann Generators via Constraned Mass Transport” deal with the learning of a Boltzmann distritbution throuhg the minimization of the reverse KL divergence, sometimes named Boltzmann Generators. The authors proposes a variational approach based on a annealing procedure, where the optimization scheme is further constrain by (i) contraining the entropy between successing temperatures and (ii) the divergence between two successives temperature. The author shows that their result present better results than other approach, and show in their experiment that their protocol doesn’t seem to suffer from mode collapse."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The papers proposes a reasonably simple protocol to optimize over the KL divergence adding further constraints. The authors derive the optimal distribution of their optimization constraint, very similar to the classical reference Neal 2001. Their experiments seem quite convincing that their method seem to perform better than the others without suffering from mode collapse."}, "weaknesses": {"value": "To my opinion, the authors do not provide a clear explanation on why their method should work best w.r.t other annealing scheme. At least, it was not clearly explained why constraining the entropy and the KL divergence should tackle the mass teleportation. It would be a strong added value to have a analytical or intuitive explanation, possibly on a toy example, on why this new optimization schemes avoid this problem."}, "questions": {"value": "1. I think it would be a strong added value to be able to apply the method on a toy model suffering from mass teleportation to understand how the optimization scheme managed to mitigate this problem.\n\n    2. It is not discuss if the additional constraint add make the method slower w.r.t. other approaches, and if yes how much slower ? That would be a valuable information.\n\n    3.It looks to me the manuscript overlooked some references which seems closely related to the present work, which might be worth discussing, concerning respectively mode collapse when using KL divergence,( Soletskyi, Gabrié, Loureiro, B. (2025). “A theoretical perspective on mode collapse in variational inference”.); the use of normalizing flows (Gabrié, Rotskoff, Vanden-Eijnden, E. PNAS (2022). “Adaptive Monte Carlo augmented with normalizing flows”); and the use of a well adapted annealing path with a different protocol to avoid mass teleportation (“Fast training and sampling of Restricted Boltzmann Machines”, Béreux et al, ICLR 2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2TwLlaBz3X", "forum": "MQmrcX5jnk", "replyto": "MQmrcX5jnk", "signatures": ["ICLR.cc/2026/Conference/Submission20106/Reviewer_hjaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20106/Reviewer_hjaT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762170333798, "cdate": 1762170333798, "tmdate": 1762933001158, "mdate": 1762933001158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of sampling from unnormalized densities with Boltzmann generators. It proposes Constrained Mass Transport (CMT), which introduces trust-region and entropy constraints to effectively learn an adaptive annealing schedule that interpolates between the prior and the target distributions. The proposed method is evaluated on several molecular systems (Alanine dipeptide, tetrapeptide, hexapeptide, and ELIL tetrapeptide)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of learning an adaptive schedule for the annealing path is conceptually interesting and well-motivated.\n* The paper is well-written and easy to follow. The mathematical details are clearly presented, and the proofs appear sound as far as I could verify."}, "weaknesses": {"value": "* **Unclear specification of the number of annealing steps.** The number of annealing steps ($I$) appears to be a hyperparameter but is never explicitly stated. From Table 7, it seems to be roughly 200 steps for ALDP (400k total / 2000 per step ≈ 200).\n    * This number is quite large, and it is unclear whether the reported gains over TA-BG stem from the adaptive schedule itself or simply from using many more steps. Ablations comparing CMT to TA-BG with an equivalent number of steps would clarify this.\n    * Additionally, the conceptual role of $I$ is somewhat unclear to me, once the trust-region parameter $\\epsilon_{\\mathrm{tr}}$ is introduced. Since $\\epsilon_{\\mathrm{tr}}$ already constrains the KL divergence between successive distributions and, as a result,  implicitly determines how many steps are required, fixing both $I$ and $\\epsilon_{\\mathrm{tr}}$ doesn’t make sense to me.\n\n* **Metric design and presentation are confusing and seem cherry-picked.** Several of the most representative metrics appear only in the appendix (Table 2). In the main table, the authors report only Rama TV. Why was this specific metric chosen? Even if one chooses to report total variation, shouldn’t the reweighted Rama TV be reported, given that the process is not explicitly pinned to the target distribution? Similarly, the reweighted Rama $\\mathbb{T}\\text{-}W_2$ metric is omitted. The rationale for which metrics appear in the main results versus the appendix should be clarified.\n\n* **Mismatch in optimization budgets across methods.** The number of gradient steps differs substantially between CMT and TA-BG (see Tables 7 vs 8–9 in the appendix). While the authors mention equalizing the number of target evaluations, comparisons should also be made with the same number of gradient steps. Otherwise, it isn’t clear how this plays a role in the reported performance improvements.\n\n* **Hyperparameter selection seems ad-hoc and system-specific.** The trust-region bound $\\epsilon_{\\mathrm{tr}}$ is fixed globally, but the entropy bound $\\epsilon_{\\mathrm{ent}}$ varies per molecular system with no principled tuning rule. Appendix C.4 even notes that entropy tuning is difficult. This per-system manual adjustment undermines the claim of a “learned schedule.” The authors should provide clearer guidance or automatic criteria for selecting $\\epsilon_{\\mathrm{ent}}$, and ablations showing its impact on training stability and performance.\n\n* **Limited intuition about the learned schedule.** The paper emphasizes that CMT learns the annealing path, yet it never visualizes or analyzes what that path looks like. Providing intuition about the learned schedule and explaining why it may be advantageous over those in TA-BG would make the results much more interpretable.\n\n* **No analysis of $\\log Z$ along the annealing path.** Since the learned intermediates depend on Monte Carlo estimates of the partition function, it is important to assess how accurate those estimates are throughout the path. The paper only reports overall $\\log Z$ bounds (ELBO/EUBO) after training, with no diagnostics of these estimates or variance of the weights along the trajectory. Showing how $\\log Z$ (or ESS) evolves over the annealing path would substantiate the claim of the papers and make it easier to evaluate the effectiveness of the proposed adaptive schedule.\n\n* **Missing justification for how CMT avoids mass teleportation.** My understanding of “mass teleportation” follows from [1], where the linear interpolation path results in an ill-conditioned and exploding vector field, which results from the fact that the relative weight of the modes isn’t preserved. However, by this definition, I do not understand how constraining the entropy decay helps. Can the authors clarify what they mean by mass teleportation?\n\n\n[1] Máté, B., & Fleuret, F. (2023). Learning Interpolations between Boltzmann Densities. arXiv preprint arXiv:2301.07388. https://arxiv.org/abs/2301.07388"}, "questions": {"value": "See questions/weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PXc4TwIG5u", "forum": "MQmrcX5jnk", "replyto": "MQmrcX5jnk", "signatures": ["ICLR.cc/2026/Conference/Submission20106/Reviewer_VDn6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20106/Reviewer_VDn6"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762204226198, "cdate": 1762204226198, "tmdate": 1762933000707, "mdate": 1762933000707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}