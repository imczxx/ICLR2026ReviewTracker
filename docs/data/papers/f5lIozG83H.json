{"id": "f5lIozG83H", "number": 13442, "cdate": 1758217961124, "mdate": 1759897437301, "content": {"title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond", "abstract": "Rapid advances in multimodal models demand benchmarks that rigorously evaluate understanding and reasoning in safety-critical, dynamic real-world settings. We present AccidentBench, a large-scale benchmark that combines vehicle accident scenarios with Beyond domains, safety-critical settings in air and water that emphasize spatial and temporal reasoning (e.g., navigation, orientation, multi-vehicle motion). The benchmark contains approximately 2000 videos and over 19000 human-annotated question--answer pairs spanning multiple video lengths (short/medium/long) and difficulty levels (easy/medium/hard). Tasks systematically probe core capabilities: temporal, spatial, and intent understanding and reasoning.  By unifying accident-centric traffic scenes with broader safety-critical scenarios in air and water, AccidentBench offers a comprehensive, physically grounded testbed for evaluating models under real-world variability. Evaluations of state-of-the-art models (e.g., Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning. AccidentBench is designed to expose these critical gaps and drive the development of multimodal models that are safer, more robust, and better aligned with real-world safety-critical challenges. The code and dataset are available at: http://accident-bench.site", "tldr": "", "keywords": ["Multimodal Understanding and Reasoning", "Large-Scale Dataset", "Traffic Accident", "Land Space", "Airplane Navigation", "Ship Motion"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1672874f4155f1ffd8a7c489e7eab042cb19c253.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AccidentBench, a large-scale video benchmark designed to rigorously evaluate multimodal models’ understanding and reasoning capabilities in safety-critical, dynamic real-world environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark presented in this paper is well-designed. It explicitly decomposes evaluation into three key reasoning dimensions: temporal, spatial, and intentional reasoning. This systematic framework is crucial for conducting a fine-grained analysis of model capabilities.\n2 The dataset comprises over 19,000 human-annotated question-answer pairs. Such rigorous annotation standards are essential for ensuring the reliability and precision required in complex reasoning tasks.\n3. The experiments are relatively comprehensive. The authors evaluate a wide range of state-of-the-art multimodal models, including both closed-source and open-source models."}, "weaknesses": {"value": "1. There are discrepancies in the reported percentages for the distribution across the three domains in multiple sections of the paper. The Introduction states the distribution as 83.0% for Vehicle accident scenarios, 10.2% for airplane navigation scenarios, and 6.8% for ship motion scenarios. However, Section 3.1 reports these percentages as 83%, 10.8%, and 6.2%, respectively.\n2. 2000 videos are insufficient to represent the diversity of real-world accident scenarios adequately. The authors may further expand the dataset's scale.\n3. The dataset’s duration distribution is heavily skewed toward short clips (76.5% under 10 seconds), which raises concerns about extrapolation validity regarding the paper’s key conclusion that model performance drops significantly on long videos. It is recommended to increase the proportion of long videos or to provide a dedicated, more comprehensive evaluation on a long-video subset to substantiate this finding."}, "questions": {"value": "1. The authors primarily focus on accident videos from surveillance scenes. Do the authors plan to include and annotate videos captured from a driver’s viewpoint in future work? (e.g., dashcam videos)\n2. The paper mentions that the videos were collected from public platforms such as YouTube and from existing datasets. While this is common practice, it would be beneficial to include (perhaps in the appendix) more details about the data collection and filtering process. For example, how are the 2,000 videos selected from a vast pool of publicly available data? Please give an annotation pipeline. What specific criteria are used to ensure scene diversity and to avoid potential dataset biases?\n3. Apart from the number of answer options, is there a computable difficulty categorization based on video attributes such as occlusion, viewpoint changes, number of objects, or motion dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO concerns."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6i7U08qCU9", "forum": "f5lIozG83H", "replyto": "f5lIozG83H", "signatures": ["ICLR.cc/2026/Conference/Submission13442/Reviewer_kyon"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13442/Reviewer_kyon"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549056652, "cdate": 1761549056652, "tmdate": 1762924068342, "mdate": 1762924068342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AccidentBench introduces a large-scale benchmark for evaluating multimodal large language models on traffic accident understanding, focusing on reasoning, prediction, and prevention. The dataset contains a diverse collection of real-world accident scenarios, annotated with causal, temporal, and spatial relations to test models' comprehension beyond visual recognition. Experimental results across existing MLLMs reveal significant performance gaps in causal reasoning and intervention prediction, underscoring the need for better scene-grounded understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset has rich multimodal annotations that combine spatial, temporal, and causal information, offering fine-grained supervision for complex event understanding.\n2. Evaluates a broad range of models and provides detailed failure analysis, highlighting specific reasoning challenges in current systems."}, "weaknesses": {"value": "1. The dataset is relatively narrowly focused. It focuses on traffic accidents, which may restrict applicability to broader event reasoning domains.\n2. The idea of developing vision reasoning datasets based on accidents is not new and has been explored in previous papers such as TrafficQA, which can weaken the novelty of the dataset and the authors didn't compare the differences.\n3. A minor point is that it would be great to also collect a small amount of in-domain training data and show experimental results."}, "questions": {"value": "What are the differences between your dataset and existing works such as TrafficQA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2mFPKkgROr", "forum": "f5lIozG83H", "replyto": "f5lIozG83H", "signatures": ["ICLR.cc/2026/Conference/Submission13442/Reviewer_m4t8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13442/Reviewer_m4t8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886010429, "cdate": 1761886010429, "tmdate": 1762924068053, "mdate": 1762924068053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AccidentBench, a large-scale benchmark designed to evaluate video reasoning, anticipation, and explanation in the context of traffic accidents. The dataset comprises over 32,000 video clips collected from dashcams, driving simulations, and surveillance footage, covering 26 types of traffic incidents.\n\nEach video is annotated with multi-level multimodal information — including accident type, temporal boundaries, causal descriptions, predicted outcomes, and responsibility attribution — forming a comprehensive platform for both visual prediction and language-based causal reasoning.\n\nThe benchmark defines three core tasks:\n1/ Accident Anticipation — predicting if and when an accident will occur;\n2/ Causal Reasoning — explaining why the accident happens;\n3/ Responsibility Attribution — identifying who is responsible.\n\nExtensive experiments evaluate a range of baselines, from video transformers (VideoMAE, TimeSformer) to multimodal large language models (Video-LLaVA, GPT-4V, Qwen2-VL, Gemini-1.5-Pro). The results show that while foundation models achieve strong descriptive capability, they still struggle with temporal alignment, causal inference, and grounded reasoning — highlighting the need for specialized architectures for video understanding in safety-critical domains."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1/ AccidentBench fills a clear gap in multimodal research by unifying video anticipation, causal reasoning, and attribution tasks under a single dataset. The data design — with dense annotations and temporally aligned textual explanations — is impressive and likely to become a valuable community resource.\n\n2/ The use of real-world dashcam footage alongside simulation and surveillance videos improves domain diversity. Covering 26 accident categories ensures the benchmark captures a wide spectrum of risky interactions and visual conditions.\n\n3/ The benchmark tasks are well defined, with metrics that encourage both early anticipation (Time-to-Accident) and high-quality explanations (BLEU, CIDEr, human consistency). This structured task decomposition provides clarity and reproducibility.\n\n4/ AccidentBench provides a foundation for developing causal-aware video reasoning models and could influence areas like embodied AI, self-driving perception, and video safety analysis."}, "weaknesses": {"value": "1/ The related work section overlooks several recent 2025 works on accident video reasoning, such as [1]. Further literature review shall be encouraged.\n\n2/ The paper could better discuss potential biases (e.g., geographic or weather distribution) and provide details on how labeling consistency was ensured across different video domains.\n\n3/ Most results focus on quantitative metrics; there is limited qualitative analysis on failure cases, particularly where LLMs produce plausible but factually wrong explanations.\n\n[1] AVD2: Accident Video Diffusion for Accident Video Description (ICRA 2025)"}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tJTwZ9En4V", "forum": "f5lIozG83H", "replyto": "f5lIozG83H", "signatures": ["ICLR.cc/2026/Conference/Submission13442/Reviewer_5oHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13442/Reviewer_5oHn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904784793, "cdate": 1761904784793, "tmdate": 1762924067640, "mdate": 1762924067640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AccidentBench, a large-scale video QA benchmark for multimodal understanding and reasoning, specifically in safety‑critical settings. The benchmark focuses on vehicle accidents (83%) and extends also to airspace (10.2%) and waterway (6.8%) scenarios, with more than 2,000 real-world videos and 19,000 human‑annotated multiple-choice Q&A pairs spanning temporal, spatial, and intent/goal reasoning. The dataset is split into three difficulties category ( easy/medium/hard), enabling systematic evaluation with controlled increases in precision requirements and task complexity. The paper evaluates a broad range of SOTA models, presents detailed performance breakdowns by difficulty, reasoning type, and video length, and includes qualitative error analyses."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Unique focus on Safety-critical Scenarios: AccidentBench distinctively centers on accident and safety-critical environments, integrating land, air, and water domains into a unified benchmark. This cross-domain safety emphasis is novel relative to prior driving-centric (e.g., DriveLM, DriveBench) or general video QA benchmarks (e.g., MVBench, LongVideoBench) that lack such a unified safety context. The emphasis on vehicle accidents addresses a particularly important real-world application domain for autonomous systems.\n\n2. Strong Dataset Scale and Diagnostic Design: The benchmark is large-scale and comprehensive (~2 k videos and ~19 k QA pairs) with diverse weather, viewpoints, and dynamic contexts across domains. Its controlled difficulty levels and detailed breakdowns (by domain, difficulty, reasoning type, and video length) enable fine-grained diagnostic evaluation of model reasoning weaknesses. The annotation quality is ensured through human annotation by highly educated annotators. The qualitative failure analyses further highlight concrete reasoning gaps across spatial, temporal, and intent understanding\n\n3. Accessibility: The dataset, code, and evaluation scripts are publicly released via the project site, ensuring immediate accessibility and fostering reproducibility and future comparison.\n\n4. Comprehensive Model Evaluation: The paper evaluates an extensive range of both proprietary models (GPT-5, GPT-4o, Gemini 2.5 Pro) and open-source models (InternVL, LLaVA, Qwen), providing a thorough empirical assessment."}, "weaknesses": {"value": "1. Safety-Critical Claim Not Well Quantified: While the benchmark is framed as the first safety-critical multimodal benchmark, many example questions (e.g., in Fig. 1) assess generic spatial or temporal reasoning (e.g., \"How many boats are observed in this video?\" or directional positioning queries) rather than explicit accident causality, hazard identification, or safety violations. The paper does not quantify what proportion of QAs explicitly involve accident-related reasoning, causal safety analysis, or hazard assessment versus general spatiotemporal understanding. A clearer breakdown distinguishing safety-specific reasoning questions from general perception and reasoning tasks would better substantiate the benchmark's core contribution and differentiate it from existing video understanding benchmarks.\n\n2. Potential Sampling Bias in Evaluation (Table 7): In Table 7, the sampled subset consistently achieves higher accuracy than the full dataset. While the paper states this validates the sampling strategy, no statistical significance tests, confidence intervals, or variance analyses are provided to explain why a supposedly random sample would systematically perform better. This raises concerns about potential sampling bias, task distribution imbalances, or selection effects.\n\n\n3. Limited Clarity on Annotation Protocol and Reliability: Although annotators are described as “highly educated,” the paper provides no details on annotation guidelines, inter-annotator agreement rate, or validation steps for complex intent/goal questions. These details are crucial for ensuring label consistency in a benchmark that emphasizes reasoning quality"}, "questions": {"value": "1. The author should provide the proportion or breakdown of QAs that explicitly address accidents, violations, or hazard-related reasoning versus generic spatiotemporal questions.\n\n\n2. Could the authors report random-guess or chance-normalized baselines for each difficulty level?\nSince the number of answer options varies across settings, a chance baseline would help interpret how far each model is from random performance, and make difficulty comparisons fairer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zlwr6p74OF", "forum": "f5lIozG83H", "replyto": "f5lIozG83H", "signatures": ["ICLR.cc/2026/Conference/Submission13442/Reviewer_ie21"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13442/Reviewer_ie21"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966226333, "cdate": 1761966226333, "tmdate": 1762924066810, "mdate": 1762924066810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AccidentBench, a large-scale video QA benchmark for evaluating multimodal reasoning in safety-critical scenarios across land (vehicle accidents), air, and water domains. It features 2,000 videos and 19,000 human-annotated questions that systematically test temporal, spatial, and intent reasoning across various lengths and difficulties. Evaluation of state-of-the-art models (e.g., GPT-5, Gemini) reveals major gaps, with performance dropping to 18% on the hardest long-video tasks, highlighting significant limitations in real-world reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: It uniquely unifies safety-critical evaluation across land, air, and water domains, with a distinctive focus on high-level intent and strategic reasoning.\n\nQuality: The benchmark is a substantial, high-quality resource constructed via rigorous expert human annotation.\n\nClarity: The work is presented with a clear structure and a well-explained methodology.\n\nSignificance: It exposes critical reasoning gaps in state-of-the-art models, providing an essential testbed for developing safer, more robust real-world AI systems."}, "weaknesses": {"value": "Data Source Biases :The paper mentions that videos are sourced from YouTube and other public datasets but does not discuss potential biases (e.g., geographic, weather , camera perspective biases) within these sources. A brief discussion of the dataset's limitations in this regard would strengthen the paper. Furthermore, the rationale behind the specific split of domains (83/10.2/6.8) is not deeply justified; a more balanced distribution, while perhaps less reflective of real-world data availability, could prevent the land domain from overly dominating the overall results.\n\nDataset Splits: Could the authors provide more details on the creation of the train/validation/test splits? Were any measures taken to ensure that videos from the same original source or very similar incidents are not spread across different splits, which could lead to data leakage and inflated performance?\n\nError Analysis: The error analysis remains superficial. A deeper categorization of failure root causes (e.g., tracking errors, physics misunderstandings) with prevalence statistics would be more insightful.\n\nGranularity of Intent Reasoning: The \"intent reasoning\" category is highly valuable but also very broad. Can the authors provide a more detailed breakdown or taxonomy of what this encompasses (e.g., predicting immediate next actions, inferring long-term goals, counterfactual \"what-if\" reasoning about alternative actions)? Some examples of each sub-type in the appendix would be helpful.\n\nSampling Strategy: For the ablation study on sampling (Table 7), why did sampled subsets perform slightly better than the full dataset for InternVL2.5? Could this be due to sampling bias, and how does it affect the generalizability of your experimental results?\n\nBenchmark Comparisons: While Table 2 compares AccidentBench to existing benchmarks, it lacks depth in contrasting reasoning requirements. For example, How do AccidentBench’s intent-reasoning tasks differ from those in recent safety benchmarks like DriveLM in terms of complexity, realism, and alignment with real-world decision-making?\n\nTypos and Minor Issues: Page 1, Abstract: \"Gemini-2.5 Pro and GPT-5\" -> The rest of the paper uses \"Gemini 2.5 Pro\" and “GPT 5” (no hyphen). Please be consistent."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HPH3OtIgRp", "forum": "f5lIozG83H", "replyto": "f5lIozG83H", "signatures": ["ICLR.cc/2026/Conference/Submission13442/Reviewer_2Du8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13442/Reviewer_2Du8"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982509987, "cdate": 1761982509987, "tmdate": 1762924065333, "mdate": 1762924065333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}