{"id": "EkmcN2u1x4", "number": 11779, "cdate": 1758203738527, "mdate": 1763074347866, "content": {"title": "Normalization vs. Crops: Learning Gaze Representations via Constrained Rotation Optimization", "abstract": "Recent advances in appearance-based gaze estimation have adopted deep learning models to directly map face images to 3D gaze directions, but most existing methods rely on face normalization processes, which are costly and error-prone in unconstrained environments.\nWhile normalization-free approaches have been explored to address these challenges, they either discard the advantages of normalization in reducing appearance variability or lack a systematic understanding of the transformations involved.\nWe revisit this problem and formalize crop-based gaze estimation through Constrained Rotation Optimization (CROp), which models face cropping as a virtual camera rotation and defines a consistent mapping between crop and camera coordinates.\nWe further adopt multi-task learning to jointly estimate gaze and head pose, improving robustness without requiring explicit landmark-based preprocessing.\nThrough extensive evaluation, we show that crop-based estimation, when treated rigorously, is a reliable alternative to normalization, especially under extreme head poses and noisy preprocessing.\nOur analysis highlights the trade-offs between the two approaches and offers practical guidelines for effective and robust gaze estimation in real-world, unconstrained settings.", "tldr": "We propose a mechanism to replace the procedure of image normalization, or rectification, commonly adopted for gaze estimation, and we combine it with multi-task learning of head-pose to remedy the lack of explicit hpe performed during rectification.", "keywords": ["Appearance based gaze estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e8934b5ad84aa8217a67e1b362967a4f5ef6627d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper formalizes a normalization-free framework for gaze estimation through a constrained rotation optimization that treats cropping as a virtual camera rotation, combined with a joint gaze–head pose learning model. The idea is theoretically clean and the empirical evaluation is thorough. However, the actual contribution is incremental, as the proposed method’s advantage over standard normalization pipelines is relatively small and mostly in noisy or extreme pose conditions. The claim of providing a “reliable alternative” is only partially supported, since the improvement margins are minor and the reliance on detector stability remains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a well-defined and practical problemL: the trade-off between normalization-based and crop-based gaze estimation.\n\n2. The multi-task design that jointly predicts gaze and head pose is technically sound and demonstrates robustness under real-world degradations \n\n3. The experimental analysis is comprehensive, covering multiple datasets, realistic pre-processing conditions, and various degradation scenarios, which supports the paper’s claims. \n\n4. The method provides measurable efficiency benefits"}, "weaknesses": {"value": "1. Despite the theoretical novelty, the practical performance improvements over standard normalization are modest (typically within 5–10%), and not statistically or conceptually transformative. \n\n2. The method’s dependence on accurate bounding boxes introduces a hidden assumption of reliable face detection, which undermines the “normalization-free” claim. \n\n3. The mathematical formulation is overcomplicated relative to its empirical gain; the optimization step essentially reparameterizes standard coordinate alignment. \n\n4. There is a lack of comparison with stronger modern baselines (e.g., transformer-based gaze estimation, or geometry-aware self-supervised methods), which makes the claimed “reliability” less convincing. \n\n5. Writing-wise, the paper spends too much space re-deriving basic camera geometry and too little analyzing failure cases or uncertainty in predictions."}, "questions": {"value": "1. How sensitive is the proposed CROp formulation to detection noise or imperfect face bounding boxes? Since the method claims to be “normalization-free,” it would be helpful to quantify how robust it remains when the detector produces slight misalignments.\n\n2. Could the authors clarify why the joint gaze–head pose model cannot be directly trained under a standard coordinate alignment or transformer-based architecture? In other words, what concrete advantage does CROp offer beyond reparameterization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U86GmyGNtq", "forum": "EkmcN2u1x4", "replyto": "EkmcN2u1x4", "signatures": ["ICLR.cc/2026/Conference/Submission11779/Reviewer_pgzd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11779/Reviewer_pgzd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574725859, "cdate": 1761574725859, "tmdate": 1762922804530, "mdate": 1762922804530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "xzXMiQNCDP", "forum": "EkmcN2u1x4", "replyto": "EkmcN2u1x4", "signatures": ["ICLR.cc/2026/Conference/Submission11779/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11779/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763074347175, "cdate": 1763074347175, "tmdate": 1763074347175, "mdate": 1763074347175, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the context of appearance-based gaze estimation this paper approximates an image crop by a rotation, allowing the cropped image to be explained with a rotation of the original camera. The authors combine this with multi-task learning to jointly predict gaze and head pose. They demonstrate that crop-based estimation can match or exceed normalisation-based methods, particularly under extreme head poses and noisy preprocessing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The mathematical formulation of CROp is rigorous and well-motivated. The constrained optimisation problem (Eq. P) elegantly captures the relationship between crop and camera coordinates, with the Kabsch algorithm providing a principled solution.\n\nThe paper includes extensive experiments across multiple datasets (MPIIFaceGaze, ETH-XGaze, EYEDIAP, EVE) with both cross-dataset and within-dataset evaluations.\n\nThe approach eliminates landmark detection, reducing computational cost by 23% while maintaining or improving accuracy. This is significant for real-world deployment.\n\nThe paper systematically examines the contribution of each component (CROp transformation vs. multi-task learning) and tests robustness under synthetic noise and occlusions."}, "weaknesses": {"value": "While the formalization is clean, the core ideas are not entirely new:\n- Using face crops for gaze estimation has been explored (Gaze360, GazeOnce)\n- Multi-task learning for gaze and head pose has been proposed before\n- The main contribution is formalising the geometric relationship, which, while useful, feels incremental\n\nThe improvements are dataset-dependent and sometimes marginal:\n- On XGaze (Table 1), CROp GT achieves 7.41° vs. Norm 7.19° (worse)\n- Improvements are most pronounced on smaller/noisier datasets (MPII, EYEDIAP)\n- Within-dataset results (Table 2) show mixed outcomes in GT setting\n\nThe paper primarily compares against normalization but doesn't adequately compare with other normalization-free approaches:\n- EFE (Balim et al., 2023) is mentioned but not empirically compared\n- Gaze360's coordinate system is discussed but not experimentally evaluated\n- Missing comparisons with other recent methods (e.g., the CLIP-based methods cited)\n\nThe paper acknowledges that \"no single rotation can perfectly align the two views\" but doesn't quantify how large this approximation error is.\n\nFigure 5 shows degradation at extreme off-axis angles, but the analysis is limited.\n\nThe assumption that gaze origin coincides with bounding box centre is mentioned in the conclusion but not analysed.\n\nTable 4 shows that adding head pose sometimes hurts gaze performance. The paper dismisses this as \"likely due to the model's balancing\" without deeper investigation. No analysis of how to weight the two losses optimally."}, "questions": {"value": "1. Can you provide the actual approximation error of the CROp transformation (i.e., the residual from the constrained optimization)?\n2. How sensitive is the method to bounding box quality? The ±30% threshold in Table 5 seems high - what happens in more realistic scenarios?\n3. Why not compare directly with Gaze360's coordinate system approach experimentally?\n4. Can you provide computational cost comparisons for training time, not just inference?\n5. How does performance vary with face size in the image?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1uJN4q4KJk", "forum": "EkmcN2u1x4", "replyto": "EkmcN2u1x4", "signatures": ["ICLR.cc/2026/Conference/Submission11779/Reviewer_sLFB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11779/Reviewer_sLFB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782922690, "cdate": 1761782922690, "tmdate": 1762922804112, "mdate": 1762922804112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CROp (Constrained Rotation Optimization), a novel gaze estimation method that eliminates the need for face normalization, landmark detection, and head pose estimation by modeling face cropping as a virtual camera rotation. It uses multi-task learning to jointly estimate gaze and head pose, improving robustness in unconstrained environments. Experiments across multiple datasets show that CROp outperforms traditional normalization methods, especially under extreme head poses and noisy data, while reducing computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n    \n2. The proposed  CROp method offers an effective pre-processing approach for gaze estimation, eliminating the need for precise head pose estimation and facial landmark detection as those in conventional normalization. \n    \n3. Experiments are extensive, including both within- and cross-dataset evaluation, estimation errors, computational cost, influences of boundbox noise and occlusions."}, "weaknesses": {"value": "1. While the paper demonstrates that joint estimation of gaze and head pose improves robustness, the necessity of incorporating head pose as a joint task with gaze estimation is not fully justified. It would be helpful to provide a more detailed explanation of why head pose estimation is critical for improving gaze accuracy, especially in unconstrained settings. The paper could also address potential trade-offs or situations where gaze estimation might still perform well without explicitly estimating head pose.\n\n2. The process of face cropping is not clearly detailed in the paper. Specifically, it is important to clarify how the crop is performed, including the following aspects:\n    Crop Size: What is the size of the cropped region? Does the crop size vary depending on the subject or image? Does it adapt to different head poses or facial feature sizes?\n     Facial Feature Position Constraints: After cropping, are there any constraints on the positions of facial features (such as the eyes, nose, and mouth) within the cropped region? For example, are the facial features aligned or normalized in any way before proceeding with gaze estimation?\n\n3. While CROp generally outperforms the normalization method (Norm) in most cases, there are instances where Norm performs better, particularly in cross-dataset experiments that are trained on the XGaze dataset. It would be useful to further analyze and discuss these cases, particularly the conditions under which Norm may outperform CROp."}, "questions": {"value": "The paper primarily uses ResNet18 as the backbone for the experiments. While this is a reasonable choice, it would be beneficial to test CROp with other popular backbones and state-of-the-art (SOTA) methods to assess its generalizability and effectiveness across different architectures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "flzhLX9aQp", "forum": "EkmcN2u1x4", "replyto": "EkmcN2u1x4", "signatures": ["ICLR.cc/2026/Conference/Submission11779/Reviewer_91af"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11779/Reviewer_91af"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998688165, "cdate": 1761998688165, "tmdate": 1762922803588, "mdate": 1762922803588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}