{"id": "bIJkCBVSgd", "number": 9551, "cdate": 1758127118821, "mdate": 1759897712680, "content": {"title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense", "abstract": "Recent advances in large reasoning models (LRMs) have enabled remarkable performance on complex tasks such as mathematics and coding by generating long Chain-of-Thought (CoT) traces. In this paper, we identify and systematically analyze a critical vulnerability we term reasoning distraction, where LRMs are diverted from their primary objective by irrelevant yet complex tasks maliciously embedded in the prompt. Through a comprehensive study across diverse models and benchmarks, we show that even state-of-the-art LRMs are highly susceptible, with injected distractors reducing task accuracy by up to 60%. We further reveal that certain alignment techniques can amplify this weakness and that models may exhibit covert compliance, following hidden adversarial instructions in reasoning while concealing them in the final output. To mitigate these risks, we propose a training-based defense that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on synthetic adversarial data, improving robustness by over 50 points on challenging distractor attacks. Our findings establish reasoning distraction as a distinct and urgent threat to LRM reliability and provide a practical step toward safer and more trustworthy reasoning systems.", "tldr": "", "keywords": ["Large reasoning model", "Prompt Injection", "Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/516803b7dbda989e264c200e758b924f1210225a.pdf", "supplementary_material": "/attachment/0a72227016acd2c8cae37ec62b65a4f9811fb1ba.zip"}, "replies": [{"content": {"summary": {"value": "The work propose a distractor injection attack for reasoning models that hijacks the reasoning process to make the model deviate from its main task and perform a hidden task, affecting the final output of the model in the context of the primary task. Substantial empirical experiments are conducted across a broad range of distractor tasks, benchmarks, and models, showing consistently that reasoning models are prone to distractions. Furthermore, the work constructs a DPO and SFT dataset based on the attack for adversarial training and show that robustness to the attack can be increased. The work is especially interesting because the threat model incorporates injection attacks to LLM judges, a highly concerning setting that has gained recent attention due to malicious prompts being injected into paper submissions in anticipation of reviews generated by LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The threat model takes the setting of injection attacks that make the target LLM defy the primary task system prompt. As mentioned in the summary, this is a timely topic considering real-world injection attacks within paper submissions targeting possible LLM reviewers. It is surprising to see that a wide range of LRMs fall prey to this attack, which is alarming because reasoning models are gaining more popularity due to better performance. The work provides substantial empirical results to back this claim."}, "weaknesses": {"value": "1. I am confused about the practicality of the threat model outside of an LLM judge setting. When attacking an LLM judge with this attack, the assumption is that the user=attacker $\\neq$ judge, where some third-party is using the LLM as a judge. The attacker, hence user, wants to bypass the LLMs main judgement to hack for a higher reward for a certain response. However, when performing this attack on a normal task like MATH, the setting is that the user=attacker is just trying to get a correct answer to a MATH problem from their own LLM. Why would the user attack its own model and try to get a distracted reasoning or wrong answer?\n2. Regarding the point above, I would like to see examples of $P_sys$ (the system prompt) for each task (MMLU-Redux, MATH, IFEval, BFCL V3, JudgeLM) to better understand what the goal of the attacker is. This is also because the threat model assumes an attack is successful if $P_sys$ is violated. \n3. What is the performance of RPO [1] for adversarial training, which is DPO+SFT simultaneously?\n4. Why is Implicit Compliance not as concerning as Covert Compliance, if not more? Implicit Compliance reveals no recognition of distraction in both the reasoning and answer which seems more problematic than recognition in at least the reasoning regardless of exposure to the user.\n\n\n**References**\n\n[1] Pang, Richard Yuanzhe, et al. \"Iterative reasoning preference optimization.\" Advances in Neural Information Processing Systems 37 (2024): 116617-116637."}, "questions": {"value": "No further comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JI59Dys0aB", "forum": "bIJkCBVSgd", "replyto": "bIJkCBVSgd", "signatures": ["ICLR.cc/2026/Conference/Submission9551/Reviewer_PydC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9551/Reviewer_PydC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604834828, "cdate": 1761604834828, "tmdate": 1762921109621, "mdate": 1762921109621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new task called \"reasoning distraction\", a type of prompt injection. The author claim \"structurally distinct subclass that uniquely exploits the chain-of-thought process, rather than issuing direct command-style manipulations.\" (which I am not completely convinced). In other words, they claim that the goal of existing prompt injection is to \"Ignores instructions or produces irrelevant/unsafe content.\", but  \"reasoning distraction\" is to \"Solves an injected complex task, abandoning or corrupting the primary task.\".\n\nUnder the task setting, they design five categories of the Distractor Task, i.e., Mathematical Reasoning, Coding, Logical Reasoning, Symbolic Reasoning and Simple Arithmetic. Next, they evaluate a diverse set of SOTA LRMs. Finally, based on the distraction datasets, they do DPO to improve model's robustness to these distraction, and the performances demonstrate the effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It focuses on LLM reasoning safety issues, and demonstrate it is still alarming and SOTA LLMs are vulnerable to such attacks\n2. The analysis is comprehensive and systematic."}, "weaknesses": {"value": "1. The differences of reasoning distraction and regular prompt injection is not clear to me. Or put it in another way, why we need to care about such as unique category of the prompt injection? any different solutions should be considered? is that more serious/difficult to solve?\n2. There is no systematic comparison between regular prompt injection and the distraction task setup. \n3. More deeper analysis is encouraged, such as  can we detect such  objective diversion in latent space? and any lightweight solution? no training required."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "znIpqodPUB", "forum": "bIJkCBVSgd", "replyto": "bIJkCBVSgd", "signatures": ["ICLR.cc/2026/Conference/Submission9551/Reviewer_9mfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9551/Reviewer_9mfL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998341114, "cdate": 1761998341114, "tmdate": 1762921109254, "mdate": 1762921109254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}