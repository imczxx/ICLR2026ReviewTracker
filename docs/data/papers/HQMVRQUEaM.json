{"id": "HQMVRQUEaM", "number": 24834, "cdate": 1758360890749, "mdate": 1759896746155, "content": {"title": "Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion", "abstract": "Multimodal Large Language Models (MLLMs) have achieved notable success in enhancing translation performance by integrating multimodal information.\nHowever, existing research primarily focuses on image-guided methods, whose applicability is constrained by the scarcity of multilingual image-text pairs.\nThe speech modality overcomes this limitation due to its natural alignment with text and the abundance of existing speech datasets, which enable scalable language coverage.\nIn this paper, we propose a \\textbf{Speech-guided Multimodal Machine Translation (SMMT)} framework that integrates speech and text as fused inputs into an MLLM to improve translation quality.\nTo mitigate reliance on low-resource data, we introduce a \\textbf{Self-Evolution Mechanism}.\nThe core components of this framework include a text-to-speech model, responsible for generating synthetic speech, and an MLLM capable of classifying synthetic speech samples and iteratively optimizing itself using positive samples.\nExperimental results demonstrate that our framework surpasses all existing methods on the Multi30K multimodal machine translation benchmark, achieving new state-of-the-art results.\nFurthermore, on general machine translation datasets, particularly the FLORES-200, it achieves average state-of-the-art performance in 108 translation directions. Ablation studies on CoVoST-2 confirms that differences between synthetic and authentic speech have negligible impact on translation quality. We will open-source our model to support the wider community.", "tldr": "A Speech-guided framework that leverages synthetic speech and a self-evolution mechanism to improve translation", "keywords": ["Speech", "Multimodal Machine Translation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f2f0b374851ad2d090b9e6580baca842d98ca30.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a Speech-guided Multimodal Machine Translation (SMMT) framework that integrates both speech and text as fused inputs into a multimodal large language model (MLLM) to enhance translation quality. The system comprises a multimodal LLM and a text-to-speech (TTS) module, augmented by a self-evolution mechanism that enables iterative model refinement. Experimental results demonstrate competitive performance on traditional machine translation benchmarks and promising improvements on the Multi30K multimodal dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Innovative self-evolution mechanism: The framework introduces a novel self-improvement process that leverages translation performance metrics as optimization objectives, facilitating continuous enhancement through iterative evolution cycles.\n- Comprehensive multimodal and multilingual support: The approach effectively unifies speech and text modalities and extends to multiple languages, showing potential for broader generalization."}, "weaknesses": {"value": "- Limited evaluation scope: Experiments are primarily conducted on a small set of benchmarks (e.g., Multi30K, FLORES-200), leaving open questions about generalization to large-scale and domain-diverse datasets.\n- Dependence on external components: The framework relies heavily on open-source models, such as the TTS model, which may constrain performance consistency and scalability.\n- Lack of detailed ablations: It remains unclear how much each component (speech input, multimodal fusion, self-evolution) contributes to the observed performance gains."}, "questions": {"value": "- Could the authors clarify the conceptual and methodological differences between the proposed self-evolution mechanism and reinforcement learning approaches typically used for LLM fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yNT5r194Gw", "forum": "HQMVRQUEaM", "replyto": "HQMVRQUEaM", "signatures": ["ICLR.cc/2026/Conference/Submission24834/Reviewer_wxJV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24834/Reviewer_wxJV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703092417, "cdate": 1761703092417, "tmdate": 1762943213454, "mdate": 1762943213454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, a Speech-guided Multimodal Machine Translation (SMMT) framework is proposed. The proposed system accepts textual input and synthesizes speech via the TTS model. Then, the MLLM processes both the text and synthetic speech to generate translations. The model is trained with two stages: pre-training and self-evolution training. The pre-training includes ASR to align speech and text modalities, speech translation and MMT to enable model translation capacity. The self-evolution training is a loop based training, i.e., TTS generates speech with different voices, and are assigned negative/positive tags by comparing the MT score and MMT score. Only those positive samples are used to update the MMT model. The model is evaluated with COMET score. The whole training loop is repeated if the model keeps improving COMET score. \nThe experimental results show the proposed system can achieve very competitive results on the MLLM dataset Multi30K, and MT dataset FLORES-200. \nThere are some details are missing in the paper and more detailed analysis is required. For example, what're the key factors that synthetic voice could be a positive sample or beneficial for MMT? Why speech data could help to reduce under-translation issues?"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Propose a Speech-guided Multimodal Machine Translation (SMMT) framework\n- The system achieves very competitive results on multiple datasets"}, "weaknesses": {"value": "- It is understandable that information from multiple sources might help the machine learn better representation. But those features are real data instead of synthetic data. In this work, synthetic speech is used augmented MMT. Deep analysis is required to reveal the main factors that contribute the good performance. For example, we don't know the synthetic speech samples are negative or positive during inference time. What's the impact if the provided speech sample is negative one? How to select the reference voice? \n- The self-evolution training could be very expensive. It is helpful to provide the training time spend on this stage."}, "questions": {"value": "- ``We synthesize speech for the evaluation text using a fixed reference voice''. The model is updated with different voices (sec 2.4.1). How do you select the reference voice?\n- what're the sizes of models in table 3?\n- why speech data helps to reduce under translation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qb9Et8c1HF", "forum": "HQMVRQUEaM", "replyto": "HQMVRQUEaM", "signatures": ["ICLR.cc/2026/Conference/Submission24834/Reviewer_w9b6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24834/Reviewer_w9b6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813002878, "cdate": 1761813002878, "tmdate": 1762943213187, "mdate": 1762943213187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Speech-guided Multimodal Machine Translation (SMMT), i.e., improving text translation by providing semantically equivalent speech at the input. During evaluation, this \"missing\" speech is generated by TTS.\nThe model uses a Whisper speech encoder which is frozen, a speech adapter based on Q-Former and an MLP, and a strong multilingual LLM based MT system (GemmaX2-28-9B) which is LoRA adapted.\n\nThe approach uses a multistage curriculum pretraining with progressively complex objectives: ASR, S2TT and SMMT. The authors also propose a \"self-evolution algorithm\", i.e. iteratively complementing the training data with synthesized speech while making sure that the additional speech improves performance.\n\nThe model is compared with other multimodal MT system which use the image modality (authentic and synthetic images) on the Multi30k benchmark. It performs best for 5 out of 6 tasks. The model also outperforms SOTA on Flores200, but the model is only test on 28 languages. Also, the base MT system, i.e., to which speech input is added, already outperforms the other systems for 5 out of 6 languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It's interesting to see that adding synthetic speech to the input of a text translation system improves performances. This seems to be more effective than adding images to the input (cf. Table 3).\n\nThe incremental training strategy is interesting and I wonder whether similar techniques could be used in other LLM settings than MT."}, "weaknesses": {"value": "The experimental results show that the method works, but the authors fail to justify, or to try to explain, why it works! The only argument is that the prosody in speech helps translation. I could agree with this if human speech were used. However, this work addresses the issue how to improve text translation by providing in addition *synthetic speech*. I am not well aware of the current SOTA in TTS, but I would be surprised that the prosody is very rich.\n\nIt is not clear which data was used to train the speech and LLM adapter. The data mentioned in Table 9 gives no data sizes. I wonder whether the observed improvements are mainly the result of the additional training data. An important ablation would be to LoRa adapt the MT systems on the data, but without speech input.\n\nIn Table 4 and 12, good improvements are reported for several languages when translating from 4 languages into 27 foreign languages. Given that the algorithmic improvements are on the input only, I am puzzled how we can achieve a large variance on the improvements when translating from the same language into many different ones. The paper would benefit from a deeper analysis, instead of numerical metrics only.\n\nI would appreciate some examples, i.e., some sentences from Flores that are better translated with speech augmented input."}, "questions": {"value": "- it's unclear on what data the model was trained on. Table 9 mentions Fleurs train, Common Voice train and Multi30k, but no sizes (per language) are given.\n\n- why you support 28 languages only? Is this limited by the TTS system? What type of training data do you need to scale to more languages?\n\n- the gains form the baseline text-only MT system and the proposed SMMT-10B are much higher for FLICK and MSCOCO than Flores. How can you explain this? What are the characteristics of the sentences in FLICK/MSCOC, e.g. very short, caption like?\n\n- Table 9 caption: \"we removed overlapping portions from the FLORES devtest set\". You report results on Flores devtest (cf. Table 10). You should never modify a test set! This makes it impossible to compare results. Which version of Flores was used in Tab 4, your modified one? Did you recalculate the scores for the other systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M6P2aDwG2M", "forum": "HQMVRQUEaM", "replyto": "HQMVRQUEaM", "signatures": ["ICLR.cc/2026/Conference/Submission24834/Reviewer_s3NW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24834/Reviewer_s3NW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900744284, "cdate": 1761900744284, "tmdate": 1762943212994, "mdate": 1762943212994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}