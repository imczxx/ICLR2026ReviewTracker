{"id": "9zYab4QELy", "number": 3893, "cdate": 1757562489920, "mdate": 1759898064234, "content": {"title": "OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation", "abstract": "Emerging deep-learning-based lens library pre‑training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown Optical Degradations (ODs).\nThis work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing OD. To improve data scalability, we expand the design specifications to increase the OD diversity of the lens source, and we sample a more uniform OD distribution by quantifying the spatial-variation patterns and severity of OD. \nIn terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe OD, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of OD priors. \nExperiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state‑of‑the‑art performance in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large‑scale LensLib. The source code and datasets will be made publicly available.", "tldr": "", "keywords": ["Optical Aberration Correction; Universal Image Restoration; Vector Quantization; Degradation Priors Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b67630e8faceb2df95a37e3fbdf71cf23f4bf6c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an image restoration method for optical aberration removal that generalizes well across diverse optical lenses. By overfitting existing optical aberrations (PSFs) using a large-scale lens dataset, the network is exposed to a variety of aberrations, thereby improving generalization performance. The key improvement of this work compared to the prior arXiv paper (OmniLens) lies in its scalability. To achieve this, the paper expands the lens design specifications and introduces a latent representation of the PSF."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I agree that training an “all-in-one” network for various lens aberrations is an interesting academic exploration, but which requires a more comprehensive benchmark."}, "weaknesses": {"value": "1. Regarding the paper's writing, two points need improvement:\n  - The baseline for this paper is the arXiv paper “OmniLens: Towards Universal Lens Aberration Correction via LensLib-to-Specific Domain Adaptation.” However, the OmniLens framework is not clearly described in the main text. Readers may also be confused about whether there is an \"OmniLens+\" between OmniLens and OmniLens++.\n  - There are too many uncommon abbreviations, for example, OD for optical degradation, LPR for latent PSF representation, CAC for computational aberration correction, and OIQ for optical image quality. These abbreviations are usually unnecessary and significantly increase reading difficulty.\n\n2. Regarding the experiments:\n  - The baseline (OmniLens) remains on arXiv, and this paper is a clear follow-up to it. Benchmarking against an un-peer-reviewed work is risky and may be hard to convince readers. The evaluation cases and dataset are highly customized by the authors, whereas a more comprehensive, standardized benchmark should be proposed.\n  - Similar to Chen et al., this paper only considers plane object scenes. However, in real scenarios, different objects appear at different depths, introducing non-uniform defocus effects. The corresponding defocus datasets and simulation works should not be ignored. Examples include:\n    - “Defocus Deblurring Using Dual-Pixel Data”\n    - “Learning to Deblur Using Light Field Generated and Real Defocused Images”\n    - “Aberration-Aware Depth-from-Focus”\n    - “Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur”\n\n3. Debate on “all-in-one” image restoration:\n  - I agree that training an “all-in-one” network for various lens aberrations is an interesting academic exploration, and there are related works on this topic for image denoising, motion deblurring, and deraining. However, different optical lenses can exhibit very different aberrations. Training on such a large-scale lens dataset may improve network performance on general camera lenses, but the results on metalenses (Figure 3) are not promising enough. In short, it is challenging to achieve zero-shot generalization to customized optics if they are not included in the dataset—this seems like a clear failure case for the overall idea of this paper. Examples include:\n    - “Perspective-Aligned AT Mirror with Under-Display Camera”\n    - \"Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Networks\"\n  - In practice, different camera sensors have distinct noise profiles, which often require image restoration networks to be retrained or fine-tuned for different camera systems. As the dataset scale increases, overfitting all existing PSFs becomes significantly more difficult and demands larger network sizes and more complex architectures, posing challenges for deployment. In such cases, the practicality of an “all-in-one” image restoration network is marginal.\n\n4. Regarding novelty: \n  - The original idea of OmniLens is new, while scaling it up (e.g., by using aspherical surfaces, which is a standard approach) seems marginal."}, "questions": {"value": "Please check weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q6PdaI9oha", "forum": "9zYab4QELy", "replyto": "9zYab4QELy", "signatures": ["ICLR.cc/2026/Conference/Submission3893/Reviewer_6hkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3893/Reviewer_6hkZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654828684, "cdate": 1761654828684, "tmdate": 1762917086195, "mdate": 1762917086195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes OmniLens++, a framework for blind computational aberration correction build on Lens Library Pre-Training (lensLib-PT). \nThe authors claim two main contributions. First, they introduce a larger, better-balanced library (AODLibpro) that expands the design space with aspheric surfaces and image-plane perturbations to broaden degradation diversity. \nSecond, they propose a Latent PSF Representation (LPR) that injects PSF knowledge into a blind correction pipeline. \nUsing LPR as guidance, the authors train a foundational aberration-correction model (FoundCAC). \nAcross RealLens-Sim and real photographs, OmniLens++ delivers consistent improvements over prior LensLib-PT variants and strong deconvolution baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Combining a broadened, more uniformly sampled synsthetic lens library with a learned latent prior is technically sound and novel.\nThe experimental validation is extensive, spanning diverse simulated aberration settings, and includes ablation studies showing that AODLibpro improves over earlier AODLib-EAOD settings and LPR guidance helps as data scale increases."}, "weaknesses": {"value": "The presentation is needlessly hard to parse. The paper is overloaded with abbreviations (OD, CAC, ODN, OIQ, etc.), many of which describe concepts that could be expressed directly or formally. Optical degradation (OD) is essentially a forward operator in an inverse problem. Computational Aberration Correction could simply be described as image reconstruction and an Optical Degradation Network is, at its core, a neural network modeling the forward process. This naming density obscures rather than clarifies the contributions.\nThe method section is dense and very hard to follow. It introduces multiple interdependent modules (VQVAE, ODN, LPR, FoundCAC) in quick succession, often without intuitive explanation or guiding diagrams. Figures 1 and 2 are overly complex and fail to provide a clear overview of the system. They require significant prior knowledge of the text to decode. They are repeatedly cited as explanatory (\"as shown in Figure 1 (a)...\", \"as shown in Figure 2 (a)...\") when they in fact demand the textual explanation to be understood. Key design flows and data construction steps should be broken into simpler, sequential diagrams. The baseline OmniLens method is introduced too late in the text. Readers unfamiliar with the earlier work will struggle to contextualize what is actually new. The phrasing \"constructing the large LensLib AODLibpro\" reads as though the dataset already existed. Clarifying that AODLibpro is newly proposed and constructed in this work would strengthen the narrative. The writing, particularly in the methodology section, reads like a technical report rather than a scientific paper. Many sentences could be made clearer by explaining the motivation and intuition behind each modeling choice."}, "questions": {"value": "- The Optical Image Quality (OIQ) metric is central to AODLibpro and the evaluation, but its motivation and validation are unclear. Could the authors justify why OIQ is needed for assessing blind lens aberration correction and show that it correlates better with perceptual or optical quality than PSNR or SSIM?\n- The experiments seem simulation-heavy. Are there quantitative evaluations on real aberrated captures, not just qualitative examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4EwUL3YV9z", "forum": "9zYab4QELy", "replyto": "9zYab4QELy", "signatures": ["ICLR.cc/2026/Conference/Submission3893/Reviewer_Q7Qo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3893/Reviewer_Q7Qo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923947161, "cdate": 1761923947161, "tmdate": 1762917085760, "mdate": 1762917085760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniLens++, a new framework for blind lens aberration correction that leverages a large-scale LensLib pre-training pipeline and a Latent PSF Representation (LPR) module. This paper aims to overcome two limitations in existing approaches: insufficient scalability of optical degradation data and the lack of explicit degradation priors in blind correction models. To address these, the authors construct AODLibpro, a uniformly sampled lens library with enriched optical specifications, and design an LPR-guided CAC (Computational Aberration Correction) model using a VQVAE-based latent PSF codebook and an Optical Degradation Network for prior regularization. Extensive experiments on synthetic and real lenses demonstrate state-of-the-art performance and strong generalization to unseen aberrations. While the technical pipeline is well-executed and the experimental evaluation is compelling, the paper would benefit from a more \nthorough explanation of the motivation behind the latent PSF representation and the specific advantages of the VQVAEODN combination."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a technically solid and conceptually novel contribution by linking large-scale lens data construction with degradation-prior learning in a blind setting. The experimental evaluation is comprehensive and compelling, demonstrating state-of-the-art performance across both simulation and real-world benchmarks. The writing is clear and well-structured, making the technical contributions straightforward to follow."}, "weaknesses": {"value": "Although the framework is comprehensive and experimentally solid, I find the overall technical novelty is relatively limited. \nThe proposed VQVAE-based design represents a straightforward structural adaptation rather than a fundamentally new methodological contribution. While the integration of the Optical Degradation Network (ODN) to regularize the latent space is a thoughtful addition, it largely follows standard practice in generative modeling and degradation-aware representation learning. The author should provide more detailed discussion or ablation study."}, "questions": {"value": "1. Could the authors provide a more detailed ablation study on the hyperparameter settings? For example, on page \n17, the LPR module is described as using a VQVAE with a codebook size of K=1024 and a latent feature \ndimension of n_z=256. It would be helpful to understand how these choices affect model performance and whether \nthe results are sensitive to variations in these parameters.\n\n2. While the paper demonstrates strong generalization across various lens types, it remains unclear how OmniLens++ \nperforms under more extreme imaging conditions, such as low-light environments or high dynamic range scenes.\n\n3. In line 362, the authors mention the suppression of purple fringing caused by chromatic aberration. Could they \nprovide a more detailed explanation of how this is achieved? This would help reviewers and readers better \nunderstand the model’s capability in handling chromatic aberrations and its implications for real-world applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yNLf4aHuMZ", "forum": "9zYab4QELy", "replyto": "9zYab4QELy", "signatures": ["ICLR.cc/2026/Conference/Submission3893/Reviewer_QyYn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3893/Reviewer_QyYn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953124996, "cdate": 1761953124996, "tmdate": 1762917084975, "mdate": 1762917084975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}