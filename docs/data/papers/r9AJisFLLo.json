{"id": "r9AJisFLLo", "number": 9849, "cdate": 1758143807567, "mdate": 1759897691867, "content": {"title": "ShapeGen4D: Towards High Quality 4D Shape Generation from Videos", "abstract": "Video-conditioned 4D shape generation aims to recover time-varying 3D geometry and view-consistent appearance directly from an input video. \nIn this work, we introduce a native video-to-4D shape generation \nframework that synthesizes a single dynamic 3D representation end-to-end from the video.\nOur framework introduces three key components based on large-scale pre-trained 3D models:  (i) a temporal attention that conditions generation on all frames while producing a time-indexed dynamic representation; \n(ii) a time-aware point sampling and 4D latent anchoring that promote temporally consistent geometry and texture; \nand (iii) noise sharing across frames to enhance temporal stability. \nOur method accurately captures non-rigid motion, volume changes, and even topological transitions without per-frame optimization. Across diverse in-the-wild videos, our method improves robustness and perceptual fidelity and reduces failure modes compared with the baselines.", "tldr": "", "keywords": ["4D reconstruction", "generative model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ebd849bddc89634c0b8155ce237d9ac2a191391.pdf", "supplementary_material": "/attachment/cc9d20da35913db8b456e78e78505b161bb2fb1f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ShapeGen4D, a feedforward framework for high-quality 4D shape generation from monocular videos. The method builds upon a pre-trained 3D generative model (Step1X-3D) and adapts it to the dynamic setting through three key innovations:\n\n1. Spatiotemporal attention layers to capture cross-frame dependencies;\n2. Temporally-aligned latent encoding to reduce jitter and improve consistency;\n3. Shared noise across frames to enhance temporal stability.\n\nShapeGen4D directly outputs a sequence of 3D meshes, supporting non-rigid motion, volume changes, and topological transitions without per-frame optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. End-to-end 4d generation framework. The paper presents a direct, feedforward approach for generating 4D mesh sequences from a single video, which is a simplification over more complex optimization-based or multi-stage pipelines.\n2. Handling of complex dynamics. The framework demonstrates the capability to generate a range of dynamic phenomena, including non-rigid motion and topological changes, which are challenging for methods restricted to simpler deformations.\n3. Clear written. The paper is clearly written and provides a well-organized summary of related work, helping to situate the contributions within the field of 4D generation."}, "weaknesses": {"value": "1. Inherent Limitations in Temporal Geometry Consistency. While the proposed techniques of latent alignment and noise sharing effectively reduce temporal jitter, they do not establish an explicit, parametric model of motion (e.g., a deformation field). The framework still generates each frame's mesh independently from a sequence of latents. This inherently discrete representation may struggle to guarantee as-rigid-as-possible or physically plausible transitions over time, potentially leading to subtle topological inconsistencies or unnatural deformations that are not explicitly regularized. This limitation is indirectly acknowledged by the authors, who note that \"local temporal jitter remains visible in some results.\"\n2. Limited Scalability to Long Video Sequences. The model is designed to generate a fixed-length sequence (e.g., 16 frames) in a single forward pass, constrained by the memory and architecture of the underlying diffusion transformer. This fixed-horizon generation prevents the method from processing arbitrarily long videos, a common requirement in real-world applications. The paper does not explore mechanisms for temporal auto-regressive generation or sliding-window inference, which would be necessary to scale to longer durations, potentially at the cost of error accumulation across segments."}, "questions": {"value": "1. Since the meshes for consecutive timesteps are generated independently from discrete latents, the resulting 4D sequence lacks an explicit, continuous deformation field. This may lead to non-smooth interpolations and visually incoherent dynamics when rendered at frame rates higher than the generation rate. Do you have plans to incorporate a post-processing step or an intermediate representation (e.g., a neural deformation field) to enable truly continuous, smooth morphing between the generated key meshes? How might this be integrated into your current pipeline?\n2. The current framework generates a fixed number of frames (e.g., 16) in a single feedforward pass. What is the potential of extending ShapeGen4D to handle arbitrarily long videos? For instance, could an autoregressive approach be adopted, where the generation of a subsequent clip is conditioned on the final frames of the previous clip, similar to the strategy employed by L4GM? If so, what would be the main technical challenges, such as error accumulation or maintaining global consistency across segments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6XHBe6NvA", "forum": "r9AJisFLLo", "replyto": "r9AJisFLLo", "signatures": ["ICLR.cc/2026/Conference/Submission9849/Reviewer_6RuC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9849/Reviewer_6RuC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631034310, "cdate": 1761631034310, "tmdate": 1762921323989, "mdate": 1762921323989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ShapeGen4D, a direct, feedforward framework for producing 4D shape sequences from monocular video input. The method extends an existing 3D shape diffusion model into the temporal domain through several architectural modifications. Firstly, the authors obtain temporally aligned shape latents through a time-aware point sampling strategy. Then, the pretrained shape DiT is finetuned with incorporation of spatiotemporal attention. During training, all shape frames share same noise to enhance temporal stability. Extensive experiments on diverse benchmarks demonstrate the strong qualitative and quantitative results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Conceptual originality: the first to adapt pretrained 3D shape diffusion generator for 4D shape sequence generation.\n* End-to-end pipeline: the proposed method produces temporally coherent mesh sequences directly from video, avoiding costly optimization.\n* Simple but effective modification: the model builds on a well-known 3D backbone with several straightforward and effective architectural modifications, achieving quantitative and qualitative improvements compared to baseline methods."}, "weaknesses": {"value": "* Underlying gap with respect to pretrained prior: the queries of latents are sampled from non-watertight mesh, while the base 3D backbone is pretrained on watertight queries. Although the authors have explained the reason to do so (to avoid costly mesh registration), the potential performance drop still exists due to this gap.\n* No explicit motion modeling: the generated mesh under each frame is independent with each other. The lack of explicit motion constraint cannot guarantee physically plausible, leading to minor jitter and limiting the application to continuous motion interpolation.\n* Technically lack of novelty: the main components of this framework are mostly common practices in this field."}, "questions": {"value": "* Current design only handles short clips (e.g., 16 frames), have the authors considered scaling the model to longer videos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VpW3dY2GT4", "forum": "r9AJisFLLo", "replyto": "r9AJisFLLo", "signatures": ["ICLR.cc/2026/Conference/Submission9849/Reviewer_pzzF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9849/Reviewer_pzzF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645142261, "cdate": 1761645142261, "tmdate": 1762921323605, "mdate": 1762921323605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper works on 4d shape generation from monocular video. This is a new task and the authors aim to generate per-frame yet temporally coherence mesh instead of static mesh and deformation field to accommodates variantions in topology and relaxe constrains on the type of possible animations. Specifically, they construct the 4d shape generation model on a 3d shape generatoin work, and add temporal attention layers to ensure temporal consistency. Besides, they use global pose registration and global texturation as post-processing steps to better present the generated results and evaluation. Experiments on public and collected datasets validate the performance of the proposed model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) the generated shape seems to be of high-quality in terms of single-frame results, probably benifiting from high-quality pretrained 3D generation model weight\n\n2) the paper is well-written and present its contribution in a clear way\n\n3 )the authors work an new task, and propose a new pipeline of 4D shape generation"}, "weaknesses": {"value": "1) The generated results show noticeable flickering artifacts in both geometry and texture, where the texture flickering may stem from the instability in geometry.\nWPOwpo\n2)  While the authors claim that per-frame mesh generation is intended to capture variations in topology and enable a wider range of animations, the paper provides only a single example illustrating this capability (the BANG case in the supplementary material). The remaining examples appear to be rendered from skeleton-based animation models without topology changes and with a limited diversity of animation types. The authors are encouraged to present examples of topology-changing animations include: object shattering, characters growing extra limbs, soft-body fusion or splitting, cloth tearing, and morphing into an entirely different mesh structure.\n\n3) Lack of comparison with dynamic mesh generation method, for example DreamMesh4D and DriveAnyMesh, both are video-4d mesh generation method. Besides, the authors are encouraged to compare the generation result with general video-4d method beyond L4GM."}, "questions": {"value": "It is unclear why the proposed method does not incorporate data augmentation using geometric or spatial transformations during training, which might have alleviated or removed the requirement for global registration.\nIf my concerns are solved, I'll raise the recommendation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dY3dzoIOKB", "forum": "r9AJisFLLo", "replyto": "r9AJisFLLo", "signatures": ["ICLR.cc/2026/Conference/Submission9849/Reviewer_uhdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9849/Reviewer_uhdb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9849/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921242732, "cdate": 1761921242732, "tmdate": 1762921323283, "mdate": 1762921323283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}