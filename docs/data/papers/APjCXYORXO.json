{"id": "APjCXYORXO", "number": 19663, "cdate": 1758298075683, "mdate": 1763378499648, "content": {"title": "R&D-Agent: An LLM-Agent Framework Towards Autonomous Data Science", "abstract": "Recent advances in AI and ML have transformed data science, yet increasing complexity and expertise requirements continue to hinder progress. Although crowd-sourcing platforms alleviate some challenges, high-level machine learning engineering (MLE) tasks remain labor-intensive and iterative.\nWe introduce R\\&D-Agent, a comprehensive, decoupled, and extensible framework that formalizes the MLE process. R\\&D-Agent defines the MLE workflow into two phases and six components, turning agent design for MLE from ad-hoc craftsmanship into a principled, testable process. \nAlthough several existing agents report promising gains on their chosen components, they can mostly be summarized as a partial optimization from our framework's simple baseline.\nInspired by human experts, we designed efficient and effective agents within this framework that achieve state-of-the-art performance.\nEvaluated on MLE-Bench, the agent built on R\\&D-Agent\\ ranks as the top-performing machine learning engineering agent, achieving 35.1\\% any medal rate, demonstrating the ability of the framework to speed up innovation and improve accuracy across a wide range of data science applications.", "tldr": "We propose R&D-Agent, a flexible framework for exploring LLM-based agent designs, through which we discover an agent system that achieves state-of-the-art performance on MLE-Bench.", "keywords": ["Data-Driven AI", "Multi-Agent System", "Large Language Models", "Auto Research", "LLM", "LLM-Agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5f427d85941916170eaa47baa26079b455571eb9.pdf", "supplementary_material": "/attachment/3d5eb5623c8eba018548e3bc36376e542a292ded.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to improve the capabilities of autonomous data science agents. This paper proposes R&D Agent, which consists of six main steps: (1) Planning (tricks on time savings); (2) Exploration path structuring (similar to AIDE); (3) Scientific reasoning pipeline (enhanced planning); (4) Memory context (share knowledge across branches); (5) Coding workflow (debugging based on a small subset and then coding, another trick time savings); (6) Evaluation (consistent data splits for consistent performance comparison). Then, this paper conducts experiments on MLE-Bench to demonstrate the effectiveness of the proposed framework."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "- A large number of experiments on the whole set of MLE-Bench are conducted.\n- The writing is of good quality.\n- Ablation studies on a subset of MLE-Bench are conducted to provide the effectiveness of the proposed framework."}, "weaknesses": {"value": "- The novelty of this paper is limited. All the contributions are more like engineering efforts and small tricks. There is no fundamental technical contribution in this paper. Also, all the techniques are already investigated by the community. I believe this paper would not bring novel technical insights for the community.\n\n- From the perspective of empirical findings, although this paper performs large-scale experiments on MLE-Bench, the experiment design is not convincing enough for ensuring fair comparison. Here are detailed comments:\n\n1. First of all, the comparison in Table 1 **must** build on top of the same settings and the same foundation models; otherwise, it is not correct to claim SOTA. For example, in Table 1, GLAB, OpenHands and AIDE are developed based on GPT-4o/o1-preview, while R&D-Agent is based on more advanced LLM models GPT-5. This is an unfair comparison. We cannot figure out whether the performance improvement comes from the foundation model or from the agent framework. **Furthermore**, the comparison between MLE-Master and R&D-Agent is also unfair. The prompt of ML-Master is optimized based on DeepSeek-R1; thus, we can find that ML-Master w/ DeepSeek R1 consistently outperforms other foundation models, even more advanced GPT-5. Therefore, this paper should provide empirical results of R&D-Agent with DeepSeek-R1, a consistent setting with ML-Master, to ensure fair comparison. **Last but not least**, I believe the current SOTA data science agent is MLE-STAR. However, this paper only provides comparison with MLE-STAR in the appendix and with unfair setups once again. As shown in Figure 5, R&D-Agent is built on top of GPT-5, while MLE-STAR is built on top of Gemini-2.5-pro. This is a clearly unfair comparison. Beyond this, please make sure MLE-STAR is also evaluated on the whole set of MLE-Bench, if the authors what to claim SOTA.\n\n2. The ablation variants in Table 3 are only run once to report the results. This is clearly unacceptable, especially considering the complicated and stochastic automated data science setting. If the computational resources are limited, the authors can only conduct the experiments on MLE-Bench-Lite. But reporting results across multiple runs is a basic requirement.\n\n- Although this paper briefly discusses related works in Table 1 and Appendix A, I think it is still necessary to comprehensively discuss related works and fundamental differences from them in a more detailed manor.\n\n- MLE-Bench is an offline benchmark, which still suffers from the risks of data leakage. It would be better to run R&D-Agent in an online Kaggle competition to see whether the agent can really win a medal. This would be a more interesting experimental setting, compared with overly reliance on MLE-Bench."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pJNrdf0uqx", "forum": "APjCXYORXO", "replyto": "APjCXYORXO", "signatures": ["ICLR.cc/2026/Conference/Submission19663/Reviewer_kPLw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19663/Reviewer_kPLw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760872294346, "cdate": 1760872294346, "tmdate": 1762931512842, "mdate": 1762931512842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for the valuable time and effort the reviewers put into reviewing this paper. Your suggestions are very helpful in improving our work for the new version.\n\nAfter meticulously reviewing the feedback and engaging in discussions with other authors, we realized that we need to significantly revise the paper's narrative.Our work focuses on building a framework for MLE-Agents, and we are exploring innovative solutions.\nBut the topic is quite hot, and a framework-level narrative would overlap with other works and obscure the contributions of innovative solutions.\n\nWe agree with the reviewer's suggestion that the paper's framing could be improved to better align with its actual contributions. However, rectifying the flaws at the framework level would require a significant reorganization of the paper's content, which may not be suitable for the rebuttal period.\nWe have made the difficult decision to withdraw our paper in order to restructure its content. Our plan is to resubmit it to a future conference."}}, "id": "GXdizI4wJK", "forum": "APjCXYORXO", "replyto": "APjCXYORXO", "signatures": ["ICLR.cc/2026/Conference/Submission19663/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19663/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763378498722, "cdate": 1763378498722, "tmdate": 1763378498722, "mdate": 1763378498722, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces R&D Agent, an autonomous agent framework that formalizes the machine learning engineering process by explicitly separating a research phase (planning, exploration, path structuring, memory context management, and reasoning pipelines) from a development phase (coding, workflow construction, and evaluation strategy). The framework demonstrates promising performance on the MLE-bench benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of separating the research and development phases is interesting and conceptually aligns with how human ML practitioners operate.\n2. The experimental results on MLE-bench are promising and suggest potential for structured LLM-driven engineering pipelines."}, "weaknesses": {"value": "Deeper analysis is needed:\n1. While the paper includes baseline comparisons and ablation studies, it is unclear what limits the overall system performance. Is the bottleneck primarily in the research phase (e.g., limited exploration despite the proposed reasoning techniques), or in the development phase (e.g., LLMs struggling to reliably use external libraries or tools)? A more detailed investigation could clarify this.\n2. The benchmark used in the paper, although containing 40 competitions, appears to consist of only a few broad categories (e.g., classification). It would be helpful to understand whether the agent’s strategies generalize across task types. For example, what is the medal rate when grouped by task category, and are the generated methods qualitatively similar across those categories?\n\nDiscussion and Suggestions:\n1. Currently, many papers propose new frameworks or agent architectures to tackle ML or reasoning tasks, often relying on similar principles such as task decomposition or multi-phase design. In this crowded space, the key contribution should not only be introducing yet another framework but also providing insightful analysis into why and how certain design choices lead to better performance.\n2. In particular, deeper interpretability and diagnostic analysis would strengthen the paper. While ablation studies help isolate the effectiveness of specific modules, they often do not explain what actually changes in the agent’s behavior or generated algorithms after ablation. For example, in this paper, it remains unclear how the removal of a certain research-phase component affects the agent’s exploration strategy or code-generation quality. Providing qualitative examples or process-level comparisons would make the paper more informative and impactful."}, "questions": {"value": "Check Weaknesses Section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HvzoenXhqX", "forum": "APjCXYORXO", "replyto": "APjCXYORXO", "signatures": ["ICLR.cc/2026/Conference/Submission19663/Reviewer_R5xk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19663/Reviewer_R5xk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433096664, "cdate": 1761433096664, "tmdate": 1762931512380, "mdate": 1762931512380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "R&D-Agent is an agent framework for autonomous ML engineering. The main contribution is the elaborate multi-agent architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n- Interesting 6 component multi-agent system. Mainly an integration of existing approaches rather than new approaches\n\nQuality\n- very comprehensive and rigorous evaluations conducted\n\nClarity\n- Well-written paper: well exposed, well documented in the appendix and clearly positioned \n\nSignificance\n- Shows it outperforms other methods on ML research tasks"}, "weaknesses": {"value": "- Limited novelty: Individual components use established techniques (MCTS, tree search, memory systems, iterative debugging). This paper is also not the first to integrate many of these into agents for ML engineering and data science. e.g. the MCTS figures even look similar to Climb-DC and and the path structure mechanism like DataInterpreter\n\n- Many missing baselines: DataInterpreter, Climb-DC, MLCopilot, LAMBDA, AutoML-Agent --- please can you outline the differences to these works\n\n- Only closed models used (e.g. GPT-5): Use open models to also understand what’s the source of gain the foundation model or architecture.\n\n- No failure analysis: where and why does the framework work well and when does it fail?\n\n- Gain ablations: given the many components, more ablations should have been done to see which components contribute the most"}, "questions": {"value": "1. Can you try other models/LLMs? One needs to decouple the framework from the capabilities bestowed by the LLM. All competitors should use the same LLM to make it a like-for-like comparison of frameworks.\n\n2. Please can you compare to the other related work (see above)\n\n3. Does it work beyond MLE-bench type tasks? Like other DS tasks to see where it’s effeccitve and fails. Is there a specific task structure this architecture works for or is it generalisable"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "72BjoCSNko", "forum": "APjCXYORXO", "replyto": "APjCXYORXO", "signatures": ["ICLR.cc/2026/Conference/Submission19663/Reviewer_4ouA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19663/Reviewer_4ouA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863195804, "cdate": 1761863195804, "tmdate": 1762931511823, "mdate": 1762931511823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims to implement an agent system that reaches state-of-the-art performance on MLE-Bench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper shows leading “Any Medal” performance on MLE-Bench when using the most recent GPT-5 model."}, "weaknesses": {"value": "1. Regarding “Gold” performance, it still does not perform as well as ML-Master that uses Deepseek-R1 released approximately half year earlier than GPT-5. It is recommended to test the performance of the proposed agent system using Deepseek-R1 for fair comparison.\n\n2. The paper claims “All the existing methods can be summarized as a partial optimization from our framework’s simple baseline”. It is obviously an over-claim since many existing methods were published/released earlier than this work, and those existing methods should not be considered an adaptation or optimization from this work. Instead, polite attributions to the previous existing methods should be included in this paper.\n\n3. Continuation to its above over-claim, the paper shows limited novelty in building its agent system. Instead, a more accurate statement seems to be: Most of this paper’s key components were invented or proposed by previous methods cited or not cited in this paper. If this statement is inaccurate, please provide more details to justify.\n\n4. The paper focuses on the optimization on MLE-Bench. However, it may be more convincing to use multiple benchmarks for evaluating the performance of the agent system. Different data science benchmarks like Economically important tasks, DA-Code and DSBench are used by different research papers and reports including OpenAI agent technical report (https://openai.com/index/introducing-chatgpt-agent/)."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AdULiTNk8X", "forum": "APjCXYORXO", "replyto": "APjCXYORXO", "signatures": ["ICLR.cc/2026/Conference/Submission19663/Reviewer_sckK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19663/Reviewer_sckK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989887203, "cdate": 1761989887203, "tmdate": 1762931511321, "mdate": 1762931511321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}