{"id": "GLELajHnCo", "number": 4705, "cdate": 1757750431507, "mdate": 1763382638434, "content": {"title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings", "abstract": "Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5\\% of dense models in one-shot pruning at 50\\% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51\\% improvement on FinMTEB and +1.73\\% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development.", "tldr": "GAPrune prunes embedding models using domain-general gradient alignment, achieving 50% sparsity while enhancing domain performance.", "keywords": ["Embedding Model; Domain Adaptation; Domain Pruning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1877125a6ffe629b2f8d014127fa7b68a0ed4c99.pdf", "supplementary_material": "/attachment/0e6556a905c253ab07248050d72ee5e74929b6fd.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a method for conducting domain-aware pruning for embedding models.The method comes in three different stages: (1) gathering representative data points from general domain dataset and domain-specific dataset using k-means sampling ($k = 5000$); (2) calculating parameter importance as measured by gradient over InfoNCE loss on general domain samples and domain-specific samples, as well as gradient alignment between the general and specific domain; (3) computing a domain-aware importance (DAI) score using the importance and alignment measures above, then prune parameters based on the DAI score. Experimental results show better performance compared to baselines such as vanilla dense embedding and magnitude pruning, and better compatibility with domain-specific re-training after pruning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well-written. The approach has many components, but all the steps are very well explained and mostly well-motivated.\n2. The method achieved significantly better performance over strong baselines under the re-training setup."}, "weaknesses": {"value": "1. Under the setup without re-training, the performance improvement over more simplistic methods like magnitude pruning is minimal, so I'm not entirely convinced if the proposal is really worth it when no re-training is performed.\n2. The necessity of each steps would be better justified if more rigorous ablation is performed. For example, while I understand the motivation of the gradient alignment term, I'm not sure if pruning \"domain specific\" parameters serves the stated goal of domain-aware embedding pruning. In defense of this proposal, this pruning choice might carry some benefit when re-training is performed (because re-training can recover domain-specific knowledge), but this needs to be validated by empirical experiments."}, "questions": {"value": "Can the authors further comment on the motivation behind the $(1 + \\alpha s^j_g)$ term? I might have missed or misunderstood some details.\n\nMinor nit comment: L629 in Algorithm 1 seems like a self-assignment? I think you can either get rid of this line, or assign the magnitude to a variable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y0A0v81srX", "forum": "GLELajHnCo", "replyto": "GLELajHnCo", "signatures": ["ICLR.cc/2026/Conference/Submission4705/Reviewer_qnve"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4705/Reviewer_qnve"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963500872, "cdate": 1761963500872, "tmdate": 1762917524894, "mdate": 1762917524894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission proposes a pruning method for domain-specific embedding models that scores parameters with a Domain-Alignment Importance (DAI) metric combining (i) domain Fisher importance and (ii) gradient alignment between general and domain objectives. On FinMTEB and ChemTEB, GAPrune outperforms magnitude/Fisher baselines at 30–50% sparsity and, after 100 retraining steps, exceeds dense performance in several settings."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This submission Clear formulation of DAI with intuitive justification.\n\nStrong results showing their work outperforms some existing methods.\n\nHelpful auxiliary analyses (layer-wise correlations/performance; embedding geometry) in the Appendix."}, "weaknesses": {"value": "Assumption support is limited. The central claim that parameters exhibit domain-dependent behavior is plausible but not directly validated; Either citing this conclusion from existing literature or providing theoretical or empirical analyses to support this assumption would be more convincing. A targeted test (e.g., per-parameter behavior shifts across domains) would help.\n\nLack of interpretability. While the multi-component DAI framework appears well-motivated, it lacks formal empirical or theoretical analysis on each component. Consequently, the work appears to be reliant on intuition rather than on rigorous validation.\n\nHyperparameter sensitivity concerns. The designed approach introduces three new hyperparameters: $\\alpha$, $\\beta$, and $\\gamma$. The submission does not have a discussion about the sensitivity to these hyperparameters. An ablation study or sensitivity analysis is crucial to understand this designed approach.\n\nReproducibility. Datasets/models/sparsity and “100 steps” are stated, but key training details (optimizers, LR, batch sizes, seeds) and code availability are missing.\n\nMissing KD comparison. Knowledge Distillation is a prevalent deployment strategy for embedders and isn’t discussed or compared.\n\nRelated work concerns. Motivation cites several non-peer-reviewed sources to show “The demand for domain-specific embedding models has grown significantly ”, which is not convincing. Adding stronger peer-reviewed evidence would strengthen the case for domain-specific embedders’ importance."}, "questions": {"value": "Empirically validate domain-dependent parameters (e.g., measure per-parameter gradient/activation shifts across domains and tie to pruning outcomes).\n\nAdd sensitivity/ablation for $\\alpha$, $\\beta$, and $\\gamma$ and sampling choices (k-means subset size).\n\nDiscuss/benchmark against KD baselines for efficiency.\n\nTighten related-work positioning with more peer-reviewed citations backing domain-specific embedders.\n\nProvide fuller training details and release code."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FvxDbQBwVg", "forum": "GLELajHnCo", "replyto": "GLELajHnCo", "signatures": ["ICLR.cc/2026/Conference/Submission4705/Reviewer_8Ku6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4705/Reviewer_8Ku6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762686652713, "cdate": 1762686652713, "tmdate": 1762917524593, "mdate": 1762917524593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes GAPrune, a pruning method for embedding models. GAPrune has two main components: they use Fisher information to measure the importance of parameters. In addition, they use gradient alignment to identify general and domain-specific parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors show consistent improvements over random pruning.\n2. The authors study pruning for embedding models, which can be very useful."}, "weaknesses": {"value": "1. This paper has no ablation study. The authors combines Fisher information with their DAI scores. However, it's unclear how much improvement each contribute individually.\n\n2. The authors unnecessarily restrict their scope to embedding models, but their approach is not specific to embedding models to my understanding. It would be ideal if the authors can test their approach in general LLMs, and compare their method with SOTA pruning methods.\n\n3. The results are generally close to magnitude-based pruning. Looking at the Table 1, the performance is very close to a simple magnitude-based pruning."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LvM5UZeuQc", "forum": "GLELajHnCo", "replyto": "GLELajHnCo", "signatures": ["ICLR.cc/2026/Conference/Submission4705/Reviewer_nY2B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4705/Reviewer_nY2B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762746013700, "cdate": 1762746013700, "tmdate": 1762917524255, "mdate": 1762917524255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revisions"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback. We have made substantial revisions to address all concerns raised and added seven appendices with new experiments:\n\n1. **Component-wise Ablation of DAI Score (Appendix F)** - *Addresses all Reviewers' concerns on component analysis* \n    - We isolate each component's contribution to demonstrate its necessity.\n\n2. **Hyperparameter Sensitivity Analysis (Appendix G)** - *Addresses Reviewer 8Ku6 on hyperparameter sensitivity* \n    - Performance varies within only 1.6% across all hyperparameter values, demonstrating robustness without extensive tuning.\n\n3. **Ablation on Sample Data Size (Appendix H)** - *Addresses Reviewer 8Ku6 on sampling sensitivity* \n    - Performance varies within 1.4% across sample sizes from 25% to 100% of baseline, demonstrating robustness to calibration data size.\n\n4. **One-shot Pruning on Higher Sparsity (Appendix I)** - *Addresses Reviewer nY2B and Reviewer qnve on performance gap with baselines* \n    - At 65% sparsity, GAPrune maintains 92% performance while magnitude pruning collapses to 45%, demonstrating substantial advantages over simple baselines.\n\n5. **Prune-and-Retrain with Knowledge Distillation (Appendix J)** - *Addresses Reviewer 8Ku6 on knowledge distillation comparison* \n    - Domain-specific retraining exceeds dense baseline by 4.5%, outperforming knowledge distillation by 6.3%, validating our pruning strategy.\n\n6. **Domain-Dependent Parameter Analysis (Appendix K)** - *Addresses Reviewer 8Ku6 on core assumption validation* \n    - 34% of parameters show significant rank shifts across domains with Spearman correlations of 0.62-0.67, empirically validating domain-dependent parameter behavior.\n\n7. **Retraining Hyperparameters (Appendix L)** - *Addresses Reviewer 8Ku6 on reproducibility*\n    - Complete documentation of optimizer, learning rate, batch size, and hardware configuration for full reproducibility.\n\n8. **Revised Related Work** - *Addresses Reviewer 8Ku6 on citation quality* \n    - Enhanced with peer-reviewed citations on domain-specific embedding models and benchmarks, removing non-peer-reviewed sources.\n\n9. **Code Release** - *Addresses Reviewer 8Ku6 on reproducibility* \n    - Complete implementation, pruning scripts, and preprocessed datasets included in supplementary material.\n\n---\n\nWe hope these revisions address the concerns raised by the reviewers and look forward to further feedback."}}, "id": "4uovyRM7Xv", "forum": "GLELajHnCo", "replyto": "GLELajHnCo", "signatures": ["ICLR.cc/2026/Conference/Submission4705/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4705/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission4705/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763383068477, "cdate": 1763383068477, "tmdate": 1763383068477, "mdate": 1763383068477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}