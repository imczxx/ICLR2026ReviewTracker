{"id": "ql30VWGyda", "number": 5282, "cdate": 1757890664148, "mdate": 1759897983402, "content": {"title": "TINY BUT MIGHTY: A SOFTWARE-HARDWARE CO- DESIGN APPROACH FOR EFFICIENT MULTIMODAL IN- FERENCE ON BATTERY-POWERED SMALL DEVICES", "abstract": "Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements (MLC, 2023a; Gerganov, 2023) have facilitated the local deployment of LLMs. However, local deployment also presents challenges, particularly in balancing quality (generative performance), latency, and throughput within the hardware constraints of mobile devices. In this paper, we introduce our lightweight, all-in-one automated benchmarking framework that allows users to evaluate LLMs on mobile devices. We provide a comprehensive benchmark of various popular LLMs with different quantization configurations (both weights and activations) across multiple mobile platforms with varying hardware capabilities. Unlike traditional benchmarks that assess full-scale models on high-end GPU clusters, we focus on evaluating resource efficiency (memory and power consumption) and harmful output for compressed models on mobile devices. Our key observations include: i) differences in energy efficiency and throughput across mobile platforms; ii) the impact of quantization on memory usage, GPU execution time, and power consumption; and iii) accuracy and performance degradation\nof quantized models compared to their non-quantized counterparts; and iv) the frequency of hallucinations and toxic content generated by compressed LLMs on\nmobile devices.", "tldr": "The smallest battery-powered device that can run VLMs in the world", "keywords": ["On-device VLM", "Efficient Inference", "Software-Hardware Co-Design", "Quantization", "NPU", "GPU"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/252ec8eb406aa8949d6595151b27d6a47bd356f6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents NANOMIND, a hardware-software co-design framework for efficient on-device LMM inference. The key idea is decomposing LMMs into modular components and dynamically scheduling each to the most suitable accelerator (NPU/GPU/CPU) on unified-memory SoCs. The authors build a custom battery-powered device with RK3566 SoC, achieving 42.3% energy reduction and enabling 20.8 hours of operation. The system features Token-Aware Buffer Manager for zero-copy transfer, battery-aware execution modes, and custom low-bit quantization kernels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Holistic system design**: The paper presents a rare end-to-end co-design spanning algorithm (model decomposition, quantization), system (scheduling, memory management), and hardware (custom PCB with PMU, parallel memory). This comprehensive approach addresses multiple bottlenecks simultaneously rather than optimizing in isolation.\n\n2. **Practical hardware validation**: Unlike many systems papers that only simulate or use off-the-shelf platforms, the authors designed and fabricated custom hardware, providing concrete evidence of feasibility and real power measurements through integrated PMU.\n\n3. **Novel heterogeneous scheduling**: The insight of mapping modular LMM components to different accelerators based on their computational characteristics (NPU for low-bit vision encoding, GPU for FP16 LLM decoding) is well-motivated and demonstrates clear benefits in the unified memory architecture context.\n\n4. **Strong empirical results**: The battery life improvements (20.8 hours for voice interaction) and energy efficiency gains (42.3% reduction) are substantial and practically meaningful for edge deployment scenarios."}, "weaknesses": {"value": "1. **Venue mismatch concern**: This work is fundamentally a hardware systems paper with custom PCB design, power management circuitry, and hardware-specific optimizations. While it has ML applications, the core contributions (hardware architecture, cross-accelerator scheduling, unified memory optimization) align more naturally with hardware/systems venues like HPCA, ISCA, or MICRO rather than ICLR, which focuses on machine learning methods and representations. The ML community may lack the expertise to properly evaluate the hardware contributions, and the authors would likely receive more targeted feedback from hardware systems reviewers.\n\n2. **Limited generalizability**: The framework is tightly coupled to the RK3566 SoC and Rockchip's RKNN ecosystem. Key components (NPU offloading, RKNN model conversion, specific driver optimizations) may not transfer to other mobile SoCs (Qualcomm Snapdragon, MediaTek Dimensity, Apple Silicon). The paper lacks discussion of how the design principles would adapt to different hardware platforms or what abstractions could enable portability.\n\n3. **Insufficient ablation studies**: While the paper shows end-to-end improvements, it doesn't systematically isolate individual contributions. What is the specific gain from: (a) parallel LPDDR4x vs. standard configuration? (b) zero-copy TABM vs. traditional buffer management? (c) NPU vs. GPU for vision encoding? (d) custom GEMM kernels vs. existing implementations? This makes it difficult to understand which design decisions are most impactful.\n\n4. **Weak baseline comparisons**: The comparisons are primarily against llama.cpp on various platforms, but the paper doesn't compare against other recent edge inference frameworks (PowerInfer-2, llm.npu) on the same hardware. The NanoVLM comparison is limited to Jetson platforms. Additionally, the claim that llama.cpp is inefficient on unified memory architectures needs more rigorous support—is the inefficiency inherent to the framework or the specific platform/configuration?\n\n5. **Missing accuracy-efficiency tradeoffs**: Figure 7 shows accuracy across quantization strategies but doesn't correlate these with latency, throughput, or power consumption. What accuracy degradation is acceptable for different battery levels? How do users navigate the performance-accuracy-power tradeoff space?"}, "questions": {"value": "1. **Portability strategy**: How would NANOMIND adapt to SoCs without dedicated NPUs (e.g., Mali-only systems) or with different NPU architectures (e.g., Qualcomm HTP)? What abstraction layer could make the framework hardware-agnostic?\n\n2. **Static shape limitation**: You mention NPUs require static input shapes, which you address by fixing image resolution. How does this impact accuracy on datasets with varying native resolutions? Did you experiment with multiple fixed resolutions or dynamic resolution selection?\n\n3. **Real-world deployment**: Your experiments use controlled benchmarks (MMBench, InfoVQA, etc.). How does the system perform in real-world usage with variable user interaction patterns, thermal throttling over extended use, and battery degradation over time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o2xKAOYBLQ", "forum": "ql30VWGyda", "replyto": "ql30VWGyda", "signatures": ["ICLR.cc/2026/Conference/Submission5282/Reviewer_PgXN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5282/Reviewer_PgXN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760684550748, "cdate": 1760684550748, "tmdate": 1762917987830, "mdate": 1762917987830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NANOMIND, a hardware–software co-design framework targeting efficient on-device inference of Large Multimodal Models (LMMs) on battery-powered systems. The authors propose decomposing multimodal models into modular components (e.g., vision encoders, LLM decoders) and dynamically mapping each module to the optimal heterogeneous accelerator (GPU, NPU, CPU) under a unified memory architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- On-device LMM/VLM execution is increasingly important for privacy, latency, and offline use. The focus on battery-powered compact devices differentiates this work from existing edge-accelerator papers.\n\n- The modular execution model, accelerator-aware scheduling, and token-aware buffer manager offer practical and technical novelty, especially under UMA constraints.\n\n- The combination of FP16 encoders with W4A16 LLMs, fused dequant-GEMM OpenCL kernels, and linear attention demonstrates solid engineering toward performance and power efficiency."}, "weaknesses": {"value": "- Baseline gaps: lacks rigorous, same-hardware comparisons against state-of-the-art mobile stacks such as MLC-LLM, llm.npu, and PowerInfer-2, weakening external validity.\n\n- Incomplete utility evidence: limited end-task accuracy and qualitative results for multimodal workloads leave the user-perceived quality underexplored.\n\n- Scalability uncertainties: generalization to larger models and to NPUs with static-shape constraints is not demonstrated, and ablations isolating hardware choices are minimal."}, "questions": {"value": "- How does the scheduling handle rapid mode switching (camera and audio streams) under burst workloads without degrading responsiveness?\n\n- Can the system support multi-image or temporal encoder models given the static-shape NPU limitations?\n\n- What is the quantitative energy-latency trade-off curve of three power modes over long-running workflows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "51U4gh4qtN", "forum": "ql30VWGyda", "replyto": "ql30VWGyda", "signatures": ["ICLR.cc/2026/Conference/Submission5282/Reviewer_VTbL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5282/Reviewer_VTbL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545817149, "cdate": 1761545817149, "tmdate": 1762917987599, "mdate": 1762917987599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NANOMIND, a novel software–hardware co-design framework for enabling efficient on-device inference of large multimodal models (LMMs) on resource-constrained, battery-powered devices. It leverages the modularity of LMMs by decomposing them into “bricks” and dynamically offloading modules to optimal compute units (NPU/GPU/CPU). The paper introduces a custom inference pipeline, quantization strategies, token-aware buffer management, and custom hardware design, achieving notable energy and memory efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The software–hardware co-design for efficient on-device inference of LMMs is very interesting and impactful.\n\n- Clear research motivation: tackling latency and energy inefficiency of LMMs on edge devices.\n\n- Practical systems-level contribution, combining model decomposition, dynamic workload scheduling, and embedded hardware design."}, "weaknesses": {"value": "- While the proposed deployment strategy is evaluated on the authors’ custom SoC, the paper does not provide validation on other commercial or widely available SoCs.\n\n- In Figure 6, Throughput (tokens/s) and Latency appear to vary according to the device’s power state, which is controlled by the proposed Power-Efficiency Strategy. If I understand correctly, the battery level determines whether the system operates in a high-performance parallel mode or in the low-power “On-Demand Cascade Inference” mode. However, the paper does not clearly explain how throughput and latency are measured or computed under different power levels.\n\n- While Figure 7 compares different quantization strategies, the paper does not include an accuracy comparison between the proposed system and existing implementations. Such a comparison is necessary to clearly demonstrate the performance advantage of the proposed design."}, "questions": {"value": "- A key architectural component is the use of a token-aware ring buffer to facilitate zero-copy data flow between heterogeneous compute units (e.g., NPU and GPU). While this design significantly optimizes memory bandwidth and latency, how does it manage the Key–Value (KV) caches?\n\n**Minor Comments**:\n\n- The references in the current version of the paper are incomplete in formatting and lack hyperlinking.\n\n- The information presented in the paragraph starting at line 354 overlaps considerably with the earlier section “Token-Aware Buffer Management” (beginning at line 327). The two sections convey similar ideas and could be merged into a single, more concise paragraph to improve the paper’s flow and avoid redundancy.\n\nI am not deeply familiar with prior work on on-device inference frameworks, so I am unsure whether other closely related studies exist. I would be happy to discuss this further during the rebuttal and consider increasing the score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d0ZaWxCyCy", "forum": "ql30VWGyda", "replyto": "ql30VWGyda", "signatures": ["ICLR.cc/2026/Conference/Submission5282/Reviewer_NZpb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5282/Reviewer_NZpb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950721501, "cdate": 1761950721501, "tmdate": 1762917987319, "mdate": 1762917987319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NanoMind, a hardware-software co-design framework for efficient on-device inference of LMMs. The key idea is to decompose LMMs into modular components and dynamically offloads them to the most suitable heterogeneous accelerators including NPU, GPU, and CPUs. This paper proposes a token-aware buffer manager for zero-copy data transfer and a dynamic power management strategy. The framework is implemented on a custom-designed device based on the RK3566 SoC. Evaluations demonstrate reduced memory usage, lower latency, and improved power efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is well-motivated, addressing the critical challenge of efficient LMM deployment on resource-constrained edge devices.\n\n- The paper conducts practical system implementation and evaluation on real hardware."}, "weaknesses": {"value": "- The paper claims to be a system-algorithm co-design method; however, the algorithm-level innovations appear to be minor, primarily leveraging existing quantization and model decomposition ideas rather than introducing novel algorithmic contributions.\n\n- The framework's adaptability to different devices with varying computational resource budgets is not explored. The experiments are conducted only on RK3566 SoC, limiting the generalizability of the proposed framework.\n\n- Lack of strong quantitative metrics demonstrating the method effectiveness. As seen in Figures 5, 6, and 7, the improvements in memory usage, latency, and accuracy over baselines appear marginal, lacking of quantitative evidence for a substantial performance improvement."}, "questions": {"value": "- How can the proposed framework be adapted to other edge SoCs with different accelerator configurations (different NPU/GPU capabilities)? What modifications or adjustments would be required?\n\n- Can you provide more compelling quantitative evidence or statistical analysis to demonstrate that NanoMind offers a significant improvement in key metrics like latency reduction, memory efficiency, or accuracy preservation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2p11q4DQ7r", "forum": "ql30VWGyda", "replyto": "ql30VWGyda", "signatures": ["ICLR.cc/2026/Conference/Submission5282/Reviewer_7bMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5282/Reviewer_7bMY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981894691, "cdate": 1761981894691, "tmdate": 1762917987080, "mdate": 1762917987080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}