{"id": "fHr1uqkdsb", "number": 9074, "cdate": 1758109550866, "mdate": 1759897745040, "content": {"title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading", "abstract": "When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you wonder ``This sounds like science fiction. Does it actually work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded solely from eye movements in reading. To address this question, we introduce goal decoding tasks and evaluation frameworks using large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal text and eye movements LLMs for these tasks. Our experiments show considerable success on the task of selecting the correct goal among several options, and even progress towards free-form textual reconstruction of the precise goal formulation. These results open the door for further scientific investigation of goal driven reading, as well as the development of educational and assistive technologies that will rely on real-time decoding of reader goals from their eye movements.", "tldr": "This paper introduces the task of decoding open-ended reading goals from eye movements in reading, and develops several models for this task.", "keywords": ["Eye Movements in Reading", "Multimodal Large Language Models", "Information Seeking", "Cognitive State Decoding"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30c97563d12685329540a7f659500490a8bd67f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the task of decoding open-ended reading goals from eye movements. Using the OneStop dataset, the authors propose two tasks-target selection and target reconstruction—and design multimodal models combining gaze and text features. Results show that gaze–text integration slightly improves performance, indicating that eye movements encode semantic goal information. This paper provides a new research paradigm for the combination of eye movement and language models, but further improvement is needed in terms of interpretability, cross-text generalization, and comparison with more powerful baselines. Overall, the paper is interesting, but its current form cannot totally support all authors' claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors claim that they are the first study to decode open-ended, text-specific reading goals from eye movements, framed as dual tasks of selection and reconstruction.\n- Integrating text and gaze features markedly improves target selection; RoBERTEye-Fixations achieves 49.3% accuracy vs. 33% baseline.\n- DalEye-Llama attains 76.3% QA accuracy on unseen participants (vs. 68.1% for human distractors), validating the gaze–goal correspondence."}, "weaknesses": {"value": "1. Lacks cognitive interpretation of gaze behavior and its link to goal decoding.\n2. Sharp performance drop on unseen texts (Kappa 0.478 → 0.069) unexplained.\n3. No comparison with fine-tuned multimodal LLMs (e.g., GPT-4o, LLaVA-1.5)."}, "questions": {"value": "1. What causes DalEye-Llama's performance to drop dramatically on new text? Is it due to limitations in feature transfer or model overfitting?\n2. The paper mentions \"inherent noise\" in eye tracking data but does not discuss mitigation strategies. Have you tested noise reduction or fixation filtering to improve model stability?\n3. The generation model is only compared to GPT-4o in zero-shot mode. Could results with a fine-tuned GPT-4o (using the same text and gaze inputs) clarify advantages in efficiency or robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eh3ZfzCWb1", "forum": "fHr1uqkdsb", "replyto": "fHr1uqkdsb", "signatures": ["ICLR.cc/2026/Conference/Submission9074/Reviewer_H8io"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9074/Reviewer_H8io"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896618072, "cdate": 1761896618072, "tmdate": 1762920783906, "mdate": 1762920783906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and important investigation into decoding open-ended, text-specific information goals from eye movements during reading. The work is rigorous, introducing a clear task framework (goal selection and reconstruction) and systematically evaluating a range of discriminative and generative models. The experiments are comprehensive, using a large-scale, high-quality dataset (OneStop) and thoughtfully evaluating model generalization. While the generative task remains highly challenging, the strong performance of the best discriminative model, especially on the difficult \"same critical span\" task, provides compelling evidence that eye movements contain fine-grained information about a reader's goal. This work opens promising new directions for both scientific inquiry and practical applications."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.This is the first work to systematically address the decoding of arbitrary, text-specific information goals from eye movements. It moves beyond previous work that classified pre-defined procedural reading tasks (e.g., reading vs. skim-reading) to a more challenging and practically relevant semantic decoding task. The contribution is significant and opens a new research direction.\n\n2.The experimental design is exemplary. The data splits are carefully constructed to evaluate generalization to \"New Participants,\" \"New Texts,\" and \"New Text & Participant,\" providing a clear and honest assessment of model robustness. The subdivision of the selection task into \"Different\" and \"Same\" critical spans offers nuanced insight into task difficulty.\n\n3 The paper provides a thorough benchmark, covering simple heuristics, adapted state-of-the-art discriminative models, and pioneering generative LLM-based approaches. This offers a valuable overview of the landscape for this new task.\n\n4. The evaluation methodology is a strength. Beyond selection accuracy, the authors propose a robust set of metrics for the generative task, including question word/category agreement, BERTScore, and a creative downstream QA accuracy metric, which convincingly demonstrates the utility of the generated questions.\n\n5. The paper is accompanied by a code repository and includes extensive details in the main text and appendices regarding model architectures, hyperparameters, and training procedures, ensuring the work is easily reproducible."}, "weaknesses": {"value": "1. As the results show, the generative task is exceptionally difficult, and model performance, especially on new texts, is still limited. The generated questions are not yet on par with human-composed ones.\n\n2.The paper successfully demonstrates that goals can be decoded, but offers less insight into how or why the models make their decisions from a cognitive perspective. The models remain somewhat black-box.\n\n\n3. While generalization is tested, the significant performance drop in the \"New Text & Participant\" regime indicates that model robustness is still limited to the distribution of the training data."}, "questions": {"value": "1.In the RoBERTEye-Fixations model, which achieved the best results, did you perform any ablation studies to understand which specific eye-movement features (e.g., fixation duration, saccade amplitude, regressions) were most critical for its performance, particularly on the challenging \"Same Span\" task?\n\n2.For the generative DalEye-Llama model, performance drops most significantly in the \"New Text\" regime. Do you attribute this primarily to the model's difficulty in comprehending the new text content itself, or in associating the novel eye-movement patterns on that text with the question generation process?\n\n3.The paper mentions promising applications in education and assistive technology. Given the current accuracy levels (e.g., ~49% for 3-way selection), what do you see as the minimum performance threshold for such systems to be reliably deployable in real-world scenarios? What are the key next steps to bridge this gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cA7VKQejYz", "forum": "fHr1uqkdsb", "replyto": "fHr1uqkdsb", "signatures": ["ICLR.cc/2026/Conference/Submission9074/Reviewer_4c7d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9074/Reviewer_4c7d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905189290, "cdate": 1761905189290, "tmdate": 1762920783558, "mdate": 1762920783558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a cognitive-state decoding problem of recovering information-seeking goals (text questions) from a reader’s eye movements while reading a piece of text. The authors propose different task formulations for recovering the stimulus – (1) question selection and (2) question generation. They develop discriminative and generative models for the above tasks, that recover the question given the text and eye movement data. Results shown on the OneStop eye-tracking data showing discriminative models (RoBERTEye-Fixations) perform better than random while generative models (DalEye-Llama) shows promising performance on less challenging cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The dataset (OneStop) and problem setup is well suited to the objective of recovering reading goals. The evaluation regimes which include splitting data by new participant and new text is well conceived and creation of two tiers of difficulty are useful in comparing model performance in challenging settings.\n\nThe authors experiment with different types of baselines – heuristics, discriminative models based on adaptions of prior work and generative LLM models (DalEye-LLaVA, DalEye-Llama)."}, "weaknesses": {"value": "1. I would like to see what types of gaze features (eg: fixation durations, word revisits) are more useful for recovering the information seeking goals. Stronger experiments are required to investigate the feature attributions by gradually phasing out these features one by one from the eye movements data to train the models.\n\n2. It is not clear in the paper if the question and the text span containing the corresponding answer have significant substring overlap. If so, the problem becomes more trivial where users just have to look for specific strings from the question in the text. A more realistic and challenging case to evaluate would be if the user would have to understand and infer the meaning from the passage if the language is phrase differently.\n\n3. The authors should more comprehensively discuss about relevant literature in the field of eye movements conditioned on information seeking goals, for instance\n- Synthesizing Human Gaze Feedback for Improved NLP Performance (EACL 2023) which generates gaze patterns conditioned on the reader’s intent / task\n- GazeXplain: Learning to Predict Natural Language Explanations of Visual Scanpaths (ECCV 2024) which predicts scanpaths during performing visual question answering tasks or when instructed to search for a particular object in the image\n\n\nQuestions / Suggestions for Improvement:\n- Show n-gram overlap between target question and corresponding text span and correlate with model correctness.\n- More thorough experiments to analyse eye feature importances"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MXrzQgbYbR", "forum": "fHr1uqkdsb", "replyto": "fHr1uqkdsb", "signatures": ["ICLR.cc/2026/Conference/Submission9074/Reviewer_rzyv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9074/Reviewer_rzyv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194543690, "cdate": 1762194543690, "tmdate": 1762920783094, "mdate": 1762920783094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}