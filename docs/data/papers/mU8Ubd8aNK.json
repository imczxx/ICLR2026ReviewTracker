{"id": "mU8Ubd8aNK", "number": 12274, "cdate": 1758206782858, "mdate": 1759897520892, "content": {"title": "EasyCreator: Empowering 4D Creation through Video Inpainting", "abstract": "We introduce EasyCreator, a novel 4D video creation framework capable of both generating and editing 4D content from a single monocular video input. By leveraging a powerful video inpainting foundation model as a generative prior, we reformulate 4D video creation as a video inpainting task, enabling the model to fill in missing content caused by camera trajectory changes or user edits. To facilitate this, we generate composite masked inpainting video data to effectively fine-tune the model for 4D video generation. Given an input video and its associated camera trajectory, we first perform depth-based point cloud rendering to obtain invisibility masks that indicate the regions that should be completed. Simultaneously, editing masks are introduced to specify user-defined modifications, and these are combined with the invisibility masks to create a composite masks dataset. During training, we randomly sample different types of masks to construct diverse and challenging inpainting scenarios, enhancing the model’s generalization and robustness in various 4D editing and generation tasks. To handle temporal consistency under large camera motion, we design a self-iterative tuning strategy that gradually increases the viewing angles during training, where the model is used to generate the next-stage training data after each fine-tuning iteration. Moreover, we introduce a temporal packaging module during inference to enhance generation quality. Our method effectively leverages the prior knowledge of the base model without degrading its original performance, enabling the generation of 4D videos with consistent multi-view coherence. In addition, our approach supports prompt-based content editing, demonstrating strong flexibility and significantly outperforming state-of-the-art methods in both quality and versatility.", "tldr": "We reformulate 4D video creation as a video inpainting task and achieving the 4D video creation with camera trajectories and edited first frames", "keywords": ["diffusion model; 4D video generation and editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5041021f9e2b48f9998f3208e7a4b2137b4521f5.pdf", "supplementary_material": "/attachment/c96a09aac8100b10453b042686cfb498975506d7.zip"}, "replies": [{"content": {"summary": {"value": "EasyCreator converts a monocular input video into a dynamic point cloud via per-frame depth estimation, uses a double-reprojection strategy to derive visibility masks aligned with the original views, and combines these with user editing masks to form composite inpainting supervision that enables both 4D completion and 4D editing in a single framework.​ A self-iterative tuning scheme progressively increases viewpoint angles while reusing model outputs as training data to stabilize large camera motions, and a temporal-packing inference concatenates selected frames across trajectories in latent space to enforce multi-view consistency without adding new attention modules."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Reformulates 4D generation as video inpainting, directly targeting occlusion holes induced by camera retargeting and unifying 4D completion and editing within one framework.​\n2. Composite mask design (visibility via double reprojection plus user editing masks) provides flexible supervision that aligns with ground-truth content and supports practical edits.​\n3. Self-iterative tuning progressively increases camera angles to enhance temporal stability and large-view robustness without heavy re-architecture or large-scale retraining.​\n4. Temporal-packing inference enforces multi-view consistency by concatenating prior frames in latent space, leveraging existing spatio-temporal attention without extra fusion layers.​"}, "weaknesses": {"value": "1. I see that the paper doesn’t compare against a very similar method named PaintScene4D, which is also a Text-to-4D Scene generation method, but instead of video-based inpainting, they use image-based inpainting and progressively fill up the scene. Seems like PaintScene4D doesn’t have the code released, but I expect at least a qualitative comparison with similar text prompts shown by the work, and compare against their image and video results. The paper doesn’t even cite this work in its related works, which needs to be rectified. \n2. I see that you rely on the fact that the initial video generated is a static camera video with only object motion and no camera motion. How do you achieve this? Any specific prompt to achieve this, and what’s the success rate? Would the method work if there were any slight perturbations? How does the method handle those error inconsistencies? \n3. In case of initial warping tends to be bad or depth maps are bad, I suppose the bad depth maps might lead to bad reprojection, and this can indeed lead to bad depth maps for another view, and so on, and the error might propagate throughout the network. How does this method resolve such errors, as I am pretty sure that having inconsistencies while depth estimation might exist?\n4. Regarding training from small camera views to large camera views for better training stability: In PaintScene4D, it is mentioned that they use a farthest-view sampling, where they sample views farther from the initial camera because that is when the diffusion model has enough room to inpaint sufficient new details. But when you move the camera slightly, the small gaps created might not task the diffusion model to edit a lot, and it might sometimes just blur those holes. How does Video Diffusion Model differ from Image Diffusion Model in this case?"}, "questions": {"value": "I have highlighted the major questions in the weaknesses already, and I am summing them up below(I have summarised them shortly so that it is easier for the reviewer to quote the exact question they are answering. Please refer to the Weakness for the detailed problem and question asked.\n\n1. Why did you not compare or cite PaintScene4D, a very similar text-to-4D scene method that uses image-based inpainting and progressive filling? At minimum, can you provide a qualitative comparison using similar text prompts and compare your image/video outputs to theirs?\n\n2. You rely on the initial generated video being a static-camera video with only object motion. How do you ensure this in practice? Do you use specific prompts or procedures, what is the empirical success rate, and how sensitive is your pipeline to slight camera perturbations?\n\n3. If the initial warping or depth maps are poor, reprojection errors can cascade (bad depth → bad reprojection → worse depth for other views). How does your method detect and correct such cascading errors or inconsistencies during inference/training?\n\n4. When the camera moves only slightly, small gaps may be blurred rather than properly inpainted. How does your video diffusion model differ from an image diffusion model in this regime, and why should it handle small inter-frame gaps or slight camera movements better than image-based approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BTa3rK30sy", "forum": "mU8Ubd8aNK", "replyto": "mU8Ubd8aNK", "signatures": ["ICLR.cc/2026/Conference/Submission12274/Reviewer_FmC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12274/Reviewer_FmC3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760847746933, "cdate": 1760847746933, "tmdate": 1762923211119, "mdate": 1762923211119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EasyCreator, leveraging a powerful video inpainting model to enhance the 4D generation/editing. Formally, the double-reprojection strategy is used to synthesize warping masks, while random editing masks are used for practical editing. The authors further proposed self-iterative tuning to address large viewpoint changes. The temporal-packing inference is used as a memory strategy to confirm the multi-view consistency. This paper includes detailed experiments to verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation and overall layout of this paper are clear.\n2. This paper has detailed and comprehensive experiments, including quantitative and qualitative ablation studies, and convincing user studies.\n3. The proposed method enjoys good performance for 4D editing as shown in the supplementary and paper experiments."}, "weaknesses": {"value": "1. The paper proposes a good approach for editing 4D generations through video inpainting, which is a promising direction. However, the core technical contributions are somewhat incremental. For instance, the key training strategy, double-reprojection masking, has been extensively utilized in prior works such as TrajectoryCrafter, See3D, and Vivid4D[1]. Similarly, the use of random editing masks is a standard technique commonly applied in image and video inpainting tasks. Regarding the other contributions, namely self-iterative tuning and temporal-packing inference, their effectiveness remains unclear and needs further clarification, as detailed below.\n\n2. A key limitation of video inpainting is that it fails to address large viewpoint generation, especially when the viewpoint change >90 degrees, leading to completely wrong masks. Intuitively, iterative inpainting methods (such as those employed in Vivid4D[1], which closely relates to this work but is not discussed) may help mitigate this issue, but at the cost of error accumulation. I guess the self-iterative tuning should be proposed to solve this problem, but the authors have not explicitly discussed this connection nor provided ablation studies that convincingly demonstrate its impact (Fig. 6b is particularly ambiguous). Fig. 6b shows that the self-iterative tuning merely removes an optional pillar rather than substantially improving the visual quality. Moreover, the self-iterative tuning is super time-consuming in this work (2h for Wan14B). How to ensure the efficacy, especially when multiple iterations might be necessary for handling large viewpoint variations.\n\n3. The temporal-packing strategy is proposed to solve the multi-view video consistency. This technique is effective and somewhat convincing, as shown in Fig7. But some detailed implementation of temporal-packing is missing. What does the \"area calculation function\" mean in Line306? It is also not specified whether the selected frames in the input tensor ($x_{input}$) require re-noising during inference. If re-noising is unnecessary, the model should have been explicitly trained under such clean multi-view conditions. Conversely, if re-noising is applied, it raises questions about how reliably these noisy frames can serve as clear references for inpainting. Moreover, some inconsistencies remain visible—for example, the pink box areas in Fig. 7 do not appear perfectly consistent, which calls for a deeper explanation.\n\n4. Several details about the experiments and implementation are not provided. It is unclear how many iterations EasyCreator’s self-iterative tuning requires in practice. The training data are not introduced, and the scale of the experimental evaluation is relatively limited, including only 40 videos.\n\n5. This paper misses some related citations and discussion:\n\n[1] Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting. ICCV2025.\n\n[2] MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing. NeurIPS2024."}, "questions": {"value": "The authors should clarify the contribution of this work. I am also looking for the response to address the concerns about  \"self-iterative tuning\" and \"temporal-packing strategy\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wAvvbz6D2c", "forum": "mU8Ubd8aNK", "replyto": "mU8Ubd8aNK", "signatures": ["ICLR.cc/2026/Conference/Submission12274/Reviewer_Urtw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12274/Reviewer_Urtw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571437743, "cdate": 1761571437743, "tmdate": 1762923210779, "mdate": 1762923210779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EasyCreator, a method that generates multi-view 4D scene representations from input videos while enabling object editing within the scene. Built upon the Wan2.1 model, EasyCreator reformulates 4D scene generation as an image inpainting task and controls editable regions through an editing mask. In addition, an iterative tuning strategy is introduced to handle large camera motions, and temporal-packing inference is employed to ensure scene consistency across multi-view camera videos. Experimental results demonstrate that the proposed method outperforms existing approaches in terms of camera motion accuracy, video generation quality, and editing capability."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper integrates camera-controlled video generation and video editing into a unified framework, leveraging the power of generative models to reformulate the task as an image inpainting problem, thereby ensuring 3D consistency in video editing.\n- The paper is well written and logically structured, making it easy for readers to understand.\n- Experimental results show that the proposed method outperforms existing approaches in terms of video quality, camera accuracy, and editing capability. Moreover, it achieves better performance than the baselines of simple video composition–based editing methods and camera-controlled video generation methods."}, "weaknesses": {"value": "1. What is the relationship between text prompt control and reference image control during the editing process? If only the first edited frame is provided without a textual description of the editing instruction, or if only the textual description is given without the first edited frame, how would the editing results differ in each case? Do both the text and the reference image influence the editing outcome?\n2. The authors should discuss the performance of their method in challenging scenarios, such as when the input video involves large camera movements or when fast motion within the scene leads to inaccurate depth prediction and thus imprecise dynamic point clouds. It is recommended that the authors analyze how their method performs under these conditions."}, "questions": {"value": "Please see the weekness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9NC2zWt7Cz", "forum": "mU8Ubd8aNK", "replyto": "mU8Ubd8aNK", "signatures": ["ICLR.cc/2026/Conference/Submission12274/Reviewer_6ax8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12274/Reviewer_6ax8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905067237, "cdate": 1761905067237, "tmdate": 1762923209315, "mdate": 1762923209315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EasyCreator, a framework that reformulates 4D video generation and editing as a video inpainting task. It adapts a pre-trained video inpainting model (Wan2.1) using LoRA and novel training/inference strategies (self-iterative tuning, temporal-packing) to generate novel-view videos and support content editing from a single input video. The empirical results show state-of-the-art performance in terms of quality and consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. High-Quality Results: The framework successfully unifies novel-view synthesis and prompt-based content editing in a single pipeline, showcasing superior quantitative and qualitative results against existing dynamic 4D methods.\n\n2.Effective Adaptation: Cleverly leverages a powerful existing video inpainting foundation model, requiring only lightweight fine-tuning (LoRA) for a new, complex 4D task."}, "weaknesses": {"value": "1. Limited Technical Originality: The core pipeline—using geometry (reprojection/depth) to define masks for a subsequent video inpainting/completion step—is a common two-stage paradigm in 3D (Text2Nerf [1], Lucid Dreamer [2], text2room [3]) and 4D generation. Extending this concept to 4D video inpainting is not a sufficiently original technical contribution.\n\n\n2. Overclaiming Scope: The use of the term \"4D Creation\" is misleading. The method relies on a 2D video inpainting model conditioned by geometric priors derived from an initial reconstruction. It lacks an explicit, continuous 4D representation  to inherently enforce multi-view geometric consistency across all synthesized views. A more accurate description of the task is Camera-Controlled Video Generation, as the fidelity relies on the inpainting model's hallucination capability rather than a proper geometric rendering function.\n\n\n[1] Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields\n\n[2] LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes\n\n[3] Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models"}, "questions": {"value": "1. The self-iterative tuning strategy uses the model's own predictions from a smaller camera angle to train for larger angles. What mechanisms are employed to mitigate error accumulation or hallucination propagation from earlier, potentially less-stable stages into the subsequent training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "40zdmmgcQn", "forum": "mU8Ubd8aNK", "replyto": "mU8Ubd8aNK", "signatures": ["ICLR.cc/2026/Conference/Submission12274/Reviewer_DsCd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12274/Reviewer_DsCd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990241485, "cdate": 1761990241485, "tmdate": 1762923208213, "mdate": 1762923208213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}