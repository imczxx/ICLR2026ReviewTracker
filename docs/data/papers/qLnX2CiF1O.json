{"id": "qLnX2CiF1O", "number": 18825, "cdate": 1758291197688, "mdate": 1759897079253, "content": {"title": "Intermediate Layers Can Be Self-Hard Negative Generator For Large Language Model Based Recommendation", "abstract": "Large language models(LLMs) have gained significant attention for their usage in recommender systems. \nOne typical method to adapt LLMs for recommendation is Supervised Fine-tuning(SFT), and subsequent studies introduce preference learning to incorporate negative samples into the training process.\nHowever, the negative samples used in existing preference learning methods are sampled at the sequence-level in an offline process, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. \nTo address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommender systems, which utilizes self-hard negative signals extracted from intermediate layers to enhance preference learning for LLMs. Specifically, we first extract self-hard negative tokens from intermediate layers, which serve as fine-grained negative signals and dynamically reflect the model's preference learning process. To incorporate these negative signals into training, we devise a fine-tuning framework consisting of two components: cross-layer preference optimization and cross-layer preference distillation, which enables the model to effectively distinguish the negative signals and enhance the informativeness of negatives generated by intermediate layers. Additionally, we introduce a small collaborative filtering model to assign reward to each penalized token, preventing potential over-penalization of false negatives. Extensive experiments on three datasets demonstrate ILRec’s effectiveness in enhancing the performance of LLM-based recommender systems.", "tldr": "", "keywords": ["Sequential Recommendation; Large Language Model Recommendation;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80089c2296eb8100cff5a5cd03e9091a302cb8f9.pdf", "supplementary_material": "/attachment/d9fcda99efd122a5e5e686daa1b71dbd956c9d6e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ILRec, a fine-tuning framework for LLM-based recommender systems that leverages self-hard negative signals extracted from intermediate layers. The key idea is that intermediate layers can act as “non-expert” models whose outputs provide fine-grained, dynamically generated negatives during training. ILRec integrates these negatives through three mechanisms: (1) Cross-Layer Preference Optimization (CPO), which penalizes high-probability negatives in the final layer’s logits; (2) Cross-Layer Preference Distillation (CPD), which transfers knowledge from the final layer to intermediate layers to enhance negative informativeness; and (3) Collaborative Reward Regularization (CRR), which uses a lightweight CF model to prevent over-penalization of false negatives. Experiments on Amazon Review datasets demonstrate consistent improvements over existing LLM-based baselines such as BIGRec, LC-Rec, and DPO-style methods"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work targets a critical challenge in LLM-based recommendation: how to efficiently and adaptively introduce effective negative samples during fine-tuning.\n\n2. The paper is well-structured and easy to follow. \n\n3. The idea of extracting negative signals directly from intermediate layers is interesting."}, "weaknesses": {"value": "1. While the idea of extracting negatives from intermediate layers is interesting, its distinction from simply selecting high-probability tokens from the final layer’s distribution (or using beam search self-generated negatives) is not fully clear. Both intuitively capture similar model uncertainty patterns. The paper would benefit from deeper analysis and ablation (e.g., comparing intermediate vs. final-layer negatives, or evaluating informativeness across layers).\n\n2. The comparison set omits several recent and relevant RLVR-based LLM recommendation methods, which represent the current frontier. The paper primarily compares against DPO-style preference optimization (e.g., RosePO, SDPO, SPRec), which, as the authors acknowledge, suffer from policy-gap issues. Including RLVR-based baselines would strengthen claims of superiority.\n\n3. The inclusion of modules like Collaborative Reward Regularization (CRR) adds complexity, yet the reported gain appears modest. A clearer justification or simplification could make the approach more elegant.\n\n4. The authors argue that sequence-level reward is a limitation of existing methods, but provide neither theoretical reasoning nor empirical evidence to substantiate this claim."}, "questions": {"value": "1. Equation (4) introduces α as a key hyperparameter controlling the threshold for selecting negative tokens. How is this parameter set in practice, and how sensitive are results to its value? A more systematic sensitivity analysis would improve reproducibility and interpretability.\n\n2. The paper averages logits from multiple layers (Equation 3) to form ensemble logits. Have the authors compared this averaging scheme with alternative intra-model metrics (e.g., maximum probability, variance, entropy-based weighting)? Averaging may blur layer-specific diversity.\n\n3. The motivation cites the idea that “expert models can be optimized by contrasting them with non-expert models.” Have the authors considered using an external collaborative recommender (e.g., SASRec) as the non-expert model to provide complementary negatives? This could be an interesting extension or ablation to demonstrate the generality of the proposed contrastive mechanism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8Ja35XXBCS", "forum": "qLnX2CiF1O", "replyto": "qLnX2CiF1O", "signatures": ["ICLR.cc/2026/Conference/Submission18825/Reviewer_UTWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18825/Reviewer_UTWL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635608663, "cdate": 1761635608663, "tmdate": 1762930795831, "mdate": 1762930795831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies that existing preference optimization methods for LLM-based recommender systems suffer from two major limitations: (1) they assign only coarse-grained rewards to entire sequences, and (2) they sample negative items in an offline manner, both of which lead to suboptimal performance.\n\nTo solve these problems, the paper proposes **ILRec**, a fine-tuning framework for LLM-based recommender systems that extracts self-hard negative tokens for fine-grained optimization. Specifically, ILRec ensembles the logits from the intermediate layers and selects the negative tokens with high logit values as the self-hard negative tokens, which are then emphasized during optimization. ILRec also incorporates a distillation loss to improve the recommendation ability of intermediate layers and a collaborative reward regularization to inject collaborative information. Extensive empirical results demonstrate that ILRec outperforms prevalent preference optimization methods and is effective across different backbone models and item representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1: Well-founded Motivation.** The motivation is well-founded. Since the quality of negative items is important to recommendation, it is crucial to improve the suboptimal offline negative sampling pattern adopted by current preference optimization methods.\n\n**S2: Excellent Generality.** ILRec is effective for both textual item representation (BigRec [1]) and semantic IDs (LC-Rec). In addition, ILRec also enhances the performance across different backbone models and collaborative models.\n\n[1] Bao K, Zhang J, Wang W, et al. A bi-step grounding paradigm for large language models in recommendation systems[J]. ACM Transactions on Recommender Systems, 2025, 3(4): 1-27.\n[2 ]Zheng B, Hou Y, Lu H, et al. Adapting large language models by integrating collaborative semantics for recommendation[C]//2024 IEEE 40th International Conference on Data Engineering (ICDE). IEEE, 2024: 1435-1448."}, "weaknesses": {"value": "**W1: Method Design.** In some previous works [1], the introduction of weak experts is motivated by the lack of supervision from stronger experts. However, since the final layer generally performs better than the intermediate layers, the negative tokens derived from the logits of the final layer can provide stronger supervision, suggesting that incorporating intermediate layers may not necessarily lead to better results.  Although the ablation studies show that using negative signals from the last layer results in suboptimal results, the distillation loss should be removed in this setting because no intermediate layers are introduced, which means there is no need to enhance them. Additional discussions on these two methods, along with experiments introducing negative tokens from the final layer **without applying** the distillation loss, could more clearly validate the effectiveness of ILRec.\n\n**W2: Loss Analysis.** It is not straightforward to derive from the loss function of CPO in Equation 7 that challenging negative signals will be penalized more. Thus, a gradient analysis on CPO loss, similar to that presented in [2], is recommended to better illustrate the feature of CPO loss.\n\n**W3: Ablation Studies.** As shown in Table 2, incorporating $\\mathcal{L}_{\\text{CRR}}$ yields only marginal relative improvements (less than 2%), which cannot fully demonstrate the effectiveness of the collaborative reward regularization. Additional explanations and experiments are needed to further justify the importance of the collaborative signals.\n\n\n\n[1] Sang J, Wang Y, Zhang J, et al. Improving weak-to-strong generalization with scalable oversight and ensemble learning[J]. arXiv preprint arXiv:2402.00667, 2024.\n\n[2] Chen Y, Tan J, Zhang A, et al. On softmax direct preference optimization for recommendation[J]. Advances in Neural Information Processing Systems, 2024, 37: 27463-27489."}, "questions": {"value": "**Q1: Implementation Details.** Since LLM-based recommenders are prone to generating invalid or out-of-vocabulary items, it is unclear how the model in this paper performs inference for the full-ranking task. Please clarify the inference strategy and specify the exact evaluation pipeline adopted during testing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "691OlvjFiH", "forum": "qLnX2CiF1O", "replyto": "qLnX2CiF1O", "signatures": ["ICLR.cc/2026/Conference/Submission18825/Reviewer_t9vT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18825/Reviewer_t9vT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808301450, "cdate": 1761808301450, "tmdate": 1762930789308, "mdate": 1762930789308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how to utilize the intermediate layers of large language models as self-hard negative generators for recommendation. Instead of relying on negative sampling, the method leverages representations from the model’s intermediate layers to automatically identify “almost positive but actually negative” candidates, which serve as informative hard negatives. The approach further integrates cross-layer preference optimization (CPO) and cross-layer distillation (CPD) to better alleviate the intermediate layer information. Extensive experiments on multiple datasets verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**The motivation for utilizing the intermediate layers as negative generators is reasonable and intuitive.**\nThis design of mining “almost positive” tokens intuitively captures the informative negative signals without external sampling. \n\n**Comprehensive experiments.**\nThe effectiveness of the proposed method is validated across several benchmarks, showing consistent gains over existing negative sampling methods and traditional baselines. The ablation results also verify the contribution of each component."}, "weaknesses": {"value": "**Lack of discussion of utilized CF models.**\nThe design of collaborative reward regularization is quite heuristic and uncontrollable since we can not ensure which CF model for providing rewards is better. Additionally, this paper does not provide a study about the effectiveness of the chosen CF model. \n\n**Lack of analysis on layer selection.**\n Although the method relies on intermediate layers to generate self-hard negatives, the paper does not include experiments or ablations comparing different layer choices. It remains unclear which layers contribute most to performance or how sensitive the method is to this design choice."}, "questions": {"value": "Refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M7JOC0unkf", "forum": "qLnX2CiF1O", "replyto": "qLnX2CiF1O", "signatures": ["ICLR.cc/2026/Conference/Submission18825/Reviewer_Crkj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18825/Reviewer_Crkj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834866387, "cdate": 1761834866387, "tmdate": 1762930711027, "mdate": 1762930711027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of indistinctiveness and low informativeness of negative samples in existing LLM-based recommendation methods. It proposes ILRec, a novel preference fine-tuning framework that extracts fine-grained self-hard negative signals from the intermediate layers of LLMs. The framework consists of three core components: 1) Self-hard negative extraction from intermediate layers, which selects high-probability non-ground-truth tokens as token-level negative signals; 2) Cross-layer preference fine-tuning, including cross-layer preference optimization (integrating negative signals into cross-entropy loss with penalty coefficients) and cross-layer preference distillation (using the final layer to supervise intermediate layers); 3) Collaborative reward regularization, which employs a lightweight collaborative filtering (CF) model to assign token-level rewards and avoid over-penalization of false negatives. Extensive experiments on three Amazon datasets (Musical Instruments, Arts, Crafts and Sewing, Video Games) demonstrate that ILRec outperforms traditional sequential recommendation models and LLM-based baselines (e.g., BIGRec, LC-Rec, SDPO) on metrics like Hit@k and NDCG@k. Ablation studies verify the effectiveness of each component, and further analysis confirms its generalizability across different model backbones and recommendation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "## 1. Novelty.\n- The idea of using LLM intermediate layers as dynamic self-hard negative generators is innovative. It breaks through the limitation of offline static negative sampling in existing DPO-based methods and provides fine-grained token-level negative signals, which is a creative combination of LLM internal structure characteristics and recommendation task requirements.\n- The cross-layer preference optimization and distillation mechanism realizes the mutual promotion of negative signal quality and model learning effect, and the introduction of collaborative reward regularization effectively solves the problem of false negative over-penalization, forming a complete and self-consistent technical framework.\n\n## 2. Clarity.\n- The paper structure is logical: it starts with the limitations of existing methods, introduces the core idea, elaborates on the technical details of each component, and presents experimental results and analysis in turn. The logical chain from problem to solution to verification is complete.\n- The figures and tables are intuitive, which helps readers understand the core design and experimental results. The mathematical formulas are clearly defined and the symbols are consistent throughout the paper.\n\n## 3. Significance.\n- For the field of LLM-based recommendation, it provides a new effective path for negative sample construction, which can effectively handle the challenge of large negative item spaces and improve the model's ability to capture fine-grained user preferences."}, "weaknesses": {"value": "## 1. Insufficient analysis of intermediate layer selection\n- The optimal number of intermediate layers (k) is determined through experiments, but there is no theoretical guidance for the selection range of k. For different scales of LLMs (e.g., 1B vs. 3B vs. 8B), the suitable k may vary significantly, and the paper does not provide relevant analysis.\n\n- The paper selects \"consecutive intermediate layers before the final output layer\" as candidate layers but does not explain why consecutive layers are preferred over non-consecutive ones. It also lacks analysis on how the position of intermediate layers (e.g., shallow vs. deep) affects the quality of negative signals.\n\n## 2. Lack of analysis on computational cost details.\n- Although the paper mentions that ILRec does not introduce excessive training time costs, it does not provide detailed computational overhead analysis of key components.\n\n## 3. Insufficient exploration of application boundaries.\n- The experiment of this study is limited to the Amazon review dataset and lacks data from other scenarios such as Goodreads and Yelp."}, "questions": {"value": "## 1. Technical Questions\n\n- For collaborative reward regularization: If the CF model's recommendation results are seriously inconsistent with the LLM's preference, how will it affect ILRec's performance? Have you designed corresponding adaptive mechanisms?\n\n## 2. Experimental Suggestions\n\n- Add experiments on LLMs of different scales to verify the generalizability of ILRec across model sizes and analyze the optimal k value for different models.\n\n- Provide detailed computational cost metrics and compare them with baselines to clarify the efficiency advantages of ILRec in practical applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "reKhHocVvy", "forum": "qLnX2CiF1O", "replyto": "qLnX2CiF1O", "signatures": ["ICLR.cc/2026/Conference/Submission18825/Reviewer_cg7x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18825/Reviewer_cg7x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995462494, "cdate": 1761995462494, "tmdate": 1762930685125, "mdate": 1762930685125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}