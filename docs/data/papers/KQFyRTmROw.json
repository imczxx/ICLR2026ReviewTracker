{"id": "KQFyRTmROw", "number": 8803, "cdate": 1758098743781, "mdate": 1759897763051, "content": {"title": "Orchestrating Pre-Trained Agents for Multi-Objective Decision Making", "abstract": "Multi-Objective Optimization (MOO) in sequential decision-making tasks is commonly tackled by adapting classical methods (e.g., scalarization, $\\varepsilon$-constraints, Pareto-based evolutionary methods) or by training reinforcement learning (RL) models from scratch. In this paper, we propose a different paradigm: orchestrating AI agents to reuse pre-trained RL policies rather than retraining them for each new MOO task. We formalize this setting and develop several algorithmic variants that leverage three core components of agent orchestration: knowledge, tools, and reflection. To demonstrate the practical impact of our approach, we apply it to the Education domain, where our experiments show that agent orchestration achieves a favorable balance between high performance and reduced computational cost.", "tldr": "Agents for multi-objective optimization", "keywords": ["Multi-Objective Optimization", "Context Engineering", "LLM-judge", "ML Reusability"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6b4ca9e0d1b6250df707c3c0ad1cff30ba7817e.pdf", "supplementary_material": "/attachment/7e83a65818a3c1d5013950edad37c9003da7a153.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes \"orquestration\" strategies in which an LLM is used to select RL policies with the intention of improving multi-objective RL. The paper dives into details of how to build the context of the LLM for this specific task and shows experimental results in a mock \"learning\" environment showing how the LLM fares against simple baselines using scalarization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is very detailed in how the context is built for the experiments.\n\nPaper is well-written and easy to understand\n\nPaper took care of evaluate costs of running the different models in the performance evaluation"}, "weaknesses": {"value": "- The major weakness that resulted in the evaluation I am giving to this paper is that the setting of the problem the paper is trying to solve makes absolutely no sense to me and feels like this paper is an attempt of force-feeding LLMs into multi-objective RL in a way that it is not the best solution for that. I cannot think of a single practical scenario where we would have available numerous single-objective RL policies pre-trained and would want to use those policies to solve a multi-objective problem. If we are dedicating the time to train the policies one would straight away use a pareto-optimizing RL algorithm, instead of optimization it many times in a single objective manner. A scenario that *maybe* would make sense to me is that we want a solution with configurable scalarization weights, so you cannot optimize for the weights beforehand because they are unknown. In this scenario - maybe - we could assume that a large collection of policies with varying combinations of weights would be available and the LLM would be in charge of selecting which one fits best with the preference of the user given in plain english. But this scenario is very different from the one presented in the paper.\n\n- The environment used for validation is an unusual one and it doesn't seem like one could extract meaningful insights from it without human experiments. If the intention was to just have a toy problem, it would have been better to use a collection of different environments from for example MO-gymnasium https://github.com/Farama-Foundation/MO-Gymnasium\n\n- The results presented look weird, how did the baselines resulted in order of magnitude higher costs when they can very easily be executed in the user's own CPU/GPU? furthermore, the LLM-orchestrated runs also have to execute the RL envrionments in the same way to solve the problem, so there shouldn't be much saving from using the LLM even without considering the cost per token.\n\n- I also don;t understand why the metrics are presented the way they are in the evaluation - it would be much simpler and more meaningful the simply showing hypervolume and sparsity."}, "questions": {"value": "No specific questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nGTAj9EJIG", "forum": "KQFyRTmROw", "replyto": "KQFyRTmROw", "signatures": ["ICLR.cc/2026/Conference/Submission8803/Reviewer_wH1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8803/Reviewer_wH1w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760674895543, "cdate": 1760674895543, "tmdate": 1762920572534, "mdate": 1762920572534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes novel approaches for reusing pre-trained RL policies to solve multi-objective decision-making tasks via an orchestration mechanism in LLMs. The proposed technique is based on the context engineering formalization by Meietal. (2025), and the main idea is that the orchestration is able to select the most appropriate base RL policy based on three different types of contextual information: Knowledge-based Orchestrator (KO), Tool-based Orchestrator (TO), and Reflection-based Orchestrator (RO). These three approaches are evaluated in an application in Education, where the goal is to recommend questions to students to attain skill mastery. In this scenario, there are three conflicting objectives: aptitude, gap, and performance. The method is compared with single RL policies optimized via scalarization of objectives or via reward machines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Multi-objective decision-making is a very relevant problem, and building intelligent agents able to control trade-offs between conflicting rewards is challenging.\n\n- The experiments are very extensive and were conducted using an insteresting and complex application. It discusses many different metrics/advantages and properties of each approach."}, "weaknesses": {"value": "- The method is an application of existing context engineering formalizations (Mei et al. (2025)), and it is not very clear if the only theoretical contribution is the application of such techniques to a multi-objective task.\n\n- The objective of the paper is posed in a misleading way. The research question of the paper is stated as:  “Can we combine existing single objective policies to better cover the Pareto frontier?”\nHowever, the proposed approach is not trying to cover the Pareto frontier. The orchestrators are learning a single policy that obtains a reasonable trade-off between all Pareto-optimal solutions. The method is not trying to cover all Pareto optimal solutions, but only to achieve a single one. How would it be possible to generate multiple solutions in the Pareto front based on dynamic user preferences via the proposed approach?\n\n- A vast literature on multi-objective RL (MORL) algorithms exists, but was ignored in this paper. See [1-4] below, for instance. These methods should have been discussed, since they intend to approximate the Pareto frontier by learning multiple policies separately or by learning a single policy conditioned on preferences that generalizes over multiple weights. The authors, however, only considered a single-objective baseline that uniformly weights all objectives. It would be very relevant to observe how these approaches would compare in the experiments. There are also MORL approaches in particular applied to LLMs that need to be discussed [5-7].\n\n- There must be a misunderstanding, but I do not see how the cost calculation is computed in a fair way. The orchestrators required a set of pre-trained RL policies, and at inferece time they select one of the available policies, as state in the beginning of Section 3.2. Hence, the cost of the orchestrator approach (e.g., TO) involves the cost of inference of each of the pre-trained candidate RL policies. The multi-objective scalarization policy (MO-S), however, only involves training and inference of a single RL policy. It is completly unclear to me how an orchestrator can be less costly than a pure RL approach if it also involves training RL-policies and selecting one of then at inference time.\n\n- The last section of the paper is Section 4 - Experiments. The paper is missing a Conclusion section to summarize the main findings, discuss the limitations and future works.\n\n[1] Alegre, Lucas N., Ana L. C. Bazzan, Diederik M. Roijers, Ann Nowé, and Bruno C. da Silva. ‘Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization’. Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems.\n\n[2] Hayes, Conor F., Conor F. Hayes, Rădulescu, Roxana, et al. ‘A Practical Guide to Multi-Objective Reinforcement Learning and Planning’. Autonomous Agents and Multi-Agent Systems 36, no. 1 (2021). https://doi.org/10.1007/s10458-022-09552-y.\n\n[3] Röpke, Willem, Mathieu Reymond, Patrick Mannion, Diederik M Roijers, Ann Nowé, and Roxana Radulescu. ‘Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning’. Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems.\n\n[4] Lu, Haoye, Daniel Herman, and Yaoliang Yu. ‘Multi-Objective Reinforcement Learning: Convexity, Stationarity and Pareto Optimality’. The Eleventh International Conference on Learning Representations. 2022. https://openreview.net/forum?id=TjEzIsyEsQ6.\n\n[5] Wang, Kaiwen, Rahul Kidambi, Ryan Sullivan, et al. ‘Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning’. arXiv:2407.15762. Preprint, arXiv, 23 October 2024. https://doi.org/10.48550/arXiv.2407.15762.\n\n[6] He, Qiang, and Setareh Maghsudi. ‘Pareto Multi-Objective Alignment for Language Models’. arXiv:2508.07768. Preprint, arXiv, 11 August 2025. https://doi.org/10.48550/arXiv.2508.07768.\n\n[7] Jafari, Yasaman, Dheeraj Mekala, Rose Yu, and Taylor Berg-Kirkpatrick. ‘MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization’. Findings of the Association for Computational Linguistics: EMNLP 2024, Association for Computational Linguistics, 2024, 9878–89. https://doi.org/10.18653/v1/2024.findings-emnlp.577."}, "questions": {"value": "- In line 130, the authors define the return variable without the discount factor $\\gamma$. However, in Table 5 the value of $\\gamma$ used by the algorithms is set to 0.99. Did the authors evaluate if there is any discrepancy between discounted and undiscounted returns?\n\n- Why is the MDP constrained to be deterministic?\n\n- “We model the environment as a multi-reward MDP” ->\nIn the multi-objective RL, this is called a multi-objective MDP (MOMDP). I would suggest using this nomenclature to be consistent with the rest of the literature.\n\n- For completeness and improved clarity, I suggest discussing and presenting the Context engineering formalization by Mei et al. (2025) in more detail at the beginning of Section 3.1.\n\n- “For MOO, we modified the training of A2C and PPO by introducing a replay buffer that stores trajectories from only non-dominated episodes.”\nCould you please elaborate on why this was necessary? Known applications of PPO for MORL do not implement such filtering in the replay buffer.\n\n- In Appendix A.2, the authors express the SARSA update equation for tabular RL. Since the authors used neural networks as function approximation, I suppose that a DQN-style loss was used instead? Please clarify.\n\n- “We align TO with the Reflexion Paradigm Shinnetal. (2023) which models a single linear test time scaling loop with three roles: an actor (performing the decision), an evaluator (assessing the quality of the decision given the environment), and a self-reflection step (feeding back into the next decision).”\nHow were these implemented? How does the evaluator assess the quality of the decision? Since this is a multi-objective decision-making policy, the quality is subjective to the user preferences. How are the preferences inferred?\n\nMinor: \n- The authors are using \\citet when the correct would be \\citep."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IIVOS02nUn", "forum": "KQFyRTmROw", "replyto": "KQFyRTmROw", "signatures": ["ICLR.cc/2026/Conference/Submission8803/Reviewer_tvPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8803/Reviewer_tvPz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760980319689, "cdate": 1760980319689, "tmdate": 1762920571805, "mdate": 1762920571805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a model that reuses pre-trained RL policies rather than retraining them for each new tasks in multi-objective decision making, and applies the model to an education system."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Motivation is clear - making use of LLM to assist in reusing pre-trained RL policies sounds reasonable."}, "weaknesses": {"value": "It is not clear to me why you want to considier multi-objective decision making - using LLM to assist pre-trained RL policies does not apply to the mono-objective case? It seems to me that the method does not make full use of the multi-objective case, except considering nondominated policies. \n\nI am not an expert in multi-objective RL, but you stated that policy gradients are computed only from steps belonging to these non-dominated trajectories. I am wondering how this could be done. Since you may have many incomparable policies, how you compute gradients without aggregating them? Especially, in the case there are many objectives, almost all policies could be incomparable. \n\nThere are no much descriptions and explanations about the figures - I feel difficulty to understand them. Figures should be self-contained - without checking the text, readers should be able to understand them (e.g. via legends and captions). \n\nTypo: ``...on an non-overlapping subset...'' in Figure 1."}, "questions": {"value": "Could you please respond to the first two comments in the weaknesses field?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "STPoRKatc2", "forum": "KQFyRTmROw", "replyto": "KQFyRTmROw", "signatures": ["ICLR.cc/2026/Conference/Submission8803/Reviewer_sRAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8803/Reviewer_sRAN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835480257, "cdate": 1761835480257, "tmdate": 1762920571507, "mdate": 1762920571507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel approach to multi-objective sequential decision-making by orchestrating pre-trained single-objective RL policies using large language models (LLMs). The goal is to avoid retraining costs while leveraging LLMs’ natural language reasoning capabilities to choose from a set of expert policies under different objective priorities. The approach is evaluated primarily in a newly introduced educational recommendation environment, with experiments spanning scalarized objectives, LLM guidance, and human-judgment comparisons.\n\nWhile the idea of post-training orchestration is timely and potentially impactful, the paper lacks sufficient theoretical and empirical grounding, particularly with respect to existing multi-objective reinforcement learning (MORL) literature and metrics. Several conceptual components (e.g., prompts, agents, contexts) also lack integration into a cohesive framework"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores an original direction, zero-shot multi-objective decision-making by combining existing policies via LLM guidance, addressing the cost and scalability challenges common in RL.\n- The emphasis on energy and computational savings aligns with pressing concerns around sustainable AI.\n- The authors provide a detailed experimental setup and introduce a domain-specific benchmark environment that may be of value to the community if released openly.\n- The modular design of the orchestrators (knowledge-based, tool-based, reflection-based) reflects creative thinking toward policy composition."}, "weaknesses": {"value": "-The paper overlooks foundational work in Multi-Objective Reinforcement Learning (MORL), where many of the addressed themes, trade-off learning, policy reuse, Pareto approximation, have already been studied in detail. Similarly, although the approach resembles zero-shot policy composition, the authors do not employ this terminology nor connect to existing zero-shot RL or policy reuse work, which raises concerns about positioning and familiarity with relevant paradigms.\n\n- The central idea, composing pre-trained single-objective policies via LLMs for multi-objective tasks, is not sufficiently justified over established MORL techniques, which are capable of learning diverse trade-offs directly. Relying on fixed single-objective policies may also inherently limit Pareto coverage.\n\n- The paper introduces important components (prompts, agents, context embeddings) in isolation without a global design blueprint. This makes it difficult to understand how the system functions holistically or to generalize it beyond the educational domain.\n\n- The experiments assess performance only using scalarized metrics, without considering standard multi-objective measures like hypervolume or Pareto cardinality. As a result, empirical claims about effectively handling trade-offs remain insufficiently demonstrated.\n\n- The approach is evaluated only on a single, synthetic domain and relies heavily on textual descriptions of policies. It is unclear how well this setup would generalize to higher-dimensional state spaces or control tasks where policy semantics are not easily verbalized.\n\n- It is not fully transparent whether the cost and CO₂ analyses include the initial single-objective RL policy training stages. If not, this skews the claimed “carbon efficiency” in favor of the proposed method."}, "questions": {"value": "- How does your approach compare to MORL methods that directly learn diverse policies or Pareto fronts, such as evolutionary MORL, preference-conditioned policies, or hypernet-based approaches? Could your framework also be interpreted as a form of zero-shot RL or post-training policy reuse?\n\n- Why were canonical multi-objective metrics such as hypervolume or ε-indicator not incorporated into the evaluation? Would including these metrics change your conclusions about policy coverage or performance?\n\n- Have you considered testing the orchestration approach on standard multi-objective RL benchmarks in robotics or resource allocation, where action spaces and objectives are more complex?\n\n- How do performance and cost scale as the number of pre-trained policies increases, or when policies are not easily described in textual form?\n\n- Does your cost estimate include all life-cycle costs of the RL policies being reused (i.e., training time and compute)? If not, can you clarify the assumptions behind your analysis?\n\n- Will the educational benchmark environment and orchestration code be made available as open-source contributions to the community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LzfDB4T6I9", "forum": "KQFyRTmROw", "replyto": "KQFyRTmROw", "signatures": ["ICLR.cc/2026/Conference/Submission8803/Reviewer_LGoB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8803/Reviewer_LGoB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996727825, "cdate": 1761996727825, "tmdate": 1762920571220, "mdate": 1762920571220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}