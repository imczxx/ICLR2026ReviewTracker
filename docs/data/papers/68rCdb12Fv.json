{"id": "68rCdb12Fv", "number": 21540, "cdate": 1758318722076, "mdate": 1759896916752, "content": {"title": "Feedback promotes efficient-coding while attenuating bias in recurrent neural networks", "abstract": "Studies of human decision-making demonstrate that environmental regularities, such as natural image statistics or intentionally nonuniform stimulus probabilities, can be exploited to improve efficiency (termed `efficient-coding'). Conversely, from a machine learning perspective, such nonuniform stimulus properties can lead to biased neural networks with poor generalization performance. Understanding how the brain flexibly leverages stimulus bias while maintaining robust generalization could lead to novel architectures that adaptively exploit environmental structure without sacrificing performance on out-of-distribution data. To address this disconnect, we investigated the impact of stimulus regularities in a 3-layer hierarchical continuous-time recurrent neural network (ctRNN) to better understand how artificial networks might exploit statistical regularities to improve efficiency while avoiding undesirable biases. We trained the model to reproduce one of six possible inputs under biased conditions (stimulus 1 more probable than stimuli 2-6) or unbiased conditions (all stimuli equally likely). Across all hidden layers, more information was encoded about high-probability stimuli, consistent with the efficient-coding framework. Importantly, reducing feedback from the final hidden layer of trained models selectively magnified representations of high-probability stimuli, at the expense of low-probability stimuli, across all layers. Together, these results suggest that models exploit nonuniform input statistics to improve efficiency, and that feedback pathways evolve to protect the processing of low-probability stimuli by regulating the impact of biased input statistics.", "tldr": "", "keywords": ["RNN", "cognitive science", "neuroscience", "bias", "decision making"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6f0d6bd21399a977515f048426f7315cc463b6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study discusses potential roles of feedback connections in recurrent networks. The authors trained a generic recurrent network (RNN) to reconstruct input series. The model has 6 input and 6 output nodes, each of which corresponds to a class. With each class (e.g., class1), one of the input nodes (e.g., input node 1) is activated accordingly, and the model is trained to activate the corresponding output node (e.g., output node 1). \n\nBy perturbing feedback connections (from late to early layers), the authors analyzed the changes in hidden activity patterns. Based on their results, they claim that 1) feedback can “facilitate neural processing of high probability stimuli” in recurrent networks and 2) this feedback connection may be useful to create efficient coding in artificial networks such as DNNs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The implications of the results are interesting and may promote interdisciplinary research between neuroscience and deep learning."}, "weaknesses": {"value": "In my opinion, the evidence to both claims is too weak. First, the given task is too simple, and the network model is also too generic to capture neural dynamics associated with high-level cognitive functions. Second, the authors used RNNs, which have feedback connections from late (high-order) and early (low-order) layers, but most DNNs do not have such feedback. Therefore, it seems difficult to justify generalizing the results reported in this study to DNNs."}, "questions": {"value": "1. The authors do not define clearly what biases, expectations, attention and nonuniform stimuli are in this study. These concepts may be relevant to one another, but they are not automatically the same. Detailed explanations of these concepts would improve readability of this manuscript.  \n\n2. The model used in this study is a generic recurrent model. Can the authors explain what they mean by “Hierarchical recurrent networks”? \n\n3. Common DNNs are feedforward networks, but the authors are testing RNNs in this study. In DNNs, a feedback loop that can deliver feedback signals in RNNs does not exist. I would like to ask the authors how much of their results in RNNs were generalized into DNNs.\n\n4. In line 205, the manuscript stated  “After training, we evaluated 20 independently initialized models on a new balanced dataset of 2,400 trials.” What does “initialization” mean here? To me, it sounds like the models are randomly constructed, which cannot be true. I presume that some units with stochastic properties (e.g., initial membrane potentials) are initialized randomly, but this should be clearly stated.\n\n5. $\\Delta$ AUC is used to measure  the bias in the study, but the authors do not explain why this metric can estimate the bias.. \n\n6. What does “hierarchical stimulus-response representations” in line 321 mean? The authors also state, “In order to encourage separate representations of the stimulus and the corresponding response behavior”, which sounds extremely ambiguous. They should clarify what they mean here. The authors may be implying that the earlier layers encode the stimuli, and the late layers encode predictions, but even if this is the case, they do not explain why this is a desirable property of recurrent networks with feedback connections from late to earlier layers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6uc59EWxqv", "forum": "68rCdb12Fv", "replyto": "68rCdb12Fv", "signatures": ["ICLR.cc/2026/Conference/Submission21540/Reviewer_FXim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21540/Reviewer_FXim"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703817582, "cdate": 1761703817582, "tmdate": 1762941827651, "mdate": 1762941827651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission investigates the role of feedback connections in shaping representations in recurrent neural networks.\nIn particular, the authors investigate this effect in the context of training networks to perform a dataset using either balanced or unbalanced training datasets, as well as in the presence or absence of significant input noise.\nExperimental results demonstrate:\n\n  - networks trained on a bias stimulus ensemble perform the decoding task better for stimuli sampled more frequently (despite having lower overall performance) when there is significant stimulus noise present. \n - when noise is absent, the internal representations reciprocate the bias in the training dataset even though the output behavior is relatively unbiased. \n - The extent of the performance bias grows across the network hierarchy in a condition where feedback between layers is attenuated.\n- These observations are preserved in a task variant that requires a delayed response (triggered by an external stimulus cue rather than at stimulus onset)."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Investigating the importance of different architectural features of in terms of the impact on stimulus representations is an interesting research direction. \n- The observation that bias emerges/increases in networks when dealing with noisy inputs is both interesting and novel to my knowledge.\n- The experiments conducted all seem scientifically sound to me\n- The authors are committed to releasing code for the purpose of reproducibility\n- The simplified setting allows for meaningful/interpretable ablations that implicate a specific set of parameters/a specific architectural element in a computation/behavioral output (top down feedback in encoding training set induced biases)."}, "weaknesses": {"value": "- Experiments are limited to an extremely toy setting, and it is difficult to say whether or not the observations from this paper would generalize to more complex settings (perhaps something like sequential MNIST could be considered)?\n- Section 4 seems to offer little to the paper. It seems to me that the only difference this ablation caused was that training networks with late cue timing reduced the impact of down-regulating  feedback in terms of model performance. This did not jump out to my eyes as a particularly obvious or important control, and thus felt slightly orthogonal to the main contributions of this work. I think this sections' contribution would have been more obvious a the main text discussion was limited to where this change had an impact and this result was accompanied by a main text figure. \n- The presentation of some results was unclear/hard to parse for me. See below my question about figure 3A for an example."}, "questions": {"value": "- In many figures the task performance of intermediate layers is considered. I assume these were ascertained by training additional linear readouts on the internal layers. Is this the case? If so, was this mentioned in the paper somewhere?\n- I am having trouble parsing Figure 3A. What differentiates the colors here? I thought the difference between A and B was whether or not feedback was attenuated but that also seems to be the difference between the colors? \n- Typo in lines 207-208: is the noise level 0.1 (0.6) )or 0.01 (0.06)? Main text and figures seem to disagree.\n- Even though there is little evidence for long range inhibitory connections, is it not true that feedback signals are/can be routed through local inhibitory interneurons? I am wondering what the impact of this architectural constraint might be. \n- What implications does this set of experiments have for our ability to understand computation in more interesting/complex neural systems (be they biological or artificial)? The author's mention this as a future direction, but I think slightly expanding on this by proposing some example experiments could strengthen/make more obvious the contribution of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rE4gIZ93uJ", "forum": "68rCdb12Fv", "replyto": "68rCdb12Fv", "signatures": ["ICLR.cc/2026/Conference/Submission21540/Reviewer_ZsFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21540/Reviewer_ZsFJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789358545, "cdate": 1761789358545, "tmdate": 1762941827442, "mdate": 1762941827442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to investigates how artificial neural networks learn from biased or imbalanced data distributions, aiming to reconcile the \"efficient-coding\" hypothesis from neuroscience with the harmful effects of biased data and poor generalization seen in machine learning. This is investigated by constructing a three-layer continuous time RNN model (ctRNN) which operates to distinguish between a set of six input signals. The authors report that, a ctRNN trained on imbalanced data can perform well and yet when inter-layer feedback connections are manipulated, networks with imbalanced data during training suffer more than models with balanced training schemes. This is taken as evidence that these feedback connections protect the representations of rarer stimuli, such that their ablation causes biases to be magnified and other representations to collapse. This is also tested in models in which a cue is also provided to the output to the network which indicates a change in task (change in mapping) such that top-down information flow could be further confirmed and this effect reproduced."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The overview of existing literature across multiple niches in neuroscience and AI was carried out relatively well and placed this research direction well\n- The proposal that feedback connections within biological networks might actively regulate and suppress harmful, prior-induced biases is an interesting and potentially powerful hypothesis.\n- Using an intervention-based method (ablation) method was a good start for examining the function of the feedback pathways in this model"}, "weaknesses": {"value": "- **Overstated contributions**: The biggest weakness of this work is simply that the decision making setups are so simple as to be unrealistic and potentially uninformative. The simple 1-of-6 reproduction tasks are used to make significant claims of contributing to a better understanding of brains and AI. Quite simply, these extremely limited toy models are a far cry from a real confirmation of the role of feedback signals. Questions are not posed or explored as to whether these results may be a fluke of this dataset or the models constructed. Questions such as: Would this same solution be found for more complex problem sets? Could a particular aspect of the network's initialization or structure be the cause of these observations?\n-  **Limited exploration of known issues with AI models**: This weakness links to the above point and expands upon it from an AI and ML perspective. Artificial neural networks are well known for finding solutions which differ depending upon how weights are initialized. For example, to solve the tasks posed in this paper, feedback connections are likely entirely unnecessary. So why is this particular solution found in these networks? I would hypothesize that it is because the feedback connections are initialized rather strongly (or as strongly as feedforward connections) something which is not typically thought to be true in biological neural networks. Furthermore, neural network models are known to suffer from catastrophic forgetting and strong biasing by data biases, effects that are not seen in humans. Questions around whether the effects seen here are purely curiosities of neural network models are also not posed. Instead, it is simply assumed that the same mechanism is likely active in brains.\n- **Lack of rigorous alternative testing**: In this work, the feedback connections in the model are perturbed in order to determine whether they are important to overcoming bias in the network's training. However, the other (feedforward and lateral) connections are never tested. Would similar effects not simply show up no matter which connections were ablated? I believe so, considering that this is a fully connected dynamical system in which information circulates. This point is never considered or approach in this setup. \n\nFinally, note that although this manuscript discusses at length the perturbation and manipulation of feedback connections from higher to lower layers, the mathematical description of the model (Equations 1) describe no top-down connectivity, only recurrent and feedforward."}, "questions": {"value": "In the weaknesses section above, I posed a number of issues. Each include questions which I believe should have been answered within this work for completion. Please consider these point by point.\n\nOne additional question which remains: Were all models trained to convergence in accuracy? The bias models seem to consistently have lower accuracies than the unbiased models (even in the low noise range). I would expect for such a task that they could both be trained to 100% accuracy, especially in ctRNNs with millions of parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HVBqOS4hvy", "forum": "68rCdb12Fv", "replyto": "68rCdb12Fv", "signatures": ["ICLR.cc/2026/Conference/Submission21540/Reviewer_usMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21540/Reviewer_usMX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857342813, "cdate": 1761857342813, "tmdate": 1762941827231, "mdate": 1762941827231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses a multilayer recurrent network with feedback trained to estimate the mean for a set of discrete noisy inputs presented on separate channels one at a time to ask the question of how the presence of feedback affects the quality of processing of uncommon vs frequent inputs. It uses variation in test input noise and modulation of feedback strength to show differential effects when training with a equal across inputs or unequal input frequency  in the training set. Test set is always equal probability. Feedback is shown to influence the biases induced by uneven training statistics, which may have some implications for understanding the role of feedback in representing prior input statistics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The question of the role of feedback in enforcing sensory priors is an interesting one that benefits from modeling and numerical investigation."}, "weaknesses": {"value": "Conceptually the idea of bias in efficient coding is a little weird. The goal of learning  for both efficient coding models in neuroscience and for ANNs trained by optimization, is to minimize error (or other loss) taken in expectation over a given data distribution. Increasing precision of representation for commonly occurring stimuli is normative and not a nuisance or failure. For the brain, out of distribution generalization to unnatural sensory experience is not ecologically relevant so using the ML language of bias and failure to generalize makes little sense in the neuroscience context. That makes part of the premise somewhat suspect. A better way to formulate the scope of the question is to ask what role do local circuit inhomogeneities in coding vs top down feedback effects play in differential processing of common versus uncommon stimuli. But even in that framing, one would need to compare results across multiple tasks for the answer to be compelling.\n\nLearning an identity input output mapping with heavily biased input statistics is very basic in terms of computational complexity. It is not clear what if any of the conclusions generalize to tasks that require some (nonlinear) computation for the input-output map. The top down cue forces feedback to be important by construction so it's not clear to me how much of those results are surprising or educational.\n\nKey elements of the implementation are not described in the main text.\n\nA fraction of the results are trivially expected, e.g. within distribution test accuracy being better than significantly out of distribution."}, "questions": {"value": "Equation 1 lists purely feedforward interaction and none of the feedback connections mentioned in the text and figure. Given the critical nature of these connections for the scientific question it makes it hard to assess the results of numerical simulations. I have operated under the assumption that the reciprocal pattern of connectivity (no skip connections) between layer is what was used but this needs clarification.   This was clarified to be true much later in the results but made methods description confusing, please fix.\n\nWhat loss function was used for training? L2 ? why not cross-entropy? does it matter?\n\nSmall comment: calling a constant mean gaussian iid input as \"time varying\" is in my opinion misleading and should be corrected\n\nFig2bc: some lines are not visible? if overlapping please use dashed lines or a small horizontal offset to clear up where the invisible lines lie\nare the decoders evaluated on the same (ood for biased training) test distribution?\n\nFig3b: why are there biases in the network trained with uniform stimulus distribution after feedback ablation? and why is it the largest then the 3 to 2 feedback is left intact (dark green)?\n\nFig3c: same issues with visuals as for fig 2\n\nWhy are the responses so nonstationary given that the task uses completely stationary inputs with iid noise? are the dynamics seen in PC space a reflection propagation time constants? does a layer specific version look more interpretable in terms of the effects of feedback? \n\nWhat is the dimensionality of the neural activity space, i.e. how much variance does the first 3pcs explain about the data?\n\nCan you understand the nature of the dynamics? e.g. by linearizing dynamics around stimulus specific fixed points?\n\nsection 4.2: i fail to understand how training without a cue can possibly resolve the ambiguity between which of the two permutation maps the circuit needs to implement in any given trial. was that blocked somehow or by \"no cue\" you mean learning a single stationary permutation map?\n\nSimilarly it's not clear why performance would not be affected by the late vs early cue distinction: in the late cue you just process the inputs in early layers then implement the map at the last layer?\n\nthe text describes the results in figure 4 and 5 but provides essentially no interpretation for any of it. What do the results mean in terms of the nature of representations and the nature of the feedback formed via different task types? can you say more about the interpretation of the results beyond that feedback makes a difference?\n\ncan you comment on the importance of recurrence in the context of the tasks presented? not clear that any of the computations require much recurrence except for the purpose of noise averaging, would you expect different effects if the network were feedforward but with additional layer y layer feedback connections making up the full circuit recurrence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EZ4ytkuDzz", "forum": "68rCdb12Fv", "replyto": "68rCdb12Fv", "signatures": ["ICLR.cc/2026/Conference/Submission21540/Reviewer_iPHi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21540/Reviewer_iPHi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942832125, "cdate": 1761942832125, "tmdate": 1762941827006, "mdate": 1762941827006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}