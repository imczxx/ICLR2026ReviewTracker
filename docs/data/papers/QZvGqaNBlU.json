{"id": "QZvGqaNBlU", "number": 10217, "cdate": 1758164292899, "mdate": 1759897665819, "content": {"title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra", "abstract": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.", "tldr": "Training-free activation vector algebra composes and dynamically steers LLM personalities at inference-time, matching fine-tuning on PersonalityBench and achieving strong win rates on the Persona‑Evolve benchmark.", "keywords": ["Large Language Models", "Personality Control", "Representation Engineering", "Model Steering", "Inference-Time Adaptation", "Compositionality"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6671ea9ba00e294fea29bc322f46df86b697bb73.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles a relevant problem for controllable, human-centric LLMs and keeps computation light by avoiding gradient updates. The extraction/steering recipe is clearly described with layer-local residual addition and sign-controlled intensity, and the algebraic hypothesis is supported by linear modulation plots and cosine-similarity structure among trait poles. The dynamic variant is straightforward to deploy as a predict-then-steer wrapper and shows consistent pairwise wins on PERSONA-EVOLVE. The PersonalityBench comparison is comprehensive and places the method alongside known activation-engineering baselines, nearly matching an SFT upper bound without training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a relevant problem for controllable, human-centric LLMs and keeps computation light by avoiding gradient updates, which reads quite interesting. The extraction/steering recipe is clearly described with layer-local residual addition and sign-controlled intensity, and the algebraic hypothesis is supported by linear modulation plots and cosine-similarity structure among trait poles. The dynamic variant is straightforward to deploy as a predict-then-steer wrapper and shows consistent pairwise wins on PERSONA-EVOLVE. The PersonalityBench comparison is comprehensive and places the method alongside known activation-engineering baselines, nearly matching an SFT upper bound without training."}, "weaknesses": {"value": "I am not yet fully convinced that the orthogonality and algebraic claims are sufficiently stress-tested. The cosine matrix in Figure 2 and the near-linear response plots show encouraging structure, but they are still correlational; there is no causal independence check across layers or tasks. A concrete fix would be to measure cross-talk under controlled multi-trait interventions: hold Outgoing fixed and sweep Compassionate, then report marginal effects on Extraversion and Agreeableness with confidence intervals; repeat across 3–5 nearby layers to confirm that vector orientation and efficacy are not layer-idiosyncratic. Please anchor these analyses around the orthogonality section and the BFI linearity section.\n\nThe evaluation pipeline leans on model judges and synthetic scenarios, which risks circularity. PERSONA-BASE uses GPT-4.1-mini both to score trait expression and (earlier) to help generate artifacts; PERSONA-EVOLVE uses LLM comparisons for win rates. To mitigate evaluation coupling, add a human-rated subset of BFI-adapted items (even 200–300 judgments) and report Pearson/Spearman vs GPT-4.1-mini, and introduce an external non-OpenAI judge to check consistency. Please point to the rubric generation and adapted BFI details and then augment them with a human slice.\n\nThe definition of dynamic control in PERSONA-FLOW is under-specified. The paper states a two-stage predict-then-steer with $\\alpha_i \\in [-2,2]$, but it does not spell out how the coefficients are predicted (prompt template only, few-shot, or a small learned head), what features are exposed to the predictor, whether history length matters, and how stability is enforced across turns to avoid oscillation. Add ablations where you vary coefficient binning, history windows, and thresholding $|\\alpha|>0.5$; report win-rate deltas and the variance of $\\alpha$ across turns in the same dialogue. Please tie these additions to the methodology block. \n\nThe near-SFT claim on PersonalityBench needs stronger controls. Table 4 aggregates means/variances but does not state the steering layer index, coefficient grids, or response-length constraints used for all baselines; it is also unclear whether the same judge model and prompts were used for every method. Re-run 2–3 strongest training-free baselines under an identical harness (same judge, same prompts, same layer/length constraints) and add paired significance tests (possibly, not a must) can help. Also cite the PersonalityBench section and Table 4 and extend with a controlled subset. \n\nThe PERSONA-EVOLVE construction would benefit from clearer provenance and quality control. The pipeline describes persona/story arcs and expected styles, but the paper does not list how many writers reviewed each scenario, inter-annotator agreement on expected tone, or leakage checks against the prompts used to extract vectors.\n\nSafety and alignment considerations are acknowledged but not quantified. Some traits are resistant to activation (e.g., self-interested) and some push against alignment objectives."}, "questions": {"value": "How stable are persona vectors across adjacent layers and model scales? Please include a “layer sweep” figure showing cosine between vectors from layers $l$ and $l\\pm k$ and the resulting BFI slopes, and a cross-model alignment study via Procrustes on Qwen-7B vs Llama-8B. \n\nWhat is the error profile of PERSONA-FLOW coefficients across turns? Report the distribution of predicted $\\alpha$ per trait, autocorrelation over turn indices, and a simple low-pass smoothing variant; link to the two-stage methodology description. \n\nCan you provide a human-rated slice for PERSONA-EVOLVE and BFI-adapted items, with agreement vs GPT-4.1-mini? This would address judge coupling concerns raised above.\n\nPlease report refusal/guardrail activation rates under steering (fraction of turns producing safety disclaimers) and add a cap on $\\alpha$ that keeps safety loss within a tolerance; also include a failure taxonomy and rates. This belongs next to orthogonality validation and the ethics statement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m1msdLVqqZ", "forum": "QZvGqaNBlU", "replyto": "QZvGqaNBlU", "signatures": ["ICLR.cc/2026/Conference/Submission10217/Reviewer_ajoV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10217/Reviewer_ajoV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761159060573, "cdate": 1761159060573, "tmdate": 1762921576550, "mdate": 1762921576550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PERSONA, a training-free framework for personality control in Large Language Models through direct manipulation of activation vectors. The approach extracts orthogonal personality trait vectors from the OCEAN model, demonstrates they support algebraic operations (scalar multiplication, addition, subtraction), and enables dynamic context-aware personality adaptation during inference. On PersonalityBench, PERSONA achieves a mean score of 9.60, nearly matching supervised fine-tuning's 9.61 upper bound without training. On their proposed PERSONA-EVOLVE benchmark for dynamic personality adaptation, the method achieves up to 91% win rates across diverse model families."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-structured framework with clear components (PERSONA-BASE, PERSONA-ALGEBRA, PERSONA-FLOW) that build upon each other logically. The empirical validation is thorough, testing on both external benchmarks and their custom PERSONA-EVOLVE dataset across multiple model architectures. The approach demonstrates impressive performance, matching fine-tuning results without requiring gradient updates, and the algebraic operations on personality vectors are well-validated through systematic experiments."}, "weaknesses": {"value": "The paper lacks detailed analysis of failure cases or limitations of the approach, particularly in scenarios where personality traits might conflict. The extraction methodology relies heavily on GPT-4.1-mini for evaluation, which could introduce biases in how traits are defined and measured. Additionally, while the authors claim orthogonality of personality vectors, Figure 2 shows significant correlations between certain traits across dimensions, suggesting the extracted vectors aren't truly orthogonal."}, "questions": {"value": "1. How might the framework handle conflicting personality traits that are simultaneously required in complex social situations, and what mechanisms could help resolve such conflicts?\n2. To what extent does the reliance on GPT-4.1-mini for trait evaluation impact the objectivity of the extracted personality vectors, and how might this be addressed?\n3. Given the correlations observed between certain trait vectors (e.g., nervous and careless), how might these interdependencies affect the algebraic operations and resulting personality expressions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LuQcvf2qHm", "forum": "QZvGqaNBlU", "replyto": "QZvGqaNBlU", "signatures": ["ICLR.cc/2026/Conference/Submission10217/Reviewer_vzcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10217/Reviewer_vzcY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761183680314, "cdate": 1761183680314, "tmdate": 1762921575868, "mdate": 1762921575868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a training-free framework for personality control in LLMs based on activation vector algebra. The core insight is that personality traits can be identified as approximately orthogonal directions in activation space and manipulated algebraically to achieve controllable behavior. PERSONA-BASE extracts orthogonal personality vectors from activation layers, PERSONA-ALGEBRA demonstrates how these vectors can be combined or suppressed through arithmetic operations, PERSONA-FLOW dynamically adjusts trait expression in context during inference through a predict-then-steer mechanism, and PERSONA-EVOLVE serves as a new benchmark for evaluating dynamic personality adaptation. Experiments on various model families show that PERSONA nearly matches fine-tuning performance on PersonalityBench, indicating that aspects of LLM personality are mathematically interpretable and controllable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper is well-motivated to reframe personality control as a problem of vector manipulation in activation space, providing a new geometric and interpretable perspective distinct from prompt engineering or fine-tuning.\n* PERSONA-FLOW's ability to modulate personality adaptively during inference is promising on controllable LLMs. It shows that behavioral alignment can be achieved through lightweight inference-time adjustments rather than parameter updates.\n* The proposed PERSONA-EVOLVE benchmark offers a systematic way to assess dynamic personality adaptation, and results across multiple model architectures show the robustness of the proposed method."}, "weaknesses": {"value": "* Intuitively, personality-related features should be distributed across multiple layers of LLMs rather than concentrated in a single one. However, PERSONA-BASE depends on selecting the most effective layer without providing empirical justification. Further analysis and ablation experiments are needed to verify whether personality information is indeed localized within a particular layer.\n* The improvements over NPTI appear marginal overall and are mostly confined to the Openness trait. It is therefore unclear what concrete advantages this method offers relative to NPTI. Moreover, Ju et al. (2025) introduce a similar personality-editing framework that achieves effective control under adversarial prompts. What specific benefits does the proposed approach provide over theirs?\n* The paper does not evaluate whether activation-space steering introduces unintended side effects on general downstream abilities such as MMLU and TruthfulQA. Also, the paper does not evaluate whether activation-space steering affects inference latency. Since the method performs additional forward passes for coefficient prediction and residual injection, it could increase computational cost during inference. \n\n[1] Ju et al., Probing then Editing Response Personality of Large Language Models, 2025."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZF5CJxNGxF", "forum": "QZvGqaNBlU", "replyto": "QZvGqaNBlU", "signatures": ["ICLR.cc/2026/Conference/Submission10217/Reviewer_22zs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10217/Reviewer_22zs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206741081, "cdate": 1761206741081, "tmdate": 1762921575041, "mdate": 1762921575041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework called PERSONA for dynamic and compositional personality control in LLMs, which moves beyond static personality prompts and resource-heavy fine-tuning by manipulating personality vectors directly within the model's activation space. The framework contains three different parts for different personality domains. The paper also gives one benchmark called PERSONA-EVOLVE benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper focuses on personality of LLM which is one important area and gives new method to use LLM. The paper is well writing. The paper is easy to follow."}, "weaknesses": {"value": "1. The architecture is training-free while it still needs LLM to generate personality vector which is related to the training data of LLM and makes the performance unclear.\n\n2. The paper gives three different personality vectors while there is no description about the method to define these three personality.\n\n3. The figure in this paper should be larger."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3GBU3oTLXr", "forum": "QZvGqaNBlU", "replyto": "QZvGqaNBlU", "signatures": ["ICLR.cc/2026/Conference/Submission10217/Reviewer_j78u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10217/Reviewer_j78u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950997930, "cdate": 1761950997930, "tmdate": 1762921574198, "mdate": 1762921574198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}