{"id": "tFNsosNMHT", "number": 15350, "cdate": 1758250462711, "mdate": 1759897312340, "content": {"title": "Secure Autoregressive Inference with Prompt Separation via Key-Value Caching", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance, driving their widespread adoption across various applications. This prevalence increases the importance of user request privacy during inference. While Fully Homomorphic Encryption (FHE) and Secure Multi-Party Computation (MPC) offer promising solutions for privacy-preserving inference, they suffer from significant latency overhead, limiting practical deployment. Prior research has explored more efficient cryptographic primitives and polynomial approximations for non-linear operations. However, the inference latency remains significantly higher than that of plaintext execution. To further mitigate computational overhead, we introduce a novel approach that leverages prompt separation with key value caching. Our method accelerates secure inference by processing non-sensitive tokens in plaintext and using their key-value caches when subsequently processing private tokens. To ensure effective contextual reasoning, we also introduce an attention mask adjustment mechanism that constrains privacy-sensitive tokens to attend to nearby tokens from their original masked positions. Through experiments across various LLM architectures and MPC frameworks, we show that our approach achieves a 1.5-2.5$\\times$ reduction in inference latency without significant performance degradation.", "tldr": "", "keywords": ["privacy-preserving inference", "large language model", "autoregressive generative models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/240e6433416adee4ec2baa5230b3a449b786665c.pdf", "supplementary_material": "/attachment/7a99b50b0ebcbba3b7691fde6502acf4d4ef4a87.zip"}, "replies": [{"content": {"summary": {"value": "Traditional MPC reasoning requires re-execution of security protocols after each token is generated, resulting in extreme latency. This work addresses the token-by-token dependency issues of auto-regressive LLMs by implementing structured predictions. This involves performing computations on non-sensitive tokens on the plaintext side first, calculating partial activations without leaking previous tokens. Furthermore, the attention buffer is secretly shared, eliminating the need for re-negotiation of keys during cross-stride reasoning, reducing communication traffic and encryption costs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research problem is valuable, and the solution is highly compatible with existing MPC frameworks (such as CrypTen and SecFormer). The method is tested on **GPT-2** and **Qwen2-0.5B-Instruct**, and under two MPC frameworks, **CrypTen** and **SPU**, respectively, achieving **1.5–2.5×** inference acceleration on several MMLU subtasks with very low accuracy loss."}, "weaknesses": {"value": "**1.Idealistic security assumptions:**\n\nThe security proof is incomplete. The authors assume that \"non-sensitive tokens are visible in plaintext,\" but overlook the fact that the length, structure, or position pattern of plaintext prompts can also lead to side-channel information leakage.\n\n**2.Limited parallelism:**\n\nAlthough the scheme reduces communication rounds, inference parallelism is still constrained by the depth of autoregressive dependencies, making it difficult to completely eliminate the sequencing bottleneck.\n\n**3.Lack of systematic comparative experiments:**\n\nWhile the paper mentions related work (MPCFormer, Bolt/Iron, etc.), it lacks cross-sectional experiments using the same dataset and framework, or overlay tests with other protocols."}, "questions": {"value": "1. Are the structure, length, and position features of the plaintext considered potentially private information?\n\n2. Can the KV be reused across requests after being converted from the plaintext domain to the ciphertext domain?\n\n3. Does it maintain near-zero performance with long contexts (>32k), enhanced retrieval (RAG), conversational security (rejection policy), and code generation?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8maLyzulWS", "forum": "tFNsosNMHT", "replyto": "tFNsosNMHT", "signatures": ["ICLR.cc/2026/Conference/Submission15350/Reviewer_F5HB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15350/Reviewer_F5HB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761058893369, "cdate": 1761058893369, "tmdate": 1762925638665, "mdate": 1762925638665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prompt separation technique to accelerate secure Transformer inference based on Fully Homomorphic Encryption (FHE) and Secure Multi-Party Computation (MPC). The core insight is that not all input tokens carry privacy-sensitive information. Leveraging this, the authors separate private and non-private tokens using a Personally Identifiable Information (PII) identifier. The key-value (KV) cache for non-private tokens, along with their corresponding masks, is computed in plaintext and reused during secure inference over private tokens. This avoids computing the entire KV-cache in the encrypted domain. Experimental results demonstrate that the proposed approach can accelerate inference by up to 2.5 times."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow. The prompt separation framework and its integration with KV caching are clearly explained.\n\n2. The research topic is important and well-motivated. Enhancing the efficiency of secure Transformer inference has direct relevance to privacy-preserving real-world applications.\n\n3. The evaluation is thorough. The authors conduct experiments on both GPT-2-small and Qwen2-0.5B-Instruct, showing consistent speedups over baseline methods."}, "weaknesses": {"value": "1. The threat model requires further clarification:\n\n- This work assumes that only certain tokens—those identified as PII (e.g., PERSON, PHONE_NUMBER, DATE_TIME)—are privacy-sensitive. This contrasts with prior works such as MPCFormer, Iron, and Bolt, which treat the entire input as private. However, contextual information like grammar, tense, or sentence structure may also leak private information. Clarifying the scope of what is considered private would make the threat model more rigorous.\n\n- Additionally, the claim in line 831 that model parameters remain hidden even in the two-party setting is inaccurate for frameworks such as Iron and Bolt, where model weights are held in plaintext by the server.\n\n2. There is noticeable accuracy degradation. Although the proposed technique improves inference efficiency, it sometimes leads to reduced accuracy, as shown in Tables 2 and 3. For instance, on the History dataset using the SPU backend, accuracy drops from 27.27 to 25.54."}, "questions": {"value": "1. Are there known failure cases of the prompt separation approach? Would fine-tuning the LLM in a mask-aware manner help mitigate the accuracy loss?\n\n2. How is privacy leakage assessed when non-private tokens are exposed to an untrusted server? Are there quantitative metrics or threat analyses that support the claimed privacy guarantees?\n\n3. The current evaluation is limited to relatively small models such as GPT-2-small. How does the proposed technique scale to larger models in terms of performance and security trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3fOngGC3nN", "forum": "tFNsosNMHT", "replyto": "tFNsosNMHT", "signatures": ["ICLR.cc/2026/Conference/Submission15350/Reviewer_x5aM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15350/Reviewer_x5aM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939172035, "cdate": 1761939172035, "tmdate": 1762925638291, "mdate": 1762925638291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an interesting idea to accelerate the performance of secure inference over private tokens by separating the private tokens from the rest of the prompt. By only hiding the computation over the private tokens, they reduce the amount of operations that needs to be performed within the secure computation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea presented in this work is novel and has the potential to substantially improve the performance of semi-private inference."}, "weaknesses": {"value": "While the idea in this paper is very interesting, I was not able to verify correctness due to a lack of explanation for the other parts of the attention block. The matrix products in the attention block mix the values across tokens, so it is not clear from the paper how these values are handled. In particular, if the intermediate states that are functions of the private tokens are not revealed, then this approach seems like if would be nullified after the first block (since the entire state would be a function of the private tokens). On the other hand, if the intermediate results are revealed except at the locations of the private tokens, this could leak information about the private tokens. More explanation is needed on these steps."}, "questions": {"value": "I would like to understand how the matrix operations within the attention blocks are performed with some of the input rows masked. It seems like most (if not all) of the entries of the final output of the attention block are functions of all input tokens, so if some of these values are masked then it’s not clear how this output is computed for the subsequent blocks. Could you give a complete description of the modified attention block (including all matrix operations) with the masked tokens? \nWhen masking private tokens, is there a generic embedding for each category? Or is the embedding for the private tokens replaced with whatever embedding the model assigns to the token “[MASKED_TOKEN_#]”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RNMORQXMM6", "forum": "tFNsosNMHT", "replyto": "tFNsosNMHT", "signatures": ["ICLR.cc/2026/Conference/Submission15350/Reviewer_HCNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15350/Reviewer_HCNH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038742582, "cdate": 1762038742582, "tmdate": 1762925637925, "mdate": 1762925637925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method to accelerate privacy-preserving inference for LLM under homomorphic encryption through a technique that separates sensitive and non-sensitive tokens in a prompt. Then, non-sensitive tokens are processed in plaintext to precompute key-value caches, while sensitive tokens are processed securely using these cached representations. An attention-mask adjustment mechanism ensures that sensitive tokens still attend to relevant context. Experiments on GPT-2 and Qwen2 under MPC frameworks show 1.5–2.5× faster inference with minimal performance loss, and communication costs drop up to 4×. This approach maintains security while making encrypted inference substantially more practical"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed framework of processing the majority of the insensitive text in plain text and only processing the sensitive information in cyphertext is quite ingenious. While there is still a security concern in terms of whether this protocol can really protect one's privacy (redacted documents often do reveal a lot of private information), given the need for some degree of security while not incurring an inordinate compute cost is a nice compromise."}, "weaknesses": {"value": "The attention mask adjustment feels unnecessary to me, and it doesn't seem like the paper provides sufficient evidence supporting the necessity of this mechanism.\n\nAlso, I was hoping the acceleration would be larger than 2x or 4x. Can the authors explain why the speedup is not more extreme despite the sensitive words only consisting of a small portion of the input prompt?"}, "questions": {"value": "What evidence do you have that the proposed attention sink mechanism is necessary? Is this really the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tyRgadl79r", "forum": "tFNsosNMHT", "replyto": "tFNsosNMHT", "signatures": ["ICLR.cc/2026/Conference/Submission15350/Reviewer_d7wb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15350/Reviewer_d7wb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079902421, "cdate": 1762079902421, "tmdate": 1762925637450, "mdate": 1762925637450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}