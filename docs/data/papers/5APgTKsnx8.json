{"id": "5APgTKsnx8", "number": 4557, "cdate": 1757706381513, "mdate": 1763643230517, "content": {"title": "Streaming Visual Geometry Transformer", "abstract": "Perceiving and reconstructing 3D geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and low-latency applications, we propose a streaming visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 3D reconstruction. This design can handle low-latency 3D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operators (e.g., FlashAttention) from large language models. Extensive experiments on various 3D geometry perception benchmarks demonstrate that our model enhances inference speed in online scenarios while maintaining competitive performance, thereby facilitating scalable and interactive 3D vision systems.", "tldr": "", "keywords": ["3D reconstruction", "geometry transformer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2f19c87531f947a1a3abc4a53b348268f05ada8.pdf", "supplementary_material": "/attachment/6718250d9ca43fd376df544545d2255326fcc4ed.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an online, low-latency 3D reconstruction pipeline. It achieves this by (1) temporal causal attention, cross-attending to cached KV-tokens from previously observed frames, and (2) utilizing knowledge distillation during training by setting VGGT (a full SA-based reconstruction model) as the teacher to provide the GT for the causal student model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow. The problem is well motivated and explained.\n2. The proposed causal attention is simple but effective, and performs well empirically."}, "weaknesses": {"value": "1. In Table 6, the impact of KD is rather drastic, which makes the contribution of the causal attention questionable. When reporting results for w/o KD, is the 'true' GT used as supervision? The authors claim that KD helps reduce error accumulation, but this claim needs a more involved analysis."}, "questions": {"value": "1. Did the authors explore strategies for constraining/truncating the memory usage for very long scenes? Perhaps simply incorporating a recency bias i.e. cacheing and cross-attending to only the most recent K scenes' tokens instead. An ablation over different K can be helpful.\n2. Can the authors explain the results reported in Figure 6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9andtlyBGo", "forum": "5APgTKsnx8", "replyto": "5APgTKsnx8", "signatures": ["ICLR.cc/2026/Conference/Submission4557/Reviewer_6AuX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4557/Reviewer_6AuX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578752833, "cdate": 1761578752833, "tmdate": 1762917438824, "mdate": 1762917438824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StreamVGGT, which reconstructs 3D geometry from videos in an online manner. They do this by adopting a causal transformer architecture and caching the historical keys and values to use for each incremental new frame. They train StreamVGGT through knowledge distillation of VGGT, which is state-of-the-art, \"offline\", but slower. For 3D reconstruction, their approach increases inference speed relative to offline methods, but with a small performance reduction. They are competitive with (and sometimes slightly better than) other streaming methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper's approach of caching memory tokens for streaming reconstruction is important for efficiency, and it is useful for the field to see the results and comparisons they report. I think this makes the paper useful and worth publishing. \n2. The paper is written clearly and provides a very readable explanation of their method and other methods. \n3. StreamVGGT is competitive with other streaming methods, while adopting a significantly different approach. The additional experiments (inference speed, memory, distillation ablations) also serve as useful information."}, "weaknesses": {"value": "1. It would be useful to explicitly compare the inference (and training, if possible, but understandably this may be harder) speed and memory consumption of their approach against other streaming reconstruction methods like CUT3R and Spann3R. I think this is crucial because the core argument of their paper is efficiency.\n2. \"StreamVGGT leverages the inherent sequential and causal nature of real-world video data, constraining the attention mechanism to past and current frames, thereby aligning with the causal structure observed in human perception.\" I agree that humans receive input frames in an online streaming manner. However, do we know if there is evidence that their representation of old views is not modified by (and so does not \"attend to\") new views, like in KV caching? One counterpoint is that our stored experiences and memories are modified by new information. It is possible that humans use some hybrid: KV-caching where stored keys/values are allowed to be modified. At a few points in this paper, the paper implicitly suggests that StreamVGGT is more human-like than full self-attention approaches. I think this requires more evidence, or otherwise hedge the claim."}, "questions": {"value": "1. To what extent is the inference speedup and memory reduction (Figure 2) due to memory token caching versus FlashAttention-2?\n2. I think it would be helpful to discuss: what are the different methods (e.g., StreamVGGT, CUT3R) good at and where do they fail? How is this linked to their method (architecture, training strategy, training data, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U8Cot1ATg0", "forum": "5APgTKsnx8", "replyto": "5APgTKsnx8", "signatures": ["ICLR.cc/2026/Conference/Submission4557/Reviewer_adCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4557/Reviewer_adCa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850971329, "cdate": 1761850971329, "tmdate": 1762917438563, "mdate": 1762917438563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StreamVGGT, a causal transformer architecture designed for real-time streaming 3D visual geometry reconstruction. Inspired by the success of large language models with causal attention, the authors integrate temporal attention and a cached token memory mechanism to enable efficient, incremental 3D scene updates without needing to reprocess full sequences. Extensive experiments on multiple 3D reconstruction benchmarks demonstrate that Stream3R achieves competitive performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This demonstrates that using cache token technology can significantly improve VGGT's inference performance with long video inputs.\n2. The technical details are clear and easy to follow."}, "weaknesses": {"value": "1. The authors’ demo video showcases the model’s capability to handle scenes with significant motion. It would be helpful if the paper included more analysis or discussion regarding these dynamic scenarios.\n2. I don't quite follow the motivation behind using knowledge distillation (KD). First, why not directly train the model on real-world data? Is it because the dataset size and quality are insufficient compared to VGGT, leading to suboptimal results? Second, since VGGT cannot be trained on long video sequences due to the computational cost of attention, wouldn't this limitation of the teacher model affect StreamVGGT’s performance on long sequences—especially for tasks like pose estimation?\n3. Figure 2 showcases the time efficiency advantage over the vanilla VGGT. However, since the authors emphasize the streaming nature of their approach, it would be more convincing if they could also compare the overall reconstruction time with other similar methods, such as CUT3R."}, "questions": {"value": "1. Since there are already many similar works (e.g., Stream3R, Lan et al.) that also adopt caching techniques, it would be beneficial if the authors could include a comparison with these approaches in the paper.\n2. Adding a pose estimation visualization and a comparison with CUT3R would make the paper more complete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qgybeTjGBq", "forum": "5APgTKsnx8", "replyto": "5APgTKsnx8", "signatures": ["ICLR.cc/2026/Conference/Submission4557/Reviewer_XfbV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4557/Reviewer_XfbV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917485838, "cdate": 1761917485838, "tmdate": 1762917438115, "mdate": 1762917438115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StreamVGGT, a transformer architecture for streaming 3D visual geometry reconstruction. The model draws inspiration from autoregressive language models and replaces global self-attention with temporal causal attention, enabling incremental 3D reconstruction with cached token memory. StreamVGGT supports online inference while maintaining spatial and temporal consistency. The approach achieves comparable reconstruction and depth estimation performance to offline methods (e.g., VGGT) while substantially improving inference speed on long sequences."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper addresses a practical problem: enabling low-latency 3D reconstruction suitable for real-time streaming systems.\n* The temporal causal attention and cached memory are conceptually simple, computationally efficient, and align well with online mapping needs.\n* The experimental evaluation is extensive, covering multiple datasets and comparing against both dense view and streaming baselines.\n* The writing is clear, structured, and easy to follow.\n* The paper demonstrates significant latency reduction (e.g., 10× faster inference for long sequences) while maintaining similar accuracy compared to VGGT."}, "weaknesses": {"value": "* The issue of error accumulation over long sequences is not thoroughly analyzed. No explicit mechanisms (e.g., drift correction) are introduced beyond distillation, and the paper does not present accuracy trends across different sequence lengths compared to VGGT.\n* The scalability and memory-growth behavior of cached tokens for very long video sequences remains unclear.\n* The connection to causal or autoregressive modeling is somewhat superficial: there is no explicit probabilistic formulation or next-frame prediction, only causal masking.\n* The paper does not explore qualitative failure cases, such as dynamic occlusions or fast ego-motion, which are critical for practical deployment."}, "questions": {"value": "* How does the model handle error accumulation or drift over long sequences (e.g., hundreds or thousands of frames)?\n* Does the cached token memory ever saturate or require pruning? If so, how is this managed without degrading accuracy?\n* Can the causal model generalize beyond static datasets (e.g., driving, dynamic human motion)?\n* How robust is StreamVGGT to missing or noisy frames, which are common in real streaming data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XvXFPfuvQB", "forum": "5APgTKsnx8", "replyto": "5APgTKsnx8", "signatures": ["ICLR.cc/2026/Conference/Submission4557/Reviewer_KPdZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4557/Reviewer_KPdZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762488238742, "cdate": 1762488238742, "tmdate": 1762917437836, "mdate": 1762917437836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}