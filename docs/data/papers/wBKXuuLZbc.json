{"id": "wBKXuuLZbc", "number": 15959, "cdate": 1758257662384, "mdate": 1759897270600, "content": {"title": "Beyond Grid-Locked Voxels: Neural Response Functions for Continuous Brain Encoding", "abstract": "Neural encoding models aim to predict fMRI-measured brain responses to natural images. fMRI data is acquired as a 3D volume of voxels, where each voxel has a defined spatial location in the brain. However, conventional encoding models often flatten this volume into a 1D vector and treat voxel responses as independent outputs. This removes spatial context, discards anatomical information, and ties each model to a subject-specific voxel grid. We introduce the NRF Neural Response Function, a framework that models fMRI activity as a continuous function over anatomical space rather than a flat vector of voxels. NRF represents brain activity as a continuous implicit function: given an image and a spatial coordinate (x, y, z) in standardized MNI space, the model predicts the response at that location. This formulation decouples predictions from the training grid, supports querying at arbitrary spatial resolutions, and enables resolution-agnostic analyses. By grounding the model in anatomical space, NRF exploits two key properties of brain responses: (1) local smoothness—neighboring voxels exhibit similar response patterns; modeling responses continuously captures these correlations and improves data efficiency, and (2) cross-subject alignment—MNI coordinates unify data across individuals, allowing a model pretrained on one subject to be fine-tuned on new subjects. In experiments, NRF outperformed baseline models in both intrasubject encoding and cross-subject adaptation. Achieving high performance while reducing the data size needed by orders of magnitude. To our knowledge, NRF is the first anatomically aware encoding model to move beyond flattened voxels, learning a continuous mapping from images to brain responses in 3D space.", "tldr": "", "keywords": ["Neural encoding model", "Computational Neuroscience", "Neuroimaging", "Medical imaging", "Implicit neural representation"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac91bfd21d6fc0f74a5d868dceb8030db18df644.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper adapts Neural Radiance Fields (NeRF) for the fMRI-to-image encoding task. In contrast to the common approach of predicting each voxel independently, this method predicts all voxel activations in a coupled manner. This strategy yields locally smooth predictions and facilitates efficient transfer learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an original approach to the challenging task of predicting fMRI activations in a structured and meaningful way.\nThe framework also facilitates an interesting subject adaptation method."}, "weaknesses": {"value": "- The reported encoding performance is very low. On the NSD dataset, a decent model for the selected voxels should yield correlations above 0.5 for subjects 1, 2, and 5, with strong models achieving values around 0.6. Notably, other works (e.g., [1, 2]) report significantly better performance on a wider selection of NSD voxels.\n\n- Applying the fWRF approach to layers from DinoV2/CLIP, combined with LoRa adaptation and well-tuned hyperparameters, would outperform the reported results by a larger margin than the one shown between the NRF method and other baselines.\n\n- The fWRF model outperforms the proposed approach on all but one reported decoding metrics.\n\nReferences:\n\n[1] Adeli, H., Sun, M., & Kriegeskorte, N. (2025). Transformer brain encoders explain human high-level visual responses. arXiv preprint arXiv:2505.17329.\n\n[2] Beliy, R., Wasserman, N., Zalcher, A., & Irani, M. (2024). The Wisdom of a Crowd of Brains: A Universal Brain Encoder. arXiv preprint arXiv:2406.12179."}, "questions": {"value": "- Can your approach be combined with competitive encoding models? \n- Does your method enable an increase in the spatial resolution of the fMRI signal, or is the benefit of local smoothness limited to the predictions themselves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZK2cZn9uq", "forum": "wBKXuuLZbc", "replyto": "wBKXuuLZbc", "signatures": ["ICLR.cc/2026/Conference/Submission15959/Reviewer_vUPt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15959/Reviewer_vUPt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475271344, "cdate": 1761475271344, "tmdate": 1762926170504, "mdate": 1762926170504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an implicit neural representation method for fMRI data, called neural response function (NRF). The proposed method utilizes local smoothness and cross-subject alignment to learn a function that can be used to predict the voxel-level response to a given stimulus continuously over R^3 space. This is presented as a significant improvement over the current state of the art methods that generally flatten 3d voxel space into a 1d vector and independently predict brain-response at each location. Which means current approaches break the anatomical reality where voxel level response is often spatially correlated and it also means cross-subject prediction is impossible as two voxels for two different subjects aren't comparable. NRF uses positional embeddings (with learnable coefficients) to embed voxel locations and then concatenates those with learnable image (stimulus) embedding and then passes these through a learnable NN model to predict the response for a subject's given voxel to the given image. Once pretrained on a single subject, the model can then be finetuned on partial data from other subjects to predict that subject's brain response. Multiple such pretrained models can be combined using regression and finetuned on a new subject to improve this prediction even further."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The idea as well is as the presentation of the paper is very strong. This is a significant improvement over current methods of fMRI encoding. Making sure that voxel responses are anatomically correlated respects the realities of fMRI data in a way that treating voxels independently does not. It's also impressive how the implementation is fairly straightforward and as the experiments show, it works."}, "weaknesses": {"value": "One weakness is that the results are demonstrated on a single dataset. Generalization across subjects is again, within the same dataset. I think the method would become an even better sell if it can be shown to generalize on other datasets, especially across sites. As that generalizability is a hard problem in the fMRI space.\n\nThe other weakness is in the ultimate usefulness, the fact that it can generalize to a new subject is great, but that still requires some data from that subject. From a practical standpoint that means you still will need to put the subject through a scanner. If we take that as an inevitability, what we'd like to see is that the only data we need is from some sort of standardized or just from the beginning of the scan - so that the subject can then be let go after a short amount of scanning. With the current set-up, (I assume) random portions of the new participant's data are being used for finetuning. That might not be practical in reality."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1cFN9ar9dY", "forum": "wBKXuuLZbc", "replyto": "wBKXuuLZbc", "signatures": ["ICLR.cc/2026/Conference/Submission15959/Reviewer_vXEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15959/Reviewer_vXEp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645511970, "cdate": 1761645511970, "tmdate": 1762926169850, "mdate": 1762926169850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is part of a broader research effort to study voxelwise encoding models for visual stimuli. While prior vision encoding studies typically used linear mapping between image features and voxel activations of image stimuli. The current work advances the prior work by proposing a Neural Response Function (NRF) that takes both voxel location (x,y,z coordinates from a 3D volume) and image features as input, and uses an MLP(multi-layer perceptron) to predict fMRI brain activity. The authors claim that including voxel coordinates provides local spatial smoothing, capturing neighboring voxels, and an MNI space brings all subjects in common space possible for cross-subject predictions. Using the NRF approach, the evaluation focuses on comparing the individual subjects with baseline models such as linear regression and feature-weighted receptive field (fWRF) method for both brain encoding and brain decoding. Furthermore, the authors test cross-subject transfer: train on one subject and then fine-tuned on a new subject for adaptation. Overall, proposed NRF shows strong encoding and decoding performance under low-data regimes, provides an anatomically aware neural encoding model, generalizes both spatial smoothness and supports cross-subject alignment.\n\n**Contributions:**\n\n* *MNI space voxel coordinates for neural encoding:* The study uses both spatial coordinates (x,y,z) of each voxel and image stimuli representations from image encoder and use these representations in NRF to predict fMRI brain response to visual stimuli. Specifically, incorporating anatomical coordinates offers local spatial smoothing and facilitates cross-subject alignment, which is methodologically novel, relative to prior work that treats voxels as flattened 1D activations. \n* *Comprehensive evaluation:* The study evaluates proposed NRF’s brain predictive performance on individual subjects and compares it against baseline models including linear regression and fWRF. For each encoding model, the authors report correlation and MSE, for decoding, they use image retrieval metrics (e.g., SSIM, PixCorr). Further, they compare cross-subject transfer by training one subject with varying data scales and fine-tuning on a new subject, quantifying new subject adaptation as a function of training steps and dataset size, with an emphasis on low-data regimes.\n\n**Technical summary:**\nThis is primarily an empirical study, and its methodology involves the following components:\n* *Mapping spatial coordinates:* The authors define a continuous function over MNI coordinates (common space for all subjects). For each voxel coordinate r =(x,y,z), the function computes a positional encoding ϕ(r) using fourier positional encoding method (which is popular in Transformer language models). The positional encoding vector is used in the NRF function during encoding to predict fMRI response.\n* *Neural Response Function:* To train the NRF, the model takes two inputs: Image stimulus representations from image encoder and positional encoding features (ϕ(r)) for each voxel coordinate r. These vectors are concatenated and passed as input to MLP that predicts target fMRI response at spatial coordinate r. The model is trained end-to-end with a weighted sum of convex combination of mean square error and cosine similarity between the predicted response  and ground truth fMRI; optimised using Adam. \n* *Cross-subject transfer:* The authors use a two-step strategy: (i) end-to-end fine-tuning- it involves fine-tuning a pretrained NRF model on a new subject's limited data and performing end-to-end fine-tuning. (ii) Voxel-wise ensembling: In this step,  for each voxel, they make predictions from K fine-tuned models for an image by learning voxel-specific weights for each fine-tuned model.\n\n**Experimental design/evaluation:**\n* *Subject-specific encoding:* The authors evaluate the capability of NRF by training subject-specific models and compare them with two baselines: linear regression and fWRF. To probe NRF under limited data samples, they train with 20, 40, 200, 400, 600, 800 and full images and report performance across four subjects. \n* *New subject adaptation:* This analysis evaluates cross-subject transfer, where NRF trained on three subjects with 20, 200, and 800 training images, then test on the held-out fourth. For the fourth subject, the authors fine-tune the pretrained NRF and compare it to an NRF trained from scratch. They also perform voxel-wise ensembling: for each voxel, predictions from K fine-tuned models are combined using voxel-specific weights.\n* *Probing anatomical awareness:* To test whether NRF really uses spatial continuity and cross-subject anatomy, the authors perturb the coordinates: they disrupt local smoothness by shuffling voxel locations within a subject, and shifting MNI coordinates across subjects to break the alignment. Performance drops under these perturbations, indicating both spatial continuity and anatomical alignment are crucial for better NRF performance.\n\n\n**Main findings:**\nAccording to the authors’ interpretation, the main findings are as follows:\n* Under subject-specific encoding across different training data regimes, with only 200 training samples, NRF outperforms baselines trained on more than 800 images, indicating stronger performance in low-data regimes.\n* During cross-subject transfer, NRF with fine-tuning + ensemble consistently outperformed NRF scratch, indicating the anatomically conditioned formulation enables efficient cross-subject transfer with limited target data while maintaining predictive accuracy.\n* Shuffling local-spatial continuity and shifting MNI coordinates both reduces NRF performance, which implies NRF relies on local spatial continuity and cross-subject anatomical alignment. Overall, authors argue that NRF serves as a resolution-agnostic representation that can integrate data across scales and subjects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I found this work to have the following strengths:\n* *Clarity:* The manuscript introduction, dataset, and key methodological details are well written and well structured. The pipeline top panel of Figure 1 is easy to follow and clearly shows a pretrained NRF for a single subject. Later, the NRF formulation is described clearly combining an image-embedding vector with a positional encoding.  The model training and cross-subject transfer are also explained well, including how fine-tuning is performed on the pretrained NRF. The results section clearly reports subject-specific encoding performance, compares NRF with two baselines, applies the approach to cross-subject transfer, and then presents perturbation analyses that probe the proposed NRF.\n* *Originality:* The idea of using common-space MNI coordinates to build positional encodings and combine with image features for handling local-spatial smoothing in a neural encoding model to predict fMRI is a simple idea that is well known in brain decoding but remains underexplored in brain encoding. Prior brain encoding studies typically use flattened 1D brain responses and learn a subject-specific ridge-regression model. In contrast, NRF provides a cross-subject model that supports subject adaptation and outperforms baselines in low-data regimes.\n* *Significance:* This work is significant in that it contributes to a better understanding of the contribution of local-spatial smoothing and anatomical alignment are crucial for better encoding performance under low-data regimes. The comprehensive analysis on the NSD dataset shows that NRF serves as a general-purpose brain model that supports cross-subject transfer and can integrate data collected at different spatial resolutions."}, "weaknesses": {"value": "From my perspective, the primary weaknesses of this study arise from the lack of comparison with prior literature, and limited evaluation:\n* *Limited methodological novelty:* The paper claims that NRF is “the first anatomically aware encoding model to move beyond flattened voxels, learning a continuous mapping from images to brain responses in 3D space”. However, prior study, MindLLM (Qiu et al. 2025) already uses voxel coordinates with Fourier positional encoding for embedding vectors for each voxel coordinate (x,y,z). Given this, I recommend authors to soften the word “first claim” and clarify how NRF differs from prior work. Further, the authors ignore the prior study, and do not cite or compare against MindLLM.\n\nQiu et al. 2025, MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding, ICML-2025\n\n* *Lack of motivation and position:* The authors argue that flattened neural 1D response vectors make them forced to train subject-specific models and it leads to poor data efficiency. However, this claim is not fully convincing. In general, the 3D volume can be mapped onto common space (e.g., MNI or fsaverage/fsaverage5/6), enabling multi-subject models and cross-subject transfer. Further, prior work does not ignore 3D spatial information as they use localizers to map language/Visual ROI voxels [Huth et al. 2016, Deniz et al. 2019]. I suggest authors clarify why conditioning on voxel coordinates within NRF is necessary beyond these standard options, and what extra benefit it provides (ideally with a direct comparison). \n\n[Huth et al. 2016] Natural speech reveals the semantic maps that tile human cerebral cortex, Nature 2016\n\n[Deniz et al. 2019] The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality, Journal of Neuroscience 2019\n\n* *Inconsistency of evaluation metrics:* The dataset section 4.1 (line 263) states that the NRF model is trained and tested with model accuracy via R^2, while line 271 states that Pearson correlation and mean-squared error as voxel-level metrics. There are some results reported with Pearson correlation in Fig2a, Table1, and 2, while Fig2b shows prediction accuracy. Therefore it is unclear, which metrics are primary, and what they reported in results? I recommend authors to clarify the primary evaluation metric and use them consistently across Tables and Figs.\n* *No statistical significance reports:* The paper reports no statistical significance tests (e.g., bootstrap testing with FDR correction or paired permutation) to determine whether model differences are significant. Without any significance tests, the performance of NRF over baselines should be considered as descriptive rather strong empirical evidence.\n* *No standard error or standard deviation across subjects:* The results reported in Table 1-2 state that all metrics are calculated across 4 subjects, without accompanying measures of variability such as standard deviation or standard error. These metrics are critical for evaluating the robustness of the findings. \nFor a complete and detailed account of both major and minor issues, please refer to the “Questions” section."}, "questions": {"value": "I would like to thank the authors for the interesting comparison between coordinate-conditioned NRF (leveraging local spatial smoothing via voxel coordinates) and linear regression models with flattened 1D brain responses for fMRI encoding during visual stimuli. However, there are several points that I believe require further attention/work. I have divided these into major issues, which should be prioritized, and minor ones, which should be addressed for a strong version of current work.\n\n**Major Comments/Questions**\n* *Motivation and positioning:* While cross-subject transfer and multi-subject models are possible with surface-level alignment (fsaverage surface). To strengthen the claims, I recommend authors to add: (i) multisubject models in average baseline, and (ii) compare NRF model in MNI space vs. multi-subject average baseline. If NRF still outperforms the fsaverage baseline, especially in low-data and cross-subject transfer settings, that would strengthen the local spatial smoothing claim and support the conclusions.\n* *Statistical significance and clarity of Fig 2:* Fig2a reports subject-specific encoding model performance for two baselines (linear regression and fWRF) and proposed NRF. However, no statistical significance tests are provided. From Fig 2a, it is hard to conclude that NRF is significantly better than baselines. I suggest authors following\n     * Please perform paired permutation/bootstrapped tests with Benjamini–Hochberg FDR correction (across voxels or per-subject summaries), report the adjusted p-values and effect sizes, and clearly indicate which model pairs are statistically significant.\n     * Select voxels which are significant for each model and report percentage of significant voxels across subject-level summaries.\n     * Repeat above two steps  while varying the number of training images and report significant results.\n     * Also clarify what evaluation metric is reported in Fig 2b. There is an inconsistency of evaluation metrics mentioned in methodology that R^2 as accuracy and Peason correlation as voxel-level evaluation metric. \n     * Similarly, no statistical significance tests are provided for Fig 4. From Fig 4, it is hard to conclude that NRF is significantly better than ROI shifted results for EBA and V1 regions.\n* *Reporting variability:*  Please report standard deviations (or standard errors) alongside mean/median values to provide a clearer picture of variability in Table 1and 2. Further, fix the caption in each Table to state explicitly whether values are means (or medians) across the 4 subjects.\n     * Table 3 shows transfer only for Subject 7. Please run the analysis for every target subject (leave-one-subject-out), and report mean ± SD (or SEM) per target subject as well as the overall average. A full source->target transfer matrix (heatmap) with uncertainty and FDR-corrected paired tests would make the results much more robust.  \n* *Clarity on fMRI-to-Image reconstruction:*  While authors report image reconstruction results in Table 2 and 3 with semantic-level metrics, and qualitative analysis of images in Fig 3 using MindEye2, there are no details about what information exactly is fed to MindEye2 from NRF model. \n     * Please add a small description about fMRI-to-Image decoding\n     * Also, in Table 2, NRF trails fWRF and MindSimulator on decoding. If decoding is not the paper’s strength, I recommend authors to reposition decoding as a diagnostic, highlighting where NRF adds value to the paper.\n* *Why an MLP (NRF) instead of a linear/ridge model?*\n     * The authors do not provide any justification why a nonlinear MLP is necessary once after concatenation of image features + positional encodings. A ridge regression as baseline on the same inputs would test whether the gains come from nonlinear interactions or simply from adding coordinates.\n\n**Minor Comments/Typos**\nWhile addressing the following points may not be critical to the paper’s core contributions, doing so would enhance the overall quality. \n* I would appreciate authors to add little details for decoding setup.\n* Clarify bottom panel Figure 1: It is unclear why both the pretrained NRF and the fine-tuned NRF are shown for Subject 1. Please state explicitly in the caption: Need better clarification in Figure 1 caption.\n* Line 42, 46, 97: Follow proper citation format: e.g., “Downing et al. (2001) -> (Downing et al. 2001)”. Please correct all over the paper and use appropriate citation format: \\citep{} or \\cite{}. \n* Line50: 2)Subject specific -> 2) Subject specific\n* Line 103: Please add citation for this sentence: “Despite individual variability->”\n* Line 152: define abbreviation: Implicit neural representations -> Implicit neural representations (INR)\n* In Table captions, specify whether metrics are per-voxel median or mean over voxels and how subject aggregation is done.\n* Figure 2b: please add evaluation metric on colorbar\n* Since NSD provides pycortex flatmaps; please add 2D cortical flatmaps to complement Fig. 2b and make differences visible: Show absolute performance maps (e.g., voxel-wise Pearson r or R^2) for 200 vs. 800 images for each model (linear, fWRF, NRF) with the same color scale across panels. Current Fig 2b, the brain maps look similar across models.\n* Move Fig. 3 to the appendix. Align Fig. 2 and Fig. 4 layouts. Use the same panel grid, and increase the font size of legends and axis-labels.\n\n**General Advice**\nThe manuscript presents a Neural Response Function (NRF) that uses voxel coordinates for local-spatial smoothing and combining with image features to perform visual encoding on NSD dataset. Using NRF, the authors evaluate subject-specific models, low-data regimes, and cross-subject transfer. However, the current version lacks strong alignment baselines (ideally, an fsaverage/surface baseline), and shows metric inconsistencies (R^2 vs. Pearson/MSE), and reports results without any statistical significance testing. The decoding section is under-specified and NRF underperforms relative to baselines. Figures would benefit from reorganization and clearer labeling, and tables should include subject-level variability. Addressing these points and the above mentioned weaknesses and major comments would make the work stronger."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BQnGBvj0pF", "forum": "wBKXuuLZbc", "replyto": "wBKXuuLZbc", "signatures": ["ICLR.cc/2026/Conference/Submission15959/Reviewer_vmAw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15959/Reviewer_vmAw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761697067594, "cdate": 1761697067594, "tmdate": 1762926169358, "mdate": 1762926169358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Neural Response Function (NRF), a novel neural encoding model that represents fMRI activity as a continuous function over 3D anatomical space. Unlike conventional models that flatten brain data into 1D vectors, NRF is an implicit neural representation that takes a visual stimulus and a standardized MNI coordinate (x, y, z) as input to predict the response at that specific location. This anatomically-aware formulation allows the model to leverage the spatial smoothness of fMRI data and enables efficient cross-subject transfer, as the MNI coordinate system is shared across individuals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Elegant Formulation: The core contribution, modeling brain responses as a continuous, coordinate-based implicit function, is a novel and elegant departure from the standard \"grid-locked\" voxel-wise approach.\n\n2. Solves a Key Practical Problem (Low Data): The model's strongest results are in the low-data regime, where it substantially outperforms baselines. This is a highly relevant contribution, as most fMRI studies outside of massive public datasets are data-scarce. The ability to exploit spatial priors (smoothness) clearly improves data efficiency.\n\n3. Effective Cross-Subject Transfer: The paper successfully demonstrates a practical framework for cross-subject adaptation. By grounding the model in MNI space, it provides a principled way to transfer knowledge, and the finetune-ensemble strategy yields impressive performance with minimal data from a new subject.\n\n4. Strong Empirical Validation: The probing experiments are commendable. By intentionally breaking the model's assumptions (shuffling coordinates) and observing the expected performance drop, the authors provide strong evidence that the model's gains are in fact due to its leveraging of spatial smoothness and anatomical alignment."}, "weaknesses": {"value": "- Unaddressed Computational Cost: The per-coordinate query architecture seems significantly less scalable than standard single-pass encoders. The paper provides no analysis of this computational overhead or its implications for full-brain, high-resolution inference.\n\n- Incomplete Related Work Context: The paper emphasizes its cross-subject adaptation. However, the related work section overlooks that many recent neural decoding models (e.g., 'MindBridge' [1]) also incorporate encoding mechanisms and explicitly address the challenge of adapting to new subjects. The Related Work section would be more comprehensive with a discussion of these approaches."}, "questions": {"value": "1. Performance in Full-Data Regime: Why does the significant performance gap observed in the low-data regime (Fig 2a) largely disappear in the full-data setting (Table 1)? Does the strong spatial-smoothness prior perhaps act as a regularizer that helps when data is scarce but hinders the model from capturing fine-grained, non-smooth voxel-specific details when data is abundant?\n\n2. Reliability of Semantic Metrics: Given that the fWRF baseline outperforms the \"Measured fMRI\" ground truth on several semantic metrics, how should we interpret these results? Does this not suggest the MindEye2 decoder is (over)fitted to fWRF-like distributions, making this entire family of metrics unsuitable for comparing these encoders?\n\nI will consider raising my score if all my concerns are well addressed by the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "odIC4Z4KXX", "forum": "wBKXuuLZbc", "replyto": "wBKXuuLZbc", "signatures": ["ICLR.cc/2026/Conference/Submission15959/Reviewer_mipv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15959/Reviewer_mipv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992301302, "cdate": 1761992301302, "tmdate": 1762926168883, "mdate": 1762926168883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}