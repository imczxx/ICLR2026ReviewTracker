{"id": "I1TBgjF9FU", "number": 1331, "cdate": 1756872132409, "mdate": 1759898214725, "content": {"title": "A Dimensional Analysis of Video Anomaly Detection Benchmarks", "abstract": "Benchmark datasets have fueled advances in video anomaly detection, yet they often embed hidden assumptions that distort both research focus and real-world applicability. Common benchmarks implicitly assume that anomalies are human-centric, visually salient, short-lived, and unambiguous to label, while neglecting object-driven, contextual, long-term, or ethically sensitive events. To expose and systematize these biases, we conduct the first dimensional analysis of anomaly detection benchmarks. Our framework organizes dataset design along four principled axes: content (e.g., taxonomy, motion, modality), annotation (e.g., density, human involvement, consistency), distribution (e.g., frequency, diversity, temporal extent), and societal impact (e.g., privacy, fairness). Applying this framework, we uncover structural imbalances: most benchmarks overrepresent conspicuous human anomalies while underrepresenting subtle or multimodal patterns, along with inconsistent annotation protocols and skewed anomaly distributions that confound fair evaluation. These design choices restrict the diversity of learnable patterns, bias algorithmic search spaces, and limit the operational robustness of deployed systems. We consolidate our findings into actionable guidelines for next-generation benchmarks that broaden anomaly coverage, enable reproducible evaluation, and embed social responsibility into dataset design. By reframing benchmarks through a dimensional lens, this work lays the foundation for more generalizable, equitable, and trustworthy video anomaly detection.", "tldr": "", "keywords": ["video anomaly detection", "benchmark", "dataset", "analysis", "comparison", "discussion"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56cfb60b1039e9658f42dc19acf682a07f6e0b39.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a principled framework for analyzing Video Anomaly Detection (VAD) benchmarks by formalizing datasets as seven-dimensional profiles. It introduces structural metrics—Coverage and Balance—and diagnostic indicators to quantify dataset bias and evaluation stability.  The work offers concrete guidelines for more reproducible and socially responsible evaluation, advocating for balanced profiles and standardized reporting of stability metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well organized, and analysis of VLA benchmark is interesting."}, "weaknesses": {"value": "1, after analysis, the authors think a benchmark should hold good coverage and balance, which is a widely acknowledged conclusion. Besides, the author also did not compare the reliability of Coverage and balance indicator with some basic statistics information to access the dataset's coverage and balance.\n\n2, It seems that the proposed indicators can also be applied to the datasets beyond VAM,  nothing special designs for VAM benchmarks.\n\n3,  In conclusion, the authors advices the benchmark with good coverage and balance, but also recommend to include long-tail data for sturctural coverage, is that conflict with the balance?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6liSMn9GBj", "forum": "I1TBgjF9FU", "replyto": "I1TBgjF9FU", "signatures": ["ICLR.cc/2026/Conference/Submission1331/Reviewer_Mk9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1331/Reviewer_Mk9x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761212382933, "cdate": 1761212382933, "tmdate": 1762915739102, "mdate": 1762915739102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes existing video anomaly detection datasets, attempting to quantitatively characterize datasets along seven different attributes and four proposed axes. Comprehensive statistical analysis is performed to identify correlations between datasets. The authors propose a set of guidelines for evaluating and reporting results in future video anomaly detection works."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Many different types of statistical analyses are performed, the experiments are comprehensive in this regard.\n2. The analysis on benchmark discriminative ability seems like a useful method to help characterize datasets. Coverage and balance are also useful for describing the datasets."}, "weaknesses": {"value": "1. The overall direction and story are quite weak. The idea to better characterize anomaly detection datasets is good, but the overall execution here is confusing and not very actionable. Out of a paper like this, I would expect a concise, profound takeaway for the field moving forward. Instead, we are left with weak guidelines loosely summarized as \"show results on more datasets\" and \"create benchmarks with more types of anomalies and modalities\".\n2. While the characterization attempts are comprehensive, it is difficult to distill meaningful information from them. Each paragraph essentially reports the metrics shown in the paired figure/graph then provides a one-two sentence plausible interpretation of the metrics. Some of these interpretations are based on an incorrect reading of the results (Lines 314-318, label density/distribution co-occurrence is actually 0.46), making trusting the explanations difficult.\n3. The paper is missing a robust description of the different types of video anomaly detection datasets. Correlations defining families are only identified after running the analysis, though some of this can be done without quantified analysis. For example, there is no formal discussion of unsupervised vs. weakly supervised datasets. I suspect readers without comprehensive knowledge of the existing benchmarks would have a very difficult time understanding much of the discussion.\n4. Many of the important figures are difficult to distill. For example, in figure one, interpreting a-c takes a lot of back and forth with the text and doesn't convey much insight about the benchmark distributions. This should be a high-impact figure that clearly highlights the important findings of the work.\n5. The figure 1 and appendix modality classes are different. RGB vs. RGB + optical flow is a method design choice and is independent of the benchmark itself."}, "questions": {"value": "1. Why have categories such as low human involvement, RGB+Audio+Text, and low motion complexity if there are no datasets that are in those categories.\n2. In figure one, what is the meaning of the dendrograms? They are difficult to follow and don't really seem to mean much."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dE07gmOwAk", "forum": "I1TBgjF9FU", "replyto": "I1TBgjF9FU", "signatures": ["ICLR.cc/2026/Conference/Submission1331/Reviewer_5Z2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1331/Reviewer_5Z2f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496877718, "cdate": 1761496877718, "tmdate": 1762915738938, "mdate": 1762915738938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic dimensional analysis of 18 Video Anomaly Detection (VAD) benchmarks. It argues that existing datasets contain implicit biases (e.g., being human-centric, RGB-only) that distort research focus and limit real-world applicability. The authors introduce a 7-dimension framework and two novel, model-agnostic metrics—Coverage and Balance—to quantify dataset structural properties. The analysis reveals systemic biases across the VAD ecosystem and identifies structural \"outliers\" (e.g., CUVA, UBnormal) valuable for testing generalization. Critically, the work links these structural properties to evaluation instability, showing that low \"Balance\" correlates with high cross-model variance and inconsistent leaderboard rankings. It concludes with actionable guidelines for reporting dataset volatility metrics ($\\sigma_{auc}$, $\\overline{\\tau}$) alongside standard performance scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This work introduces a novel 7-dimension framework (e.g., taxonomy, modality, privacy risk) that provides a model-agnostic language for quantifying dataset properties beyond simple size or domain metrics. The resulting metrics, Coverage and Balance, offer a new, quantitative method for dataset selection and for integrating ethical considerations (like the \"Privacy Risk\" dimension) directly into the analysis.\n\n2.The paper provides clear, quantitative evidence of systemic biases in VAD, such as the over-reliance on RGB-only data. The co-occurrence analysis (Fig. 3, Table 4) is highly insightful, uncovering potential model \"shortcuts,\" like the strong correlation between \"taxonomy level\" and \"human involvement\". The identification of \"outlier\" and \"bridge\" datasets (Fig. 5) provides an immediate, actionable guide for stress-testing model generalization.\n\n3.The paper links low structural \"Balance\" to high cross-model score variance (Fig. 4b), providing a structural reason for unstable leaderboards. Furthermore, the analysis of \"discriminative power\" ($\\sigma_{auc}$, Fig. 6) and rank stability (Kendall's $\\tau$, Fig. 7) gives researchers practical tools to identify which benchmarks are most effective for separating models (e.g., ShanghaiTech) and which produce unstable rankings (e.g., CUHK Avenue)."}, "weaknesses": {"value": "1.The 7-dimension framework relies on ordinal scales with qualitative boundaries (e.g., \"medium\" vs \"high\" motion complexity) that are subject to annotator interpretation. The paper states two annotators were used but critically fails to report any inter-annotator agreement (IAA) metrics (e.g., Krippendorff's Alpha). This omission makes it difficult to assess the reliability of the foundational data in Table 2.\n\n2.The core empirical validation in Sec 5 is based on a small sample of only four baseline models (RTFM, MGFN, TEVAD, EGO). This small set may not capture the diversity of modern architectures. This is compounded by using only six datasets with several missing data points, which reduces the statistical power and confidence in the correlation analyses presented in Fig. 4 and Fig. 7.\n\n3.The \"Coverage\" and \"Balance\" indicators are presented as heuristic constructions. The paper does not theoretically justify the choice of these specific metrics (e.g., Shannon entropy for \"Balance\") over other established measures of diversity or dispersion (e.g., Gini impurity, statistical variance, etc.). Furthermore, there is no sensitivity analysis provided. It is unclear how robust these metrics are to changes in the framework, such as the number of dimensions used or the discrete number of levels (e.g., 4 vs. 5) chosen for each dimension.\n\n4.The paper's excellent proposal that future work report the $(\\sigma_{auc}, \\overline{\\tau}, n)$ tuple lacks a key component for standardization. The metrics $\\sigma_{auc}$ and $\\overline{\\tau}$ are highly dependent on the set of baseline models ($n$) used in the calculation. Because the paper does not propose a \"standard basket\" of 3-5 representative baselines, these metrics will not be comparable across different papers, as each paper may choose a different $n$. This makes the recommendation difficult to adopt in a standardized way."}, "questions": {"value": "1. Could the authors please address the reliability of their scoring? Specifically, (a) what was the inter-annotator agreement (IAA) score (e.g., Krippendorff's Alpha) for the dataset-by-dimension matrix (Table 2)? and (b) how were qualitative boundaries like \"medium\" vs \"high\" motion complexity consistently adjudicated?\n\n2. Given the small and incomplete sample (n=4 models, n=6 datasets, with missing values), how confident are the authors in the generality of the correlations found in Fig. 4 and Fig. 7? How might these results change if more diverse, modern architectures were included?\n\n3. Regarding the \"Balance\" metric, could the authors comment on why normalized Shannon entropy was chosen over other potential measures of statistical dispersion (like Gini impurity or variance)? Was any sensitivity analysis performed on the robustness of these metrics?\n\n4. For the proposed $(\\sigma_{auc}, \\overline{\\tau}, n)$ tuple to be a comparable metric across papers, the set of baseline models ($n$) would need to be standardized. Can the authors explain and analyze what would constitute a minimal, representative 'basket' of models to serve this purpose? This analysis is not specified in the manuscript but is crucial for the proposal's practical adoption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "92soD2p6ZA", "forum": "I1TBgjF9FU", "replyto": "I1TBgjF9FU", "signatures": ["ICLR.cc/2026/Conference/Submission1331/Reviewer_JAUd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1331/Reviewer_JAUd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707468433, "cdate": 1761707468433, "tmdate": 1762915738663, "mdate": 1762915738663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research, Proposes a 7-dimension profiling framework for VAD datasets (taxonomy, human involvement, modality, motion complexity, label density, distribution, privacy). Introduces Coverage (breadth of levels used) and Balance (normalized entropy of level usage) as dataset-level indicators. Evaluates 18 datasets, visualizing structure (matrix + t-SNE) and identifying dataset “families,” outliers, and bridges. Finds strong RGB/pedestrian bias, limited multimodality, sparse temporal labels, and single-location concentration. Links dataset structure to evaluation stability via AUC variance and mean Kendall’s ; recommends reporting these alongside headline AUC/AP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Turns qualitative dataset critique into auditable, reproducible profiles.\n2. Clear reporting checklist suggestions, highlights outlier/bridge sets for testing generalization.\n3. Matrix, clusters, and metadata make comparisons intuitive, and Framework applies regardless of the underlying VAD approach."}, "weaknesses": {"value": "1. Mapping datasets to ordinal levels relies on human judgment, and limited detail on inter-annotator agreement.\n2. Scope limits: Volatility analysis uses a small set of baselines/datasets—generalization of correlations may be brittle.\n3. Granularity gaps: Some dimensions (e.g., “distribution,” “privacy”) can be coarse and context-dependent."}, "questions": {"value": "1. Can you release the scoring rubric with inter-annotator reliability (e.g., κ) and versioned logs?\n2. How do Coverage/Balance and volatility metrics behave with more (and newer) baselines, including multimodal/foundation models?\n3. Could some dimensions be automated (e.g., modality checks, motion statistics) to reduce subjectivity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DBKIqWFbGg", "forum": "I1TBgjF9FU", "replyto": "I1TBgjF9FU", "signatures": ["ICLR.cc/2026/Conference/Submission1331/Reviewer_c5Um"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1331/Reviewer_c5Um"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895845299, "cdate": 1761895845299, "tmdate": 1762915738450, "mdate": 1762915738450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}