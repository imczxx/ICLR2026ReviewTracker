{"id": "sCYQPqiBrF", "number": 21680, "cdate": 1758320486863, "mdate": 1763590635549, "content": {"title": "Conformalized Predictions in Hypergraph Neural Networks via Contrastive Learning", "abstract": "Hypergraph representation learning has gained immense popularity over the last few years due to its applications in real-world domains like social network analysis, recommendation systems, biological network modeling, and knowledge graphs. However, hypergraph neural networks (HGNNs) lack rigorous uncertainty estimates, which limits their deployment in critical applications where the reliability of predictions is crucial. To bridge this gap, we propose Contrastive Conformal HGNN (CCF-HGNN) that jointly accounts for aleatoric and epistemic uncertainties in hypergraph-based models for guaranteed and robust uncertainty estimates. CCF-HGNN accounts for epistemic uncertainty in HGNN predictions by producing a prediction set/interval that leverages the topological structure and provably contains the true label with a pre-defined coverage probability, while accounting for aleatoric uncertainty that originates from augmentations on the structure of the hypergraph. To enhance the power of the predictions, CCF-HGNN performs an additional auxiliary task of hyperedge degree prediction with an end-to-end differentiable sampling-based approach. Extensive experiments on real-world hypergraph datasets demonstrate the superiority of CCF-HGNN by improving the efficiency of prediction sets while maintaining valid coverage.", "tldr": "Uncertainty Quantification by jointly minimizing contrastive augmentations and conformal prediction loss on hypergraphs", "keywords": ["Hypergraph Neural Networks", "Uncertainty Quantification", "Contrastive Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa9847bfc58da1240d759a95d4bb0c4d16058e2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces two methods for conformal prediction on hypergraph neural networks (HGNNs): CCF-HGNN and CF-HGNN. As noted by the authors, CF-HGNN is a naive extension of CF-GNN (Huang et al., 2024) for hypergraphs, which they include as a baseline for CCF-HGNN. CCF-HGNN employs two additional mechanisms—contrastive augmentations and a loss and degree prediction head—to account for aleatoric uncertainty.\n\nWhile CF-HGNN is not part of the authors' main contribution, I believe it is still a valuable contribution, as it gives another CP method for HGNNs that achieves coverage requirements and is likely more scalable (see Weaknesses)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper deals with an important and relevant area, which is uncertainty quantification for HGNNs\n- The paper accounts for **both** aleatoric and epistemic uncertainty, which is missing in prior (graph) CP literature\n- Both methods achieve the desired coverage guarantees and outperform baselines"}, "weaknesses": {"value": "- One limitation of CF-GNN compared to other graph CP methods (e.g., DAPS, NAPS), it is a lot more computationally expensive (as you are training a separate conformal model). Could you discuss how CF-HGNN and CCF-HGNN scale\n- Two of the four datasets used were for binary classification. While CP still works, it is less meaningful when the only set options are nothing, either a 0/1 label, or everything. It would be nice to have seen more multi-class datasets. There seem to be several datasets here (under Hypergraphs with labeled nodes): https://www.cs.cornell.edu/~arb/data/"}, "questions": {"value": "- In Assumption 1, do you mean to say permute calibration and test *nodes*, rather than *edges*? Should L165 end with be (..., $\\mathcal{V} _ {\\pi}$, \n$\\mathcal{E} _ {\\pi}$) rather than (..., \n$\\mathcal{V}$, \n$\\mathcal{V} _ {\\pi}$)\n- For your Bounded Coverage assumption for Theorem 1, do you have any examples of contrastive augmentations that would violate this assumption? It would be nice to include those in the manuscript\n----\n- Nit: Algorithm 1 has some abuse of notation ($\\alpha$ is attention and significance level), which makes it a bit confusing"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zToUsZpbyq", "forum": "sCYQPqiBrF", "replyto": "sCYQPqiBrF", "signatures": ["ICLR.cc/2026/Conference/Submission21680/Reviewer_GCfZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21680/Reviewer_GCfZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327686457, "cdate": 1761327686457, "tmdate": 1762941887710, "mdate": 1762941887710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper works on uncertainty quantification of hypergraph neural networks. The paper proposes a Contrastive Conformal\nHGNN (CCF-HGNN) that jointly accounts for aleatoric and epistemic uncertainties in hypergraph-based models for guaranteed and robust uncertainty estimates. Compared with previous methods, it accounts for aleatoric uncertainty by leveraging contrastive learning on the structure of the hypergraph. The authors provide extensive experiments to show the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The challenges of uncertainty quantification on hypergraph neural networks are clearly shown and explained. And the proposed method appropriately addresses all the listed challenges.\n2. Theoretical and quantitative analyses are provided to show the rigorousness of the work.\n3. The paper is well written and organized."}, "weaknesses": {"value": "1. This paper argues that aleatoric uncertainty is not covered in the conformalized methods. This confuses me, as based on my understanding, the epistemic and aleatoric uncertainty have already been covered in the conformal prediction. Please explain why the CF-GNN or other conformal prediction methods do not cover aleatoric uncertainty.\n\n2. The baselines in this paper are not enough. The authors argue that structural high-order information is considered to address the challenges in uncertainty quantification of hypergraph neural networks. Then, methods such as [1] should also be included as a competitive baseline that have already considered high-order structural information.\n\n3. What is the backbone model used in the experiments? I understand that this paper focuses on the uncertainty of hypergraph neural networks. But will different hypergraph models have different influences on the results?\n\n[1] Soroush H Zargarbashi, Simone Antonelli, and Aleksandar Bojchevski. 2023. Conformal prediction sets for graph neural networks. In the International Conference on Machine Learning. PMLR, 12292–12318."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sXGR4k3zVO", "forum": "sCYQPqiBrF", "replyto": "sCYQPqiBrF", "signatures": ["ICLR.cc/2026/Conference/Submission21680/Reviewer_WUvR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21680/Reviewer_WUvR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578050594, "cdate": 1761578050594, "tmdate": 1762941887477, "mdate": 1762941887477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles how to give reliable uncertainty for hypergraph neural networks. The authors propose CCF-HGNN, which combines two ideas: (1) conformal prediction (APS/RAPS) to guarantee that the true label is covered, plus a topology-aware, differentiable training trick to make the prediction sets as small as possible; and (2) contrastive learning with simple structural augmentations (dropping hyperedges or edges) so the model learns representations that are robust to noisy structure (handling data uncertainty). They also add a lightweight auxiliary task that predicts hyperedge degrees, using attention and differentiable top‑k selection to focus on the most informative hyperedges, which further sharpens the predictions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is important and timely: uncertainty quantification for HGNNs is underexplored yet crucial for reliable deployment.\n\n2. The proposed method appears reliable, with sound design choices (conformal calibration, contrastive robustness, auxiliary structural task) and both theoretical and empirical support.\n\n3. The paper clearly attributes techniques to prior work, allowing readers to trace components (e.g., conformal prediction, contrastive learning, attention, Gumbel-Softmax) to their sources."}, "weaknesses": {"value": "The method is largely a composition of existing techniques, as the authors themselves acknowledge (combining aleatoric via contrastive augmentation and epistemic via conformal prediction). The conformal setup closely follows Huang et al. (2024), the contrastive augmentations draw from Wei et al. (2022), and the auxiliary degree prediction leverages standard attention and differentiable top-k sampling. While the integration is well-executed, the incremental novelty—particularly relative to Huang et al. (2024) on the conformal side—feels modest. I am less familiar with hypergraph-specific precedents, but within conformal prediction the step beyond prior art seems mild."}, "questions": {"value": "In Table 2, APS coverage appears substantially above the target 0.95 across multiple datasets. While APS can be conservative in practice, the magnitude and consistency of the overshoot suggest a potential calibration or implementation issue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E1fkXiEj2E", "forum": "sCYQPqiBrF", "replyto": "sCYQPqiBrF", "signatures": ["ICLR.cc/2026/Conference/Submission21680/Reviewer_jvNm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21680/Reviewer_jvNm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907870048, "cdate": 1761907870048, "tmdate": 1762941886969, "mdate": 1762941886969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Responses to All Reviewers"}, "comment": {"value": "We would like to thank the reviewers for taking the time to read our paper and pose valuable questions and critiques. We have tried to address the questions asked in our revised version. The summary of our revisions is as follows:\n\n- Fixed Notation /text ambiguities (Assumption 1 and Algorithm 1).\n\n- Moved Sensitivity Study 3 from the Appendix to the Main Text\n\n- Added the performance of Multi-Class Hypergraph Datasets and added descriptions of 2 new datasets in Appendix A.5.\n\n- Added results of DAPS in the Appendix A.8.\n\n- Revised the Proof of Lemma 2 in Appendix A.2 for clarity.\n\n- Added results of ED-HNN in Appendix A.9.\n\n- Added scalability results in Appendix A. 10.\n\n- Added examples of violation of Assumption 1 in Theorem 2 in Appendix A.11.\n\n---\n\nWe hope that this addressed the concerns of all the reviewers. Please let us know if we can improve the quality of the manuscript further and let us know if you have further concerns we can address.\n\nRegards,\n\nAuthors of Submission 21680."}}, "id": "V9BSh4ubnj", "forum": "sCYQPqiBrF", "replyto": "sCYQPqiBrF", "signatures": ["ICLR.cc/2026/Conference/Submission21680/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21680/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission21680/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763593992944, "cdate": 1763593992944, "tmdate": 1763593992944, "mdate": 1763593992944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}