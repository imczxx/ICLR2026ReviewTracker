{"id": "SHRNQHueMm", "number": 18402, "cdate": 1758287250527, "mdate": 1759897105645, "content": {"title": "BEYOND IMITATION: RECOVERING DENSE REWARDS FROM DEMONSTRATIONS", "abstract": "Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation learning process that only trains a policy to imitate expert behavior on demonstration datasets. In this work, we challenge this view by establishing a fundamental equivalence between SFT and Inverse Reinforcement Learning. We prove that the SFT objective is a special case of Inverse Q-Learning, which implies that the SFT process does not just learn a policy, but also an implicit, dense, token-level reward model that explains the expert demonstrations. We then show how to recover this dense reward signal directly from the SFT model by formulating a baseline-relative reward function. The availability of such a dense reward model offers numerous benefits, providing granular credit assignment for each token generated. We demonstrate one key application by using these recovered rewards to further improve the policy with reinforcement learning. Our method, Dense-Path REINFORCE, consistently outperforms the original SFT models on instruction-following benchmarks. This work reframes SFT not merely as policy imitation but as a powerful reward learning mechanism, opening new possibilities for leveraging expert demonstrations.", "tldr": "We prove that SFT is equivalent to IRL, which allows us to extract a dense, token-level reward signal directly from an SFT model and use it to significantly improve the model's performance through further fine-tuning.", "keywords": ["LLM", "Reinforcement Learning", "SFT", "Inverse Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab53424f730509f2f7708741e61bc6d62d6d14a2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes that supervised fine-tuning (SFT) can be viewed as a special case of inverse soft-Q learning under a deterministic token MDP with γ = 1 and a linear conjugate. Building on this, it defines a baseline-relative dense reward and introduces Dense-Path REINFORCE (DPR), a token-level REINFORCE update using log-probability differences between the SFT model and an earlier checkpoint as reward signals. Experiments on AlpacaEval, MT-Bench, LIMA, and Arena-Hard show modest but consistent gains over SFT and competitive results with SPIN and GSIL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Theoretical link between SFT and inverse RL is clear and well-motivated.\n\n2. The telescoping argument makes the equivalence intuitive under γ = 1.\n\n3. DPR is simple, reproducible, and the dense reward idea is easy to understand.\n\n4. Ablations on γ, checkpoint choice, and baseline removal are informative and thoughtfully designed."}, "weaknesses": {"value": "1. Narrow validity: The equivalence holds only for γ = 1, deterministic token transitions, and a linear conjugate. The paper does test γ < 1, showing expected degradation, which confirms the limitation rather than extending the theory.\n\n2. Inconsistent assumptions: The equivalence relies on a linear conjugate, while the later stability theorem assumes strong convexity. These are mathematically incompatible but presented as part of one framework.\n\n3. Evaluation design: DPR is trained and tested on the same prompt set, so gains could reflect continued fine-tuning instead of genuine reward recovery.\n\n4. Limited robustness: The temperature ablation is minimal and doesn’t analyze stochastic effects or variance.\n\n5. Weak empirical evidence: All evaluations rely on GPT-4 judges without error bars, human checks, or multiple seeds; gains over SFT are small (a few percent).\n\n6. Missing controls: No baseline comparing DPR to simply extending SFT training, making attribution unclear."}, "questions": {"value": "1. Can the same ψ be both linear (for equivalence) and strongly convex (for contraction)?\n\n2. Why does the halfway checkpoint consistently give the best result?\n\n3. Did you test multiple seeds or new prompts to confirm robustness?\n\n4. Could a continued-SFT baseline match the reported gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "re2IMk2kEz", "forum": "SHRNQHueMm", "replyto": "SHRNQHueMm", "signatures": ["ICLR.cc/2026/Conference/Submission18402/Reviewer_rgwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18402/Reviewer_rgwj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943456980, "cdate": 1761943456980, "tmdate": 1762928107051, "mdate": 1762928107051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Concerns Regarding the Characterization and Novelty Claims in Relation to Prior Work (Li et al., 2025)"}, "comment": {"value": "Dear Authors and Reviewers,\n\nWe are writing as the authors of \"Li et al. (2025)\" [arxiv link](https://arxiv.org/abs/2506.23235) to address several critical inaccuracies in how our work is presented and contextualized in this manuscript. While we appreciate the citation, the current description fundamentally misrepresents our contributions and, as a result, inflates the novelty of the present submission.\n\nWe wish to correct the record on three major points:\n\n**1. Mischaracterization of Our Reward Formulation:**\n\nThe manuscript claims that our work's \"focus is to extract sentence-level rewards.\" This is factually incorrect. Our paper explicitly introduces and develops a **token-level dense reward**. The core formulation presented in this manuscript is not merely similar to ours; it is identical. Specifically:\n*   **Equation (4)** in this manuscript is a direct replication of **Equation (11)** in our paper (Li et al., 2025).\n*   The subsequent Monte-Carlo estimation of returns described below the Equation (4) is precisely what we formulated in our **Equation (12)**.\n\nThe claim that our work is limited to \"sentence-level rewards\" while this submission develops a \"shaping- and baseline-based reward construction\" is a severe misrepresentation. Our work already established this token-level dense reward mechanism.\n\n**2. The Claimed \"Main Contribution\" is Prior Work:**\n\nThe manuscript highlights the \"SFT ≡ IQ-Learn equivalence at the token level\" as a main contribution. This equivalence is the central theoretical pillar of our paper. We formally stated, proved, and discussed this exact concept in **Proposition 1** of our work. Therefore, this is not a novel finding of this manuscript but a core result that was previously established by Li et al. (2025).\n\n**3. Inappropriate Use of \"Concurrent Work\":**\n\nFinally, we must contest the labeling of our paper as a \"concurrent effort.\" Our work was made publicly available on arXiv on **June 29, 2025**. The ICLR 2026 submission deadline was **September 24, 2025**. The nearly three-month gap between our public disclosure and this submission deadline provides more than sufficient time for our paper to be considered **prior art**, not concurrent work. The term \"concurrent\" is typically reserved for works developed and released in a much narrower, overlapping timeframe, which is not the case here. This classification minimizes the foundational nature of our contribution to the ideas presented in this submission.\n\nIn summary, this manuscript inaccurately describes our technical contributions, claims novelty for a core theoretical result we had previously published, and improperly designates our work as \"concurrent.\"\n\nWe strongly urge the authors to issue a significant revision of the paper to accurately represent our work and properly acknowledge its contributions. We also respectfully ask the reviewers to take these points into careful consideration when evaluating the novelty and intellectual merit of this submission.\n\nSincerely,\n\nAuthors of (Li et al., 2025)"}}, "id": "4Yrb2NmVAA", "forum": "SHRNQHueMm", "replyto": "SHRNQHueMm", "signatures": ["~Yi-Chen_Li1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Yi-Chen_Li1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18402/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763298897918, "cdate": 1763298897918, "tmdate": 1763298969204, "mdate": 1763298969204, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes SFT under a token-MDP view (γ≈1) and shows that the SFT objective is equivalent to an inverse soft-Q objective. This motivates extracting a dense proxy reward as a log-likelihood ratio between a final SFT model and a reference checkpoint, followed by a short, critic-free RL step (REINFORCE with KL to SFT). The method is closed-loop (no environment reward, no preference data, no reward model), simple to implement, and reports consistent gains over plain SFT across several backbones/evals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Dense credit assignment: Per-token signals are actionable and address SFT’s plateau on long sequences and intermediate steps.\n\nLow engineering overhead: Uses existing SFT artifacts (final SFT + ref checkpoint). No external judges or reward models.\n\nTheory ↔ practice alignment: The SFT ↔ inverse-soft-Q connection and the safe-improvement style argument justify using the recovered dense signal for a small policy-gradient step.\n\nStable optimizer: REINFORCE + KL to SFT (no critic) keeps the pipeline robust and easy to reproduce."}, "weaknesses": {"value": "Reference sensitivity: The approach assumes π_SFT is meaningfully better than π_ref. If ref is too weak (noisy signal) or too strong (vanishing signal), the log-ratio reward becomes brittle or tiny.\n\nLog-ratio gaming: The policy may drift toward stylistic artifacts that inflate log π_SFT − log π_ref without improving task quality.\n\nDomain narrowness: Because SFT and ref share training data, the proxy reward is intrinsically domain-tied; out-of-domain generalization of the dense signal is unclear.\n\nEval reliance: Gains are primarily shown via LLM-as-judge; stronger human or task-grounded metrics would strengthen the case.\n\nminor\nS1. Reference selection by evaluation, not step count:\n– Choose π_ref via validation metrics (MT-Bench, small human slice, task-specific set) to target the “elbow” where the signal-to-noise of the log-ratio is highest.\n– Alternatively use an EMA of SFT weights as π_ref to smooth noise.\n\nS2. Multi-reference ensemble:\n– Define log π̄_ref = logsumexp_i(log π_ref,i) − log k (geometric mean). Reward becomes r̂ = log π_SFT − log π̄_ref. This damps idiosyncrasies of any single checkpoint and makes “progress” less gameable.\n\nS3. Dual-KL regularization and reward shaping to reduce gaming:\n– Keep KL(π_θ || π_SFT) and add a small KL(π_θ || π_ref).\n– Clip/normalize the per-token log-ratio (e.g., cap magnitude or z-score by position).\n– Penalize tokens where both π_SFT and π_ref assign low probability (both uncertain) even if the difference is large.\n– Add light style/fluency guards (repetition rate, perplexity bounds under a separate LM).\n\nS4. Correlation-gated updates:\n– On each batch, compute the correlation between r̂-improvement and a cheap proxy (exact-match on small QA, code unit tests, math verifier). If correlation drops below a threshold, reduce step size or increase KL. This is a simple “reward sanity check.”\n\nS5. Leverage the re-forward pass to incorporate useful rewards (when available), without changing the core method:\n– Hybrid reward: use R = α·r̂ + (1−α)·r_ext, where r_ext can be any lightweight verifier signal (unit tests for code, arithmetic checker, safety filter, format validator). α can be annealed from 1→0.8.\n– Doubly-robust/token-aware AWR: advantage-weight the SFT tokens by r̂ (and r_ext if present), i.e., reweight the teacher-forced loss with w_t = exp(β·A_t) to unify imitation and RL in one pass.\n– Counterfactual filtering: when the re-forward reveals contradictory beams (both low confidence), zero out r̂ for those tokens to avoid amplifying noise.\n\nS6. Report a reference sweep and ablations:\n– Show downstream metrics vs. ref placement (early/mid/late) to directly address “is π_SFT actually better than π_ref?”\n– Include ablations for single-ref vs. multi-ref, with/without dual-KL, and with/without correlation gate."}, "questions": {"value": "How is the reference checkpoint chosen? Is it purely by training step or by validation metrics? What is the sensitivity curve (early/mid/late)?\nQ2. Can an ensemble of references reduce variance/bias? (Geometric mean of checkpoints as a smoother ref.)\nQ3. How do you detect or mitigate reward-hacking (odd outputs that maximize the log-ratio)?\nQ4. What is the robustness out of domain (math/code/safety) where SFT confidence calibration differs?\nQ5. Since the method already performs fresh forward passes, can those passes be leveraged to incorporate additional rewards or verifiers when available (see Suggestions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mpZT8oI0qE", "forum": "SHRNQHueMm", "replyto": "SHRNQHueMm", "signatures": ["ICLR.cc/2026/Conference/Submission18402/Reviewer_Y4qp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18402/Reviewer_Y4qp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969146118, "cdate": 1761969146118, "tmdate": 1762928106659, "mdate": 1762928106659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel idea that Supervised Fine-Tuning (SFT) can be viewed as a special case of Inverse Q-Learning, suggesting that SFT does not merely imitate expert policies but implicitly learns a dense, token-level reward model. The authors then recover this implicit reward from the SFT model and propose Dense-Path REINFORCE (DPR), which leverages the recovered reward to more efficiently optimize large language models (LLMs). Experimental results demonstrate that the proposed method outperforms standard SFT across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel idea that Supervised Fine-Tuning (SFT) can be viewed as a special case of Inverse Q-Learning, offering a new perspective for understanding SFT.\n\n2. The authors provide a comprehensive theoretical analysis to support the proposed formulation.\n\n3. Experimental results show considerable improvements over traditional large language model (LLM) training methods."}, "weaknesses": {"value": "1. In the theoretical analysis, the authors make several strong assumptions about the setting—for example, assuming a deterministic token sequence and a fixed discount factor of $\\gamma=1$, rather than a value smaller than 1 as typically used in RL.\n\n2. In the proposed DPR method, the reference policy $\\pi_\\text{ref}$ is not formally defined. The authors state that it is an SFT checkpoint trained with half of the training samples; however, if the dataset is sufficiently large, wouldn’t this reference policy also become fully trained, thereby reducing the meaningful difference between $\\pi_\\text{ref}$ and $\\pi_\\text{SFT}$?"}, "questions": {"value": "1. The reviewer is not an expert in LLMs, but I question whether the current training paradigm of LLMs is primarily driven by SFT. Wouldn’t RLHF have a greater overall impact on model alignment and performance? I therefore have some concerns about the potential contribution and significance of this work to the broader LLM literature.\n\n2. If SFT is theoretically equivalent to IQL, would it be possible to directly apply IQL methods to learn the reward function instead of recovering the reward from SFT?\n\n3. The authors choose REINFORCE for policy optimization. Could the authors clarify why this choice was made instead of using more advanced RL algorithms, such as actor–critic or PPO-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2mO4khnNhT", "forum": "SHRNQHueMm", "replyto": "SHRNQHueMm", "signatures": ["ICLR.cc/2026/Conference/Submission18402/Reviewer_HC6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18402/Reviewer_HC6d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981248345, "cdate": 1761981248345, "tmdate": 1762928106316, "mdate": 1762928106316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}