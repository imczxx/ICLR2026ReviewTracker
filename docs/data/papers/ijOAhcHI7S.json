{"id": "ijOAhcHI7S", "number": 18616, "cdate": 1758289503573, "mdate": 1759897091531, "content": {"title": "MSCR: Exploring the Vulnerability of LLMs’ Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement", "abstract": "LLMs demonstrate performance comparable to human abilities in complex tasks such as mathematical reasoning, but their robustness in mathematical reasoning under minor input perturbations still lacks systematic investigation. Existing methods generally suffer from limited scalability, weak semantic preservation, and high costs. Therefore, we propose MSCR, an automated adversarial attack method based on multi-source candidate replacement. By combining three information sources including cosine similarity in the embedding space of LLMs, the WordNet dictionary, and contextual predictions from a masked language model, we generate for each word in the input question a set of semantically similar candidates, which are then filtered and substituted one by one to carry out the attack. We conduct large-scale experiments on LLMs using the GSM8K and MATH500 benchmarks. The results show that even a slight perturbation involving only a single word can significantly reduce the accuracy of all models, with the maximum drop reaching 49.89\\% on GSM8K and 35.40\\% on MATH500, _while preserving the high semantic consistency of the perturbed questions._ Further analysis reveals that perturbations not only lead to incorrect outputs but also substantially increase the average response length, which results in more redundant reasoning paths and higher computational resource consumption. These findings highlight the robustness deficiencies and efficiency bottlenecks of current LLMs in mathematical reasoning tasks.", "tldr": "Automated attack algorithm reveal the vulnerability of large language models in mathematical reasoning.", "keywords": ["Large Language Models", "Adversarial Attack", "Mathematical Reasoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4293d0c75a5b1a563aaf2d5512131c94e67285b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MSCR, an automated adversarial attack method that systematically probes the robustness of LLMs in mathematical reasoning. By leveraging multi-source candidate word replacements (embedding similarity, WordNet, and masked language models), MSCR generates semantically similar perturbations to input questions. Experiments on GSM8K and MATH500 benchmarks show that even single-word changes can drastically reduce LLM accuracy and increase response length, revealing significant vulnerabilities in current models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a scalable, automated adversarial attack framework that does not require manual intervention.\n- Demonstrates effectiveness across 12 open-source and 2 commercial LLMs, with clear quantitative results.\n- Reveals not only accuracy drops but also increased response lengths, highlighting efficiency and robustness issues.\n- Very interesting find on how seemingly atomic perturbations are causing the model generation to collapse."}, "weaknesses": {"value": "- Limited Defense Discussion: The paper does not propose or evaluate potential defenses or mitigation strategies for the identified vulnerabilities. Also there is a need for more elaborate analysis and root causing on how single word flips is causing such significant drops in performances.\n- More Analysis on perturbation quality : tying this back to the lack of root-cause analysis, it would be great to understand the quality/extent of perturbations in the adversarial examples. Also, analysis on what sort of perturbations cause the most performance degradation (in term of both accuracy and response token lengths)\n- Over-emphasis on single word perturbations : would be good to understand what the performance degradation looks like for multiple word flips as well - otherwise it does not provide a complete picture. \n\nThe aim here should not be to just point out a gap, but also a more thorough analysis on where this gap comes from, how reproducible it is, and how accurate the testing process is. Lack of these aspects do not affirm confidence in the process. Overall, I would suggest a deeper dive into the issue at hand."}, "questions": {"value": "Addressed in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TzS35cwbwj", "forum": "ijOAhcHI7S", "replyto": "ijOAhcHI7S", "signatures": ["ICLR.cc/2026/Conference/Submission18616/Reviewer_ohXA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18616/Reviewer_ohXA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520177412, "cdate": 1761520177412, "tmdate": 1762928331498, "mdate": 1762928331498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The vulnerability of LLMs in mathematical reasoning is a significant area of research. This work introduces an automatic adversarial attack method that generates perturbations by replacing words with candidates from three information sources.  The method can successfully reduce the accuracy of all models by altering just a single word. The authors also discovered that these perturbations cause a substantial increase in the average length of the model's responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a valuable contribution by proposing an automatic method to study the vulnerability of LLMs in mathematical reasoning. The methodology is clearly explained, and the results appear reasonable.  The finding regarding response length is interesting, which offers a novel insight into model robustness beyond simple accuracy."}, "weaknesses": {"value": "The main performance results do not offer new insights beyond established literature, making the method's added value unclear.\n\nKey analyses are absent, including an ablation study on the role of the three information sources and any metrics regarding the attack's efficiency (e.g., the number of candidate replacements needed for success)."}, "questions": {"value": "Please clarify whether the measured response length for models like the DeepSeek-R1-Distill-Qwen series includes their long chain-of-thought tokens. If not, a discussion of whether the adversarial attacks had a differential impact on the long cot could reveal insights into their relative robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9uzNxkaRaS", "forum": "ijOAhcHI7S", "replyto": "ijOAhcHI7S", "signatures": ["ICLR.cc/2026/Conference/Submission18616/Reviewer_EQZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18616/Reviewer_EQZP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728346542, "cdate": 1761728346542, "tmdate": 1762928331008, "mdate": 1762928331008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an automated adversarial attack framework, MSCR, which uses pre-existing external information sources and LLM embedding spaces to identify semantically coherent replacements for a single word in an input mathematical problem. Use of this framework demonstrates that even a single word, semantically similar replacement that is non-task-specific (irrelevant to the mathematical information) is a perturbation significant enough to cause reasoning failures, even in frontier models. This work also demonstrates collateral liabilities - namely the longer (incorrect) responses, and associated higher computational resource consumption."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This work presents a useful analysis framework by demonstrating the distribution of response lengths and increase resource consumption, beyond the reduction in accuracy of output\n2. This work effectively demonstrates how even minor, non-task-specific perturbations cause a significant decline in model performance"}, "weaknesses": {"value": "1. The observations and conclusions drawn in this work are not novel. The following works have established more substantial claims and contributions in the domain of adversarial input generation, including for mathematical problems on GSM8K\n- Cutting through the Noise Anantheswaran et al. (2025)\n- MathAttack Zhou et al. (2024)\n- Adversarial Math Word Problem Generation Xie et al. (2024)\n- An LLM can Fool Itself Xu et al. (2023) \n- Universal and Transferable Adversarial Attacks on Aligned Language Models Zou et al. (2023)\n- Evaluating Models’ Local Decision Boundaries via Contrast Sets Gardner et al. (2020)\n\nThe adversarial attacks in this work are of a lower complexity and frequency than the related works. While this work demonstrates that even minor perturbations can cause model failures, the framework proposed is not novel or usable for high complexity adversarial attacks\n2. Fig 1: The example demonstrates the effect of a single word replacement. However, the work fails to detail \n- 2.1 how the substitution of semantically similar candidates makes for better adversaries than random replacement\n- 2.2 why, in this case, the substitution of a generic term (\"he\") in the absence of a strongly semantically-related term is effective\n3. Why the substitution causes model reasoning failures is not adequately explored or hypothesized on - especially since the perturbations are not specific to the mathematical task."}, "questions": {"value": "1. Is there a causal link or a hypothesis that could explain the effect of the substitutions on the model reasoning patterns?\n2. This work could benefit from ablation studies exploring the effects of replacing more than one word; replacing words with candidates that have a cosine similarity above a certain threshold (thus ensuring a strong semantic similarity, not just the closest semantically similar word); the effects of replacing different words or sentence components for the same input and whether the choice of word to replace can offer explanations as to why the model reasoning is derailed by these minor perturbations.   \n3. This work could also benefit from conducting the same inference experiments on LLMs such as GPT/Claude/Gemini, even if on a smaller subset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zX9SOtW8lt", "forum": "ijOAhcHI7S", "replyto": "ijOAhcHI7S", "signatures": ["ICLR.cc/2026/Conference/Submission18616/Reviewer_ZoDp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18616/Reviewer_ZoDp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781540100, "cdate": 1761781540100, "tmdate": 1762928329091, "mdate": 1762928329091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MSCR (Multi-Source Candidate Replacement), an automated adversarial attack method designed to evaluate the robustness of Large Language Models (LLMs) on mathematical reasoning tasks. The core idea is to generate semantically similar, single-word perturbations for input questions and observe the impact on model performance. MSCR creates a set of candidate replacement words from three sources: cosine similarity in the model's embedding space, the WordNet dictionary, and contextual predictions from a masked language model. The authors conduct extensive experiments on 12 open-source LLMs and test the transferability of the generated attacks on 2 commercial LLMs (OpenAI-03 and GPT-4o), using the GSM8K and MATH500 benchmarks. The main contributions are:\nAn effective, automated attack framework (MSCR) that requires no manual effort and can systematically probe LLM vulnerabilities in mathematical reasoning. Demonstration of significant robustness failures: The study shows that even a single-word perturbation can cause drastic accuracy drops across all tested model. Adversarial examples generated for one model are shown to be effective against other models, including powerful commercial ones like GPT-4o. The paper reveals that under attack, LLMs not only produce incorrect answers but also generate significantly longer responses. This phenomenon, termed \"response length collapse\" (though it's more of an expansion), indicates that models enter more complex and inefficient reasoning paths when confused, increasing computational costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The study's strength lies in its evaluation across 12 diverse open-source LLMs and 2 commercial models, providing a broad and picture of the vulnerability.\n* MSCR is a practical and scalable method. By being fully automated, it overcomes the cost and scalability limitations of manual prompt engineering, making rigorous robustness testing more accessible.\n* The discovery that adversarial perturbations lead to significantly longer and more convoluted reasoning paths is a key strength. This \"response length collapse\" finding connects robustness to efficiency, revealing a hidden cost of model failure that has been less explored."}, "weaknesses": {"value": "* The paper excels at demonstrating that the attacks are successful but offers limited insight into why they work. An analysis of which candidate source (embeddings, WordNet, MLM) generates the most effective perturbations, or what grammatical types of words (e.g., proper nouns, prepositions, verbs) are most vulnerable, would add more insights.\n* The use of qwen3-max-preview for secondary evaluation introduces a potential confounder. The results are implicitly dependent on the reliability and potential biases of this specific model."}, "questions": {"value": "1. Could you provide a more detailed breakdown of the successful attacks? Specifically, what percentage of successful perturbations originated from each of the three candidate sources (cosine similarity, WordNet, and MLM)? This would help in understanding which type of semantic similarity LLMs are most sensitive to.\n2. Following up on the analysis of attack mechanisms, have you observed any patterns in the types of words that are most critical to perturb? For instance, are proper nouns, which require grounding, more vulnerable than common nouns or verbs? The provided examples are excellent; a systematic categorization of them would be very insightful.\n3. Regarding the secondary evaluation with qwen3-max-preview, what was the approximate rejection rate? That is, what percentage of attacks that produced a different numerical answer (preliminary success) were later judged by the evaluator LLM to be based on a correct reasoning path for the perturbed question (and thus not a true attack success)?\n4. Have you performed a qualitative analysis of these longer responses? Do they show the model expressing confusion, getting stuck in reasoning loops, exploring multiple contradictory paths, or simply being more verbose before failing? A few examples illustrating these behaviors would greatly enrich the paper.\n5. The paper focuses on a single-word substitution attack. Did you explore other minimal perturbations, such as the insertion or deletion of a single, seemingly innocuous word (e.g., an article or adverb)? It would be interesting to know if the models are similarly sensitive to such structural changes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "if2LpyHCWa", "forum": "ijOAhcHI7S", "replyto": "ijOAhcHI7S", "signatures": ["ICLR.cc/2026/Conference/Submission18616/Reviewer_5XZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18616/Reviewer_5XZn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984662722, "cdate": 1761984662722, "tmdate": 1762928328692, "mdate": 1762928328692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}