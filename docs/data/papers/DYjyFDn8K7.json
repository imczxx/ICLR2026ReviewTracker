{"id": "DYjyFDn8K7", "number": 18344, "cdate": 1758286678092, "mdate": 1759897109558, "content": {"title": "Fine-Tuning Diffusion Models via Intermediate Distribution Shaping", "abstract": "Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an 8.81% relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.", "tldr": "Rejection Sampling on noisy intermediate samples for diffusion models implicitly performs PPO and is effective in practice.", "keywords": ["diffusion", "fine-tuning", "reinforcement learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e9cb30edbff3633633d5145a312b2af60dc7a5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces fine-tuning methods for diffusion models through intermediate distribution shaping. The authors first unify rejection sampling-based fine-tuning methods as GRAFT (Generalized Rejection sAmpling Fine-Tuning) and prove it implicitly performs PPO with reshaped rewards, enabling marginal KL regularization for diffusion models despite intractable likelihoods. They then propose P-GRAFT (Partial-GRAFT), which fine-tunes only until intermediate denoising timesteps by assigning final generation rewards to partial noisy states. This is justified through a bias-variance tradeoff: while variance of rewards conditioned on intermediate states increases with noise level (Lemma 4.3), the score function becomes easier to learn at higher noise levels (Theorem 4.4). Additionally, they introduce Inverse Noise Correction for flow models, which trains a model to correct the distribution shift in initial noise without explicit rewards. Empirically, P-GRAFT applied to Stable Diffusion v2 achieves 8.81% relative improvement in VQAScore over the base model on text-to-image benchmarks and outperforms policy gradient methods. For unconditional generation, Inverse Noise Correction improves FID at lower FLOPs/image."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The unification of rejection sampling methods (RAFT, RSO, BoN) under the GRAFT framework with explicit connection to PPO is novel and theoretically grounded (Lemma 3.2)\n* P-GRAFT represents a genuinely new approach to diffusion model fine-tuning by operating at intermediate noise levels rather than final outputs\n* The bias-variance analysis providing theoretical justification for intermediate distribution shaping is insightful and well-executed\n* Mathematical formulations are generally clear with good notation"}, "weaknesses": {"value": "1. **Limited theoretical analysis for P-GRAFT optimality**: There is no formal analysis of the optimal choice of $N_I$ or convergence guarantees. The paper relies heavily on empirical validation across different $N_I$ values, but theoretical guidance for selecting $N_I$ would strengthen the contribution.\n\n2. **Assumption dependencies**: Lemma 5.1 requires $\\eta L < 1$ for backward Euler, but the paper doesn't discuss how this constraint affects practical step size choices or what happens when the Lipschitz constant $L$ is large. The impact on computational cost is not analyzed.\n\n3. **Gap between theory and practice**: The theoretical analysis assumes continuous-time SDEs, but experiments use discrete-time DDPM/DDIM schedulers. The discretization error and its interaction with P-GRAFT is not discussed."}, "questions": {"value": "1. How does the optimal $N_I$ depend on task characteristics (reward smoothness, data modality, model capacity)? Can you provide theoretical or empirical guidance for selecting $N_I$?\n\n2. Theorem 4.4 shows score functions at later times are exponentially closer to Gaussian. Have you verified this empirically by measuring $H_t^T$ at different timesteps?\n\n3. The backward Euler method (Algorithm 6) requires solving a fixed-point equation. How many iterations $N_b$ are typically needed in practice, and what is the computational cost relative to forward sampling?\n\n4. Can P-GRAFT be combined with classifier-free guidance training? How does the guidance scale interact with the intermediate timestep choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xp0ujdSjNo", "forum": "DYjyFDn8K7", "replyto": "DYjyFDn8K7", "signatures": ["ICLR.cc/2026/Conference/Submission18344/Reviewer_xSnM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18344/Reviewer_xSnM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621166842, "cdate": 1761621166842, "tmdate": 1762928052878, "mdate": 1762928052878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate:\n(1) Rejection sampling and itâ€™s uses in sampling and fine-tuning diffusion models. The authors derive the sampling distribution for top-k sampling, with arbitrary reward functions and de-duplication criterion, for diffusion models. The authors then propose fine-tuning on the trajectories yielded by rejection sampling.\n\n(2) An inverse noise correction algorithm that trains a flow model from N(0, 1) to the t=1 distribution of the flow model, and then sample from the flow model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "See questions"}, "weaknesses": {"value": "See questions"}, "questions": {"value": "Will the authors clarify what their contributions are exactly, as\n\n1. The sampling distribution of top-k sampling is known:\n    1. As the authors acknowledge, the derivations are known from Amini et al 2024. The derivations there are not specific to any particular kind of method of generation, it applies to any generative model, including both AR, diffusion models. See theorem 2 in Amini et al 2024. \n2. Top-k sampling for fine-tuning has been proposed and shown in RAFT, see Dong et al\n\nQuestions regarding fine-tuning with Top-k sampling:\n\n1. What distribution does alg 2 (P-GRAFT inference) sample?  Alg 2 stiches together two different diffusion models, with the first one being fine-tuned and sampling from the reward tilted intermediate distribution, followed by the base model. \n\nQuestions regarding the motivation for inverse-noise correction:\n\n1. The motivation for the method is not clear from the text, either in the main paper or appendix. \n\nExperimental Clarifications: \n\n1. Can the authors clarify what objective was used to fine-tune the diffusion model in alg 1 (P-GRAFT train)\n2. The GenEval scores reported in the paper for SDv2 and SDXL-base are higher than those reported in the GenEval paper. Can the authors clarify what prompts were used for producing the numbers reported in table 2. \n\nMinor clarification regarding using the term PPO distribution: PPO is a method for learning the reward tilted distribution, p(x) exp(r(x)). This distribution can be learned using methods other than PPO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QfBmSvP9E0", "forum": "DYjyFDn8K7", "replyto": "DYjyFDn8K7", "signatures": ["ICLR.cc/2026/Conference/Submission18344/Reviewer_sMVE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18344/Reviewer_sMVE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681387720, "cdate": 1761681387720, "tmdate": 1762928052512, "mdate": 1762928052512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper recognizes the importance of reward-guided fine-tuning of diffusion models and the complexity of leveraging proximal PG methods due to intractable likelihoods. The authors introduce a unified rejection sampling framework, show that fine-tuning according to it leads to the same solution of proximal PG schemes, and introduce a method for controlling also the tilt of intermediate distributions of the diffusion process. They present a mathematical analysis for bias-variance trade-off, and propose a noise-correction scheme for reward-independent improvement of flow models. Ultimately, the present an experimental evaluation of the proposed contributions."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper correctly identifies a limitation of RL-based schemes for fine-tuning of diffusion models (i.e., intractable log likelihoods), a timely task with high-relevance.\n\n- The idea proposed within Sec. 5 seems interesting, but I could not fully understand its logic on an intuitive level.\n\n- The experimental sections includes diverse dataset spanning images, layout gen., and molecular design."}, "weaknesses": {"value": "The paper presents an excessive amount of ideas, each not sufficiently justified, motivated, or explained. As the paper structure is very complex due to the multitude contributions, I will discuss weaknesses of each contribution a-d as listed in Sec. 1.\n\n**a)** \n1. The paper often seems to confuse a *problem*, in particular solving an entropy-regularized MDP, with an algorithm, i.e. PPO. I understand that the presented algorithm has solution corresponding to the optimal solution of entropy-regularized MDPs, which can be tackled by PPO, but what does 'GRAFT enables PPO' or 'implicitly perform PPO' even mean? This confusion seems repeated several times within the work. Concretely, I believe these statements are effectively wrong, as the propose method, while inducing the same solution, does *not* enable/perform PPO.\n2. It is well-known that a general class of inference-time schemes for diffusion induces this solution class (see [1], Eq. 1). One such case is [2] (see Sec. 3.2). So it seems there is already a variety of inference-time schemes solving this problem via diverse techniques (c.f., [1]). Moreover, there exist fine-tuning control-theoretic schemes that can solve this entropy-reg. problem as well (e.g., [3]) without value bias problem, as also mentioned by the authors. Ultimately, leveraging inference-time schemes solving this problem to then fine-tune a model, which seems to me the core algorithmic idea here, is already presented in [1, Sec. 9] arguably in a more performative fashion than the one presented within this paper (i.e. , via policy distillation rather than plain training). As a consequence, it seems to me that there isn't significant novelty within the presented contribution.\n\n**b)**\n1. The KL is typically enforced on the data level distribution as a way to preserve the high-probability set learned by the pre-trained model. The justifications presented within the paper (both in theory 4.1 and experiments) do not seem convincing to me regarding why we should instead enforce the KL-reg. at another time-step. In particular, it seems to me that the theoretical investigation in Sec. 4.1 does not really provide a concrete answer to this question. In a sense, it shows that this problem might be easier, but this does not imply (practical or theoretical) relevance. Since the authors here are presenting a novel problem setting, it would be essential to motivate it clearly.\n\n**c)**\n- (writing) this section (i.e. Sec. 5) is not clearly written to the point that I could not fully grasp the presented idea. The problem tackled seems not particularly related with the problems treated in the rest of the paper, and is not sufficiently formalized to properly understand the gains of the proposed methodology. I would strongly suggest to also introduce algorithmic aspects with an intuitive presentation of their workings before/after presenting their implementation.\n\n\n**Overall (writing/structure)**\nThe paper is poorly structured and lacks a solid narrative. Concretely, it presents multiple ideas without sufficient clarification of their motivation, and/or workings. The text often lacks conceptual explanations of new concepts/mechanisms and their implications. \n\n\n**References**:\n\n[1] Inference-Time Alignment in Diffusion Models with Reward-Guided Generation: Tutorial and Review, 2025\n\n[2] Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding, 2024\n\n[3] Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control, 2024"}, "questions": {"value": "- Did I misinterpret or misunderstand any of my points above within (a) or (b)?  \n- What is the core algorithmic intuitive idea for the method introduced in Sec. 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cpI6HePqZm", "forum": "DYjyFDn8K7", "replyto": "DYjyFDn8K7", "signatures": ["ICLR.cc/2026/Conference/Submission18344/Reviewer_iasd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18344/Reviewer_iasd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837565579, "cdate": 1761837565579, "tmdate": 1762928052174, "mdate": 1762928052174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}