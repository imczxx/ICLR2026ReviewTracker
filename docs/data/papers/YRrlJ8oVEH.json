{"id": "YRrlJ8oVEH", "number": 6144, "cdate": 1757954488037, "mdate": 1759897933146, "content": {"title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors", "abstract": "Few-shot anomaly detection streamlines and simplifies industrial safety inspection. However, limited samples make accurate differentiation between normal and abnormal features challenging, and even more so under category-agnostic conditions. Large-scale pre-training of foundation visual encoders has advanced many fields, as the enormous quantity of data helps to learn the general distribution of normal images.\nWe observe that the anomaly amount in an image directly correlates with the difference in the learnt embeddings and utilize this to design a few-shot anomaly detector termed FoundAD.\nThis is done by learning a nonlinear projection operator onto the natural image manifold.\nThe simple operator acts as an effective tool for anomaly detection to characterize and identify out-of-distribution regions in an image.\nExtensive experiments show that our approach supports multi-class detection and achieves competitive performance compared to other approaches, while surpassing them in model size and inference efficiency.\nBacked up by evaluations with multiple foundation encoders, including fresh DINOv3, we believe this idea broadens the perspective on foundation features and advances the field of few-shot anomaly detection. Our code will be made public.", "tldr": "We introduce a few-shot anomaly detection method using foundation visual encoders and a nonlinear projection onto the natural image manifold. It detects structural anomalies efficiently, supports multi-class cases, and delivers strong performance.", "keywords": ["Representation Learning", "Few-Shot Anomaly Detection", "Applications of Foundation Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b5ce5313e36f50e9c19dd46476b1d4d569c4e9a.pdf", "supplementary_material": "/attachment/eaf65316d939dea778074d29c3943b01e091710f.zip"}, "replies": [{"content": {"summary": {"value": "This paper observes that in foundation visual encoders, the embedding distance naturally correlates with the amount of anomaly within an image, indicating that these pretrained models implicitly capture the structure of the normal image manifold. Building on this observation, the authors propose a few-shot anomaly detection framework that leverages this intrinsic property without retraining the encoder."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow. The motivation, method design, and experimental setup are clearly articulated, making the overall contribution easy to understand.\n2. The proposed method achieves impressive performance across multiple benchmarks, including the multi-class few-shot setting, the improvements over prior methods are consistent and substantial.\n3. The proposed method introduces only a small projection network with minimal trainable parameters and fast inference. Also not relied on text prompts."}, "weaknesses": {"value": "1. The method trains the projector using synthetic anomalies generated by CutPaste, which differ significantly from real-world defects in texture and scale. The paper does not provide a clear empirical justification for why this training signal generalizes well to real anomalies. The paper should include a more explicit discussion of this transferability. \n2. Whether using more realistic synthetic data, e.g., anomalies generated by modern diffusion-based methods, would further improve or alter performance? An ablation in this direction would better explain the role of synthesis process in this framework.\n3. The method uses features from a single mid-level layer of the foundation encoder. Have the authors considered aggregating multi-layer embeddings to capture both fine-grained and semantic anomalies?\n4. The paper should include a baseline using the raw frozen encoder features without fine-tuning to see how much performance comes from the inherent foundation encoder."}, "questions": {"value": "See above weaknesses. The method is conceptually simple and technically lightweight. Although the empirical results are strong, I would be more interested in a deeper discussion of the experimental setting and underlying factors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Kz47dOuEY1", "forum": "YRrlJ8oVEH", "replyto": "YRrlJ8oVEH", "signatures": ["ICLR.cc/2026/Conference/Submission6144/Reviewer_Bnw6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6144/Reviewer_Bnw6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376550114, "cdate": 1761376550114, "tmdate": 1762918498417, "mdate": 1762918498417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FOUNDAD introduces a simple yet effective few-shot anomaly detection framework using frozen foundation encoders and a lightweight projector trained in latent space. The approach achieves strong results on standard industrial benchmarks with minimal supervision, outperforming more complex baselines. Its simplicity, efficiency, and robustness in multi-class settings make it attractive for practical deployment. While some limitations remain in generalization and synthetic anomaly reliance, the contribution is clear."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes FOUNDAD, a simple yet effective framework for few-shot anomaly detection using frozen foundation encoders and a lightweight projector trained in latent space. The core idea is that pre-trained encoders already capture a natural image manifold, and deviations from it can indicate anomalies. This intuition is clearly demonstrated both conceptually and empirically.\n\nThe method is efficient,requiring significantly fewer parameters than recent baselines, yet achieves competitive or superior results across multiple benchmarks (MVTec-AD, VisA), especially in the low-shot regime. FOUNDAD also operates without text prompts or heavy multi-model setups, which makes it more practical for real-world industrial applications. Presentation is mostly clear, and the supplemental material is thorough, including failure cases and ablations."}, "weaknesses": {"value": "The reliance on synthetically generated anomalies (via CutPaste-like methods) introduces a potential domain gap. While the authors argue that the projector generalizes well, the method struggles with unseen anomaly types like rotated objects or subtle defects. This raises concerns about robustness.\nAdditionally, the approach heavily depends on the choice of encoder—DINOv3 works well, but others perform worse. It remains unclear how well this method would transfer to domains less similar to natural images (e.g., medical or satellite imagery).\nThe few-shot setup could be explained more clearly. It’s not immediately obvious how support images are selected and used, especially in the multi-class setting."}, "questions": {"value": "1. Have you considered stronger augmentation strategies or geometric alignment (e.g., from RegAD) to improve generalization to rotated or appearance-shifted anomalies?\n2. Can you clarify the few-shot protocol? Are normal samples provided per class, or pooled globally?\n3. Did you experiment with partially fine-tuning the encoder instead of freezing it entirely?\n4. How sensitive is performance to the CutPaste threshold or other synthesis hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CSXedLyJSA", "forum": "YRrlJ8oVEH", "replyto": "YRrlJ8oVEH", "signatures": ["ICLR.cc/2026/Conference/Submission6144/Reviewer_vtuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6144/Reviewer_vtuY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994856052, "cdate": 1761994856052, "tmdate": 1762918498022, "mdate": 1762918498022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FOUNDAD, a lightweight few-shot anomaly detection method leveraging pre-trained foundation visual encoders like DINOv3 without text prompts. It introduces a nonlinear manifold projector that maps image embeddings onto a natural image manifold, enabling anomaly localization using only few normal samples. Experiments on MVTec-AD and VisA datasets show FOUNDAD achieves state-of-the-art performance with fewer parameters than competing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed nonlinear manifold projector is simple yet effective, requiring minimal parameters and training, while achieving state-of-the-art few-shot anomaly detection performance.\n\n- Evaluations across few-shot settings (1, 2, 4) demonstrate efficacy of the method."}, "weaknesses": {"value": "- The idea of using latent feature distances for anomaly detection resembles prior projection or reconstruction-based methods. In particular, projecting anomaly feature back into normal manifold is a long established idea for anomaly detection, e.g. GAN or AutoEncoder based methods.\n\n- The details of projector training is missing. For example, the number of training epochs, how to generate training data, etc. These are very important information because it remains elusive how the projector can be trained with a single image (1-shot). Moreover, the number of training epochs/iterations also reveal some key insights into the reason why the model is effective.\n\n- The reliance on CutPaste-like synthesis might limit realism and generalization to complex industrial defects not represented by structural cut-paste anomalies. Although the performance on MVTec and VisA demonstrate the effectiveness, evaluation on more challenging datasets may better reveal the effectiveness.\n\n- CutPaste augmentation is a very important component for this method. The original CutPaste work directly trained a classification model differentiating good from cut paste augmented pseudo defects. Therefore, a comparison with the original CutPaste method is essential to demonstrate the effectiveness. Importantly, the original CutPaste should enjoy the same foundation model as backbone.\n\n- The proposed method benefits from the multi-class setting due to more data available for training the projector. It will further improve the significance if single-class setting is evaluated.\n\n- Only two industrial datasets (MVTec-AD, VisA) are used, thus limiting the significance of conclusion."}, "questions": {"value": "I encourage the authors to address the following issues in the rebuttal and I would adjust the rating based on the results.\n\n- Please provide more details of training procedure and evaluate how the training procedure may affect the results.\n\n- Additional datasets beyond MVTec and VisA are necessary to demonstrate the effectiveness.\n\n- Please compare with original CutPaste with the same foundation model as backbone.\n\n- More evaluations under single-class setting is necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6SFgkZLeyX", "forum": "YRrlJ8oVEH", "replyto": "YRrlJ8oVEH", "signatures": ["ICLR.cc/2026/Conference/Submission6144/Reviewer_BLKp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6144/Reviewer_BLKp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005265302, "cdate": 1762005265302, "tmdate": 1762918497574, "mdate": 1762918497574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Acknowledgements and Clarifications"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback! In particular, we appreciate that Reviewers `BLKp` and `vtuY` found FoundAD to be **“simple yet effective”**, and that Reviewer `Bnw6` considered the “motivation, method design, and experimental setup” to be **“clearly articulated”**. Before addressing the individual comments, we first recap our main contributions:\n\n**Recap:**\n > (1) We empirically demonstrate a strong correlation between embedding distance and anomaly amount for frozen foundation visual encoders.\n\n > (2) We propose a feature projection that exploits the property in point (1) to separate normal from anomalous features in the latent space.\n\n > (3) We achieve competitive few-shot anomaly detection performance without any textual supervision.\n\n > (4) Our method is lightweight and attains higher efficiency than previous state-of-the-art methods.\n\nWe appreciate the reviewers’ attention to detailed quantitative comparisons and have added several experiments accordingly. We hope that our additional experiments will be helpful during the discussion phase. At the same time, we would like to emphasize that the key contribution lies in the *underlying observations* and the *framework*,  which demonstrate that foundation visual encoders, with a small projector trained on simple synthetic anomalies, already constitute a strong and efficient few-shot anomaly detector. We hope that this **conceptual perspective**, together with the **competitive results** across multiple datasets, will be taken into account in the final evaluation."}}, "id": "dqQ9oNbLQ0", "forum": "YRrlJ8oVEH", "replyto": "YRrlJ8oVEH", "signatures": ["ICLR.cc/2026/Conference/Submission6144/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6144/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6144/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763590623652, "cdate": 1763590623652, "tmdate": 1763653957667, "mdate": 1763653957667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}