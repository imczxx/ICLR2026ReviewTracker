{"id": "ImRhA9xmay", "number": 5012, "cdate": 1757833289754, "mdate": 1759898000332, "content": {"title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting", "abstract": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses.\nTo overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280×518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. The code and pretrained models will be made public.", "tldr": "", "keywords": ["3D Gaussian splatting", "feedforward model", "novel view synthesis", "pose-free"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/575a193ea292603e131a98efc39a0b47c83e7a95.pdf", "supplementary_material": "/attachment/58f8dc3f53459160032ef98ecf5a77e4cfc9d8c3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces YoNoSplat, a feedforward framework for 3D Gaussian reconstruction that achieves state-of-the-art performance in both pose-free and pose-dependent settings, across an arbitrary number of input views.\n\nThe authors identify the entanglement between pose estimation and geometry learning as a core challenge in previous 3DGS methods and propose a mix-forcing training strategy that stabilizes optimization and mitigates exposure bias by blending predicted and ground-truth poses during training.\n\nTo further enable reconstruction from uncalibrated images, the paper resolves the scale ambiguity problem via an intrinsic-prediction-and-conditioning pipeline combined with a pairwise distance normalization scheme.\n\nOverall, YoNoSplat offers a single-pass, feedforward solution that unifies pose-free and calibrated reconstruction under one model, achieving efficient, robust, and accurate 3D Gaussian generation from arbitrary view inputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Novelty and Conceptual Contribution. As the authors claim, the paper introduces YoNoSplat, the first feedforward framework for 3D Gaussian Splatting (3DGS) capable of operating in both pose-free and pose-dependent settings with an arbitrary number of input views. Unlike prior optimization-based or iterative reconstruction methods, YoNoSplat performs single-pass 3D reconstruction, which is conceptually novel and significantly improves inference efficiency. The identification of pose–geometry entanglement as a key bottleneck and the proposal of a mix-forcing strategy to stabilize training demonstrate deep insight into the learning dynamics of 3DGS systems.\n\n(2) Solid Performance and Robustness. YoNoSplat achieves state-of-the-art performance across multiple benchmarks and scenarios, showing strong results in both calibrated and uncalibrated settings. The method performs consistently well under varying numbers of views, highlighting robust generalization and stability. The new scale ambiguity resolution pipeline and pairwise distance normalization enable accurate 3D reconstruction even from uncalibrated images, a setting where most existing methods struggle.\n\n(3) Thorough Experiments and Ablation Studies. The experimental section is comprehensive and well-designed, covering comparisons with leading 3DGS and feedforward reconstruction models. Ablation studies clearly validate each component — including the effect of mix-forcing, scale normalization, and intrinsic prediction — providing strong empirical justification for the proposed design choices. The experiments are reproducible and transparent, with clear evaluation protocols and diverse test conditions.\n\n(4) Clarity of Writing and Presentation. The paper is clearly written and easy to follow, with a well-organized structure that guides the reader from motivation to methodology and results. Figures effectively illustrate the architecture, pipeline, and visual results, complementing the textual explanations. The tone and presentation are professional and concise, making complex ideas accessible without oversimplification.\n\n(5) Practical Impact and Scalability. By enabling fast, calibration-free 3D reconstruction, YoNoSplat has strong potential for real-world deployment in 3D vision applications such as AR/VR and robotics. Its feedforward nature offers a clear path toward real-time or large-scale reconstruction, marking a meaningful advance beyond optimization-heavy NeRF or 3DGS pipelines."}, "weaknesses": {"value": "(1) Overclaim on Generality (“Arbitrary-View Pose-Free and Pose-Dependent” Claim Not Fully Convincing). The paper claims that YoNoSplat works for an arbitrary number of views and supports both pose-free and pose-dependent reconstruction. However, the experiments do not fully substantiate this claim. There is no evaluation on single-view or extremely sparse-view scenarios, which are crucial to support the “arbitrary-view” statement. Comparisons focus mostly on internal baselines or a limited subset of view settings. The paper does not include direct comparisons with leading feedforward or hybrid 3DGS methods such as GS-LRM, PixelSplat, VGGT, or DiffusionGS, which already demonstrate strong performance in few-view or uncalibrated settings. As a result, the claimed “state-of-the-art across arbitrary poses and views” feels overstated and not rigorously validated.\n\n(2) Lack of Code and Reproducibility. The code and pre-trained models are not submitted and the reproducibility cannot be checked. Given that the pipeline involves custom intrinsic-prediction modules, mix-forcing training, and pairwise normalization, reproducibility is difficult without access to implementation details. This limitation raises concerns about the verifiability of reported results and practical adoption by the community.\n\n(3) Limited Comparison Scope. The baseline selection is narrow, excluding several state-of-the-art feedforward or diffusion-based reconstruction frameworks (e.g., VGGT, DiffusionGS, PixelSplat). Without quantitative or qualitative comparisons to these strong baselines, it is unclear whether YoNoSplat’s improvements stem from its architecture or from dataset or training differences. The evaluation therefore lacks contextual benchmarking, weakening the impact of its performance claims.\n\n(4) Insufficient Analysis of Failure Cases and Limitations. The paper does not provide a detailed analysis of failure modes, such as how the model performs under extreme camera pose errors, illumination changes, or non-Lambertian scenes. Given that the method aims to handle pose-free inputs, discussing robustness to pose noise would be essential. Similarly, the mix-forcing strategy’s sensitivity to inaccurate supervision or dataset scale is not explored."}, "questions": {"value": "Could you clarify whether YoNoSplat has been evaluated on single-view or very sparse-view settings, since the paper claims support for “arbitrary numbers of views”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rNtXq9IAol", "forum": "ImRhA9xmay", "replyto": "ImRhA9xmay", "signatures": ["ICLR.cc/2026/Conference/Submission5012/Reviewer_HPFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5012/Reviewer_HPFA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923244924, "cdate": 1761923244924, "tmdate": 1762917821355, "mdate": 1762917821355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents YoNoSplat, a feed-forward model for 3D Gaussian Splatting that predicts per-view 3D Gaussians, camera poses, and intrinsics from an arbitrary number of unposed, uncalibrated images. The paper proposes multiple novel methods for feed-forward 3D Gaussian estimation : (i) local Gaussian estimation continued by a aggregation to the canonical space with the estimated camera pose, (ii) a “mix-forcing” training strategy which first trains the model with ground truth camera poses and progressively blends with the estimated camera poses. Extensive experiments demonstrate state-of-the-art performance on both pose-free and pose-dependent settings, with detailed ablations on normalization and architectural components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- YoNoSplat is able to process images with or without camera pose/intrinsic information, where incorporating additional information of pose/intrinsics leads to improved performance, making it suitable for both in-the-wild settings and industry level deployment.\n- The paper reveals multiple recipes such as scale normalization, mix-forcing, local estimation which is valuable for the research community for future development.\n- YoNoSplat largely outperforms previous approaches, setting the new state-of-the-art for the task.\n- The paper also performs extensive experiments and ablations improving the interpretation and re-implementation of the work easier."}, "weaknesses": {"value": "- **Large computation and training time:** In Section 4.1, it is explained that YoNoSplat requires 16 GH200 GPUs for 150k steps, where the required computation seems to be extremely large. Adding additional comparison of the required computation with prior works would be nice to better understand the contributions of YoNoSplat.\n- **Architectural Novelty:** The proposed architecture seems to be a direct extension of NoPoSplat with $\\pi^3$, which makes the architectural novelty of the work a bit limited. However, other analyses such as ablations for scene normalization and the intrinsic estimation module seems to be highly valuable.\n- **Overstatement:** The statement in L145-147 : “ In contrast, by replacing point clouds with\n3D Gaussians as the scene representation, YoNoSplat enables both novel view synthesis and training on datasets without ground-truth depth.” seems to be too strong as initializing the weights of YoNoSplat with $\\pi^3$ would be extremely important. I think the statement can be toned down a bit.\n- **Missing baselines:** Teacher forcing has also been explained as one of the important components in a previous work CoPoNeRF [1], where proper reference seems to be missing."}, "questions": {"value": "Q1. Could the authors provide a comparison with prior works in terms of computation and training time?\n\nQ2. Why is the mix-forcing probability set to an extremely small weight (0.1)? Could the authors provide additional analysis with different weights?\n\nI am eager to raise my score after the authors provide additional explanations for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "huPUuR9EGA", "forum": "ImRhA9xmay", "replyto": "ImRhA9xmay", "signatures": ["ICLR.cc/2026/Conference/Submission5012/Reviewer_3cs1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5012/Reviewer_3cs1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965809889, "cdate": 1761965809889, "tmdate": 1762917820815, "mdate": 1762917820815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents YoNoSplat, a versatile feedforward model for 3D Gaussian Splatting that reconstructs scenes from an arbitrary number of unposed and uncalibrated images. The authors' main contributions are a novel \"mix-forcing\" training strategy to resolve the entanglement between geometry and pose learning and a method to resolve scale ambiguity, enabling the use of uncalibrated inputs. The model achieves state-of-the-art (SOTA) performance in both pose-free and pose-dependent settings, demonstrating significant flexibility and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n\n2. The paper's primary strength is the model's versatility. It is designed to handle a wide, practical range of input conditions: an arbitrary number of views, both posed and unposed, and both calibrated and uncalibrated images.\n\n3. The model demonstrates state-of-the-art performance across multiple standard benchmarks."}, "weaknesses": {"value": "Reliance on Post-Optimization Undermines the \"Feedforward\" Claim. The paper presents itself as a feedforward model, but its strongest results (e.g., in Table 1) rely on an \"Optional Post-Optimization\" step. This optimization is not feedforward and adds a significant time cost (e.g., 165s for 24 views). The large performance gap between the feedforward-only output and the optimized output suggests that the feedforward prediction is, by itself, substantially suboptimal. This weakens the central claim of achieving state-of-the-art results via a purely feedforward pass."}, "questions": {"value": "The field of 3D reconstruction has recently been revolutionized by unified feed-forward models, such as VGGT and DUSt3R. This progress has spurred the development of conditional input models like Pow3R [1] and MapAnything [2], which can benefit from various optional auxiliary inputs (e.g., camera poses, depth maps, etc.). These methods inherit the advantages of unified feed-forward models and produce a unified representation for a wide range of downstream tasks, including depth estimation, multi-view stereo, camera pose estimation, and novel view synthesis.\n\nIn contrast, the method presented in this paper can only selectively input camera poses and is primarily focused on the novel view synthesis task. What, then, are the specific advantages of this work when compared to these more versatile, multi-task conditional models?\n\n[1] Pow3R: Empowering Unconstrained 3D  Reconstruction with Camera and Scene Priors\n\n[2] MapAnything: Universal Feed-Forward Metric 3D Reconstruction"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ht9XgSbe30", "forum": "ImRhA9xmay", "replyto": "ImRhA9xmay", "signatures": ["ICLR.cc/2026/Conference/Submission5012/Reviewer_PxrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5012/Reviewer_PxrR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992504374, "cdate": 1761992504374, "tmdate": 1762917819753, "mdate": 1762917819753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes YoNoSplat, a feed-forward model that predicts per-view local 3D Gaussians together with camera extrinsics and intrinsics from an arbitrary number of unposed and uncalibrated images, and aggregates them into a global scene. The key ideas are:\n\n- Local→Global output space: predict per-view Gaussians plus poses, then transform to a shared frame using predicted or provided poses.\n- Mix-forcing training: a curriculum that starts with teacher-forcing and linearly mixes in self-forcing to mitigate pose-geometry entanglement and exposure bias.\n- Scale ambiguity handling: (i) max pairwise camera-distance normalization of training scenes, and (ii) an Intrinsic Condition Embedding (ICE) module that predicts focal length and conditions the decoder via ray features.\n\nYoNoSplat reports state-of-the-art NVS under both pose-free and pose-dependent settings shows strong cross-dataset generalization to ScanNet++. Optional fast post-optimization of poses/centers/colors offers further gains."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Solid experimental coverage.** Results span multiple priors (p/k/none), multiple view counts (6/12/24; 32/64/128), and include cross-dataset tests (DL3DV→ScanNet++). The pose AUC comparisons further substantiate the quality of the predicted geometry.\n2. **Methodical ablations.** The paper dissects (i) output space (local vs canonical), (ii) training regime (mix/self/teacher), (iii) normalization choices, (iv) ICE usefulness, and (v) Plücker rays, providing good insight into *why* choices matter.\n3. **Clear problem framing & practicality.** Handling arbitrary view counts, unposed + uncalibrated inputs, but also leveraging priors when available, is exactly what real pipelines need. The local-Gaussian design keeps the method compatible with either predicted or GT poses."}, "weaknesses": {"value": "1. The proposed method predicts per-pixel Gaussians, which may become computationally inefficient for large-scale scenes (with large number of input images). While the paper introduces opacity regularization and Gaussian pruning to mitigate this, it does not quantify how much these steps actually reduce the number of Gaussians or memory footprint. A comparative analysis with AnySplat in terms of Gaussian count and memory efficiency would strengthen the claims about scalability.\n2. ICE train–test mismatch. During training, the network is conditioned on GT intrinsics, whereas at inference it may use predicted intrinsics (when GT is absent). The paper mentions instability when conditioning the decoder on encoder-predicted intrinsics during training but leaves the mechanism under-analyzed. This could introduce residual exposure bias for k. Could further mixing (analogous to mix-forcing for poses) reduce this gap?\n3. Memory limits and pruning are mentioned, but there’s little qualitative analysis on thin structures, specular/transparent surfaces, or very wide baselines (where local Gaussian fusion and pose heads might struggle).\n4. The mix-in schedule seems tuned but not justified beyond a single setting; no sensitivity curve is provided. Could higher r benefit pose-free usage further?"}, "questions": {"value": "1. How sensitive are results to (t_start, t_end, r)? Did you try higher final mixing ratios (e.g., r≥0.3) to further harden the model for pose-free inference? Any signs of instability as r increases?\n2. Normalization choice edge cases. Max pairwise camera-distance normalization is empirically best, but it could be brittle with outliers or heavy view clustering (small parallax + one far-away view). Can this normalization generalizable for those settings?\n3. ICE helps with uncalibrated inputs, but how does performance degrade with systematic intrinsics bias (e.g., ±10–20% focal scale) or radial distortion in images, unseen in training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "US4MwjREZu", "forum": "ImRhA9xmay", "replyto": "ImRhA9xmay", "signatures": ["ICLR.cc/2026/Conference/Submission5012/Reviewer_aKcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5012/Reviewer_aKcD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994081673, "cdate": 1761994081673, "tmdate": 1762917819388, "mdate": 1762917819388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}