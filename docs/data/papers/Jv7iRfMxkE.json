{"id": "Jv7iRfMxkE", "number": 16581, "cdate": 1758266340580, "mdate": 1759897231503, "content": {"title": "Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs", "abstract": "Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. \nCurrent code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage.\nCurrently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential.\nTo address these limitations, we introduce \\textit{Lita} (\\textbf{Lit}e \\textbf{A}gent), which operationalizes \\textit{liteness}, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding.\nExperiments on the Aider polyglot and SWEbench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. \nOur results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the \\textbf{Agent Complexity Law}: \\textit{the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.}", "tldr": "We showed that complex agent design hide the true ability of LLMs and argue the necessity of lite agents for a fair and truthful evaluation", "keywords": ["LLM", "code agent", "LLM evaluation", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f148f752d75bef705ad3d93fa155003c9be9615b.pdf", "supplementary_material": "/attachment/c64b00c8c6e096be55054a4935273c836a2073ea.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses issues in current LLM-based code agent design, such as overengineering and unfair evaluation, and proposes a lightweight agent framework named Lita. Its core contributions include:\n\n1. Introducing the concept of a “lightweight agent,” emphasizing minimization of human intervention (e.g., complex workflows, task-specific optimizations) to more accurately assess the coding capabilities of LLMs;  \n2. Developing the Lita prototype system, which contains only essential tools (e.g., editor, terminal, reasoning module) and supports multi-turn autonomous interaction;  \n3. Proposing a method to adapt traditional code benchmarks (e.g., HumanEval, SWE-Bench) into an agent evaluation format, ensuring fairness and portability in evaluation;  \n4. Providing empirical evidence that Lita achieves or surpasses complex baselines (e.g., OpenHands, Aider) on multiple benchmarks, while significantly reducing token consumption and design costs;  \n5. Proposing the Agent Complexity Law: as model capabilities improve, the performance gap between agents of different complexity will shrink to a negligible level."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper systematically critiques the issue of “over-scaffolding” in current code agent evaluation and proposes “lightweight design” as a solution. The perspective is novel, and the approach is validated through multi-benchmark (Polyglot, SWE-Bench) and multi-model (GPT, Claude, Qwen series) experiments, demonstrating that the proposed framework offers better cost efficiency compared to other frameworks. It points out that boosting benchmark scores through complex engineering may obscure the model’s true capabilities, and it establishes a practical foundation for a fairer and more sustainable evaluation paradigm, which serves as an important caution for the community."}, "weaknesses": {"value": "- The Agent Complexity Law is derived purely from experimental observations, lacking formal modeling or theoretical derivation (e.g., convergence conditions, mathematical relationships between complexity measures and performance). Moreover, it aligns closely with common industry understanding and intuition — namely, that models with stronger capabilities (e.g., reasoning, instruction-following) are less affected in performance by different frameworks.\n- The benchmark coverage remains relatively narrow: it does not include more complex software engineering tasks (e.g., multi-module project debugging, cross-file refactoring) or other mainstream agent scenarios. Furthermore, certain comparative baselines are missing — for instance, on SWE-Bench there is no direct comparison with workflow-based paradigms (such as agentless approaches), which weakens the conclusion that the agent paradigm outperforms workflows.\n- The method description is rather general, and the main figure is relatively crude, making it difficult for readers to clearly understand the implementation details of the work."}, "questions": {"value": "- Is there a deeper theoretical explanation for the Agent Complexity Law? Could it be applicable to other, broader domains?\n- Although Lita’s toolset (e.g., Editor, Search) is claimed to be “minimal and necessary,” does its selection still rely on subjective judgment? Have the authors considered using automated methods (e.g., analysis of tool usage frequency) to further optimize the toolset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qCp5a6LgVA", "forum": "Jv7iRfMxkE", "replyto": "Jv7iRfMxkE", "signatures": ["ICLR.cc/2026/Conference/Submission16581/Reviewer_Qj99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16581/Reviewer_Qj99"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761288375965, "cdate": 1761288375965, "tmdate": 1762926658347, "mdate": 1762926658347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper puts forward the idea that current coding agent systems, from more restrictive LLM workflows to more open-ended LLM-based agents are too elaborate and heavily hand-designed for the task or model at hand. This, the authors argue, precludes fair and consistent evaluations between models since various popular scaffolds have been tuned to certain model families and make heavy assumptions about the nature of the task being solved.\n\nThe authors then propose a lightweight agent scaffold, with the goal of enabling a more unified evaluation between models with fewer assumptions made. After translating some common coding benchmarks into a consistent agentic form, the authors run their Lita agent (and variants) and demonstrate good pass rates, and token consumption from this simple agent in comparison to more sophisticated scaffolds.\n\nFor coding agents, they show that having structured file editing tools are beneficial for weaker agents, and the most parsimonious terminal-only agent does suffice, but only on stronger models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper advances a relevant and timely message, which is that on many simple coding and software engineering tasks, the rigid workflows and agent scaffolds that practitioners and researchers alike have used may no longer be acting in our favour as the underlying models are trained to perform these tasks without such heavy-handed assistance. As demonstrated by the results in Table 1, in many cases this can lead to inflated token costs and redundant work.\n\nOn the fairness of model evaluations and agentic performance, the paper also makes good observations, for instance that CodeX and OpenHands' prompts are particularly well suited to GPT-series models from OpenAI and moreover focus on SWE-Bench style tasks quite heavily, which is a somewhat contrived subset of software engineering tasks one might undertake with a coding agent.\n\nThe paper's \"liteness\" metric is sound, and the authors do a good job of explaining why the Lita agent is not merely an open-ended loop with a single Bash-based terminal tool."}, "weaknesses": {"value": "One aspect I believe the paper does not dwell on sufficiently is the acknowledgement of the reasons for which a Lita-style agent does well today, and the potential for a continuing need for scaffolds and hand-crafted prompts. Workflows and highly-tuned tool prompts can indeed alter the performance of a model in an agent system, which is highly necessary to get weaker models to do useful work. It may also enable the generation of synthetic data and traces from which to warm-start the next iteration of the model through behaviour cloning or imitation learning. Ostensibly the reason many scaffolds appear over-engineered and heavy-handed today is because the behaviours these were trying to engender have been trained into the strongest base models by commercial model providers. However, the \"Lita Design Philosophy\" appears to assume that the task at hand closely matches the tasks the underlying model has been extensively trained on. If a model has been trained to write edits using diffs, or to systematically explore a project to gather information before starting a task, or to manage its 'memories', then of course one can (and should) remove prompts and scaffolding intended to induce this behaviour, which are often rigid and brittle. However, if one has a task that the model providers have not yet trained on, or requires more sophisticated and higher-level behaviours, then there is still a case for potentially elaborate prompt-based model behaviour steering and scaffolds.\n\nAnother slight weakness of the paper is proposing that the transformation of common coding benchmarks like HumanEval or SWE-Bench Verified is novel or a core contribution. This has been done by practitioners and researchers as part of running agent evals for at least a year now so the novelty of this claim is low. While I agree that having some agreed-upon and widely used evaluation protocol for agent benchmarks would be useful, the descriptiveness of the transformation in Section 2.3 is somewhat terse and lost in the paper. I suggest weakening the claim of this contribution, or substantiating it with far more detail and examples. This could indeed be a separate paper with a more extensive set of transformations on a large number of benchmarks, with an accompanying repository, and clear prescriptions about how to adapt past single-turn benchmarks as well as guidelines for new benchmarks."}, "questions": {"value": "- Why are the Qwen3-Coder solve rates in Table 2 missing for Lita: is this because these models are not able to successfully perform edits or complete the task without support from a more elaborate scaffold?\n- The types of 'skills' tested by HumanEval, Aider Polyglot and SWE-Bench Verified are firmly in the distribution of tasks the most powerful commercial models are trained on. Do you believe the advice to build a 'Lite agent' still holds for more difficult tasks that span the range of software engineering tasks (conducting research, systems design, implementing new algorithms, fixing merge conflicts, etc)?\n- Do your findings hold outside of coding for LLM agents more broadly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UNDT9X8guv", "forum": "Jv7iRfMxkE", "replyto": "Jv7iRfMxkE", "signatures": ["ICLR.cc/2026/Conference/Submission16581/Reviewer_xjWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16581/Reviewer_xjWm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913564012, "cdate": 1761913564012, "tmdate": 1762926657870, "mdate": 1762926657870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lita, a lightweight coding agent scaffold that aims to provide a simpler and more extensible framework for analyzing model behavior in coding agent scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear: the community currently employs a wide variety of scaffolds, and the lack of disclosure of scaffold details for closed-source models makes fair comparison difficult.\n2. The design strives to minimize prompt complexity, avoiding unnecessary sophistication while effectively reducing model cost."}, "weaknesses": {"value": "1. The novelty of Lita’s design is not sufficiently demonstrated. From the perspective of the toolset, Editor and Terminal are standard tools, while Finish and Search are also common (the former appearing as submit in SWE-Agent). Thus, the toolset definition lacks distinctiveness. Similarly, the memory design shows limited originality. The motivation for introducing an additional reasoning module is unclear; if the underlying model is inherently capable of reasoning, this component may be redundant.\n2. Several claims appear subjective. For example, in CHALLENGE 1: Fairness, the statement that “OpenHands prompts are particularly well suited to GPT-series models, creating hidden advantages” lacks sufficient evidence, as claude-sonnet4 also performs remarkably well under the OpenHands scaffold.\n3. The experiments do not include a comparison with SWE-Agent. Moreover, Mini-SWE-Agent already represents a simpler scaffold and performs comparably to Lita in Table 2, raising the question of whether developing yet another scaffold is necessary."}, "questions": {"value": "please refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A9Mdo0ZEgL", "forum": "Jv7iRfMxkE", "replyto": "Jv7iRfMxkE", "signatures": ["ICLR.cc/2026/Conference/Submission16581/Reviewer_7uPt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16581/Reviewer_7uPt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980965879, "cdate": 1761980965879, "tmdate": 1762926657354, "mdate": 1762926657354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}