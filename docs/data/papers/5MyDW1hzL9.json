{"id": "5MyDW1hzL9", "number": 19520, "cdate": 1758296926010, "mdate": 1759897034785, "content": {"title": "Sampling from multimodal distributions with warm starts", "abstract": "Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. \nIn light of hardness results for sampling---classical MCMC methods, even with tempering, can suffer from exponential mixing times---a natural question\nis how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes.\nFor this problem, we prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. For this, we introduce a modified version of the Annealed Leap-Point Sampler (ALPS). \nSimilarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and \nat the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is critical in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance on a mixture of heavy-tailed distributions, comparing it against the ALPS algorithm on the same distribution.", "tldr": "We formalize the problem of sampling given warm starts and give the first proof for a polynomial-time algorithm in this setting.", "keywords": ["Sampling", "Simulated Tempering"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60c0a37fa18b54fb87eb40b284caa4d503966dfa.pdf", "supplementary_material": "/attachment/a24eed8793a2da98b571b2a0cf48669f8f3d2bef.pdf"}, "replies": [{"content": {"summary": {"value": "This paper provides nonasymptotic analysis to ALPS. The main statement upper-bounds the total number of iterations and the number of samples for sampling from a stationary distribution pi."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Understanding the complexity of sampling from mixture distribution is an important research topic and can have wide applications.\nThe algorithm studied in this framework is practical and when proper assumptions are made, enjoy fast convergence."}, "weaknesses": {"value": "This submission is very difficult to follow. I have the following questions which prevent me from understanding the work well, or giving higher scores. \n\n1. Notations are confusing. Examples:\na. Mixtures are indexed sometimes as k = 1,..., M, or i = 1,...,\nb. g_# is not defined.\nc. Which markov chain is Asumption1.1.4 referring to? Only distributions are defined so far. Is this the Markov chain defined by later algorithms?\n2. Assumptions are not rarely discussed. For example, \na.when does a Markov chain satisfy Asumption1.1.4? Does it hold for any Langevin dynamics? \nb.When can warm starts be achieved? If q_1(x) is Gaussian, and pi_i are some delta distributions, how does ctilt scale with radius and dimension?\nc. when does p_beta,i has better poincare coeff than pi_i? The expression for p_beta,i looks like a simple reweighting rather than smoothing, why would it permit faster mixing?\n3. The algorithm is hard to understand, especially given Alg 2, 3 are in supplementary.\n4. The theorem 3.3 bounds total number of samples, but does it bound total run time? Alg 2 can fail and require rerun. How can one bound number reruns?\n\nAside from all the above questions which stop me from understanding what is going on. I have a hard time positioning this work agains other analysis for sampling from mixture distributions. \n1. Can the authors compare the algorithm against some known ones (e.g. EM with random init + restart?) on some simple settings? When does it lead to better computation / mixing?\n2. Compared to naive Langevin dynamics with temperature ~ epsilon, does the proposed algorithm converge faster?\n3. What are some special distribution familiies for which all assumption are satisfied?\n4. Can it be compared to other nonasymptotic rates for studying mixing on mixture distributions using Markov decomposition theorem?\n\nAt this stage, I cannot confidently judge the quality of the paper, and I think it might be very confusing for general readers as well."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "nC2OUfbGjy", "forum": "5MyDW1hzL9", "replyto": "5MyDW1hzL9", "signatures": ["ICLR.cc/2026/Conference/Submission19520/Reviewer_i8Tb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19520/Reviewer_i8Tb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760616323076, "cdate": 1760616323076, "tmdate": 1762931412712, "mdate": 1762931412712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Sampling method based on ALPS for sampling from multi-modal distributions given a set of \"warm-starts\" $\\{x_i\\}$ (points near the modes). In simulated tempering (or most tempering/annealing type sampling algorithms) sampling from multi-modal distributions is handled by increasing the temperature of the sampling distribution, allowing jumps between modes. Here instead they decrease the temperature of the target distribution so that it becomes very peaked around the warm start points and then provide a way for the sampler to \"teleport\" from between warm start points when at the coldest temperature. This method is an extension of ALPs to deal with distributions where the probability of moving between modes becomes vanishingly small at some temperature levels."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This proposed method directly addresses a weakness of the previous method ALPS and is a novel, if incremental contribution building on ALPS\n- I liked that limitations to their method are discussed in the conclusion with an eye to further work"}, "weaknesses": {"value": "- The distribution in the experiments gets directly at the weakness of the previous method and comparative strength of the present method, but I would have liked to have seen a \"real world\" example where this method is needed.\n- Without reading the appendix following this paper is quite challenging, especially the algorithm which depends heavily on two other algorithms only defined in the appendix.\n- This method seems quite computationally inefficient, needing up to three ALPs runs per main algorithm iteration (if I am understanding correctly that algorithm 2 is run three times in algorithm 3)\n\nSmall issues:\n- Line 371 \"We accompany this provide a lower bound...\""}, "questions": {"value": "Would this also work in the case when j is not uniformly sampled (2.2)?\n\n\"We ran each algorithm for the same amount of time, with the same burn-in length and Metropolis random walk steps.\" Here I presume you mean for the same number of iterations as figure 1 shows, does this include the iterations needed when algorithm [3] runs algorithm [2] multiple times?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AI5xeySXlK", "forum": "5MyDW1hzL9", "replyto": "5MyDW1hzL9", "signatures": ["ICLR.cc/2026/Conference/Submission19520/Reviewer_qZeH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19520/Reviewer_qZeH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761141564669, "cdate": 1761141564669, "tmdate": 1762931412203, "mdate": 1762931412203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a particular scenario for non-log-concave sampling: assuming the target distribution is multimodal but within each mode there is a known point that can be used to serve as a warm start. The paper proposes an algorithm based on simulated tempering with an annealing path that connects the target distribution to a mixture of delta distributions at the known warm start points, and leverages the ALPS algorithm with weight estimation to sample from the target distribution. The authors establishes polynomial mixing time guarantees for the proposed algorithm under certain assumptions that can be satisfied by mixtures of log-smooth densities with Gaussian tempering. There are also experimental results that demonstrate the empirical performance of the proposed algorithm compared to the ALPS algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, although not to easy to read for non-experts, and requires a good understanding of MCMC sampling techniques, especially simulated tempering and ALPS. As I don't have enough knowledge about related works, I cannot fully assess the originality of the paper. The theoretical results in the paper are rigorously presented with detailed proofs in the appendix, although I don't have enough expertise and time to verify all the technical details. The warm start assumption has not been well-studied in the literature, and I believe it is a practical assumption in many real-world applications (e.g., sampling from distributions in statistical physics or molecular dynamics, where a few known samples can be obtained from prior simulations or experiments). Finally, the authors also discuss potential research directions in the conclusion section, which is insightful. I generally think this is a good paper, but I don't have enough confidence to recommend strong acceptance due to my limited knowledge about related works. It's better suited for more theoretical venues like COLT or STOC rather than ICLR."}, "weaknesses": {"value": "As mentioned above, I suggest the authors to provide more context and discussion about related works on non-log-concave sampling, especially those that also consider annealing. Also, many of the details are postponed to the appendix (such as Algorithms 2 and 3, which are important for understanding the proposed method), making it hard to follow. I suggest the authors to combine the pdfs of the main paper and the appendix into a single document for easier reading."}, "questions": {"value": "Some minor comments: on line 161, the $q _ \\beta$'s are actually not Gaussian densities but Gaussian-like, as later in proposition 3.2 $q _ \\beta=\\mathrm{e}^{-\\beta||x||^2/2}$ is considered without normalization, and clearly $q _ 0$ is not the delta distribution. On line 237, $g _ {jj'}(x)$ seems to be $x-x _ j+x _ {j'}$ judging from the definition D.1 and the later usage in proposition 3.2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XV8zz5Y24d", "forum": "5MyDW1hzL9", "replyto": "5MyDW1hzL9", "signatures": ["ICLR.cc/2026/Conference/Submission19520/Reviewer_NdJv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19520/Reviewer_NdJv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964990861, "cdate": 1761964990861, "tmdate": 1762931411897, "mdate": 1762931411897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies sampling from multimodal target distributions when the practitioner is given “weak advice” in the form of one warm-start point per mode. It proposes a modified version of the Annealed Leap-Point Sampler (ALPS) that (i) tilts the target toward a mixture centered at the warm starts, (ii) anneals toward colder distributions, and (iii) performs teleportation between warm-start neighborhoods at the coldest level. Unlike ALPS, the method rebalances both *modal* and *level* weights by estimating component partition functions via Monte Carlo; this is meant to prevent bottlenecks across both modes and temperatures without requiring Hessians or near-Gaussian geometry. The analysis gives what the authors claim to be the first polynomial-time total-variation guarantee for this warm-start setting under a “tilt mass” assumption and local functional inequalities. The proof introduces a “good component” view of the chain, bounds mixing using local Poincare constants and a decomposition theorem, and shows that learned weights keep the projected chain well-conditioned. Experiments on a bimodal Student‑t mixture show higher acceptance rates and ESS than ALPS as dimension grows. The paper is explicit about limitations (e.g., requiring warm starts and some inefficiencies) and suggests a hybrid, more practical variant for future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper clearly positions itself against prior impossibility results without advice and prior asymptotic ALPS analyses, and then supplies a total‑variation guarantee that depends only on local mixing and a warm‑start mass condition, filling a notable theoretical gap. \n* The explicit *component* and *level* balance criteria (eqs. 2.3 and 2.4) plus Monte Carlo estimation of partition functions are simple knobs that directly control flow in the projected chain. This is an elegant explanation of why tempering often fails and how to fix it. \n* The proof technique that analyzes convergence on the “good” mass and quantifies leakage from bad cross‑terms provides an idea that may extend to other multimodal samplers that create cross‑mode interactions. \n* Weaker geometric requirements than ALPS: The method does not need Hessians and is not restricted to near‑Gaussian wells; the heavy‑tailed example aligns with this goal. \n* Clear articulation of limitations and future directions: The conclusion frankly states that the present algorithm is not yet a practical replacement for ALPS and suggests online updates and hybridization."}, "weaknesses": {"value": "* The warm‑start tilt assumption may be strong in practice. The mass condition c_tilt requires that, at small tilt, each component captures a constant fraction of the mass around its warm start. For modes with very different shapes or scales (e.g., narrow vs. flat), c_tilt could be tiny, inflating the polynomial constants and the number of levels. The paper notes separation may be needed in some cases. \n* Proposition 3.2 suggests O(d^2) levels with Gaussian tilts, which could be burdensome. Each level requires Monte Carlo weight estimation; although polynomial, it may be costly for many modes. The paper’s experiments include just one learning step and a small number of modes. \n* The chain within levels is a Metropolis random walk, but the paper does not examine sensitivity to proposal scale, jump and teleportation rates, or ladder spacing beyond one schedule.\n* Section 6 acknowledges the current analysis assumes exactly one warm start per true mode and that every mode has non‑negligible mass at epsilon=0, which may be unrealistic when some modes are tiny but still relevant."}, "questions": {"value": "* Can the authors provide concrete bounds or examples of c_tilt for standard families (mixtures of Gaussians with varying covariances, skewed or heavy‑tailed components) so readers can anticipate how the polynomial depends on geometry? Are there diagnostics a practitioner can compute from data to estimate c_tilt or detect when it is too small? \n* Section 6 hints at adaptive tilts. Could the theory extend to tilts with learned covariances (e.g., local empirical Hessians or covariance estimates around warm starts) to stabilize c_tilt across anisotropic modes? \n* Theorem 3.3 abstracts the Monte Carlo step. How do finite‑sample errors in modal and level weights affect the spectral gap of the projected chain and the final TV bound? Is there a practical rule for choosing the number of samples per level and when to stop re‑estimation? \n* The assumptions require adjacent levels to be close in KL. Can the authors translate this into a practical ladder‑spacing heuristic or an adaptive controller that targets empirical swap rates, similar to parallel tempering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1axrHtQ4OI", "forum": "5MyDW1hzL9", "replyto": "5MyDW1hzL9", "signatures": ["ICLR.cc/2026/Conference/Submission19520/Reviewer_hsfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19520/Reviewer_hsfH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969780238, "cdate": 1761969780238, "tmdate": 1762931411356, "mdate": 1762931411356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}