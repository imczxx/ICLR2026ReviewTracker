{"id": "m00gjQfpCc", "number": 13934, "cdate": 1758225501061, "mdate": 1759897402519, "content": {"title": "Composer: A Search Framework for Hybrid Neural Architecture Design", "abstract": "Hybrid model architectures that combine computational primitives (e.g., Attention, MLP) in different ratios have shown promising performance beyond Transformers. Some studies have shown that different interleavings of primitives can affect model quality as well. However, prior works explore the hybrid model architecture design space manually. Due to the large design space and training costs, discovering hybrid models that combine key computational primitives for pre-training is challenging. In this work, we take a principled approach in designing a modular hybrid model architecture search framework — Composer. Composer explores model architectures at a small scale and extrapolates the top-performing model architectures to a larger scale using our proposed scaling strategies. Using Composer, we discover new hybrid LLM architectures that outperform Llama 3.2. Compared to Llama 3.2 and previous state-of-the-art baselines, the new model architectures consistently reduce validation loss at parameter scales of 350M-3B and improve evaluation accuracy on the downstream tasks by up to 2.8-8.3% (1.1-3.1% on average) while improving both training and inference efficiency.", "tldr": "", "keywords": ["Neural architecture search", "hybrid models", "efficient ML"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7acdcc429610cb3b653955416ea6946f8f0b686.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the limitations of traditional Transformer architectures in large language models (LLMs), which rely on a fixed 1:1 interleaving of self-attention and multi-layer perceptron (MLP) layers.\nThe authors note that hybrid architectures, using varied arrangements of computational primitives such as attention and MLPs, have demonstrated superior performance in prior manual designs.\nHowever, the vast design for a 32-layer model using only attention and MLP—coupled with high training costs, makes manual exploration intractable.\nComposer introduces an automated framework for hybrid neural architecture search (HNAS) tuned to pre-training from scratch, aiming to discover architectures that outperform baselines like Llama 3.2 at scales from 350M to 3B parameters.\n\nThe framework comprises four core components:\n* The Search Engine employs Bayesian Optimization with Gaussian Processes for one-shot searches or incremental methods (end-layer and middle-layer) to explore small-scale models with reduced widths.\n* The Evaluator assesses these candidates using small proxy datasets to provide rapid feedback predictive of large-scale performance.\n* The Aggregator generates top candidates through Nc clustering, selecting primitives layer-by-layer based on conditional frequencies among clustered architectures.\n* The Extrapolator scales up the models to the target size using stacking (repeating blocks) or stretching (proportional expansion of primitive groups).\n\nExperiments yield hybrids surpassing Llama 3.2. The authors remark the benefits of 1:2 attention-to-MLP ratios and non-standard interleavings over the standard 1:1 Transformer structure.\n\nProxy datasets like MAD (and other synthetic datasets like BabiStories) enable efficient evaluation, outperforming downscaled DCLM in cost.\n\nDuring aggregation, Stacking is robust across search depths, but stretching excels for larger small-scale searches (e.g., 16 layers).\n\nThe resulting \"Composite\" architectures, consisting of 1:2 interleavings of Grouped Query Attention and SwiGLU layers, reduce validation loss by 0.05-0.08 and improve downstream task accuracy by 1.1-3.1% on average, while improving training throughput by 1.25x."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a principled, module framework for NAS. The framework's four components (Search Engine, Evaluator, Aggregator, and Extrapolator) provide a blueprint that is extensible to other primitives.\n\nThrough experiments, the author find \"creative\" 1:2 attention-to-MLP ratios and non-standard interleavings that represent useful alternative architectures for future exploration."}, "weaknesses": {"value": "The novelty of the method is minimal, as the Composer framework consists of common building blocks.\n\nOne critical weakness of the paper is that the proposed method proposes no theoretical or empirical justification of optimality. Through experiments, the authors are able to find hybrid architectures that improve upon the LLaMA3.2 baseline, however in the absence of theoretical grounding, we would like to see the Oracle of these experiments, i.e. a large scale experiment to train all candidates to completion, in order to validate the use of proxy evaluations.\n\nSimilarly, there is no evidence that the method scales to models that are larger than 3B parameters, how do the authors justify the use of the Composer framework for these?\n\nThe paper is also limited in scope, since it only mentions in the end, but does not test the method on state-space models, or less conventional attention mechanisms.\n\nEmpirical results, while positive (e.g., 0.05-0.08 validation loss reductions and 1.1-3.1% average downstream accuracy gains), show modest and context-specific advantages. Some variants, like those from downsampled DCLM or BabiStories, only marginally outperform or even underperform Llama 3.2 at higher training budgets."}, "questions": {"value": "Will the code be publicly available?\n\nWhat is the performance of Composer-discovered hybrids on long-context tasks like needle-in-a-haystack or multi-document QA? These tasks have been shown to benefit from attention architecture improvements.\n\nHas the framework been applied to other primitives, such as state-space models (e.g., Mamba) with attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LOu0Bi86KK", "forum": "m00gjQfpCc", "replyto": "m00gjQfpCc", "signatures": ["ICLR.cc/2026/Conference/Submission13934/Reviewer_PWjK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13934/Reviewer_PWjK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831671101, "cdate": 1761831671101, "tmdate": 1762924438410, "mdate": 1762924438410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Composer, a systematic framework for automating the design of hybrid neural architectures that combine computational primitives (e.g., Attention, MLP) in novel ratios and interleavings. Key contributions include: A modular search framework with four core components: HNAS Engine (search algorithm), Evaluator (dataset selection), Aggregator (candidate synthesis), and Extrapolator (scaling to large models) ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Composer addresses a critical gap in hybrid architecture design by automating the exploration of vast design spaces (e.g., 4 billion configurations for 32-layer models) . The integration of modular components (e.g., clustering-based aggregation  and synthetic dataset evaluation) ensures both efficiency and robustness.\n\n2.The proposed stacking and stretching techniques enable seamless scaling of small architectures to large models. For example, stretching 16-layer configurations preserves global interleaving patterns, outperforming stacking in larger models ."}, "weaknesses": {"value": "1. While MAD enables efficient search, its token-manipulation tasks may not fully represent real-world complexity. The paper lacks a comparison with other proxy datasets.\n2. Results are validated up to 3B parameters, but the framework’s applicability to larger models (e.g., 7B+) remains untested. \n3. The paper does not fully analyze the final searched structures, e.g., why specific interleavings (e.g., 1:2 Attention/MLP ratios) outperform others."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ATFTf4MZHD", "forum": "m00gjQfpCc", "replyto": "m00gjQfpCc", "signatures": ["ICLR.cc/2026/Conference/Submission13934/Reviewer_naoJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13934/Reviewer_naoJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897788676, "cdate": 1761897788676, "tmdate": 1762924438014, "mdate": 1762924438014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for optimising the combination of different computational primitives (like attention or MLP blocks) in a neural network, towards hybrid architectures. It uses scaling strategies beyond scaling laws to search on small models that can later be scaled up. They focus on finding architectures for pretraining and ablate over choices for search, evaluation, aggregation and extrapolation in a comprehensive manner. They report consistent improvements in their found hybrid architectures on LLM benchmarks, including improved efficiency and downstream performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experimental methodology is strong, with different components tested in isolation, and the claims of the paper appear well evidenced.\nThe paper is presented well overall, notably figure 1 and the 4 key design questions neatly structure the paper. This paper includes a strong ablation study across several dimensions of the proposed framework, which is presented in a clear structure with highlighted observations/key findings and further discussion.\nThe overall contribution of this paper is to provide a well designed framework for searching and scaling transformer architectures for LLM pretraining that can be useful for the broader ICLR community."}, "weaknesses": {"value": "While the presentation of the method and experiments are good, the main paper is missing a Related Work section, which would significantly help in clarifying the surrounding literature and the contribution of this work, especially compared to the STAR method. Moving this in part from the appendix to the main paper would improve the clarity here.\n\nMore ablation could be done on the clustering method that is claimed to smooth out noise. Only Figure 6 (bottom) and the short discussion on it go into the effect of this clustering approach, which seems to be a core reason for the strong performances achieved. I also found the clustering explanation the hardest section to understand. Perhaps a visual aid could clarify it, e.g. a diagram of N_0 vs N_1 vs N_all or similar.\n\nIssues in text and tables:\n* L265-267: I don’t see how both One-Shot methods have lower search cost than End-Incremental. Figure 2 (left) suggests it is the same for 6-layer and more costly for 16-layer.\n* L320-321: What is meant by better capturing global information here?\n* Key results 1 and 2: Are these results computed across both the Composite - Stacked and Composite - Stretched models? A clarifiction is needed here\n* Table 6, final row reports an incorrect Avg. value for the Composite - Stretched model. It should be 62.0 instead of 66.9. I would point out to make sure the percentage claims in the abstract and elsewhere are not based on this mistake. \n* C.6 Baselines: If I understand correctly, the Striped attention should have its 1:4 ratio as 5x(1A+4M) but it says 5x(1A +2M). Same for 1:8."}, "questions": {"value": "- To quantify the robustness of the Composer framework for other types of pretraining data, it would be good to have an experiment that swaps DCLM for another corpus. Would the models found on different pretraining data also outperform Llama 3.2?\n- How many trials are performed of Bayesian Optimisation for the One-Shot 6-layer setup? The search space is 2^6 = 64 and it seems as 100 trials were performed for the 16-layer setup. Can a comprehensive evaluation of all architectures in the search space be done with an analysis how well the best can be found?\n- What task is the search accuracy evaluated on for Figure 1 (right)?\n- What are the details on the Bayesian Optimisation approach for One-Shot searches? What kernel/acquisition function/hyperparameters? There seems to be no details on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "blwZbmeWOE", "forum": "m00gjQfpCc", "replyto": "m00gjQfpCc", "signatures": ["ICLR.cc/2026/Conference/Submission13934/Reviewer_romG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13934/Reviewer_romG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946378282, "cdate": 1761946378282, "tmdate": 1762924437693, "mdate": 1762924437693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Composer, a modular framework for automatically discovering hybrid large language model architectures that outperform standard Transformers. While standard transformers are composed by a uniform 1:1 ratio of alternating attention and mlp layers, the authors found that a 1:2 ratio is a/ more efficient to compute and b/ (marginally) better performing on multiple choice benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- very relevant topic, other works also propose that the standard 1:1 ratio might be suboptimal\n- LLM's can be more efficient (computation) and potentially more performant.\n- solid framework and experimental design"}, "weaknesses": {"value": "W1 unfortunately only multiple choice downstream benchmarks were tested. you propose to alter 'the core' of current LLM's, it is pretty unclear whether this architecture change will also perform on disciplines as long context, coding, math, extended reasoning, or massive multi-linguality. at least for the final 3b models you should do one more ablation in these directions, baseline vs your 1:2 mix, to also validate this finding still holds.\n\nW2 isn't it correct that your 1b MLP has a different scale-up factors? shouldn't that skew search results?\n\nW3 will you publish your code before the conference?"}, "questions": {"value": "please address above's weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ysudAit547", "forum": "m00gjQfpCc", "replyto": "m00gjQfpCc", "signatures": ["ICLR.cc/2026/Conference/Submission13934/Reviewer_ziXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13934/Reviewer_ziXT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990141286, "cdate": 1761990141286, "tmdate": 1762924437301, "mdate": 1762924437301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Overall, this work addresses important issues in NAS and demonstrates a fairly high level of research, providing extensive experimental results through numerous experiments. The writing is relatively clear, and the technical details are complete, though it lacks deeper theoretical analysis. \n\nIf the theoretical analysis and intuitive understanding of the experimental results and proposed methods could be strengthened, I would be willing to raise the score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This article proposes a search framework for hybrid neural architecture design, conducts a variety of experiments, and achieves significant performance improvements."}, "weaknesses": {"value": "1. The fourth key issue raised by the authors is important among the four issues. This paper introduces the approach of neural architecture extrapolators and empirically demonstrates its effectiveness, but the rationale behind this extrapolation method lacks analysis and justification. For how to extrapolate from smaller-scale networks to larger-scale networks, especially the possible explanation, it is suggested to refer to the following literature for a theoretical explanation.\n\nMathNAS: If Blocks Have a Role in Mathematical Architecture Design, Sihai Zhang et al, NeurIPS, 2023\n\n2. The first key issue raised in this paper is not that important, unless this small-scale model architecture search differs significantly from other small-scale architecture searches when scaling up, which is not discussed in this paper.\n\n3. The second key issue proposed in this paper is also difficult to explain. The paper only completes experimental comparisons and does not, and I believe cannot, provide insightful explanations as to why this dataset is used rather than that one.\n\n4. Perhaps the third key issue raised in this paper is the true contribution of the work. Using the idea of clustering to form the final network seems interesting. However, the related content included in the appendix does not discuss why clustering works. It is suggested that the authors provide an explanation for this."}, "questions": {"value": "1. I feel that the main focus of this work is not clearly highlighted. I suggest the author clarify what the central focus of this work is. Is it the discovered Composite architecture or the proposed Composer framework? \n\n2, Although four key questions are raised, they are not well answered. I recommend that the author provide deeper and more insightful conclusions for one or two of these questions. This might be better.\n\n3. The author claims, 'During search, the HNAS Evaluator trains and evaluates candidate hybrid LLMs with a small dataset to provide fast, reliable signals on the potential quality of the architecture at scale.' Please explain why a small dataset can provide reliable quality signals for large-scale models. How dependent is this signal on the dataset? I am very curious about this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IvrhRpt4F1", "forum": "m00gjQfpCc", "replyto": "m00gjQfpCc", "signatures": ["ICLR.cc/2026/Conference/Submission13934/Reviewer_USsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13934/Reviewer_USsZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173450830, "cdate": 1762173450830, "tmdate": 1762924436818, "mdate": 1762924436818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}