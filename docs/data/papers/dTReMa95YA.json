{"id": "dTReMa95YA", "number": 13613, "cdate": 1758219806911, "mdate": 1763643528237, "content": {"title": "Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval", "abstract": "Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems.", "tldr": "We propose UNITE, a universal multimodal embedder that achieves SOTA performance through strategic modality-aware data curation and training configurations, providing comprehensive insights for multimodal retrieval.", "keywords": ["Universal Multimodal Retrieval", "Information Retrieval", "Multimodal Large Language Model"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9e8f7c237086a7995bc310b979944b9a4c15ad1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses modality heterogeneity and cross-modal alignment in Multimodal Information Retrieval (MIR) by proposing UNITE, a universal embedding framework with two core innovations: systematic modality curation and Modal-Aware Masked Contrastive Learning (MAMCL). Trained via a two-stage pipeline, UNITE achieves SOTA on 40+ tasks and supports simultaneous fine-grained/instruction-based retrieval for text/image/video/fusions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tUNITE achieves leading results across 40+ retrieval tasks (coarse-grained, fine-grained, instruction-based), outperforming both smaller specialized models and larger competitors (e.g., 2B UNITE surpasses 7B VLM2Vec on WebVid-CoVR).\n2.\tSystematic analysis reveals T-V pairs excel in general retrieval and even outperform T-I pairs in image-text tasks—contradicting traditional assumptions and guiding more efficient MIR data curation. It's a interesting finding."}, "weaknesses": {"value": "1.\tInadequate Theoretical and Modal Coverage in Modality Curation. The paper focuses on analysis of T-V pair effectiveness but lacks theoretical explanations for why certain data types (e.g., T-V outperforming T-I in image-text tasks) yield better results. Additionally, while it emphasizes \"curating modality data\" as a core contribution, the analysis is mostly limited to T→V retrieval conclusions. It fails to clarify how other modalities (e.g., text-text, image-text) should be managed—whether they are still mixed as in traditional methods, or if there are optimized proportion strategies?\n2.\tMAMCL’s design lacks sufficient novelty, as prior contrastive learning works (e.g., sampling batches from a single data source to ensure uniform modality) have already addressed cross-modality interference, making MAMCL a similar but not groundbreaking approach. Moreover, MAMCL masks negative examples of different modalities, which wastes a portion of negative samples. Given that the quantity and quality of negative examples are critical for contrastive learning performance, this waste may limit the model’s ability to learn discriminative representations."}, "questions": {"value": "1.\tTable 7 (1→2) shows that MAMCL reduces the performance of WebVid CoVR. Does clarifying this mean that MAMCL will cause losses in specific circumstances?\n2.\tWhat other functions does the conclusion obtained in Analysis 5 have, apart from guiding the addition of T-V data in retrieval adaptation?  Is the data- curation meaningless, merely indicating that the data of TV needs to be included? Because in the end, all the data were mixed together, just like the previous work. Moreover, the proportion of different modal data has not been explored either.\n3.\tThe article does not provide specific prompts for Instruction-based Retrieval. Since instructions determine the tuning results, omitting them will hinder replication. Supplement these in appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4cIWLGkPQs", "forum": "dTReMa95YA", "replyto": "dTReMa95YA", "signatures": ["ICLR.cc/2026/Conference/Submission13613/Reviewer_MRmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13613/Reviewer_MRmf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707934171, "cdate": 1761707934171, "tmdate": 1762924195434, "mdate": 1762924195434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UNITE, a multimodal embedding training paradigm in two stages -- retrieval adaptation followed by instruction tuning, and a loss variant called Modal-Aware Masked Contrastive Learning (MAMCL). \nSpecifically, MAMCL restricts negatives to the same target-modality type to reduce inter-modal interference. \nThe authors emphasize modality data curation (proportions and sequencing of TT/TI/TV pairs) and report state-of-the-art results on instruction-based and fine-grained retrieval benchmarks, notably MMEB and WebVid-CoVR."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Broad generalization and strong performance in video retrieval: the proposed UNITE models perform strong across various retrieval scenarios, tasks, and granularities. On WebVid-CoVR, UNITE_instruct-7B exceeds baselines under their reported settings.\n2. Proper ablations: The paper includes a dedicated MAMCL ablation (Table 7) and a full training-data composition analysis (TT/TI/TV mix, under fixed data budget)."}, "weaknesses": {"value": "1. Marginal performance of the MAMCL component: while MAMCL is conceptually sound, its average gains are small (about +0.3 overall on MMEB, avg of +0.5 on WebVid-CoVR with 7B parameters), and it can trade off specific metrics (e.g., CoVR R@5 at 7B). I recommend deeper analysis on when/why it helps.\n2. Lack of efficiency analysis: MAMCL changes the effective negative set via a modality mask, but the paper does not report compute comparisons to standard InfoNCE; only high-level training setup (e.g., 64×A100, single-epoch runs, time) is provided. Adding a theoretical analysis or a wall-clock comparison would strengthen this paper."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WBa265dzpW", "forum": "dTReMa95YA", "replyto": "dTReMa95YA", "signatures": ["ICLR.cc/2026/Conference/Submission13613/Reviewer_oVez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13613/Reviewer_oVez"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791367405, "cdate": 1761791367405, "tmdate": 1762924195161, "mdate": 1762924195161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UNITE, a framework for building unified embeddings for multimodal information retrieval, supporting text, images, videos, and their combinations. A key contribution is the introduction of MAMCL (Masked Contrastive Learning), which mitigates interference by masking contrastive terms between candidates of different modality types, thereby improving cross-modal alignment in the shared embedding space. The training methodology is structured into two stages: a retrieval adaptation phase, which leverages diverse modality pairs to construct a robust embedding space, and an instruction tuning phase, where the model is fine-tuned on instruction-based datasets to handle more complex and nuanced retrieval queries. Experiments on multiple benchmarks show consistent improvements, and ablation studies confirm MAMCL’s effectiveness across various instruction-based retrieval tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel Modality-Aware Masked Contrastive Learning (MAMCL) approach that extends contrastive learning to better accommodate heterogeneous modalities. The framework's ability to integrate video retrieval alongside text and image retrieval broadens its applicability and underscores the model's versatility in handling complex multimodal data.\nComprehensive evaluations across diverse multimodal benchmarks substantiate the benefits of both MAMCL and the modality-aware data design. The inclusion of both in-distribution and out-of-distribution analyses strengthens the empirical validity of the claims.\nThe paper is generally well-structured and clearly written. The motivation, methodology, and experimental results are communicated with precision, and the technical formulations are clearly presented and easy to follow. While some training details are appropriately included in the appendix, certain key aspects—such as the specific large language model (LLM) used and the preprocessing steps for each modality, particularly for video—would be better placed in the main body to improve transparency and reproducibility.\nThis work makes a notable contribution by proposing the MAMCL approach, which mitigates cross-modal interference and enhances alignment across heterogeneous modalities. The proposed framework advances efforts toward building universal multimodal embeddings that can jointly handle text, image, and video retrieval tasks. The results demonstrate clear improvements on several benchmarks, indicating the approach's practical value and potential for broader application."}, "weaknesses": {"value": "While the paper presents a well-motivated and empirically supported approach, several aspects could be strengthened to enhance its clarity and overall impact.\n1- Frozen projector and vision encoder:\nThe authors freeze the projector and vision encoder, but do not analyze the implications of this choice. It remains unclear how fine-tuning these components—particularly during instruction tuning—might affect multimodal alignment and retrieval performance. An ablation study comparing frozen versus trainable projectors would provide valuable insight into the trade-off between stability and adaptability, strengthening the paper’s empirical analysis.\n2- Limited model diversity:\nExtending experiments to other state-of-the-art vision–language models for multimodal information retrieval would provide more substantial evidence of robustness and generality, and help disentangle the contribution of MAMCL from the underlying model architecture.\n3- Suboptimal video sampling strategy\nThe paper employs a uniform frame sampling rate of one frame per second for the video modality, which may be too coarse to capture meaningful temporal dynamics. This approach could overlook key motion cues or fine-grained visual changes that are crucial for accurate retrieval. Exploring more efficient or adaptive sampling strategies could improve video representation quality and strengthen overall multimodal retrieval performance.\n4- Lack of qualitative success and failure examples:\nThe paper focuses heavily on quantitative results but omits illustrative examples of both successful and failed retrieval cases. Including such visual or textual samples would help readers better understand the model’s strengths, limitations, and failure patterns—particularly for instruction-based or fine-grained retrieval scenarios."}, "questions": {"value": "- Have the authors considered conducting an ablation study with the projector unfrozen during training to examine its effect on learning dynamics, stability, and retrieval performance?\n- Since MAMCL is presented as a general loss function, could the authors clarify why it was evaluated only on Qwen2-VL-2B?\nDo you expect similar performance trends if integrated into other VLMs such as CLIP, BLIP-2, or LLaVA?\nIf computational limits prevented broader testing, could you provide reasoning or partial results indicating its generalizability?\n- Have the authors considered testing alternative video sampling strategies (e.g., motion-based, content-aware, or adaptive sampling) instead of the fixed rate of one frame per second to evaluate their effect on retrieval accuracy and temporal representation quality?\n- Would it be possible to include qualitative examples of both successful and failed retrieval cases (especially for instruction-based tasks)? Such examples could clarify where MAMCL contributes most effectively and where it struggles—insights that would be very helpful to the community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YewjfcxBld", "forum": "dTReMa95YA", "replyto": "dTReMa95YA", "signatures": ["ICLR.cc/2026/Conference/Submission13613/Reviewer_bQSb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13613/Reviewer_bQSb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145242909, "cdate": 1762145242909, "tmdate": 1762924194693, "mdate": 1762924194693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a training strategy for large multimodal models (LMMs) for the task of cross-modal retrieval. The proposed strategy involves a masking matrix that informs the loss function about the type of data modality involved in the data used to calculate the contrastive objective. In addition, the paper conducts experiments that evaluate the importance of different data compositions for training, identifying data curation regimes that improve performance. The experimental results indicate that performance improves in various tasks and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Comprehensive evaluation across datasets and tasks.\n* Simple, yet effective strategy.\n* Good experimental results that indicate effectiveness of the proposed strategy.\n* Generally speaking the study seems well designed and well conducted."}, "weaknesses": {"value": "* Writing style is too distracting. Every time the paper indicates that something is critical or crucial, it is not.\n* The contribution seems incremental with tweaks to existing models and based primarily in data curation.\n* The paper reports results that they say contradicts previous observations, but no reference results or citations are provided. \n* The interpretation of results and the insights is limited to highlighting numeric differences."}, "questions": {"value": "* Are the results really surprising? How can these observations be explained beyond performance differences? Any hypotheses that can be tested or just black box performance differences that cannot be explained?\n* Are the data curation results just a selection of data useful to solve the benchmarks rather than learning generalizable reasoning to match content?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cQhejXBr28", "forum": "dTReMa95YA", "replyto": "dTReMa95YA", "signatures": ["ICLR.cc/2026/Conference/Submission13613/Reviewer_mqhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13613/Reviewer_mqhb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762360939235, "cdate": 1762360939235, "tmdate": 1762924194158, "mdate": 1762924194158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}