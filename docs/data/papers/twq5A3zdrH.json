{"id": "twq5A3zdrH", "number": 21069, "cdate": 1758313431117, "mdate": 1759896943694, "content": {"title": "Context-aware Heterogeneous Graph-driven Multimodal Representation Learning for Emotion Recognition", "abstract": "Multimodal emotion recognition (MER) aims to infer human affect from verbal, vocal, and visual signals, a core challenge in representation learning for human–AI interaction. State-of-the-art approaches, including standard Transformers and graph-based models, often collapse modalities into uniform structures, ignoring modality-specific temporal dynamics and asymmetric dependencies. We propose a novel context-aware heterogeneous graph-driven representation learning that explicitly encodes both structural and semantic heterogeneity. Each modality is first contextualized with dedicated Transformer encoders, enriching unimodal features before graph construction. We then introduce a relation-aware graph transformer that performs type-conditioned message passing, enabling specialized transformations across sequential, cross-modal, and speaker-conditioned edges. The topology is adapted to the target regime: in multi-party dialogue (IEMOCAP, MELD), we distinguish within-speaker and cross-speaker temporal flows, while in single-speaker videos (CMU-MOSEI), we extend k-step temporal links to capture offset dynamics. In both settings, co-temporal edges synchronize audio, visual, and textual cues. Experiments demonstrate consistent gains over prior state-of-the-art, showing that structural and semantic heterogeneity are indispensable for robust multimodal representation learning. Our results establish that explicitly modeling interaction structure, rather than relying on generic sequence attention, is critical for advancing multimodal learning. To support reproducibility and further research, we will release our source code.", "tldr": "", "keywords": ["Multimodal Emotion Recognition", "Heterogeneous Graph Neural Network", "Context-aware Graph Transformer", "Graph-driven Multimodal Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bc5ba06713a18d5c607fa86e8c5be53f11fde3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a heterogeneous graph-based multimodal emotion recognition model that connects textual, acoustic, and visual features through relation-aware attention across temporal and cross-modal links. It effectively supports both single- and multi-speaker dialogues and achieves state-of-the-art results on the IEMOCAP and CMU-MOSEI datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is sound, as the model incorporates typed temporal, cross-modal, and relation-conditioned projections shared across layers. This design goes beyond conventional homogeneous GAT/GNN fusion and standard sequence attention mechanisms.\n2. The framework is versatile, effectively supporting both multi-speaker and single-speaker scenarios.\n3. The proposed method achieves state-of-the-art performance on the IEMOCAP and CMU-MOSEI datasets."}, "weaknesses": {"value": "1. This work differs from previous studies in that it treats each modality within every utterance as individual graph nodes for attention learning. However, this design naturally leads to excessive information aggregation, which may dilute the model’s focus on important nodes or modalities. Moreover, it can cause the model to become overly dependent on these aggregated features. In real-world scenarios, modalities are often missing or ambiguous, which could easily lead to model failure.\n2. The main innovation lies in the design of the heterogeneous graph and the relation-aware attention mechanism. The former simply integrates multiple modality nodes into a unified graph, while the latter modifies the standard GAT input from homogeneous to heterogeneous. These adjustments cannot be considered major innovations. I would expect to see more originality in the graph structure itself, rather than merely combining all data into a single graph and allowing the model to learn attention automatically. Furthermore, the performance improvement is marginal, 0.4 on IEMOCAP and 0.6 on CMU-MOSEI, with no SOTA on MELD, and the absence of statistical analysis makes it difficult to justify the claimed contributions.\n3. Figure 1 and Figure 2 essentially illustrate the same concept, and there is no need to include both. The authors should focus more on explaining the proposed method in depth. For instance, the most critical component, the Heterogeneous Graph Encoder, is only described in half a page. Additionally, the authors should elaborate on the originality and underlying principles of the proposed graph structure. In MERC, the receptive field of the graph is crucial; when handling long conversations, the model should pay more attention to nearby utterances, but the paper fails to explain how this is achieved."}, "questions": {"value": "1. Provide a more detailed explanation of the advantages of the proposed graph structure, including why it outperforms existing graph designs and why it is necessary. For example, it would be helpful to illustrate how the model differentiates its attention toward heterogeneous information and how it effectively mitigates issues of information redundancy and dilution.\n2. Explain how the model handles long-term conversational dependencies and assess its robustness to missing modalities. Such clarification would enhance the understanding of the model’s applicability to real-world, imperfect multimodal data.\n3. The authors claim that the proposed model offers interpretability. It would significantly strengthen the paper if they could provide concrete examples, such as how the model identifies and prioritizes key information within a dialogue, to illustrate the interpretive capacity and validate the effectiveness of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u4XUsngBln", "forum": "twq5A3zdrH", "replyto": "twq5A3zdrH", "signatures": ["ICLR.cc/2026/Conference/Submission21069/Reviewer_rnw7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21069/Reviewer_rnw7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761302272514, "cdate": 1761302272514, "tmdate": 1762940639217, "mdate": 1762940639217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that existing methods overlook modality-specific temporal dynamics and asymmetric dependencies. To address these issues, the authors propose a heterogeneous GNN that encodes both structural and semantic heterogeneity. Experiments on IEMOCAP, MELD, and CMU-MOSEI demonstrate state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of the paper is interesting, and the authors correctly identify that existing methods fail to consider all relevant factors within a unified framework. We find the motivation well-grounded.\n\n2. The analysis of the experimental results is concise and clearly presented.\n\n3. The conclusion provides insightful directions for future work."}, "weaknesses": {"value": "1. From the Introduction section, the research motivation of this paper appears to be incremental — aiming to jointly consider both structural and semantic heterogeneity. However, the paper mainly redefines the types and updating mechanisms of edges within a heterogeneous network, without departing from the conventional heterogeneous-graph-based framework for multimodal emotion recognition. Therefore, we believe the novelty of this work is limited.\n\n2. The Methodology section is unclear and lacks a detailed description of the training loss, which we consider an essential part of the method.\n\n3. The paper is missing crucial ablation studies. It does not provide evidence showing whether the model benefits from the proposed structural heterogeneity or from the semantic heterogeneity. In particular, an ablation analysis on the edge construction in Figure 2 is absent.\n\n4. The paper lacks a comparison and analysis of time complexity. Since the authors employ both Transformer and GNN components, we suspect that the proposed approach may not scale well to large models. Including comparisons of runtime or efficiency with existing methods would significantly strengthen the work.\n\n5. The reported results are not as strong as those of GraphSmile, and on the IEMOCAP dataset, some results even fall behind the baselines. Thus, the authors’ claim of achieving state-of-the-art performance seems overstated."}, "questions": {"value": "Please refer to the above-mentioned weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3Ag9dkpaa5", "forum": "twq5A3zdrH", "replyto": "twq5A3zdrH", "signatures": ["ICLR.cc/2026/Conference/Submission21069/Reviewer_yZQF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21069/Reviewer_yZQF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620625203, "cdate": 1761620625203, "tmdate": 1762940638835, "mdate": 1762940638835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a context-aware heterogeneous graph framework for multimodal emotion recognition (MER). \nThe authors argue that existing models fail to distinguish between different types of interactions\nThe proposed method first uses standard Transformer encoders to contextualize each modality independently.\nThen, it constructs a heterogeneous graph where nodes represent modal features and edges are typed to distinguish between temporal, cross-modal, and speaker-conditioned links .   \nA relation-aware graph transformer performs message passing using different transformations for each edge type.\nThe model adapts its topology for multi-party dialogue versus single-speaker monologues."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Transformers and homogeneous GNNs treat all interactions uniformly. Explicitly modeling different relationship types as a heterogeneous graph is a well-motivated approach\n\n2. The model's design cleverly adapts to the input data. It uses speaker-aware edges for multi-party dialogues and k-step temporal edges for monologues, showing flexibility.\n\n3. The paper provides useful ablation studies."}, "weaknesses": {"value": "1. The core relation-aware graph transformer is close to a standard implementation of a Heterogeneous Graph Transformer, reduce the novelty.\n\n2. The paper notes it is competitive but below GraphSmile for emotion (Table 2) and slightly below DialogueCRN for sentiment (Table 4).     This undermines the claim of superiority and suggests the model may not fully handle the multi-party complexity.\n\n3. The model is a two-stage pipeline: a stack of Transformer encoders (one for each modality) followed by a stack of heterogeneous graph layers. This serial design is parameter-heavy and computationally complex compared to more integrated approaches.\n\n4. The ablation study shows that removing the unimodal Transformer pre-processing causes a catastrophic performance drop.  For example, on CMU-MOSEI sentiment, accuracy falls from 66.84 to 52.39. This suggests that the initial Transformer encoders are responsible for the vast majority of the performance, and the heterogeneous graph component may only provide a marginal improvement, reducing its effectiveness.\n\n5. The t-SNE visualizations do not convincingly show a major improvement from the graph. \n\n6. This late-fusion step is simple and may be a bottleneck, as it does not allow for higher-order interactions between the contextualized modal embeddings before classification."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PcZQPDYdEA", "forum": "twq5A3zdrH", "replyto": "twq5A3zdrH", "signatures": ["ICLR.cc/2026/Conference/Submission21069/Reviewer_ch2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21069/Reviewer_ch2m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633214800, "cdate": 1761633214800, "tmdate": 1762940638360, "mdate": 1762940638360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses multimodal emotion recognition (MER) challenges where existing models ignore modality-specific dynamics and asymmetric dependencies. It proposes a context-aware heterogeneous graph-driven method: using dedicated Transformer encoders for unimodal features, a relation-aware graph transformer for message passing, and adaptive topology for different datasets. Experiments show it outperforms state-of-the-art, verifying heterogeneity’s necessity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The results of the experiment are promising."}, "weaknesses": {"value": "1. The proposed relation-aware attention mechanism differs mathematically and conceptually from previous works such as HMG-Emo or HHGN.\n2. Are there theoretical guarantees or formal proofs supporting the claim that the model better captures structural and semantic heterogeneity?\n3. While the paper consistently outperforms transformer-based (e.g., Joyful, SACL-LSTM) and dialogue-specific baselines (e.g., DialogueCRN, MMGCN), the margin of improvement over graph-based methods like GraphSmile and HHGN is relatively small in certain benchmarks (e.g., MELD).\n4. Extending the experimental evaluation to other multimodal sentiment datasets (such as EmoReact or MuSE) or missing modality scenarios will further validate the generalization ability of the proposed method."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hq8JH15f8I", "forum": "twq5A3zdrH", "replyto": "twq5A3zdrH", "signatures": ["ICLR.cc/2026/Conference/Submission21069/Reviewer_rNLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21069/Reviewer_rNLE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895777092, "cdate": 1761895777092, "tmdate": 1762940637952, "mdate": 1762940637952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}