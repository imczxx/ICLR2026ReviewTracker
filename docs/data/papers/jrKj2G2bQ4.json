{"id": "jrKj2G2bQ4", "number": 871, "cdate": 1756821611777, "mdate": 1759898237896, "content": {"title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding", "abstract": "Vision‚Äìlanguage alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs.", "tldr": "", "keywords": ["Mixup", "Augmentation", "MLLM", "Image Classification", "Visual Alignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0012d8f7d4d480bc14c71dc5b8cd9bcfe7331ab9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MergeMix, a unified augmentation framework for visual and multimodal learning. It combines token merging and mixup to create attention-aware mixed images, which are then used to build preference pairs for training. The model treats clean samples as ‚Äúwinners‚Äù and mixed ones as ‚Äúlosers,‚Äù optimized with the SimPO loss to bridge supervised fine-tuning (SFT) and reinforcement learning (RL). Experiments on image classification (CIFAR100, ImageNet-1K, Stanford Cars) and MLLM benchmarks (LLaVA, Qwen2.5-VL) show that MergeMix improves accuracy, calibration, and training efficiency compared with existing mixup and alignment methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a unified augmentation framework that connects supervised fine-tuning and preference optimization.\n\n- The idea of using token merging to generate mixed samples is a reasonable extension of existing mixup methods and leverages attention information effectively.\n\n- Experiments cover both image classification and multimodal benchmarks, showing consistent  improvements over baselines such as CutMix, MixPro, and SeVa.\n\n- The method achieves competitive performance with lower computational cost and good training efficiency, suggesting potential for practical use."}, "weaknesses": {"value": "- Incremental Overlap with Prior Work: While token merging and mixup are cleverly combined, the conceptual novelty may be perceived as moderate since both techniques are pre-existing; the innovation mainly lies in their integration.\n\n- Marginal Multimodal Gains: Improvements on LLaVA benchmarks (0.8%) and Qwen2.5-VL (2.9%) are positive but not substantial, raising questions about statistical significance."}, "questions": {"value": "- 1.Comparison with RL-based Preference Methods: Since MergeMix aims to bridge SFT and RL, it would be helpful to include direct comparisons with RL-based methods such as PPO or GRPO on MLLM benchmarks to quantify the alignment improvement.\n\n- 2.Statistical Significance of Gains: Many reported improvements (e.g., +0.8% on LLaVA) are relatively small. Are these consistent across multiple random seeds? Please report standard deviations or significance tests to confirm reliability.\n\n- 3.Generality Beyond ViT-based Models: MergeMix is mainly evaluated with ViT and DeiT architectures. Could the authors test it on convolutional backbones or hybrid models to verify broader applicability?\n\n- 4.Computational Overhead of Token Merge: Although the paper claims better efficiency, the token merging and reconstruction steps may introduce overhead. Can the authors provide a detailed runtime breakdown to clarify the trade-offs?\n\n- 5.Visualization and Qualitative Analysis: Including visual examples of merged attention maps or mixed images could help readers understand what semantic information MergeMix preserves compared to conventional mixup.\n\nI will adjust the score based on the authors‚Äô response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XyR3vCdSrv", "forum": "jrKj2G2bQ4", "replyto": "jrKj2G2bQ4", "signatures": ["ICLR.cc/2026/Conference/Submission871/Reviewer_eiyX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission871/Reviewer_eiyX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713345108, "cdate": 1761713345108, "tmdate": 1762915633922, "mdate": 1762915633922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MergeMix, a unified augmentation paradigm for visual and multi-modal understanding. MergeMix builds on the concept of image mixing, specifically leveraging token merge strategies to generate mixed images and preference pairs for model alignment. The approach integrates ideas from supervised fine-tuning and preference-based optimization, aiming to improve both efficiency and robustness in multi-modal large language models (MLLMs) and image classification tasks. Experimental results show competitive performance across several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Quality: The experimental setup is rigorous, with thorough benchmarking and ablation studies.\n2. Clarity: The methodology and results are clearly explained.\n3. Significance: The approach demonstrates practical improvements in accuracy, calibration, and efficiency.\n4. Applicability: MergeMix is shown to be effective across both image classification and multi-modal tasks."}, "weaknesses": {"value": "1. Limited impact on multi-modal tasks: The gains for MLLMs are marginal, suggesting the method‚Äôs strengths are domain-specific.\n2. Outdated baselines: Most compared methods in classification tasks are from two years ago, which may not represent the latest advances.\n3. Scope of contribution: The paper could better clarify its impact boundaries.\n4. Discussion of limitations: More explicit discussion of why the method is less effective for multi-modal tasks and why recent baselines were not included."}, "questions": {"value": "1. Recency of baselines: Why were recent state-of-the-art methods not included in the classification comparisons? Can the authors provide results against more current baselines?\n2. Domain-specific effectiveness: Why is MergeMix more effective for classification than for multi-modal tasks?\n3. Future directions: What modifications might enhance MergeMix for multi-modal models?\n4. Broader applicability: Are there other domains (audio, video) where MergeMix could be tested?\n5. Limitations: Please discuss scenarios where MergeMix may not be suitable or where its benefits are minimal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cExZdJ5PlQ", "forum": "jrKj2G2bQ4", "replyto": "jrKj2G2bQ4", "signatures": ["ICLR.cc/2026/Conference/Submission871/Reviewer_poJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission871/Reviewer_poJM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751474969, "cdate": 1761751474969, "tmdate": 1762915633668, "mdate": 1762915633668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel image-data augmentation method, **MergeMix**, which first produces clustered attention maps by Token-Merge and then mixes the important visual tokens rather than raw pixels, yielding semantically smoother synthetic images. \nThe authors embed this augmentation into two tasks:\nImage classification: MergeMix is used to train ViT/DeiT models \n  Results: +2.5 % accuracy vs. the best prior mixup, **15 % higher throughput**, **‚Äì0.68 G FLOPs**, and lowest calibration error (ECE) under severe occlusion.\nMLLM alignment: clean images are treated as winners and MergeMix images as *losers*; a SimPO ranking loss is added to SFT, without any reward model**.\n  Results: LLaVA-7B gains +0.83 % on nine VQA/understanding benchmarks; Qwen2.5-VL-7B gains +2.88 % on reasoning sets, while vision tokens can be reduced to 25 % without performance drop.\nOverall, MergeMix provides a unified, reward-free training paradigm that simultaneously improves accuracy, efficiency and calibration for both pure-vision and multi-modal models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A novel image mixing augmentation method is proposed, which demonstrates significant improvements across multiple datasets.\n\n* The mixed images are directly used as the \"loser\" in a pairwise ranking setup via SimPO, eliminating the cost and potential bias of training a separate Reward Model (RM) and simplifying the pipeline.\n\n* Extensive experiments are conducted, providing multi-faceted validation of the method's effectiveness."}, "weaknesses": {"value": "* The application of MergeMix to image classification and MLLM alignment tasks shows some innovation, but the degree of novelty is limited.\n\n* The assumption that attention-based merged images are inherently of lower quality than original images lacks substantiating evidence. \nWhile the paper discusses the method from an MLLM perspective, validation is only conducted in the visual modality, leaving its efficacy in other modalities unexplored.\n\n* The performance drop on MMBench and MathVista after MergeMix compared to SFT results suggests that the visual enhancement method is not universally effective.\n\n* Although MergeMix employs token compression, it does not enhance model inference efficiency. Yet, the paper incorrectly claims this as a merit of the method."}, "questions": {"value": "* What is the specific mechanistic link between the ViT-enhancing visual mixing method and the broader objective of preference alignment, as discussed in the paper?\n\n* What evidence can substantiate that the attention-based mixed regions genuinely correspond to semantically \"more important\" objects within the images?\n\n* How can it be guaranteed that training the vision encoder of VLM with MergeMix does not adversely impact the model's capabilities in other tasks or modalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kVDlJ7B0xC", "forum": "jrKj2G2bQ4", "replyto": "jrKj2G2bQ4", "signatures": ["ICLR.cc/2026/Conference/Submission871/Reviewer_dRWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission871/Reviewer_dRWW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762451042433, "cdate": 1762451042433, "tmdate": 1762915633334, "mdate": 1762915633334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MergeMix, a training strategy combining token merging and mixup to bridge supervised fine-tuning (SFT) and reinforcement learning (RL) for both image classification and multi-modal large language. The key idea is to merge semantically similar vision tokens before applying mixup, thereby reducing redundant visual tokens while preserving spatial semantics, and then form preference pairs (winner/loser) for preference optimization via the SimPO loss, enabling preference alignment without reward models.  The authors demonstrate improvements on CIFAR100, ImageNet-1K, and multiple MLLM benchmarks (LLaVA, Qwen2.5-VL), with notable efficiency and calibration benefits."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The introduction of a token merge‚Äìbased mixing mechanism and attention recovery via bipartite soft matching is novel within the mixup literature. Compared to heuristic or random masking, the method provides a more structured way to preserve salient regions during interpolation.\n2.The token-merge + mixup combination is reasonable, and the design (Top-K attention selection, Œª re-scaling, ranking loss) can be integrated into existing ViT and MLLM frameworks with minimal modifications.The authors demonstrate that MergeMix can reduce FLOPs and slightly improve throughput, confirming that the design is at least practically implementable."}, "weaknesses": {"value": "1.The paper‚Äôs organization hinders readability. The introduction directly dives into technical detail without motivating the gap, and the related work section is largely enumerative rather than analytical. Notation is inconsistent and transitions are abrupt, making it difficult to follow the method‚Äôs rationale.\n2.The paper mixes several technical ideas‚Äîtoken merging, mixup, Œª re-scaling, and ranking loss‚Äîwithout a clear unifying formulation.\nIt is unclear how the policy P(¬∑,¬∑) determines masks, how ŒªÃÇ interacts with attention, or how the ranking loss relates to reward modeling.The method section reads as a collection of components rather than a coherent algorithmic framework.\n3.The paper‚Äôs central claim is that MergeMix bridges SFT and RL paradigms, but the experiments do not provide direct or conceptual evidence of this.There are no comparisons with preference optimization methods such as DPO, PPO, or SimPO; no analysis of reward-like behavior; and no ablation isolating the contribution of the ranking loss.Consequently, the connection to RL remains metaphorical rather than empirical."}, "questions": {"value": "1.Since the paper claims to bridge SFT and RL through preference-style optimization, have you compared MergeMix with established preference optimization methods such as DPO, PPO, or SimPO? Such a comparison would be essential to substantiate the claimed ‚Äúbridge‚Äù between SFT and RLHF paradigms.\n2.The paper defines ŒªÃÇ as derived from a policy ùëÉ, but the implementation details remain vague. Is ŒªÃÇ obtained from attention scores, gradients, or optimized independently? How sensitive are results to this design choice?\n3.How exactly does the ranking loss applied to synthetic mixed pairs emulate human preference learning? Does the ‚Äúpreference‚Äù in this context reflect semantic quality, output diversity, or another measurable signal?\n4.Could you report standard deviations across multiple runs and clarify whether all baselines share identical training setups (e.g., frozen encoders, same number of epochs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "80X04VOQrs", "forum": "jrKj2G2bQ4", "replyto": "jrKj2G2bQ4", "signatures": ["ICLR.cc/2026/Conference/Submission871/Reviewer_nEzW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission871/Reviewer_nEzW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762780813691, "cdate": 1762780813691, "tmdate": 1762915633194, "mdate": 1762915633194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}