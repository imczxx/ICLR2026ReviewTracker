{"id": "ADRwyhQWzY", "number": 5620, "cdate": 1757923498509, "mdate": 1759897964571, "content": {"title": "Bayesian Speech Synthesisers Can Learn from Multiple Teachers", "abstract": "Codec-based text-to-speech (TTS) models have recently gained traction for their efficiency and strong performance in voice cloning. However, codec-based TTS faces limitations due to the challenges of pretraining robust speech codecs and the quality degradation introduced by quantization errors. Emerging evidence suggests that continuous-valued generative models can alleviate these issues and serve as a promising alternative. Yet, effectively modelling diverse speech patterns and developing reliable sampling strategies for continuous-valued autoregressive (AR) TTS remains underexplored. In this work, we propose BELLE, \\textbf{B}ayesian \\textbf{e}vidential \\textbf{l}earning with \\textbf{l}anguag\\textbf{e} modelling for TTS, a novel continuous-valued AR framework that directly predicts mel-spectrograms from textual input. BELLE treats each mel-spectrogram frame as a Gaussian distribution sampled from a learned hyper-distribution, enabling principled uncertainty estimation, particularly in scenarios with parallel data (\\textit{i.e.}, one text-audio prompt paired with multiple speech samples). To obtain such data, diverse speech samples are synthesized using multiple pre-trained TTS models given the same text-audio prompts, which are distilled into BELLE via Bayesian evidential learning. Experimental results indicate that BELLE demonstrates highly competitive performance compared with the current best open-source TTS models, even though BELLE is trained on a large amount of synthetic data and uses only approximately one-tenth of their training data. Audio samples generated by BELLE are available at \\url{https://belletts.github.io/Belle/}. The code, checkpoints, and synthetic data will be released after the paper is accepted.", "tldr": "", "keywords": ["TTS", "Bayesian Evidential Learning", "Multi-teacher Distillation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9eec067f4459917011d764029e07375e3de692c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents BELLE, a continuous-valued autoregressive TTS that predicts mel-spectrograms with evidential deep learning (EDL). Each mel frame is modeled as Gaussian whose mean/variance are sampled from a Normal–Inverse-Gamma (NIG) hyper-distribution, yielding a Student-t predictive and controllable sampling (via $\\beta$). The architecture uses a decoder-only Transformer with a sampling module predicting NIG parameters, plus a postnet and stop head; a streaming variant (BELLE-stream) interleaves chunked text/audio for low latency. To address the single-text–single-audio limitation, the authors introduce multi-teacher, multi-reference training: for each text, multiple audios synthesized by six open-source TTS models are combined with the human recording, and the model optimizes a weighted sum of losses over all references. Trained on 706h human + synthetic data totaling ~4,817h, BELLE achieves MOS close to ground truth and competitive/better WER and speaker similarity vs strong baselines, while outperforming MELLE (Gaussian sampling). Diversity is tunable via $\\beta$; BELLE-stream reaches RTF ≈ 0.55 and ~440 ms first-packet latency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation for evidential sampling in continuous-valued AR TTS is clear and practical. Specifically,\n1) EDL provides a principled way to model both parameter uncertainty and data uncertainty, enabling $\\beta$-controlled sampling and more calibrated stochasticity than a plain single-Gaussian sampler.  \n2) Multi-teacher multi-reference training directly addresses single-text–single-audio limitations; more effective than naive mixing."}, "weaknesses": {"value": "1) While the engineering motivation for EDL is solid, the paper lacks deeper theoretical analysis comparing EDL vs Gaussian sampling.  \n2) \"Distillation\" terminology is imprecise: The proposed \"multi-teacher distillation\" is closer to multi-reference supervision or dataset-level ensembling (computing full losses per teacher sample and weighting) rather than classical KD (no soft-target/logit or feature-matching loss, no teacher–student consistency regularization).  \n3) An independent Gaussian per log mel bin is a useful approximation, but true distributions are often much more complex. Dropping cross band and temporal covariance may weaken uncertainty calibration and physical consistency.  \n4) Beyond swapping the per-bin Gaussian head for an evidential NIG head, the paper offers few orthogonal advances over MELLE; as a result, the perceived novelty rests largely on the likelihood change."}, "questions": {"value": "1) In MELLE, the reported WERs for the MELLE-limited model (trained only on 960h LibriSpeech) are 1.53 (WER_C, Continuation) and 2.21 (WER_C, Cross-Sentence). In your paper, the reproduced MELLE baselines trained with more data perform noticeably worse on the same WER_C metrics. What factors account for this degradation despite using more data?  \n2) Since increasing $\\beta$ improves diversity by inflating the NIG variance, how does this affect objective and subjective quality? Do WER/SIM/MOS change systematically with $\\beta$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "caRlUC3uCf", "forum": "ADRwyhQWzY", "replyto": "ADRwyhQWzY", "signatures": ["ICLR.cc/2026/Conference/Submission5620/Reviewer_3iJQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5620/Reviewer_3iJQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761016930777, "cdate": 1761016930777, "tmdate": 1762918162616, "mdate": 1762918162616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an AR TTS model leveraging evidential deep learning to train and sample within a continuous-value Mel-Spectrogram space, in addition to employing knowledge distillation from multiple teachers via paired speech generated by publicly available, pre-trained TTS models. The core architecture is based on MELLE, with primary modifications made to the sampling module, associated loss functions, and the integration of multiple teacher models.\nExperimental results on the en-US task indicate that the proposed approach achieves performance comparable to baseline models that utilize ten times more training data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper explores sample strategies in continuous mel-spectrogram space for TTS, using paired data augmentation to enhance training diversity. The methodology demonstrates conceptual clarity."}, "weaknesses": {"value": "Experimental coverage is narrow, and some claims lack adequate explanation."}, "questions": {"value": "•\tIn multi-teacher learning, predefined weights are allocated to all teacher models. What criteria are used to determine these weights? Has an ablation study been conducted to validate this approach?\n•\tFor baselines that are not officially open-sourced, how were their results obtained to ensure a fair comparison?\n•\tIn Table 3, the paper doubles βt to enhance generation diversity. How does the WER vary with different values of βt?\n•\tThe experiments involving BELLE-Streaming require clarification, e.g. regarding the misalignment between text and mel-spectrogram sequence partitions: does this affect training? Additionally, there is insufficient detail regarding the failure of streaming BELLE in the zero-shot scenario.\n•\tUtilizing existing TTS models as data generators (teachers) may introduce pronunciation errors or unnatural prosody. Has this potential issue been addressed in the study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7A3LUoDF3Z", "forum": "ADRwyhQWzY", "replyto": "ADRwyhQWzY", "signatures": ["ICLR.cc/2026/Conference/Submission5620/Reviewer_Em4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5620/Reviewer_Em4X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100371487, "cdate": 1761100371487, "tmdate": 1762918162406, "mdate": 1762918162406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper described a novel system that utilize Bayesian evidential learning to auto-regressively model continuous Mel-spectrograms. In short, the main system majorly follows MALLE, but replacing the original KL divergence loss with the normal distribution with its objective derived from Bayesian evidential learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of leveraging Bayesian evidential learning in TTS is novel, and the paper did empirically show its potential on improving upon MELLE."}, "weaknesses": {"value": "There are several reasons that I consider this work not ready for publication:\n\n1. The use of Bayesian evidential learning is technically unjustified. It is not straightforward to me why it is better to use such objective instead of using the original Gaussian KL divergence as MELLE did. Although the authors provide empirical evidence, I did not see explanation, or at least speculation on why the proposed objective can better model the distribution of Mel-spectrogram. Is it because of having more parameters? Or enables more complex sampling methods? The authors should justify their motivation to use Bayesian evidential learning.\n\n2. The paper proposed two other stuff that are not related to the paper's main novelty: multi-teacher learning (Section 4.4) and streaming capability (Section 3.2.4). The multi-teacher learning is simply a heuristic way to fine-tune with the synthetic data. However, on Table 2, I think it actually makes the comparison between MELLE and BELLE unfair, as we do not know if the improvement comes from multi-teacher learning or BELLE. Although the paper runs ablation on Table 4, the numbers are not comparable to Table 2 since the datasets trained on are different. I am not sure what role does the streaming capability plays in this system. The paper evaluates the streaming version but the comparison with the other streaming methods in Table 2 lacks most of the numbers. Also, is there a reason that using Bayesian evidential learning allows you to have better streaming capability?\n\n3. The experiment result section felt incomplete. There is too many missing values on Table 2 for the authors to have rigorous conclusions from it. Moreover, on MaskGCT, the only two metrics evaluated are both better than BELLE, does this suggests that it will be better than BELLE? The authors should fill the Table 2. Additionally, the MOS scores should have confidence intervals included so we know if the results are statistically significant.\n\n4. In the related work section, the authors did a good job on listing several relevant papers that auto-regressively generates continuous features, e.g., FELLE, DiTAR, ARDiT. Is there any reason that the authors do not compare with these methods? I understand that it would be infeasible to compare with every existing baselines. However, the authors should give explanation on why these methods are selected as baseline, not the others (e.g., each of them can represent a line of research)."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9SN1ecM6JS", "forum": "ADRwyhQWzY", "replyto": "ADRwyhQWzY", "signatures": ["ICLR.cc/2026/Conference/Submission5620/Reviewer_1F23"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5620/Reviewer_1F23"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761277632242, "cdate": 1761277632242, "tmdate": 1762918162174, "mdate": 1762918162174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "BELLE is an innovative text-to-speech AI with significant limitations. While the model achieves impressive speech synthesis using only 5,000 hours of mostly synthetic data, it suffers from critical weaknesses. The research relies heavily on artificial speech data, which undermines its fairness and generalizability. Despite claiming to be \"Bayesian,\" the model performs no true Bayesian inference and offers only superficial uncertainty estimation. It was tested exclusively on a small, homogeneous English speech dataset, lacking validation in multilingual, noisy, or emotionally diverse speech scenarios. The model's improvements are modest, with gains of only 2-5% in performance metrics. Although BELLE demonstrates potential for efficient speech generation, its theoretical contributions are incremental rather than groundbreaking, and its practical applications remain unproven"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* BELLE introduces a groundbreaking method for text-to-speech synthesis by using Bayesian evidential learning. Unlike previous models that simply generate speech, BELLE can actually estimate the uncertainty in its predictions. Think of it like a smart speaker that not only talks but also knows how confident it is about what it's saying. This approach allows for more nuanced and flexible speech generation.\n\n* Most advanced speech synthesis models require massive amounts of training data - we're talking hundreds of thousands of hours of speech. BELLE does something remarkable: it achieves near state-of-the-art performance using only about 5,000 hours of data, with over 85% of that being synthetically generated. This is like learning to paint masterfully after practicing on just a few canvases, while other artists need entire museum collections to train.\n\n* The researchers developed an innovative training strategy where they use multiple pre-trained text-to-speech models to generate additional training data. By combining insights from six different TTS systems, BELLE learns a more robust and diverse way of generating speech. It's similar to learning a language by talking to multiple native speakers instead of just one instructor.\n\n* One of BELLE's most exciting features is its ability to control speech diversity. By adjusting a single parameter (β), researchers can make the generated speech more or less variable. This means the model can produce more consistent speech for professional contexts or more varied, natural-sounding speech for creative applications."}, "weaknesses": {"value": "Weaknesses:\n\n* The most significant issue with this paper lies in its data selection strategy. The researchers primarily used an overwhelming amount of synthetic speech (around 4,800 hours), with genuine human speech accounting for only 700 hours. In other words, over 85% of the training data consists of artificially generated \"fake\" speech created by other AI models. This approach is akin to copying homework: the model isn't truly learning human speech characteristics but instead mimicking a few \"teacher\" models. Such a methodology fundamentally undermines the model's credibility, as we cannot confidently determine its performance in real-world, noisy, or emotionally nuanced speech scenarios.\n\n* From a technical standpoint, BELLE is almost identical to its predecessor, MELLE, with minimal architectural modifications. The only substantive change is swapping the output layer from a Gaussian distribution to a more complex Normal-Inverse-Gamma distribution. This is tantamount to changing the paint job on an existing car and claiming it's a revolutionary new vehicle. The improvements reported (2-5% in Word Error Rate or Mean Opinion Score) are marginal at best, especially considering the added computational complexity.\n\n* Despite boldly claiming to be a \"Bayesian Speech Synthesis\" approach, the model performs no genuine Bayesian inference. It lacks fundamental Bayesian techniques like posterior sampling, prior updates, or variational estimation. The \"Bayesian\" aspect is nothing more than a sophisticated regression technique that predicts confidence parameters. While the model can visualize variance, it fails to leverage this uncertainty for meaningful decision-making, such as adaptive learning or improving speech generation in challenging contexts.\n\n* The authors argue for \"low-resource efficiency\" in 2025, a claim that rings hollow in the current technological landscape. Numerous large-scale, open-source speech corpora are now readily available, including collections with tens of thousands of hours of data. Many academic labs routinely train on hundreds of thousands of hours of speech. By artificially constraining their training to just 5,000 hours—mostly synthetic—the researchers undermine their own generalization claims.\n\n* The model's evaluation is disappointingly narrow. It was tested solely on LibriSpeech test-clean, a small and homogeneous read-speech corpus. There's no evidence of performance across critical domains like cross-lingual communication, conversational speech, or emotionally varied contexts. Without comprehensive validation, the model's claimed Bayesian sampling technique remains unproven in real-world, diverse speech environments.\n\n* While the paper emphasizes \"uncertainty modeling,\" the practical benefits remain purely theoretical. The researchers provide no correlation analysis between predicted uncertainty and perceptual variability. They fail to demonstrate how this uncertainty might improve crucial downstream tasks like speaker adaptation or prosody control. Essentially, they've created a technically clever but practically limited regression approach."}, "questions": {"value": "* How can we implement genuine posterior sampling in mel-spectrogram generation?\n* What probabilistic reasoning mechanisms could replace the current evidential regression approach?\n* Can we develop a true Bayesian framework that updates priors dynamically during speech synthesis?\n* How might we use uncertainty estimates for adaptive decision-making in speech generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OV1iZv8fDz", "forum": "ADRwyhQWzY", "replyto": "ADRwyhQWzY", "signatures": ["ICLR.cc/2026/Conference/Submission5620/Reviewer_hFUg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5620/Reviewer_hFUg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681710323, "cdate": 1761681710323, "tmdate": 1762918161828, "mdate": 1762918161828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Motivation for BELLE and its Distinction from MELLE"}, "comment": {"value": "In natural human speech, uncertainty is intrinsic, meaning that the same text are often spoken with variations in prosody, rhythm, and acoustics, even by the same speaker. Most existing TTS systems introduce this variability only implicitly, via empirical mechanisms such as dropout, top-p sampling, or diffusion noise. In contrast, we adopt an explicit Bayesian formulation that models and estimates this uncertainty in a principled and interpretable way.\n\nWhereas prior work MELLE assumes a fixed variance of 1, BELLE learns the variance distribution directly from data through a Bayesian evidence-based framework. This enables the model to capture fine-grained, text-conditioned uncertainty and produce more realistic, better-calibrated speech.\n\nA key challenge is that evidence-based learning requires multiple speech realisations of the same text from the same speaker, which typical TTS datasets do not provide. To address this, we synthesize such multi-sample sets using a pretrained TTS model, allowing us to construct the diverse training samples needed for reliable Bayesian estimation."}}, "id": "BQ0oj4xH5U", "forum": "ADRwyhQWzY", "replyto": "ADRwyhQWzY", "signatures": ["ICLR.cc/2026/Conference/Submission5620/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5620/Authors"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission5620/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763707511903, "cdate": 1763707511903, "tmdate": 1763707511903, "mdate": 1763707511903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}