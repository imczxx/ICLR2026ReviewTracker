{"id": "7VEECFBzmm", "number": 1215, "cdate": 1756865648911, "mdate": 1759898220868, "content": {"title": "DanceTogether: Generating Interactive Multi-Person Video without Identity Drifting", "abstract": "Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds “who” and “how” at every denoising step by fusing robust tracking masks with semantically rich but noisy pose heat maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 h of dual-skater footage with more than 7 000 distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centred on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalisation to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence.", "tldr": "DanceTogether is an end-to-end diffusion framework for controllable multi-actor video generation with identity preservation, enabled by a novel adapter, new datasets, and a benchmark supporting human–robot and cross-domain scenarios.", "keywords": ["Controllable video generation", "Multi-person Interactive Video Generation", "Multi-person Pose Estimation", "Multi-object Tracking"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e70d1facdaa8e8825a9cccbda71352d0511a127.pdf", "supplementary_material": "/attachment/61b3da6d8dc63a33e48029c263121bc331e551ba.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DanceTogether, an end-to-end diffusion framework for controllable multi-person video generation that addresses the challenges of identity drift and appearance bleeding in complex interactions. The method utilizes a MaskPoseAdapter to fuse robust tracking masks with noisy pose heatmaps, ensuring persistent identity-action binding throughout the denoising process. The authors also contribute two new datasets—PairFS-4K and HumanRob-300—and a benchmark called TogetherVideoBench for evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly structured and easy to follow. \n\n2. Experimental results show improvements in identity consistency, interaction coherence, and video quality over compared methods. The results are further bolstered by the provided webpage showcasing the generated videos, which offers valuable qualitative evidence."}, "weaknesses": {"value": "1. The technical contributions appear incremental. Multi-person video generation is an established research topic, and the use of conditional diffusion models for this task is now a relatively conventional approach. \n\n2. The experimental comparison may be unfair. The proposed model was trained on significantly more data (e.g., the PairFS-4K dataset) than the baseline methods. It is therefore unclear whether the performance gains stem from the novel architecture or simply from the increased scale of the training data. \n\n3. The evaluation is insufficient for claiming superiority in multi-person generation. The authors primarily compare against methods designed for single-person video generation. To properly validate their claims, comparisons against existing state-of-the-art multi-person video generation models on established benchmarks are necessary. [a] Follow-your-multipose: Tuning-free multi-character text-to-video generation via pose guidance, arXiv 2024. [b] Magicfight: Personalized martial arts combat video generation, ACM MM 2024.\n\n4. The paper introduces several hyperparameters but provides no ablation studies or sensitivity analysis for them. The impact of these design choices on the final performance remains unquantified, making it difficult to assess their necessity.\n\n5. Despite the strong quantitative results, the qualitative evidence from the provided video samples is less convincing. The generated videos often exhibit noticeable artifacts, temporal inconsistencies, and an overall synthetic appearance that is easily discernible to the human eye. \n\n6. The scope of the work is limited to two-person interactions. The method's scalability and effectiveness on videos involving three or more persons remain an open question, which limits the generalizability of the claimed contributions to broader multi-person scenarios.\n\n7. The paper lacks any analysis of computational cost, inference speed, or model complexity. Given that the proposed framework involves multiple encoders and a diffusion model, its practical efficiency is a significant concern for real-world applications."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2N5dqRHPpE", "forum": "7VEECFBzmm", "replyto": "7VEECFBzmm", "signatures": ["ICLR.cc/2026/Conference/Submission1215/Reviewer_E8i4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1215/Reviewer_E8i4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589024656, "cdate": 1761589024656, "tmdate": 1762915710840, "mdate": 1762915710840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DanceTogether, an end-to-end diffusion framework for controllable two-person interaction video generation from a single reference image plus independent per-person pose and mask sequences. Key ideas are a MaskPoseAdapter that fuses pose heatmaps with tracking masks to bind “who” and “how” at every denoising step, and a MultiFace Encoder that injects compact identity tokens throughout the U-Net. The authors also curate PairFS-4K (about 26hrs figure-skating pairs), HumanRob-300 (about 1hr human-robot), and propose TogetherVideoBench to evaluate identity consistency, interaction coherence, and video quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear target & mechanism. The paper diagnoses identity drift in multi-person controllable video and proposes a concrete architectural answer (pose–mask fusion + identity tokens) rather than only post-hoc temporal smoothing. The design is described with implementation-level detail. \n\n2. Comprehensive evaluation. Three-track benchmark with MOT-style identity metrics, motion/pose adherence, and perceptual quality, along with ablations that remove each module/input. Reported gains are consistent across tracks. \n\n3. Useful data curation. PairFS-4K fills a gap for dual-actor interactions with many IDs; TogetherVideoBench focuses evaluation on human-masked regions to reduce background bias."}, "weaknesses": {"value": "1. The primary concerns about this paper is that the scope is limited to two people. The method is explicitly “optimized for up to two actors,” and scaling to larger groups is acknowledged as non-trivial in both compute and design; the datasets/benchmark are also two-person-only. This narrows impact for wider multi-human scenes and social group interactions. How would the architecture and compute scale to N>2 actors? Any preliminary 3–4 person results?\n\n2. Besides, I also have concerns about the novelty. Many components (e.g., StableAnimator backbone, identity adapters) are adapted/extended; the contribution feels like a strong systems integration with a new adapter and data pipeline rather than a fundamentally new generative principle.\n\n3. A common problem for methods in this task is heavy reliance on external controls. Quality hinges on the quality of provided mask/pose. However, it is not easy to get high-quality mask/pose pair in in-the-wild applications. Did the author investigate how to get mask/pose pair easily, or how to handle the noise in the input mask/pose pair?"}, "questions": {"value": "1. The authors acknowledges that they \"assume a mostly static camera and relatively simple backgrounds; dynamic camera motion or highly cluttered scenes may introduce artifacts or identity confusion.\" How the moving cameras will impact the results? Could this pipeline involves camera control (e.g., by estimating and conditioning on camera motion or via background stabilization)?\n\n2. For human-robot, the visualization results is not strong enough to clarify the contribution. The cross-domain section shows qualitative success after ≈1 h fine-tune but lacks quantitative comparisons or baselines in that domain. As this paper only uses the appearance of a robot from images, what is the major difference between human-robot and human-human? Is it just for showing generalization ability to non-human appearances? Apparently, the robot movement in this paper is not related to embodied AI training or world model.\n\n\n3. Will it be possible to train this paper's idea on a DiT backbone? I understand that the reason choosing U-net and StableAnimator as the baseline could be limited GPU resources. However, some problems in results from U-net could just vanished with a DiT backbone. It is hard to assess the contribution of this paper from a bad backbone or pretrained model.\n\nOverall, it is a good paper with adequate content for publication. The authors have shown as many experimental results as possible, which will be beneficial to this domain. However, concerns about scope limited to two people and other novelty concerns motivate me to rate it as marginally below the acceptance threshold. I could adjust my rating during author rebuttal and discussion period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tNBg0axRVX", "forum": "7VEECFBzmm", "replyto": "7VEECFBzmm", "signatures": ["ICLR.cc/2026/Conference/Submission1215/Reviewer_sBvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1215/Reviewer_sBvg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689000268, "cdate": 1761689000268, "tmdate": 1762915710072, "mdate": 1762915710072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a limitation in controllable video generation (CVG): the inability of existing systems to generate long, photorealistic multi-person interaction videos while preserving identity and respecting noisy control signals (e.g., poses, masks). The authors propose DanceTogether, the first end-to-end diffusion framework tailored for this task, which synthesizes videos from a single reference image and independent per-person pose/mask sequences.\nKey technical innovations include:\n1. MaskPoseAdapter: A novel conditional adapter that fuses stable tracking masks with semantically rich but noisy pose heatmaps, enforcing \"identity-action binding\" at every denoising step to eliminate identity drift and appearance bleeding.\n2. MultiFace Encoder: Distills compact identity tokens from the reference image and injects them into cross-attention layers, ensuring consistent subject appearance across frames.\n3. Curated Datasets:\nPairFS-4K: 26 hours of dual-skater footage with 7,000+ unique identities.\nHumanRob-300: 1 hour of humanoid-robot interaction data for cross-domain generalization.\n4. TogetherVideoBench: A comprehensive benchmark with three tracks (Identity-Consistency, Interaction-Coherence, Video Quality) and the DanceTogEval-100 test suite (covering dance, boxing, skating, etc.)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Experiments are comprehensive: 8 baselines, ablation studies for all key modules, and cross-domain validation. Metrics are carefully chosen to measure identity, interaction, and quality—avoiding overreliance on visual subjective assessments.\n2. Dataset curation: The data pipeline uses state-of-the-art tools to extract high-quality annotations, and PairFS-4K is the first large-scale dual-skating dataset with diverse identities.\n3. The work advances CVG from single-subject to multi-actor scenarios, a critical step toward real-world applications (e.g., virtual film sets, AI-driven animation).\n4. TogetherVideoBench provides a standardized evaluation framework, which will accelerate progress in the field by enabling fair comparisons between methods."}, "weaknesses": {"value": "1. DanceTogether’s performance degrades under severe occlusions, motion blur, or detector failures. The paper does not propose solutions to robustify against noisy inputs (e.g., denoising modules for poses/masks), which is a practical limitation for real-world use.\n2. The model assumes mostly static cameras and simple backgrounds. Dynamic camera motion or cluttered scenes introduce artifacts, but the paper does not explore adaptations (e.g., camera motion estimation) to address this.\n3. The HumanRob-300 dataset is only 1 hour long, and the paper provides little detail on interaction types (e.g., handshakes vs. collaborative tasks) or robot models. More diverse human-robot data and experiments would better validate cross-domain generalization."}, "questions": {"value": "1. Authors note that extending to >2 actors would require hierarchical conditioning. Could you elaborate on potential designs (e.g., grouping actors into sub-pairs, using graph-based attention) and share any preliminary results or challenges (e.g., computational cost, identity confusion)?\n2. You note DanceTogether is computationally intensive. Are there opportunities to optimize inference speed (e.g., model distillation, smaller backbones) without sacrificing quality? What is the current inference latency per frame, and what are your targets for real-world use?\n3. How might DanceTogether be modified to handle low-quality pose/mask signals (e.g., from occluded frames or low-resolution videos)? Would integrating a denoising module for poses/masks (e.g., using diffusion to refine inputs) improve performance, and have you tested this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QUiTsPa8Jh", "forum": "7VEECFBzmm", "replyto": "7VEECFBzmm", "signatures": ["ICLR.cc/2026/Conference/Submission1215/Reviewer_rryZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1215/Reviewer_rryZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751411564, "cdate": 1761751411564, "tmdate": 1762915709803, "mdate": 1762915709803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DanceTogether, an end-to-end diffusion framework designed to generate interactive, multi-person videos without the identity drift that plagues existing methods. The system takes a single reference image and independent pose-mask control streams to produce long, photorealistic videos while strictly preserving each actor's identity. The primary technical contribution is the novel MaskPoseAdapter, which explicitly binds identity to motion by fusing robust tracking masks with noisy pose-maps at every denoising step. The authors also introduce significant contributions for training and evaluation: the PairFS-4K and HumanRob-300 datasets, and the TogetherVideoBench benchmark. The method demonstrates state-of-the-art results on their benchmark and shows strong generalization to human-robot interaction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1: The proposed DanceTogether aims to address a critical and well-defined problem: the failure of existing models to maintain actor identities during complex interactions. By explicitly fusing robust tracking masks (the \"who\") with semantic pose maps (the \"how\"), the model establishes a persistent identity-action binding throughout the diffusion process.\n\n2: The well-curated PairFS-4K (26 hours of dual-skater footage) and HumanRob-300 (1 hour of human-robot interaction) datasets are helpful for the community to dive into the area of multi-person interaction video generation, addressing a clear lack of large-scale, diverse multi-person interaction data.\n\n3: The proposed TogetherVideoBench is a comprehensive, three-track benchmark that thoughtfully evaluates the distinct challenges of Identity-Consistency, Interaction-Coherence, and Video Quality. \n\n4: Extensive experiments show the effectiveness of the proposed framework and datasets on multi-person interaction human animation."}, "weaknesses": {"value": "1: **The presentation could be significantly improved.** For instance, Figure 2, which illustrates the core pipeline, is overly crowded, making it difficult for a reader to easily follow the main architectural components and data flow. Furthermore, the method description, particularly in Section 3.3 on pages 5 and 6, is saturated with low-level implementation details like specific activation functions (e.g., Sigmoid) and normalization layers (e.g., LayerNorm). This focus on minutiae tends to obscure the high-level innovations that differentiate the method from related works. This confusion is amplified by a disconnect between the diagram and the text. For example, terms like \"Attention Pre-Conv\" are used in Figure 2 but are not clearly defined in the text, while key steps described in the text, such as the \"Cross-Person Integration\" (Equation 13), are difficult to locate within the figure. This lack of clear mapping makes the overall method presentation somewhat confusing to follow.\n\n2: **The clarity of the core methodological innovation is limited.** The ablation study (Table 5) demonstrates that removing either the \"mask input\" or the \"MaskPoseAdapter\"  severely degrades performance. This highlights the importance of mask conditioning, a point the paper also emphasizes. However, using per-person tracking masks and pose sequences as conditional inputs is a relatively common practice in human animation, so the use of \"mask input\" itself isn't a significant novelty. The ablation \"w/o MaskPoseAdapter\" (which uses the original PoseNet) also performs poorly, suggesting the fusion method is key. Yet, the current experiments do not clearly disentangle the performance gains of the novel fusion strategy within the adapter from the more straightforward, non-novel benefits of simply using mask conditioning in the first place. This makes it difficult to pinpoint the most critical novel design choice responsible for the performance gains."}, "questions": {"value": "1: Why is there a 0.95 factor in the Residual Blend shown in Figure 2?\n\n2: What does the PoseNet0401 mean? What's 0401?\n\n3: There seems to be a typo:  \"Pose Gete\" in Figure 2.\n\n4: What does \"Intg\" in Figure 2 mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X5ydHbrCoK", "forum": "7VEECFBzmm", "replyto": "7VEECFBzmm", "signatures": ["ICLR.cc/2026/Conference/Submission1215/Reviewer_9khT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1215/Reviewer_9khT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976700215, "cdate": 1761976700215, "tmdate": 1762915709611, "mdate": 1762915709611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}