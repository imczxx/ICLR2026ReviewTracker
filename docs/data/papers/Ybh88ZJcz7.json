{"id": "Ybh88ZJcz7", "number": 11080, "cdate": 1758188790420, "mdate": 1759897609948, "content": {"title": "Face-Feature Tuning: Post-hoc Calibration for Fair and Accurate Deepfake Detection", "abstract": "Deepfake detectors often show large performance gaps across demographic groups, undermining trust in deployment. Existing fairness approaches typically require demographic labels, retraining the detector, or accept notable drops in overall accuracy. We introduce Face-Fairness (FF), a plug-and-play framework for mitigating bias in deepfake detection. Our primary contribution, Face-Feature Tuning (FFT), is the first demographic label-free fairness method demonstrated for deepfake detection: a lightweight calibrator that performs post-hoc logit remapping conditioned on frozen face embeddings, trained on a held-out validation split. We complement FFT with two thresholding variants: FF-Max, which maximizes worst-group accuracy when demographics are available on validation, and FF-Discover, which applies the same objective to embedding-discovered groups when demographics are unknown. Across in-domain, cross-protocol, and cross-dataset test settings, FF consistently reduces FPR/TPR gaps and improves minimum group accuracy while maintaining (often improving) overall accuracy. The approach is detector-agnostic, adds negligible runtime overhead, and requires no access to identity attributesâ€”making fairness in deepfake detection practical for real-world systems.", "tldr": "We introduce Face-Feature Tuning (FFT), a post-hoc calibration method that achieves sota fair deepfake detection", "keywords": ["deepfake detection", "fairness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b23fb4f236337392330176d8d856a0c4b2e8343.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper aims to advance group fairness (i.e. mitigating disparate performance across demographic groups) in deepfake detection. The authors propose three post-processing methods to maximize worst-group accuracy and achieve fair and accurate preditions: FF-Max that learns group-specific decision thresholds (with supervised demographic labels); FF-Discover which clusters face embeddings and learns cluster-conditional thresholds (without demographic labels); FFT that learns a decision boundary via training a lightweight multi-layer perceptron (without demographic labels). Experiments are carried out on both in-the-wild and canonical deepfake detection test beds, comparing with standard and state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The experiments are extensive. The choices of metrics encompass both group fairenss (equal opportunity) and min-max fairness (worst group accuracy) definitions.\n- The literature review is well-organized, precise and complete.\n- The paper is well-written and clearly positions the contribution in literature. Also, the mathematical notation and tables are clean and minimal, effectively conveying the message."}, "weaknesses": {"value": "Unfortunately, at the current state, the paper feels quite narrow in terms of impact, given its strong application-specific nature. Below I'm listing the main reasons for the recommendation, which require further attention:\n\n**W1.**\n\nThe contribution feels limited, apart from the method and some (good) related works. A starting point could be to dedicate a section to testing why and how the method works, when it breaks, and if there are theoretical guarantees that could be derived. Maybe also some ablation study (eg. on the width of the two-layer in FFT) could be an addition. \n\nFor instance, there are some claims in Section 3.2.1. that could be expanded and theoretically tested: the discussions about FF-Max and FF-Discover being special cases, FFT delivering \"better-conditioned probabilities\" etc. (LL265-269) could be shown to hold over some controllable setup.\n\n------------------\n\n**W2.**\n\nThe results reported in the Tables seem to tightly cluster together, leading to very little improvements which at the moment feel not decisive. Furthermore, it is unclear if the results are averages of multiple runs: in this case, given they are very close it would be helpful to report also standard deviation. Otherwise, I'd suggest to try multiple runs with different seeds and report means and standard deviations.\n\nOn the same topic, Figure 1 could be improved. First of all, text and markers could be made bigger and non-overlapping for improved readability (this applies also for Figure 2). Secondly, as it is currently plotted, it seems the contribution of FFT is drastic, while the reality is that it improves by just 1% to 3% with respect to the worst method under comparison. Also here I'd suggest to report some confidence intervals."}, "questions": {"value": "Thanking in advance for their response, I'd kindly invite the authors to address the points raised in the Weaknesses section of this review.\n\nIn addition, I'd kindly ask the following question:\n\n- I couldn't find the reason of choosing older architectures like MobileNet or Xception, instead of newer architectures (eg. Vision Transformers). Is this an arbitrary choice, or, are they linked to the experimental test beds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E8izLRet8N", "forum": "Ybh88ZJcz7", "replyto": "Ybh88ZJcz7", "signatures": ["ICLR.cc/2026/Conference/Submission11080/Reviewer_SH1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11080/Reviewer_SH1w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761049933575, "cdate": 1761049933575, "tmdate": 1762922262097, "mdate": 1762922262097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their time. We are encouraged that the reviewers reached a strong consensus on the fundamental strengths of our work. Reviewer aCaf said the method is \"sound and novel,\" and Reviewer SH1w praised the \"extensive experiments\" and \"precise\" literature review. Reviewer aCaf highlighted the \"key contribution\" that, unlike existing methods, we do not require retraining or demographic labels.\n \nGiven the strong consensus on the method's validity and experimental rigor, we wish to clarify the primary point of contention regarding \"impact\" and \"margins.\" First, critiques regarding \"narrow impact\" (SH1w, pjMC) overlook the strict legal and technical constraints of real-world deployment. Methods requiring demographic labels are illegal to deploy in many jurisdictions. FFT is the only post-hoc method which avoids such labels. The only demographic label-free in-processing alternative, DAG-FDD, requires expensive retraining, rendering it incompatible with increasingly popular fixed third-party APIs (e.g., Incode) relied upon by practitioners.\n \nFFT often reaches parity or outperforms these resource-heavy methods, especially under hard shift as seen in Tables 4 and 5. Furthermore, we emphasize that at the scale of real-world deployment (e.g., social media platforms), a small percentage difference in Worst-Group Accuracy is non-trivial, translating to the protection of millions of individuals from algorithmic bias. And as we see under hard shift the differences are not small in settings that mimic real world deployment. \n \nFinally, regarding the critique of \"narrow impact\" due to our application-specific focus (SH1w, pjMC), we respectfully remind the reviewers that: (1) The work still holds technical novelty. It involves the development of a new approach for algorithmic fairness (which is an explicit subject area of interest at ICLR) and it is not a direct application of an existing framework to a new domain. (2) Papers on the applications of ML are encouraged at ICLR. The call for papers lists 3 subject areas that have ``applications'' in the title: applications to robotics, autonomy, planning; applications to neuroscience \\& cognitive science; and applications to physical sciences. And there have been many interesting papers published at ICLR focusing directly on other applications such as medical image classification or 3d reconstruction.\n \nBeyond this, the application domain is high impact. Already a combination of deep-fake detection and ID verification is used for access to: (1) online banking; (2) Online Government services, noticeably in Germany; (3) legally mandated age verification for access to online spaces, this includes LGBQT+ friendly spaces. Anywhere where one has been asked to upload a video of themselves alongside photo ID as part of the registration process is likely to include this. As such, small improvements in performance which can reduce the FPR by 3\\% in absolute terms, or a third in relative terms can be highly significant. They correspond to a system now working for millions of people that it previously didn't work for. Moreover, systematic issues of unfairness here risk disenfranchising swathes of the population.\n \nWe intentionally targeted this domain because generic fairness methods fail to address its unique privacy and deployment constraints; solving this specific bottleneck for a technology affecting global information integrity is, we argue, a contribution of significant breadth and urgency.\n \nIn the discussion, we address each reviewer's specific questions and concerns, incorporating new experimental results where requested. We are happy to run additional experiments beyond those in our initial responses generally but note that these are compute heavy, slow and costly."}}, "id": "mx73nnTpLF", "forum": "Ybh88ZJcz7", "replyto": "Ybh88ZJcz7", "signatures": ["ICLR.cc/2026/Conference/Submission11080/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11080/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11080/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729791034, "cdate": 1763729791034, "tmdate": 1763729791034, "mdate": 1763729791034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Face-Feature Tuning (FFT) method that aims to achieve fairness in facial analysis without relying on demographic labels, retraining, or compromising accuracy. The key idea is that face embeddings extracted from pre-trained models inherently encode visual patterns that correlate with detector failures in a continuous feature space. To address this, FFT trains a lightweight neural network that refines the decision boundaries based on these embeddings, effectively recalibrating predictions in regions where systematic biases appear while maintaining consistent and reliable performance elsewhere.\n\nSeveral concerns arise regarding the depth and impact of this work:\n\n- The proposed FFT framework appears to be more of an engineering tweak on existing face-feature representations rather than a fundamentally new approach. The paper offers limited exploration of underlying principles, theoretical insights, or broader implications for fairness in vision models.\n\n- The reported baselines, whether pre-processing, in-processing, or post-processing methods, already exhibit near-saturated performance, suggesting that the chosen task may not present substantial difficulty. This diminishes the perceived necessity and overall impact of the proposed method.\n\n- The experiments are restricted to two relatively basic backbones, MobileNetV3 and Xception, without evaluation on forensics-oriented architectures that have been developed in recent deepfake detection or fairness-related research. This narrow experimental design limits the generalizability and significance of the findings, making it unclear how well FFT would perform in more advanced or domain-specific settings.\n\n- (?) The title in the paper does not match the title in the system.\n\nOverall, while FFT is technically sound, the novelty, motivation, and empirical validation of the paper remain modest, and its contribution to advancing fairness research in face analysis appears limited."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed FFT method is lightweight, computationally efficient, and does not require retraining or access to sensitive demographic information, making it appealing for real-world deployment."}, "weaknesses": {"value": "See summary"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UhWzRsB8J4", "forum": "Ybh88ZJcz7", "replyto": "Ybh88ZJcz7", "signatures": ["ICLR.cc/2026/Conference/Submission11080/Reviewer_pjMC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11080/Reviewer_pjMC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798033710, "cdate": 1761798033710, "tmdate": 1762922261675, "mdate": 1762922261675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel approach to deal with bias and fairness in the deepfake detection task exhibited by existing models to improve the performance gaps across demographic groups. This key contribution of this work is that existing models do not need to be retrained to remove their biasness. The authors propose 3 methods to tackle the biasness issue depending on the availability of demographic labels, and they show that their technique improves detection fairness and FPR/TPR across model backbones and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper tackles a very important problem of existing deepfake detection methods, which is the problem of fairness against demographic groups caused due to underrepresentation in training datasets.\n- The proposed method is statistically sound, and covers 3 different regimes of detection depending on availability of demographic labels. \n- The biggest strength of the work compared to existing ones is that it does not require any retraining and can be added to detectors as a post training step.\n- The paper is structured well and explains the related work in depth - this is important for readers no familiar with the domain."}, "weaknesses": {"value": "- Some explanations and notation are hard to understand, particularly section 3.3 and 2.2.2. Instead of writing a bunch of equations a bit more explanation would be useful.\n- The plot in Figure 2 requires better captioning or more description in the text. What is ECE? What does the baseline diagonal represent?\n- The introduction / motivation needs to show an analysis of existing datasets or model performances to verify the claims made by the authors regarding the demographic imbalance."}, "questions": {"value": "- Although the method is sound and novel, the results show only marginal improvement compared to existing techniques, such as performance improvements of 0.01 or 0.03 (Table 2). The training free methods FF Max and FF Discover perform worse than other post processing methods across the board. What is the cause of this? The result section needs more discussion on limitations and degree of improvement.\n- The result also lacks comparison against existing deepfake detectors. The authors only use Xception and MobileNetv3 as backbone models. What were these models trained on?  There are far better performing or specialized deepfake detection networks (https://arxiv.org/pdf/2506.03007v1). Is this method similarly applicable for these models? \n- A robustness study is necessary. How does a model augmented with FFT handle the typical image attacks such as image compression, noise addition or augmentations. \n\nI am happy to change my score given a proper discussion to address the problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f1zTAiT6Ff", "forum": "Ybh88ZJcz7", "replyto": "Ybh88ZJcz7", "signatures": ["ICLR.cc/2026/Conference/Submission11080/Reviewer_aCaf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11080/Reviewer_aCaf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833011022, "cdate": 1761833011022, "tmdate": 1762922260820, "mdate": 1762922260820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}