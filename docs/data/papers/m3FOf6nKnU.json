{"id": "m3FOf6nKnU", "number": 13522, "cdate": 1758218888006, "mdate": 1759897431148, "content": {"title": "Forget Vectors at Play: Universal Input Perturbations Driving Machine Unlearning in Image Classification", "abstract": "Machine unlearning (MU), which seeks to erase the influence of specific unwanted data from already-trained models, is becoming increasingly vital in model editing, particularly to comply with evolving data regulations like the \"right to be forgotten\". Conventional approaches are predominantly model-based, typically requiring retraining or fine-tuning the model's weights to meet unlearning requirements. In this work, we approach the MU problem from an input perturbation-based perspective, where the model weights remain intact throughout the unlearning process. We demonstrate the existence of a proactive input-based unlearning strategy, referred to forget vector, which can be generated as an input-agnostic data perturbation and remains as effective as model-based approximate unlearning approaches. We also explore forget vector arithmetic, whereby multiple class-specific forget vectors can be combined through simple operations (e.g., linear combinations) to generate new forget vectors for unseen unlearning tasks, such as forgetting arbitrary subsets across classes. Extensive experiments validate the effectiveness and adaptability of the forget vector, showcasing its competitive performance relative to state-of-the-art model-based methods while achieving superior parameter efficiency.", "tldr": "", "keywords": ["Machine Unlearning", "Forget Vector", "University Input Perturbation", "Image Classification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac799a25c7768c8514df6181b0eeedc456e07925.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reframes machine unlearning for image classification as a data-based operation. Instead of updating weights, the authors learn a universal, input-agnostic perturbation, “forget vector”, that is added to inputs to degrade predictions on the forget set while preserving utility on retain and test data. They formulate an optimization with an untargeted margin loss on forget set and a cross-entropy retain regularizer on retain set, plus an L2 norm penalty. The model itself stays frozen. They further propose compositional unlearning by linearly combining class-wise forget vectors to address random data deletion. Experiments on CIFAR-10 and ImageNet-10 with ResNet-18 / VGG-16 / ViT-Base compare against Retrain and several approximate MU baselines, reporting competitive unlearning performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. Achieving unlearning by keeping the model fixed and using a visual-prompting-like input perturbation is novel, offering a fresh perspective on approximate machine unlearning.\n\n3. The idea of compositional Forget Vector arithmetic is interesting.\n\n4. Grad-CAM visualizations provide intuitive evidence supporting the reported effects."}, "weaknesses": {"value": "1. The proposed method that perturbs only the input via a Forget Vector by a linear operation in image input space has inherent limitations.\n\n    * Effectiveness: Although the paper achieves competitive unlearning accuracy, this comes at the cost of degraded RA and TA due to the added perturbation.\n\n    * Efficiency: According to Table A2 (RTE), the Forget Vector is not clearly superior to other model-based unlearning methods in runtime, and its performance does not significantly surpass them.\n\n    Therefore, for a model maintainer, there may be insufficient reason to prefer the Forget Vector approach as the unlearning mechanism. Exploring hybrid approaches that combine Forget Vectors with other unlearning methods could better demonstrate its value.\n\n2. The unlearning settings (only single-class forgetting and 10% random sample forgetting) considered are limited. A robust unlearning method should handle arbitrary numbers of classes and larger proportions of random samples. This may be particularly challenging for universal perturbations, as in the Forget Vector method.\n\n3. The application scope in this paper is narrow, focusing on image classification. Unlearning is also relevant in image generation models, large language models, and multimodal models. The transferability of this approach remains unknown. For example, in vision–language models, whether an image-side Forget Vector can achieve comparable forgetting effects is an important unresolved question."}, "questions": {"value": "1. How does Forget Vector perform in the following scenarios aimed at addressing the aforementioned weaknesses: (i) its integration with other unlearning methods, (ii) evaluation under more challenging unlearning settings, and (iii) evaluation on vision–language models?\n\n2. In the experiments related to Figure 3, the combination coefficient $w_1=-1$ for Forget Vectors is negative. While this is plausible from an optimization perspective, it seems at odds with the intended semantics of a forget vector. Could you provide a clear explanation for this phenomenon?\n\n3. In Table 1, the green/red highlighting appears to be based on performance gaps, which might conflict with the up/down arrows (e.g., TA in the Random Data Forgetting, ImageNet-10, ViT-Base setting). It would help to clearly state the convention in the caption to avoid ambiguity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kPiep0Lf2h", "forum": "m3FOf6nKnU", "replyto": "m3FOf6nKnU", "signatures": ["ICLR.cc/2026/Conference/Submission13522/Reviewer_xw9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13522/Reviewer_xw9H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298613084, "cdate": 1761298613084, "tmdate": 1762924128767, "mdate": 1762924128767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a machine unlearning method that introduces an augmentation vector to perturb input data, preventing a trained machine learning model from making correct predictions on the designated forget datasets. The augmentation vector is treated as a set of trainable variables, optimized through a combined forget and retain loss functions to balance unlearning performance and knowledge retention. Experimental results demonstrate that the proposed approach is effective for both class-level and random unlearning tasks. However, the method also leads to noticeable performance degradation on the retain and test datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and presents a well-motivated discussion on how the proposed method differs from existing unlearning approaches.\n2. The experimental evaluation is comprehensive and includes standard benchmarks and metrics commonly used in the literature for assessing unlearning algorithms."}, "weaknesses": {"value": "I have several concerns regarding the novelty of the proposed method and its impact on performance degradation over the retain datasets.\n\nAlthough the paper claims the approach to be data-based, the method essentially trains an operator to modify the input, which is conceptually similar to techniques explored in the visual prompting literature, as acknowledged in the background section. The primary difference lies in the definition of the encoding function $f$, where previous works employ linear or nonlinear projections, while this method defines $f(x)=x+w$. This formulation represents a relatively minor variation rather than a fundamentally new idea.\n\nThe experimental results indicate a noticeable degradation in performance on the retain and test datasets compared with other methods. This suggests the limitation of augmenting inputs via simple vector addition. It would strengthen the paper to include an analysis of the geometric properties of the learned augmentation vectors relative to the training data, to better illustrate how they contribute to domain shift.\n\nBTW: the green and red tags indicating the best and second-best performance in Table 1 appear to be mislabeled. E.g, RA on the random data forgetting task for ImageNet-10."}, "questions": {"value": "Can the authors show what the vectors learned captured, is it a vector that move input representation to the representation area of retain data points (maybe a specific class)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wCK1zPnNGV", "forum": "m3FOf6nKnU", "replyto": "m3FOf6nKnU", "signatures": ["ICLR.cc/2026/Conference/Submission13522/Reviewer_oFB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13522/Reviewer_oFB6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577291919, "cdate": 1761577291919, "tmdate": 1762924128492, "mdate": 1762924128492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on machine unlearning, particularly on various aspects of the generalization. In the first part, the authors examine the efficacy of  a generic unlearning method in handling images with corruptions or perturbations. In the second part, the authors propose a method for the optimization of forget vectors (which are input perturbations), and these forget vectors can be combined for compositional unlearning on an unseen unlearning task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The premise of the paper is interesting, where input perturbations are optimized, and that the learnt forget vectors can be combined in a compositional manner to tackle unseen forgetting tasks.\n\nThe writing and presentation are quite clear."}, "weaknesses": {"value": "I am not sure how the two parts of the paper are connected. The first part investigates corruptions and perturbations on a generic unlearning method, but it is not clear what learning points are pertinent in proposing the subsequent method.\n\n\nAlthough I like the idea of the compositional combination of forget vectors for unlearning, I find that the implementation itself is not too interesting. Specifically, there is another optimization step where the linear combination weights of the learned forget vectors are optimized. Thus, the “transfer” of forget vectors is not exactly a zero-shot transfer, since training is still required.\n\nFurthermore, since optimization is still required, can the “compositional” method really be claimed to be performed on an “unseen task”?\n\nIn the experiment results, the proposed method is compared to various works, but many of them are not the latest works in the field. A suggestion would be to compare against more recent works in the field, such as the following:\n\nLearning to Unlearn for Robust Machine Unlearning. ECCV 2024\n\nAdversarial Machine Unlearning. ICLR 2025\n\nAdversarial Mixup Unlearning. ICLR 2025\n\nDecoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks. CVPR 2025"}, "questions": {"value": "During the investigation of corruptions, there are actually many times and strengths of corruptions available. How did the authors decide which corruptions to use, and which intensities to use?\n\nCan the forget vectors be combined in a “zero-shot” manner? For instance, in the example given with the automobile and the bird, can they be combined with simple averaging to be used on airplanes, without optimization?\n\nIf a trained forget vector for a class is used for another class, is the accuracy still maintained? Furthermore, does it affect the GradCAM saliency maps?\n\nPlease see weaknesses for some other questions. \n\nI tentatively recommend a borderline reject score due to my concerns with the disconnected parts of the paper, as well as my concerns with how compositional the vectors actually are. Yet, I think this paper has an interesting premise. If my concerns are addressed, I am open to raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KXeWxL1gkc", "forum": "m3FOf6nKnU", "replyto": "m3FOf6nKnU", "signatures": ["ICLR.cc/2026/Conference/Submission13522/Reviewer_gacL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13522/Reviewer_gacL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583555872, "cdate": 1761583555872, "tmdate": 1762924128092, "mdate": 1762924128092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a model-free, perturbation based machine unlearning that claims to achieve the unlearning of forget set based on the perturbing the data and keeping the model weights intact throughout the unlearning process. They propose the forget vectors that can be applied to the input forget data as an input-agnostic data perturbation and remains as effective as model-based approximate unlearning approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The research questions that paper investigated are very interesting such as \"For instance, it remains unclear whether, current MU approaches generalize effectively to “shifted” forget data\".\n\nProposing the forget vectors that can demonstrate the direction of unlearning."}, "weaknesses": {"value": "Since the model's parameters are unchanged and the information of forget set still encoded in the model, the unlearning has not take place and this method is not compatible with data privacy regulations.\n\nFrom the general perspective the idea can be interpreted as controlling the inputs to change model's output. This idea has been explored for explaining a black box prediction and how its prediction changes in the literature (model agnostic local explainers) but their proposed method won't change the model's parameters and this is in conflict with the idea of MU.\n\n\nLine 258 // $\\rightarrow$ \"(without unlearning)...\" $\\rightarrow$ a bit confused about the experimental setup:\n\nfor the experiment that is depicted in Figure 2, the authors applied different levels (methods) of perturbation on the whole data and passed it to the original model without unlearning and unlearned model using retraining. but the issues is that the original model performance downgraded on the test and remaining data but not on the forget set. If original model is not unlearned, how it's performance stayed the same on the forget set?"}, "questions": {"value": "Line 54 // $\\rightarrow$ \"MU design...\" $\\rightarrow$ How this method can comply with the GDPR and privacy regulations? Does it mean that the model can still remember the forget dataset ? wouldn't it oppose the purpose of MU?\n\n\nLine 91 $\\rightarrow$ \"eliminate the influence of...\" $\\rightarrow$ How the influence of specific data / subset of data is eliminated since the parameters has remained unchanged?\n\n\nLine 184 // $\\rightarrow$ \"to achieve unlearning...\" $\\rightarrow$ So in this case the inputs are perturbed to achieve the unlearning but the model remains the same. So in this case, I expect the paper to investigate that the model treats the original and perturbed inputs the same meaning the prediction for both of them are similar and the encoding and representation of the model for both are the same. Even slight introduction of noise and perturbing the input can influence the model's prediction. So I would like to ask the authors to conduct an experiment to see how close the hidden representations of these two samples are.\n\nline 196 // $\\rightarrow$ \"based on (2)...\" $\\rightarrow$ I expect the authors to also investigate the following question: 1. is the perturbed data treated similarily to the original data? If this is treated as a different data point then how we can claim the model is already unlearned because the model can not recognize the class, and the model's performance has remained the same.\n\n\nline 293 // $\\rightarrow$ \"this yields the full...\" $\\rightarrow$\n\nI like the idea of optimizing the perturbation, but still the model's parameters wouldn't change and the model is the same but the forget data is manipulated. THis question can be risen that if I pass another datapoint as forget sample, what would be the issue? \n\nI know the answer to this question might sound trivial but the issue is coming from the point that model's parameters are not changed.\n\n\nSince the forget vector can show a direction to change the model while preserving the performance on the retain set, have authors considered fine tuning or applying grad Ascent using the direction of forget set? what would happen if fine tune the model's paremeted based on forget vectors?\n\n\nThe experiments section > The most important question that I hope authors can shed some light on that is that, does the representation of the perturbed forget data is similar to the original data? or how far it would get?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nBQxrzhZaS", "forum": "m3FOf6nKnU", "replyto": "m3FOf6nKnU", "signatures": ["ICLR.cc/2026/Conference/Submission13522/Reviewer_MhaY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13522/Reviewer_MhaY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976367905, "cdate": 1761976367905, "tmdate": 1762924127558, "mdate": 1762924127558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}