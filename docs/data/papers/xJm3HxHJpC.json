{"id": "xJm3HxHJpC", "number": 3506, "cdate": 1757451393616, "mdate": 1759898084096, "content": {"title": "Robust Fact-Checking under Contaminated Evidence Sources via Claim Decomposition and Dynamic Reweighting", "abstract": "Fact-checking seeks to assess the veracity of claims with respect to a knowledge base from which supporting or refuting evidence can be retrieved. However, most existing approaches assume access to a clean and reliable knowledge source. In practice, retrieved evidence is often contaminated with misinformation, which substantially reduces verification accuracy. In this paper, we address the task of fact checking under contaminated knowledge bases and propose a framework designed to remain robust in noisy environments. Our approach first decomposes each claim into subclaims, then retrieves candidate evidence for each subclaim. A large language model (LLM) is subsequently employed to classify documents into supporting, refuting, or unrelated categories, and subclaim veracity is determined through a carefully weighted majority stance. To further enhance robustness, documents are dynamically reweighted: supporting evidence is upweighted as likely truthful, while refuting evidence is downweighted as potentially misleading, and these weights are incorporated into subsequent retrieval through reranking. To rigorously evaluate this setting, we introduce a method for constructing adversarially contaminated knowledge bases by generating misinformation derived from gold evidence and false claims, which effectively misleads standard retrievers. Experimental results across open-source LLMs and datasets demonstrate that contamination severely degrades baseline fact checking performance, while our framework substantially mitigates this effect.", "tldr": "We perform claim verification on contaminated knowledge base and detect misinformation in the knowledge base via a document weight updating method.", "keywords": ["Fact Checking", "Misinformation Detection", "Large Language Model", "Contaminated Knowledge Base"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2fa01f3e622e1a0d1ee83542836859a566e75de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce automated fact checking under corrupted knowledge bases. The authors manually generate corrupted evidence, and then introduce a \"document trust\" score which increases trust in a document, if its stance towards a sub-claim is part of the majority (and decreases otherwise). They show that using an update function conditioned on the document retrieval score (the more relevant the document, the more the update of the trust score) works better than using linear updates or no updates at all, across models and LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Method seems to work consistently, according to the experiments\n- Lots of experiments across different datasets and models\n- Somewhat clever idea (but I think it's flawed)"}, "weaknesses": {"value": "- My biggest concern is that the problem statement seems ad hoc and artificial, and will just work in the specific scenario the authors introduce. They introduce a specific amount of misinformation, smaller than the amount of \"true\" information in a database (due to redundancy) and under these circumstances, the introduced method works fine. If there's more misinformation than that, everything breaks. If I would be orchestrating a misinfo campaign, I would make sure that there's enough misinfo around my claims in a knowledge base and it would be super redundant (e.g., 10x as much as in the paper), such that it would easily fool the introduced method (which basically counts how often things are part of a majority decision vs. minority decision). For example, see here: https://en.wikipedia.org/wiki/Operation_Orangemoody Now, one will obviously argue what could automated fact checking possibly do in such cases, and I would reply with: I think this is where current computational methods and evidence-based methods fail, and I think the paper should engage with this, and acknowledge that it's an artificial setting and only works as long as there's more true information than misinformation. Perhaps, also reframing towards \"accidental\" misinformation or something else helps? \n- Second, it seems curious that performance in Table 4 seems to be overall much higher than the misinfo baseline results in Table 1, and sometimes even higher than the performance of the clean method. See results for Climate-FEVER Qwen3-14B (66.26 in the clean setting vs. 66.88 using misinformation documents). I think this needs more discussion and explanation, but perhaps I just misunderstood what's going on here.\n- Tons of typos and sloppy writing (see a few pointers in Questions)"}, "questions": {"value": "- Line 39: A more appropriate citation here would be https://aclanthology.org/N18-1074/ \n- Would cite Pan et al., 2023 on line 46 aclanthology.org/2023.acl-long.386.pdf\n- https://www.nature.com/articles/s44168-025-00215-8 also experiment with an adversary knowledge base (see Table 2), perhaps this is also a useful framework?\n- Line 104: \"sources. fact\" (2 white spaces) --> \"sources. Automated fact checking ...\" + citation, e.g., FEVER, or https://aclanthology.org/2022.tacl-1.11.pdf or something\n- Line 105: I don't think I believe the part about misinformation. Add citations to where this is coming from? \n- Line 121: \"entityrela-\" \"entity-relation\"\n- line 128: \"misinformationa\"\n- Line 139: Weller et al., (2022) (can be done with \\citet{weller_2022} in overleaf)\n- Line 195: Nave --> \"Naive\"\n- 236: Or it might imply that the approach generates \"misinformation\" with tons of lexical overlap, making the retrieval trivial? Would love to see numbers on lexical overlap of fabricated evidence vs. true evidence\n- what is a dense knowledge base?\n- line 398:  \"documents in Table. 4, as a\" some typos (documents in Table 4 as a secondary)\n- line 409: \"By hybriding \" isn’t standard English\n- line 459: many typos: \"LLMsQwen3-14B and Llama-3.1-8B-Instructare\"\nWeaknesses: I disagree with the problem statement\n- Overall, paper needs a thorough pass to improve writing, clarity and fixing typos"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Mkz4de27Y", "forum": "xJm3HxHJpC", "replyto": "xJm3HxHJpC", "signatures": ["ICLR.cc/2026/Conference/Submission3506/Reviewer_oMqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3506/Reviewer_oMqg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761004263833, "cdate": 1761004263833, "tmdate": 1762916767540, "mdate": 1762916767540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles robust fact-checking when the knowledge base is contaminated. It proposes a framework using Claim Decomposition and a novel Dynamic Reweighting mechanism. This method iteratively updates document reliability weights based on LLM verification outcomes, steering subsequent retrieval toward trustworthy sources. The method uses an LLM to verify subclaims and iteratively updates the reliability weights of documents based on verification outcomes. The paper also introduces a protocol for generating adversarial misinformation to create a contaminated KB benchmark, which is used for evaluation. Experiments show that this benchmark degrades baseline performance, while the proposed method mitigates this degradation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The work focuses on the practical challenge of fact-checking against unreliable, misinformation. The framework introduces a dynamic feedback loop that updates document reliability based on verification outcomes, moving beyond static retrieve-and-read pipelines.\n2. Benchmark Contribution: The paper introduces a rigorous protocol for generating adversarial misinformation , providing a valuable benchmark for future robustness studies."}, "weaknesses": {"value": "1.  The iterative update to the global weight map W may introduce a dependency on the claim processing order. This potential confounder is not discussed or evaluated.\n2. The framework requires maintaining a state for every document in the KB. The feasibility of this approach for web-scale KBs is questionable and not addressed.\n3. The analysis of robustness is limited to the created contamination levels  and reduced contamination. The framework's performance under overwhelmingly redundant misinformation is not explored."}, "questions": {"value": "1. Was the processing order of claims randomized during experiments? Have you analyzed the potential impact of this order on the final results?\n2. Can you comment on the scalability of maintaining a global weight map for web-scale KBs?\n3. How would the framework be expected to perform if the ratio of misinformation to gold evidence were significantly higher than what was tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "euCsSM1FOk", "forum": "xJm3HxHJpC", "replyto": "xJm3HxHJpC", "signatures": ["ICLR.cc/2026/Conference/Submission3506/Reviewer_UwZ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3506/Reviewer_UwZ8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809376771, "cdate": 1761809376771, "tmdate": 1762916766562, "mdate": 1762916766562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors investigate the automatic fact-checking task when the evidence corpus is noisy and contains misinformation. They use an LLM to generate fabricated dataset for evaluation. Their method is to decompose claims, make a prediction for each sub-claim, and then aggregate the results using a weighted majority voting to obtain the final veracity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is to read, and the method is straightforward to re-implement."}, "weaknesses": {"value": "- As a person who has experience in automatic fact checking, my understanding is that the main problem in fact checking is twofold: 1) to retrieve relevant evidence, and 2) to resolve contradictory evidence. Therefore resolving contradiction has been already part of the problem and is not new. The contradiction might be due to existence of contradictory sources or due to the presence of false (inaccurate or fabricated) information. One early example of this is the TREC challenge on COIVD fact checking [1]. So I don't think that authors have discovered anything new or proposed any new research task.\n- Regarding the dataset composed by the authors: LLMs are not a good choice to generate and simulate human generated data [2], because they may unintentionally encode cues in the text that may be used by machine learning models as a shortcut to solve the task. An automated generated dataset can be used for training and validation, but not for evaluation.\n- Regarding the method: I can easily imagine that most real world claims consist of one single subclaim. This reduces the proposed model to a simple majority voting. The method is too naive and insignificant. (a widely accepted practice is to use meta data and source credibility to resolve misinformation)\n\n\n\n[1] https://pages.nist.gov/trec-browser/trec30/misinfo/overview/\n\n[2] A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wJtj4sWpSS", "forum": "xJm3HxHJpC", "replyto": "xJm3HxHJpC", "signatures": ["ICLR.cc/2026/Conference/Submission3506/Reviewer_6kAj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3506/Reviewer_6kAj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966134338, "cdate": 1761966134338, "tmdate": 1762916766195, "mdate": 1762916766195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies fact-checking when the evidence store is contaminated with misinformation. The proposed framework (i) decomposes a claim into subclaims, (ii) retrieves evidence per subclaim, (iii) uses an LLM to do stance grouping (support / refute / unrelated), and (iv) applies iterative document re-weighting so that supportive sources are up-weighted and refuting sources are down-weighted during subsequent retrieval/reranking. The authors also construct contaminated corpora by generating adversarially topical misinformation from gold evidence and false claims. Across four open-source LLMs and six benchmarks, the method recovers a substantial portion of the performance lost under contamination."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explicitly targets the realistic setting where the KB is not clean, and formalizes the learning loop with a verifier V, retriever R, and a weight map updated from stance decisions. The GETWEIGHT/UPDATEWEIGHT design is simple and transparent. \n2. The study spans four LLMs (Qwen3-14B, Llama-3.1-8B-Instruct, Gemma-3-12B-it, Mistral-7B-Instruct-v0.3) and six datasets (HoVer, EX-FEVER, HotpotQA, SciFact, PubHealth, Climate-FEVER), showing consistent improvements of the proposed reweighting over contaminated baselines. \n3. The contamination protocol—deriving misinformation from gold evidence and false claims—yields strong adversarial distractors that measurably degrade baselines, enabling rigorous stress-testing."}, "weaknesses": {"value": "1.\tThe problem formulation implicitly assumes misinformation is a minority and truth is redundant (KB redundancy). While often reasonable for Wikipedia, the method’s behavior when misinfo ≥ truth (or redundancy is low) is less clear. Some ablations reduce contamination, but the extreme-noise regime and failure modes deserve more analysis.\n\t2.\tThe reweighting hinges on LLM stance accuracy; systematic stance errors could amplify misinformation. A more explicit error-propagation analysis or confidence calibration for the verifier would strengthen the argument. (Figure/algorithm explain the mechanics but not robustness to verifier noise.)\n\t3.\tThe contamination strategy is topical (entity-consistent) and adversarial, but more discussion is needed on (a) how close it is to in-the-wild web noise, and (b) safeguards against inadvertently encoding dataset labels into style cues that retrievers/verifiers might exploit.\n\t4.\tThe paper mentions integrating weights into reranking; however, it would help to give a more explicit formula and discuss alternatives (e.g., score calibration/fusion variants). The algorithm box gives the core update but not the full fusion equation.\n\t5.\tThe approach requires LLM verification per subclaim over top-k documents; reporting wall-clock and cost (per example) would inform practical deployment."}, "questions": {"value": "1.\tHow sensitive is performance to stance classification accuracy? Could you report results using a weaker verifier and/or with controlled label noise injected into stance labels?\n\t2.\tWhat happens when the ratio of misinformation is pushed beyond your default protocol (e.g., >70% of retrieved evidence for a subclaim)? Any catastrophic flips? \n\t3.\tPlease provide the exact scoring equation used for reranking (e.g., how normalized retrieval scores combine with the sigmoid-transformed weights).\n\t4.\tHave you tried a non-Wikipedia KB (e.g., curated domain corpora) with less redundancy to test the reliance on redundancy assumptions?\n\t5.\tAny qualitative analysis where the system wrongly down-weights true refutations (i.e., the claim is false, but the loop favors supporters)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s1qHNwMBpw", "forum": "xJm3HxHJpC", "replyto": "xJm3HxHJpC", "signatures": ["ICLR.cc/2026/Conference/Submission3506/Reviewer_Tfww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3506/Reviewer_Tfww"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128728245, "cdate": 1762128728245, "tmdate": 1762916765585, "mdate": 1762916765585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}