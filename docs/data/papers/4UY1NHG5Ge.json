{"id": "4UY1NHG5Ge", "number": 6841, "cdate": 1757997789468, "mdate": 1759897889118, "content": {"title": "Genomic Foundationless Models: Pretraining Does Not Promise Performance", "abstract": "The success of Large Language Models has inspired the development of Genomic Foundation Models (GFMs) through similar pretraining techniques. However, the relationship between pretraining performance and effectiveness in downstream genomic tasks remains unclear. Additionally, the high computational cost of pretraining raises questions about its cost-efficiency. To assess the usefulness of pretraining in genomics, we evaluated seven different GFMs across 52 diverse genomic tasks, comparing them to their counterparts with randomly initialized weights. Surprisingly, we found that randomly initialized models can match or even surpass the performance of pretrained GFMs in finetuning and feature extraction tasks. We also discovered that pretrained GFMs fail to capture clinically relevant genetic mutations, which are crucial for understanding genetic disorders and phenotypic traits. Our results indicate that most of the current pretrained GFMs lack a ``foundational'' understanding of genomics and provide minimal utility, even for basic tasks such as sequence classification. These findings collectively highlight the need to critically rethink the pretraining approaches for genomics.", "tldr": "Pretrained Genomic Foundation Models offer little to no advantage over randomly initialized models on many genomic tasks.", "keywords": ["ai4science", "foundation models", "genomics", "biology", "pretraining", "deep learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6ddc18e944602b79d61b56b4dde977f75fa4258.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper evaluates the effectiveness of pretraining in Genomic Foundation Models (GFMs). The authors benchmark seven GFMs across 52 genomic tasks, comparing them with randomly initialized counterparts. Results show that randomly initialized models often match or even outperform pretrained GFMs in both fine-tuning and feature extraction settings. The study concludes that current research lacks a clear understanding of how pretraining contributes to model performance and offers new insights into how to achieve more efficient genomic modeling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Meaningful Perspective: The paper focuses on evaluating the effectiveness of pretraining in GFMs and raises new research questions and perspectives for genomic modeling, providing valuable insights for the community.\n\n- Comprehensive Evaluation: The study benchmarks 7 representative GFMs across 52 diverse genomic tasks, covering structural, functional, and regulatory dimensions.\n\n- Extensive Experimental Setup: The authors conduct detailed comparisons of fine-tuning techniques, including full fine-tuning and LoRA. Also perform extensive hyperparameter searches over learning rate, batch size, and other factors with nearly 10,000 fine-tuning experiments."}, "weaknesses": {"value": "- Potentially Unfair Comparison Setup: Randomly initialized models received embedding dimension optimization, but similar architectural optimization was not performed for pretrained models. What would happen if the pre-trained model also used a character tokenizer and a larger embedding dimension?\n- Analysis of Pretraining: The observation that a character-level tokenizer benefits randomly initialized models may indicate design issues in the pretrained models rather than reflecting the true effect of pretraining on genomic models.\n- Task Distribution: The paper evaluates 52 tasks, but it is unclear what criteria were used to select them. Are there specific types of tasks like long-range dependency tasks where pretrained models might perform better?\n- Generative Tasks: All seven models evaluated in the paper are classification models, and the 52 tasks focus primarily on classification, with no assessment of generative tasks. Although the authors explain their reasons for not using Evo, recent large-scale genomic models such as Evo2[1] and GenomeOcean[2] are designed for generative purposes. Do the conclusions drawn in this study also hold for generative models, or are they only valid when generative models are applied to classification tasks?\n- Quantized Model: In this paper, all models are tested under full precision. However, quantized genomic foundation models (GFMs) like GERM [3] also exist. Does the same conclusion hold for these models?\n\n[1] Genome modeling and design across all domains of life with Evo 2.\n\n[2] GenomeOcean: An Efficient Genome Foundation Model Trained on Large-Scale Metagenomic Assemblies.\n\n[3] Fast and Low-Cost Genomic Foundation Models via Outlier Removal."}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pCm71Kjrxz", "forum": "4UY1NHG5Ge", "replyto": "4UY1NHG5Ge", "signatures": ["ICLR.cc/2026/Conference/Submission6841/Reviewer_rYMn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6841/Reviewer_rYMn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447476919, "cdate": 1761447476919, "tmdate": 1762919100844, "mdate": 1762919100844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the prevailing assumption that large-scale unsupervised pretraining is inherently beneficial for Genomic Foundation Models (GFMs). The authors conduct an extensive empirical study comparing seven different GFMs against their randomly initialized counterparts across 52 diverse genomic tasks, spanning finetuning and feature extraction. The core finding is that randomly initialized models, when properly tuned, often match or even exceed the performance of their billion-scale pretrained counterparts.\n\nFurthermore, the paper introduces a critical set of analyses on genomic variation, demonstrating that current GFMs are largely insensitive to subtle, clinically relevant mutations like SNPs. The models perform at near-random chance on variant classification tasks and produce nearly identical embeddings for reference and mutated sequences. The authors conclude that current pretraining strategies, largely adapted from NLP, are insufficient for genomics and that the field must rethink its approach to tokenization, pretraining objectives, and evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Large-Scale, Rigorous Evaluation: The sheer scale of the study (7 models, 52 tasks) provides very strong evidence. The authors' commitment to a rigorous hyperparameter search for all models (random and pretrained) makes their comparison fair and robust.\n\n2. Novel and Critical Analysis: The genomic variation analysis (Sec 3.3) is a major strength. By showing that GFMs are insensitive to SNPs and fail on ClinVar data, the authors expose a critical blind spot in current models and evaluation methods. This finding alone is a significant contribution.\n\n3. Actionable Insights: The paper provides clear hypotheses for these failures (k-mer tokenization obscuring SNPs, high masking rates) and points toward concrete areas for improvement (e.g., character-level tokenizers, as used in their own Mistral model)."}, "weaknesses": {"value": "1. Limited Scope (Generative Tasks): The paper's claims are based entirely on classification and feature extraction tasks. The authors briefly concede in the discussion and conclusion that pretraining might still be valuable for generative tasks. This is an important limitation, and the bold title (\"Pretraining Does Not Promise Performance\") might be a slight overstatement. The weakness is minor, as the paper's scope is already large, but it should be stated more prominently.\n\n2. \"What\" over \"Why\": The paper excels at showing that pretraining fails but is less definitive on why. The discussion of tokenization and masking rates is insightful but largely correlational. The paper would be strengthened by even a single targeted ablation, e.g., pretraining two identical small models, one with k-mer and one with character-level tokenization, to prove that the tokenizer is the key factor."}, "questions": {"value": "1. The authors convincingly argue that k-mer/BPE tokenizers are a major issue, especially for the genomic variation tasks. Their own Mistral model, which uses a character tokenizer and performs well, seems to support this. Could the paper's central finding be more narrowly (and perhaps more accurately) stated as \"Current k-mer-based pretraining strategies are ineffective\" rather than a blanket statement about all pretraining?\n\n2. Following on the previous point, the feature extraction results (Table 2, Fig. 3) suggest that a randomly-initialized model with a character tokenizer and an optimized embedding dimension is a top performer. Is this also true for the finetuning tasks? A comparison of \"best random (char-tokenizer)\" vs. \"best pretrained\" would be very illuminating.\n\n3. The authors attribute their different findings from prior work (e.g., Dalla-Torre et al., 2024) to their more rigorous hyperparameter search. As a sanity check, were they able to reproduce the original paper's results (i.e., showing a benefit for pretraining) by using the original fixed hyperparameters? This would definitively confirm that the HPO sweep is the key methodological difference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5DoVrXeAFY", "forum": "4UY1NHG5Ge", "replyto": "4UY1NHG5Ge", "signatures": ["ICLR.cc/2026/Conference/Submission6841/Reviewer_4ABt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6841/Reviewer_4ABt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814864447, "cdate": 1761814864447, "tmdate": 1762919100138, "mdate": 1762919100138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission evaluates seven Genomic Foundation Models (GFMs) on 52 tasks spanning NT Benchmark, GUE, and Genomic Benchmarks, comparing pre-trained checkpoints against randomly initialized counterparts under fine-tuning, frozen‑feature (“feature extraction”), and genomic variation settings. The headline result is that models trained from scratch often match or surpass their pre-trained versions; moreover, embeddings from multiple GFMs are notably insensitive to clinically relevant variants, with cosine similarity remaining ~0.9–0.999 even after many SNPs are introduced. The paper argues that (i) existing NLP‑style pre-training is a poor investment for regulatory genomics classification and (ii) tokenization/architectural choices (e.g., character tokenization, larger embedding dimensions) dominate performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **(S1)** Breadth and scale of evaluation with reproducibility intent. The study spans 7 models and 52 tasks across three popular benchmarks, with nearly 10k fine-tuning runs and LR sweeps, and reports model‑wise subgroup results. Datasets and checkpoints are standard, and code is linked (anonymized) in the paper, which, taken together, improves reproducibility. Meanwhile, the Figure 1 summary is easy to parse and consistently shows a limited advantage for pre-training when judged against a strong random baseline. The authors also empirically prefer full fine-tuning over LoRA, reducing a common confound in cross‑model comparisons.\n\n- **(S2)** Variant-sensitivity diagnostics reveal a critical gap. The mutation-sensitivity experiments and ClinVar log-likelihood–ratio AUROCs near 0.5 jointly show that many GFMs’ representations are remarkably insensitive to clinically relevant point mutations, echoing long-standing concerns that subword/k‑mer tokenization blurs SNV granularity.\n\n- **(S3)** Positioning within ongoing benchmarking discussions. The paper’s thesis aligns with emerging benchmark evidence that SFT matters and that supervised long-range models can still dominate on gene expression, underscoring that what we benchmark matters as much as how."}, "weaknesses": {"value": "- **(W1)** “Pre-training vs. random” comparisons are confounded by architecture/tokenizer changes. The feature-extraction claim that random models can beat pre-trained hinges on changing the tokenizer (char rather than original k‑mer/BPE) and increasing the embedding size for the random arm (Table 2 and Figure 3), whereas pre-trained arms keep their native tokenizers/widths. This violates ceteris paribus, conflating the benefit of model design with the absence of pre-training. A fair test needs identical architectures/tokenizers/widths with/without pre-training. As is, the headline conclusion is directionally plausible but not causally isolated.\n\n- **(W2)** Variant sensitivity method is too blunt to support strong claims. The mutation‑sensitivity analysis relies on global pooling and cosine similarity of full‑sequence embeddings, while high similarities that sometimes increase with more mutations likely reflect pooling/normalization effects rather than true biological blindness. Moreover, the LLR analysis lacks detail for encoder-only masked LMs (pseudo-likelihood vs. left-to-right), complicating an apples-to-apples comparison with decoders. Token-level distances, attribution at mutated loci, and clearly specified pseudo-LLR for encoders are necessary.\n\n- **(W3)** Task coverage underweights where long-range biology is known to matter. The selected tasks span mainstream benchmarks, but the suite lacks gene-expression regression (e.g., bulk RNA/CAGE) and enhancer–gene linkage tasks for which long-range inductive biases and explicit supervised training (e.g., Enformer) remain strong baselines and stress tests.\n\n- **(W4)** Positioning more concurrent architecture findings could be sharper. The paper’s narrative at times attributes the observed wins to the absence of pretraining, when the architecture/inductive bias could be dominant. Independent recent results indicate that simple, well-tuned CNNs can outperform SSM/Transformer DNA models on many tasks without pretraining (e.g., ConvNova), suggesting that architectural priors and receptive-field design can rival or exceed pretraining gains. This alternative explanation deserves explicit treatment in the Discussion."}, "questions": {"value": "- **(Q1)** Can the authors provide apple-to-apple ablations where architecture, tokenizer, embedding size, positional encoding, and training schedule are identical, and the only difference is with vs. without pre-training? This would directly test the causal value of pre-training beyond the confounds noted in Table 2 and Figure 3.\n\n- **(Q2)** How do the findings change in label-scarce regimes (e.g., 1%, 5%, and 10% of labels)? Please include learning curves and area under the data curve statistics to test whether pre-training is more helpful at low data.\n\n- **(Q3)** For mutation sensitivity, could the authors further provide in-silico mutagenesis with base-resolution attribution (e.g., Grad-CAM) and compare to CADD/Enformer scores and to eQTL/sQTL ground truths (AUPRC, per-gene AUROC)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no obvious Code of Ethics concerns. The work uses public genomic resources (IGSR/1000G, GRCh38, GENCODE, ClinVar) and reports only aggregate performance. Please confirm that all data is de‑identified and used under the respective licenses."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FwRoAIwdWC", "forum": "4UY1NHG5Ge", "replyto": "4UY1NHG5Ge", "signatures": ["ICLR.cc/2026/Conference/Submission6841/Reviewer_zXbv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6841/Reviewer_zXbv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028278974, "cdate": 1762028278974, "tmdate": 1762919099593, "mdate": 1762919099593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}