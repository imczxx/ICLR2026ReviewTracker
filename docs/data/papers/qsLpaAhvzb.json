{"id": "qsLpaAhvzb", "number": 25131, "cdate": 1758364527621, "mdate": 1759896733040, "content": {"title": "Learning to Reject Low-Quality Explanations via User Feedback", "abstract": "Machine Learning predictors are increasingly being employed in high-stakes applications such as credit scoring. Explanations help users unpack the reasons behind their predictions, but are not always ``high quality\". That is, end-users may have difficulty interpreting or believing them, which can complicate trust assessment and downstream decision-making. We argue that classifiers should have the option to refuse handling inputs whose predictions cannot be explained properly and introduce a framework for learning to reject low-quality explanations (LtX) in which predictors are equipped with a rejector that evaluates the quality of explanations. In this problem setting, the key challenges are how to properly define and assess explanation quality and how to design a suitable rejector. Focusing on popular attribution techniques, we introduce ULER (User-centric Low-quality Explanation Rejector), which learns a simple rejector from human ratings and per-feature relevance judgments to mirror human judgments of explanation quality. Our experiments show that ULER outperforms both state-of-the-art and explanation-aware learning to reject strategies at LtX on eight classification and regression benchmarks and on a new human-annotated dataset, which we publicly release to support future research.", "tldr": "We introduce a framework for learning to reject low-quality explanations in which predictors are equipped with a rejector that evaluates the quality of explanations and propose ULER, which learns a simple rejector to mirror human judgments.", "keywords": ["Learning to Reject", "Explainable AI", "Explanation quality metrics", "Human-annotated data"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4042f3e80ca677f80d756c5b8879124d445ba08.pdf", "supplementary_material": "/attachment/718f763a6d170f3ca882f4c122607dfbf0d071fc.zip"}, "replies": [{"content": {"summary": {"value": "This authors introduce Learning to Reject Low-Quality Explanations (LtX), extending the traditional Learning to Reject framework to consider explanation quality as a criterion for deferring predictions. The proposed method, ULER (User-centric Low-quality Explanation Rejector), trains a rejector to predict when explanations are rated as low-quality by users. ULER leverages a small set of human-labeled data and optional per-feature feedback through a data augmentation scheme. Experiments on eight tabular benchmarks with simulated human judgments and a new human-annotated soccer dataset show that ULER outperforms standard LtR methods and PASTARej in filtering out low-quality explanations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors address a crucial yet under-explored dimension of trustworthy AI - the quality of explanations as a basis for deferring predictions to human experts. This is an important step toward integrating XAI into selective prediction frameworks.\n\n- ULER’s formulation is modular, combining human feedback, lightweight data augmentation, and a threshold-based rejector. The design is compatible with existing explainers like SHAP or LIME and does not depend on model architecture.\n\n- The experiments are extensive, spanning multiple tabular benchmarks and a new human-annotated dataset, and the authors detail datasets, hyperparameters, and reproducibility plans. \n\n- The authors plan to release a new dataset of human-rated explanations, which will be valuable to the community and help standardize evaluation of human-aligned XAI metrics."}, "weaknesses": {"value": "- The paper treats explanation quality as equivalent to human agreement, but it does not clearly define the conceptual dimensions this label captures or explain why they are important.\n\n- Previous work has highlighted that explanations can appear plausible to humans without accurately reflecting the model’s reasoning, motivating metrics that assess faithfulness rather than plausibility [1,2,3]. This paper, instead, emphasizes incorporating human judgment into the Learning to Reject framework over machine-side metrics. The motivation and rationale for this alternative perspective should be more clearly articulated.\n\n- From the experiments, it appears that the paper assumes human experts understand the ground-truth data-generating process and can reliably identify important features. Making this assumption explicit would help clarify why human judgment is prioritized over machine-side metrics, and better define the range of tasks that ULER can be applied on (tasks where human know the ground-truth data-generating process)\n\n- ULER and PASTA both aim to approximate human judgments of explanation quality. While the paper lists three main differences - PASTA is designed for image data with an embedding network, lacks a rejection mechanism, and seeks a dataset-agnostic metric - the experimental setup for PASTARej removes the embedding network, fits its scoring network to predict human judgment, and uses it for rejection. The paper could more clearly clarify the conceptual distinctions between ULER and PASTARej.\n\n- The benchmark experiments rely on simulated human judgments generated by an LLM. Since ULER (and PASTARej) is trained on these labels while other baselines are not, the reported performance advantage may be partially influenced by this setup.\n\n- Converting 5-point Likert responses into binary labels reduces annotation noise but removes information about gradations in perceived quality. This simplification may limit the rejector’s ability to handle borderline or ambiguous cases that could be practically relevant.\n\n[1] Chowdhury, Townim, et al. \"Looking in the Mirror: A Faithful Counterfactual Explanation Method for Interpreting Deep Image Classification Models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n[2] Agarwal, Chirag, Sree Harsha Tanneru, and Himabindu Lakkaraju. \"Faithfulness vs. plausibility: On the (un) reliability of explanations from large language models.\" arXiv preprint arXiv:2402.04614 (2024).\n[3] Lu, Xiaolei, and Jianghong Ma. \"Does faithfulness conflict with plausibility? an empirical study in explainable ai across NLP tasks.\" arXiv preprint arXiv:2404.00140 (2024)."}, "questions": {"value": "- Could you formalize what “user-perceived explanation quality” entails beyond trust? Are there specific criteria that distinguish a high-quality explanation from a low-quality one?\n\n- Could you clarify why human judgment is preferred over machine-side metrics for assessing explanation quality, especially considering prior work suggesting that explanations can appear plausible to humans without faithfully reflecting the model’s reasoning?\n\n- Does your framework assume that human experts understand the ground-truth data-generating process and can reliably identify important features? If so, how might this assumption limit the types of tasks where your framework is applicable? If not, how should readers interpret the reliability of human judgment in this context?\n\n- Could you clarify the distinctions between ULER and PASTARej, particularly in the context of your experiments?\n\n- The paper relies on simulated judgments from an LLM. Were these validated against real human ratings, and if not, how should readers interpret the benchmark results?\n\n- In lines 212–216, it is stated that “ULER tends to yield marginal improvements in predictive performance for the accepted inputs by rejecting explanations of incorrect predictions.” Which experimental results directly support this claim?\n\n- How adaptable is ULER to non-feature-based explanations (e.g., textual rationales or counterfactual examples)? Is the framework specific to feature attributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xpt1SscVW0", "forum": "qsLpaAhvzb", "replyto": "qsLpaAhvzb", "signatures": ["ICLR.cc/2026/Conference/Submission25131/Reviewer_zyBV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25131/Reviewer_zyBV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761284831877, "cdate": 1761284831877, "tmdate": 1762943338892, "mdate": 1762943338892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for identifying and rejecting low-quality training samples to improve the performance of ML models. The authors formulate this as a learning-to-reject problem, training a rejection function alongside the primary predictive model to detect data samples that may be noisy, mislabeled, or otherwise detrimental to model generalization. The proposed approach uses meta-learning to optimize the rejection mechanism with respect to a held-out validation objective. Experiments across multiple tabular datasets demonstrate improved generalization and robustness compared to baseline data filtering or robust training methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors run extensive experiments across eight diverse tabular benchmarks and a real-world soccer analytics user study with 5,250 human annotations."}, "weaknesses": {"value": "1. The paper evaluates all competitors on eight benchmark datasets using simulated human judgments generated by Llama-3.1-8B-Instruct. While using LLMs as judges is an emerging and sometimes practical trend, the choice of such a small model (8B) raises **serious reliability concerns.** The core issue is not the general idea of using an LLM judge, but rather the limited evaluative and reasoning capacity of this particular model size. Current literature suggests that larger models (e.g., GPT-4, Claude 3, or at least Llama-3.1-70B) align more closely with human judgments, whereas 8B-class models exhibit inconsistent performance, particularly on domain-specific tasks. Results derived from Llama-3.1-8B-Instruct (Q1 and Q2) should be treated as exploratory or diagnostic, rather than definitive empirical evidence. \n2. Evaluating the quality of explanations in XAI has been widely studied, with numerous metrics and criteria already established. The concept of learning to reject or penalizing low-quality explanations is not novel. It has antecedents in selective prediction, abstention learning, and robust loss design. Modern approaches such as Direct Preference Optimization (DPO) offer a more principled and interpretable framework for learning from preferences or penalties, especially in the post-LLM era. Although I am not a big fan of LLM, the idea behind the current work and the experimental setting should not be targeted at ICLR. \n3. Presentation issue: most figures could be improved for professionalism, clarity and readability. Moreover, the mathematical exposition of the rejection function lacks necessary precision."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lvjDsHieH2", "forum": "qsLpaAhvzb", "replyto": "qsLpaAhvzb", "signatures": ["ICLR.cc/2026/Conference/Submission25131/Reviewer_fLkm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25131/Reviewer_fLkm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800521314, "cdate": 1761800521314, "tmdate": 1762943338659, "mdate": 1762943338659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new problem setting called \"Learning to Reject Low-Quality Explanations\" (LtX), which argues that machine learning models in high-stakes domains should not only provide predictions but also have the ability to withhold predictions when their accompanying explanations are of low quality from a user's perspective. To address this, the authors propose a novel framework called \"User-centric Low-quality Explanation Rejector\" (ULER). ULER trains a rejector model on a small set of human-annotated explanations to learn a policy for identifying and rejecting unsatisfactory explanations. A key component of ULER is a data augmentation strategy that leverages per-feature feedback from users to create a more robust training set for the rejector. The authors conduct a comprehensive empirical evaluation of ULER on eight benchmark datasets with simulated human feedback and a new human-annotated dataset from a real-world soccer analytics task. The results demonstrate that ULER outperforms existing \"Learning to Reject\" (LtR) strategies and various explanation quality metrics in identifying and rejecting low-quality explanations, and is better at mimicking human judgments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the novel and well-motivated problem of \"Learning to Reject Low-Quality Explanations\" (LtX), which addresses a critical gap in the existing \"Learning to Reject\" (LtR) literature that has traditionally focused only on prediction quality while ignoring the quality of the accompanying explanations. \n\n- The proposed ULER framework is user-centric, directly learning from human feedback to align the rejection mechanism with human judgments of explanation quality. The use of both high-level quality ratings and more granular \"per-feature relevance judgments\" to inform the rejector is a sensible approach. The data augmentation strategy, which perturbs \"the features with correct relevance scores\" for low-quality explanations, is a creative method to expand the training data from a small set of annotations.\n\n- The empirical evaluation is extensive, covering eight benchmark datasets with simulated human judgments and a user study with a newly collected human-annotated dataset. The authors' commitment to creating and releasing \"the first larger-scale (1050 examples, 5 annotations each) data set of human-annotated explanations\" is a valuable contribution to the research community that will facilitate future work in this area. The user study itself is thoughtfully designed, with considerations for participant expertise, clear instructions, and filtering criteria to ensure data quality (Section 4.2)."}, "weaknesses": {"value": "- The framing of ULER as \"explainer-agnostic\" is a significant overstatement. The methodology is fundamentally tied to feature attribution methods that produce a \"relevance score zi ∈ R to each input feature xi\" . The entire ULER framework, including the per-feature feedback (\"indicate as Wz (resp. Cz) the indices of the features whose relevance the user deems wrong (resp. correct)\") and the data augmentation strategy, is designed around manipulating these feature importance vectors. It is unclear how ULER would be applied to other important classes of explanations, such as counterfactual explanations, example-based explanations, or concept-based explanations, which do not naturally produce per-feature relevance scores. This limitation significantly narrows the practical applicability of ULER beyond the realm of feature attribution methods.\n\n- The use of a LLM (Llama-3.1-8B-Instruct) to \"simulate human quality judgments\" for the benchmark experiments is a major methodological weakness. While the authors followed an existing approach, this simulation introduces a significant potential for confounding variables. An LLM's \"understanding\" of feature importance and its alignment with human intuition is not guaranteed and is heavily dependent on the prompting strategy. The paper presents the results from these simulated experiments as strong evidence of ULER's effectiveness, but it is plausible that ULER is simply better at modeling the specific artifacts and biases of the LLM used for data generation rather than genuine human reasoning. The paper lacks a critical discussion of the limitations of this simulation and how it might affect the validity and generalizability of the findings from the benchmark datasets. (There also seems to be some relevant work in this direction: https://openreview.net/pdf?id=MOtZlKkvdz)\n\n- The experimental design has a potential flaw in its choice to use only KernelSHAP for generating explanations. While KernelSHAP is a popular method, different explanation techniques have distinct properties and failure modes. The paper's conclusions about the ineffectiveness of machine-side metrics (e.g., stability, faithfulness) might be specific to KernelSHAP. It is possible that for other explainers, these metrics could be more effective at identifying low-quality explanations. By not evaluating ULER with a more diverse set of explainers (e.g., LIME, Integrated Gradients), the paper misses an opportunity to demonstrate the true robustness and generalizability of the proposed approach. The claim that ULER can \"assess the perceived quality of attributions irrespectively of how these are computed\" is not sufficiently supported by the experiments.\n\n- The scalability and practicality of the proposed annotation process raise concerns. The authors state that ULER \"uses modest amounts of human annotations\", but the user study still required collecting 5250 annotations for a single dataset with only a few features. In many real-world applications, models can have hundreds or thousands of features, which would make the per-feature feedback process (\"select individual features they believed were misused in the explanation\") prohibitively time-consuming and cognitively demanding for human annotators. The paper does not adequately address how the ULER framework would scale to such high-dimensional settings, which could be a significant barrier to its adoption in practice."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uhLDCZU3YI", "forum": "qsLpaAhvzb", "replyto": "qsLpaAhvzb", "signatures": ["ICLR.cc/2026/Conference/Submission25131/Reviewer_4xMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25131/Reviewer_4xMk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919492220, "cdate": 1761919492220, "tmdate": 1762943338101, "mdate": 1762943338101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on developing a framework such that predictor models can learn to reject when explanations are low-quality.\n* The authors propose ULER (User-centric Low-quality Explanation Rejector), a rejector model trained on human annotated data and per-feature relevance judgements.\n* ULER outperforms other strategies on eight benchmarks and a new human-annotated soccer dataset.\n* The authors also release the dataset of human-annotated explanations in the soccer domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The method addresses an important alignment gap: explanation quality is important for high-stakes applications but is typically not considered in rejection strategies.\n* The authors provide comprehensive results across many standard benchmark datasets, including a new dataset they created and released. \n* The methodology is well presented and straightforward."}, "weaknesses": {"value": "* As acknowledged in the paper, the work focuses on tabular data and the approach requires human annotations, which limits how generalizable it is. \n* Many of the baselines (besides PASTA) are not optimized/designed for rejecting low-quality explanations, so the comparison on that metric feels a bit biased. Also, the method seems to be quite similar to PASTA except for the rejection framing. The paper could be improved by including some of the details on the differences to PASTA from Appendix B.2 in the main body.\n* The method seems to be reliant on having a clear ground truth for the per-feature annotations. How would it perform if there is annotator disagreement or if the features are highly correlated?"}, "questions": {"value": "How did you assess the quality of the simulated human judgements from Llama-3.1-8B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BaCHYZfUfb", "forum": "qsLpaAhvzb", "replyto": "qsLpaAhvzb", "signatures": ["ICLR.cc/2026/Conference/Submission25131/Reviewer_JGDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25131/Reviewer_JGDU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955335186, "cdate": 1761955335186, "tmdate": 1762943337385, "mdate": 1762943337385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}