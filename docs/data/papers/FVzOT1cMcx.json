{"id": "FVzOT1cMcx", "number": 20462, "cdate": 1758306434276, "mdate": 1759896976332, "content": {"title": "SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories", "abstract": "Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development (TDD) - \"test first, write code later\", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs (e.g., GPT-5, Claude Sonnet), with limited exploration of open-source models likely due to their weaker performance. To address this, we propose **SWE-Tester** - a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of **41K** instances from **2.6K** open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to **10\\%** in success rate and **21\\%** in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data and larger models These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.", "tldr": "A novel pipeline for training open-source LLMs for generating bug reproduction tests.", "keywords": ["Software Testing", "Open source LLMs", "Fine tuning for SWE tasks", "Datasets", "Benchmark"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/474e80a88ffdecdf4b4bcdb4a85a44c18b8b612a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes training open source models specifically on the task of repository-level test generation. They collect a large number of issues from GitHub repositories, restore a state where relevant issue-reproducing tests are missing from a state that contains issue-reproducing tests, and finetune several open-weight LLMs on creating/editing relevant tests (no finetuning on issue localization). Their evaluation demonstrates significant improvements in test generation capabilities across a range of open-weight models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- I agree with the authors that test generation is underexplored and think their approach to mirror approaches like SWE-smith into test generation makes sense. Focusing only on code editing is well motivated and admissable.\n- The ablations are clearly defined and interesting\n- Writing and figures are clear, legible and concise"}, "weaknesses": {"value": "The main weakness I see is that this work is nothing fundamentally novel. However, I think there are interesting insights about the bottleneck of open-source models (i.e., that this is the editing step) and how to generate training data for test generation.\n\nGeneral disclaimer: I am not an expert in finetuning/training LLMs, so I may have missed crucial details in this domain.\n\nSmall nitpicks\n- Line 225 could point out that filtering out swt-bench instances is not only relevant to reduce noise but also to avoid training contamination\n- Line 238 should use citep instead of citet\n- Typo in Line 399 \"isolated\""}, "questions": {"value": "Can you evaluate the related works LIBRO and Zero-Shot on the respective base models evaluated? This would allow to assess whether the trained LLMs clearly outperform the respective untrained, but scaffolded base LLMs. This would also more clearly highlight the benefits of the proposed training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wctPN1eb9Q", "forum": "FVzOT1cMcx", "replyto": "FVzOT1cMcx", "signatures": ["ICLR.cc/2026/Conference/Submission20462/Reviewer_ja9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20462/Reviewer_ja9d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655778355, "cdate": 1761655778355, "tmdate": 1762933904134, "mdate": 1762933904134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the suboptimal performance of open-source models in issue reproduction by constructing a dedicated training dataset for this task. The proposed approach improves the performance of multiple open-source models on issue reproduction. However, the proposed method lacks methodological novelty, and the experiments do not include appropriate baselines for comparison."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important problem in software engineering—**bug reproduction**—and improves LLM performance on this task through targeted training.\n- Conducts training across multiple models and provides detailed analyses of experimental results."}, "weaknesses": {"value": "- The data construction method and reproduction pipeline are largely adapted from well-established approaches in the issue resolution literature; the work mainly applies these existing methods to the issue reproduction task, which limits its methodological novelty for a top-tier conference like ICLR.\n- Focuses solely on the “edit exactly one test file” scenario, which may hurt generalizability.\n- Lacks appropriate baselines. Although few prior works explicitly target issue reproduction, many **code agents** perform issue reproduction as part of the issue resolution process; comparisons with training methods designed for such agents would make the evaluation more convincing."}, "questions": {"value": "- Why are different localization pipelines used for source code and test code?\n- Typo in line 250: “atleast”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLihzS0mlF", "forum": "FVzOT1cMcx", "replyto": "FVzOT1cMcx", "signatures": ["ICLR.cc/2026/Conference/Submission20462/Reviewer_Ca4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20462/Reviewer_Ca4f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900167464, "cdate": 1761900167464, "tmdate": 1762933902687, "mdate": 1762933902687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **SWE-Tester**, a framework for training open-source LLMs to automatically generate issue reproduction tests from natural language issue descriptions and buggy repositories. The proposed workflow follows a two-step static pipeline: (1) code localization to retrieve relevant source and test files, and (2) code editing to modify test files using a Search/Replace format. A large dataset of 41K instances from 2.6K repositories is curated, and multiple open LLMs (Qwen2.5-Coder, Llama3.1, Gemma3) are fine-tuned. The models achieve up to +10% success rate and +21% change coverage improvements on SWT-Bench Verified. The paper provides solid empirical results and a valuable dataset contribution, but its overall framework remains agentless and relies heavily on test-time scaling rather than true agentic reasoning or autonomy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors evaluate multiple open models of different sizes and families, analyze scaling effects in both training data and inference-time compute, and offer detailed quantitative insights.\n\n- The dataset of 41K issue–test pairs is well-filtered and reproducible, providing a strong foundation for open-source SWE research.\n\n- The workflow is simple and interpretable, with carefully described steps for localization, editing, and evaluation.\n\n- The reported gains show that open-source LLMs can meaningfully improve on real-world SWE benchmarks through fine-tuning."}, "weaknesses": {"value": "- The proposed framework is purely a static two-step pipeline—there is no reasoning loop, reflection, or autonomous planning. As the community rapidly transitions toward agentic SWE systems, this direction feels inherently limited and non-scalable. It lacks the ability to generalize beyond the fixed workflow or adapt dynamically to complex issue contexts.\n\n- The performance improvements are largely achieved through sampling multiple patches and reranking rather than stronger modeling or reasoning capabilities. This kind of test-time scaling can inflate benchmark scores but does not address the underlying challenge of autonomous issue understanding or causal reasoning in code."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7iwDFYoSIM", "forum": "FVzOT1cMcx", "replyto": "FVzOT1cMcx", "signatures": ["ICLR.cc/2026/Conference/Submission20462/Reviewer_2xMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20462/Reviewer_2xMV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921486219, "cdate": 1761921486219, "tmdate": 1762933902228, "mdate": 1762933902228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-step static pipeline for automatically generating issue reproduction tests from natural language descriptions and buggy code. The workflow first localizes relevant source and test files, then modifies the test files using a Search/Replace format. This methodology is encapsulated in a framework named SWE-Tester, for which several open-source LLMs (Qwen2.5-Coder, Llama3.1, Gemma3) were fine-tuned on a newly curated dataset of 41K instances. The resulting models demonstrate strong performance, with up to a +10% success rate and +21% change coverage increase on SWT-Bench Verified. Despite its solid empirical results and valuable dataset contribution, the framework's overall design remains agentless, depending on test-time scaling over genuine agentic reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Significant Performance Gains: The study reports substantial performance improvements, demonstrating the significant potential of open-source LLMs to effectively address real-world software engineering benchmarks.\n\n2. Solid & Reproducible Foundation: The research is grounded in a well-curated and reproducible dataset of 41,000 issue-test pairs, establishing a solid foundation for future studies in open-source software engineering.\n\n3. Transparent & Simple Workflow: The paper introduces a straightforward and interpretable workflow, with each step—including localization, editing, and evaluation—being meticulously detailed.\n\n4. Comprehensive Model Analysis: A comprehensive evaluation is conducted across a diverse set of open models of various sizes and families. This analysis yields detailed quantitative insights into the scaling effects of both training data and inference-time compute."}, "weaknesses": {"value": "* Superficial Performance Gains: The reported improvements are primarily driven by a brute-force approach of sampling and reranking multiple patches, rather than by genuine advancements in the model's reasoning capabilities. This reliance on test-time scaling may inflate benchmark scores but fails to address the core challenge of autonomous issue comprehension and causal reasoning in code.\n\n* Limited and Inflexible Architecture: The framework is fundamentally a static, two-step pipeline, devoid of any reasoning loops, reflection, or autonomous planning. This rigid design is inherently limited and non-scalable, particularly as the research community shifts towards more dynamic, agentic software engineering systems. As a result, it lacks the ability to generalize beyond its fixed workflow or adapt to the complexities of real-world issues."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Ecf0psPA6", "forum": "FVzOT1cMcx", "replyto": "FVzOT1cMcx", "signatures": ["ICLR.cc/2026/Conference/Submission20462/Reviewer_pK24"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20462/Reviewer_pK24"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000628439, "cdate": 1762000628439, "tmdate": 1762933901666, "mdate": 1762933901666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}