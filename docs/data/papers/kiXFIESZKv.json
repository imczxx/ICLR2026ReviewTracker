{"id": "kiXFIESZKv", "number": 7814, "cdate": 1758037287676, "mdate": 1763568853360, "content": {"title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward—so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.", "tldr": "Zero-Variance Prompts is a valuable source of learning signals for RLVR to improve LLM Reasoning.", "keywords": ["large language models", "reinforcement learning with verifiable rewards", "llm reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5eaaa9ddc50b93c64d81dc06a2c9cc34772710e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical inefficiency in Reinforcement Learning with Verifiable Rewards (RLVR), known as the \"advantage vanishing\" problem. Standard methods, such as GRPO, fail to learn from \"zero-variance prompts\"—instances where all sampled responses are either uniformly correct or incorrect—because their advantage calculation collapses to zero. The authors compellingly argue that this constitutes a significant source of inefficiency, as these prompts are both computationally expensive (rollouts consume approximately 50% of training time) and highly prevalent (comprising 30-99% of the data). The proposed method, RL-ZVP, leverages these prompts rather than filtering them. It achieves this by defining a new advantage signal: the direction is set to positive for all-correct groups and negative for all-incorrect groups, while the magnitude is modulated by token-level entropy. This \"entropy-guided\" shaping mechanism rewards high-entropy (uncertain, complex) tokens in correct responses and penalizes low-entropy (confident) tokens in incorrect responses. Rigorous experiments demonstrate that RL-ZVP significantly outperforms GRPO. Notably, it also surpasses modern filtering-based baselines (e.g., GRPO-DS, GRESO), even when those baselines are allocated 3–5 times more computational budget for rollouts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength lies in its novel and intuitive solution to a practical and well-defined problem. The entropy-guided advantage shaping mechanism is a well-motivated and insightful design that effectively rewards productive, uncertain reasoning steps while penalizing confident errors. This strong methodology is validated by a particularly rigorous experimental setup. The authors compare RL-ZVP not only to the standard GRPO but also to two strong baselines (GRPO-DS, GRESO) specifically designed to filter the problematic prompts. By testing under both equal-rollout (fair) and equal-gradient-step (baseline-favored) conditions, the paper offers compelling evidence for RL-ZVP's superior performance and efficiency. This quantitative strength is substantiated by in-depth analysis, including an examination of training dynamics that explains its enhanced stability (leveraging signals from both easy and hard prompts at different training stages) and qualitative examples (Appendix D) that demonstrate a tangible improvement in reasoning correctness, not just stylistic complexity."}, "weaknesses": {"value": "1）The method proposed is highly sensitive to the $\\alpha$ hyper-parameter. The experiments results in Table 3 demonstrate that the optimal performance window for the scaling factor $\\alpha$ is very narrow. This raises concerns that the method may require extensive and precise fine-tuning when applied to different datasets or models.\n\n2) The description on negative prompts seems confusing. The logic for advantage shaping (Section 3.2) is asymmetrical. Positive prompts reward high-entropy tokens far more than  negative prompts penalize high-entropy tokens. Could the author explain why use this setting?\n\n3 I think it will be better to provide more sufficient ablation Studies. The ablation study Table 2, while strong, omits a key comparison. It does not test a symmetrical penalty formula for negative prompts (e.g., $\\hat{A} = -\\alpha H_{i,t}$, which would penalize high-entropy error tokens more to see how it would affect performance."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4gCLA4JQHz", "forum": "kiXFIESZKv", "replyto": "kiXFIESZKv", "signatures": ["ICLR.cc/2026/Conference/Submission7814/Reviewer_3GQ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7814/Reviewer_3GQ5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726409782, "cdate": 1761726409782, "tmdate": 1762919855836, "mdate": 1762919855836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to leverage Zero-Variance Prompts (consisting of fully correct or fully incorrect responses) during RL to enable more efficient learning. The authors propose using token-level entropy-based advantage updates for these responses. Extensive experiments are conducted to demonstrate the effectiveness of the method on reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow.\n\n- The proposed method of assigning advantages to tokens in responses to zero-variance prompts is intuitive and well motivated.\n\n- Extensive experiments are conducted to demonstrate the effectiveness of the method on reasoning tasks."}, "weaknesses": {"value": "- Significance of the problem: Are there any empirical statistics on the proportion of zero-variance prompts? I notice that the authors only train 8B models on the harder DAPO-Math-17k dataset. It would be helpful to also show empirical results on Math with 8B models (scenarios with fewer zero-variance prompts) to evaluate the effect of the newly designed advantage. We might expect that it does not hurt performance in scenarios with fewer zero-variance prompts, while providing greater benefits when the number of zero-variance prompts is larger.\n\n- How is it ensured that the advantages assigned to tokens in zero-variance prompts are on the same scale as normal advantages? Could differences in scale affect training stability?\n\n- How should an appropriate value of α be chosen? Are there any guidelines or heuristics for selecting it?"}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ABsPBNAYx", "forum": "kiXFIESZKv", "replyto": "kiXFIESZKv", "signatures": ["ICLR.cc/2026/Conference/Submission7814/Reviewer_xC6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7814/Reviewer_xC6w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989321550, "cdate": 1761989321550, "tmdate": 1762919854727, "mdate": 1762919854727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets a failure mode in RLVR training (e.g., GRPO) where prompts whose rollouts are uniformly correct or uniformly incorrect yield zero reward variance, causing normalized advantages to collapse and producing no gradient signal. The authors propose RL-ZVP, a simple modification that preserves the standard GRPO update when variance is nonzero and substitutes an entropy-guided, sign-corrected token-level advantage when variance is zero. Intuitively, positive zero-variance prompts should reinforce the behavior, while negative ones should be discouraged; using token entropy modulates the update to emphasize informative positions. Experiments on multiple math benchmarks with Qwen backbones report consistent improvements over GRPO under comparable training setups and show smoother training dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Well-motivated objective:** The paper clearly identifies a practical inefficiency in GRPO, zero-variance prompts producing zero gradients, then directly targets it with a minimal change to the loss. Conceptually, reclaiming learning signal from these otherwise “wasted” batches is appealing because it improves sample/compute efficiency without redesigning the overall RLVR pipeline. The proposed branch only activates when the variance is zero and leaves GRPO behavior untouched elsewhere, so the method respects the existing training regime rather than replacing it.\n- **Solid empirical results:** Across several math datasets and multiple model sizes, the method reports consistent gains over GRPO under comparable setups. The improvements appear meaningful rather than marginal, and the training dynamics presented suggest more stable progress rather than spiky or brittle behavior."}, "weaknesses": {"value": "- **Checkpoint selection leans to optimistic:** The evaluation mixes “best checkpoint during training” with other reporting choices, which can yield optimistic numbers and makes comparisons harder to interpret. It would be beneficial to follow the ML practice where checkpoints are chosen based on a held-out validation set, and results are reported from a single, consistently selected checkpoint (not a mixture).\n- **Limited training-curve comparisons:** Training curves (Fig. 4) are shown only for GRPO and the proposed method. Including additional baselines would clarify whether RL-ZVP’s dynamics are uniquely beneficial or simply reflect broader trends across methods, which helps to better assess the proposal approach."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CO6kmcO7EI", "forum": "kiXFIESZKv", "replyto": "kiXFIESZKv", "signatures": ["ICLR.cc/2026/Conference/Submission7814/Reviewer_sAk1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7814/Reviewer_sAk1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006031738, "cdate": 1762006031738, "tmdate": 1762919854134, "mdate": 1762919854134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response Regarding the Revised Manuscript"}, "comment": {"value": "We sincerely thank all reviewers for their constructive feedback and thoughtful concerns, which have greatly helped us improve our work. \n\nWe have updated the revised manuscript by **adding a new Appendix E**, which includes the figures referenced in our rebuttal. To avoid any potential confusion for readers, we have left the **main content of the paper unchanged at this stage**. All additional details will be properly integrated and placed in their appropriate sections in the final camera-ready version.\n\nOnce again, we deeply appreciate your time, effort, and valuable insights. Thank you for helping us strengthen our work."}}, "id": "QcUzYGJ1WR", "forum": "kiXFIESZKv", "replyto": "kiXFIESZKv", "signatures": ["ICLR.cc/2026/Conference/Submission7814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7814/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission7814/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763610938567, "cdate": 1763610938567, "tmdate": 1763610938567, "mdate": 1763610938567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}