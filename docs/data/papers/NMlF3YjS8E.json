{"id": "NMlF3YjS8E", "number": 12528, "cdate": 1758208425750, "mdate": 1759897503771, "content": {"title": "FZOO: Fast Zeroth-Order Optimizer for Fine‑Tuning Large Language Models towards Adam‑Scale Speed", "abstract": "Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633~GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually need tens of times more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD, for instance, demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer towards Adam-Scale Speed. On the one hand, FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step-sizes based on the standard deviation of batch losses. On the other hand, it accelerates per-batch computation through the use of Rademacher random vector (±1) perturbations, which also enables further speedups through batched evaluation. Extensive experiments on diverse models (including RoBERTa-large, the OPT family (350M-66B), Phi-2, and Llama3) across 11 varied downstream tasks validate FZOO's effectiveness. On average, FZOO outperforms MeZO by +3% in accuracy while requiring 3$\\times$fewer forward passes. Notably, for the RoBERTa-large model, FZOO achieves average improvements of +5.6% in accuracy and 18$\\times$reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO’s formal equivalence to a normalized-SGD update rule and establishing its convergence guarantees. Beyond full-parameter tuning, FZOO plugs smoothly into PEFT techniques, unlocking even larger memory savings. Taken together, our results make single-GPU, high-speed, full-parameter fine-tuning realistic today and point toward future work on memory-efficient pre-training. Code: https://anonymous.4open.science/r/FZOO-5927", "tldr": "FZOO achieves fine‑tuning speed within the same order of magnitude as Adam for LLMs while using only inference‑level GPU memory.", "keywords": ["Zeroth‑order optimization", "Large language models", "Fine‑tuning", "Adaptive step size", "Batch gradient estimation", "Memory efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7170023a518ffe6c685cfc7b8e9a1e86aa14aeec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new zeroth-order optimizer leveraging the idea from normalized-SGD. In comparison to the zeroth-order optimizer of MeZO,  the authors propose the following changes: (1) Generate the perturbation using Rademacher random vectors instead of a Gaussian distribution; (2) Average multiple one-sided difference estimates instead of using one two-sided difference estimate; (3) Normalize the gradient its estimated standard deviation; (4) Enable efficient implementation via batched forward pass.\n\nThe authors show that the proposed method can achieve better performance with a greatly improved the convergence rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written, clear, and, easy to follow. The proposed method is elegant, simple, but effective.\n\n2. FZOO shows sizable improvements over MeZO in both convergence speed and model quality.\n\n3. The authors conduct extensive experiments to support the effectiveness of the proposed method."}, "weaknesses": {"value": "1. This paper doesn't have an extensive ablation study to test the effectiveness of each proposed change. (The ablation study only tests the effect of N, the number of perturbation directions)."}, "questions": {"value": "1. All the files under https://anonymous.4open.science/r/FZOO-5927 show the error message of \"The requested file is not found.\".\n\n2. This paper doesn't include an extensive ablation study. In particular, the reviewer wonders which change contributes the most for improving the convergence speed. It seems the normalization step is the most important one. Does normalization + Gaussian perturbation work? Could the authors run further ablation studies to test the effectiveness of each proposed change?\n\n3. It seems that averaging multiple estimates naturally enables using one-sided difference estimates. Have the authors tested the difference between averaging over N one-sided estimates v.s. averaging over N/2 two-sided estimates?\n\n4. Could the authors elaborate a bit more on why batching recovers little speed-up for MeZO?\n\n5. It would be useful to also compare against Addax (https://openreview.net/forum?id=QhxjQOMdDF)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HMoOSYq0Y0", "forum": "NMlF3YjS8E", "replyto": "NMlF3YjS8E", "signatures": ["ICLR.cc/2026/Conference/Submission12528/Reviewer_jSd5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12528/Reviewer_jSd5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711502438, "cdate": 1761711502438, "tmdate": 1762923393575, "mdate": 1762923393575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the memory and convergence tradeoff in zeroth-order optimization. The paper proposes FZOO (Fast Zeroth-Order Optimizer), a novel optimization method designed to fine-tune large language models with inference-level memory usage while achieving Adam-like convergence speed. The method combines batched Rademacher random perturbation with adaptive learning rate to mimic normalized-SGD (and Adam) behavior. The paper shows that the proposed algorithm converges in $O(\\sqrt{d}/\\sqrt{T})$ by properly choosing learning rate and perturbation stepsize.\n\nNumerical results shows that the proposed method outperforms existing MeZO method in both final accuracy and convergence speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposed a novel approach to compute the activations for the perturbed model with Rademacher perturbation. The proposed method speeds up the loss computation.\n2. The paper proposed using the batched loss variance to normalize the stepsize, which enables accelerated convergence speed.\n3. The paper demonstrated promising numerical results showing FZOO outperforms MeZO and HiZOO in multiple settings."}, "weaknesses": {"value": "1. Missing discussion on the effect of normalization with estimated variance. The author should further compare the convergence results with and without normalization. The current discussion in section 3.4 is insufficient to demonstrate the usefulness of the normalization. As we know, normalized SGD actually fails to converge to a stationary solution in specific problems.\n\n2. Missing reference to existing ZO algorithms that use structured/directional perturbation. E.g., [R1-R4]. The proof technique is standard for ZOO with either Normal or Rademacher random perturbation.\n\n3. The paper should compare the proposed FZoo method with other structural perturbation methods or variance-reduced methods.\n\n[R1] Belouze, G. Optimization without backpropagation, 2022. URLhttps://arxiv.org/abs/2209.06302.\n\n[R2] Rando M, Molinari C, Villa S, Rosasco L. Stochastic zeroth order descent with structured directions. Computational Optimization and Applications. 2024 Dec;89(3):691-727.\n\n[R3] Ma S, Huang H. Revisiting zeroth-order optimization: Minimum-variance two-point estimators and directionally aligned perturbations. In The Thirteenth International Conference on Learning Representations 2025.\n\n[R4] Shao, W., Albayrak, S. (2023). Adaptive Zeroth-Order Optimisation of Nonconvex Composite Objectives. In: Nicosia, G., et al. Machine Learning, Optimization, and Data Science. LOD 2022"}, "questions": {"value": "Please address the above weakness, especially the comparison with other structural perturbation methods.\n\nAlso, in the equations on page 20, I think some expectations are missing.\n\nThe final steps on page 21 (from line 1904 to lines 1104) do not look correct to me. I think a V term is missing in the first term, and thus, the main theorem 3.6 is incorrect. \n\nBy having the term $\\sigma_*/\\sqrt{(\\bar{\\sigma}_t^{-2})^{1/2}} \\geq 1$, it seems like the normalization is slowing down the convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wcUI4jxAC3", "forum": "NMlF3YjS8E", "replyto": "NMlF3YjS8E", "signatures": ["ICLR.cc/2026/Conference/Submission12528/Reviewer_ovaX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12528/Reviewer_ovaX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879878424, "cdate": 1761879878424, "tmdate": 1762923392894, "mdate": 1762923392894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FZOO, a zeroth‑order optimizer for LLM fine‑tuning that (i) uses batched one‑sided function evaluations and an adaptive step size normalized by the batch loss standard deviation, linking its update to normalized‑SGD, (ii) accelerates each step via Rademacher (±1) perturbations that enable efficient batched parallelism. Experiments show FZOO typically beats MeZO by ~+3% accuracy while using ~3× fewer forward passes across various datasets and tasks. On RoBERTa‑large reports Adam‑like convergence at inference‑level memory, with accompanying proofs of normalized‑SGD equivalence and convergence guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. FZOO brings normalized‑SGD’s normalization into the ZO regime by scaling updates with the batch loss standard deviation and using one‑sided estimates, making ZO steps both more stable and step‑efficient.\n\n2. The paper proves a formal link to normalized‑SGD and a convergence guarantee under standard smoothness/variance assumptions\n\n3. Rademacher (±1) perturbations enable per‑layer sign‑flip/add operations and batched parallelism, yielding speed‑up.\n\n4. Strong empirical gains across scales and tasks, and Inference‑level memory footprint"}, "weaknesses": {"value": "1. 3.3 argues Rademacher allows “bit‑level” sign flips so additions replace multiplies, there is no roofline or kernel profile to substantiate “addition beats multiply” benefits at scale.\n\n2. The paper compares to prefix‑tuning, but omits head‑to‑head QLoRA/LoRA+Adam under matched memory/throughput budgets, which are widely used and could potentially alter the claimed end‑to‑end efficiency picture."}, "questions": {"value": "1. Theorem 3.6 requires d framed as “parameter dimension”, what is the correct setting?\n\n2. The advertised 1.92× speedup (OPT‑125M, N=8) compares against an 8‑perturbation sequential baseline, realistic strong baselines like MeZO with N=1 or two‑sided ZO with efficient batching. what causes this inflation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nbSQIa3daJ", "forum": "NMlF3YjS8E", "replyto": "NMlF3YjS8E", "signatures": ["ICLR.cc/2026/Conference/Submission12528/Reviewer_pkyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12528/Reviewer_pkyA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977329479, "cdate": 1761977329479, "tmdate": 1762923392502, "mdate": 1762923392502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FZOO, a fast zeroth-order optimizer designed to reduce GPU memory in LLM fine-tuning. FZOO employs two primary strategies: it uses batched one-sided estimates to adapt its step-sizes based on loss variance and it utilizes Rademacher random-vector perturbations to accelerate per-batch computation. The authors demonstrate FZOO's effectiveness with experiments and provide theoretical convergence guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. FZOO introduces a novel implementation that estimates more accurate zeroth-order gradients using batched one-sided estimates and Rademacher random-vector perturbations. This approach eliminates the two forward passes required by MeZO.\n2. In experiments, FZOO demonstrates both improved speed-ups and higher model quality.\n3. Similar to the original MeZO, FZOO is orthogonal to PEFT techniques."}, "weaknesses": {"value": "1. I am unclear on the convergence results in Theorem 3.6 due to an apparent inconsistency in the definition of $d$ . The main text defines $d$ as the total number of model parameters; however, Appendix H. 1 clarifies that $d$ represents the per-layer input width, not the total parameter count. It is surprising that the zeroth-order method's convergence rate would be independent of the total number of model parameters.\n2. The paper do not discuss and compare with an important, closely related line of work: hybrid first-order (FO) and zeroth-order (ZO) methods (e.g., [1,2]). A notable example, Addax [1], which adaptively combines FO and ZO gradient estimations based on input sequence length, is particularly relevant and warrants comparison.\n\n[1] Li, Zeman, et al. \"Addax: Utilizing zeroth-order gradients to improve memory efficiency and performance of sgd for fine-tuning language models.\" arXiv preprint arXiv:2410.06441 (2024).\n\n[2] Chen, Jiahe, and Ziye Ma. \"VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction.\" arXiv preprint arXiv:2505.13954 (2025)."}, "questions": {"value": "1. Could the authors provide results for Adam fine-tuning in their experiments to serve as a baseline comparison?\n2. Would it be possible for the authors to include an experimental comparison with Addax [1]? Given that Addax is a prominent hybrid FO-ZO method, this comparison would provide valuable context for the proposed approach.\n3. Could the authors provide a more detailed explanation of the term $d$ in their theoretical results? Specifically, clarification is needed on why  $d$ in Theorem 3.6 denotes the per-layer input width rather than the total number of model parameters, as the latter is a more common definition in this context.\n\nI would be willing to reconsider my score if these concerns are satisfactorily addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Td7mr0Y8U", "forum": "NMlF3YjS8E", "replyto": "NMlF3YjS8E", "signatures": ["ICLR.cc/2026/Conference/Submission12528/Reviewer_1YPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12528/Reviewer_1YPH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155193559, "cdate": 1762155193559, "tmdate": 1762923392174, "mdate": 1762923392174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}