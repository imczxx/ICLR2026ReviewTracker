{"id": "tephmbQcQG", "number": 22717, "cdate": 1758334719178, "mdate": 1759896850967, "content": {"title": "Finetuning-free Alignment of Diffusion Model for Text-to-Image Generation", "abstract": "Diffusion models have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained diffusion models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel fine-tuning-free alignment framework by leveraging the underlying nature of the alignment problem---sampling from reward-weighted distributions. Moreover, we give an in-depth discussion of adopting current guidance methods for text-to-image alignment. We identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. To address this, we propose a regularization strategy that stabilizes the guidance signal. We evaluate our approach on a text-to-image benchmark and demonstrate comparable performance to state-of-the-art models with one-step generation while achieving at least a 60% reduction in computational cost.", "tldr": "", "keywords": ["Diffusion model", "Text-to-image generation", "Alignment"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10cf7db9fe7a36e9ab5792a8cf4c612eda014f26.pdf", "supplementary_material": "/attachment/89ad727b1260fe63f8a56c82c44250a073fd3b6c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an approach to aligning text-to-image diffusion models with human preferences. Instead of relying on computationally expensive fine-tuning of the base model (such as DPO), the authors propose a lightweight, plug-and-play guidance mechanism. The key contributions include a diagnosis of why naive guidance methods often fail, attributed to the adversarial nature of the guidance signal, and a solution that trains a small, regularized guidance network to provide a stable, artifact-free signal for diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The finding that the adversarial nature of the guidance can lead to undesirable artifacts in the generated images is interesting.\n- The ablation studies effectively demonstrate the effectiveness of each proposed component."}, "weaknesses": {"value": "Despite the theoretical discussion in this work, there still lacks solid experiments to validate the proposed approach.\n\n- The method's effectiveness has not been validated across different diffusion model architectures, leaving its generalizability to other frameworks unclear.\n\n- The method's performance has not been demonstrated on other datasets, which limits claims of general applicability.\n\n- The paper lacks a sensitivity analysis for its newly introduced hyperparameters, making the method's robustness to parameter variations unclear."}, "questions": {"value": "Besides, there are formatting issues in Lines 72–74: the manuscript appears to contain white text, e.g., “ted in one or very few steps, the two samples would only exhibit small differences in details. SPM allows us to capture such detail differences and guide the diffusion model.”\n\nThis raises concerns about potential prompt injection targeting AI-assisted reviewers or, alternatively, author oversight in document preparation."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Please see #Questions about the white text in the manuscript."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TuLCzh27fN", "forum": "tephmbQcQG", "replyto": "tephmbQcQG", "signatures": ["ICLR.cc/2026/Conference/Submission22717/Reviewer_z6Mz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22717/Reviewer_z6Mz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829225116, "cdate": 1761829225116, "tmdate": 1762942354601, "mdate": 1762942354601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a finetuning-free alignment framework for text-to-image diffusion models that avoids the computational cost and limited generalization of existing RLHF- or DPO-based fine-tuning approaches. Instead of modifying model weights, the authors reinterpret preference alignment as sampling from a reward-weighted distribution, showing that the aligned score function can be decomposed into the original diffusion model score and an additional guidance term derived from a learned reward model. Experiments on text-to-image benchmarks demonstrate that the method achieves comparable or superior alignment quality to state-of-the-art fine-tuning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper tackles a principled decomposition of the aligned score function into the diffusion model score, combined with the reward-based guidance term. The proposed method provides a conceptually elegant connection between preference learning and inference-time guidance, clarifying the relationship between RLHF/DPO-style methods and diffusion sampling.\n\nS2. The proposed method modifies neither the diffusion model parameters nor the text encoder, making it model-agnostic and straightforward to integrate with existing text-to-image pipelines. \n\nS3. The paper’s method achieves strong alignment performance while avoiding the heavy training overhead required by RLHF or DPO approaches. Combined with Stable Diffusion XL-Turbo, the method supports one-step T2I generation, making the overall pipeline to be suitable for practical usage and user-interactive generation scenarios.\n\n\nS4. The method consistently improves PickScore, HPS-v2, ImageReward, and Aesthetic score. Qualitative examples also show visually appealing improvements compared to the baselines."}, "weaknesses": {"value": "W1. The proposed method assumes that the forward diffusion process remains unchanged when aligning the model to human preferences, meaning the aligned distribution $q(x_t | x_0)$ is assumed to match the original pretrained model’s noising process $p(x_t | x_0)$. This assumption effectively preserves the base diffusion model’s denoising trajectory, which determines the global layout, and object composition of the generated image. As a result, while the proposed method is well-suited for adjusting aesthetic properties or making small semantic refinements, it may struggle to generate plausible image output conditioned on prompts requiring strong semantic correction, multi-object reasoning, or compositional control (e.g., enforcing spatial relations or specific attribute assignments). I was wondering if the paper’s method can also handle such generation tasks.\n\nW2. The guidance network outputs cannot be differentiated to denote where or which components of the image fail to match the textual specification. Consequently, the approach may struggle on prompts that involve explanations on multiple objects or spatial relations (*e.g.*, “to the left of,” “behind”).\n\nW3. The authors use Stable Diffusion XL-Turbo for experiments. However, recent works use the Transformer-based diffusion model, beyond U-Net based Stable Diffusion XL. Is it possible to apply the proposed method into the DiT-based model, such as Stable Diffusion v3 or even FLUX?\n\nW4. Because the diffusion backbone remains frozen, generated outputs closely reflect the inductive biases of the reward function. Is there any additional methods or strategy to alleviate the inductive biases of the given reward function, such as PickScore or Aesthetic Score?"}, "questions": {"value": "Please check the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fz9TdodAcI", "forum": "tephmbQcQG", "replyto": "tephmbQcQG", "signatures": ["ICLR.cc/2026/Conference/Submission22717/Reviewer_9tVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22717/Reviewer_9tVa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965087240, "cdate": 1761965087240, "tmdate": 1762942353765, "mdate": 1762942353765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a finetuning-free method that improves the alignment of text-to-image diffusion models. It frames the alignment as a sampling problem from a reward-weighted distribution. Specifically, this paper decomposes the scoring function with a guidance term and proposes a regularization technique to train the model. Experimental results on Pick-a-Pic dataset show the improvement of the proposed method over baseline studies.."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a finetuning-free method that is efficient compared to finetune-based methods. The proposed regularization strategy stabilizes the guidance signal and improves the  text-to-image diffusion models."}, "weaknesses": {"value": "1. I have concerns regarding the evaluation of the proposed method.  According to line 418, the evaluation is conducted *using 500 validation prompts from the validation unique split of Pick-a-Pic.*  How are these prompts selected? Moreover, the baseline method SPO is evaluated on 4K prompts from Pick-a-Pic, which is eight times more than this method. \n\n2. This paper evaluates its method based on SDXL-Turbo, which was released in 2023. Considering the rapid emergence of new models,  SDXL-Turbo is kind of 'old' and cannot well support the effectiveness and generalization of the proposed method. How does the proposed method perform when generalized to recent models?\n\n3. Figure 1 shows some visualization results, while the prompts are provided in the appendix. It is kind of difficult for me to find the improvement of the proposed method over baselines. It seems the baseline method already gets good enough results.\n\n4. Is it expected to include the related work in Section 1.1 instead of Section 2? \n\n5. The citations of the paper could be improved, such as line 107\n>  In (Liang et al., 2024), Liang et al. propose...."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fGAdBjDPqF", "forum": "tephmbQcQG", "replyto": "tephmbQcQG", "signatures": ["ICLR.cc/2026/Conference/Submission22717/Reviewer_r1pw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22717/Reviewer_r1pw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985434056, "cdate": 1761985434056, "tmdate": 1762942353569, "mdate": 1762942353569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a finetuning-free, plug-and-play alignment strategy for diffusion models in text-to-image generation by casting the problem as sampling from a reward-weighted distribution. The authors analyze the challenges of existing guidance-based alignment schemes—particularly the emergence of adversarial artifacts—and propose a novel regularization for guidance signal stabilization. The method is evaluated on established text-to-image benchmarks, achieves strong alignment to human preferences using a lightweight guidance network, and demonstrates substantial computational savings over finetuning-based approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Formulation Innovation:** The paper reframes text-to-image alignment as direct sampling from a reward-weighted distribution, moving away from common parameter fine-tuning approaches and offering a generic plug-and-play control mechanism.\n- **Practical estimator for the guidance:** The paper adopts a simple regression trick (Eq. 13) to approximate the conditional expectation and then converts it to a guidance gradient (Eq. 14), avoiding expensive backprop through the sampler.\n- **Stabilization for adversarial guidance:** The instability of naïve guidance with increasing strength is documented and addressed via a consistency regularizer merged in Eq. 16.\n- **Lightweight and fast:** The guidance net is only ~72 MB and reuses the reference model's VAE/tokenizer/text encoder. Combined with SDXL-Turbo, this enables effective **one-step** generation.\n- **Agnostic to Reward:** The method supports both differentiable and non-differentiable reward settings, with Table 4 in the appendix demonstrating applicability on GenEval with binary rewards."}, "weaknesses": {"value": "1. **Analysis Depth of Regularization:** While the regularization is empirically justified and its effect visualized (see Figure 2), the theoretical underpinnings and limits of this regularization are not fully elucidated. What modes of artifact are suppressed, and does the regularization always guarantee avoidance of adversarial guidance? The practical selection of the regularization hyperparameter $\\eta$ (Eq. 13 & 15) also remains ad hoc.\n2. **Reward Dependence & Generality:** Although the proposed scheme is reward-agnostic in form, its empirical evaluation—especially in Table 2—is predominantly based on PickScore and similar human-preference proxies. It is unclear how robust the approach is to poorly calibrated, biased, or low-signal rewards. There is only a narrow demonstration on non-differentiable rewards in Table 4 (GenEval), which is limited in scope and size.\n3. **Scope of generalization is narrow:** Experiments are concentrated on SDXL-Turbo; the paper asserts model-agnosticism and one-step benefits, but offers limited cross-backbone verification or stress tests on distribution shift.\n4. **Hyperparameter Sensitivity:** The proposed method claims to \"fix\" the problem of carefully tuning the guidance strength, but practical recipes or robustness studies for the guidance parameter, regularization weight, or hyperparameter $\\beta$ are lacking.\n5. **Comparisons to very recent other alignment methods are light:** Table 2 includes Tweedie/Backprop and two finetuning methods (Diffusion-DPO, SPO), but a broader slate of strong alignment related methods (and best-practice configs) would better establish relative advantage. \n6. **Not Strictly Finetuning-free:** Please refer to the precise definition of finetuning-free. The scenario described in this paper can at best be considered \"no base-model fine-tuning\"."}, "questions": {"value": "1. **Regularization Mechanics:** Can the authors provide more intuition on how the proposed regularization term shapes the guidance network’s landscape? Are there scenarios or reward functions where this regularization might fail or even worsen adversarial behaviors?\n2. **Sensitivity Analysis:** How does performance vary with η and β? Please provide curves (PickScore/HPSV2/ImageReward/Aesthetic vs. η, β) and report variance across seeds.\n3. **Extension to Other Backbones:** Have you tested non-Turbo SDXL or SD 2.1 latent backbones, or text-conditional DiT variants(like Flux)? Are there empirical results or qualitative observations on data distribution shifts not covered by the current benchmarks?\n4. **Robustness to Reward Misspecification:** Beyond GenEval's binary reward, how does the method fare under noisy, sparse, or biased rewards? Can the guidance network overfit to reward artifacts, and how would the regularizer respond?\n5. **Comparisons with Other Alignment Related Work:** Where are the practical/theoretical boundaries vs. other plug-and-play or inference-time guidance alignment related methods? This will help substantiate the method's effectiveness and superiority."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ft6HOBoxxE", "forum": "tephmbQcQG", "replyto": "tephmbQcQG", "signatures": ["ICLR.cc/2026/Conference/Submission22717/Reviewer_ZRAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22717/Reviewer_ZRAs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101753274, "cdate": 1762101753274, "tmdate": 1762942353143, "mdate": 1762942353143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}