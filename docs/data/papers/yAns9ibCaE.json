{"id": "yAns9ibCaE", "number": 4196, "cdate": 1757630508196, "mdate": 1759898048045, "content": {"title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions", "abstract": "Editing images with instructions to reflect non-rigid motions—camera viewpoint shifts, object deformations, human articulations, and complex interactions—poses a challenging yet underexplored problem in computer vision. Existing approaches and datasets predominantly focus on static scenes or rigid transformations, limiting their capacity to handle expressive edits involving dynamic motion. To address this gap, we introduce ByteMorph, a comprehensive framework for instruction-based image editing with an emphasis on non-rigid motions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong baseline model built upon the Diffusion Transformer (DiT), named ByteMorpher. ByteMorph-6M includes over 6 million high-resolution image editing pairs for training, along with a carefully curated evaluation benchmark ByteMorph-Bench. Both capture a wide variety of non-rigid motion types across diverse environments, human figures, and object categories. The dataset is constructed using motion-guided data generation, layered compositing techniques, and automated captioning to ensure diversity, realism, and semantic coherence. We further conduct a comprehensive evaluation of recent instruction-based image editing methods from both academic and commercial domains.", "tldr": "We present a large scale dataset and benchmark for non-rigid motions including camera motion, human motion and object motions, along with a baseline method.", "keywords": ["Image Editing; Diffusion Model; Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36780ca275c285e224b6b17bb7ec7f91591dbc1a.pdf", "supplementary_material": "/attachment/8532006b1c48c87922e8fcc066375c4e52c2d37a.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the underexplored challenge of non-rigid motion editing in instruction-guided image manipulation by introducing ByteMorph, a comprehensive framework. Its core contributions include: a large-scale training dataset (ByteMorph-6M) with over 6 million high-quality editing pairs, a carefully curated evaluation benchmark (ByteMorph-Bench), and a strong DiT-based baseline model (ByteMorpher). Using motion-guided generation and automated captioning pipelines, the dataset captures diverse dynamic scenarios including camera motion, object deformation and human-object interactions. Experiments demonstrate that ByteMorpher finetuned on this dataset significantly outperforms existing open-source methods, while also revealing the limitations of current industrial foundation models in handling such complex motion-aware edits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles the challenging problem of spatial manipulation in image editing, and the model trained on the proposed dataset demonstrates convincing performance in handling such tasks, making the research direction highly meaningful.\n- The authors have constructed an exceptionally large-scale dataset of 6M samples and developed a comprehensive and well-designed benchmark, demonstrating substantial research effort.\n- The paper is well-written and clearly structured, with professionally designed figures and illustrations."}, "weaknesses": {"value": "- While the use of video generation models for dataset construction is understandable, the reliance on Seaweed's generative capabilities raises concerns about whether the editing performance can generalize effectively to other editing tasks beyond the ByteMorph-Bench evaluation.\n- Beyond fine-tuning the FLUX model to demonstrate the dataset's effectiveness, for such a challenging task, it would be valuable to additionally validate the dataset's utility by fine-tuning unified models such as Bagel, UniWorld, or Qwen-Image-Edit. Furthermore, it remains unclear how effectively the 6M dataset would support more complex instruction-based editing scenarios. The authors are encouraged to provide preliminary experimental results addressing these aspects.\n- Regarding related work, the authors may consider citing \"Unireal: Universal image generation and editing via learning real-world dynamics\" as additional relevant literature."}, "questions": {"value": "My current concerns primarily stem from the weaknesses outlined above. Additionally, I have reviewed 3-4 contemporaneous papers following a similar paradigm - focusing on complex image editing and utilizing video models for data construction. While the existence of parallel work alone does not justify rejection, the weaknesses identified prevent me from assigning a higher score at this stage. I will reconsider my rating based on the authors' rebuttal and other reviewers' comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K9NV9DbT8M", "forum": "yAns9ibCaE", "replyto": "yAns9ibCaE", "signatures": ["ICLR.cc/2026/Conference/Submission4196/Reviewer_8HRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4196/Reviewer_8HRk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645139905, "cdate": 1761645139905, "tmdate": 1762917223115, "mdate": 1762917223115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ByteMorph, a motion-centric framework for instruction-guided image editing with non-rigid motions. It contributes:\n- ByteMorph-6M: a large training set (~6.4M source–target pairs) synthesized from video generations and re-labeled with motion-focused instructions.\n- ByteMorph-Bench: a curated evaluation set (613 hard cases) covering five motion categories—camera zoom, camera move, object motion, human motion, and human–object interaction.\n- ByteMorpher: a DiT-based baseline initialized from FLUX.1-dev; training concatenates noisy source/target latents along the sequence dimension with shared positional encodings.\n\nFor evaluation, the paper proposes $ \\mathrm{CLIP\\text{-}D}_{\\text{img}} $, which compares *difference vectors* in CLIP space to better capture edit quality under camera changes.\n\nExperiments benchmark against open-source and commercial systems with both VLM-based and human evaluations, showing that ByteMorpher achieves competitive or superior performance, especially on motion-driven edits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Elevates motion (non-rigid, articulation, camera pose) as a first-class editing dimension; introduces $ \\mathrm{CLIP\\text{-}D}_{\\text{img}} $ to evaluate edits as *changes* rather than absolute content similarity.\n- Large-scale training set + curated hard benchmark; broad comparisons (open-source & industrial), repeated sampling, and both human and VLM judgments.\n- Clear problem framing and taxonomy; tables position prior editing datasets/methods effectively; training details (backbone, losses) are given.\n- Bridges a gap between instruction-guided appearance edits and motion-centric edits; likely to become a default benchmark for this subarea if maintained."}, "weaknesses": {"value": "- Training relies on synthesized videos; even with filtering, motion/texture statistics may diverge from real-world photos. Add more purely real frame-pair data or zero/low-shot tests on real-image edit benchmarks to quantify the gap.\n- Provide a larger rank-correlation study (Spearman/Kendall) per category (camera/human/object/interaction) and analyze systematic failure modes (e.g., composite camera + articulation edits).\n- Since training uses latent concatenation (source, target), include a concise inference-time diagram/pseudocode showing where source features and text are injected when only (source, instruction) are available.\n- Report sensitivity to API sampling settings (resolution, steps, guidance/temperature, resampling) to demonstrate robustness of conclusions.\n- More explicitly connect with camera/motion-controlled T2V (e.g., camera trajectory parameterizations) to transfer motion priors into the image-editing setting."}, "questions": {"value": "1. What are the Spearman/Kendall correlations between $ \\mathrm{CLIP\\text{-}D}_{\\text{img}} $ and human preferences across the five categories? Any systematic mismatches (e.g., simultaneous camera + expression changes)?\n2. How does the model perform on purely real editing datasets (e.g., MagicBrush/HQ-Edit) without synthetic video support?\n3. Sensitivity to the instruction templating and VLM labeling settings (temperature/prompt variants) during data construction?\n4. Interplay with InstructMove: Does joint training with real video-frame pairs improve generalization on ByteMorph-Bench and real-image edits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v6RKqyYTBD", "forum": "yAns9ibCaE", "replyto": "yAns9ibCaE", "signatures": ["ICLR.cc/2026/Conference/Submission4196/Reviewer_KJoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4196/Reviewer_KJoM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712659460, "cdate": 1761712659460, "tmdate": 1762917222917, "mdate": 1762917222917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ByteMorph, a framework for instruction-based image editing with focus on non-rigid motions. The work introduces: (1) ByteMorph-6M - a large-scale dataset of 6.4 million image pairs covering with five motion categories, using an automated pipeline combining video generation and VLM-based annotation; (2) ByteMorph-Bench - a curated evaluation benchmark; and (3) ByteMorpher - a DiT-based model fine-tuned on FLUX.1-dev. The method demonstrates superior performance in dynamic editing tasks compared to existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset is well-motivated. ByteMorph-6M effectively addresses the gap in motion-focused editing data with comprehensive coverage of non-rigid transformations. The release of datasets, benchmarks, and code provides significant value to the community.\n2. The automated construction using video generation and VLMs ensures scalability while maintaining quality and semantic coherence."}, "weaknesses": {"value": "1. The technical contribution is limited, as the methodology relies purely on fine-tuning a pre-existing DiT model (FLUX.1-dev) on ByteMorph-6M without introducing architectural innovations or tailored optimizations.\n2. The experimental analysis does not sufficiently address the trade-offs of fine-tuning on ByteMorph-6M. While it is intuitive that specialization improves motion-specific performance, the paper omits evaluation of the model's original capabilities on standard instruction-based benchmarks. This raises concerns about potential performance degradation in general editing tasks, such as stylization or attribute modification, which could limit the model's applicability in broader contexts. A comparative analysis on conventional benchmarks would offer a more balanced perspective on robustness and generalization.\n3. Experiments are primarily conducted on curated synthetic data (ByteMorph-Bench), with insufficient validation on real images contains real-world complexities such as occlusions and lighting variations."}, "questions": {"value": "1. Does fine-tuning on ByteMorph-6M compromise performance on standard instruction-based benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "79LgqTK4tl", "forum": "yAns9ibCaE", "replyto": "yAns9ibCaE", "signatures": ["ICLR.cc/2026/Conference/Submission4196/Reviewer_2iZC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4196/Reviewer_2iZC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894855596, "cdate": 1761894855596, "tmdate": 1762917222737, "mdate": 1762917222737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ByteMorph, a comprehensive framework for instruction-guided image editing with a specific and novel focus on non-rigid motions. The authors identify a significant gap in existing research, where current datasets and models primarily excel at static, appearance-centric edits but fail to handle dynamic transformations such as camera viewpoint shifts, object deformations, and human articulations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a well-defined and high-impact limitation of current image editing models. Non-rigid motion is a fundamental aspect of the visual world, and enabling instruction-based control over it is very important.\n\n2.  Experimental results are comprehensive, which make this paper very solid."}, "weaknesses": {"value": "My main concern is about the dataset construction. The entire training set (ByteMorph-6M) relies entirely on a synthetic pipeline (ChatGPT-4o and Seaweed).  I mean, 6.4M data comes from a single video generation model. This directly results in the dataset's quality being limited by the capabilities of this specific video model. It seems like that training on ByteMorph-6M is actually distilling Seawead. Specifically, if the Seawead model has systematic limitations in generating certain types of motion, the ByteMorpher model will inevitably learn these same flaws."}, "questions": {"value": "1. About Table 4 of the Ablation Study. The authors claim notable gains after fine-tuning on the ByteMorph-6M, but this point is not reflected in the Table.  Could the authors give more explanation?\n\n2.  During the dataset construction, how to handle the generation failures? I mean, what if the video generation model (Seaweed) fails to follow the motion caption $C_m$? Is there any filter strategy?\n\n3. Seed Data Diversity. The pipeline in Figure 2 begins with a frame from real video, which acts like a seed for the dataset generation. Therefore, the diversity of the entire 6.4M dataset (like the number of different objects) is highly dependent on the diversity of this initial seed set. Could the authors provide more details on these initial real-world frames (like the number, variety)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QjcH0wAWX0", "forum": "yAns9ibCaE", "replyto": "yAns9ibCaE", "signatures": ["ICLR.cc/2026/Conference/Submission4196/Reviewer_S7Wu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4196/Reviewer_S7Wu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924637220, "cdate": 1761924637220, "tmdate": 1762917222421, "mdate": 1762917222421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}