{"id": "2OO399hRD6", "number": 15324, "cdate": 1758250253162, "mdate": 1759897313556, "content": {"title": "Reshaping Reasoning in LLMs: A Theoretical Analysis of RL Training Dynamics through Pattern Selection", "abstract": "While reinforcement learning (RL) demonstrated remarkable success in enhancing the reasoning capabilities of language models, the training dynamics of RL in LLMs remain unclear. In this work, we provide an explanation of the RL training process through empirical analysis and rigorous theoretical modeling. First, through systematic reasoning-pattern-level and token-level analysis across the RL training process, we show that while different reasoning patterns exhibit relatively stable success rates during training, RL primarily optimizes a sparse subset of critical tokens, thereby reshaping reasoning pattern distributions to affect model performance. Building on these empirical insights, we develop a theoretical framework to understand the training dynamics of RL with two typical rewards: verifiable reward (RLVR) and model's internal feedback (RLIF). For RLVR, we analyze the training dynamics under two special cases: one where models readily converge to optimal reasoning strategies, and another where optimization becomes challenging, revealing that the base model's reasoning quality is crucial for determining convergence behavior. For RLIF, we examine how internal rewards initially improve model performance but can potentially lead to degradation with continued training. Extensive experiments validate our findings, advancing both theoretical understanding and practical applications of RL in language model enhancement.", "tldr": "", "keywords": ["Reinforcement Learning", "Language Models", "Reasoning Patterns", "Training Dynamics"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2740ac59141d18b22f685ecfe16a6e2e9eb233e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work first conducts analysis on reasoning-pattern and token levels. They claim that there are patterns obtaining stable accuracy, and distribution changes are only limited to a few tokens. Then, basing on the findings with the patterns, they theoretically analyze the training dynamics on RLVR and RLIF. In the experiment section, they conduct simulation on the RLVR and RLIF basing on their assumptions. The empirical results are consistent with theoretical claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, In section 4.2, this work identifies patterns in LLM mathematical reasoning. This could provide inspire further explorations.\n\n2, In section 5, basing on the assumptions, theoretical results are derived for RL training dynamics. Furthermore, these claims are supported by simulations in the experiment section."}, "weaknesses": {"value": "1, My biggest concern is with the assumptions. Specifically, \n- Assumption 5.1 assumes that success rate for each pattern remains the same. But this assumption is only demonstrated on one dataset; and there is not discussion on why this assumption is reasonable;\n- Assumption 5.5 assumes that correct answer $a*$ has the highest probability across all possible answers, for all answers. I do not think this claim is well supported by Wang et al. (2022). \n- In equation (5.3), the LLM is simplified as a one-step policy, where the distribution of $y_l$ is only determined by its previous token $y_{l-1}$. Can you explain whether this is an reasonable simplification?\n\n2, Claims about the token distribution is not convincing; and is not connected to other parts of the manuscript. First, Table 1 is hard to interpret. Can you elaborate on how to understand this table? Second, in line 220, it claims that less than 10% of tokens experienced shift. But considering the vocabulary size and token distribution (a few tokens occupies significant probability mass), 10% rank shift could be significant. Can you also report the probability mass shift?\n\n3, The conclusion in line 203 actually depends on what patterns are. Could you explain what these patterns are? Will the pattern for a prompt persist or change?"}, "questions": {"value": "1, Besides the simulation, could you verify your findings on a real dataset (e.g., MATH)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2fh564ejPT", "forum": "2OO399hRD6", "replyto": "2OO399hRD6", "signatures": ["ICLR.cc/2026/Conference/Submission15324/Reviewer_BMWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15324/Reviewer_BMWN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800350018, "cdate": 1761800350018, "tmdate": 1762925618923, "mdate": 1762925618923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes how RL post-training changes LLM reasoning, showing empirically that RLVR promotes higher-success reasoning patterns while RLIF remains unstable. A two-stage q → r → a model and tabular gradient-flow analysis explain these behaviors, and controlled experiments support the overall conclusions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper effectively combines empirical findings with a clean theoretical analysis, offering useful insights for understanding LLM reasoning behavior."}, "weaknesses": {"value": "The experimental scope is narrow: all evaluations are conducted on math reasoning tasks and within the Qwen2.5 model family. Prior work has shown RLIF to produce gains on non-math general domains, so broader experiments across model families and tasks are necessary to substantiate the generality of the conclusions."}, "questions": {"value": "1. How is pattern-level accuracy evaluated? A single response may contain multiple reasoning patterns—how do you attribute correctness to individual patterns in such mixed instances?\n2. How should the proposed training-dynamics framework inform practical RLVR training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JrPZNIXC1y", "forum": "2OO399hRD6", "replyto": "2OO399hRD6", "signatures": ["ICLR.cc/2026/Conference/Submission15324/Reviewer_MU4Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15324/Reviewer_MU4Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988705403, "cdate": 1761988705403, "tmdate": 1762925618305, "mdate": 1762925618305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides new insights into the reinforcement learning (RL) training dynamics of large language models (LLMs) through a combination of empirical analysis and theoretical modeling. The authors focus on two reward types: verifiable rewards (RLVR) and internal feedback (RLIF). They find that RL primarily optimizes a sparse subset of “critical” tokens, which reshapes the distribution of reasoning patterns. For RLVR the authors show two scenarios, one were optimization converges efficiently and another where weaker base models experience slow convergence. For RLIF the authors show that it is initally improving performance but can degrade performance with continued training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides an explanation for the stability of RLVR and the instability of RLIF\n- Relevant topic: Understanding the behavior of RL training in the context of LLMs\n- Multiple datasets and model scales are used which strengthens the generality of the findings.\n- Introduces an interesting and interpretable analysis framework at both the token and reasoning-pattern levels"}, "weaknesses": {"value": "- The paper would benefit from a more comprehensive description of the experimental setup for the different training/evaluation runs. Important implementation details (e.g. temperature settings, sampling strategies, and other hyper parameters) are missing making it difficult to fully reproduce or interpret the reported results.\n- It remains unclear which exact implementation was used for RLVR training. For example, if vanilla GRPO was used, information about rollout numbers, optimization parameters, and possible comparisons to more recent methods (e.g., CISPO[1] or Dr.GRPO[2]) would help contextualize the empirical findings and assess their robustness.\n\n1: MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention  \n2: Understanding R1-Zero-Like Training: A Critical Perspective"}, "questions": {"value": "- **Q1:** For generating base model responses in the token-level analysis, was the sampling temperature set to 0?\n- **Q2:** Do the authors expect qualitatively different dynamics for different RLVR variants (e.g., GRPO, PPO, or CISPO), or would the theoretical conclusions remain consistent across these variants?\n- **Q3:** Could the authors elaborate on Assumption 5.1? In practice, the success rate of individual reasoning patterns may change during training. How realistic is it to assume this remains constant, and how might deviations affect the theoretical results?\n- **Q4:** How sensitive are the observed “sparse token optimization” effects to the choice of base model or training dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4EJGTX4fJU", "forum": "2OO399hRD6", "replyto": "2OO399hRD6", "signatures": ["ICLR.cc/2026/Conference/Submission15324/Reviewer_xtTG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15324/Reviewer_xtTG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991684959, "cdate": 1761991684959, "tmdate": 1762925617930, "mdate": 1762925617930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides new insights into the reinforcement learning (RL) training dynamics of large language models (LLMs) through a combination of empirical analysis and theoretical modeling. The authors focus on two reward types: verifiable rewards (RLVR) and internal feedback (RLIF). They find that RL primarily optimizes a sparse subset of “critical” tokens, which reshapes the distribution of reasoning patterns. For RLVR the authors show two scenarios, one were optimization converges efficiently and another where weaker base models experience slow convergence. For RLIF the authors show that it is initally improving performance but can degrade performance with continued training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides an explanation for the stability of RLVR and the instability of RLIF\n- Relevant topic: Understanding the behavior of RL training in the context of LLMs\n- Multiple datasets and model scales are used which strengthens the generality of the findings.\n- Introduces an interesting and interpretable analysis framework at both the token and reasoning-pattern levels"}, "weaknesses": {"value": "- The paper would benefit from a more comprehensive description of the experimental setup for the different training/evaluation runs. Important implementation details (e.g. temperature settings, sampling strategies, and other hyper parameters) are missing making it difficult to fully reproduce or interpret the reported results.\n- It remains unclear which exact implementation was used for RLVR training. For example, if vanilla GRPO was used, information about rollout numbers, optimization parameters, and possible comparisons to more recent methods (e.g., CISPO[1] or Dr.GRPO[2]) would help contextualize the empirical findings and assess their robustness.\n\n1: MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention  \n2: Understanding R1-Zero-Like Training: A Critical Perspective"}, "questions": {"value": "- **Q1:** For generating base model responses in the token-level analysis, was the sampling temperature set to 0?\n- **Q2:** Do the authors expect qualitatively different dynamics for different RLVR variants (e.g., GRPO, PPO, or CISPO), or would the theoretical conclusions remain consistent across these variants?\n- **Q3:** Could the authors elaborate on Assumption 5.1? In practice, the success rate of individual reasoning patterns may change during training. How realistic is it to assume this remains constant, and how might deviations affect the theoretical results?\n- **Q4:** How sensitive are the observed \"sparse token optimization\" effects to the choice of base model or training dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4EJGTX4fJU", "forum": "2OO399hRD6", "replyto": "2OO399hRD6", "signatures": ["ICLR.cc/2026/Conference/Submission15324/Reviewer_xtTG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15324/Reviewer_xtTG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991684959, "cdate": 1761991684959, "tmdate": 1763069523112, "mdate": 1763069523112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}