{"id": "d97Q8r7ZKZ", "number": 268, "cdate": 1756732939327, "mdate": 1763568071979, "content": {"title": "AlphaBench: Benchmarking Large Language Models in Formulaic Alpha Factor Mining", "abstract": "Formulaic alpha factor mining (FAFM) is a central problem in quantitative investment, where interpretable formulas are designed to extract predictive signals from historical financial series. With the emergence of large language models (LLMs), recent studies have begun to explore their roles in FAFM, yet their capabilities across different tasks and configurations remain unclear. In this work, we introduce AlphaBench, the first systematic benchmark for evaluating LLMs in FAFM. AlphaBench covers three core tasks, including factor generation, factor evaluation, and factor searching, which are all popular tasks integrated in the workflow of quantitative researchers. Beyond task-level evaluation, we further analyze how different LLM settings, including model type, prompting paradigm, and reasoning strategy, influence performance. Our experiments on a range of open-source and closed-source models reveal that LLMs hold strong potential in automating factor mining, while also facing persistent challenges in robustness, search efficiency, and practical usability.", "tldr": "", "keywords": ["Alpha Mining", "LLM Benchmark", "LLM Agent", "Data Science and Engineering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c6e8ed6800ae06cee8f686eae7b9efa980e8515.pdf", "supplementary_material": "/attachment/e1b45e824ceb05a571f7633740c13e4efcfd091f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a benchmark to evaluate how large language models perform in quantitative finance tasks involving the generation, evaluation, and searching of interpretable factors. Using the CSI300 market data and Qlib framework, the benchmark tests models across metrics such as reliability, stability, accuracy, and cost. Results show that while LLMs can reliably produce syntactically valid and intuitive factor expressions, they struggle to assess factor quality without data, and reasoning strategies offer limited benefit."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and clearly organized. I commend the authors for the scope of their analyses. As a potential user of AlphaBench, I believe that the benchmark dataset will be of interest to many researchers.\n\nIt is good to see that frontier models already achieved a supreme performance in generations tasks. The models also achieved a moderate performance in search tasks."}, "weaknesses": {"value": "My concerns are primarily related to the evaluation task. Given its statistical nature, probably LLMs are not well-suited for factor evaluation. LLMs might help human quant researchers to construct creative new factors but it might be better to outsource quantitative assessment to traditional backtesting models.\n\nSection 4.3 reports very weak zero-shot evaluation performance, which is near random. The discussion (Section 5) attributes this mainly to missing supervision and execution context. Although this explanation is convincing, the paper can further test the following:\n(1) Provide a controlled ablation showing how much of this failure is due to lack of numeric context versus model scale or prompt design.\n(2) Demonstrate even a small fine-tuning (e.g., adding example factor–IC pairs) to show whether performance improves. This would make the benchmark actionable for future model development.\n\nThe authors implicitly make two foundational assumptions:\n (1) LLMs can “understand” factor expressions semantically, and\n (2) they can reason about predictive strength without data\nThese assumptions are not empirically grounded. Alpha factors embed statistical relations, not linguistic ones; their performance often depends on noise sensitivity, normalization, market characteristics, time-specific regimes, and data-specific effects that linguistic/symbolic interpretation cannot capture. Hence, the current design may be more a test of syntactic familiarity (e.g., knowing that Mean + Std are stable factors) rather than genuine financial reasoning.\nThis can only be tested if the authors can show that human experts (quant researchers) can perform the same ranking/scoring from formulas alone (without access to numerical data). This would establish an upper bound for LLM expectations.\n\nCurrently, if I did not misunderstand, ground truth labels are derived from backtests on the same CSI300 data used to generate the factors. If the LLM learned statistical patterns from similar data distributions (e.g., public quant research corpora), then its apparent understanding could partially reflect memorization, not reasoning. A stronger design would use out-of-sample or cross-market validation (e.g., use CSI300 for training labels and SP500 for testing).\n\nAnother potential concern is that the paper’s contributions may appeal to a financial AI audience rather than the broader general AI community. It is difficult to identify insights that generalize beyond the domain of quantitative finance. The authors could strengthen the paper’s broader relevance by adding a sentence or two in the conclusion to highlight how AlphaBench’s methodology can inform LLM benchmarking or reasoning research in other structured domains."}, "questions": {"value": "Please see above (weaknesses)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N8vHn89tMI", "forum": "d97Q8r7ZKZ", "replyto": "d97Q8r7ZKZ", "signatures": ["ICLR.cc/2026/Conference/Submission268/Reviewer_4Cyb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission268/Reviewer_4Cyb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917751822, "cdate": 1761917751822, "tmdate": 1762915481668, "mdate": 1762915481668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AlphaBench, the first benchmark for evaluating LLMs in Alpha Factor Mining, where it's an important task in quantitative finance focused on discovering interpretable mathematical expressions that predict asset returns. \n\nThe benchmark includes: 687 generation prompts, 1170 evaluation instructions, 27 search tasks, 10 LLMs, and different prompting strategies. AlphaBench covers comprehensive evaluation metrics for Generation, Evaluation, and Searching."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark focuses on the factor mining task, but is comprehensive. The paper provides lots of insights: a) LLMs show high reliability in generating syntactically valid factors, but accuracy drops significantly for complex instructions. b) Factor evaluation remains a major bottleneck. 3) LLM-guided search improves factor quality at a reasonable computational cost; d) CoT sometimes doesn't work on large models. That's important for the FinTech domain.\n\n2. The authors build on a real-market dataset using Qlib-compatible backtesting. They also provide detailed documentation for the curated dataset."}, "weaknesses": {"value": "1. I have some confusion about the factor evaluation task. The factor evaluation (ranking/scoring) task sometimes outputs near-random results across all models. Could the author provide more explanation on that?\n\n2. The dataset covers only daily equity factors on the CSI-300 (China) with ~1,700 factors. \n\n3. The search step is helpful, and search quality is measured via IC improvement from backtesting. But this could vary due to noise in short-term financial returns."}, "questions": {"value": "See in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P3DX3wH42C", "forum": "d97Q8r7ZKZ", "replyto": "d97Q8r7ZKZ", "signatures": ["ICLR.cc/2026/Conference/Submission268/Reviewer_QP1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission268/Reviewer_QP1w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016537815, "cdate": 1762016537815, "tmdate": 1762915481465, "mdate": 1762915481465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response to Reviewers: On Evaluation Tasks and Broader Implications"}, "comment": {"value": "We sincerely thank all reviewers for the constructive and insightful feedback. Several comments across the reviews raised similar questions regarding: (1) the feasibility of LLM-based factor evaluation, (2) the design and purpose of our atomic tasks, and (3) the broader utility of AlphaBench as a data and evaluation framework. We summarize the shared concerns and our unified clarifications below.\n\n### **1. Why design atomic evaluation tasks?**\n\nAcross multiple reviews, there was interest in whether LLMs can act as *fast evaluators*, either as partial replacements for backtesting or as inexpensive early-stage filters during factor search.\n\nTo study this question cleanly, we introduced two *minimal, well-controlled* atomic tasks:\n\n1. **Noise vs. non-noise factor classification**\n2. **A–B pairwise selection (choose the better factor)**\n\nThese tasks remove confounding variables and test the core ability of LLMs to infer factor quality *solely from structure*. They also allow us to systematically study whether additional supervision, structured representations (AST trees), or contextual information can help.\n\n### **2. What did these tasks reveal?**\n\nThe results across all reviewers’ questions consistently show:\n\n- **Absolute noise classification is extremely difficult** for current LLMs\n   → Performance is near-random even after careful prompting and structured input.\n- **Pairwise selection is learnable**\n   → Fine-tuning improves performance, and cross-market transfer is possible.\n- **AST-style structured representations do not significantly help**, suggesting that text-tree structures may not capture factor compositionality effectively.\n- **Adding market context does not improve classification**, confirming that the difficulty lies in the inherent statistical nature of factor performance, not in prompt design.\n\nThese findings support a unified conclusion:\n **LLMs can assist with structure-based generation and generation, but cannot yet replace statistical backtesting.**\n\n### **3. Why expand evaluation tasks in the revised version?**\n\nSeveral reviewers asked whether our evaluation suite is sufficient to characterize the limitations of LLMs.\n To address this, we have expanded the evaluation in the revised manuscript:\n\n- Both CSI300 and S&P500 datasets included in all atomic tasks\n- Fine-tuning and zero-shot comparisons across markets\n- Cross-market transferability analysis\n- Tests with structured operator-tree representations\n- Additional reasoning about why noise classification remains fundamentally statistical"}}, "id": "awh4pFuEIx", "forum": "d97Q8r7ZKZ", "replyto": "d97Q8r7ZKZ", "signatures": ["ICLR.cc/2026/Conference/Submission268/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission268/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission268/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763567056233, "cdate": 1763567056233, "tmdate": 1763567056233, "mdate": 1763567056233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper presents - AlphaBench which introduces first benchmark for evaluating LLMs in (FAFM) which is pretty noval and insightful\n2. Three main pillars at which model measures are- generation, evaluation, and searching tasks for financial factors and their discovery.\n3. A far I could evaluate in manuscript - GPT-5 performs best overall; \n4. Chain-of-Thought (CoT) yields minimal gains which was also expected, sometimes reduces stability.\n5. For all models, evaluation or judgement remains a big challenge\n6. Gemini models are competitive; open-source models lag behind."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A very detailed and comprehensive coverage of FAFM lifecycle.\n2. Very good benchmarking with quantitative metrics (IC etc..)\n3. The paper also presents a very strong validation on real financial datasets (CSI300, 2020–2025).\n4. Manuscript also rightly highlights trade-offs between model size, cost."}, "weaknesses": {"value": "1. LLMs struggle in factor evaluation — low accuracy in ranking/scoring that was observed on some experiments\n2. CoT prompting often hurts large-model performance.\n3. Lack of supervised data limits evaluation reliability. Some strong large scale data would be helpful\n4. Benchmark restricted to daily equity factors; excludes intraday or multi-asset tests."}, "questions": {"value": "1. How can supervised or weakly labeled data be created for training factor evaluators?\n2. Can structured representations improve evaluation interpretability and what experiments can be done more on it?\n5. What is the role of specific domain knowledge in guiding LLM outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d2FCa8oI3c", "forum": "d97Q8r7ZKZ", "replyto": "d97Q8r7ZKZ", "signatures": ["ICLR.cc/2026/Conference/Submission268/Reviewer_Mm9t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission268/Reviewer_Mm9t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034346775, "cdate": 1762034346775, "tmdate": 1762915481264, "mdate": 1762915481264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}