{"id": "YklIjgIZzn", "number": 12730, "cdate": 1758209851791, "mdate": 1759897490965, "content": {"title": "Multimodal vision-language models with guided cross-attention for crisis event understanding", "abstract": "Understanding crisis events from social media posts to support response and rescue efforts often requires robust multimodal reasoning over both visual and textual content. However, existing models often struggle to fully leverage the complementary nature of these modalities, particularly in noisy and information-sparse settings. In this work, we propose a novel multimodal framework, CapFuse-Net, that integrates pretrained vision-language models (VLMs) with a guided fusion strategy for improved crisis event classification. We first augment textual input with VLM-generated image-grounded captions, providing richer context for textual reasoning. A Cross-Feature Fusion Module (CFM) is then used to fuse the original and generated text using cross-attention, followed by a Guided Cross-Attention module that enables fine-grained interaction between visual and textual features. To further refine this fusion, we incorporate a Differential Attention mechanism that enhances salient feature selection while suppressing noise. Extensive experiments on three crisis classification benchmarks demonstrate that our method consistently outperforms unimodal and standard multimodal baselines. In addition, an ablation study demonstrates the importance of each proposed component, in particular, the synergy between VLM-based captioning and attention-guided fusion. Finally, we present results for qualitative interpretability through Grad-CAM visualizations and robustness across diverse crisis scenarios.", "tldr": "Proposes a Multimodal model, CapFuse-Net, which combines VLM captions and attention-guided fusion to enhance multimodal crisis understanding, improving accuracy, stability, and transferability.", "keywords": ["Multimodal understanding", "Vision Language Models", "Crisis Event analysis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ae4c2fdda2b559a0710032f8af364c519713d3e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel multimodal framework, CapFuse-Net, for processing social media data in crisis events, integrating both textual and visual information. The core idea of the framework is to generate image-text pair descriptions using an enhanced Vision-Language Model (VLM), and then combine them with a Cross-Feature Fusion Module (CFM), Guided Cross-Attention Mechanism, and Differential Attention Mechanism to improve crisis event classification performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The CapFuse-Net model proposed in this paper cleverly combines image descriptions generated by the Vision-Language Model with textual content. \n2. The paper thoroughly validates the effectiveness and robustness of the proposed method through experiments on multiple crisis classification benchmark datasets.\n3. Ablation experiments clearly demonstrate the contribution of each module, further supporting the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. CapFuse-Net introduces several complex mechanisms (such as CFM, Guided Cross-Attention, and Differential Attention) in the multimodal fusion process, which may result in a high computational cost. This could become a performance bottleneck, especially when real-time processing of large amounts of social media data is required.\n\n2. Although the model achieves excellent performance on most tasks, it performs poorly in certain tasks (such as damage severity classification) when handling categories with moderate damage. Future work could consider using more diverse crisis event datasets or collecting additional data from actual disaster response scenarios to further validate the model's generalization capability.\n\n3. The paper enhances textual information by generating image descriptions using LLaVA, but the LLaVA model may introduce the risk of generating false information (i.e., \"hallucinations\"). How to control this risk requires further discussion."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OWJsn4hbeZ", "forum": "YklIjgIZzn", "replyto": "YklIjgIZzn", "signatures": ["ICLR.cc/2026/Conference/Submission12730/Reviewer_xHpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12730/Reviewer_xHpf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528148826, "cdate": 1761528148826, "tmdate": 1762923551906, "mdate": 1762923551906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce a novel multimodal framework, named CapFuse-Net, to enhance the understanding and classification of crisis events from multimodal social media data. The main contributions include: 1) a novel multimodal framework that augments sparse textual data with VLM-generated captions for richer context; 2) an innovative Cross-Feature Fusion Module (CFM) that effectively merges original and generated text using cross-attention ; and 3) a guided fusion pipeline combining Guided Cross-Attention and Differential Attention to refine the alignment between visual and textual features while suppressing noise. \n\nThe motivation for this work stems from the observation that existing multimodal models often struggle to effectively leverage the complementary nature of visual and textual data in crisis scenarios, particularly when dealing with noisy and information-sparse social media posts. To address this, the authors propose a framework that first enriches the textual input by generating image-grounded captions using a Vision-Language Model. The core innovations of their method include the Cross-Feature Fusion Module (CFM), which fuses the original tweet with the generated caption to create a richer text representation. This is followed by a Guided Cross-Attention mechanism for fine-grained interaction between visual and textual features, and a final Differential Attention layer to enhance salient features while suppressing noise. \n\nExtensive experiments demonstrate that the proposed model, CapFuse-Net, achieves superior performance across multiple crisis classification benchmarks, consistently outperforming existing state-of-the-art baselines. Furthermore, the interpretability analysis, including Grad-CAM visualizations, effectively shows that the model learns to focus on crisis-relevant visual regions, confirming the effectiveness of its attention mechanisms. The key contributions of this work include the CapFuse-Net framework that effectively enriches sparse text and improves cross-modal alignment, the introduction of the Cross-Feature Fusion Module (CFM) for enhanced textual representation, and the design of a multi-stage, attention-guided fusion pipeline that successfully improves both the performance and interpretability of crisis event classification models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors employ a Vision-Language Model to generate image-grounded captions, effectively enriching sparse social media text. A novel Cross-Feature Fusion Module then integrates this new context to create a superior textual representation for analysis.\n\n2. The model utilizes a sophisticated fusion pipeline with Guided Cross-Attention and Differential Attention for effective cross-modal alignment. This advanced architecture leads to state-of-the-art performance while simultaneously improving model interpretability."}, "weaknesses": {"value": "1.The paper's experiments fail to demonstrate the effectiveness of the Differential Attention module in Table 2, as its inclusion sometimes degrades performance. More tellingly, the authors' final, best-performing model configuration omits this module, indicating it is not an essential component.\n\n2.The paper neglects to analyze the critical trade-off between performance gains and the high computational cost of relying on a large VLM, which raises significant concerns about the model's practical feasibility in real-time crisis response scenarios.\n\n3.The framework's novelty is somewhat diminished as it primarily relies on assembling existing and widely-used modules, such as cross-attention, rather than introducing fundamentally new architectural concepts."}, "questions": {"value": "1.Given that the Differential Attention module did not improve experimental performance and was omitted from the final model, could you clarify its contribution to the overall architecture?\n\n\n2.Considering the significant computational overhead from the VLM, can you analyze the trade-off between performance gains of the classification task and the feasibility of deploying the model in real-time crisis response scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "adcfQ7YaeH", "forum": "YklIjgIZzn", "replyto": "YklIjgIZzn", "signatures": ["ICLR.cc/2026/Conference/Submission12730/Reviewer_VHa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12730/Reviewer_VHa6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550252493, "cdate": 1761550252493, "tmdate": 1762923551661, "mdate": 1762923551661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CapFuse-Net, a multimodal crisis event classification framework that leverages vision–language models (VLMs) within a structured fusion architecture. The approach enriches textual inputs (e.g., social media posts) with VLM-generated image captions, then integrates the original and augmented text using a Cross-Feature Fusion Module (CFM). Multimodal alignment is further improved through Guided Cross-Attention and Differential Attention mechanisms. Experiments on the CrisisMMD, DMD, and N24News benchmarks demonstrate state-of-the-art results across informativeness classification, humanitarian category recognition, and damage severity assessment. Comprehensive ablation studies and interpretability analyses (e.g., Grad-CAM visualizations) confirm the effectiveness and generalizability of each component in the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive Evaluation: Extensive multi-dataset and multi-task benchmarking against strong baselines (e.g., CaMN, CrisisKAN) demonstrates consistent and significant performance gains across settings.\n2. Interpretability: Grad-CAM visualizations (Fig. 2) and modality attribution analyses (Fig. 3) offer clear insights into how different modalities contribute to the model’s decisions.\n3. Robustness Analysis: Cross-dataset and split-wise evaluations (Table 4) confirm the model’s stability and resilience under distributional shifts.\n4. Ablation Rigor: Detailed module-level ablations (Table 3) systematically quantify the contribution of each architectural component to overall performance."}, "weaknesses": {"value": "1. Novelty Gap: The proposed fusion strategy closely parallels existing approaches (e.g., CFM vs. CAM), offering incremental refinements rather than a fundamentally new paradigm.\n2. Data Limitations: While CrisisMMD’s class imbalance (Fig. 9) is acknowledged, the paper does not implement or evaluate mitigation strategies such as resampling or loss reweighting.\n3. Computational Cost: The framework depends on LLaVA-based captioning and multiple attention modules, yet omits analysis of efficiency metrics (e.g., inference latency) critical for real-time crisis response.\n4. Hallucination Mitigation: The integration of Jiang et al. (2025) appears superficial, as no ablation study is provided to assess its necessity or its influence on caption fidelity."}, "questions": {"value": "1. Model Efficiency: How does CapFuse-Net’s computational efficiency compare to simpler fusion paradigms (e.g., early or late fusion)? Do the observed performance gains justify its architectural complexity, particularly for resource-constrained or real-time deployments?\n2. VLM Choice: Were alternative vision–language models (e.g., BLIP, GPT-4V) evaluated for caption augmentation? If not, how might the choice of VLM influence the quality and downstream performance of the fused representations?\n3. Modality Attribution: The modality attribution analysis (Fig. 3) indicates stronger reliance on visual cues. Does this suggest that the framework’s advantage lies primarily in mitigating textual noise rather than achieving deeper multimodal integration?\n4. Class Imbalance: For minority categories, were any data augmentation or synthetic sampling strategies employed to alleviate imbalance and enhance generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bV4hZSiizr", "forum": "YklIjgIZzn", "replyto": "YklIjgIZzn", "signatures": ["ICLR.cc/2026/Conference/Submission12730/Reviewer_J6NZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12730/Reviewer_J6NZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890001499, "cdate": 1761890001499, "tmdate": 1762923550829, "mdate": 1762923550829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}