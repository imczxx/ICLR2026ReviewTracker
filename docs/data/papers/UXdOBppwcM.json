{"id": "UXdOBppwcM", "number": 19684, "cdate": 1758298360234, "mdate": 1759897026082, "content": {"title": "The Consequences of the Intrinsic Gap Between Reward Beliefs and MDP Rewards", "abstract": "Deep neural policies have gained the ability to learn and execute sequences of decisions in MDPs that involve complex and high-dimensional states. Despite the growing use of reinforcement learning in diverse fields from language agents to medical and finance, a line of research has focused on constructing reward functions by observing how an optimal policy behaves, with the underlying premise that this will result in policies that are aligned with the intended outcome. In this line of research, several studies have proposed algorithms for learning a reward function or an optimal policy from observed optimal trajectories, with the goal of achieving sample-efficient, robust, and aligned policies. In this paper, we analyze the implications of learning with reward beliefs in high-dimensional state representation MDPs and we demonstrate that standard deep reinforcement learning yields more resilient and value-aligned policies when compared to learning from the behaviour of other policies in MDPs with complex state representations.", "tldr": "", "keywords": ["reward beliefs", "intrinsic reward gap"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/062ddbee011b605a653262f2a9fbcbe05fde09c0.pdf", "supplementary_material": "/attachment/37c09be534d0368646cabd13ebc7abcdcc58a271.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a theoretical analysis of the implications of learning with reward beliefs in high-dimensional state representation MDPs in order to demonstrate that standard deep reinforcement learning yields more resilient and value-aligned policies when compared to learning from the behaviour of other policies in MDPs with complex state representations. \nThe authors focus on analyzing robustness. They mathematically show and explain how training via the state-of-the-art deep imitation learning algorithms leads to policies that are intrinsically non-robust and provide theoretical analysis on the causes of the adversarial vulnerabilities of inverse deep neural policies. The authors also study the cost of learning from expert demonstrations, providing empirical verification of the outcome of slight deviation of the optimal trajectory and its effects on the reward predictions and the true rewards obtained from the environment, and showing that small deviations from the optimal policy result in significant decrease in both the rewards obtained, and the accuracy of the predicted rewards for the inverse deep neural policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Overall, this paper could be a significant algorithmic contribution. The authors focus on answering several questions: 1. How do the state-of-the-art algorithms that focus on learning via emulating affect the policy robustness compared to standard vanilla training algorithms? 2. What are the costs of learning from expert demonstrations compared to learning via exploration? 3. Does learning without rewards in high-dimensional state representation MDPs yield learning non-robust features independent from both the MDP and the algorithm?\n\nThe main strengths of this research can be summarized as follows:\n\n1. The authors investigate the robustness of the state-of-the-art deep neural policies that focus on learning via emulating against adversarial directions independent from both the algorithms the policies are trained with and the MDPs the policies are trained in.\n2. They provide a mathematical analysis that explains that learning from expert demonstrations (Learning without rewards) comes with a great cost compared to learning from exploring and that it causes sequential decision making processes to be extremely sensitive towards slight deviations from their optimal trajectories.\n3. The authors empirically show that vanilla trained deep reinforcement learning policies are significantly more robust and aligned compared to algorithms proposed to learn without the presence of a reward function."}, "weaknesses": {"value": "For the theory, there are a few steps that need clarification. In section 3, the outcomes of lack of exploration in learning without rewards, the proof of Lemma 3.1; the transition in equations is not clear. Proof of proposition 3.3, why is the gradient of the state-action value function equal zero? \nIn addition, one of the questions your research focuses on is whether learning without rewards in high-dimensional state representation MDPs yields learning non-robust features independent from both the MDP and the algorithm? It seems like there is no theoretical analysis that gives a clear answer. \n\nIn section 4, for novelty, there is a statement where you describe the methods used to cause deviations from the optimal trajectory in order to understand the robustness of policies, “These are described below in more detail. For the adversarial directions, we utilize the methodology described in Korkmaz (2022).” Are definitions 4.1 and 4.2 completely new? Or are they somehow an extension?"}, "questions": {"value": "For the experiments, the following should be addressed.\n\n1. The central contribution is showing the cost of learning from experts and that small deviations from the optimal policy result in a significant decrease in both the rewards obtained and the accuracy of the predicted rewards for the inverse deep neural policy. It would be beneficial to empirically demonstrate with several algorithms and different environments. How would other deep RL algorithms and environments change the results? Would the cost remain? \nThis will yield more discussion on the impact of the adversarial direction as stated, “ it is much larger on the inverse-Q learning policy than on the vanilla policy.” \nThe same goes for breaking the link between imitation and inverse reinforcement learning. Would other empirical experiments change the conclusion “without the correlation between true rewards and reward predictions the decisions of the policy will be random, i.e., untrained.”\n\nOn page 9, you stated that “The fact that subtle deviations from the optimal trajectory break the beliefs of the inverse deep neural policy on the MDP rewards, raises significant questions regarding misalignment.” I would like to see some discussion on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QXUtreN2JK", "forum": "UXdOBppwcM", "replyto": "UXdOBppwcM", "signatures": ["ICLR.cc/2026/Conference/Submission19684/Reviewer_rR4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19684/Reviewer_rR4E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691389678, "cdate": 1761691389678, "tmdate": 1762931527526, "mdate": 1762931527526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the robustness limitations of reward-free imitation and inverse reinforcement learning in high-dimensional MDPs. Under the assumption that expert trajectories are confined to a low-dimensional subspace, the authors provide theoretical results showing that gradient updates in inverse Q-learning remain restricted to that subspace, leaving unseen states untrained and reward estimates uncorrelated with true rewards. Empirical evaluations on several Atari environments apply small perturbations and adversarial directions to compare inverse Q-learning with standard deep reinforcement learning (DDQN). The results show that reward-free policies degrade sharply under minor deviations, while vanilla deep RL agents are more robust. The paper concludes that learning solely from expert demonstrations, without exploration, leads to intrinsic non-robustness and misalignment between predicted and true rewards."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is technically sound and internally consistent. The theoretical results, definitions, and empirical tests are clearly connected.\n\n2. The experimental framework is well designed and could be reused for future robustness evaluations.\n\n3. The work provides a formal framing of the commonly discussed issue that reward-free imitation learning lacks exploration and therefore generalization, which may be useful for future theoretical reference."}, "weaknesses": {"value": "1. The main conclusion, that lack of exploration in reward-free imitation learning leads to non-robust behavior, is widely recognized in the field and follows almost directly from the stated assumptions. The work therefore adds limited conceptual novelty.\n\n2. The theoretical framework depends heavily on a restrictive subspace-contained expert trajectories assumption. No sensitivity or coverage analysis is provided to show when the reward-belief gap would vanish.\nThe paper does not offer quantitative bounds, convergence analysis, or algorithmic extensions that could make the results more generally useful.\n\n3. The writing, while grammatically correct, is verbose and repetitive."}, "questions": {"value": "1. Have the authors considered analyzing how the proposed reward-belief gap changes as expert coverage increases? A quantitative relationship between state-space coverage and robustness could strengthen the claims.\nIn addition, what happens in environments where the state dimension is low or the expert trajectories cover most of the state space? The current theory appears silent in that regime.\n\n2. How sensitive are the theoretical results to the specific subspace-contained assumption? Would the conclusions still hold if the expert trajectories only approximately (rather than strictly) lie in a subspace, or if the manifold were nonlinear?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q33qDQbDbH", "forum": "UXdOBppwcM", "replyto": "UXdOBppwcM", "signatures": ["ICLR.cc/2026/Conference/Submission19684/Reviewer_TuwV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19684/Reviewer_TuwV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848388071, "cdate": 1761848388071, "tmdate": 1762931526948, "mdate": 1762931526948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper seems to study the relationship between learning from interactions (reinforcement learning) and learning from demonstrations (IRL/IL/RLHF) in terms of robustness, \"costs\", and \"alignment\".\n\nI am unable to understand the problem statement, core claims, or evaluation criteria because the abstract and introduction use vague/undefined terminology, conflate distinct problem settings, and make unsubstantiated, absolute claims."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "A theoretical understanding of the limitations of learning without rewards is important.\nIf this paper really has insightful results, the community may benefit from them."}, "weaknesses": {"value": "I cannot reliably interpret the paper due to its unclear writing.\n\nBecause the central terms (e.g., \"reward beliefs\", \"alignment\"), the exact comparison (which algorithms/baselines, \"vanilla trained deep reinforcement learning policies\"), and the claimed novelty are not clearly defined in the abstract and introduction, I cannot assess soundness, significance, or originality of this paper.\nThe exposition fails to communicate a precise problem or contribution."}, "questions": {"value": "## 1. Dangling logical contrast\n\nThe abstract opens with \"**Despite** the growing use of reinforcement learning... a line of research has focused on constructing reward functions by observing how an optimal policy behaves...,\" which asserts a contrast but provides none.\nThis reads as a non sequitur and leaves the scope ambiguous.\n\n## 2. Undefined, nonstandard terminology\n\nThe key phrase \"learning with **reward beliefs** in high-dimensional state representation MDPs\" is not defined.\nIt is unclear what \"reward beliefs\" precisely denotes and \"state representation MDPs\" is not a standard term.\n\n\"deep reinforcement learning policies are vastly utilized in manifold settings\": \"manifold settings\" is nonstandard and not defined.\n\n\"robust… aligned policies,\" \"robustness and adversarial weaknesses,\" \"alignment problem and vulnerabilities\": These terms are used as if self-evident without operational definitions or metrics.\n\n## 3. Conflation of problem families\n\nThe introduction moves between RLHF (preference learning for LLMs), IRL (inferring rewards), and imitation learning (policy cloning) without delineating boundaries or the specific setting studied in this work.\nAs a result, I cannot tell what is actually being compared.\n\n## 4. Overstated, absolute claim without support\n\n\"The proposal of learning a soft-Q function is currently **the only** algorithm that can achieve a performance level that can match deep reinforcement learning in high dimensional state representation MDPs.\"\nThis is a sweeping claim presented without careful positioning or evidence.\n\n\"**first one** to focus on adversarial vulnerabilities of... without a reward function\": priority claim not substantiated.\n\n## 5. Ambiguous use of loaded terms, unclear anthropomorphic language\n\nThe abstract claims vanilla RL is more \"value-aligned,\" but no operational definition of \"alignment\" is given upfront (e.g., what metric, measured where and how).\n\nSimilarly, later text refers to an \"inverse deep neural policy\" that \"**believes** that it in fact received larger rewards than it did before,\" and \"a gap between what the policy actually did and what the policy itself **believed** what it did\", but \"belief\" is not defined anywhere.\nThese omissions prevent proper evaluation.\n\n## 6. Inconsistent terminology for the setting\n\nThe paper alternates between \"high-dimensional state observations,\" \"complex state representations,\" and \"high dimensional state representation MDPs\" as if they were synonyms; the inconsistency adds confusion about the precise scope.\n\n## 7. Other writing issues\n\nFor example:\n- \"adversarial directions independent from both the algorithms... and the MDPs\": how is \"independent\" established?\n- \"standard vanilla training algorithms,\" \"vanilla trained deep reinforcement learning policies are significantly more robust and aligned\": \"vanilla\" is not specified (algorithm, objective, training setup).\n- \"extremely sensitive towards slight deviations,\" \"significantly more robust and aligned,\" correlation is \"utterly broken\": Intensity words without quantification.\n- \"We provide a theoretical analysis... and introduce a mathematically rigorous investigation that explains this phenomenon that learning from expert demonstrations comes with a great cost compared to learning from exploring.\": No concrete theorem/statement is given at this point; the text \"teases\" without conveying any result.\n\n## 8. Typos and basic writing errors\n\nE.g., \"human human values.\" Such errors further undermine clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2rR5t34Hla", "forum": "UXdOBppwcM", "replyto": "UXdOBppwcM", "signatures": ["ICLR.cc/2026/Conference/Submission19684/Reviewer_1LNV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19684/Reviewer_1LNV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851901175, "cdate": 1761851901175, "tmdate": 1762931526551, "mdate": 1762931526551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of deep neural policies that learn without a reward function (as in imitation learning and inverse reinforcement learning). It provides a plausible mathematical analysis of why learning with demonstrations makes the policy extremely sensitive to adversarial directions. For this purpose, the paper introduces various types of adversarial attacks on the deep policy manifold. On the other hand, the authors show empirically that vanilla trained policies that learn with a reward function are much more robust."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a relatively simple theoretical model, which is however consistent with some of the observations regarding the sensitivity of the deep policy to adversarial directions and lack of robustness.\n- The authors introduce various types of adversarial directions, ranging from random deviations to worst possible deviations. In one of the cases, the adversarial direction is added to the visited states of another policy, which I found very interesting.\n- The experimental study is interesting, as it shows multiple things: (i) extreme sensitivity of policy learned without a reward function to adversarial directions, (ii) resilience of vanilla trained reinforcement learning algorithms, (iii) inverse deep neural policies can be attacked via black-box adversarial attacks, (iv) even very-low-frequency attacks can make the reward predictions of inverse deep reinforcement learning completely unreliable and misaligned with the intended objective."}, "weaknesses": {"value": "- The theoretical model makes a large number of assumptions - for instance, the idea that the set of expert trajectories remains subspace-contained is a strong assumption. It is not clear why this should be true in a deep neural policy. The authors did not provide any empirical evidence suggesting that such a phenomenon is indeed taking place. Without such irrefutable evidence, I'm not sure how easy it is to accept the premise of Definition 3.2.\n- Lemma 3.1 seems to only serve the purpose of motivating Definition 3.2 - indeed, it is not used anywhere else to the best of my understanding. That said, I feel that the leap from Lemma 3.1 to Definition 3.2 is quite big; it is one thing to claim that the projection on a subspace is larger, it is another thing to proceed to claim that the expert trajectory is subspace contained.\n- Even though I do not disagree with the authors' finding that standard vanilla trained reinforcement learning algorithms are more robust, there are use case scenarios where learning from demonstrations may be the only viable option. I feel it would have been more interesting to practitioners to possibly discuss how the shortcomings of learning without a reward function can be mitigated. In the current draft, the main takeaway message seems to be that there is nothing that can be done about these vulnerabilities except use standard vanilla RL, which may nevertheless not be possible in a number of use cases.\n- I was a bit confused about how the relevance of this work to current LLM research. Learning with demonstrations is common practice with current LLMs. For instance, in RLHF we learn a reward function from human preferences (but notice that no reward function is initially known). And even in supervised fine-tuning, the model essentially learns to imitate the style and instruction-following capabilities of a human expert. I think it is safe to say that these approaches can work quite well in practice, despite their shortcomings. How is all that related to this paper? Is the idea that learning from demonstrations inherently flawed, or that it is not robust at all, despite the fact that it has shown some success with LLMs?"}, "questions": {"value": "- Why Definition 3.2 follows from Lemma 3.1? I'm eager to believe that the projection on the subspace for high-value states is larger, but how does that imply that the expert trajectory is subspace contained?\n- Have the authors tried any experiments to visualize the subspace containment hypothesis? Have they managed to produce any empirical evidence that it is true in deep neural policy networks?\n- In practice, how would the adversarial deviations take place? Once I have a policy trained, I will simply follow what my policy says without producing any deviations whatsoever. With the exception of a malicious adversary who can select different actions than what the policy dictates, how can these adversarial attacks occur in practical use cases?\n- How is this work relevant to modern LLMs, where we often use learning from experts (e.g., supervised fine-tuning) or learning without rewards (e.g., RLHF with preference demonstrations from humans)? \n\nOverall, this is an interesting work, but I feel it would help to clarify the aforementioned points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5srJiRf8v1", "forum": "UXdOBppwcM", "replyto": "UXdOBppwcM", "signatures": ["ICLR.cc/2026/Conference/Submission19684/Reviewer_CwKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19684/Reviewer_CwKs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210327360, "cdate": 1762210327360, "tmdate": 1762931526150, "mdate": 1762931526150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}