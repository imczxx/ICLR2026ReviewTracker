{"id": "fsZkx8gek0", "number": 15148, "cdate": 1758248283972, "mdate": 1759897325054, "content": {"title": "RepIt: Steering Language Models with Concept-Specific Refusal Vectors", "abstract": "Current safety evaluations of language models rely on benchmark-based assessments that may miss targeted vulnerabilities. We present RepIt, a simple and data-efficient framework for isolating concept-specific representations in LM activations. While existing steering methods already achieve high attack success rates through broad interventions, RepIt enables a more concerning capability: selective suppression of refusal on targeted concepts while preserving refusal elsewhere. Across five frontier LMs, RepIt produces evaluation-evading models that answer questions related to weapons of mass destruction while still scoring as safe on standard benchmarks. We find  the edit of the steering vector localizes to just 100-200 neurons, and robust concept vectors can be extracted from as few as a dozen examples on a single A6000, highlighting how targeted, hard-to-detect modifications can exploit evaluation blind spots with minimal resources. By demonstrating precise concept disentanglement, this work exposes critical vulnerabilities in current safety evaluation practices and demonstrates an immediate need for more comprehensive, representation-aware assessments.", "tldr": "We can selectively jailbreak models while preserving refusal in other contexts, producing unsafe models that evade standard detection.", "keywords": ["interpretability", "representations", "steering", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa47b1ed443cf15404487a97b1868ef87b1e75c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes REPIT, which can selectively suppress models' refusal towards specific target concepts, while maintaining on other harmful topics. In this way, REPIT can create models that can circumvent safety benchmark tests, where models appear safe but actually conceal specific malicious capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper defines a timely problem: how to isolate specific harmful concepts from a broad refusal behaviors.\n- The proposed disentanglement process is supported by closed-form solutions that does not require costly optimization or search."}, "weaknesses": {"value": "Although there have been many studies on the manipulation of refusal vectors, I appreciate the authors' proposed disentanglement framework for extracting more fine-grained concept vectors from refusal vectors. However, I have some questions regarding the experimental setup, and addressing them could improve the persuasiveness of the paper.\n\n1. **The experiment focuses on weapons of mass destruction**. This is certainly important, but why is it the only focus? What side effects would arise if applied to other concepts, such as the 21 non-target concepts the authors have already collected? Furthermore, benchmarks, like HarmBench, have prepared critical evaluations for this topic, which cannot support the claim of evaluating backdoors.\n2. **The definition of ASR the authors use is unclear**. From appendix, I guess the authors used LlamaGuard to evaluate whether the answers are harmful, which should be explained in detail in the main text. I suggest the authors use GuidedBench [1] to reproduce their evaluation, as it provides explainable results, which are more suitable for assessing whether suppression and single-concept jailbreak are truly successful.\n3. **The scale of the dataset involved in the experiments is also not mentioned**, making it difficult to assess the confidence of the results.\n4. **The number of neurons to be edited in the paper is not very rigorous**. Different models have different numbers of neurons; can the authors clarify whether only 100-200 neurons need to be edited across various models of different sizes?\n\n[1] GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of In-the-wild LLM Jailbreak Methods, https://arxiv.org/abs/2502.16903"}, "questions": {"value": "1. What is the effect of REPIT when applied to GPT-OSS series models? These models seem to have isolated harmful content in the training corpus; for example, when asked queries related to bombs, they would normally refuse to respond. However, under certain steering frameworks, even if they do respond, harmful content is not visible.\n2. The authors should ensure that the paper is self-contained. For instance, the COSMIC algorithm is not widely known; could the authors briefly describe its principles in the main text? In the paper, the use of COSMIC appears before its citation (first mentioned in L138, but the citation appears in L218).\n3. Can the authors provide at least one concept that appears harmful but is not included in current safety benchmarks, and demonstrate the results manipulated through REPIT? Otherwise, the motivation for evaluating backdoors is not correct—at least it feels alarmist."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The authors need to clarify whether this work has received IRB approval, as it involves potential amplification for malicious content."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YvdUmPVQ5c", "forum": "fsZkx8gek0", "replyto": "fsZkx8gek0", "signatures": ["ICLR.cc/2026/Conference/Submission15148/Reviewer_A2cG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15148/Reviewer_A2cG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760673618560, "cdate": 1760673618560, "tmdate": 1762925463288, "mdate": 1762925463288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new way to find vectors that are only targeting specific concepts. Instead of manipulating refusal behavior of the LM on all harmful data, the authors show that their vectors do not alter refusal behavior on harmful questions with different content e.g. cyber vs bio. The method seems to be very effective and localizes to 100-200 neurons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Relevant problem\n- Effective & simple solution\n- Brings more evidence to work with similar findings"}, "weaknesses": {"value": "- I would like to see more ablation studies. \n    - How important is re-weighting, whitening, …\n    - Is there a baseline where instead of alpha P random vectors are subtracted but with the same search for rho\n- Different model sizes would be interesting. It seems logical that higher dimensional residual streams can represent different refusal concepts in more distinct ways. Seeing how repit scales would be very interesting.\n- Presentation: The start is very good. Then it becomes a bit too brief. I would like to see more explanation on the main method and the ablation study. In that experiment it is difficult to understand which steps have been performed with the three components before using them to jailbreak the models.\n- Minor: \n    - COSMIC on page 3 in Methodology is not introduced\n    - 4.1 could be rather background instead of methodology"}, "questions": {"value": "- In 6 the authors argue that entanglement enhances jailbreak effectiveness. I would like to see more evidence/understand the evidence for this claim.\n  - How many data samples are influencing the non-target basis? Does the non-target basis vector have more data points than the basic dim in this experiment?\n  - I think adding repit to figure 3 would make it easier to understand the performance differences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s6ZYycOhSe", "forum": "fsZkx8gek0", "replyto": "fsZkx8gek0", "signatures": ["ICLR.cc/2026/Conference/Submission15148/Reviewer_Cexb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15148/Reviewer_Cexb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656099139, "cdate": 1761656099139, "tmdate": 1762925462955, "mdate": 1762925462955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RepIt, which addresses the problem of representational entanglement in LLMs where different concepts are entangled in an LLM's internal circuitry, which makes targeted modifications difficult. The authors develop a three step process to disentangle concept vectors, which can then be used for suppressing refusal on concept specific harmful datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths are outlined as follows:\n1) The motivation and novelty of the setup is well defined and the derivation is clear, leading to a nice, very simple closed form solution\n2) The data efficiency of the algorithm is very appealling.\n3) The paper is overall very well written."}, "weaknesses": {"value": "The main weakness I have is the experimental setup is limited towards Bio, Chem, and Cyber attacks from the WMDP dataset. Since the knowledge required to answer these questions is highly concentrated, it will be easier to identify concept vectors. I would like to see experiments on broader safety benchmarks covering many different, overlapping concepts to see the effectiveness of this method."}, "questions": {"value": "Can you explain why all three steps (reweighting, whitening, and orthogonalization) are necessary? What would happen if one of the steps, particularly the whitening step to address collinearity, was omitted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "e4GFrin7In", "forum": "fsZkx8gek0", "replyto": "fsZkx8gek0", "signatures": ["ICLR.cc/2026/Conference/Submission15148/Reviewer_ctF9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15148/Reviewer_ctF9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930657963, "cdate": 1761930657963, "tmdate": 1762925462534, "mdate": 1762925462534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for selectively disabling safety mechanism for specific harmful concepts while preserving refusal behavior for other harmful concepts. The method employs a three-step procedure (reweighting, whitening, orthogonalization) to disentangle concept-specific representations from broader refusal behaviors. The authors demonstrate that their technique can create models that answer weapons-of-mass-destruction questions while maintaining refusal on other safety benchmarks, using as few as 12 examples and affecting only 100-200 neurons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clarity: The paper is generally well-written with clear mathematical exposition. The figures effectively demonstrate the core claims about selective targeting, and the technical methodology is described with sufficient detail for reproduction.\n\nEmpirical Validation: The results consistently show the intended selectivity across multiple models and domains."}, "weaknesses": {"value": "Fundamental Motivation Problems: The threat model is deeply flawed. The paper claims to expose \"evaluation evasion\" vulnerabilities, but the method requires test-time activation steering with weight access. An attacker with such capabilities could trivially remove all safety mechanisms rather than pursuing this complex selective approach. The framing misrepresents how safety evaluation actually works in practice.\n\n\nLimited Technical Novelty: The core mathematical operation — finding vectors aligned with targets while orthogonal to non-targets — is a standard linear algebra problem. The paper fails to compare against obvious baselines like LEACE (Belrose et al., 2023), standard Gram-Schmidt orthogonalization, or regularized projection methods. The \"novel\" components (ridge regularization, partial orthogonalization) are well-established techniques presented without proper context.\n\n\nInflated Security Claims: The characterization as an \"urgent threat to AI safety infrastructure\" is unsupported by realistic threat analysis. The paper conflates technical capability with practical security risk without considering simpler attack vectors or deployment constraints."}, "questions": {"value": "Why wasn't REPIT compared against LEACE, standard orthogonalization methods, or other concept removal techniques? These seem like obvious baselines for the stated problem.\n\nCan you provide a realistic scenario where an attacker would prefer selective concept isolation over simply removing all safety mechanisms? How would this work in actual deployment contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LmSZYFsSy2", "forum": "fsZkx8gek0", "replyto": "fsZkx8gek0", "signatures": ["ICLR.cc/2026/Conference/Submission15148/Reviewer_KWf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15148/Reviewer_KWf7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970390797, "cdate": 1761970390797, "tmdate": 1762925462058, "mdate": 1762925462058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}