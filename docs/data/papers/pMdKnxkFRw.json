{"id": "pMdKnxkFRw", "number": 13979, "cdate": 1758226416091, "mdate": 1763768190815, "content": {"title": "STAR: Speculative Decoding with Searchable Drafting and Target-Aware Refinement for Multimodal Generation", "abstract": "Speculative decoding (SD) has proven to be an effective technique for accelerating autoregressive generation in large language models (LLMs), however its application to vision-language models (VLMs) remains relatively unexplored. We propose~\\textit{STAR}, a novel SD framework designed specifically for fast and efficient decoding in VLMs. STAR leverages a neural architecture search (NAS) framework with target-aware supernet training to automatically identify both the optimal interaction strategy between the draft and target models, and the most suitable draft model architecture for the underlying hardware implementation platform. STAR additionally incorporates adaptive intermediate feature distillation, guided by attention entropy, to enable efficient draft training. Experiments on a range of well-established VLMs, including LLaVA series, Pixtral, and SmolVLM, demonstrate that STAR achieves up to a $3.8\\times$ speedup compared to standard decoding approaches and significantly outperforms existing SD baselines in both inference throughput and speculative acceptance length across a wide spectrum of VLMs.", "tldr": "", "keywords": ["Speculative decoding", "Vision-language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1063bc9c664f9f98aa2073e9a5f9369f3cddb78.pdf", "supplementary_material": "/attachment/c8b9ffdd89180b883e995461a6d6f89e98f4b80f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes STAR, a speculative decoding method for VLMs, which utilizes a NAS framework to find the optimal draft model configuration. STAR is comprised of a two-phase training strategy and three distinct loss functions. On various tasks, STAR outperforms existing speculative decoding methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the highly relevant challenge of speculative decoding for VLMs.\n2. The paper is clear and well-presented.\n3. STAR demonstrates performance gains over existing methods."}, "weaknesses": {"value": "1. The comparison appears unfair. STAR uses dataset specific draft models selected via an exhaustive search to find the highest speedup for each benchmark. Baselines (e.g., Medusa, EAGLE) likely use general configurations. Was STAR also evaluated using a single, fixed configuration for a fair comparison?\n\n2. The exhaustive search after TPPT is complete introduces a substantial cost (e.g., approximately 12 minutes per 100 mini-batches on MMT-Bench), limiting practical adaptability. Have the authors considered a test-time adaptation method?\n\n3. The work is an effective application of existing techniques (NAS, pruning, distillation), limiting its conceptual novelty.\n\n4. The evaluation lacks comparisons to other recent speculative decoding methods specifically designed for multimodal LLMs, such as DREAM [1] and MASSV [2].\n\n[1] DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding (NeurIPS 2025)  \n[2] MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models (arXiv 2025)"}, "questions": {"value": "Please refer to the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hqboeU16Vw", "forum": "pMdKnxkFRw", "replyto": "pMdKnxkFRw", "signatures": ["ICLR.cc/2026/Conference/Submission13979/Reviewer_73is"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13979/Reviewer_73is"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494066944, "cdate": 1761494066944, "tmdate": 1762924478934, "mdate": 1762924478934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STAR, a speculative decoding method for VLMs. It employs a neural architecture search framework to automatically identify the optimal interaction strategy between the draft and target model and the ideal draft model architecture. Moreover, STAR introduces a intermediate feature distillation for training of draft model. Experimental results show that it achieves a better performance than Ealge-2 etc. on multiple VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed integrated training and distillation shows clear speed-up gains over naïve pruning.\n- The entropy + delta-entropy metric provides a principled way to select informative layers for distillation/architecture decisions.\n- The authors conduct experiments across multiple multimodal benchmarks with"}, "weaknesses": {"value": "- TPPT requires target model forward passes and intermediate feature distillation, the overhead need to be clarified.\n- While fine granularity gives little benefit here, optimal settings may differ for other model families or tasks.\n- The related work[1] needs to be discussed and compared.\n\n[1] ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding, NeurIPS’2025."}, "questions": {"value": "- How can we monitor acceptance length and draft rejections online to autotune γ and compression ratios per workload?\n- What is the expected GPU-hour if the target model grows to 70 B or 110 B parameters? Is there a transfer protocol so that a super-net trained for LLaVA-7 B can warm-start the 13 B variant?\n- AIFD uses sum of entropy and ∆-entropy. Why not a weighted sum or a learned gating network? Did the authors try other indicators, e.g., Fisher information or gradient variance?\n- Have the authors tried tree-based verification to accept discontinuous spans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PWFgDixppe", "forum": "pMdKnxkFRw", "replyto": "pMdKnxkFRw", "signatures": ["ICLR.cc/2026/Conference/Submission13979/Reviewer_kfez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13979/Reviewer_kfez"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902076439, "cdate": 1761902076439, "tmdate": 1762924478483, "mdate": 1762924478483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents STAR, a speculative decoding framework tailored for vision-language models (VLMs). Unlike prior speculative decoding methods designed for text-only LLMs, STAR addresses multimodal challenges such as visual token redundancy and cross-modal feature alignment. The framework introduces three main components: (1) Searchable Drafting: a neural architecture search (NAS)–based approach to automatically find the optimal draft model structure, pruning ratio, and feature interaction strategy; (2) Target-Aware Refinement: an adaptive intermediate feature distillation method that selects target layers features based on attention entropy and stability for better multimodal supervision; (3) Adaptive Pruning: dynamic visual and textual token pruning guided by the target model’s attention maps.\n\nThrough these components, STAR jointly optimizes speed, accuracy, and hardware efficiency. Experiments on multiple VLMs (LLaVA, Pixtral, SmolVLM) and six benchmarks demonstrate up to 3.8× decoding speedup with minimal performance loss, showing both strong system-level integration and transferability across architectures and devices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper systematically extends speculative decoding from text-only LLMs to multimodal VLMs through a well-structured framework combining searchable drafting, target-aware refinement, and adaptive pruning. This integration demonstrates strong system-level design ability and effectively addresses VLM-specific issues such as visual token redundancy and cross-modal feature alignment. \n\n- The method shows good transferability, working consistently across multiple VLM architectures (LLaVA, Pixtral, SmolVLM) and diverse multimodal benchmarks (ScienceQA, MMBench, SEED-Bench, MathVista). The experiments are comprehensive and carefully executed, covering throughput, acceptance ratio, and hardware efficiency, which provides solid empirical validation."}, "weaknesses": {"value": "- The algorithmic novelty is modest. While the proposed STAR framework is well-motivated and demonstrates strong empirical results, its technical novelty appears incremental and compositional rather than conceptual. Each component—Neural Architecture Search (NAS), attention-based intermediate feature distillation, and adaptive token pruning—has been extensively studied in prior literature. STAR primarily reassembles these existing techniques within the context of speculative decoding for VLMs, without introducing a fundamentally new algorithmic mechanism. Consequently, the contribution seems more system-level and application-driven than theoretically or algorithmically innovative. The key value lies in integrating multiple known techniques effectively to address the multimodal bottlenecks in speculative decoding, rather than in proposing a novel computational principle.\n\n- Another concern is that all baselines (SPD, Medusa, Hydra, EAGLE, etc.) were designed for text-only LLMs. Although the authors state they “adapt” these methods to VLMs, the adaptation process is not described in sufficient detail. As a result, it is unclear whether the improvements arise from STAR’s architectural innovations or simply from modality-specific adaptations (e.g., pruning visual tokens, target-aware distillation) that are unavailable to the baselines."}, "questions": {"value": "The authors should better substantiate STAR’s originality and fairness. \n\n- To address the limited novelty concern, consider formalize the Target-Aware Refinement beyond a simple attention-entropy heuristic, introduce a multimodal-specific NAS objective rather than reusing standard search schemes, and show ablations quantifying how each module (NAS, refinement, pruning) contributes to both speedup and acceptance ratio. \n\n- To address the fairness concern, they should clearly describe how LLM baselines (SPD, Hydra, EAGLE, etc.) were adapted to VLMs, or include stronger multimodal variants (e.g., EAGLE + vision token pruning) to ensure comparable settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ox5dQP9Byq", "forum": "pMdKnxkFRw", "replyto": "pMdKnxkFRw", "signatures": ["ICLR.cc/2026/Conference/Submission13979/Reviewer_FoVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13979/Reviewer_FoVz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978143094, "cdate": 1761978143094, "tmdate": 1762924477961, "mdate": 1762924477961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "STAR accelerates VLM inference through speculative decoding with neural architecture search to jointly optimize draft model configuration (attention head pruning, visual token compression, feature injection) and target model alignment. The framework uses adaptive intermediate feature distillation and two-phase progressive training, achieving up to 3.8× speedup and consistently outperforming existing methods across multiple VLMs and benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves consistent improvements over a number of reported baselines and datasets.\n2. The paper applies NAS to VLM speculative decoding, which jointly optimizes draft model architecture and target feature alignment through entropy-guided distillation, addressing a previously unexplored optimization space in multimodal acceleration.\n3. The method is clearly introduced."}, "weaknesses": {"value": "1. The connection between motivation and methodology is unclear. The authors stated that the VLMs require more computation than text-only LLMs, but did not explain where the extra computation comes from. For decoder-only VLMs, does the extra compute come from the visual encoder, which is separate from the speculative decoding process studied in this paper?\n2. The experimental settings are not described clearly enough. For example, for the data in Table 2, what inference batch size was used, what inference framework was employed, and on what hardware were the measurements taken?\n3. Lack of a comparable baseline: [EAGLE-3](https://arxiv.org/abs/2503.01840) is not included."}, "questions": {"value": "1. Table 3 compares STAR with EAGLE in different GPU settings. What is the performance of STAR where no hardware-aware searching is applied in different GPUs? (or with the same searching result applied to all GPUs)\n2. Other questions in the \"Weakness\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SOAcJyEdf6", "forum": "pMdKnxkFRw", "replyto": "pMdKnxkFRw", "signatures": ["ICLR.cc/2026/Conference/Submission13979/Reviewer_b8ad"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13979/Reviewer_b8ad"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980505853, "cdate": 1761980505853, "tmdate": 1762924477545, "mdate": 1762924477545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "This paper proposes **STAR**, a speculative decoding framework tailored\nfor *vision-language models* (VLMs). STAR combines a NAS-based\n*searchable draft model* with *target-aware refinement* via adaptive\nintermediate feature distillation and *adaptive pruning* of visual and\ntextual tokens. The method jointly optimizes the draft architecture\n(attention head pruning, visual token compression, feature injection\nstrategy) and alignment with a frozen target model. Experiments on\nmultiple VLMs (LLaVA, Pixtral, SmolVLM) and six multimodal benchmarks\nshow up to $3.8\\times$ speedup, consistently outperforming existing speculative decoding methods (e.g.,\nEAGLE-2).\n\nAcross reviewers, there is strong agreement on several key strengths:\n\n-   The paper addresses a **timely and underexplored challenge**, extending\n    speculative decoding from text-only LLMs to multimodal VLMs and\n    directly addressing visual token redundancy and cross-modal\n    alignment.\n\n-   The **NAS-based searchable drafting** and **entropy-guided\n    intermediate feature distillation** are recognized as principled and novel, offering a systematic alternative to heuristic-based draft designs.\n\n-   The **system-level integration** of searchable drafting,\n    target-aware refinement, and adaptive pruning is considered cohesive and hardware-aware.\n\n-   The **empirical evaluation is thorough**, covering multiple VLM\n    architectures, diverse multimodal benchmarks, and detailed\n    speed/acceptance/hardware metrics, with consistent improvements\n    across settings.\n\n-   The paper is **clear and well-presented**, effectively communicating a complex system architecture.\n\nWe thank all reviewers for their valuable comments and constructive feedback. We have addressed the weaknesses and concerns raised by reviewers point-by-point in the authors' rebuttal, where we provide additional clarifications, new experimental comparisons, and detailed justifications for our design choices."}}, "id": "uKCMy4YlH1", "forum": "pMdKnxkFRw", "replyto": "pMdKnxkFRw", "signatures": ["ICLR.cc/2026/Conference/Submission13979/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13979/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission13979/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763770903845, "cdate": 1763770903845, "tmdate": 1763770903845, "mdate": 1763770903845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}