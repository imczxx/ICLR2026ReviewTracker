{"id": "vtgjVWvWgO", "number": 14319, "cdate": 1758232846603, "mdate": 1759897377108, "content": {"title": "Unfolding Generative Flows", "abstract": "Continuous Normalizing Flows (CNFs) offer elegant generative modeling but remain bottlenecked by slow sampling: producing a single sample requires solving a nonlinear ODE with hundreds of function evaluations. Recent approaches such as Rectified Flow and OT-CFM accelerate sampling by straightening trajectories, yet the learned dynamics remain nonlinear black boxes, limiting both efficiency and interpretability. We propose a fundamentally different perspective: globally linearizing flow dynamics via Koopman theory. By lifting Conditional Flow Matching (CFM) into a higher-dimensional Koopman space, we represent its evolution with a single linear operator. This yields two key benefits. First, sampling becomes one-step and parallelizable, computed analytically via the matrix exponential. Second, the Koopman operator provides a spectral blueprint of generation, enabling novel interpretability through its eigenvalues and modes. We derive a practical, simulation-free training objective that enforces infinitesimal consistency with the teacher’s dynamics and show that this alignment preserves fidelity along the full generative path, distinguishing our method from boundary-only distillation. Empirically, our approach achieves competitive sample quality with dramatic speedups, while uniquely enabling spectral analysis of generative flows.", "tldr": "", "keywords": ["Generative Model", "Dynamical Systems"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e80b253b1b3c6b934e322ac1d5b75305737e3130.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel acceleration and interpretation framework for Continuous Normalizing Flows (CNFs) using Koopman operator theory. By lifting nonlinear CNF dynamics into a linear latent space, the model enables one-step sampling via a matrix exponential and offers spectral interpretability through Koopman eigenmodes. A simulation-free consistency loss ensures fidelity to the teacher CNF’s vector field."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents an algebraic perspective on generative flows by applying Koopman operator theory to Continuous Normalizing Flows (CNFs).\n\n2. The Koopman eigenmodes uncover semantically meaningful latent directions, enabling controllable and interpretable manipulations of generated samples.\n\n3. The proposed method replaces multi-step ODE integration with a single matrix exponential, yielding significant speedups while preserving sample quality."}, "weaknesses": {"value": "1. The experimental evaluation includes limited baselines, making it difficult to fully assess the significance and comparative advantages of the proposed method. The results in Table 1 do not clearly highlight its superiority over existing approaches.\n\n2. Computing large matrix exponentials may become computationally expensive or numerically unstable when applied to high-dimensional or high-resolution data.\n\n3. The method depends on a pre-trained CNF as a teacher, resulting in a two-stage distillation process rather than a fully end-to-end training framework."}, "questions": {"value": "1. The training objective involves four loss coefficients, but the paper does not clearly explain how these terms are balanced or tuned during optimization.\n\n2. The Koopman model without the consistency loss reportedly achieves better performance than the version with it. This raises the question of the practical role and necessity of the consistency loss in the overall framework.\n\n3. Since the matrix exponential is defined as an infinite series, it is unclear how it is approximated in practice. Moreover, for large matrices \nL, computing the exponential can be computationally expensive. How to reduce this cost or improve numerical efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cmmLgE4r8v", "forum": "vtgjVWvWgO", "replyto": "vtgjVWvWgO", "signatures": ["ICLR.cc/2026/Conference/Submission14319/Reviewer_mSkg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14319/Reviewer_mSkg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396407311, "cdate": 1761396407311, "tmdate": 1762924753991, "mdate": 1762924753991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Koopman-operator view of Conditional Flow Matching (CFM). It proposes to distill an encoder, a decoder and a generator matrix from an existing CFM teacher to connect state space to the Koopman representation so that the non-linear CFM dynamics become linear and thus analytically solvable in one step by a matrix exponential. A key technical piece is a simulation-free infinitesimal consistency loss that aligns the learned observables with the teacher vector field along the entire path. Empirically, on MNIST, CIFAR-10, and 32×32 FFHQ with an OT-CFM teacher, the one-step sampler yields competitive FIDs and large speedups, and exposes interpretable spectral identities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Distilling CFM dynamics into a globally linear dynamics enabling 1-step analytical sampling and spectral analysis is interesting. The time-augmentation + affine-lift construction is clean. Has wall-clock improvements compared to multi-NFE CFM. \n- Ablation showing that removing the consistency term can improve FID but harms trajectory fidelity. This justifies the method’s design. \n- The paper is written clearly and describes the necessary prerequisits for readers."}, "weaknesses": {"value": "- The paper itself notes challenges scaling to high-resolution images. Exponentiating a large dense matrix can be costly and numerically delicate. Results stop at 32×32.\n- Baselines focus on OT-CFM and Consistency Flow Matching. Missing broader experiment settings on other variants of CFM models or comparisons to SOTA few/one-step approaches.\n- Paper claims a key contribution of interpretability by exposing the Koopman dynamics. Two methods of interpretation are discussed: Dirac delta perturbations and spectral decomposition. However, it's not intuitively clear how such interpretations could shed more insight into flow matching training. Also not clear about the implications of image effects after perturbation in Figure 3."}, "questions": {"value": "1. Beyond figures in the paper, can authors make the interpretability claim more rigorous or discuss more about potential implication of such interpretation on training? \n2. How does performance vary with teacher quality? If the teacher is changed, does the learned $L$ differ qualitatively in interpretations (e.g., spectrum)? \n3. Do eigen-modes learned on one dataset transfer to others? Any mode-sharing across classes in CIFAR-10?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4p62EAVm93", "forum": "vtgjVWvWgO", "replyto": "vtgjVWvWgO", "signatures": ["ICLR.cc/2026/Conference/Submission14319/Reviewer_5DZh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14319/Reviewer_5DZh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560390265, "cdate": 1761560390265, "tmdate": 1762924753541, "mdate": 1762924753541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method motivated by Koopman theory to design a new training framework that learns an invertible map to lift data into Koopman space, and then learns a linear operator to perform flow in this space. This approach enables faster generation while providing interpretability of the learned transformations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a common challenge in flow-based models: achieving faster generation while maintaining interpretability. \n- The proposed method is well-motivated by solid mathematical theory, grounding the approach in a principled framework. Additionally, leveraging Koopman theory offers a promising avenue for combining efficiency with interpretability, which is a key strength of this work."}, "weaknesses": {"value": "- While the proposed methods are well-motivated, the numerical results seem somewhat limited. The experiments only compare against OT-CFM and Consistency FM, without including many state-of-the-art one-step models such as CTM or MeanFlow. Including these comparisons would better demonstrate the practical advantages of the proposed approach.\n\n- The learned objectives in the proposed method appear quite complex and may be difficult to train in practice. Could the authors provide additional insights or guidance on training stability and implementation challenges?\n\n- Overall, I think this is an interesting paper and provides a promising framework to accelerate generative modeling. However, as mentioned in the conclusion, the current method does not perform well in a complex dataset. This definitely limits its applicability right now."}, "questions": {"value": "I listed some questions directly in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XtjxONW6aY", "forum": "vtgjVWvWgO", "replyto": "vtgjVWvWgO", "signatures": ["ICLR.cc/2026/Conference/Submission14319/Reviewer_12ML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14319/Reviewer_12ML"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982095507, "cdate": 1761982095507, "tmdate": 1762924753110, "mdate": 1762924753110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes learning a finite-dimensional Koopman linearization for a pre-trained CFM teacher: an encoder/decoder map data Koopman space, and a learned linear generator yields one-step sampling. The key idea is to develope a smulation-free “infinitesimal consistency” loss that aligns the linear dynamics with the teacher’s vector field along the entire path, not just endpoints. The model yeilds competitive FID with 1-step sampling and spectral interpretability of modes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem framing & novelty in training objective. The marginal consistency estimator (Prop. 3) keeps training simulation-free while targeting the true marginal loss, not the conditional surrogate.\n\n2. The interpretability is good. Eigen-analysis and canonical directions provide editable semantics, for example sunglasses in FFHQ.\n\n3. Solid ablation study shows that trajectory fidelity markedly better with consistency loss vs pure boundary distillation."}, "weaknesses": {"value": "1. Many comparisons focus on a distilled ablation and OT-CFM; broader few-step one-step baselines (e.g., strong consistency/rectified variants) and compute-matched timings would strengthen claims. \n\n2. Image domains are small/medium (MNIST, CIFAR-10, 32×32 FFHQ). Add at least a latent-ImageNet or class-conditional setup to test generality."}, "questions": {"value": "1. Does the training on use any stop gradient? How about the computational overhead compared to the simulation-free flow matching, meanflow, consistency flow matching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ghgB69qfvX", "forum": "vtgjVWvWgO", "replyto": "vtgjVWvWgO", "signatures": ["ICLR.cc/2026/Conference/Submission14319/Reviewer_EtF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14319/Reviewer_EtF6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14319/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982912169, "cdate": 1761982912169, "tmdate": 1762924752691, "mdate": 1762924752691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}