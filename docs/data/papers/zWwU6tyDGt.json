{"id": "zWwU6tyDGt", "number": 22765, "cdate": 1758335198401, "mdate": 1759896847919, "content": {"title": "Evolution-Aware Positive-Unlabeled Learning for Protein Design", "abstract": "We consider prediction of protein function, focusing on protein functionalities that enhance survival for one or more organisms. Sequencing these organisms provides plentiful positive training examples. In contrast, synthesizing and characterizing a protein with a mutation unseen in nature requires time-consuming wet lab experiments, making negative training examples scarce. Thus, datasets are often imbalanced, hindering classifier accuracy outside the training data.  Positive-unlabeled (PU) learning attempts to address this issue by considering unlabeled protein sequences to be part of the data and modeling them as positive with a probability called the class prior. This class prior is often constant. Our insight is that an understanding of evolution suggests a novel sequence-dependent class prior when learning from sequencing data. We propose Evo-PU, a PU learning framework that integrates our novel class prior to create a likelihood for training classifiers. We evaluate Evo-PU on multiple real-world tasks: identifying immune-evasive viral epitopes, predicting human receptor binding peptides and classifying peptide with membrane fusion capability. Using influenza genomic surveillance data and held-out laboratory assays of mutants unseen in nature, Evo-PU outperforms state-of-the-art PU learning, one-class classification, and evolutionary conservation-based methods, demonstrating the benefit of combining evolutionary modeling with data-driven learning for protein design.", "tldr": "", "keywords": ["Positive-Unlabeled Learning", "Protein Design", "One-class classification"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1f8683a640dedf7d87299654f4b464f5879455e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents EVO-PU, a positive-unlabeled learning framework for classifying favorable versus disruptive protein variants. The problem it addresses is important but not highlighted enough at machine learning conferences, and the proposed method is novel and interesting. Nevertheless, the work is disconnected from modern machine learning approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Data scarcity and the lack of negative data are common challenges in machine learning for biology. This work directly addresses these issues.\n- The proposed EVO-PU method is well-explained, novel, and well-justified.\n- The results show that EVO-PU outperforms other positive-unlabeled learning and one-class classification approaches."}, "weaknesses": {"value": "The paper's main weakness is its lack of connection to modern machine learning approaches.\n- The evaluation is performed on three custom-built datasets based on the Influenza virus. The paper lacks an evaluation on standard benchmarks, such as the well-established ProteinGym [1]. If none of standard datasets used in prior work is suitable, this should be explained.\n- The baselines consist mostly of traditional machine learning algorithms. The paper lacks a comparison with modern machine learning models, such as AlphaMissense [2] or even simple ESM2 [3] log-likelihoods [4].\n- EVO-PU uses the Wide and Deep (WD) network architecture from 2016 with a simple featurization of protein sequences. The paper would benefit from using a more modern, for example Transformer-based architecture [3]. This would address the issue mentioned on Line 345 (“Directly optimizing the loss in Eq. 7 over discrete amino acid sequences is intractable”), as discrete amino acid sequences can be represented as continuous features [3].\n\n[1] Notin et al., 2023, “ProteinGym: Large-Scale Benchmarks for Protein Design and Fitness Prediction” https://www.biorxiv.org/content/10.1101/2023.12.07.570727v1\n\n[2] Cheng et al., 2023, “Accurate proteome-wide missense variant effect prediction with AlphaMissense” https://www.science.org/doi/10.1126/science.adg7492\n\n[3] Lin et al., 2023, “Evolutionary-scale prediction of atomic-level protein structure with a language model” https://www.science.org/doi/10.1126/science.ade2574\n\n[4] Meier et al, 2021, “Language models enable zero-shot prediction of the effects of mutations on protein function” https://www.biorxiv.org/content/10.1101/2021.07.09.450648v1.full"}, "questions": {"value": "1. What is the sequence distance between the training and test data for each of the three datasets?\n2. The abstract states, “We consider prediction of protein function, focusing on protein functionalities that enhance survival for one or more organisms.” This suggests that identifying favorable variants is the primary application. If so, would other retrieval-based metrics that account for class imbalance, such as precision and recall, be more informative than the standard AUROC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wTu6IYh1qb", "forum": "zWwU6tyDGt", "replyto": "zWwU6tyDGt", "signatures": ["ICLR.cc/2026/Conference/Submission22765/Reviewer_8tkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22765/Reviewer_8tkS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714689747, "cdate": 1760714689747, "tmdate": 1762942376967, "mdate": 1762942376967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors are interested in predicting the effects of mutations from observed sequences from evolution. They cite a biophysical model, suggest a modification based on the choice of negatives and infer models based on this model. They Show their model better predict the effects the effects of mutations from an influenza assay."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The authors suggest a modification to a biophysical model\n* They some some (inconsistent) improvement on an assay."}, "weaknesses": {"value": "The main weakness of this work is that it does not properly interface with the mainstream methods for learning evolutionary conserved information from protein sequences, that is, generative models trained on sequences seen across life.\n* They only cite EVE and EVEscape. What about ESM, Progen, etc...?\n* They claim \"While generative models effectively capture conserved constraints, they focus on single-point mutations and often struggle to predict activity for sequences with a few mutations from known positives\" without citation. On the contrary, according to ProteinGym, these models are also state of the art for multiple mutations as well.\n* They only compare to EVE rather than state-of-the art models that do much better on viral sequence inference.\n\nAs well, modern models are evaluated on ProteinGym, which contains hundreds of assays each with thousands of measurements. In contrast, the authors only evaluate their model on a single assay with less than 50 measurements."}, "questions": {"value": "* Why did you train EVE on prevalence data rather than evolutionary data from across life, say by running an alignment. This is another confusion of mine -- in principle your model can be used to perform inference \n* \"However, since the true wild types for our test sequences are unknown, we instead calculate this index for each test sequence against the top 20 most frequently observed sequences in Dn and select the minimum index. These minimum indices are then modeled with a two-component Gaussian mixture model (GMM) used to predict the probability that a test sequence possesses the property of interest.\" Why is this a fair choice?\n* Much simpler biophysical models have been used to justify mainstream generative models [example](https://proceedings.neurips.cc/paper_files/paper/2022/file/247e592848391fe01f153f179c595090-Paper-Conference.pdf). Could you compare your theory to these?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lhPQmYQDn9", "forum": "zWwU6tyDGt", "replyto": "zWwU6tyDGt", "signatures": ["ICLR.cc/2026/Conference/Submission22765/Reviewer_JQFg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22765/Reviewer_JQFg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761051062148, "cdate": 1761051062148, "tmdate": 1762942376670, "mdate": 1762942376670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Evo-PU, designed for protein binary function prediction where only positive examples are available. The core methodological novelty is the introduction of a sequence-dependent, \"evolution-aware\" class prior. This evolutionary prior is integrated into a custom likelihood function to train a classifier. The authors evaluate Evo-PU on three tasks related to influenza virus proteins (fusion, binding, and evasion), comparing it against other PU learning methods, one-class classifiers, and an evolution-based generative model (EVE). They report state-of-the-art performance, which they attribute to their more biologically realistic learning framework."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper's core assumption that the sampling bias in natural sequence data is not uniform and can be modeled by evolutionary preference is a strong and valid argument. \n\n2. Strong Ablation (E-GEN vs. RAND): The comparison of baselines using both E-GEN and RAND unlabeled data is a strong control experiment. It successfully suggests that the performance gain of Evo-PU is likely attributable to its unique loss function, not just the quality of the generated unlabeled sequences."}, "weaknesses": {"value": "1. Critically Incomplete and Potentially Misleading Baselines: The experimental evaluation is fundamentally flawed and ignore the most basic and powerful baselines.\n\n      a. The paper fails to compare against standard similarity-based methods, such as a simple k-NN classifier on either BLAST scores, ESM-2 embeddings, or FoldSeek/SaProt structural alignments. These are the fast, robust, and established first-line approaches for function prediction. \n\n     b. The comparison against EVE is conceptually questionable. EVE is a generative model rather than classification model. Furthermore, the reported AUC scores of < 0.5 is flawed. \n\n\n2. Novelty is Limited and Overstated: The paper's novelty hinges on its \"evolution-aware\" component. However, this is philosophically very similar to the direct use of scores from pre-trained protein language models (like ESM, AlphaMisense, EVE) to estimate a sequence's plausibility. While the authors integrate this concept into the PU framework in a novel way, the underlying idea is not new. The work feels like a clever recombination of existing ideas (PU learning + evolutionary models) without biological applications and working scenarios. \n\n3. Lack of Connection to Practical Utility: It never demonstrates that its improved AUC score on this specific binary task leads to any tangible downstream benefit. A successful paper would show that its model, for example, can be used to propose a novel immune-evasive viral epitope that is later validated, or that its functional predictions correlate well with clinically relevant outcomes like disease severity. As it stands, the work is an isolated academic exercise in improving a specific metric."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CW6pZ4A9yO", "forum": "zWwU6tyDGt", "replyto": "zWwU6tyDGt", "signatures": ["ICLR.cc/2026/Conference/Submission22765/Reviewer_HAMc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22765/Reviewer_HAMc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957547230, "cdate": 1761957547230, "tmdate": 1762942376345, "mdate": 1762942376345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Evo-PU, a positive-unlabeled learning framework for detecting functional protein sequences. Evo-PU introduces a sequence-dependent class prior derived from a probabilistic model of evolutionary emergence at the nucleotide level. The model estimates A(x), the probability that a protein sequence is functional, effectively serving as a fitness predictor. Empirically, Evo-PU achieves superior AUC scores compared to Protein-PU and other baselines on three influenza case studies (fusion, binding, and immune evasion)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a new and well-developed theoretical framework with clear definitions and explanations.  \n- The proposed method is shown to outperform directly comparable approaches such as Protein-PU.  \n- The paper is well written and logically organized."}, "weaknesses": {"value": "- Missing comparison to zero-shot protein language model-based fitness (i.e., A(x)) predictors. The paper does not compare Evo-PU to zero-shot fitness predictors derived from protein language models, which have become standard in the field (for example, the ESM-1v paper https://www.biorxiv.org/content/10.1101/2021.07.09.450648v2 or more recent methods for example from the ProteinGym benchmark https://proteingym.org/benchmarks). A comparison on the same influenza benchmark would clarify whether Evo-PU captures additional biological signal beyond what these models already encode implicitly. Without such a comparison, it is difficult to position Evo-PU relative to current state-of-the-art fitness predictors.  \n\n- Limited evaluation. The paper evaluates Evo-PU only on a single custom influenza dataset. While this benchmark is carefully constructed and biologically relevant, it has not been used in previous studies. Evaluating Evo-PU on a well-established dataset such as ProteinGym would enable direct comparison to standard baselines and better demonstrate the method’s generality."}, "questions": {"value": "- Could the authors comment on the computational efficiency of Evo-PU? If my understanding is correct, the model must be retrained for each protein sequence, and the computational complexity scales exponentially with sequence length through D_n. \n\n- The theoretical framework operating with both nucleotide and amino acid sequences does not seem to be well justified but introduces a substantial complexity. Would not it be enough to work only on the level of protein sequneces? For example, observability directly operates with protein sequneces in Section 2.2 but is defined on the level of nucleotide sequences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2NfwqDLQx", "forum": "zWwU6tyDGt", "replyto": "zWwU6tyDGt", "signatures": ["ICLR.cc/2026/Conference/Submission22765/Reviewer_GfTM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22765/Reviewer_GfTM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996144554, "cdate": 1761996144554, "tmdate": 1762942376080, "mdate": 1762942376080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}