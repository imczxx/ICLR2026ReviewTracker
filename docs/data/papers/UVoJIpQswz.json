{"id": "UVoJIpQswz", "number": 18999, "cdate": 1758292627772, "mdate": 1759897068232, "content": {"title": "SLED: Self-Supervised Dataset Distillation for Lightweight Experience Replay", "abstract": "Experience Replay (ER) is central to off-policy reinforcement learning, but its reliance on massive buffers creates prohibitive storage and sampling costs. We introduce SLED, a self-supervised dataset distillation framework that replaces conventional replay with a compact, learnable synthetic dataset. SLED progressively shifts the agent’s training from real interactions to a small, self-evolving knowledge base by decoupling data writing from training sampling. On the writing side, a temporal schedule gradually substitutes real trajectories with optimized synthetic samples, leaving the buffer composed solely of distilled data. On the sampling side, a quota-based strategy shapes the training distribution, enabling a seamless transition from real-dominated to synthetic-dominated updates without altering the base algorithm. To preserve the long-term utility of synthetic data, SLED adopts an online-validated evolutionary optimization scheme: candidate synthetic datasets undergo brief parallel training trials, followed by real-environment evaluation, yielding a dataset-level fitness signal that guides their continual refinement. The framework is plug-and-play with mainstream off-policy methods. Overall, SLED systematically extends the idea of dataset distillation to the non-stationary regime of reinforcement learning, providing a practical alternative to large-scale replay buffers. Extensive experiments on DMControl, Habitat, and Atari confirm that SLED delivers superior efficiency and scalability over existing ER approaches, demonstrating its broad effectiveness across diverse domains.", "tldr": "", "keywords": ["Dataset Distillation", "Reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9dc3640b0ca92314139a07b5b18febf010e8f2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SLED (self-supervised lightweight experience distillation), a framework that distills online experience into a small learnable synthetic set for off-policy RL. The synthetic dataset is optimized using evaluations after brief L-step training trials and, to balance with real data, is stored and sampled via decoupled schedules. On DMControl, Atari, and visual navigation in Habitat, the method reports strong sample efficiency and high final performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated and presents a plug-and-play method that achieves strong empirical performance across DMControl, Atari, and Habitat."}, "weaknesses": {"value": "Learning the synthetic set requires training trials and return evaluations, which introduce additional overhead. While the paper argues that trial overhead is small when $R_{\\text{ES}}=\\tfrac{PL}{K}$ is small, clarifying the practical range of $R_{\\text{ES}}$ needed to achieve the reported gains would be helpful. The $\\alpha(t)$ and $\\beta(t)$ schedules appear to be heuristic, and the paper provides limited analysis on how to choose them."}, "questions": {"value": "* How is the learner evaluated after each training trial—does this require additional environment interaction? If so, for a fair comparison, please clarify whether those steps are included in the reported sample efficiency.\n* What are the exact forms and values used for the shedules $\\alpha(t)$ and $\\beta(t)$? What values of $P$ and $L$ are used? How sensitive are the results to these hyperparameters?\n* A brief comparison to ES-based RL methods (e.g., [1]) would be helpful.\n\n[1] Salimans et al., \"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\", 2017"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2JJ81yvvh", "forum": "UVoJIpQswz", "replyto": "UVoJIpQswz", "signatures": ["ICLR.cc/2026/Conference/Submission18999/Reviewer_Gqi8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18999/Reviewer_Gqi8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821716176, "cdate": 1761821716176, "tmdate": 1762931051215, "mdate": 1762931051215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SLED, a novel self-supervised dataset distillation approach to improve the memory and computational efficiency of traditional replay buffers. The framework is compatible with many existing codebases and demonstrates strong empirical performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The algorithmic design is novel. SLED combines a synthetic dataset with real experiences, and optimizes the synthetic dataset using zeroth-order updates to improve performance while avoiding the challenges of a non-differentiable target. Furthermore, it gradually replaces real experiences with synthetic data to ensure training stability. The algorithm consistently outperforms existing baselines and represents a timely contribution to the community."}, "weaknesses": {"value": "To optimize the synthetic dataset, SLED requires additional parallel training trials per update in order to perform zeroth-order optimization. This introduces extra computational cost and may not scale well to large-scale tasks."}, "questions": {"value": "The paper emphasizes the importance of decoupling the write schedule $\\beta(t)$ from the sampling schedule $\\alpha(t)$, showing that coupling them degrades performance. However, beyond this binary ablation, there is little analysis on the choice of functional forms for $\\alpha(t)$ and $\\beta(t)$. For example, one might imagine linear decay, or exponential decay. Could the authors comment on how sensitive SLED is to the exact shapes of $\\alpha(t)$ and $\\beta(t)$? Have the authors tried alternative schedules, or is the method robust as long as both are monotone decreasing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qiA8wYGiWD", "forum": "UVoJIpQswz", "replyto": "UVoJIpQswz", "signatures": ["ICLR.cc/2026/Conference/Submission18999/Reviewer_FcMd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18999/Reviewer_FcMd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862718252, "cdate": 1761862718252, "tmdate": 1762931050657, "mdate": 1762931050657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SLED (Self-supervised Dataset Distillation for Lightweight Experience Replay), a framework designed to replace the large buffers central to conventional off-policy reinforcement learning (RL) with a compact, learnable synthetic dataset, thereby reducing substantial storage and sampling costs. SLED systematically extends dataset distillation to the non-stationary regime inherent to RL. The core operation relies on a decoupled dual-scheduling mechanism that separates the buffer's physical composition from the agent’s training distribution: a temporal schedule gradually substitutes real experiences with synthetic ones until the buffer stores only distilled data, while a separate quota-based sampling strategy governs the ratio of real-to-synthetic transitions in training batches, ensuring a smooth curriculum shift for the learner without changing the base algorithm's update rules. To maintain the long-term utility of the small synthetic dataset despite non-stationarity, SLED uses an online-validated evolutionary optimization strategy. The framework is plug-and-play and experiments across DMControl, Habitat, and Atari confirm that SLED delivers superior efficiency and scalability, consistently matching or surpassing baselines like Prioritized Experience Replay while drastically reducing the required memory footprint and maintaining negligible wall-clock overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and presents its ideas in a clear and organized manner. Each component of the proposed framework is thoroughly explained, with logical flow and sufficient detail to make the methodology understandable. The authors provide clear definitions and diagrams where necessary, which helps readers grasp both the high-level concepts and the finer technical aspects. Overall, the clarity and structure of the writing make the paper accessible and easy to follow."}, "weaknesses": {"value": "One notable weakness of the paper is that it does not sufficiently discuss more recent works related to dataset distillation in RL paradigms, such as [1][2]. Incorporating these references would provide a more comprehensive view of the current state of the field. \n\nAdditionally, the paper lacks sufficient experimental comparisons between the proposed method and existing distillation approaches within RL, making it difficult to assess the relative advantages or limitations of the proposed approach in the context of prior work.\n\n\n[1] Dataset Distillation for Offline Reinforcement Learning, 2024\n\n[2] Offline Behavior Distillation, 2024"}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VThhkxeSu3", "forum": "UVoJIpQswz", "replyto": "UVoJIpQswz", "signatures": ["ICLR.cc/2026/Conference/Submission18999/Reviewer_Tvhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18999/Reviewer_Tvhn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981105851, "cdate": 1761981105851, "tmdate": 1762931049976, "mdate": 1762931049976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an approach to reducing the replay buffer size of deep RL agents by creating and maintaining a synthetic dataset. The authors argue big uniform buffers, commonly used today, require large memory and compute resources, while naively retaining many low value transitions. They propose instead, to maintain a small set of synthetic transitions, optimized to improve the performance of the RL agent and slowly move the data distribution from real transitions to synthetic ones.\n\nThe idea is to maintain a small, first in first out, buffer of transitions alongside a synthetic dataset. Two time-dependent probabilities gradually shift the distribution of transitions in mini-batches from real to synthetic. To improve the synthetic dataset, an evolutionary strategy periodically tries perturbations of a generator’s parameters before training on the new dataset for evaluation and fitness score calculation.\n\nThe authors claim this approach can seamlessly be integrated into off-policy RL algorithms. They demonstrate combining the proposed algorithm with DQN on some Atari games and SAC on some DM-Control and Habitat domains."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- In my opinion this paper explores an interesting and relevant problem for the RL community. While I do not agree that large replay buffers incur compute costs, their memory footprint is prohibitive to be used in small agents or edge devices. Furthermore, exploring experience replay mechanisms beyond uniform sampling from a large recency buffer is an important research direction toward developing data and compute efficient RL algorithms.\n- Some of the design choices, such as slowly shifting the data distribution from real to synthetic by adapting data ingress and sampling probabilities is interesting and should be noted as such.\n- I think the paper’s exploration of evolutionary strategies in RL agents is novel and worth highlighting, even though its description is vague and missing details.\n- While there are many issues with the experiments, I appreciated that the authors try several domains and show their approach can be combined with both value based and actor-critic algorithms."}, "weaknesses": {"value": "- Several aspects of the approach are not discussed in the text:\n  - The synthetic dataset is updated via some parameterized generator but the form of this generator and the process to generate new samples is not described.\n  - The process for controlling the data ingress and sampling probabilities is confusing and unclear. It is unclear why mini-batches are sampled from both the replay buffer and the synthetic dataset if the replay buffer is populated with the synthetic data overtime anyway.\n  - The fitness score for optimizing the synthetic data is confusing. Equation (9) seems to suggest two stages for the evaluation: TrialReturn and also Return. This is not explained in the text.\n- The writing’s tone and language in some parts is not appropriate for a scientific article:\n  - The authors, on several occasions, use overly flowery and confusing terms that fails to effectively communicate knowledge or describe what was done.\n  - The citation format is incorrect. If a citation is a part of the sentence, it should be printed without parentheses, but if it is not part of the sentence, it should be enclosed in parentheses.\n  - Algorithm 1 on page 6 seems incomplete with several missing steps.\n  - Lines 324 to 340 are very unclear. I do not understand what the authors want to say here.\n- The experiment section has many issues that undermines the claims regarding superior performance of the proposed algorithm:\n  - The choice of hyperparameters is neither described nor justified.\n  - Some results do not have any measure of uncertainty or confidence.\n  - When results do provide a range or shaded region, their meaning is neither defined nor justified.\n  - The choice of Atari games and the evaluation scheme is not described.\n  - The w/o ES ablation needs more justification, because I think if the synthetic dataset is never updated, then the initial warm-start real transitions are kept in the synthetic dataset for the entire experiment length? This is not a reasonable baseline for comparison. A better approach would have been to try simpler approaches to generate synthetic data."}, "questions": {"value": "1. What is the process for generating the synthetic data? What is parameterized by $\\phi$? I assume some kind of generative model but I am unsure.\n2. How/Why did you choose these Atari games for your experiments?\n3. I am unsure if Equation (9) has a bug or I misunderstood the fitness score used to adapt the synthetic data.\n4. Your experiments on Atari are run for 2 million steps. This is in contrast to the default 50 million steps (or 200 million frames) commonly used when evaluating RL agents on Atari games. During the first two million steps all algorithms being compared actually perform quite bad. Then why is it okay to compare RL algorithms in this setting?\n5. What are the statistical justifications for your claims?\n6. What are the error bars in the learning curves/tables?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jSFDsUJQNG", "forum": "UVoJIpQswz", "replyto": "UVoJIpQswz", "signatures": ["ICLR.cc/2026/Conference/Submission18999/Reviewer_ERXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18999/Reviewer_ERXd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762673038662, "cdate": 1762673038662, "tmdate": 1762931049451, "mdate": 1762931049451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}