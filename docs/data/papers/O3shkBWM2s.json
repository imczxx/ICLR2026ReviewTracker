{"id": "O3shkBWM2s", "number": 2483, "cdate": 1757114080170, "mdate": 1763760800655, "content": {"title": "Robust Optimization for Mitigating Reward Hacking with Correlated Proxies", "abstract": "Designing robust reinforcement learning (RL) agents in the presence of imperfect reward signals remains a core challenge. In practice, agents are often trained with proxy rewards that only approximate the true objective, leaving them vulnerable to reward hacking, where high proxy returns arise from unintended or exploitative behaviors. Recent work formalizes this issue using \nr-correlation between proxy and true rewards, but existing methods like occupancy-regularized policy optimization (ORPO) optimize against a fixed proxy and do not provide strong guarantees against broader classes of correlated proxies. In this work, we formulate reward hacking as a robust policy optimization problem over the space of all \nr-correlated proxy rewards. We derive a tractable max-min formulation, where the agent maximizes performance under the worst-case proxy consistent with the correlation constraint. We further show that when the reward is a linear function of known features, our approach can be adapted to incorporate this prior knowledge, yielding both improved policies and interpretable worst-case rewards. Experiments across several environments show that our algorithms consistently outperform ORPO in worst-case returns, and offer improved robustness and stability across different levels of proxy–true reward correlation. These results show that our approach provides both robustness and transparency in settings where reward design is inherently uncertain.", "tldr": "", "keywords": ["Reward hacking", "Robust Reinforcement Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/580cfd6df5f3f10628518b49955acc7131b6a44a.pdf", "supplementary_material": "/attachment/cdf91857376d6583cd3e6ab556de58708c6f7410.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to design reinforcement learning (RL) agents that remain robust to imperfect or proxy reward signals. The authors formulate reward hacking as a robust policy optimization problem over all proxy rewards that are $r$-correlated with the true reward, deriving a tractable max–min formulation. When rewards are linear in known features, the method incorporates this structure to yield more interpretable and robust policies. Experiments across several environments show improved worst-case performance compared to existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and clearly presented.\n\nIt addresses an important problem in reinforcement learning (RL) where the reward function is imperfect. There have also been recent papers on similar settings in language models, such as:\n- [1] https://arxiv.org/abs/2504.03784\n- [2] https://arxiv.org/abs/2405.11204\nConsidering the growing popularity of LLMs, it would be interesting to discuss the potential applications of this approach.\n\nThe proposed approach appears principled, and the theoretical results effectively support the empirical findings."}, "weaknesses": {"value": "- Equations (9) and (13) do not include the importance sampling ratio, whereas Section 3.3 does. I think this is because the chi-square divergence involves this ratio—if so, the authors could clarify this more explicitly. \n\n- The authors claim to have developed efficient algorithms, but the computational complexity of the proposed method is not discussed. Including empirical comparisons of runtime and memory usage against benchmarks would make the paper more complete. \n\n- How are the feature mappings selected? Are the theoretical/empirical results sensitive to the choice or number of mappings?\n\n- The experiments could be strengthened by comparing with recent related works, such as:\n  - InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling\n  - RRM: Robust Reward Model Training Mitigates Reward Hacking\n  - Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking"}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rOhd8W15vj", "forum": "O3shkBWM2s", "replyto": "O3shkBWM2s", "signatures": ["ICLR.cc/2026/Conference/Submission2483/Reviewer_46uB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2483/Reviewer_46uB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761474437381, "cdate": 1761474437381, "tmdate": 1762916251529, "mdate": 1762916251529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies robustness to reward misspecification by assuming access to a proxy reward $R_{\\text{proxy}}$ and a reference policy $\\pi_{\\text{ref}}$.\nIt defines an uncertainty set of true rewards whose Pearson correlation with the proxy reward under $\\pi_{\\text{ref}}$ equals $r$, and trains a policy to maximize worst-case return over that set.\nA change-of-measure via the Radon--Nikodym derivative (density ratio) was used, which results in a regularized optimization objective the authors note \"closely resembles\" occupancy-regularized policy optimization (ORPO) objective, differing by a regularization scaling and an extra penalty term inside the square root.\nConvergence guarantee of the proposed method is provided, and empirical evidence on four \"real-world inspired\" reward hacking environments is given."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper gives a clean derivation of a max-min robust RL objective under a correlation-constrained reward set and reduces it to a tractable form via duality and change of measure.\nThe resulting penalty can be read as an \"orthogonalized\" $\\chi^2$ term, penalizing the component of occupancy shift not aligned with proxy improvement, which is a neat geometric refinement relative to ORPO's regularization.\nThe implementation details for ratio estimation and the comparison to ORPO will be useful to practitioners."}, "weaknesses": {"value": "## Reward-space vs. behavior-space\n\nFirst, the core uncertainty model is reward-space, not behavior-space.\nCorrelation under $\\mu_{\\pi_{\\text{ref}}}$ ignores the well-known fact that many behaviorally equivalent rewards (e.g., potential-based shapings) can have arbitrary correlation with the proxy, while tiny changes in reward can flip the optimal policy near decision boundaries.\nAs a result, the method may penalize or ignore the wrong directions in practice; a behavior-centric formulation (e.g., safe improvement stated in terms of behaviorial metrics) would better match the problem that matters.\n\nThe paper repeatedly motivates \"guarding against reward hacking,\" yet provides no behavioral safe-improvement guarantee and even asserts that the correction term \"enforces robustness to potential reward hacking,\" which is not supported by a theorem.\nOver-pessimistic and over-optimistic rewards can both lead to suboptimal behaviors, and the agent may just learn a different reward hacking behavior.\n\n## Reference policy\n\nSecond, the entire construction is anchored to a reference policy.\nHow's it chosen? Does it need to be a good policy?\nThe correlation constraints are imposed under $\\mu_{\\pi_{\\text{ref}}}$, and are vacuous off its support; the adversary may set arbitrarily bad rewards in unvisited regions.\nConsequently, the guarantee weakens precisely where a newly learned policy might go.\nThe paper assumes $\\pi_{\\text{ref}}$ is given and does not offer a principled selection rule, beyond practical choices.\n\n## Correlation hyperparameter\n\nThird, $r$ is a hyperparameter that must be known or tuned. The authors state there is \"no principled method\" to pick it and resort to grid search over $r \\in \\\\{0.1,\\dots,0.9\\\\}$, which is unintuitive for a Pearson correlation meant to represent true-proxy alignment.\nResults also depend on the training/evaluation $r$ mismatch.\n\n## Succesor representation\n\nFinally, the linear-reward assumption $R(s,a)=\\theta^\\top\\phi(s,a)$ is long-established (**successor representations/features/measures**); the section omits direct attribution even though the subsequent optimization uses discounted feature expectations exactly as in that literature."}, "questions": {"value": "In its current form, the paper offers a tidy derivation and a small geometric refinement of ORPO, but it does not resolve the central mismatch between reward-space uncertainty and behavior-space objectives, and it relies on an unintuitive correlation hyperparameter and a reference-policy anchor.\nI encourage the authors to\n- clarify the relationship between reward function correlation under a reference policy and behavior difference;\n- clarify the implications of off-support vacuity and provide coverage-aware constraints; and\n- justify or estimate $r$ more principledly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oMNrBH5xdQ", "forum": "O3shkBWM2s", "replyto": "O3shkBWM2s", "signatures": ["ICLR.cc/2026/Conference/Submission2483/Reviewer_uNZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2483/Reviewer_uNZH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843217541, "cdate": 1761843217541, "tmdate": 1762916251298, "mdate": 1762916251298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Building off Laidlaw et al (2025)'s definition of \"r-correlated\" proxy reward functions, the authors introduce a robust optimization framework aimed at reducing reward hacking. Their method focuses on maximizing the minimum reward achievable across all possible r-correlated reward functions. This leads to a loss function that, while similar to that of Laidlaw et al, is distinct in its exact regularization term. Across several environments, they show that their approach achieves higher worst-case reward, compared to ORPO from Laidlaw et al, while typically maintaining similar expected reward."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has several strengths:\n- Intuitive conceptual contribution of optimizing worst-case reward over all reward functions that are r-correlated to the given proxy reward function. This seems like the natural progression from Laidlaw et al (2025).\n- Theoretically sound framework. They apply several tools from robust optimization to derive a tractable loss function, which though ultimately similar to Laidlaw et al (2025), differs in the exact regularization term, and they show this leads to improved worst-case rewards in their experiments\n- Improvement for linear reward setting. They are able to derive a refinement for the case where the reward function is known to be linear.\n- Better occupancy measure estimates lead to also improving ORPO. Both this work and Laidlaw et al (2025) use a discriminator network to measure the occupancy measure divergence between a given policy and the reference policy. The authors note that in the original ORPO implementation, the discriminator network is not fully optimized, and thus, the resultant policies still end up visiting states that are low frequency under the original policy. By training the discriminator for longer, they also obtain improved results compared to the original ORPO implementation.\n- Extensive experiments and ablations. The authors test their method on five environments that were designed for studying reward hacking, and include several additional results in the appendices (e.g. results with unnormalized rewards, results on robustness to \"r\", etc)"}, "weaknesses": {"value": "One paragraph that I thought was a bit strange was:\n> \"We adopted a similar approach used by ORPO (Laidlaw et al., 2025). For each environment, we first performed a grid search over several different values of r, and for each fixed r, we trained the policy using our algorithm. We then selected the rvalue that leads to the policy with the best expected worst-case return ... Notice that ORPO selects the optimal r that yields the best expected return under the true reward, which is infeasible in practice when the true reward is unknown during training. In contrast, our approach for choosing r is more practical.\"\n\nIt's true that the authors' approach is computable without the true reward unlike the process used in ORPO, but it's not much more practical, as without knowing the \"correct\" value of r, it's not clear which measure of the worst-case reward is most meaningful.  Therefore, I would hesitate to describe this approach as more \"practical\" than ORPO. Instead, it might be more helpful for the authors to directly reference the two practical methods for selecting r discussed in Appendix I.\n\nBy the way, the authors say they include results for all r that they considered in H.5, but then H5 only includes results for the Traffic and Tomato environments. Also strangely, they consider r \\in {0.3, 0.5, 0.9} for Traffic and r \\in {0.1, 0.4, 0.7, 0.9} for Tomato. Why do they not use the same grid for all environments? Also, it is somewhat strange that the grid for Traffic is not uniform either. Can the authors please include results for all environments?\n\nThe authors' process for selecting \"r\" also highlights one aspect of their framework that I found counter-intuitive. The set R_corr(r) as defined in Eq 4 includes all reward functions that are *exactly* r-correlated with the proxy. Apriori, I would have expected them to define this set as including all reward functions that are *at least* r-correlated with the proxy. Then, increasing r would monotonically increase the maximal worst-case reward over the functions in R_corr(r). As I understand it, with the current defn, this kind of monotonicity does not necessarily hold? And that is also why in the selection process for r that they use in the experiments (noted above), the highest r is not always picked (though, noise in the occupancy measure estimation and policy training process may also affect this empirically)? It seems odd that, under this framework, selecting r = 0.3 makes the policy robust to reward functions with exactly 0.3 correlation, but not to those with higher correlation, such as 0.6. Could the authors clarify this design choice?"}, "questions": {"value": "- Why is the same grid for r not used in all environments?\n- Why did the authors not define R_corr as all reward functions that are *at least* r-correlated to the proxy, rather than *exactly* r-correlated to the proxy?\n- How would one obtain the \"reference policy\" in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5MOCqALGbS", "forum": "O3shkBWM2s", "replyto": "O3shkBWM2s", "signatures": ["ICLR.cc/2026/Conference/Submission2483/Reviewer_CXL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2483/Reviewer_CXL3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129713170, "cdate": 1762129713170, "tmdate": 1762916251070, "mdate": 1762916251070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of reward hacking in reinforcement learning, where an agent may optimize for an imperfect proxy of the reward function which could lead the agent to diverge from the intended true objective. To tackle this, the authors take a robust approach to optimize for the feature weights of the true underlying reward from an uncertainty set if feature weights defined by $\\chi^2$-divergence. This allows them to model this as a max-min optimization problem over the proxy rewards which are constrained via correlation with the true rewards of the system, which the authors then extend to linear rewards. Ultimately, they prove convergence of their algorithm and sample complexity bounds on the occupancy estimation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is relevant and highly motivated by practical implementation. It builds upon the state of the art to improve upon interpretability and derive a tractable objective for the non-convex robust optimization problem. The paper is very clear and progresses fluidly.\n- A closed-form solution is proposed and derived for the worst-case reward feature vector of the adversary by transforming this vector into a whitened version of itself, $\\tilde{\\phi}$, such that the $Q$-function becomes the identity matrix.\n- From what I could determine, the proposed method is backed up by a strong theoretical analysis. Subsequently, the authors provided much empirical validation of this theory to verify their claims as well as sufficient detail to reproduce these experiments."}, "weaknesses": {"value": "- In practice the correlation between a proxy and rewards, $r$, is unknown. The authors briefly mention this in the appendix and use a grid search to find this. However, as the author's mention, there is not a principled method for selecting the optimal $r$ and thus it may not scale.\n- An assumption is made that the true rewards lie within the defined uncertainty set. In practice, this may not always be the case. The proposed reference policy may not provide sufficient coverage of the feature space which could lead to the problem."}, "questions": {"value": "How could this extend to other uncertainty sets?\n\nMinor things:\n- $\\gamma$ should be defined prior to equation 1 where you define the objects in the MDP tuple.\n- It would be nice to see one of the algorithms appear in the main text as well as the convergence bounds stated formally in section 3.3 to help highlight the contribution of this work.\n- Missing space between \"latent\" and \"(true)\" on line 1026."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kO7uIsye8q", "forum": "O3shkBWM2s", "replyto": "O3shkBWM2s", "signatures": ["ICLR.cc/2026/Conference/Submission2483/Reviewer_b81V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2483/Reviewer_b81V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762323998420, "cdate": 1762323998420, "tmdate": 1762916250912, "mdate": 1762916250912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}