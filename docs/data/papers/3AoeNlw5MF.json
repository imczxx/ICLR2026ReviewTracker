{"id": "3AoeNlw5MF", "number": 25569, "cdate": 1758369154009, "mdate": 1759896715075, "content": {"title": "D-MOE-EVAL: A Dynamic Mixture Of Experts Framework For Human-Aligned Nuanced Large Language Model Evaluation", "abstract": "The growing paradigm of using Large Language Models (LLMs) as evaluators, known as LLM-as-a-Judge, offers significant scalability for automated assessment. However, this approach struggles from certain limitations. The different architectures and training of LLMs, leads them to develop varied expertise, making any single monolithic agent prone to bias and limited in adaptability across different reasoning scenarios. This inherent bottleneck leads to measurement imbalance across evaluation criteria and an over-prioritization of narrow technical correctness at the expense of diverse human-centered dimensions. To address these challenges, this paper presents a scenario-aware multi-dimensional evaluation framework that operationalizes a Mixture-of-Experts (MoE) architecture. The framework features instance-level scenario classification, dynamically mapping inputs to the most appropriate evaluation context, with each scenario linked to its own tailored set of evaluation dimensions. The dimension experts are specialized LLMs, dynamically selected after validation on a multi-dimensional dataset to systematically profile and identify their strengths across specified dimensions. This adaptive routing ensures that each instance receives a contextually relevant assessment across multiple complementary dimensions simultaneously. The expert evaluations are synthesized by a \"Panel of Judges\" as a deliberation layer, with multiple agents in structured debate to reconcile discrepancies and ensure fairness and logical consistency in the final judgments. The results of this study, evaluated over the MDEval and LLMBar benchmarks, demonstrate proposed framework’s superior performance on existing baselines across diverse tasks, showcasing the robustness, versatility, and generalizability of a Mixture-of-Experts approach for context-aware LLM evaluation.", "tldr": "This paper proposes a scenario-aware, multi-dimensional LLM evaluation framework using a MoE approach, across multiple domains and profiling dimension-specific experts, deliberating through a Panel of Judges ensuring human-aligned nuanced evaluation.", "keywords": ["Large Language Models", "Fine Grained Evaluation", "Multi-Dimensional Evaluation", "Mixture of Experts", "Scenario Aware Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dce8069e973deb65a4b5cf1276bbc2a5d5938dac.pdf", "supplementary_material": "/attachment/271e70f1decb97e57b9ae42d23e1999a6d9da4e7.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a scenario-aware, multi-dimensional evaluation framework that operationalizes a Mixture-of-Experts (MoE) architecture. For each scenario, it associates a tailored set of evaluation dimensions and, on each dimension, routes to the best-performing LLM as the dimension expert. In addition, a Panel of Judges aggregates and deliberates over the expert evaluations. Experiments on the MDEval and LLMBar benchmarks show that the framework outperforms existing baselines across diverse tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Leverages a Mixture-of-Experts (MoE) architecture to deliver nuanced, multi-dimensional evaluation of LLM outputs; the evaluation pipeline ensures the selected expert is validated/authoritative for the current dimension.\n\n2. The Panel of Judges, comprising a General Judge and a Critic Judge, helps surface errors or biases in the judgments and improves the overall robustness of the system."}, "weaknesses": {"value": "**Main concerns**\n\n1. Cost and latency analysis. It is important to add an analysis of time and monetary cost to demonstrate that the proposed pipeline offers better cost-effectiveness than a single judge. The D-MoE-Eval framework invokes multiple LLM components (e.g., Dimension Selector, Dimension Experts, Jury Panel). It is therefore necessary to quantify the additional runtime and cost introduced by these modules.\n\n2. Comparison with LLM Juries (Cohere et al. [1]). The related work mentions the concept of “LLM Juries” by Cohere et al., but it is not included as a baseline. A direct performance comparison between Cohere’s approach and the method in this paper is warranted.\n\n3. Fairness of model size in candidate profiling vs. baselines. Appendix A.2 shows that candidate profiling includes mostly large models (e.g., GLM-4.5-Air, 106B; Qwen-2.5, 72B; Kimi K2, 1T; Llama 3.3, 7B), whereas Tables 1 and 2 compare against 7B/8B/13B methods. Is this an unfair comparison?\n\n**Minor issues**\n\n1. Throughout the paper, the opening and closing quotation marks/apostrophes are the same glyph.\n2. Missing space at line 165.\n\n\n[1] Replacing judges with juries: Evaluating LLM generations with a panel of diverse models."}, "questions": {"value": "1. For the two-member Jury Panel, the paper states that “from our baseline comparisons, performs counterfactual checks.” How is this implemented in practice, and which models are used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tw9G5BIm1t", "forum": "3AoeNlw5MF", "replyto": "3AoeNlw5MF", "signatures": ["ICLR.cc/2026/Conference/Submission25569/Reviewer_2tsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25569/Reviewer_2tsy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749843555, "cdate": 1761749843555, "tmdate": 1762943478877, "mdate": 1762943478877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a systematic design of multi-stage evaluation pipeline for LLM-as-a-judge. This MoE structure involves several routing and scoring components, including a scenario selector, a dimension selector(which identifies the corresponding evaluation criteria), a parallelized router which dispatches evaluation tasks, a score aggregation module and a final evaluation-review module called Jury Panel. Experiments are conducted on two benchmarks to demonstrate the effectiveness of the MoE-structured evaluation system and its superiority over the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 The overall design of the MoE structure is technically sound. The arrangement of the components is logical and clear. This makes a strong motivation for leveraging the advantages of MoE system for LLM evaluation.\n\n2 Through this deliberate MoE structure design, it is conceptually solid to conclude that final aggregated evaluation results can be more convincing than single LLM Judges. \n\n3 The paper is well-organized and the writing is easy to understand."}, "weaknesses": {"value": "1 The nature of MoE structure means that it can be more expensive when using multiple LLMs for a single evaluation task. The cost depends on how many LLM are used.\n\n2 Though the MoE structure design is clearly presented in the paper, I find less details are given in the experiment sections (Section 4 and 5). For example, the training/tuning details of the scenario classifier is missing, and how the score aggregation/adjustment procedures are conducted is unclear.\n\n3 The scenario and dimension concepts in the MoE Classifier/Selector components seem to only fit the settings of MD-EVAL, where 36 scenarios and 42 criterion dimensions are properly defined. In LLMBar, however, only a \"subset\" is labelled as the dataset attribute. It is unknown how the Classifier/Selector components apply to the settings of LLMBar."}, "questions": {"value": "1 Figure 3 of the appendix lists all the winning models from the candidate profiling phase. If they are all used as the dimension experts, is it possible to estimate the total cost of the MoE evaluation pipeline? \n\n2 There are multiple settings for the w_j in eq(3) (average or weighted) and adjust(*, *) (averaging or human-review). What is the actual setting in the experiments?\n\n3 The authors should give a description that how the settings of scenario and dimension in MD-EVAL are transferred to LLMBar.\n\n4 Minor issue: why are the results of GPT-4o missing in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WguSlSQW9f", "forum": "3AoeNlw5MF", "replyto": "3AoeNlw5MF", "signatures": ["ICLR.cc/2026/Conference/Submission25569/Reviewer_k8ZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25569/Reviewer_k8ZS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880974519, "cdate": 1761880974519, "tmdate": 1762943478640, "mdate": 1762943478640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes\nD-MoE-Eval\n, a dynamic Mixture-of-Experts (MoE) framework to address the limitations of monolithic \"LLM-as-a-Judge\" paradigms."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "MoE Adaptation for Evaluation\n:\nPioneers applying the Mixture-of-Experts (MoE) paradigm to LLM evaluation, decomposing the task into 42 dimension-specific subtasks. This addresses monolithic \"LLM-as-a-Judge\" limitations by leveraging specialized expertise of diverse LLMs.\n\nData-Driven Expert Profiling\n:\nEstablishes a rigorous static routing map via quantifying candidate LLMs' agreement with human annotations for each dimension, ensuring transparent and reproducible expert selection.\n\nRobust Bias Mitigation\n:\nIntroduces a two-tier Jury Panel (General + Critic Judge) for counterfactual validation and holistic scoring reconciliation, enhancing resilience against adversarial cases and inherent LLM biases.\n\nPractical Scalability\n:\nAdopts parallel expert routing for efficiency and uses interpretable weighted aggregation, supporting easy updates to the expert pool and facilitating real-world deployment."}, "weaknesses": {"value": "Section 3.1 (Architectural Overview) contains lengthy, low-readability descriptions of the framework, while subsequent method details are overly brief—this imbalance makes it hard to \nform an intuitive understanding of the otherwise simple method.\n\nInconsistent stage division: Section 3.1 mentions 4 stages, but Figure 1 and subsequent text only reference 2 phases, causing confusion.\n\nFigures 1 and 2 are overly simplistic, failing to clearly illustrate the framework’s workflow.\n\nStructural misplacement: The second paragraph of Section 4.3 does not involve implementation details and is improperly positioned here.\n\nTable formatting errors: Table headers are not placed above the tables as required.\n\nNo justification is provided for defining exactly 42 evaluation dimensions in Section 3.2, lacking a basis for this specific number.\n\nCore components like the Scenario Classifier and Expert Router are critical to the framework, but Section 3.3 provides no detailed information about their design (e.g., model architecture of the classifier, routing logic of the router).\n\nRedundant component positioning: Eq. 2 assigns one optimal LLM to each dimension, and the Scenario Classifier selects dimensions for inputs—yet the paper fails to clarify the unique role of the Expert Router, making its necessity unclear.\n\nUnclear distinction between “scenarios” and “dimensions”: MD-Eval involves 36 scenarios and 42 dimensions, but the paper does not define how scenarios differ from dimensions, leading to conceptual confusion.\n\nInadequate implementation details in Section 4.3: It does not specify which candidate LLMs were used for expert identification, the total number of candidate models, or the final number of selected experts, undermining reproducibility.\n\nUnvalidated baseline metrics: “Dim Acc.” (Dimensional Accuracy) is a dimension-specific metric unique to the proposed method, but the paper does not explain how baselines (e.g., GPT-4o-mini) were tested on this metric, casting doubt on result comparability.\n\nInconsistent table formatting: The paper does not define what bold text in tables represents (presumably indicating the best performance), but inconsistencies exist: in Table 2, D-MoE-Eval is bolded in the “Natural” subset despite underperforming GPT-4o, and no bold text is used in the “Neighbor” subset.\n\nKey components are not ablated individually: The Jury Panel’s ablation only removes the entire panel, without testing the impact of removing either the General Judge or Critic Judge alone; there is no ablation of the Scenario Classifier or specific evaluation dimensions to verify their necessity.\n\nWhile the framework uses parallel expert routing to improve efficiency, it does not quantify the computational costs of its MoE design—for example, how the number of experts (one per 42 dimensions) affects inference time or resource consumption compared to monolithic evaluators. Given that MoE architectures typically incur higher memory or latency costs due to expert activation, this omission makes it hard to assess the framework’s practicality in resource-constrained environments.\n\nThe framework decomposes evaluation into 42 dimensions and assigns dedicated experts to each, but it does not analyze or mitigate potential knowledge redundancy between experts. For instance, two experts may learn overlapping text quality features, leading to redundant parameter usage and inefficient computation—a common issue in MoE architectures where multiple experts often encode shared knowledge for related tasks."}, "questions": {"value": "1. For the inconsistent stage division (4 stages in Section 3.1 vs. 2 phases in Figure 1/subsequent text), will you clarify the framework’s core pipeline and correct this discrepancy? Also, will you define p_i in Eq. 1?\n\n2. What empirical or theoretical basis supports the choice of exactly 42 evaluation dimensions in Section3.2? \n\n3. Could you provide details on the Scenario Classifier’s architecture and the Expert Router’s routing logic?\n\n4. Given Eq. 2 maps dimensions to experts and the Scenario Classifier selects dimensions, what unique role does the Expert Router serve? How do you formally distinguish “scenarios” from “dimensions” in MD-Eval?\n\n5. Section 4.3 lacks details on candidate LLMs for expert identification—will you supplement these to ensure reproducibility?\n\n6. How were baselines tested on “Dim Acc.,”?\n\n7. What defines bold text in tables?\n\n8. Can you quantify how the 42-dimension expert setup impacts inference time/memory compared to monolithic evaluators?\n\n9. Have you analyzed knowledge redundancy between experts, and if so, how do you mitigate it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D538coTG5k", "forum": "3AoeNlw5MF", "replyto": "3AoeNlw5MF", "signatures": ["ICLR.cc/2026/Conference/Submission25569/Reviewer_ZMSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25569/Reviewer_ZMSh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929438842, "cdate": 1761929438842, "tmdate": 1762943478435, "mdate": 1762943478435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes D-MoE-Eval to address the limitations of the LLM-as-a-Judge paradigm. To mitigate the domain-specific bias and reasoning imbalance that commonly arise in single-model evaluators, the authors introduce a four-stage evaluation pipeline consisting of scenario classification, dimension selection, expert routing, and jury validation. They predefine 42 evaluation dimensions and profile various LLMs to map the most suitable expert model to each dimension. Given a pair of model responses, the system dynamically routes the evaluation to the appropriate expert and finally adjusts the score through a jury panel. Experiments on MD-EVAL and LLMBar benchmarks demonstrate superior agreement compared to both open- and closed-source baselines, and ablation studies confirm that both the routing and jury components contribute to the overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. $\\textbf{Clear problem definition and intuitive approach:}$\nThe paper clearly identifies model-selection bias as a fundamental limitation in existing LLM-as-a-Judge research and proposes a direct solution through expert routing among multiple models. This idea is both intuitive and convincing.\n2. $\\textbf{Robust pipeline design:}$\nThe hierarchical structure of scenario, dimension, expert, and jury enhances interpretability and modularity of the evaluation process.\nIn particular, the explicit validation step allows human observers to trace and verify how evaluation scores are derived, partially improving transparency and reliability.\n3. $\\textbf{Consistent performance improvement:}$\nAcross MD-EVAL and LLMBar, the proposed method consistently outperforms previous evaluators, achieving over 10% improvement on adversarial subsets.\n4. $\\textbf{Valid ablation analysis:}$\nRemoving either the Jury Panel or expert routing results in a 9 ~12% performance drop, empirically showing that both components are essential to the system’s effectiveness."}, "weaknesses": {"value": "1. $\\textbf{Lack of comparison with a simple per-dimension best ensemble:}$\nWhile the main contribution lies in combining dimension-specific experts, there is no baseline that simply uses the best-performing model for each dimension (an oracle ensemble).\nIt remains unclear whether the observed gains come from the dynamic routing and jury mechanism, or merely from combining strong models.\n2. $\\textbf{Insufficient justification for the 42 predefined dimensions:}$\nThe paper defines 42 evaluation dimensions and performs profiling using a held-out dataset, but it is not clear how this taxonomy was derived or whether it can generalize to real-world evaluation settings.\nIt is uncertain whether all possible evaluation scenarios can be covered by these 42 categories, and the construction of the held-out dataset is also not well explained.\nFor example, in creative writing tasks, essay and script require entirely different evaluation criteria.\nMoreover, the framework’s ability to generalize to unseen or more fine-grained evaluation dimensions is not demonstrated.\n3. $\\textbf{Unclear utilization of LLM-based evaluation baselines:}$\nThe paper employs open-source LLMs such as LLaMA and Mistral as evaluation baselines, yet it does not describe in detail how these models were actually used for evaluation.\nCombined with the lack of diverse comparison methods, it remains unclear whether these baselines were applied with appropriate prompting or calibration strategies.\n4. $\\textbf{No analysis of computational cost or efficiency:}$\nSince D-MoE-Eval requires parallel invocation of multiple models, its inference cost is inevitably higher than that of single-judge evaluators.\nHowever, the paper provides no analysis of the average number of calls, token usage, latency, or financial cost per evaluation.\nTo assess its practical usability, a discussion on the cost–performance trade-off is necessary.\n5. $\\textbf{Missing comparison with standard LLM-as-a-Judge frameworks:}$\nRecent evaluation frameworks such as G-Eval and SE-Eval have achieved notable improvements in judge reliability and human alignment.\nDespite this, the paper only compares against open-source or evaluation-finetuned models, without any direct comparison to these standard approaches."}, "questions": {"value": "1. If each dimension simply uses the best-performing model selected during the profiling stage (oracle ensemble), how does the performance differ from D-MoE-Eval?\nDo the routing and jury components provide any additional benefit beyond model combination?\n2. How were the 42 evaluation dimensions determined?\nCan such predefined dimensions generalize to real-world evaluation scenarios (e.g., creative writing for essay vs. for script) that require different evaluation criteria?\n3. For the open-source evaluators used as baselines, what prompting or calibration strategies were applied?\nWere all evaluators prompted with the same evaluation template?\n4. How much higher is the average evaluation cost of D-MoE-Eval compared to single-judge baselines?\nPlease report the average number of model calls, token usage, latency, and approximate monetary cost per evaluation.\n5. Why are frameworks such as G-Eval and SE-Eval not included in the comparison?\nCan results be reported using the same human agreement metrics to enable fair comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "39piDKfb3M", "forum": "3AoeNlw5MF", "replyto": "3AoeNlw5MF", "signatures": ["ICLR.cc/2026/Conference/Submission25569/Reviewer_ErC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25569/Reviewer_ErC3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998381143, "cdate": 1761998381143, "tmdate": 1762943478219, "mdate": 1762943478219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}