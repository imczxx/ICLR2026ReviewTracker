{"id": "EdTt8nMAMA", "number": 9459, "cdate": 1758123235689, "mdate": 1759897722154, "content": {"title": "Multi-Agent Debate with Memory Masking", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various language-based reasoning tasks (e.g., math reasoning). Among all LLM reasoning frameworks, _multi-agent debate_ (MAD), which employs multiple LLM agents and performs reasoning in the way of multi-round debate, has emerged as a powerful reasoning paradigm since it allows agents to access previous memories to refine their reasoning iteratively in each debate round and facilitates LLMs in alleviating the potential intrinsic self-preference bias. Although MAD improves the reasoning capabilities of LLMs significantly, in this paper, however, we theoretically demonstrate that the performance of MAD is closely related to the quality of memories. This indicates that MAD is still vulnerable to wrong reasoning memories, which poses a threat to the robustness of MAD. To address this problem, we introduce a simple yet effective multi-agent debate framework, _multi-agent debate with memory masking_ (MAD-M$^2$), to enhance the robustness of MAD by allowing LLM agents to select memories in the previous debate round before they perform reasoning in the current debate round. In this way, MAD-M$^2$ can polish the contextual information at the beginning of each debate round by preserving as many informative and meaningful memories as possible while dropping the noisy memories and, in turn, achieve better reasoning results. Extensive empirical results on several mainstream mathematical and logical reasoning benchmarks demonstrate that MAD-M$^2$ is able to achieve better results than the typical MAD.", "tldr": "", "keywords": ["multi-agent debate", "memory selection", "robustness"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f289c2e948fa9e0008d2a2272b768f4310cb99b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates a fundamental limitation of the Multi-Agent Debate (MAD) framework, demonstrating that LLM agents exhibit vulnerability to erroneous memories from preceding debate rounds, which can propagate incorrect reasoning trajectories. Through rigorous theoretical analysis, the authors establish that MAD performance is intrinsically bounded by e^(-αNₑ), where Nₑ denotes the cardinality of erroneous memories and α characterizes the agent's robustness coefficient. To address this vulnerability, they propose MAD-M² (Multi-Agent Debate with Memory Masking), a framework that incorporates a memory evaluation and masking mechanism, enabling agents to selectively filter low-quality contextual information prior to each reasoning iteration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is the first to demonstrate that Multi-Agent Debate (MAD) frameworks are vulnerable to false memories, providing theoretical proof of this phenomenon.\n2. This study offers a thorough analysis of the key characteristics of the proposed MAD-M² framework in relation to existing methods."}, "weaknesses": {"value": "1. The primary contribution of this paper lies in its theoretical insights rather than methodological innovations.\n\n2. The proposed MAD-M² framework achieves state-of-the-art performance in only a limited subset of experimental scenarios, while underperforming traditional MAD or CoT-SC methods in the majority of cases. Moreover, MAD-M² incurs significantly higher token consumption compared to baseline methods. From a performance-efficiency trade-off perspective, MAD-M² does not demonstrate clear contributions. If additional metrics can substantiate the contributions of MAD-M², please include them in Table 1."}, "questions": {"value": "Refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bsfJH9UskG", "forum": "EdTt8nMAMA", "replyto": "EdTt8nMAMA", "signatures": ["ICLR.cc/2026/Conference/Submission9459/Reviewer_gCX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9459/Reviewer_gCX4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718634548, "cdate": 1761718634548, "tmdate": 1762921051487, "mdate": 1762921051487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness issue in the multi-agent debate (MAD) framework for large language model (LLM) reasoning. The authors theoretically demonstrate that the performance of MAD degrades when agents rely on “wrong memories” from prior debate rounds, since each round’s reasoning depends on previous outputs. To mitigate this, they propose MAD-M2 (Multi-Agent Debate with Memory Masking), which allows each agent to evaluate and selectively mask unreliable memories before generating new responses. The method is conceptually simple yet well-motivated. Experiments across four reasoning benchmarks and diverse models show consistent or improved results compared with vanilla MAD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation grounded in a theoretical gap: The authors identify an underexplored limitation in multi-agent debate frameworks — vulnerability to low-quality reasoning memories — and back it up with intuitive examples (Figure 1) and formal analysis.\n\n2. The analysis in Section 2 provides explicit probabilistic bounds linking performance degradation to the number of wrong memories. The inclusion of both hard and easy reasoning settings (HPR/EPR) offers a nuanced interpretation of how memory errors affect debate robustness."}, "weaknesses": {"value": "1. My biggest concerns are about approach performance. I only see the marginal performance gains in stronger models. While MAD-M2 improves certain benchmarks on easy tasks, the improvements for DeepSeek-V3 are minor or inconsistent. This suggests limited scalability to highly capable LLMs. Could the authors provide more results on diverse and powerful LLM benchmarks?\n\n2. The proposed masking step roughly doubles token consumption in some cases, but the paper provides limited quantitative analysis of efficiency–performance trade-offs. \n\n3. The masking mechanism is described at a high level but not deeply analyzed. For instance, how different scoring heuristics or thresholds affect the results remains unclear. The authors are suggested to add more fine-grained ablations (e.g., varying mask strictness).\n\n4. Although the paper compares conceptually to S-MAD and S2-MAD, it doesn’t empirically benchmark against them."}, "questions": {"value": "1. How sensitive is MAD-M2 to the rule for generating the binary mask?\n\n2. Did the authors explore partial masking or soft weighting? Could such approaches improve stability?\n\n3. Can the authors provide more comprehensive experiments on more LLM backbones and compare more baseline MAD approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGIUZPhYWf", "forum": "EdTt8nMAMA", "replyto": "EdTt8nMAMA", "signatures": ["ICLR.cc/2026/Conference/Submission9459/Reviewer_7GkP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9459/Reviewer_7GkP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986766432, "cdate": 1761986766432, "tmdate": 1762921051119, "mdate": 1762921051119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the robustness of multi-agent debate frameworks to wrong memories from previous debate rounds. The authors provide a theoretical analysis showing that MAD performance is closely related to the number of wrong memories, then propose MAD-M², which allows agents to evaluate and mask undesirable memories before reasoning in the next debate round. Experiments on MATH, MMLU-Pro, AIME24/25 show MAD-M² improves over standard MAD, particularly on easy tasks with weaker models (Qwen2.5-72B, Mixtral-8x7B), though gains on hard tasks and stronger models (DeepSeek-V3) are limited"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a real issue in MAD: agents can be misled by wrong memories from previous rounds, as illustrated in Figure 1 where Agent 1 initially answers correctly but is misled in Round 2 after seeing Agent 2's incorrect response. This is intuitive and well-demonstrated.\n\nMAD-M² is straightforward to implement: agents evaluate memories, generate binary masks, and reason with filtered memories. The method doesn't require additional models or complex infrastructure, making it practical for real deployment.\n\n\nThe theoretical analysis in Section 2.2 provides useful insights. Propositions 2.2 and 2.3 formalize the performance bounds for CoT-SC and MAD, showing how wrong memories (Ne) exponentially degrade performance through the term e^(-αNe). The distinction between easy problem reasoning (EPR) and hard problem reasoning (HPR) settings offers guidance for when memory masking helps."}, "weaknesses": {"value": "MAD-M² shows improvement on MATH (0.90→0.92, +2%) and AIME24 (0.37→0.50), but degrades or maintains performance on MMLU-Pro (0.83→0.83) and AIME25 (0.40→0.40). Meanwhile, MAD-M² shows improvement on MATH (0.90→0.92, +2%) and AIME24 (0.37→0.50), but degrades or maintains performance on MMLU-Pro (0.83→0.83) and AIME25 (0.40→0.40)\n\n\nToken consumption analysis in Table 1 shows MAD-M² consumes 50-100% more tokens than standard MAD. The cost-benefit ratio is poor, especially when standard MAD already improves factuality and reasoning significantly over single-agent methods\n\nFor assumption 2.1, the claim that the probability of correct answers given memories follows e^(-αNe) is asserted without justification. Why exponential decay? Why not linear, polynomial, or other forms?\n\nHoeffding requires independent samples, but debate rounds are sequential and dependent - each round conditions on previous memories. The analysis doesn't account for this dependency.\n\n\nFigures 3-4 show that scaling agents help on easy tasks only (consistent with theory), but MAD-M² doesn't consistently outperform MAD even there"}, "questions": {"value": "How often do agents correctly identify wrong memories? What's the precision/recall?\n\n\nFigure 2 shows one cherry-picked example. Could you also provide the failure cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zM3zOV1CFr", "forum": "EdTt8nMAMA", "replyto": "EdTt8nMAMA", "signatures": ["ICLR.cc/2026/Conference/Submission9459/Reviewer_u3Fc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9459/Reviewer_u3Fc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109403582, "cdate": 1762109403582, "tmdate": 1762921050845, "mdate": 1762921050845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}