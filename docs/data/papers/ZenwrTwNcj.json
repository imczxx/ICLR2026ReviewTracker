{"id": "ZenwrTwNcj", "number": 14976, "cdate": 1758246408087, "mdate": 1759897337934, "content": {"title": "Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics", "abstract": "Object recognition and motion understanding are key components of perception that complement each other.\n    While self-supervised learning methods have shown promise in their ability to learn from unlabeled data, they have primarily focused on obtaining rich representations for either recognition or motion rather than both in tandem.\n    On the other hand, latent dynamics modeling has been used in decision making to learn latent representations of observations and their transformations over time for control and planning tasks.\n    In this work, we present Midway Network, a new self-supervised learning architecture that is the first to learn strong visual representations for both object recognition and motion understanding solely from natural videos, by extending latent dynamics modeling to this domain.\n    Midway Network leverages a _midway_ top-down path to infer motion latents between video frames, as well as a dense forward prediction objective and hierarchical structure to tackle the complex, multi-object scenes of natural videos.\n    We demonstrate that after pretraining on two large-scale natural video datasets, Midway Network achieves strong performance on both semantic segmentation and optical flow tasks relative to prior self-supervised learning methods.\n    We also show that Midway Network's learned dynamics can capture high-level correspondence via a novel analysis method based on forward feature perturbation.\n    Code is provided at https://anonymous.4open.science/r/midway-network.", "tldr": "Midway Network is a new SSL architecture that extends latent dynamics modeling to natural videos to learn rich representations for object recognition and motion understanding.", "keywords": ["self-supervision learning", "representation learning", "latent dynamics", "natural videos", "object recognition", "motion understanding", "computer vision"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81aa2304353508405ed7c7151947cadc3061d83b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors, in this work, introduce a new self-supervised learning (SSL) architecture called Midway Network.\n\n- They extend the concept of latent dynamics modeling, used in control and decision-making, to the visual perception domain.\n\n- Midway network is a self-supervised video model designed to learn both: Object recognition (semantic understanding) and Motion understanding (how things move over time) purely from natural, unlabeled videos ‚Äî without relying on curated image datasets or external motion labels like optical flow.\n\n- Midway Network outperforms or matches the best SSL baselines (like DINO, DoRA, CroCo v2) on both tasks: Semantic segmentation and Optical flow estimation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Conceptual Soundness\n- The authors ground their approach in predictive coding and latent dynamics modeling, both well-established ideas in neuroscience and machine learning.\n\n- The theoretical motivation, that perception arises from predicting sensory changes is sound.\n\n- The authors identify a clear gap: self-supervised models typically learn either recognition (DINO, iBOT) or motion (CroCo, FlowE), not both.\n\n### 2. Architectural Novelty\n- The architecture is a careful combination of: 1. Inverse dynamics (midway path) to infer motion, 2. Forward prediction (dense feature-level) to learn temporal coherence, and 3. Hierarchical refinement (inspired by optical flow networks e.g., PWC-Net, RAFT).\n\n- The design is justified by analogies to biological systems and prior SSL hierarchies (e.g., Ladder Networks, DINO).\n\n- The inclusion of gating units in transformer residuals is thoughtful as it prevents trivial identity mappings, a known issue in predictive models.\n\n### 3. Experimental Soundness\n- Comprehensive evaluations across recognition (semantic segmentation) and motion (optical flow).\n\n- Extensive comparison against relevant baselines: DINO, DoRA, PooDLe, CroCo v2, DynaMo, etc.\n\n- Consistent use of natural video datasets (BDD100K and Walking Tours) supports the claim of \"learning from natural data only.\"\n\n- Sufficient ablation studies test each architectural component's impact on both semantic and motion metrics.\n\n### 4. Interpretation Soundness\n- The \"forwarded feature perturbation\" method is a novel and interpretable way to visualize what the dynamics model learns.\n\n- It qualitatively demonstrates non-trivial motion correspondence ‚Äî a rare strength in SSL papers."}, "weaknesses": {"value": "### 1. Conceptual Limitations\n- While conceptually coherent, \"latent dynamics\" is borrowed from control/planning literature and adapted here somewhat heuristically ‚Äî the paper lacks a strong theoretical derivation connecting latent dynamics to semantic learning in videos.\n\n- The link between motion prediction and semantic invariance is intuitive but not formally analyzed.\n\n### 2. Architectural Limitations\n- The midway and backward paths add significant complexity; it‚Äôs unclear if all components are essential (though the ablation studies help).\n\n- The architecture might overfit to short temporal correlations (1-second gaps between frames) and may not generalize to longer-horizon motion.\n\n### 3. Experimental Limitations\n- Only two pretraining sources (BDD and WT-Venice). That‚Äôs small compared to large-scale pretraining regimes (e.g., Kinetics-700, Ego4D). Therefore, it is hard to tell if results scale to diverse or indoor/outdoor mixed environments.\n\n- The authors do not measure performance over multiple time steps (e.g., predicting motion 10 frames ahead).\n\n- Some of the baselines (e.g., PooDLe) use higher resolution or external flow networks, making cross-comparison imperfect.\n\n- Few metrics report standard deviation or multiple seeds.\n\n### 4. Interpretation Limitations:\n- The forward feature perturbation analysis is qualitative. There‚Äôs no quantitative measure of how well it aligns with ground-truth motion.\n\n- The theoretical justification for why this analysis reflects \"high-level correspondence\" is intuitive but not formalized."}, "questions": {"value": "Please see the discussion in weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J5um38HQkK", "forum": "ZenwrTwNcj", "replyto": "ZenwrTwNcj", "signatures": ["ICLR.cc/2026/Conference/Submission14976/Reviewer_m9iU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14976/Reviewer_m9iU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760770483473, "cdate": 1760770483473, "tmdate": 1762925308272, "mdate": 1762925308272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Midway Network, a self-supervised learning framework that jointly learns object recognition and motion understanding from natural videos through latent dynamics modeling. The key idea is to integrate inverse and forward dynamics modules within a hierarchical architecture, where motion latents describe transformations between consecutive frames and are refined top-down across feature levels. Experiments show that Midway Network achieves strong performances in both semantic segmentation and optical flow tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. Unified SSL framework for both object recognition and motion understanding: The paper presents a coherent approach that bridges predictive coding theory with modern self-supervised Transformers, achieving representation learning for both semantics and motion within a single framework.\n\nS2. In-depth analysis: The analysis of learned motion latents in Section 4.4 (forwarded feature perturbation) is technically sound and creative. It provides interpretable evidence that the model captures spatial correspondences.\n\nS3. Diverse Evaluation Domains: The proposed method is evaluated on several tasks and benchmarks."}, "weaknesses": {"value": "W1. Incremental novelty: the proposed inverse + forward dynamics framework for learning latent motions follows a well-established approach used in latent world models and video prediction literature [a, b, c]. Moreover, the hierarchical refinement design is conceptually similar to the Spatial Dynamics Module (SDM) used in PooDLe..\n\nW2. No guarantee of motion: The paper does not theoretically ensure that the learned latent $m_t$ encodes motion rather than directly leaking target feature information ($ùëß_{ùë°+1}$). Although empirical analyses (optical flow results, Sec. 4.4 perturbation study) suggest motion-like behavior, the model could still ‚Äúhack‚Äù the objective by embedding target information in $ùëö$ making the representation less interpretable.\n\nW3. Limited quantitative analysis of motion learning:\nSection 4.4 provides compelling qualitative examples, but the conclusions would be stronger with quantitative validation. For instance, comparing perturbation-based correspondence against pseudo ground-truth from an off-the-shelf tracker could more rigorously substantiate the claimed motion alignment.\n\nW4. Marginal effect of design components: As seen in Table 3 (e.g., 2‚Üí7, 3‚Üí8, 4‚Üí9 comparisons), adding design components (e.g., backward layers, gating) only yields small incremental gains, which questions their relative contribution to the overall improvement.\n\nW5. (Minor) Lack of planning or world-model evaluation: Despite the discussion of potential applications to planning and world modeling, no experiments demonstrate this capability, slightly limiting the perceived impact of the proposed approach. \n\n[a] Bruce et al., ‚ÄúGenie: Generative Interactive Environments,‚Äù ICML, 2024.\\\n[b] Ye et al., ‚ÄúLatent Action Pretraining from Videos,‚Äù ICLR, 2025.\\\n[c] Gao et al., ‚ÄúAdaWorld: Learning Adaptable World Models with Latent Actions,‚Äù ICML, 2025."}, "questions": {"value": "Q1. Why are optical flow results for PooDLe not reported in Tables 1‚Äì2?\nWas there a fundamental limitation in adapting its architecture for flow prediction, or were these experiments omitted for other reasons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "egg7UE8fxN", "forum": "ZenwrTwNcj", "replyto": "ZenwrTwNcj", "signatures": ["ICLR.cc/2026/Conference/Submission14976/Reviewer_9dmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14976/Reviewer_9dmU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490021765, "cdate": 1761490021765, "tmdate": 1762925307610, "mdate": 1762925307610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel self-supervised learning architecture for learning visual representations that jointly capture object recognition and motion understanding from video inputs. The proposed Midway Network achieves this by approximating a hierarchical latent embedding to model motion signals, thereby removing the need for an external motion predictor. Extensive experiments on object segmentation and optical flow tasks demonstrate the effectiveness of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By integrating latent optical flow estimation within a self-supervised framework, the proposed method enables simultaneous learning of motion information and content features through a single encoder. This design allows motion cues to be naturally incorporated into semantic representations.\n\n2. The joint learning of motion and semantic features leads to consistent improvements in both semantic segmentation and optical flow performance."}, "weaknesses": {"value": "1. While the paper contains several strong ideas, the overall framework is somewhat difficult to follow at first. It took multiple readings to fully grasp how the individual components contribute to the overall system. This could be improved by adding a concise, high-level overview of the architecture‚Äîperhaps at the beginning of Section 3‚Äîalong with a summarizing figure that highlights the key components and their interactions.\n\n2. Since one of the main contributions is the joint learning of motion and semantic representations from video, it would strengthen the paper to include a comparison with MC-JEPA to better contextualize performance gains."}, "questions": {"value": "1. Figure 1 is referenced in the text but not included in the main manuscript.\n\n2. Were all the additional components used during inference, or only in training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xpJfmTLZdD", "forum": "ZenwrTwNcj", "replyto": "ZenwrTwNcj", "signatures": ["ICLR.cc/2026/Conference/Submission14976/Reviewer_BVMm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14976/Reviewer_BVMm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535719739, "cdate": 1761535719739, "tmdate": 1762925307295, "mdate": 1762925307295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new self-supervised learning framework that jointly learns object recognition and motion understanding from natural videos. It extends latent dynamics modeling to the video representation learning domain. The model combines an inverse-dynamics midway path, a dense forward prediction objective, and a hierarchical refinement structure to learn both semantic and motion features from unlabeled data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper tackles an important and underexplored problem, learning both recognition and motion representations from unlabeled videos\n2. It‚Äôs conceptually well-grounded, drawing inspiration from neuroscience to motivate the overall framework.\n3. The architecture itself is Innovative. The midway path for motion latent inference feels like a fresh and thoughtful design choice."}, "weaknesses": {"value": "1. It‚Äôs hard to disentangle architecture gains from data scale and model capacity\n2. While the combination is novel, many components are incremental extensions of ideas from CroCo, DynaMo, or PooDLe."}, "questions": {"value": "1. In Table 1, ViT-B only modestly improves flow over ViT-S and CroCo v2 still wins. What‚Äôs the failure mode for Midway at that scale?\n2.The forwarded feature perturbation analysis is cool, could it be extended into a quantitative metric?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vRu0VyhKFc", "forum": "ZenwrTwNcj", "replyto": "ZenwrTwNcj", "signatures": ["ICLR.cc/2026/Conference/Submission14976/Reviewer_RAZm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14976/Reviewer_RAZm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883806210, "cdate": 1761883806210, "tmdate": 1762925306920, "mdate": 1762925306920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}