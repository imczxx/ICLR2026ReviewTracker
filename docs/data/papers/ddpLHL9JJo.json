{"id": "ddpLHL9JJo", "number": 24399, "cdate": 1758356486088, "mdate": 1759896768177, "content": {"title": "MIRAGE: MULTI-HOP INTERLEAVED REASONING AND RETRIEVAL-GROUNDED EVIDENCE", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nacross a wide spectrum of natural language tasks, especially information retrieval\nand comprehensive reasoning. However, existing benchmarks typically evaluate\nthese abilities dependently, failing to capture the process with interleaving and in-\ntegrated reasoning and retrieval for complex queries, which is commonly needed\nin real-world applications. To address this limitation, we propose a novel bench-\nmark—MIRAGE—the first benchmark specifically designed to assess an LLM’s\nability of step-wise reasoning on a decomposed query and iterative retrieval based\non intermediate reasoning outcomes across multiple interactions, ultimately syn-\nthesizing a coherent and comprehensive answer. MIRAGE consists of 579 real-\nworld queries spanning diverse domain including Finance, Legal, Technology,\nAcademia. We collected both open-sourced dataset and real-world forum con-\nversation as our data sources, and we further curate the dataset into a multi-turn\nformat along with a consicse question and comprehensive answer. To support fur-\nther development and scalability, we also introduce a data generation pipeline for\nbenchmark expansion. We evaluate state-of-the-art approaches on the our bench-\nmark and find that none exhibit consistently strong performance, underscoring\ntheir lack of ability of interleaving reasoning and retrieval as well as the need for\nfurther advancement. Our benchmark provides a foundation for future research\ninto more robust and dynamic information-seeking agents.", "tldr": "", "keywords": ["Iterative Reasoning", "Document Retrieval", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e5150d58b9acf892efa23925f693fb20a17c1f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MIRAGE, a benchmark designed to evaluate llm-based RAG systems on the interleaved task of multi-hop retrieval and evidence-grounded reasoning. \nThe authors claim that this interleaved RAG capabilities is critical for real-world problem-solving, but this spectrum is not well addressed by existing benchmarks.\nTherefore, this paper propose MIRAGE, which consists of 579 instances across four domain (Legal, FInance, Tech, Academy). Both of them needs llms to generate sub-queries iteratively, retrieve knowledge from adversarial corpus, to find the final answer.\n\nThe authors conduct experiments comparing closed-book LLMs, single-turn RAG, multi-hop RAG, and interleaved RAG systems, finding that even SOTA RAG frameworks achieve at most 61.5% Fact-Score, with failures attributed to premature reasoning and distractor vulnerability.\nThe paper positions MIRAGE as a public resource to advance agentic reasoning capabilities in future research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces the MIRAGE benchmark for Retrieval-Augmented Generation (RAG), designed to address challenges such as interleaved retrieval and noise resistance in the era of AI agents. The benchmark focuses on comprehensive evaluation of the retrieval-augmented workflow, assessing not only the accuracy of final answers but also capabilities like robustness to retrieval interference. It also provides relevant environments with both standard and adversarial settings, ensuring a high degree of reproducibility.\n\n2. The authors evaluated current academic and industrial models and products on MIRAGE, drawing preliminary conclusions from the results."}, "weaknesses": {"value": "1. What are the key distinctions between the benchmark proposed in this paper and previous or subsequent RAG benchmarks?\n    - Pre-agent era benchmarks: Multi-hop QA datasets such as HotpotQA and MuSiQue.\n    - Agent-era benchmarks: Web browsing benchmarks represented by BrowseComp.\n    - Both types of benchmarks involve multi-turn, interleaved retrieval and generation, making them suitable for evaluating agent-era reasoning models. Moreover, some multi-hop QA benchmarks evaluate not only the correctness of the final answer but also the accuracy of intermediate retrieval steps.\n\n2. The proposed benchmark has a relatively small sample size, containing only 500+ instances, which limits its applicability to evaluations in narrow domains.\n\n3. The benchmark lacks sufficient difficulty. A high-quality benchmark should possess foresight by focusing on challenges that current models struggle to solve. For example, when BrowseComp was introduced, most existing methods achieved only single-digit scores. In contrast, the benchmark discussed here appears to lack challenge, as some models already achieved scores above 60 at its release. This raises the question of whether the benchmark fails to provide meaningful guidance for future model optimization due to its limited difficulty."}, "questions": {"value": "please see weakness section for my quesiton."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vo1LqxuTMa", "forum": "ddpLHL9JJo", "replyto": "ddpLHL9JJo", "signatures": ["ICLR.cc/2026/Conference/Submission24399/Reviewer_WyBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24399/Reviewer_WyBe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722878213, "cdate": 1761722878213, "tmdate": 1762943071870, "mdate": 1762943071870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIRAGE, a benchmark designed to evaluate multi-hop interleaved reasoning and retrieval-grounded evidence generation. It aims to simulate realistic RAG workflows where reasoning and retrieval alternate across multiple steps. The dataset covers several specialized domains (finance, legal, technology, academia) and includes human verification and adversarial distractor construction. Experiments benchmark various LLM-based RAG systems under single-hop, multi-hop, and interleaved settings, accompanied by detailed error analyses distinguishing planning vs. synthesis failures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a timely and practically relevant problem: evaluating RAG systems that require iterative retrieval and reasoning.\n2. The benchmark is well-structured, with clear design motivation, adversarial distractor generation, and human verification steps.\n3. Experiments are comprehensive, covering multiple reasoning configurations and offering useful error analyses (e.g., planning vs. synthesis failures).\n4. The paper is clearly written and easy to follow, with reproducible methodology and well-presented results."}, "weaknesses": {"value": "1. The paper’s identified gap: evaluating interleaved reasoning and retrieval, feels primarily conceptual rather than substantive. Similar processes have been studied in prior RAG and agentic frameworks (e.g., ReAct, RARR). MIRAGE’s contribution lies more in its framing and benchmark packaging than in a fundamentally new task formulation.\n2. Given that MIRAGE emphasizes multi-hop reasoning, the evaluation could better reflect step-level performance (e.g., scoring per hop, planning accuracy, intermediate retrieval relevance). The current setup uses rather standard metrics and separates \"reasoner\" and \"retriever\" evaluation without integrating them into a unified process-level metric. This limits the benchmark’s ability to assess true multi-hop reasoning quality. Also, there's a typo in Table 6, Reasoner instead of Resoner."}, "questions": {"value": "1. The core task of interleaved reasoning and retrieval appears conceptually similar to prior agentic frameworks (e.g., ReAct). Could the authors please elaborate on what substantively differentiates the MIRAGE task formulation from these existing paradigms? Is the novelty primarily in the challenging, domain-specific corpus and the adversarial distractors, or is there a fundamental difference in the required reasoning process itself?\n2. Given that MIRAGE is explicitly designed to evaluate the interleaved process of multi-hop reasoning, the evaluation in Table 6  seems focused on end-to-end performance (Fact-Score, nDCG@10) rather than the quality of the intermediate steps. Could the authors comment on why process-level metrics (e.g., planning accuracy of sub-queries, or hop-by-hop retrieval relevance) were not included? How can the benchmark effectively diagnose \"Planning Failures\" without such metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZ1mskqMq1", "forum": "ddpLHL9JJo", "replyto": "ddpLHL9JJo", "signatures": ["ICLR.cc/2026/Conference/Submission24399/Reviewer_VW56"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24399/Reviewer_VW56"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911407548, "cdate": 1761911407548, "tmdate": 1762943071661, "mdate": 1762943071661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIRAGE, a benchmark designed to evaluate LLMs' ability to dynamically interleave multi-hop retrieval and evidence-grounded reasoning. The benchmark comprises 579 tasks across four domains (Legal, Finance, Technology, Academia), constructed from real-world conversational data through a three-stage pipeline: (1) sourcing conversational seeds, (2) synthesizing complex reasoning tasks, and (3) constructing adversarial retrieval corpora. Comprehensive experiments reveal a stark performance gap that the best academic system achieving only 61.5% task success, with interleaved RAG agents significantly outperforming single-turn and structured multi-hop approaches. The authors identify two primary failure modes: planning failures and synthesis failures."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a genuine evaluation gap. MIRAGE is designed to evaluate the interleaved process where agents must dynamically decide when to retrieve and what to search for.\n2. The paper includes comprehensive evaluation across multiple architectures (single-turn, multi-hop, interleaved, commercial systems).\n3. The paper is well-written with clear motivation, good use of examples, and visualizations."}, "weaknesses": {"value": "1. 579 tasks is relatively small for a benchmark, especially when split across 4 domains.\n2. The paper does not report statistical significance of the results.\n3. Domain distribution is highly skewed (Legal: 216, Technology: 184, Academia: 110, Finance: 69). This imbalance may affect the generalizability of conclusions."}, "questions": {"value": "1. BERTScore might be unsuitable for such kind of abstractive, evidence-grounded answers. For future papers, if the authors can only pick one metric to report, which one would the authors recommend?\n2. Would it be feasible to conduct a human evaluation on some samples (e.g., 100 examples) to validate the automated metrics and provide qualitative insights?\n3. Can you provide a breakdown of results by domain? Are certain domains systematically harder?\n4. Are there any considerations for avoiding potential benchmark fitting in the future?\n4. \"disconnect\" -> \"disconnection\" in line 52; \"Soucing\" -> \"Sourcing\" in Figure 1. \"Resoner\" -> \"Reasoner\" in Table 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XehNWZeYwQ", "forum": "ddpLHL9JJo", "replyto": "ddpLHL9JJo", "signatures": ["ICLR.cc/2026/Conference/Submission24399/Reviewer_mKar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24399/Reviewer_mKar"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966959038, "cdate": 1761966959038, "tmdate": 1762943071450, "mdate": 1762943071450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}