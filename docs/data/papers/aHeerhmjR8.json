{"id": "aHeerhmjR8", "number": 23009, "cdate": 1758338058293, "mdate": 1759896836038, "content": {"title": "Revisiting the Past: Data Unlearning with Model State History", "abstract": "Large language models are trained on massive corpora of web data, which may include private data, copyrighted material, factually inaccurate data, or data that degrades model performance. Eliminating the influence of such problematic datapoints on a model through complete retraining---by repeatedly pretraining the model on datasets that exclude these specific instances---is computationally prohibitive. To address this, unlearning algorithms have been proposed, that aim to eliminate the influence of particular datapoints at a low computational cost, while leaving the rest of the model intact. However, precisely unlearning the influence of data on a large language model has proven to be a major challenge. In this work, we propose a new algorithm, MSA (**M**odel **S**tate **A**rithmetic), for unlearning datapoints in large language models. MSA utilizes prior model checkpoints--- artifacts that record model states at different stages of pretraining--- to estimate and counteract the effect of targeted datapoints. Our experimental results show that MSA achieves competitive performance and often outperforms existing machine unlearning algorithms across multiple benchmarks, models, and evaluation metrics, suggesting that MSA could be an effective approach towards more flexible large language models that are capable of data erasure.", "tldr": "We propose a method for unlearning by leveraging model checkpoints during training, achieving improved performance for data-level unlearning", "keywords": ["machine unlearning", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44ba44e0ffb6b5061a9b4d376dff24af3c81461e.pdf", "supplementary_material": "/attachment/a7d56fc7036512bfe163dba8d26296d03a673666.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method for unlearning past datapoints.\n\nSpecifically, the authors assume they have access to an intermediate training checkpoint that has not really been exposed to the information we want to unlearn. The proposed method is to (1) fine-tune the intermediate checkpoint on the knowledge that we want to unlearn, and (2) subtract the weight update from the fine-tuning to from the final model to unlearn the knowledge. \n\nThe paper presents a rigorous empirical evaluation of this relatively simple method against baselines and finds that it performs overall favorably. \n\nThe paper is very well written. \n\nI'm not an unlearning researcher, but I believe that I understand this paper reasonably well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper has a good structure in terms of problem, method, and evaluation, all of which are very clear\n- The paper is very well written.\n- The proposed method is interesting and the empirical results are strong enough to warrant acceptance at ICLR\n- The proposed method is of general interest that goes beyond the application of unlearning. What model behaviors can be added and ablated in this way?"}, "weaknesses": {"value": "- There seems to be no real discussion of general side-effects of adding the vector to the final model apart from the unlearning evaluation. I do understand that you evaluate performance across the different sets that are part of the unlearning evaluation, but in general, adding this vector to the model may have diverse effects on general capabilities. \n- The experiments that are ultimately performed are a bit different from what is depicted in Figure 1. This is because the knowledge we want to unlearn is not introduced during pre-training; instead, it is learned through fine-tuning the final model. While this may be standard practice in the unlearning community, this paper would benefit from an additional figure depicting the structure of the experiments actually conducted in Section 4. \n- The datasets that are used for evaluation are potentially part of the pre-training data of OLMo-2-7B. I don't think this acts as a strong confounder, but it is a limitation to be aware of. \n- As a reader who does not know the details of unlearning, understanding the Table 1, Table 2, and Table 3 took me quite a while. For example, the sentence \n\n*\"We report +100% when performance matches or exceeds\nthat of the ideal model. Otherwise, if at least one baseline outperforms the ideal, we report the\nratio relative to the ideal model; if not, we report the ratio relative to the best-performing baseline.\"* \n\nis not exactly self-explanatory. Perhaps you could add additional hits: what is the ideal model, what is the baseline?\n- Another confusion that I had while reading the paper: In Figure 1, the model at point (b) is called \\theta_C, and the model at point (c) is called \\theta_D. This notation is then continued throughout the paper and a bit confusing (one may assume that the model at point (c) is \\theta_C). Also, in the description of Figure 1, (c) is discussed before (b). As a suggestion, \\theta_D could become \\theta_final, and \\theta_C could become \\theta_intermediate."}, "questions": {"value": "**Question 1:** The intermediate OLMo-Checkpoints that you use come with the state of the optimizer. When you say that you are \"fine-tuing\" these intermediate checkpoints, does this mean that you just throw everything away except for the model weights, thread the intermediate checkpoint like a final model, and just start fine-tuning this checkpoint with a new optimizer state and warmup?\n\n**Question 2:** I assume that you are performing full-parameter fine tuning (no LoRA), both on the final model to add the information, and on the intermediate checkpoint to determine the forget vector?\n\n**Question 3:** It is interesting for me to think about what this method can and cannot do. Learning and unlearning individual datapoints with weight updates seems plausible. But what about more complex behaviors? Have you thought about this?\n\n**Comment:** If you are performing full-parameter fine-tuning, then I would be interested in ablations with LoRA. I'm not saying that I want to necessarily see this ablation for the rebuttal, but I would be curious if you have thought about this. Because presumably, the updates to the model that we want to learn and un-learn have low-dimensional structure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T6MvOy7U6n", "forum": "aHeerhmjR8", "replyto": "aHeerhmjR8", "signatures": ["ICLR.cc/2026/Conference/Submission23009/Reviewer_bmvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23009/Reviewer_bmvB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485597420, "cdate": 1761485597420, "tmdate": 1762942476678, "mdate": 1762942476678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Model State Arithmetic (MSA) to efficiently eliminate the influence of problematic training data, such as private or copyrighted material. MSA achieves this by leveraging prior model checkpoints, which are model states recorded at different stages of pretraining. Specifically, MSA computes a forget vector by finetuning a checkpoint that precedes the introduction of the unlearning target. This vector captures the data's influence in the weight space and is then arithmetically applied to produce an unlearned model. This design is hypothesized to result in a more precise unlearning compared to previous task arithmetic methods that only use the final trained model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Whereas prior approaches, such as task arithmetic, typically only use the last model parameters, this study uses a model checkpoint that precedes the model's exposure to the unlearning documents. Leveraging these prior model checkpoints yields much more effective unlearning performance, achieving superior or competitive performance compared to prior methods.\n\nMSA consistently achieves superior or competitive performance compared to task vectors across a variety of metrics and scenarios, by using multiple benchmarks and models."}, "weaknesses": {"value": "The rationale for using pre-exposure checkpoints in MSA is intuitive, hypothesizing that it yields more semantically meaningful forget vectors by avoiding entanglement with knowledge already acquired by the final model. However, the study needs stronger justification through empirical analysis (e.g., showing the calculated forget vector is better aligned with the unlearning direction) and a theoretical framework defining the superior properties of vectors derived from early checkpoints versus those derived from fully trained parameters.\n\nThe context of Influence Functions (IF) and related literature already encompasses methods designed to estimate the influence of specific training data by considering the timing of their introduction during training, with the goal of approximating the model state where the training data had been absent.\nMSA’s mechanism also aims to reproduce a model equivalent to one not trained on the target data by factoring in the training chronology via pre-exposure checkpoints, necessitating a theoritical discussion linking it to prior work.\n\nFor instance, the paper \"[Data Cleansing for Models Trained with SGD](https://proceedings.neurips.cc/paper_files/paper/2019/hash/5f14615696649541a025d3d0f8e0447f-Abstract.html)\" introduces an estimator for SGD-Influence (Equation 2), which shows that the resulting final parameter difference when removing data point $j$: $θ_{-j}^{[T]}-θ^{[T]}$ is approximated by the initial influence of the data point measured at the moment it was learned ($g(z_j ;θ^{[π(j)]})$) multiplied by a sequence of propagation matrices involving ($I−ηH$). Given that MSA calculates the forget vector $g(z_j ;θ^{[π(j)]})$ based on the immediate effect of $z_j$ on a checkpoint $θ^{[\\pi(j)]}$, and then applies this vector directly to the final model ($θ_{-j}^{[T]} = θ^{[T]}-g(z_j ;θ^{[π(j)]})$), MSA can arguably be viewed as approximating the complex propagation term ($Z_{T-1}Z_{T-1}, \\ldots, Z_{T-1}$ where $Z_t = I-\\eta_tH^{[t]}$) with the Identity matrix ($I$).\nA necessary theoretical discussion should clarify this connection, explicitly address this approximation ($I−ηH≈I$), and validate its suitability, particularly considering the highly non-convex nature of LLMs."}, "questions": {"value": "How were the checkpoints utilized for MSA (500B, 2207B, 3691B, 3859B trained tokens) selected for OLMo experiments? These values feel somewhat arbitrarily chosen. To eliminate this doubt, it would be better to use checkpoints saved according to a specific rule (for example, using checkpoints saved every 1000B tokens)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X3E78YWpIV", "forum": "aHeerhmjR8", "replyto": "aHeerhmjR8", "signatures": ["ICLR.cc/2026/Conference/Submission23009/Reviewer_3L2G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23009/Reviewer_3L2G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724265941, "cdate": 1761724265941, "tmdate": 1762942476227, "mdate": 1762942476227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on unlearning, i.e., mitigating the influence of specific data points on an already trained model without affecting the whole model. Authors introduce an approach, referred to as Model State Arithmetic, that utilizes model checkpoints saved at different stages of pretraining to estimate and counteract the effect of targeted datapoints. Specifically, a forget vector is computed from a checkpoint that precedes exposure to the unlearning documents and this vector is applied to the final model that has already been trained on the unlearning documents. In addition to the forget vector, a retain vector is also estimated using a small retain set when unavailable and is applied to the final model. \n\nExperiments were conducted using three popular unlearning benchmarks and the proposed approach has been shown to achieve competitive performance when compared to various alternative strategies in the specific setup under consideration (where we have access to an intermediate checkpoint that has not been exposed to the unlearning targets)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach is simple and easy to use.\n\nWhile weights arithmetic has been explored before for the problem of unlearning, this paper proposes a simple but effective modification by leveraging intermediate checkpoints. \n\nThe paper has been written well and is easy to understand."}, "weaknesses": {"value": "The proposed approach explicitly relies on an intermediate checkpoint that has not been exposed to the unlearning documents. In order to effectively use this weights arithmetic strategy, one needs to have access to the intermediate checkpoints and also know when the unlearning targets were introduced into the training process. Hence, the proposed strategy is applicable only to specific scenarios. For example, one cannot use it for unlearning any given target from an OSS model for which we only have the last checkpoint. Even if we have access to intermediate checkpoints, we need to know exactly when the unlearning target was introduced into the training process so that we can select a checkpoint prior to that. This could be difficult and we may not be able to do it accurately since same facts can appear multiple times in different ways in the pretraining corpus and we may not be able to easily identify them using standard deduplication approaches. \n\n\nWhile the paper looks into the effect of number of tokens between the intermediate checkpoint \\theta_C and the unlearning targets, it does not study the effect of number of tokens between unlearning targets and the target checkpoint \\theta_D. So, it is unclear if the proposed approach would work in situations where we want the model to unlearn something that it has learned a lot of tokens ago."}, "questions": {"value": "What is the effect of the number of tokens between unlearning targets and the final checkpoint \\theta_D?\n\nSince access to a checkpoint that is not exposed to unlearning targets is a key element in the proposed approach, authors should discuss how one can effectively identify such checkpoints for any given unlearning target that could appear at multiple locations in the large pertaining corpus. The current experimental setup assumes that we exactly know when the unlearning target is introduced into the training process and in fact assumes that the unlearning target is not too many tokens into the past when compared to checkpoint \\theta_D."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "31sgekAHpy", "forum": "aHeerhmjR8", "replyto": "aHeerhmjR8", "signatures": ["ICLR.cc/2026/Conference/Submission23009/Reviewer_qoRY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23009/Reviewer_qoRY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951672402, "cdate": 1761951672402, "tmdate": 1762942475984, "mdate": 1762942475984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper hypothesizes that using earlier checkpoints (specifically those before seeing the forget data) in training to decide the unlearning update applied to the final model can outperform common methods which only see the final model. They devise a specific method which compute an unlearning update by training a checkpoint on the forget set and retrain set seperately and adding a weighted sum of the directions. Comparison against past methods on several benchmarks show the method matches or outperforms past methods. In particular, for ToFU they introduce several new metrics to test additionally to the previous metrics, and show they help distinguish methods (e.g., the failures of RMU) better."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The experiments are thorough, including several benchmarks and additional metrics over past work\n\n2) The experiments generally support that the method performs better than the tested baselines"}, "weaknesses": {"value": "I mention several concerns below; I am happy to reconsider my score given the authors can resolve some of these. See the questions for a breakdown of the concerns into specific questions.\n\n1) Contrary to the paper’s claim, this is not the first work to use previous model states to compute the unlearning. In fact some of the first approximate unlearning methods used previous model checkpoints to compute the model update: Graves et al., [1] compute the update by using several checkpoints during training, while Thudi et al., study how effectively one can unlearn by computing the unlearning update from the model checkpoint before training on the dataset containing the forget data [2]. More recent work has even investigated how to analyze the guarantees of unlearning by restarting training from an earlier checkpoint [3]. No mention or comparison to this existing literature is made.\n\n2) A claim of this paper is that computing updates with checkpoints before seeing the forget data is better than using models that had already trained on the data. However, there are no experiments ablating performance as the proposed method is applied to checkpoints obtained at different stages of training with the forget data; the authors instead compare to other methods. The current evaluation hence doesn’t seems to answer whether the method actually does better at checkpoints before seeing the forget data than when applied to checkpoints after; in fact the method gets better as it is applied to models with more training. \n\n3) Moreover, if the claim is to understand the importance of checkpoints, it seems reasonable to also ablate the other methods by applying them to earlier checkpoints and understand how important their specific unlearning update direction is. This relates back to point 1 where methods using previous checkpoints already exist and comparing to other methods would help disentangle the role of the checkpoint to the proposed method.\n\n[1] Graves, Laura, Vineel Nagisetty, and Vijay Ganesh. \"Amnesiac machine learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 13. 2021.\n\n[2] Thudi, Anvith, et al. \"Unrolling sgd: Understanding factors influencing machine unlearning.\" 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P). IEEE, 2022.\n\n[3] Mu, Siqiao, and Diego Klabjan. \"Rewind-to-delete: Certified machine unlearning for nonconvex functions.\" arXiv preprint arXiv:2409.09778 (2024)."}, "questions": {"value": "Given my previously mentioned concerns, I have the following questions which can answer them.\n\n1) Could the authors tone down the claims of the paper to focus more on the impact of the checkpoint used to compute the unlearning update than using checkpoints altogether; as mentioned past work has already proposed methods that use certain checkpoints, but this paper can add to this literature by focusing itself on empirically investigating which checkpoints lead to better unlearning.\n\n2) On the above, could the authors clarify how their specific method works when applied to checkpoints obtained when training on the forget data; are their implicit findings somewhere in the paper and I somehow missed them? \n\n3) Specifically can the authors claim their method applied to a model fully trained on the forget data performs worse than their method applied to the pre-trained checkpoint (before seeing the forget data)? \n\n4) Furthermore, can the authors have evidence for why the best checkpoint to use is the one just before training on the forget data and not one that comes after starting to train on the forget data?\n\n5) Do the authors have results on what happens when other methods are applied on earlier checkpoints? E.g., one can think of rewind-to-delete as applying fine-tuning on the retain set at an earlier checkpoint, and one could do the same with other methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AG6L3eHee1", "forum": "aHeerhmjR8", "replyto": "aHeerhmjR8", "signatures": ["ICLR.cc/2026/Conference/Submission23009/Reviewer_eFij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23009/Reviewer_eFij"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762384028155, "cdate": 1762384028155, "tmdate": 1762942475587, "mdate": 1762942475587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}