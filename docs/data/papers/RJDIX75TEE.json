{"id": "RJDIX75TEE", "number": 14161, "cdate": 1758229403744, "mdate": 1759897386717, "content": {"title": "Sample Complexity and Representation Ability of Test-time Scaling Paradigms", "abstract": "Test-time scaling paradigms have significantly advanced the capabilities of large language models (LLMs) on complex tasks. Despite their empirical success, theoretical understanding of the sample efficiency of various test-time strategies---such as self-consistency, best-of-$n$, and self-correction---remains limited. \nIn this work, we first establish a separation result between two repeated sampling strategies: self-consistency requires $\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$ only needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap between the correct and second most likely answers. \nNext, we present an expressiveness result for the self-correction approach with verifier feedback: it enables Transformers to simulate online learning over a pool of experts at test time. Therefore, a single Transformer architecture can provably solve multiple tasks without prior knowledge of the specific task associated with a user query, extending the representation theory of Transformers from single-task to multi-task settings. \nFinally, we empirically validate our theoretical results, demonstrating the practical effectiveness of self-correction methods.", "tldr": "", "keywords": ["Large Language Models", "Test-time Scaling", "Sample Complexity", "Representation Theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc422b425167ab748ef828f71c5932b1e9d6aeb4.pdf", "supplementary_material": "/attachment/e14bf33bcd3caf2f8b1451864395eff7da294aa6.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores the theoretical foundations of test-time scaling methods for large language models (LLMs), focusing on the sample efficiency and representational power of strategies like self-consistency, best-of-n, and self-correction. The authors first establish that best-of-n sampling is significantly more sample-efficient than self-consistency for arriving at a correct answer. Secondly, they demonstrate that a Transformer model, when provided with verifier feedback, can simulate online learning over a pool of \"expert\" models during test time. This allows a single Transformer architecture to solve a variety of tasks without needing to know the specific task in advance, thereby extending the theory of Transformer expressiveness to multi-task scenarios. The paper's theoretical claims are then validated through empirical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper provides a novel conceptual framework for how a single Transformer can achieve multi-task representation.\n* Framing self-correction as an online learning problem (i.e., selecting an optimal \"expert\" ) is an interesting approach."}, "weaknesses": {"value": "* The comparison between best-of-n and self-consistency is unfair. BoN is granted access to a \"perfect verifier\" while SC is not, making its superior efficiency an expected outcome of having an external signal based not he standard concentration results\n* The theoretical model relies on non-standard components, e.g., generalized position encoders. It is unclear if this specific construction applies to the mechanisms of standard Transformers given information beyond positions provided.\n* The experiment in 5.1 demonstrates the capacity for self-correction, but it does not validate that this occurs via the specific \"expert-selection\" mechanism proposed in the theory."}, "questions": {"value": "- Is it possible to establish a sample complexity result of the self-correct method for a comparison with boN and SC to match the result in Table 1?\n- While I appreciate the multitask construction run general purpose transformer, it seems an independent result which is used to construct an online learning transformer. This result seems does not directly connect to self correctness but instead in-context RL [1][2], which is widely studied in literature. Could authors clarify more on this?\n\n\n[1] Lin, Licong, Yu Bai, and Song Mei. \"Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining.\" arXiv preprint arXiv:2310.08566 (2023).\n[2] Lee, Jonathan, et al. \"Supervised pretraining can learn in-context reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 43057-43083."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yyvFQwySSN", "forum": "RJDIX75TEE", "replyto": "RJDIX75TEE", "signatures": ["ICLR.cc/2026/Conference/Submission14161/Reviewer_YT7w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14161/Reviewer_YT7w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585836219, "cdate": 1761585836219, "tmdate": 1762924623342, "mdate": 1762924623342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically investigates the sample complexity and representation ability of test-time scaling paradigms for Large Language Models (LLMs), such as self-consistency, best-of-n, and self-correction.\n\nThis paper should be rejected because (1) The paper employs complex and abstract theoretical concepts that are either disconnected from the subsequent proposed method, (2) The paper's structure is fundamentally fragmented, (3) The central Self-Correction method's theoretical framework relies on the existence of an accruate verifier with feedback, which is unrealistic and severe limitation in standard Test-Time Scaling (TTS) paradigms. Seed detailed comments below."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper provides a novel theoretical framework to analyze the sample complexity of Test-Time Scaling (TTS) methods, establishing a clear separation result between self-consistency ($\\Theta(1/\\Delta^2)$) and best-of-n ($\\Theta(1/\\Delta)$).\n    \n2.  It offers a new perspective on self-correction, proving its representational power to enable a single Transformer to simulate online learning over a pool of experts (Bandit problem) at test time, thus extending the theory of Transformers from single-task to multi-task settings."}, "weaknesses": {"value": "1.  The paper lacks a unified motivation, splitting into two seemingly disconnected parts: the sample complexity analysis of repeated sampling methods (self-consistency and best-of-n) and the theoretical analysis of Self-Correction with Verifier Feedback. The connection between the two main results is not clearly established.\n    \n2.  The analysis of Self-Correction with Verifier Feedback relies on the existence of an _accurate_ verifier, which is generally not a realistic assumption for a test-time scaling paradigm. Specifically, the modeling of the LLM inference process as a Bandit problem, where a General-Purpose Transformer (conceptually similar to MoE) learns to select an expert based on verifier feedback, is novel but appears disconnected from practical Test-Time Scaling (TTS) scenarios.\n    \n3.  The paper's structure is confusing. The Introduction elaborates on the implementation details of the \"General-Purpose Transformer,\" while the dedicated Method section provides minimal subsequent discussion. This makes the Introduction overly dense and difficult to follow. The authors should prioritize an intuitive understanding and the main \"takeaway\" conclusions of the General-Purpose Transformer concept in the Introduction, moving the implementation specifics to the Method section or an Appendix.\n    \n4.  The reasoning in the claim \"We illustrated this mechanism in Figure 2. As a result, the action sequence achieves $o(1)$ regret and the response sequence is generated from the corresponding expert selected by the latest action. Therefore, the response sequence also achieves regret $o(1)$\" is unclear. It is not immediately obvious how merely illustrating a mechanism in Figure 2 leads to the conclusion of $o(1)$ regret for the action sequence.\n    \n5.  The Generalized Position Encoder introduced in the Preliminary section appears to be completely irrelevant to the rest of the paper's analysis and discussion."}, "questions": {"value": "- What is the relation between the sample complexity proof and the proposed method?\n- How can one obtain a oracle verifier with feedback in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QMkvGltQDB", "forum": "RJDIX75TEE", "replyto": "RJDIX75TEE", "signatures": ["ICLR.cc/2026/Conference/Submission14161/Reviewer_jvdL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14161/Reviewer_jvdL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900667655, "cdate": 1761900667655, "tmdate": 1762924622867, "mdate": 1762924622867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the test‑time scaling of LLMs along two directions. First, it proves a sample‑complexity separation between repeated‑sampling strategies. Self‑consistency needs $\\Theta(1/\\Delta^2)$ samples while best‑of‑n needs $\\Theta(1/\\Delta)$, where $\\Delta$ is the probability gap between the most likely correct answer and the next alternative. Second, it develops a general‑purpose expressiveness framework, showing that a single wider Transformer equipped with verifier feedback can implement online learning over a pool of expert Transformers at inference time, yielding simple‑regret $\\mathrm{reg}(T)$ and final reward within $\\lambda+\\mathrm{reg}(T)$ of optimal. Experiments also illustrate that self‑correction can boost accuracy on a synthetic task and that best‑of‑n with a verifier can outperform self‑consistency on AIME’24/’25 even with far fewer samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper connects strands across CoT scaling and verification and makes a clear theoretical contribution on sampling and self‑correction.\n- The paper has good technical depth and the mathematical statements/proofs are rigorous with matching upper/lower bounds, together with complementary experiments.\n- It's also interesting to have that general‑purpose Transformer constructions manage to route to the correct expert in far less than $K$ trials, which is equivalent to brute‑force trials."}, "weaknesses": {"value": "- The separation results assume a perfect reward for best‑of‑n, the theory does not capture the settings with noisy/imperfect verification.\n- The unified construction of transformer using experts is already engineered to convey the claim that transformer does online learning over a pool of experts with verification, so the conclusion feels built‑in. If it was the other way around (i.e., inductive bias of trained transformer on forming experts), the story would be more convincing.\n- It would be good to have confidence intervals on AIME accuracies with only 4 samples. \n- A full section is devoted to Section 3, yet the technical novelty here is limited as the separation bounds follow directly from known approximation results (w.r.t $\\ell_1, \\ell_2$, etc.) for multinomial distribution."}, "questions": {"value": "- It's not clearly specified what kind of verification feedback is obtained from Qwen3-32B model and how many steps of verification are performed on AIME benchmarks.\n- The expressiveness results depend on an attention‑precision assumption, could you justify why this is necessary?\n- The expressiveness proof depends on $\\epsilon-$thresholded attention assumption, which causes sparsity. Could you justify the necessity of this assumption?\n- I also want to bring to authors' attention that there's a previous work analyzing 3-SAT problems for self-correction, \"Learning to Self-Correct through Chain-of-Thought Verification\", as the relation to that work is not discussed.\n\nI don't have any more questions, please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ncxEtUd2M", "forum": "RJDIX75TEE", "replyto": "RJDIX75TEE", "signatures": ["ICLR.cc/2026/Conference/Submission14161/Reviewer_9kos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14161/Reviewer_9kos"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922584954, "cdate": 1761922584954, "tmdate": 1762924622552, "mdate": 1762924622552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to provide a theoretical foundation for several popular test-time scaling paradigms. The paper's contributions are twofold:\n\nSeparation of Sample Complexity: The paper provides the first rigorous theoretical separation between two repeated sampling strategies: self-consistency and best-of-n (BofN). The authors prove that to achieve the same success probability, self-consistency requires a sample complexity of $\\Theta(1/\\Delta^2)$, whereas BofN only requires $\\Theta(1/\\Delta)$. Here, $\\Delta$ represents the probability gap between the correct answer and the second-most-likely answer. This result theoretically explains why BofN (when equipped with a good verifier) is generally more sample-efficient than self-consistency.\n\nRepresentation Ability of Self-Correction: The paper then analyzes the representation ability of self-correction with verifier feedback. The authors introduce the concept of a \"General-Purpose Transformer\" and prove that a Transformer architecture can be constructed to simulate an online learning algorithm over a pool of \"experts\" at test-time. Specifically, this Transformer can leverage reward feedback from a verifier to adaptively select experts and minimize regret, enabling it to solve multi-task problems without prior knowledge. This extends the known representation theory of Transformers from single-task to multi-task settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work provides a solid theoretical basis for two widely-used but poorly-understood practical heuristics (BofN vs. Self-consistency). The $\\Theta(1/\\Delta)$ vs. $\\Theta(1/\\Delta^2)$ separation result is clear, important, and appears fundamental.\n\n- The framework of a \"General-Purpose Transformer\" and \"test-time online learning\" is a novel perspective. The proof that a Transformer architecture can (by construction) implement regret-minimizing online learning is a significant extension of Transformer representation theory, moving beyond standard \"universal approximator\" or \"Turing complete\" claims.\n\n- The synthetic experiments (on the 3-SAT task) are cleverly designed and successfully demonstrate both (1) the theoretical sample complexity separation and (2) the representation ability of the model to improve its performance via self-correction, validating the paper's core theoretical"}, "weaknesses": {"value": "- The theoretical construction of the \"General-Purpose Transformer\" (Propositions 4.2, 4.4) appears highly complex and relies on a specific \"Generalized Position Encoder\" (Definition 2.2) and attention sink techniques. This feels more like an existence proof (i.e., \"we can construct a Transformer that does this\") rather than an explanation of how existing LLMs might learn this behavior through standard pre-training.\n\n- The proof of self-correction's representation ability relies on a non-standard, more powerful \"Generalized Position Encoder\" that has access to set membership information of preceding tokens. This limits the applicability of this theoretical proof to specific (and perhaps not yet existing) architectures."}, "questions": {"value": "The experiments in Section 5.1 show that larger models (GPT-mini, Gopher-44M) achieve near-perfect accuracy with self-correction feedback. Does this imply that these models are truly simulating the \"online learning over an expert pool\" described in Theorem 4.7? Or is it more likely that they are simply learning a much stronger, but simpler, heuristic (e.g., \"if the answer is 'no', flip the string\")? How do the authors view the gap between this complex theoretical construction and the observed empirical result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "A5bdZqszNJ", "forum": "RJDIX75TEE", "replyto": "RJDIX75TEE", "signatures": ["ICLR.cc/2026/Conference/Submission14161/Reviewer_zTQW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14161/Reviewer_zTQW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101506913, "cdate": 1762101506913, "tmdate": 1762924622149, "mdate": 1762924622149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}