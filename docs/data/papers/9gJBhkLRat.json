{"id": "9gJBhkLRat", "number": 5738, "cdate": 1757930828417, "mdate": 1759897957364, "content": {"title": "Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks", "abstract": "Large Language Models (LLMs) should refuse to answer questions beyond their knowledge. This capability, which we term knowledge-aware refusal, is crucial for factual reliability. However, existing metrics fail to faithfully measure this ability. On the one hand, simple refusal-based metrics are biased by refusal rates and yield inconsistent scores when models exhibit different refusal tendencies. On the other hand, existing calibration metrics are proxy-based, capturing the performance of auxiliary calibration processes rather than the model’s actual refusal behavior. In this work, we propose the Refusal Index (RI), a principled metric that measures how accurately LLMs refuse questions they do not know. We define RI as Spearman’s rank correlation between refusal probability and error probability. To make RI practically measurable, we design a lightweight two-pass evaluation method that efficiently estimates RI from observed refusal rates across two standard evaluation runs. Extensive experiments across 16 models and 5 datasets demonstrate that RI accurately quantifies a model’s intrinsic knowledge-aware refusal capability in factual tasks. Notably, RI remains stable across different refusal rates and provides consistent model rankings independent of a model’s overall accuracy and refusal rates. More importantly, RI provides insight into an important but previously overlooked aspect of LLM factuality: while LLMs achieve high accuracy on factual tasks, their refusal behavior can be unreliable and fragile. This finding highlights the need to complement traditional accuracy metrics with the Refusal Index for comprehensive factuality evaluation.", "tldr": "", "keywords": ["Large language models", "Knowledge-aware refusal", "Factuality evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f3afd552f6e609e6970aab98079d0c83bb904d0.pdf", "supplementary_material": "/attachment/b2c0c79179ff5f67a51db6d36b8df53dc3e7d08b.zip"}, "replies": [{"content": {"summary": {"value": "This paper makes a contribution to the field of LLM evaluation, specifically addressing the critical issue of \"knowledge-aware refusal.\"  Firstly, the authors clearly identify a major gap in existing evaluation methodologies for LLM refusal behavior. Therefore, they propose the primary contribution-the Refusal Index (RI), a rigorously defined metric based on Spearman’s rank correlation between a model's refusal probability and its error probability. This metric is designed to directly and faithfully quantify a model's intrinsic capability to refuse questions beyond its knowledge. The extensive empirical validation across 16 models and 5 datasets demonstrate that RI is stable, consistent, and independent of a model's overall accuracy and refusal rate."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a critical and under-explored problem in LLM reliability—\"knowledge-aware refusal.\" The introduction of the Refusal Index (RI) is a novel and timely contribution that addresses a clear gap in the existing evaluation landscape.\n\n2. The paper provides extensive and compelling experimental evidence to support its claims. Experiments across 16 models and 5 datasets offer a robust demonstration of RI's stability, consistency, and superiority over existing metrics, and further delivers some insightful ideas.\n\n3. The proposed two-pass evaluation method for estimating RI is lightweight and practical. This thoughtful design makes the metric feasible for researchers and practitioners to adopt without requiring excessive computational resources, enhancing its potential impact."}, "weaknesses": {"value": "1. Although the empirical results of this paper is promising, the technical contribution seems not solid and sound. The rationality of the method is not well presented. Therefore, the technical validity is not convincing.\n\n2. The paper is not well written. There are many concepts introduced in this paper. However, these concepts are not rigorously clarified. The details can be found in Questions."}, "questions": {"value": "1. In section 2.1, why choose bivariate gaussian distribution to model the probablilty value of error and refusal. Does some empirical results or previous works support this point? I think it is a rough characterization.\n\n2. How does the rank in the definition of spearman's rank correlation play the role in the estimation of RI?\n\n3. In equation (3), how can we obtain the value of $r_i$ and $w_i$ ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4eeAH5DQuR", "forum": "9gJBhkLRat", "replyto": "9gJBhkLRat", "signatures": ["ICLR.cc/2026/Conference/Submission5738/Reviewer_5y9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5738/Reviewer_5y9a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761362899863, "cdate": 1761362899863, "tmdate": 1762918230303, "mdate": 1762918230303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to evaluate a model’s level of refusal solely based on its output text. To this end, the authors propose a new evaluation metric called the Refusal Index (RI). Unlike traditional metrics that are heavily affected by the refusal rate, RI remains stable across different refusal rates. The paper validates the effectiveness of RI on multiple models and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper observes that traditional metrics are highly affected by the refusal rate and proposes a new metric called RI.\n\n2.The overall writing of the paper is clear and fluent.\n\n3.The experiments are thorough, and the effectiveness of RI is validated across multiple models and datasets."}, "weaknesses": {"value": "1. The paper lacks sufficient explanation of traditional evaluation metrics. In the introduction, the authors briefly mention some of their weaknesses, but it remains unclear what these metrics actually are and why they exhibit such shortcomings. This background is essential for understanding the motivation of your proposed approach. I suggest moving this part to Section 2 and clearly introducing the limitations of traditional metrics before presenting your own.\n\n2. The authors claim that external calibrators, such as verbalized confidence or linear probes, cannot replace direct refusal measurements. However, the rationale for this statement is not clearly articulated. Could the authors elaborate on why these methods are unsuitable? For instance, since we can explicitly train models to output verbalized confidence, it is not immediately clear why such signals cannot serve as a proxy for model confidence or be used in place of direct refusal measures.\n\n3. Starting around line 129, the definition of the key notion raises potential confusion: are refusals also counted as errors? This point needs clearer explanation. Later sections suggest that each question has its own error rate (and refusals are re-answered for measurement), but this is not obvious when first introduced. Additionally, in Table 1, the formula c/(1–r) is unclear — does c represent the number of correct answers among the non-refused cases? Please clarify this.\n\n4. The proposed metric seems conceptually related to AUROC, which also reflects the consistency between confidence and ability. This is similar to your statement that “its refusal probability increases monotonically with error probability.” Could the authors explain more explicitly how their metric fundamentally differs from AUROC? Why should I use RI instead of AUROC? Refusal probability can also be reflected through confidence scores, rather than being binarized into a simple “refuse or not” decision.\n\n5. The “refusal tendency” appears analogous to a fine-grained confidence estimation, while the “error tendency” seems related to accuracy over multiple responses. Have the authors explored this connection? Why not estimate model confidence directly and then threshold it according to user preferences, instead of measuring refusal explicitly.\n\n6. Since the proposed metric relies on Gaussian estimation, I am concerned about its robustness under limited sample sizes. How accurate is the estimation in such cases? Furthermore, is the Gaussian assumption itself empirically justified?\n\n7. The choice of baselines could be further discussed. For fine-grained confidence, AUROC and ECE are typically appropriate. For binary confidence, accuracy (as a measure of ability) and alignment (whether refusal matches correctness) might be more relevant.\nClarifying why your chosen baselines are suitable would strengthen the experimental section.\n\n8. While the paper discusses the influence of refusal rate, accuracy also substantially affects metric behavior. For example, AUROC can appear artificially high under extremely imbalanced accuracy (e.g., only 10 correct samples out of 1000). This suggests that model ability significantly impacts evaluation. I am also curious about the authors’ perspective on how model competence affects alignment — if a model is either very strong or very weak, learning when to refuse may become trivially easy. Is this an expected or desirable property?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y0fMcGRONg", "forum": "9gJBhkLRat", "replyto": "9gJBhkLRat", "signatures": ["ICLR.cc/2026/Conference/Submission5738/Reviewer_y9ka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5738/Reviewer_y9ka"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634155312, "cdate": 1761634155312, "tmdate": 1762918229695, "mdate": 1762918229695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper investigates knowledge-aware refusal in LLMs—the ability to refuse questions they are unlikely to answer correctly while not refusing questions they can answer. The authors identify that existing metrics like correctness conditioned on non-refusal are easily biased by refusal tendency and manipulated through system prompts.\n- The authors propose the Refusal Index, which measures the Spearman correlation between refusal probability and error probability. They develop an efficient estimation procedure using Gaussian copula fitting that requires only two samples per question, making it tractable compared to expensive sampling-based calibration methods.\n- The paper validates this metric by showing: (1) stability across different prompts for refusal tendencies, (2) clean correlation with sampling-based calibration methods, and (3) consistent model rankings across evaluation settings.\n- Using the validated metric, the authors show: (1) cautious prompts increase refusal rates but do not improve calibration, (2) the Refusal Index is largely independent of model capability and aligns more with model family, and (3) removing or adding misleading context degrades refusal calibration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The Refusal Index captures something fundamental about a model's calibration and remains stable across different prompting strategies. \n- Despite the mathematical complexity, the metric requires only two samples per question—nearly as cheap as computing accuracy. \n- RI has strong correlation with expensive sampling-based calibration metrics \n- Interesting and diverse experimental results. The finding that refusal calibration is largely independent of model capability and instead aligns with model family is particularly striking—it suggests calibration may be a distinct dimension of model quality worth optimizing separately."}, "weaknesses": {"value": "- The mathematical presentation of RI feels dense. Figure 1 hints that the Refusal Index captures convexity of the curve in refusal rate vs. correct answer space, and further developing this intuition and/or motivating the Gaussian copula fit could aid clarity.\n- Greater discussion on whether the Refusal Index measures something fundamentally different than sampling-based calibration methods, or simply serves as a more sample-efficient proxy, would be helpful. The appendix contains interesting results on sample efficiency—a direct head-to-head comparison showing how much more sample-efficient RI is than naive calibration-based metrics would better motivate its advantages.\n- The stability results in Table 2 focus on Qwen and Mistral models, but Figure 4 shows Gemma-3-12b's Refusal Index varying considerably across prompts (roughly 0.1 to 0.3). It's unclear whether Gemma is an outlier or if this variation is typical. Showing both the Figure 4 analysis and Table 2 stability results on a broader, consistent set of models would clarify how stable the metric actually is in practice. This concern applies more broadly to later experiments—either evaluate more models consistently or be more intentional about which models are presented and why.\n- The frontier model evaluation figure is interesting, but it's not possible to identify which specific model corresponds to each data point—only the model family is discernible, not the generation or size."}, "questions": {"value": "- How is stability computed in Table 2? Could differences in distribution concentration affect the apparent variability of different metrics? What level of RI variation across prompts (e.g., 0.1 to 0.3 for Gemma-2-9b) should be considered acceptable? Would it be possible to show empirical data points on iso-RI curves (as in Figure 3) for more models? This would help clarify whether the Gemma-2-9b variability pattern is typical.\n- The finding that RI aligns with model family rather than capability is interesting, but it's difficult to identify specific models in Figure 5. Could you provide clearer labels or a table showing individual model RI scores?\n- Is RI measuring something fundamentally different from sampling-based calibration methods, or is it primarily a more sample-efficient approximation? A naive alternative would be to estimate RI through extensive sampling without Gaussian copula fitting—how would this compare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nwhmHHQjY5", "forum": "9gJBhkLRat", "replyto": "9gJBhkLRat", "signatures": ["ICLR.cc/2026/Conference/Submission5738/Reviewer_hVoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5738/Reviewer_hVoC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723336610, "cdate": 1761723336610, "tmdate": 1762918229378, "mdate": 1762918229378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the blackbox approach to factual knowledge, the authors point out that current methods are limited. They will mostly look at the rejection rate and/or correct answer rate. Here, the authors propose to check whether those two things happen together, beyond chance. This method additionally avoids a lot of sampling which is often relied on for similar approaches. They provide extensive empirical testing on different models and datasets, and discuss model variations. They also check effect of prompt variation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Reduces a true gap in a very empiric field. As an empirical study it is quite solid, checking a variety of models, datasets, but also checking the effect of prompt variation to some extent. \n\nThe empirical section is not only quantitatively very strong, but actually makes good use of it's data. Datasets and models are not only listed, but are used to make compelling arguments on different effects. Figures are clear and very understandable. Multiple appendices make the effort of testing many variations of the setup to ensure it is correctly validated."}, "weaknesses": {"value": "An issue I find generally in the blackbox tradition of model factual knowledge is that a lot of definitions are arbitrary. \nI would worry that these poorly defined targets of \"checking if a model knows\" or \"checking if a model answers when it knows\" are moving goalposts which lead to incremental progress to evaluate models which purposedly (hence blackbox) do not provide the required information to properly move forward.\n\nWere this a journal paper in a major venue, I would ask to rework the definition of \"knowledge\". As this is a conference paper in a major venue I can only notify that this definition is very shaky (we are not discussing observable correct answer for a factual question, but a more interesting but very abstract notion of \"knowing\" a fact). I nonetheless acknowledge that this paper is taking a step in the right direction by decoupling model behaviour of refusal to answer from model knowledge, and this is why I've set a positive score. \nI remain nonetheless worried that definitions are not well set - much like in previous empirical works, there is no clear gold standard for \"knowing\". Reasoning then becomes somewhat circular - we empirically define knowing as RI, and then show that it is better than previous methods which had set different definitions. \n\nAlong the same line the authors criticize the notion of proxy metrics. I did not understand how this new RI method is not a proxy metric, even if a better one. \n\nOn a much less important note, I've noted moments where I was confused reading. Should they seem personal, feel free to ignore them.\n* Line 015/016 : \"simple refusal based metrics are biased by refusal rates and yield inconsistent scores when models exhibit different refusal tendencies\" --> confusing\n* starting L050: I was confused again by the third paragraph of the introduction.\n\nFor both of those I only understood what was going on from the examples in l110 onwards which made your point as well as the difference between refusal based metric, refusal bias, refusal tendencies, and refusal itself as a concept much clearer. I would advise either clarifying earlier, or rephrasing.\n\n* 024 \"RI accurately quantifies a model's intrinsic knowledge-aware refusal rates capability in factual tasks.\" --> intrinsic knowledge aware refusal rates is not defined later in the paper, and confusing here"}, "questions": {"value": "1) could you please re-explain why you consider calibration a proxy, and not RI?\n\n2) L151/152 \"While overall refusal rates can be adjusted through input context or preference learning, the discriminative capability for knowledge-aware refusal remains more robust and consistent\" - I am confused by this statement. How can changing refusal rates be more consistent than discriminating? more consistent for what? I think I understand your point that refusal rates are not the only thing we want to act on - but I don't think RI as a metric is acting on anything.\n\n3) more of a personnal curiosity point: is there a reason why you are using Spearman rank correlation rather than another?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GkSLLbVE2J", "forum": "9gJBhkLRat", "replyto": "9gJBhkLRat", "signatures": ["ICLR.cc/2026/Conference/Submission5738/Reviewer_Zb1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5738/Reviewer_Zb1L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934828107, "cdate": 1761934828107, "tmdate": 1762918228901, "mdate": 1762918228901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}