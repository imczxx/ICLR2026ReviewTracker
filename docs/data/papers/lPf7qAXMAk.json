{"id": "lPf7qAXMAk", "number": 3120, "cdate": 1757338137338, "mdate": 1763044411461, "content": {"title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models", "abstract": "Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment.\nIn this work, we propose TTRV to enhance vision–language understanding by adapting the model on-the-fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times.\nFurther, we also propose to control the diversity of model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution.\nOur approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to $52.4$% and $29.8$%, respectively, and average boosts of $24.6$% and $10.0$% across $16$ datasets. \nRemarkably, on image recognition, TTRV applied to Intern-VL-8B surpasses GPT-4o by an average of $2.3%$% over $8$ benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. \nFinally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to $5.5$% in recognition tasks.", "tldr": "We introduce the first Test-time Reinforcement Learning Framework for Vision Language Models.", "keywords": ["VLMs", "LLMs", "Test-time-adaptation", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/64507dd9c3643614bac5bbcdc2225ea02a84b293.pdf", "supplementary_material": "/attachment/f3acff5cc6a144324c41d49f136429973217f212.zip"}, "replies": [{"content": {"summary": {"value": "TTRV introduces a new paradigm of test-time reinforcement learning (RL) for Vision-Language Models (VLMs). Instead of fine-tuning on labeled datasets, the model self-adapts during inference by performing RL updates based on unsupervised reward signals derived from its own outputs. The authors design two rewards:\n1. Frequency reward—encouraging consistent outputs across rollouts, and\n2. Diversity control reward—penalizing over-confident or repetitive predictions.\nThe model uses Group Relative Policy Optimization (GRPO) to perform these updates at inference. This yields significant improvements across 16 datasets (up to +52% accuracy on recognition and +29% on VQA) and sometimes surpasses GPT-4o."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel paradigm:\nShifting RL from training to inference time is a radical and creative idea. It opens up self-improving model behavior at deployment without retraining.\n2. Empirical strength: The reported gains—particularly in zero-shot recognition—are large and consistent.\n3. General applicability: The method is architecture-independent and compatible with various pretrained VLMs (Qwen2.5-VL, InternVL3, etc.).\n4. Interpretability: The frequency and diversity rewards are intuitive and explainable, giving transparency to the adaptation process.\n5. Practicality: Test-time adaptation can be done on unlabeled data, an important benefit for real-world deployment.\n6. Strong motivation and clear writing:\n7. The paper builds its case logically from human analogy—how humans improve from feedback even after “training.”"}, "weaknesses": {"value": "1.\tHeuristic reward design:\nThe rewards (frequency/diversity) are empirical and lack theoretical justification. Their combination could yield unstable gradients.\n2.\tRisk of overfitting:\nPer-instance adaptation may bias the model to recent examples, harming generalization.\n3.\tComputational cost:\nRequiring multiple rollouts per sample increases inference time substantially, undermining efficiency.\n4.\tLimited comparison with test-time training (TTT):\nMethods like Tent and Entropy Minimization are not compared, missing key baselines.\n5.\tNo memory mechanism:\nThe adaptation is local to each test input; there is no evidence of accumulation of learned behavior over time.\n6.\tAblation limitations:\nThe contribution of each reward term is shown but not systematically studied under noise or domain shift"}, "questions": {"value": "1.\tHow does TTRV compare to entropy-based test-time training (e.g., Tent, 2023)?\n2.\tDoes adaptation persist across samples or reset after each inference?\n3.\tHow is reward scaling handled to avoid gradient explosion?\n4.\tWhat is the average latency increase per test sample?\n5.\tCould the method degrade in open-ended generative tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GaeQv0Mh3G", "forum": "lPf7qAXMAk", "replyto": "lPf7qAXMAk", "signatures": ["ICLR.cc/2026/Conference/Submission3120/Reviewer_ZgqZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3120/Reviewer_ZgqZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400591002, "cdate": 1761400591002, "tmdate": 1762916559193, "mdate": 1762916559193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "REnh4QD0NA", "forum": "lPf7qAXMAk", "replyto": "lPf7qAXMAk", "signatures": ["ICLR.cc/2026/Conference/Submission3120/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3120/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763044410807, "cdate": 1763044410807, "tmdate": 1763044410807, "mdate": 1763044410807, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for test-time RL for VLM models. Unlike traditional RL framework for VLM model that require post-training on a labeled dataset, this paper proposed to directly conduct RL on the testing dataset without any label. To enable this, they proposed to combine a frequency-based and entropy-based reward function. Evaluation performance on a decent amount of image recognition and VQA benchmarks shows the effectiveness of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting idea to extend TTRL to VLM with entropy loss.\n\nThis paper extends TTRL framework to VLMs with an explicit entropy regularization term. Although this might seem to be a small modification, as shown in Table 3, the combination of frequency and entropy reward leads to significantly better performance. I believe this suite of aware would be helpful for future research.\n\n2. Strong performance on the benchmarks.\n\nThe proposed method shows consistent strong improvements across a diverse set of VLM benchmarks, indicating this approach is not limited to a single task or model family, as shown in Table 1 and Table 2. This makes the conclusion of the paper more convincing.\n\n3. Extensive experiments with clear ablation studies\n\nThis paper provides clear ablations (with/without entropy, different VLM base models, using random rewards) to further prove the effectiveness of TTRV in different perspectives. I think this study would be very interesting to many researchers in the area.\n\n4. Interesting result to show effectiveness with only 1 sample.\n\nKind of surprisingly, the method also show meaningful improvement even with a single unlabeled sample, suggesting practical value for on-the-fly training for the testing data."}, "weaknesses": {"value": "1. Might be hard to transfer this method to open-ended question answering tasks.\n\nThe method's frequency-based reward is best aligned with closed-form answers, where you can meaningfully compute the distribution of answer. For open-ended or multi-valid responses, this reward might have limited usefulness. It would be helpful to discuss possible ways to extend to these tasks and have discussions of this limitation in the paper.\n\n2. Lack of discussion of how to deal with the potential risk of circulating the errors in the model.\n\nBecause rewards are derived from the model's own sampled outputs, there is a risk of amplifying initial errors through this self-reinforcement stage and suppressing diversity under strong entropy pressure. It would be helpful to discuss some failure cases of when this happens, or explain why this would rarely happen in real applications."}, "questions": {"value": "How does doing the RL with the same loss on the training data work? Would the model also benefit from this? This would be important for the case of single-example setting of TTRV, because if the method work better with more examples from the training data, there is no point of training it on a single testing example."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kSFcaFGYVK", "forum": "lPf7qAXMAk", "replyto": "lPf7qAXMAk", "signatures": ["ICLR.cc/2026/Conference/Submission3120/Reviewer_oFjH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3120/Reviewer_oFjH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600868342, "cdate": 1761600868342, "tmdate": 1762916559036, "mdate": 1762916559036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission proposes TTRV, a test‑time reinforcement learning framework for VLMs. For each test prompt, the model samples candidate responses, forms an empirical distribution over the unique outputs, and uses two unsupervised rewards:\n\n- a frequency reward proportional to empirical response frequency;  \n- a diversity‑control term that rewards low entropy of the empirical distribution.\n\nThe final reward is optimized with GRPO during inference. Experiments on 16 datasets (8 recognition, 8 VQA) report very large gains, e.g., InternVL3‑2B on ImageNet from 56.0% to 98.31% after adapting on 20 unlabeled test samples per dataset, with additional ablations and latency measurements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Simple recipe with fixed hyper‑parameters ((N=32), (alpha=0.75)), pseudocode, and broad experimental surfac.\n- Negative‑control check on random rewards and some cross‑dataset transfer."}, "weaknesses": {"value": "1) Incremental technical contribution. The method is essentially a direct and superficial combination of test-time RL [1] and VLM architectures. The reward formulation mirrors TTRL’s majority-vote reward with a trivial entropy reward. This is far from a substantial conceptual advance.\n\n2) Lack of baseline. There is no self‑consistency/best‑of‑\\(N\\) baseline (same decoding budget/temperature, etc.). The paper also does not position TTRV against R1‑style VLM RL (MM-EUREKA [2] , Vision-R1 [3], Perception‑R1 [4]). Those works leverage verifiable rewards and report nuanced phenomena (e.g., reward hacking, data quality effects) that bear directly on claims here.\n\n3) Converting recognition to 4‑way MCQ and standardizing VQA to MCQ (Appendix A) changes the task. Massive leaps (ImageNet to ~99%) likely reflect selection‑from‑options under heavy sampling rather than genuine classification ability.\n\n4) Main results focus on InternVL; a small Qwen2.5‑VL check appears only in ablations. Given the 2025 Spurious Rewards [5] findings, strong claims about test‑time RL should be demonstrated on diverse model families in the main table.\n\n[1] TTRL: Test-Time Reinforcement Learning (arXiv:2504.16084) \n\n[2] MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with Rule-based Reinforcement Learning (arXiv:2503.07365)\n\n[3] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models (arXiv:2503.06749)\n\n[4] Perception-R1: Pioneering Perception Policy with Reinforcement Learning (arXiv:2504.07954)\n\n[5] Spurious Rewards: Rethinking Training Signals in RLVR (arXiv:2506.10947)"}, "questions": {"value": "1. How does TTRV differ fundamentally from TTRL beyond the input modality and entropy term?\n2. Can you show compute-matched self-consistency baselines (same (N)), and more R1-style VLM baselines?\n3. How does TTRV behave on different model families?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XONf0fAVVP", "forum": "lPf7qAXMAk", "replyto": "lPf7qAXMAk", "signatures": ["ICLR.cc/2026/Conference/Submission3120/Reviewer_uzF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3120/Reviewer_uzF6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807395662, "cdate": 1761807395662, "tmdate": 1762916558859, "mdate": 1762916558859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TTRV, a test-time RL procedure for decoder-style VLMs. For each test prompt, the model samples N outputs, forms an empirical distribution over responses, and uses a frequency‑based reward plus an entropy (diversity) reward inside GRPO to update the policy on the fly.\n\nThe paper shows good performance improvement and also studies ablations, sample efficiency, cross‑dataset transfer, and latency. Implementation details are clear."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Good extension of test time training to VLM. Simple, general mechanism (self‑consistency + GRPO) that can be bolted onto existing decoder‑VLMs without labels. The pipeline is easy to get.\n\nLarge empirical gains across diverse domains, including OOD classification and several VQA benchmarks. The method often remains effective even with 1 adaptation sample."}, "weaknesses": {"value": "The main weakness is experiments miss other test time training baselines. For example, a simple test‑time entropy minimization baseline. \nThis would be crucial to understand the exact value of TTT on GRPO."}, "questions": {"value": "The paper mention the trade-off between convergence and diversity using two rewards.\nMeanwhile, I am still confused why that is the case?\n\nSpecifically \"explores diverse reasoning modes initially (as encouraged by the frequency-based reward)\". Why that is the case? The frequency read gives higher reward to responses that appear more often, would that do the exact opposite?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Sbx18FxOc", "forum": "lPf7qAXMAk", "replyto": "lPf7qAXMAk", "signatures": ["ICLR.cc/2026/Conference/Submission3120/Reviewer_tPxN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3120/Reviewer_tPxN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051949977, "cdate": 1762051949977, "tmdate": 1762916558674, "mdate": 1762916558674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}