{"id": "0TmVqOpBbK", "number": 14856, "cdate": 1758244736581, "mdate": 1759897345019, "content": {"title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs", "abstract": "Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1\\% higher accuracy and 26\\% greater inference throughput compared to LLaMA-3.2.", "tldr": "", "keywords": ["Scaling Laws", "Model Architecture", "Inference-Efficient"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a4ed529cb2a7033414d55d230305e7d85a8c722.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a challenge in the efficient deployment of large language models. Whiile existing scaling laws guidfe the trade-off between model size and training data for optimal accuracy, they largely ignore inference efficiency. This work bridges the gap by investigating how architecture hyperparameters impact inference throughput and model accuracy, including hidden size, mlp-to-attention ratio, and GQA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel and practical contribution**: The focus on inference scaling law is highly relevant for the practical deployment of LLMs. The proposed conditional scaling law is a clever and effective extension to the scaling law.\n2. **Rigorous and extensive experiments**: The scale of the study is impressive, with over 200 models trained across different parameter scales. This provides a solid empirical foundation for their claims."}, "weaknesses": {"value": "1. **The design for the conditional scaling law is lack of theoretical support**. The paper doesn't present why the conditional scaling law is fomulated as a multiplicative or additive equation. Why these two forms act similar? Are they still similar if the candidate models are much larger (e.g., 70B+)? Which one should we take in the future research?\n2. **Lack of a clear summarization**: It is encouraged to present an summarization on the conclusions about how to design an efficient LLM architecture. It is also encouraged to present an comparison on the design choices of existing open source models about which models are closer to the optimal design.\n3. **Lack of analysis on GQA**: Why the relationship between loss and GQA is highly fluctuating? This conclusion is counterintuitive and requires a discussion."}, "questions": {"value": "The questions are listed in the Weaknesses. I believe this direction is worth researching and this paper will be a good start if the above problems are well solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gXmqEHLAIy", "forum": "0TmVqOpBbK", "replyto": "0TmVqOpBbK", "signatures": ["ICLR.cc/2026/Conference/Submission14856/Reviewer_qzeo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14856/Reviewer_qzeo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760958074312, "cdate": 1760958074312, "tmdate": 1762925209446, "mdate": 1762925209446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits scaling laws for large language models (LLMs) through the lens of model architecture and inference efficiency.\nWhile existing works (e.g., Chinchilla) focus on training efficiency by relating model parameters, data tokens, and loss, this work introduces architecture-aware scaling laws that explicitly incorporate structural hyperparameters affecting inference cost.\n\nThrough a large empirical study (>200 models from 80M–3B parameters trained on 8–100B tokens), the paper shows that performance and inference throughput follow predictable trends governed by these architecture variables. Using these fits, the authors predict “optimal” configurations for new small models (e.g., Panda-1B, Panda-3B) that achieve ~26% higher inference throughput and ~2% higher accuracy than LLaMA-3.2-1B."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel perspective: Introduces an architecture-aware formulation of scaling laws, bridging the gap between training efficiency and inference efficiency.\n2. Extensive empirical coverage: The study includes over 200 model configurations across hidden sizes, attention/MLP ratios, and token budgets.\n3. Practical insight: Demonstrates that scaling law fits from small models generalize to larger ones, potentially guiding efficient architecture search."}, "weaknesses": {"value": "1. Empirical “optimality” is descriptive: The proposed “conditional scaling law” is fitted post-hoc to data rather than derived from optimization principles.\n2. Hardware dependence: Reported inference efficiency improvements rely on specific implementation details (vLLM, A100), which may not generalize to other hardware or serving stacks."}, "questions": {"value": "1. How sensitive are the predicted “optimal” ratios to the hardware platform (A100 vs. H100 or TPU)? Could authors provide several experiments on another hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b65BIr6RN9", "forum": "0TmVqOpBbK", "replyto": "0TmVqOpBbK", "signatures": ["ICLR.cc/2026/Conference/Submission14856/Reviewer_Bn1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14856/Reviewer_Bn1c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607803211, "cdate": 1761607803211, "tmdate": 1762925208836, "mdate": 1762925208836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical and underexplored trade-off between the accuracy and inference efficiency of Large Language Models (LLMs). While traditional scaling laws focus on training compute, parameters (N), and data (D), they largely ignore the architectural choices that significantly impact inference cost, which is a dominant expense in real-world deployment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed conditional scaling law is a novel extension of established Chinchilla scaling laws12. By formulating the loss as a separable, calibrated function of architectural ratios relative to a Chinchilla-optimal reference loss, the authors provide a practical tool for architecture design.\n\nBesides, the claims are substantiated by a large-scale empirical study involving over 200 trained models, spanning a wide range of parameter counts (80M to 3B) and token budgets (8B to 100B). This comprehensive dataset provides a strong foundation for fitting the scaling laws."}, "weaknesses": {"value": "1. The experiments are validated up to 3B parameters. While this is a substantial undertaking, the authors acknowledge that the evaluation does not extend to the 7B+ scale, which is a very common size for deployed open-source models.\n2. The entire analysis is confined to dense decoder-only transformers. It is unclear if these findings, particularly the U-shaped loss curves and the separability assumption, would hold for other popular architectures like Mixture-of-Experts (MoE).\n3. The study finds that GQA significantly impacts inference efficiency (Figure 9) but has a \"highly fluctuating\" and inconsistent relationship with training loss (Figure 13). Consequently, GQA is not integrated into the conditional scaling law itself. Instead, it is handled via a separate \"local search\" after other parameters are optimized. This feels less principled and integrated."}, "questions": {"value": "The authors note the \"highly fluctuating\" relationship between GQA and loss (Figure 13)33. Do you have a hypothesis for why GQA behaves so erratically compared to the smooth curves for $d_{model}$ and $r_{mlp/attn}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pd6R2REmat", "forum": "0TmVqOpBbK", "replyto": "0TmVqOpBbK", "signatures": ["ICLR.cc/2026/Conference/Submission14856/Reviewer_rBcz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14856/Reviewer_rBcz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831225399, "cdate": 1761831225399, "tmdate": 1762925208452, "mdate": 1762925208452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work builds on the Chinchilla scaling laws by adding architecture-based dimensions like hidden size, MLP-to-attention ratios, and grouped-query attention, to analyse the trade-off between inference efficiency and downstream accuracy. Authors fit a conditional scaling law on >200 Llama-style models (80M-30B #params), authors identify Pareto-optimal configurations and improve Panda/Surefire models that improve downstream accuracy and inference throughput over Llama-3.2 baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Conditioning the Chinchilla scaling laws on hidden size and MLP-to-attention ratios is timely, useful, and interesting\n- Experiments over 200+ trained models provide robust empirical results\n- Architectures suggested by the new scaling laws yield sensible gains -- Panda models raise average zero-short accuracy, while Surefire significantly improves the inference throughput by up to ~25%"}, "weaknesses": {"value": "- Results stop at 3B parameters -- what happens at larger model scales?"}, "questions": {"value": "- What happens for larger models? Do the authors have architectural recommendations for these regimes?\n- The paper's results are based on vllm -- do results transfer also to e.g. SGLang (https://github.com/sgl-project/sglang)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zlIQEVYkFj", "forum": "0TmVqOpBbK", "replyto": "0TmVqOpBbK", "signatures": ["ICLR.cc/2026/Conference/Submission14856/Reviewer_fQwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14856/Reviewer_fQwj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762191679968, "cdate": 1762191679968, "tmdate": 1762925207841, "mdate": 1762925207841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}