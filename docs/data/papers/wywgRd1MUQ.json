{"id": "wywgRd1MUQ", "number": 10070, "cdate": 1758159615223, "mdate": 1759897676142, "content": {"title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners", "abstract": "Multimodal Large Language Models (MLLMs) have shown significant promise in powering Graphical User Interface (GUI) agents to automate complex digital tasks. However, the prevailing monolithic training paradigms often create a structural mismatch with the hierarchical nature of capabilities required for robust performance. Specifically, the efficacy of methods like Reinforcement Learning (RL) is critically predicated on the agent possessing a high-quality behavioral prior of key reasoning skills, such as spatial reasoning and goal decomposition, which are often absent. To resolve this impasse, we propose Actor2Reasoner, a novel two-stage hierarchical training paradigm grounded in the principle of Endow First, Internalize Later. The first stage, Cognitive Endowment, employs targeted supervised fine-tuning to instill these crucial thinking patterns, forging a Capable Actor. Subsequently, the second stage, Policy Internalization, utilizes RL to evolve this actor into a Deliberative Reasoner by internalizing the endowed abilities into a robust, context-aware decision-making policy. We instantiate our paradigm in InfiGUI-R1, an agent that achieves state-of-the-art performance on challenging benchmarks, including AndroidControl. Our work demonstrates that decoupling the endowment of foundational abilities from the internalization of policy provides a more effective and principled path toward developing sophisticated and resilient GUI agents.", "tldr": "", "keywords": ["GUI Agents", "MLLMs", "Reinforcement Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ae571618c77a70768a63fc9176d33fa437a5eff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce InfiGUI-R1, a vision-language-action model to interact with user interfaces and accomplish tasks. The model is trained in a 2-stage training pipeline, first with supervised fine tuning over a clean action dataset, and second with reinforcement learning over trajectories that are “just hard enough” so as to remain on the learning frontier of the model. The model is trained with several different objectives depending on the task at hand, varying from bounding-box IoU to sub-goal matching to action prediction. As part of the learning process, the authors also propose a sub-goal prediction/matching task, and an error correction task. These are both claimed to help improve final performance.\n\nInfiGUI-R1 is compared to much larger models on ScreenSpot benchmarks for grounding performance, and on AndroidControl and AndroidWorld benchmarks for interaction performance. The model achieves comparable or superior performance when compared to baselines, despite being smaller. The authors also conduct an ablation study in which they remove either their SFT phase or their RL phase, and they show that both are necessary to the success of the final model. Ablations on the subgoal and error recover tasks seem to indicate that they are less important to the overall success of the pipeline, with the “no error recovery” model even outperforming the full model along some metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The final performance of the InfiGUI-R1 model performs very well, achieving superior performance than larger models\n+ The proposed pipeline is intuitive and reflects many sensible design decisions, and would be easy to replicate by other researchers in their own workflows.\n+ The experiments evaluate InfiGUI-R1 along several important axes, showing results for different GUI instantiations (mobile/web/desktop), grounding performance on different tasks/applications, and task success on different benchmarks. These greatly help to illustrate the strengths of the model, and to contextualize it in the scope of related work."}, "weaknesses": {"value": "- **Novelty**: SFT into RLVR is not a particularly new training paradigm. Every major LLM since ChatGPT has featured some combination of these two training paradigms in this order, and so the insight to apply SFT and then RLVR for GUI navigation is not particularly novel or insightful. In fact, one of the baselines in the paper, UI-TARS, employs an SFT-> DPO training pipeline, which is of course very similar. Similarly, many vision-language-action models in the robotics community have already demonstrated the success of SFT -> RLHF for task learning with VLMs.\n- Clarity: Elements of the paper are left unclear. For example, the authors introduce either a point-based reward or a bounding-box reward for certain UI tasks, but then there is no comparison between these two, or discussion on when to use each. This does not help future work to build on anything that the authors may have learned from experimenting with these rewards for UI navigation or grounding. Similarly, elements of the paper are left slightly unclear, such as how the Accuracy reward is normalized to [-1, 1], but sometimes it is set to the sum of 3 rewards ranging from [0, 1]… So it should range from [0, 3]. Is this reward then re-normalized to [-1, 1]? If so, is it a valid learning signal?\n- (Minor) Formatting: nearly all of the opening quotes are backwards, and some of the closing quotes are backwards. The paper could use a formatting check.\n\nWhile does not seem to be anything technically wrong with the paper and the results are fairly strong, the contributions are minimal. The techniques being used, like data filtering, SFT -> RL, and RLOO, are all well established in the literature at this point, and so there is not much new being added. Nonetheless, the results are strong, and so I lean towards weak accept."}, "questions": {"value": "The error recovery component seems to be both very trivial to solve (hit a back button) and also somewhat damaging to performance (as evidenced by the occasionally higher performance of the InfiGUI method that doesn’t use error recovery in training). Does this suggest that that task is bad for helping to learn general GUI navigation? Or is there something else that explains why the method does better without error recovery?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TmqjtvRmzc", "forum": "wywgRd1MUQ", "replyto": "wywgRd1MUQ", "signatures": ["ICLR.cc/2026/Conference/Submission10070/Reviewer_BcjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10070/Reviewer_BcjR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760730041215, "cdate": 1760730041215, "tmdate": 1762921462932, "mdate": 1762921462932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Actor2Reasoner, a novel hierarchical training paradigm for GUI agent learning. The authors identify a structural mismatch between existing monolithic training approaches and the hierarchical nature of capabilities required for GUI tasks, proposing a two-stage training methodology based on the principle of \"Endow First, Internalize Later.\" The first stage, Cognitive Endowment, employs supervised learning to instill core cognitive abilities such as spatial reasoning and goal decomposition. The second stage, Policy Internalization, uses reinforcement learning to internalize these abilities into a robust decision-making policy. The approach is instantiated in InfiGUI-R1, which achieves SOTA performance on benchmarks including AndroidControl."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated and easy-to-read.\n- Performance evaluation across diverse benchmarks (AndroidControl, GUI-Odyssey) with detailed ablation studies effectively demonstrates the method's effectiveness. The experimental results showing synergistic effects between the two stages are particularly compelling."}, "weaknesses": {"value": "- The main content of this paper is a two-stage learning algorithm where the first stage learns relatively general abilities through a teacher model, and the second stage learns GUI agent tasks through reinforcement learning. There are various papers using such two-stage structures [1, 2], and when reading this paper, it is difficult to understand what specifically changes for GUI environments beyond the reward structure.\n- In particular, the implementation of this two-stage approach seems to consist entirely of the detailed design of CoT and rewards for GUI agents in Sections 3.1-3.2, which appears to be the entirety of the framework.\n- There is insufficient theoretical analysis of why the two-stage approach is more efficient than a single-stage approach. Analysis of learning complexity or sample efficiency would have made a stronger contribution.\n- Detailed appendices about datasets or implementation are not included.\n\n[1] Song, Huatong, et al. \"R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\" arXiv preprint arXiv:2503.05592 (2025).\n\n[2] Liu, Zijia, et al. \"Time-R1: Towards Comprehensive Temporal Reasoning in LLMs.\" *arXiv preprint arXiv:2505.13508* (2025)."}, "questions": {"value": "- In the proposed methodology, does the effect of Cognitive Endowment diminish when using a base model that is already large enough to have sufficient reasoning performance?\n- To accurately compare with UI-TARS, have you also applied the proposed method to Qwen2-VL? Or, could you provide the performance of Qwen2-VL and Qwen2.5-VL?\n- Are the abilities learned in the Cognitive Endowment stage well maintained without catastrophic forgetting during the Policy Internalization stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8RPnkvEfkJ", "forum": "wywgRd1MUQ", "replyto": "wywgRd1MUQ", "signatures": ["ICLR.cc/2026/Conference/Submission10070/Reviewer_sQ4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10070/Reviewer_sQ4b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540548476, "cdate": 1761540548476, "tmdate": 1762921462529, "mdate": 1762921462529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to provide a methodology to improve an MLLM’s ability to perform digital GUI-based tasks. The paper highlights an important issue that when prompted to perform tasks in a GUI, LLMs struggle due to challenges faced in spatial localization and goal decomposition. They propose a two-step training paradigm to first build a base model with improved localization and grounding capabilities, and then operationalize these faculties through reinforcement learning. The authors validate their approach on three benchmarks to highlight the utility of their proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors address an important problem regarding improving the capabilities of MLLM agents navigating a stochastic digital world.\n- The authors conduct experiments on an expansive set of baselines to cover the breadth of existing approaches to solve this problem.\n- I appreciated the authors efforts to ensure high-fidelity for their synthetic data via a self-verification based filtering step."}, "weaknesses": {"value": "- The writing describing the technical details in the approach presented needs to be improved.\n    - While explaining the reward structure, it is unclear how the final reward is constructed. The author introduces various components of the reward, such as R_sub or R_esc, however, these terms are not included in the final description of the reward.\n    - The final reward in equation-6 was defined as R(a,B), however the training reward was previously defined as R_total. Are these equivalent? Furthermore, the “B” variable in the reward function has not been defined.\n    - In Section 3.3.3, what is the specific algorithm utilized for reinforcement learning? Reinforcement Learning with Verifiable Rewards is a very broad class of methods, which could involve a variety of policy-gradient based approaches, i.e. GRPO, PPO, DPO, etc. The authors need to more clearly describe the approach they selected and the motivation behind their selection. Furthermore, if the authors adopted an off-the-shelf algorithm, I would recommend that they move this information to an experiments or preliminaries section as it is not a novel part of their technical approach.\n- I think the authors have some positive findings in their experimental results. However, their reporting lacks detail, and some important results are not reported/explained.\n    - It is concerning that the authors did not discuss Table-1 at all in their discussion of their results. On this benchmark, many of the baseline approaches outperform InfiGUI-R1, countering the claims in the discussion.\n    - Ablations seemed to be cherry picked, as they have been reported only for two of the four benchmarks\n    - The results of the other baselines are absent in Table-4, while reporting the success rate of InfiGUI on the AndroidWorld benchmark. Since there was a significant drop in InfiGUI’s performance, compared to AndroidControl, I would be interested in contextualizing this performance drop compared to other methods.\n- [Minor] The approach of improving grounding capabilities through two-step training procedures is well-studied in the embodied-llm space [1,2,3,4]. I think this paper would benefit from including a discussion of these approaches and highlight how their method/problem is different from prior work.\n\n[1] - Yang, Ganlin, et al. \"Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning.\" arXiv preprint arXiv:2510.11027 (2025).\n[2] - Ahn, Michael, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" arXiv preprint arXiv:2204.01691 (2022).\n\n[3] - Carta, Thomas, et al. \"Grounding large language models in interactive environments with online reinforcement learning.\" International Conference on Machine Learning. PMLR, 2023.\n\n[4] - Huang, Wenlong, et al. \"Grounded decoding: Guiding text generation with grounded models for embodied agents.\" Advances in Neural Information Processing Systems 36 (2023): 59636-59661."}, "questions": {"value": "Listed in the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ImkZkDa0RW", "forum": "wywgRd1MUQ", "replyto": "wywgRd1MUQ", "signatures": ["ICLR.cc/2026/Conference/Submission10070/Reviewer_TC4U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10070/Reviewer_TC4U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887939017, "cdate": 1761887939017, "tmdate": 1762921461997, "mdate": 1762921461997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents INFIGUI-R1: a GUI agent that is trained in two stages (SFT/Distillation then RL).\n\nThe paper grounds the two stage training pipeline in the principle of \"Endow First, Internalize Later\", which gives a new justification to why this common paradigm seems to work well.\n\nIn the SFT/Distillation phase: Structured Chains-of-Thought (CoT) are generated and then validated from a larger teacher model (to produce the ground truth actions when fed as prompt) , then appended to the original prompt, and then used to fine-tune the base model.\n\nIn the RL phase: a comprehensive set of rewards are designed to cover targets like response formatting and action accuracy, and supplemented by specifically curated reward signals to strengthen spatial reasoning and error recovery. \n\nThe paper has many evaluations and shows uplift on various UI benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This work highlights an important problem for agents trained with RL (i.e. for RL to succeed, the MLLM needs to have some basic behavioral prior that enables it to solve certain aspects of the control problem). \n\n- The structured CoT synthesis process and the focus on the three core reasoning patterns relevant to GUI (Spatial Reasoning, Goal Decomposition, and Reflection) and the final verification-filtering step is optimized for GUI and seems to be a very important part of why the system shows good performance.\n\n- The reward design and formulation is very simple and scalable, and seems to be one of the main reasons the system has good performance.\n\n- The training set size is small (~3k in SFT, and ~44k in RL), which might indicate that this regiment is more data-efficient.\n\n- Good ablation studies that show the improvement from the 2-stage training regiment."}, "weaknesses": {"value": "- Although I liked the framing of  \"Endow First, Internalize Later\", as it grounds the common pattern of SFT/Distillation then RL into a pedagogical framework, however, I could not find any reference to the principle or how it's used in other work. A citation in the intro or on line 62 would make the principle used easier to understand.\n\n- The claimed paradigm shift from monolithic to  two-stage training regiment is not especially novel, many other relevant works are using the same two-stage hierarchical training paradigm (some form of SFT then some form of RL) for embodied and UI LLM-based models. This is before and after the DeepSeek-R1paper made it more popular.\n\n- It's important for these models to be tested at various sizes. I recommend at least a 7B model for comparison.\n\n- The model is not tested on OSWorld, even though the training data seems to include desktop GUI examples (e.g. OmniAct).\n\n- Please include results from baseline models on AndroidWorld on Table 4 to make the comparison easier. The success rate of the model in isolation is not very useful to compare."}, "questions": {"value": "- From table 4: It seems that  the cognitive endowment is where most of the performance comes from, do you have justification for this? Why aren't the other signals/phases as important for AndroidWorld?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b4hICtwVVO", "forum": "wywgRd1MUQ", "replyto": "wywgRd1MUQ", "signatures": ["ICLR.cc/2026/Conference/Submission10070/Reviewer_cbWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10070/Reviewer_cbWR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762176294996, "cdate": 1762176294996, "tmdate": 1762921461748, "mdate": 1762921461748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}