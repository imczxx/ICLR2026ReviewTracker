{"id": "X7ITc8NmSv", "number": 14186, "cdate": 1758230005480, "mdate": 1759897385467, "content": {"title": "Personalized Federated Fine-tuning for Heterogeneous Data: An Automatic Rank Learning Approach via Two-Level LoRA", "abstract": "We study the task of personalized federated fine-tuning with heterogeneous data in the context of language models, where clients collaboratively fine-tune a language model (e.g., BERT, GPT) without sharing their local data, achieving personalization simultaneously. While recent efforts have applied parameter-efficient fine-tuning techniques like low-rank adaptation (LoRA) in federated settings, \nthey typically use single or multiple independent low-rank adapters with predefined maximal and minimal ranks, which may not be optimal for diverse data sources over clients.\n\nTo address this issue, we propose PF2LoRA, a new personalized federated fine-tuning algorithm built on a novel **automatic rank learning approach via two-level LoRA**. Given the pretrained language model whose weight is frozen, our algorithm aims to learn two levels of adaptation simultaneously: the first level aims to learn a common adapter for all clients, while the second level fosters individual client personalization. A key advantage of PF2LoRA is its ability to adaptively determine a suitable rank based on an individual client's data, rather than relying on a predefined rank that is agnostic to data heterogeneity. We present a synthetic example that highlights how PF2LoRA automatically learns the ground-truth rank for each client, tailoring the adaptation to match the properties of their individual data. Notably, this approach introduces minimal additional memory overhead, as the second-level adaptation comprises a small number of parameters compared to the first level. Our experiments on natural language understanding and generation tasks demonstrate that PF2LoRA significantly outperforms existing federated fine-tuning methods.", "tldr": "", "keywords": ["Large language model", "Federated learning", "Parameter-efficient fine-tuning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de98db23232027f0eace14579da2164cc4285f53.pdf", "supplementary_material": "/attachment/24e46e1f068b28e9714f2af1ba8ecda84e460d1a.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes PF2LoRA, a personalized federated fine-tuning algorithm built on an automatic rank learning approach via two-level\nLoRA. Given the pretrained language model whose weight is frozen, their algorithm aims to learn two levels of adaptation simultaneously: the first level aims to learn a common adapter for all clients, while the second level fosters individual client personalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work proposes a novel bilevel formulation for personalized fine-tuning on heterogeneous data and develops a two-level low-rank adaptation framework to efficiently fine-tune the foundation model in the scenario of FL.\n- The proposed PF2LoRA has the ability to adaptively determine a suitable rank based on an individual client’s data, rather than relying on a predefined rank that is agnostic to data heterogeneity."}, "weaknesses": {"value": "- The concepts of global adapters and client-specific adapters have been widely adopted in FL with LoRA fine-tuning [1-3], which diminishes the novelty of this work.\n- The explanation of how the algorithm can achieve dynamic rank is not well articulated. Specifically, in Lines 198-199, \"our specific parameterization (2) explicitly encourages each adapter \\\\( W_k \\\\) for the k-th client to vary over k: it can have different ranks in the range \\\\( (r - \\widetilde{r}, r + \\widetilde{r}) \\\\).\" How is this achieved? Why can the rank be between \\\\( (r - \\widetilde{r}, r + \\widetilde{r}) \\\\)?\n- The theoretical analysis is unrelated to the proposed method. The theoretical analysis is based on the single machine case (M = 1), which is unrelated to the proposed method in the FL setting.\n- Many works on FL with LoRA fine-tuning across heterogeneous ranks [4-10] are missing and need to be compared.\n\n[1] Long, G., Shen, T., Jiang, J. and Blumenstein, M., 2024. Dual-personalizing adapter for federated foundation models. Advances in Neural Information Processing Systems, 37, pp.39409-39433. \\\n[2] Qi, J., Luan, Z., Huang, S., Fung, C., Yang, H. and Qian, D., 2024. Fdlora: Personalized federated learning of large language model via dual lora tuning. arXiv preprint arXiv:2406.07925. \\\n[3] Nguyen, D.P., Munoz, J.P., Roosta, T. and Jannesari, A., 2025, May. Federated Multimodal Learning with Dual Adapters and Selective Pruning for Communication and Computational Efficiency. In 2025 IEEE 25th International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (pp. 01-10). IEEE. \\\n[4] Byun, Y. and Lee, J., 2024. Towards federated low-rank adaptation of language models with rank heterogeneity. arXiv preprint arXiv:2406.17477. \\\n[5] Chen, S., Tavallaie, O., Nazemi, N. and Zomaya, A.Y., 2024, November. Rbla: Rank-based-lora-aggregation for fine-tuning heterogeneous models in flaas. In International Conference on Web Services (pp. 47-62). Cham: Springer Nature Switzerland. \\\n[6] Su, Y., Yan, N., Deng, Y., Dohler, M. and Schober, R., 2024. HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization. arXiv preprint arXiv:2411.06581. \\\n[7] Bai, J., Chen, D., Qian, B., Yao, L. and Li, Y., 2024. Federated fine-tuning of large language models under heterogeneous tasks and client resources. Advances in Neural Information Processing Systems, 37, pp.14457-14483. \\\n[8] Chen, S., Tavallaie, O., Nazemi, N., Chen, X. and Zomaya, A.Y., 2024. Autorank: Mcda based rank personalization for lora-enabled distributed learning. arXiv preprint arXiv:2412.15553. \\\n[9] Wu, F., Hu, J., Min, G. and Wang, S., 2025. Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models. arXiv preprint arXiv:2501.14406. \\\n[10] Liu, Q., Zhang, Z., Yao, X. and Liu, B., 2025. HLoRA: Efficient federated learning system for LLM heterogeneous fine-tuning. arXiv preprint arXiv:2503.00813."}, "questions": {"value": "Lines 240-242, \"Empirically, we adopt AdamW as the upper-level optimizer (line 6) and SGD as the lower-level optimizer (line 5) to fine-tune a language model.\" Why choose these two optimizers? Is this based on some preliminary experimental results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dzoT7UyZqF", "forum": "X7ITc8NmSv", "replyto": "X7ITc8NmSv", "signatures": ["ICLR.cc/2026/Conference/Submission14186/Reviewer_YmR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14186/Reviewer_YmR9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550323131, "cdate": 1761550323131, "tmdate": 1762924642254, "mdate": 1762924642254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PF2LoRA, a novel personalized federated fine-tuning algorithm for heterogeneous data using a two-level LoRA framework. The method introduces a bilevel optimization formulation where the first level learns a common adapter shared across clients, and the second level learns lightweight, client-specific adapters. A key contribution is the ability to automatically learn client-specific ranks based on local data heterogeneity, without relying on predefined rank bounds or extensive hyperparameter tuning. The authors demonstrate through synthetic and real-world experiments on NLU and NLG tasks that PF2LoRA outperforms existing federated fine-tuning baselines, including HETLoRA, with minimal additional memory overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The two-level LoRA formulation and automatic rank learning mechanism are novel and creatively address data heterogeneity in federated fine-tuning.\n\n2. The method is rigorously evaluated across multiple tasks and models, with thorough comparisons to state-of-the-art baselines.\n\n3. The paper is well-written, with clear explanations of the motivation, method, and results.\n\n4. The approach reduces the need for manual rank tuning and improves performance in personalized federated learning, which is a growing area of importance."}, "weaknesses": {"value": "1. While the method is evaluated on language models, its applicability to other modalities (e.g., vision, multimodal) is not explored, though this is acknowledged as future work.\n\n2. As shown in Table 18, PF2LoRA incurs the highest computational cost per communication round, representing a significant trade-off for its performance gains that may limit deployment on highly resource-constrained devices.\n\n3. The theoretical analysis is limited to a single-machine setting; extending it to the full federated case would strengthen the theoretical contribution.\n\n4.  While an improvement over HETLoRA, PF2LoRA still introduces new hyperparameters (e.g., the client-adapter rank $\\tilde{r}$), whose selection remains heuristic and lacks a principled guidance strategy.\n\n5. The ablation study shows a performance drop without bilevel optimization, but a more detailed analysis quantifying its contribution relative to the two-level adapter structure itself would strengthen the argument for its necessity."}, "questions": {"value": "1. Could the authors discuss the scalability of PF2LoRA when the number of clients is large (e.g., >100)? How does the bilevel optimization scale in such settings?\n\n2. The paper mentions that the client-specific adapters are not communicated. Have the authors considered the impact of client drift or non-IID data on the convergence of the common adapter?\n\n3. In the ablation study, the framework without bilevel optimization still performs reasonably well. Can the authors further analyze the relative contribution of the bilevel formulation vs. the two-level adapter structure?\n\n4. How sensitive is PF2LoRA to the choice of $\\tilde{r}$ (e.g., $\\frac{r}{2}$ or $\\frac{r}{4}$)? Is there a principled way to set this parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p3xmISqgDO", "forum": "X7ITc8NmSv", "replyto": "X7ITc8NmSv", "signatures": ["ICLR.cc/2026/Conference/Submission14186/Reviewer_yZVy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14186/Reviewer_yZVy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873877248, "cdate": 1761873877248, "tmdate": 1762924641692, "mdate": 1762924641692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies personalized federated fine-tuning for language models under heterogeneous client data. Existing federated LoRA approaches use fixed or independent low-rank adapters, which may not suit diverse client data. To address this, PF2LoRA is proposed, featuring a two-level LoRA design that simultaneously learns a common adapter for all clients and a personalized adapter for each client. PF2LoRA adaptively determines the optimal rank for each client based on local data, introducing minimal additional memory overhead. Experiments on natural language understanding and generation tasks show that PF2LoRA consistently outperforms existing federated fine-tuning methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly presented and easy to follow.\n2. The paper is well motivated and the problem is clearly defined.\n3. The paper provides a theoretical analysis."}, "weaknesses": {"value": "1. The method lacks technical novelty. The proposed PF2LoRA method is largely similar to FedDPA [1], i.e., one LoRA shared across clients and another serving as a personalized adapter.  However, the author do not cite and discuss it. The author should provide a detailed discussion on the differences between the proposed approach and FedDPA.\n[1] Dual-Personalizing Adapter for Federated Foundation Models. NeurIPS, 2024.\n\n2.  The paper would benefit from a discussion of its limitations and potential directions for future work.\n\n3. Experiments are limited to smaller models such as RoBERTa and GPT-2; evaluating on larger models (e.g., LlaMA2-7B and Qwen2.5-32B) would better demonstrate scalability."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OgpOeybxQb", "forum": "X7ITc8NmSv", "replyto": "X7ITc8NmSv", "signatures": ["ICLR.cc/2026/Conference/Submission14186/Reviewer_4oqs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14186/Reviewer_4oqs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892311459, "cdate": 1761892311459, "tmdate": 1762924641290, "mdate": 1762924641290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The contextual background of this study is an NLP task, which employs a federated learning paradigm to fine-tune foundation models using LoRA across multiple clients. The objective is to achieve personalized adaptation while minimizing computational overhead. To this end, this paper proposes PF2LoRA, which introduces a two-level LoRA structure that effectively decouples into a common adapter and a personal adapter, thereby enabling lightweight personalization. Experimental results demonstrate that PF2LoRA achieves certain performance improvements compared to both a homogeneous single-LoRA configuration and a state-of-the-art method from 2024."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This work presents solid research, supported by great experimental validation across multiple NLP tasks and datasets.\n2.The theoretical foundation is rich, with detailed proofs provided in the Appendix.\n3.The methodology is described with sufficient clarity, effectively conveying the core insight of the proposed approach."}, "weaknesses": {"value": "1. The novelty of the proposed method is somewhat limited, as it constitutes a relatively straightforward application of the LoRA framework. This is illustrated by prior works such as:\n[1]FedDecomp (MM 2024): This work also employs LoRA as a personalized adapter and similarly finds that preserving client-specific knowledge requires only lower-capacity personalized parameters, although it was applied to a computer vision (CV) task.\n[2]PERADA (CVPR 2024): This study likewise designs separate personalized and global adapters, but also within the domain of CV.\n\n2. The paper lacks a comprehensive analysis of computational and communication efficiency. \n\n[1] Wu X, Liu X, Niu J, et al. Decoupling general and personalized knowledge in federated learning via additive and low-rank decomposition[C]//Proceedings of the 32nd ACM International Conference on Multimedia. 2024: 7172-7181.\n[2]Xie C, Huang D A, Chu W, et al. Perada: Parameter-efficient federated learning personalization with generalization guarantees[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024: 23838-23848."}, "questions": {"value": "1. Whether foundation models can even be deployed on client devices is a significant open question. Has the paper discussed the resources required for clients to perform fine-tuning tasks on such large foundation models?\n2. Is it truly necessary to perform federated fine-tuning on a foundation model that has already been fully pre-trained? Is there any experimental evidence demonstrating that the original foundation model fails to handle personalized NLP tasks?\n3. Why was prompt tuning not included as a baseline? What are its clear disadvantages compared to the LoRA approach within the federated learning paradigm? For the authors' reference, I am attaching a previously seen method that employs prompt tuning for federated fine-tuning:\n[1] Bai S, Zhang J, Guo S, et al. Diprompt: Disentangled prompt tuning for multiple latent domain generalization in federated learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 27284-27293."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VeVAkYcVld", "forum": "X7ITc8NmSv", "replyto": "X7ITc8NmSv", "signatures": ["ICLR.cc/2026/Conference/Submission14186/Reviewer_b8Qd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14186/Reviewer_b8Qd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915869028, "cdate": 1761915869028, "tmdate": 1762924639690, "mdate": 1762924639690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}