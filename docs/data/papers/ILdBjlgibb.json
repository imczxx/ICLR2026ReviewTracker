{"id": "ILdBjlgibb", "number": 601, "cdate": 1756752542205, "mdate": 1763728280885, "content": {"title": "FastVMT: Eliminating Redundancy in Video Motion Transfer", "abstract": "Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: **motion redundancy** arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; **gradient redundancy** occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43× speedup without degrading the visual fidelity or the temporal consistency of the generated videos.", "tldr": "We emphasize the motion redundancy and gradient redundancy existing in the training-free motion transfer task and propose FastVMT, an efficient framework using DiT-based video generative model to transfer motion efficiently.", "keywords": ["Video Motion Transfer; Efficiency; Diffusion model;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e78f8a4a398ca1a390530be211ff70601461d40b.pdf", "supplementary_material": "/attachment/86669e0c2eb24aca1db103a811422ff01727288c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FastVMT, a training-free video motion transfer framework aiming at enhancing computational efficiency without sacrificing video quality or motion fidelity. To achieve this, FastVMT includes two key innovations: 1) sliding-window motion extraction which only computes attention with local spatial neighborhoods to better capture motion correspondences while eliminating unnecessary token interactions; 2) step-skipping gradient optimization that reuses gradients across optimization steps as the authors found that gradients change slowly along the diffusion trajectory. Experiments show that FastVMT yield an average 3.43× speedup and up to 14.9× lower latency compared to prior state-of-the-art methods (e.g., MotionDirector, DiTFlow, MOFT) without visible quality degradation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is elegant and well-motivated. For example, both the ideas of sliding window motion extraction and step-skipping gradient optimization are lightweight but sounds effective and should work smoothly with existing video diffusion models. To introduce these two modules, the authors provide an insightful analysis (i.e, Figure 2), showing that motion is locally consistent and the gradient in consecutive steps are mostly similar.\n- The efficiency improvement is significantly. Specifically, the authors demonstrate 3.43× speedup and up to 14.9× lower latency compared to the existing models, which are impressive.\n- The authors provide comprehensive comparisons with the up to 7 existing models (including both training-free and finetuning-based baselines).\n- The authors also provide extensive ablation studies, which clearly show the necessity of each proposed component. The results look promising.\n- The paper is well written and easy to follow. The figures are well-plotted and informative which can make readers quickly understand the core ideas."}, "weaknesses": {"value": "My first two concerns circle around the usage of sliding window attention:\n\n- First, the usage of sliding-window attention leads to irregular and non-contiguous tiling (i.e., the attention mask is no longer a uniform square but contains irregular zero-paddings to mask out non-local tokens). This irregularity would make the model incompatible with Flash Attention that requires full or casual mask. How did the authors handle this issue? If the model does not use Flash Attention, did the authors notice a speed degradation when switching to other attention functions?\n- Second, the Register Tokens paper [1] found that the transformer models tend to learn a few of register tokens which are attached by most of the tokens and used to aggregate and spread global information. However, the proposed sliding-window design restrict the receptive field of a token to its local neighborhood and further block such information exchange mechanism. This could make the model fail to handle the videos with larger dynamics or longer duration where the global information exchange is critical. However, all the video examples provided in the paper and the supplementary material are 5 seconds and only include smooth and slight motion. Could the authors provide the videos with longer sequences and larger motion to evaluate whether the proposed model can handle such cases?\n\nOther concerns:\n- The paper is lack of the report of GPU memory consumption. Since the sliding-window attention and AMF modules require storing multiple latent tensors, attention maps, and cached gradients, this may increase GPU memory usage (especially for long or high-resolution videos). However, the authors only provide runtime speedups without the profiling of memory consumption.\n- The ablation on the selection of some important hyperparameter selection is also missing. For example, the window size and the gradient skip intervals could largely affect the balance / trade-off between visual quality and runtime. Could the authors also provide such ablation?\n\n[1] \"Vision Transformers Need Registers\", ICLR 2024"}, "questions": {"value": "- How do you handle the irregular tiling that comes from the sliding window attention mechanism?\n- Does the model use Flash Attention or other memory-efficient attention operations? If not, what is the runtime or memory overhead when switching to other attention functions?\n- Could the authors provide the video outputs with longer durations or more complex motion (including both object motion and camera movement)?\n- Since GPU memory consumption is also an important metric to evaluate the efficiency, could the authors also provide the report of memory usage?\n- Could the authors include an ablation study for key hyperparameters, such as the window size, stride, temporal span, and gradient skip interval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FzWDkXGMqE", "forum": "ILdBjlgibb", "replyto": "ILdBjlgibb", "signatures": ["ICLR.cc/2026/Conference/Submission601/Reviewer_RmCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission601/Reviewer_RmCa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760936847388, "cdate": 1760936847388, "tmdate": 1762915562440, "mdate": 1762915562440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inference speed bottleneck of Video Motion Transfer (VMT) that uses training-free Attention Motion Flow. Specifically, it employs,\n\n(1) Sliding-window motion extraction, which assumes that motion correspondences between frames are local\n\n(2) Step-skipping gradient optimization (reusing cached gradients), based on the observation that gradients across consecutive inner optimization steps are highly similar\n\nThe paper claims their proposed method achieves an average 3.43× speed-up without compromising visual fidelity or temporal consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a practical problem such as inference speed improvement in training-free motion transfer\n- The proposed method also maintains (or even improves) the quality of the video\n- Clearly identifies and analyzes existing problems such as motion and gradient redundancy through experiments"}, "weaknesses": {"value": "- The sliding-window strategy rests on the assumption that inter-frame motion is local and small. The paper lacks analysis or discussion of performance limitation when this assumption breaks (e.g., very fast and large motions, aggressive camera movements, occlusions).\n- In Table 2, FastVMT uses WAN-2.1 as the base model, while other baselines may rely on different backbones. The paper states “fair backbone: WAN-2.1,” but it is unclear whether this means all baselines were re-implemented and re-evaluated on WAN-2.1, or merely that FastVMT used WAN-2.1. If the former, the results are very compelling. Otherwise, the quality advantage might partly stem from the newer backbone.\n- The core acceleration idea of step-skipping is a fairly common design principle in other areas, so its novelty is somewhat limited."}, "questions": {"value": "- In Table 2, are those baselines re-implemented by the authors using the WAN-2.1 backbone, or are these numbers quoted from the original papers (potentially with different backbones)?\n- In Table 2, it looks like \"Ours\" is included under Tuning-Based Methods. Is that intentional?\n- When is the window center re-estimated along the diffusion trajectory? Fixed at (t=0), or updated progressively during the first 20% of guided steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VQV6SmQ50y", "forum": "ILdBjlgibb", "replyto": "ILdBjlgibb", "signatures": ["ICLR.cc/2026/Conference/Submission601/Reviewer_TXvo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission601/Reviewer_TXvo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991831341, "cdate": 1761991831341, "tmdate": 1762915562233, "mdate": 1762915562233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the inefficiency issue of motion transfer in the DiT architecture and proposes a method to improve it without sacrificing quality. Specifically, the authors point out that motion redundancy arises from the neglect of motion smoothness across frames, while gradient redundancy occurs due to ignoring the slow gradient changes along the diffusion trajectory. Accordingly, they propose a sliding-window strategy that operates on downsampled attention maps and a step-skipping gradient computation strategy, which together enhance the efficiency of motion transfer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written, clearly motivated, and easy to follow.\n2. The proposed efficiency improvement strategies, involving the sliding-window and step-skipping gradient optimization, make sense to me, and the illustration of their rationale in the method section is intuitive.\n3. The experimental results demonstrate that the proposed method maintains video quality while achieving notable speedup."}, "weaknesses": {"value": "1. Evaluation dataset. The authors use 50 videos selected from the DAVIS dataset, which is rather small in scale and may not cover sufficient scene and motion diversity. I notice that benchmarks used in different motion transfer papers vary—perhaps the authors follow the test set of DiTFlow? However, how does the proposed method perform on other test sets used in related works? For example, please refer to Table 1 of DeT.\n\n2. Implementation details. The authors mention that “for fair comparison, they adapt Wan-2.1 as the same backbone.” Is the 14B model or the 1.3B model used? Previous works adopt different backbones such as CogVideoX and Hunyuan. Are the authors reimplementing these methods using the Wan2.1 model? If so, there may be a risk that some methods cannot perform optimally, as they can be sensitive to hyperparameters. Overall, it would be helpful if the authors could provide more details about how each baseline is implemented.\n\n3. It would be beneficial if the authors could include and analyze some typical failure cases of the proposed method."}, "questions": {"value": "In the ablation study, adding the step-skipping strategy brings improvements in certain metrics such as aesthetics or text–frame similarity. However, I think the operation of reusing previous gradients at specific timesteps is essentially an approximation. Even if it does not cause a performance drop, it theoretically should not lead to improvement. Could the authors provide some explanation for this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XTuVV08vUT", "forum": "ILdBjlgibb", "replyto": "ILdBjlgibb", "signatures": ["ICLR.cc/2026/Conference/Submission601/Reviewer_sSov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission601/Reviewer_sSov"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998137038, "cdate": 1761998137038, "tmdate": 1762915562107, "mdate": 1762915562107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}