{"id": "5ZaoXB3MdP", "number": 20114, "cdate": 1758302641659, "mdate": 1759897000960, "content": {"title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents", "abstract": "Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLM’s tendency to hallucination and their reliance on static training knowledge, which could lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models – future state prediction and reward estimation – through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs’ limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) com-\npared to baselines, with particular advantage in longer-horizon simulations.", "tldr": "", "keywords": ["Large Language Model", "Computer-use Agent", "World Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ced17ba12d3646fdf693fb48e750a64c63c64850.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes R-WoM (Retrieval-augmented World Model), a framework designed to enhance computer-use agents. Traditional world models for agents often struggle with long-horizon reasoning. R-WoM addresses the issue by integrating retrieval-augmented memory into a world model. \nThe method is evaluated on benchmarks of computer-use agents, showing that R-WoM significantly outperforms baseline world models and state-of-the-art methods in terms of task success rate, with particular advantage in longer-horizon simulations.\n\nOverall, this is a strong and well-motivated paper. It contributes a novel and effective combination of retrieval and world models, demonstrating strong empirical results on computer-use agents. Its main areas for improvement are in experiment soundness, experimental analysis breadth & deepness, scalability, and retrieval interpretability. With these addressed, the work could be highly impactful for the development of practical, general-purpose computer-use assistants."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses an increasingly important challenge: enabling LLM-powered agents to use computers effectively.\n- Novel integration of retrieval augmentation with world models in the specific context of computer-use agents.\n- This paper demonstrates that retrieval + world models can substantially improve generalization and robustness. Experimental evaluation shows consistent improvements over baselines across two comprehensive benchmarks designed for multi-round interactions in realistic computer-use environments.\n- This paper is well-written and logically structured, in spite of more experimental analysis are advised. \n- Potentially impactful for building personal assistant agents capable of robust computer-use in real-world settings."}, "weaknesses": {"value": "- Two benchmarks are not sound enough. Experiments are limited to a specific set of tasks. It is unclear how R-WoM would perform on diverse real-world applications (e.g., enterprise software, web navigation with dynamic layouts).\n- The evaluation lacks stress tests for extreme long-horizon/complex tasks (hundreds of steps).\n- This paper does not deeply analyze the computational overhead of retrieval at inference time. Scaling to large memory banks or diverse user contexts could introduce latency.\n- While retrieval helps generalization, it is not always clear what is retrieved and why. More analysis of retrieval quality and relevance would strengthen the contribution.\n- The world model’s predictive accuracy is not reported in detail (e.g., prediction error across horizons). Understanding when it fails would provide useful insights.\n- While strong baselines are included, it would be useful to see how R-WoM compares with end-to-end planning agents trained with reinforcement learning at scale."}, "questions": {"value": "- How well does R-WoM scale when retrieval memory grows to millions of trajectories? Do you employ approximate retrieval or pruning strategies?\n- Have you tested R-WoM on completely unseen applications (e.g., apps with novel layouts or interaction paradigms)? How robust is the method in such settings?\n- Can you provide examples of what the retrieval module retrieves in successful vs. failed cases? Does retrieval sometimes introduce misleading prior experiences?\n- How accurate is the predictive component of the world model across multiple horizons? Do errors accumulate in long trajectories?\n- In a real-world assistant scenario, how would you balance retrieval latency, model complexity, and user responsiveness?\n- Could this framework generalize to other domains requiring long-horizon reasoning (e.g., robotics, scientific workflows), or is it specialized to computer-use tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YAjfR4UX6O", "forum": "5ZaoXB3MdP", "replyto": "5ZaoXB3MdP", "signatures": ["ICLR.cc/2026/Conference/Submission20114/Reviewer_1XKm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20114/Reviewer_1XKm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761325116323, "cdate": 1761325116323, "tmdate": 1762933011215, "mdate": 1762933011215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the limitations of using Large Language Models as world models for computer-use agents, particularly their unreliability in long-horizon planning due to hallucinations and static knowledge. To diagnose this, the authors first conduct a systematic probing analysis, revealing that while LLMs excel at short-term state prediction and recognizing meaningful progress, they fail to generate plans that align with real environment dynamics over extended steps. To address this, the paper proposes the Retrieval-augmented World Model, a framework that grounds the LLM's simulation process in external knowledge retrieved from tutorials. Key components of R-WoM include a reasoning-based RAG pipeline for better retrieval, a CoT-based mechanism for efficient multi-step rollouts, and a listwise reward estimation strategy for more robust action selection. Experiments on two challenging benchmarks, OSWorld and WebArena, show that R-WoM significantly outperforms strong baselines across multiple LLM backbones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear Motivation: The paper is motivated by a systematic probing analysis that pinpoints the specific failure modes of ungrounded LLM-based world models in long-horizon tasks, providing a clear foundation for the research.\n\nReasonable Framework Design: R-WoM is a reasonably designed framework that combines ideas like a RAG pipeline, CoT-based simulation, and listwise reward estimation.\n\nEvaluation on Multiple Benchmarks: The evaluation demonstrates performance gains over baselines across two challenging benchmarks and three different LLMs."}, "weaknesses": {"value": "Limited Novelty: The core idea of the paper is to apply Retrieval-Augmented Generation (RAG) to world models. While this is a logical engineering direction, it feels more like a direct combination of existing, mature technologies (RAG, CoT, World Models) and lacks fundamental theoretical or methodological innovation. This makes the paper's contribution feel more applied and engineering-focused rather than a breakthrough in basic research.\n\nSevere Generalization Limits: The method's effectiveness is severely constrained to domains that have high-quality, comprehensive tutorials. This is a critical flaw, as it means the method is nearly inapplicable in many real-world scenarios where documentation is sparse, outdated, or non-existent. The paper fails to adequately discuss or experimentally verify this limitation, which significantly undermines its claims of general utility.\n\nUnfavorable Cost-Benefit Ratio: Although R-WoM is more efficient than the WebDreamer baseline, its computational overhead is still excessive compared to a standard RAG approach. The results show that achieving a limited performance gain (sometimes in the single digits) requires a several-fold increase in computational cost. This cost-benefit trade-off is unacceptable for many practical applications and casts doubt on the method's utility.\n\nEffectiveness of Core Component is Questionable: The results with an \"oracle\" retriever expose the performance bottleneck of the current retrieval module. This is not just a technical detail; it directly challenges the paper's central thesis that world models can be effectively grounded via retrieval. If the retrieval itself is unreliable, the foundation of the entire framework is shaky."}, "questions": {"value": "1. Could you comment on the expected performance of R-WoM in environments with limited or no available tutorial documentation? How gracefully does the model's performance degrade as the quality of the knowledge base decreases?\n\n2. The listwise reward estimation is an interesting and intuitive design choice. Have you performed an ablation study to specifically isolate its contribution? For instance, how does the full R-WoM framework compare to a version that uses a more traditional absolute reward scoring (e.g., a 1-10 scale) on the generated rollouts?\n\n3. Table 5 shows a clear trade-off between cost and performance. For example, using Claude-3.7 on WebArena, R-WoM is ~4.75x slower than RAG for a ~7.2% relative improvement. In what practical scenarios do you believe this trade-off is justified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ci0hw6I2cX", "forum": "5ZaoXB3MdP", "replyto": "5ZaoXB3MdP", "signatures": ["ICLR.cc/2026/Conference/Submission20114/Reviewer_tkQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20114/Reviewer_tkQf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622606156, "cdate": 1761622606156, "tmdate": 1762933010782, "mdate": 1762933010782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the capabilities and limitations of Large Language Models (LLMs) when used as \"world models\" for computer-use agents. The authors first conduct a systematic probing analysis across three tasks (next-state identification, full-procedure planning alignment, milestone transition recognition) to evaluate LLMs' ability to predict future states and estimate rewards. Their findings indicate that while LLMs excel at short-term predictions, their performance degrades significantly over longer horizons, failing to align with specific environment dynamics. To address this limitation, the paper introduces the R-WoM, a framework that grounds the LLM's simulation process in external, up-to-date knowledge retrieved from tutorials. R-WoM employs a reasoning-based retrieval pipeline and a listwise reward estimation strategy to improve the accuracy of multi-step rollouts. The authors demonstrate through experiments on the OSWorld and WebArena benchmarks that R-WoM substantially outperforms baseline methods, including vanilla agents, standard RAG, and an ungrounded world model (WebDreamer), particularly in tasks requiring long-horizon planning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tClear problem framing and probing suite. The three tasks isolate immediate state prediction vs. long-horizon planning vs. reward-related transition recognition, which yields diagnostic insights about LLM strengths/weaknesses.  \n2.\tEffective and practical method. R-WoM is straightforward to implement (retrieval + rerank + CoT rollouts + listwise ranking) and integrates well with existing LLM stacks.  \n3.\tStrong empirical gains. Consistent and sometimes large improvements across models and benchmarks (Table 2). \n4.\tInsightful ablations and visualization. Figures 3 provide clear comparisons of ungrounded, retrieval-based, and oracle-knowledge models. Figure 4 detail how imagination horizon impacts agent success, directly supporting claims about longer-horizon stability and providing actionable insights for the field."}, "weaknesses": {"value": "1.\tLimited novelty in core formulation. The high-level idea of retrieval-augmented LLMs for world modeling is not entirely new; prior works such as WorldGPT also consider augmentation or world modeling for digital environments. The contribution here is more in the detailed design (e.g., reasoning-based retrieval + listwise scoring) and empirical analysis than a fundamentally new methodological direction.\n2.\tPotential biases in evaluation. The use of LLM-based judges (Claude 3.5 Sonnet and Claude 3.7 Sonnet) to score plan alignment and trajectory success (especially in full-procedure planning) risks evaluation circularity (model judge vs. model being evaluated) and inherits bias from the same family of models. While necessary at present, some acknowledgment (and potentially a human evaluation subset) would bolster the claims.\n3.\tSelection bias toward tutorial-rich tasks. The method requires relevant tutorials; the authors explicitly sample task subsets that have tutorials available. The generalization to tutorial-scarce settings is unclear. Include experiments on a held-out set with sparse/no tutorials, or provide a quantitative analysis of how retrieval coverage correlates with gains.\n4.\tLimited Analysis of Failure Modes: While the paper demonstrates strong overall performance, a more comprehensive analysis of R-WoM's failure modes would strengthen the work. For instance, a discussion of scenarios where the model degrades (e.g., when faced with poor, irrelevant, or incorrect tutorials) or remains brittle (e.g., with ambiguous goals or novel UI elements) would provide significant value for practitioners seeking to build upon these findings."}, "questions": {"value": "1.\tTask subset selection: please quantify how many tasks were excluded due to absent tutorials and report the distribution of retrieval coverage (retrieval recall per task). This will help readers assess generality.  ￼\n2.\tReranker details: Is it zero-shot or fine-tuned? How sensitive are results to the reranker model choice?  \n3.\tListwise reward computation: provide the exact mathematical definition (or pseudocode) of  in Eq. (11) $f_w(\\{R(\\hat \\tau^{(j)},g,\\mathcal{E})\\})$ . How are rewards normalized across rollouts, and how does candidate set size m affect ranking quality?  \n4.\tJudge bias: how consistent are the plan alignment judgments across different judge models or human annotators? Can you release a small human-annotated subset for reproducibility? \n5.\tFailure modes: can the authors provide qualitative examples where retrieval misleads the model (noisy / tangential tutorials) and explain any heuristic filters applied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pm5ZNrsuGK", "forum": "5ZaoXB3MdP", "replyto": "5ZaoXB3MdP", "signatures": ["ICLR.cc/2026/Conference/Submission20114/Reviewer_Cuhc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20114/Reviewer_Cuhc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623310555, "cdate": 1761623310555, "tmdate": 1762933010284, "mdate": 1762933010284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an important challenge in developing computer-use agents: enabling LLMs to serve as effective world models for long-horizon planning in digital environments. The authors begin with a thoughtful diagnostic study using three probing tasks that reveal LLMs excel at short-term state predictions (86% accuracy) but struggle with long-horizon planning alignment (50-65%). To address this limitation, they propose R-WoM (Retrieval-augmented World Model), which grounds LLM-based simulations with external tutorials retrieved at inference time. The framework employs chain-of-thought rollouts and listwise reward estimation for action selection. Experiments on WebArena and OSWorld demonstrate improvements of up to 25.3% and 18.1% respectively over the WebDreamer baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Well-Motivated Diagnostic Study:** I appreciate the systematic probing analysis in Section 3. The three-task evaluation (next-state identification, full-procedure planning alignment, milestone transition recognition) provides clear empirical evidence for why existing LLM-based world models struggle with long-horizon planning. This diagnostic approach effectively motivates the need for external grounding mechanisms.\n\n**2. Reasonable Architectural Integration:** The paper successfully brings together two established paradigms—world models from model-based RL and retrieval-augmented generation—and applies them to the challenging computer-use domain. I find this combination sensible and well-suited to the problem.\n\n**3. Comprehensive Ablation Studies:** I commend the authors for including ablations on imagination horizons (Table 3) and grounding sources (retrieved vs. oracle tutorials in Figure 3). These studies help understand which components contribute to the overall performance.\n\n**4. Practical Efficiency Considerations:** The use of CoT-based rollouts instead of expensive iterative interactions represents a pragmatic design choice. Table 5 shows R-WoM is more efficient than WebDreamer, which is valuable for practical deployment.\n\n**5. Solid Engineering Effort:** The collection of 30k+ tutorial chunks from diverse sources and the design of the reasoning-based RAG pipeline with query rewriting and reranking demonstrates significant implementation effort."}, "weaknesses": {"value": "**1. Limited Baseline Comparisons:** I am concerned that the experimental evaluation could be more comprehensive. The paper primarily compares against WebDreamer and basic policy models, but I understand there are other benchmarks in this space that could provide important context. For example, I noticed that OpenAI's Computer Use Agent (CUA) published results showing 58.1% on WebArena (available since January 2025), which would be a valuable reference point. I recognize the authors' results of 35.11% show improvement over their chosen baseline, but including broader comparisons would help readers better understand where R-WoM stands in the current landscape. I encourage the authors to discuss or compare with additional publicly available benchmarks to strengthen the evaluation.\n\n**2. Characterization of Related Methods:** I noticed that the paper characterizes some related work in ways that may not fully capture their methodologies. For instance, lines 310-316 discuss WebEvolver (Fang et al., 2025) in the context of \"absolute reward estimation,\" but from my reading, WebEvolver appears to use supervised fine-tuning with rejection sampling rather than RL-based absolute rewards. I suggest the authors revisit these characterizations to ensure they accurately represent the cited methods, which would strengthen the positioning of the listwise reward contribution.\n\n**3. Retrieval Quality Analysis:** While Figure 3 shows that oracle tutorials outperform retrieved ones, I would appreciate more analysis of how retrieval failures affect performance. The appendix mentions Recall@5 metrics (94.4% OSWorld, 85.7% WebArena), suggesting some retrieval errors occur. I recommend adding analysis of what types of tasks or scenarios are most affected by retrieval failures and how these errors propagate through the world model rollouts.\n\n**4. Computational Cost Discussion:** Table 5 shows R-WoM requires 3.8-12h runtime compared to 0.7-2.1h for vanilla methods—a 4-10× increase. While this is better than WebDreamer's 15-43h, I believe the practical deployment implications deserve more discussion in the main paper. For real-world applications, this trade-off between performance and computational cost would be important to consider.\n\n**5. Gap Between Diagnosis and Solution:** The probing study identifies 50-65% full-procedure planning alignment as the core problem. However, I would be interested to see whether R-WoM actually improves this alignment metric, or if the task success improvements come from other mechanisms. This would help validate that the solution addresses the diagnosed problem."}, "questions": {"value": "**Q1:** Could you provide more context on how your results compare with other available benchmarks in this space? Specifically, I noticed OpenAI CUA reported 58.1% on WebArena in early 2025. Are there methodological or setup differences that make direct comparison inappropriate, or would it be valuable to discuss these results to help readers understand the current state of the field?\n\n**Q2:** Regarding the characterization of WebEvolver using \"absolute reward estimation\" (line 311): could you clarify this? From my understanding, WebEvolver uses supervised fine-tuning with rejection sampling. I would appreciate clarification on how you're categorizing their reward mechanism.\n\n**Q3:** Your probing study identifies long-horizon planning alignment as the key limitation (50-65% accuracy). After applying R-WoM with tutorial grounding, what is the new alignment score on this metric? This would help demonstrate that your solution directly addresses the diagnosed problem.\n\n**Q4:** Could you provide more analysis of retrieval failure cases? What types of tasks or scenarios are most affected when the retrieval system fails to find appropriate tutorials, and how do these failures impact the final task success?\n\n**Q5:** Have you considered any techniques to reduce the 4-10× computational overhead? Are there scenarios where the performance gains justify this cost, and others where a lighter-weight approach might be preferable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MLLQF2XiBY", "forum": "5ZaoXB3MdP", "replyto": "5ZaoXB3MdP", "signatures": ["ICLR.cc/2026/Conference/Submission20114/Reviewer_W8D9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20114/Reviewer_W8D9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159504885, "cdate": 1762159504885, "tmdate": 1762933009745, "mdate": 1762933009745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}