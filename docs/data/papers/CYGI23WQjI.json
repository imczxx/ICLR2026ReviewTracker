{"id": "CYGI23WQjI", "number": 10109, "cdate": 1758160964046, "mdate": 1759897673282, "content": {"title": "HiSpec: Hierarchical Speculative Decoding for LLMs", "abstract": "Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is $4\\times$ slower than token generation when a 3B model speculates for a 70B target model), but most prior works focus only on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces verification time by discarding inaccurate draft tokens early, but existing methods incur substantial training overheads in incorporating the intermediate verifier, increase the memory footprint to orchestrate the intermediate verification step, and compromise accuracy by relying on approximate heuristics. \n\nWe propose $ \\textit{\\ }\\underline{\\mathit{Hi}}\\textit{erarchical\\ }\\underline{\\mathit{Spec}}\\textit{ulative\\ Decoding\\ (HiSpec)} $, a framework for high-throughput speculative decoding that exploits $\\textit{early-exit (EE) models}$ for low-overhead intermediate verification. EE models allow tokens to exit early by skipping layer traversal and are explicitly trained so that hidden states at selected layers can be interpreted, making them uniquely suited for intermediate verification without drastically increasing compute and memory overheads. To improve resource-efficiency even further, we design a methodology that enables HiSpec to re-use key-value caches and hidden states between the draft, intermediate verifier, and target models. To maintain accuracy, HiSpec periodically validates the draft tokens accepted by the intermediate verifier against the target model. Our evaluations using various representative benchmarks and models show that HiSpec improves throughput by 1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline single-layer speculation without compromising accuracy.", "tldr": "HiSpec repurposes higher intermediate layers as lightweight verifiers to reject draft tokens early, boosting speculative decoding throughput without hurting accuracy.", "keywords": ["speculative decoding", "self-speculation", "LLMs", "early-exits"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bcff49134791be77757dbdb4948848d03e75e83.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes HiSpec to speed up LLM inference by inserting a low-overhead intermediate verification step between the draft and target model using early-exit (EE) layers of the target, proposing reusing KV caches and hidden states, and periodic full-model verification.  Several experiments across multiple tasks and model families demonstrate the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is overall well-written.\n- Using exit layers avoids training an additional auxiliary verifier and reduces memory complexity.\n- The design addresses alignment challenges which may be useful for real systems."}, "weaknesses": {"value": "- Although the paper explains the omission (unavailable verifier models and accuracy costs), the author may want to provide an approximate or partial reproduction to contextualize the gains of SPRINTER.\n- The “¼-depth verifier / ⅛-depth drafter” rule of thumb could benefit from a more systematic cross-family analysis (beyond the provided heatmaps) or an adaptive policy.\n- The author may want to provide more results using more recent LLMs, including Llama3-70B and Qwen3 series models.\n- From a practical perspective, the authors should consider combining HISPEC with existing speculative sampling methods, such as Eagle, to demonstrate the effectiveness of HISPEC.\n-  The default Ni=4 is justified by ablations,  which seems like a magic number without other guarantees. The author may need to conduct more exploration of adaptive Ni (e.g., driven by acceptance confidence/entropy) to provide more insight into HISPEC."}, "questions": {"value": "- How does HiSpec behave under high-throughput server settings (many concurrent sequences), especially regarding memory pressure and cache fragmentation when pruning KV for rejected tokens? Any interactions with paged attention/paging strategies?\n-  Could Ni and Nd be adapted online for real servers?\n-  Beyond Llama/CodeLlama, have you observed similar “¼-depth works best” dynamics on other transformer families (e.g., OPT, Qwen, or Gemma families)? We need some insight rather than just a magic number."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W8xYXZDeeN", "forum": "CYGI23WQjI", "replyto": "CYGI23WQjI", "signatures": ["ICLR.cc/2026/Conference/Submission10109/Reviewer_sixV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10109/Reviewer_sixV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760928813115, "cdate": 1760928813115, "tmdate": 1762921489047, "mdate": 1762921489047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HiSpec, a hierarchical early-exit speculative decoding method leveraging middle-layer exit as intermediate verifier. The paper reveals that the bottleneck of speculative decoding lies in verification stage, and intermediate verification (before final verification) can largely mitigate this bottleneck. Therefore, HiSpec leverages middle-layer early exit for intermediate verification, creating a hierarchical speculation framework. Furthermore, the hidden states from early-exit drafting can be re-used in verification, avoiding re-computation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The topic is highly related to practical issues of LLM acceleration. The observation of speed gap between drafting and verification is important, and essential in optimizations of current research.\n\nHiSpec involves no training overheads. It leverages existing early-exit checkpoints from Layerskip for early-exit drafting and middle-layer intermediate verification. \n\nThe ablation studies in fig.6 and fig.7 about experimental configurations (e.g. speculation lengths) are comprehensive and empirically convincing."}, "weaknesses": {"value": "The accuracy of intermediate verification can largely affect the overall performances, while the paper only provided end-to-end speed, but no speculation accuracies. The accuracy results would further demonstrate the effectiveness of the method.\n\nThe configurations of baselines are not sufficiently specified and tuned. While HiSpec uses 1/8 layers as drafter and 1/4 layers as intermediate verifier, the exit layer of LayerSkip, and the configuration for LookAhead should be specified. Moreover, these configs for baselines should be tuned for optimal performances.\n\nThe usage is limited to already-trained early-exit models like Layerskip. The paper claims that HiSpec can also be applied to post-trained models, but provides no empirical evidence for it.\n\nThere are some other techniques to improve final verification accuracy, e.g. tree attention, while this paper adopts none of them. It is unclear whether the effectiveness of intermediate verification still preserves when combined with these techniques, as the false rejection of intermediate verification may outweigh the benefits when the final accuracy is high.\n\nThe paper can be better organized. The ‘method’ section should focus more on the overall design, while the experimental details (e.g. the exit layer) should be put to the ‘experiment’ section."}, "questions": {"value": "1. What is the speculation accuracy of intermediate verification? How is it compared to final verification?\n2. Can you provide more detailed configurations of baselines, such as the exit layer of Layerskip and the configs of LookAhead? Can you tune these hyper-parameters and report the optimal performances?\n3. Can you provide evidences of the wider applicability to post-trained models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VRfY5xML9n", "forum": "CYGI23WQjI", "replyto": "CYGI23WQjI", "signatures": ["ICLR.cc/2026/Conference/Submission10109/Reviewer_cjaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10109/Reviewer_cjaU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463004997, "cdate": 1761463004997, "tmdate": 1762921488545, "mdate": 1762921488545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HiSpec introduces Hierarchical Speculative Decoding (HiSpec) — a framework that employs Early-Exit (EE) models to perform low-overhead intermediate verification within the target model itself. The authors observed that the verification step is up to 10× slower than draft generation in typical speculative decoding. Thus, they proposed to use EE models to perform drafting and verification at shallower model layers. HiSpec manages the KV cache dynamically and share the KV cache across the drafter, intermediate verifier and full verifier to reduce memory footprint."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a novel way to deal with verification cost, it is among the first few to target the verification wall effectively.\n- The motivation is well stated and supported by data\n- The idea to reuse early-exit checkpoints as hierarchical verifiers is good — no extra training, minimal overhead."}, "weaknesses": {"value": "- line 362: hug -> hub\n- This paper selects the baselines which cut down the draft generation time and discards the comparison with the verification-focused methods, which makes its evaluation incomplete\n- This paper assumes that EE checkpoints are available, which are not applicable for all LLMs; adapting HiSpec to vanilla models might need further training."}, "questions": {"value": "- while one-fourth of the model is sufficient to generate up to 69% of the output tokens correctly, Figure 4 shows that for many tasks, the accuracy is well below 50%, which is quite low. How does this accuracy affect the final speedup?\n- How are the acceptance lengths/rates like as they are not presented in the experiments?\n- can you explain why is it hierachical?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eXRKxfTWwI", "forum": "CYGI23WQjI", "replyto": "CYGI23WQjI", "signatures": ["ICLR.cc/2026/Conference/Submission10109/Reviewer_ZW4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10109/Reviewer_ZW4r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984756237, "cdate": 1761984756237, "tmdate": 1762921488084, "mdate": 1762921488084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}