{"id": "945vCIUZWt", "number": 19105, "cdate": 1758293599872, "mdate": 1759897059120, "content": {"title": "Guided and Interpretable Neural Operator Design for Partial Differential Equation Learning", "abstract": "Accurate numerical solutions of partial differential equations (PDEs) are crucial in numerous science and engineering applications. In this work, we introduce a novel neural PDE solver named AFDONet, which incorporates neural operator learning and adaptive Fourier decomposition (AFD) theory for the first time into a specifically designed variational autoencoder (VAE) structure, to solve a general class of nonlinear PDEs on smooth manifolds. AFDONet is the first neural PDE solver whose architectural and component design is fully guided by an established mathematical framework (in this case, AFD theory), turning neural operator design from an art to a science. Thus, AFDONet also exhibits exceptional mathematical explainability and groundness, and enjoys several desired properties. Furthermore, AFDONet achieves outstanding solution accuracy and competitive computational efficiency in several benchmark problems. In particular, thanks to its deep connections with AFD theory, AFDONet shows superior performance in solving PDEs on i) arbitrary (Riemannian) manifolds, and ii) datasets with sharp gradients. Overall, this work presents a new paradigm for designing explainable neural operator frameworks.", "tldr": "We develop a novel neural PDE solver whose design is guided by adaptive Fourier decomposition theory.", "keywords": ["Partial Differential Equation", "Neural Operator", "Adaptive Fourier Decomposition", "Variational Autoencoder"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9af4a328d1437b757dec6cbf7dac0dde625ebae.pdf", "supplementary_material": "/attachment/cced5100744e4e66c2e3dcceaa56791cbd07e87d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes AFDONet, a neural operator architecture for solving nonlinear PDEs on smooth manifolds. The approach claims to be the first neural PDE solver fully guided by Adaptive Fourier Decomposition (AFD) theory, thereby enabling theoretically interpretable architecture design. Concretely, the authors build a VAE-based backbone with a latent-to-RKHS mapping and an AFD-type dynamic convolutional kernel decoder. This is intended to mimic adaptive pole selection and orthogonalization steps in AFD, yielding theoretically grounded solution representations. The paper further provides convergence theorems under the chosen loss function, then evaluates AFDONet on several PDE benchmarks on manifolds (Helmholtz, Poisson, Navier–Stokes), comparing primarily against baseline neural operators such as FNO, D-FNO, WNO, and DeepONet. The reported results show lower MAE and relative L2 errors on several test sets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Mathematical grounding: The paper’s attempt to connect AFD theory to neural operator architecture design is conceptually interesting and could, in principle, lead to more interpretable solvers.\n\n- Clear motivation: The introduction nicely articulates why PDEs on manifolds are challenging for Euclidean-domain neural operators, which is a timely and relevant research direction.\n\n- Some theoretical analysis: Unlike many empirical neural operator papers, the authors provide explicit error bounds and convergence results derived from their theoretical formulation.\n\n- Readable technical exposition: The manuscript is well structured, with a clean presentation of AFDONet’s components and algorithmic steps."}, "weaknesses": {"value": "This submission mainly falls short in several key aspects:\n\n- Overstated novelty: The claim that this is “the first neural PDE solver whose design is fully guided by a mathematical framework” is exaggerated. Several works in operator learning incorporate explicit spectral theory or approximation-theoretic structures (e.g., FNO itself relies on Fourier analysis; wavelet-based operators do the same). AFD-based decomposition is presented as fundamentally different, but in practice the architecture is just another Fourier-like latent-space factorization with some pole selection heuristics.\n\n- Baseline evaluation is insufficient.\nThe empirical comparison is restricted to classical neural operator baselines (FNO, D-FNO, WNO, DeepONet). For a claim of a “new paradigm,” the method must be compared against more modern operator learning approaches, in particular: (1) Koopman neural operator as a mesh-free solver of non-linear partial differential equations, (2) Solving High-Dimensional PDEs with Latent Spectral Models. These are state-of-the-art spectral operator approaches highly relevant to this setting. The absence of these comparisons is a serious empirical gap.\n\n- Limited experimental diversity: Only three PDE cases are shown, all relatively standard testbeds. No experiments on irregular manifolds beyond simple geometries (e.g., torus, quarter-cylinder). No scaling or robustness experiments beyond simple dataset size scaling. No performance metrics beyond MAE and relative L2 error — no runtime, memory, or parameter efficiency evaluation.\n\n- Theoretical contribution is overstated: The theoretical results are essentially standard learning-theoretic bounds derived via covering numbers, not a fundamentally new convergence analysis specific to AFDONet. The proof sketches are mostly boilerplate (Lipschitz continuity + RKHS structure), offering little genuine insight into the unique aspects of the method.\n\n- Ablations are not fully convincing: The ablation studies are narrowly framed and sometimes show marginal differences. Improvements over simple baselines are not statistically characterized — no confidence intervals or rigorous error analysis are presented. It remains unclear whether the gains come from the “AFD-guided” aspect or simply from more network capacity and a dynamic decoder.\n\nClarity issues and overclaiming: Phrases such as “turning neural operator design from an art to a science” are unjustified. The method still involves architectural heuristics and trainable components not directly derived from AFD theory. The connection between AFD theory and the actual training objective remains somewhat loose.\n\nMissing discussion of limitations: No analysis of computational cost of pole selection, orthogonalization, or decoder complexity. No indication of how the approach would scale to more complex PDE families or higher-dimensional manifolds."}, "questions": {"value": "- AFD vs. Fourier/Wavelet approaches: What is concretely new about using AFD compared to adaptive or learned Fourier bases in FNO or WNO? Is the “maximal selection principle” essential, or could learned poles perform similarly?\n\n- Scalability and complexity: What is the computational cost of Gram–Schmidt orthogonalization and pole selection in your decoder? How does this scale with the number of modes compared to standard FNO?\n\n- Generalization and robustness: Please provide explicit experiments on out-of-distribution manifolds or PDE parameters. How sensitive is the method to the choice of pole number N?\n\n- Theoretical contribution clarity: Please clarify what part of your convergence analysis is genuinely novel and not directly adapted from standard RKHS and neural network approximation theory.\n\n- Ablation interpretation: The ablation results are inconsistent: e.g., Helmholtz gains are small while Navier–Stokes gains are large. Can the authors explain this discrepancy beyond saying “AFD matches the structure”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0sDPfujpvQ", "forum": "945vCIUZWt", "replyto": "945vCIUZWt", "signatures": ["ICLR.cc/2026/Conference/Submission19105/Reviewer_CVhj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19105/Reviewer_CVhj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760853607684, "cdate": 1760853607684, "tmdate": 1762931131615, "mdate": 1762931131615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AFDONet, a neural operator framework grounded in Adaptive Fourier Decomposition theory, designed to provide a theoretically interpretable and generalizable solver for nonlinear partial differential equations on smooth manifolds. AFDONet employs a variational autoencoder backbone, integrates a latent-to-RKHS network to project latent variables onto an optimal reproducing kernel Hilbert space, and utilizes an AFD-inspired dynamic convolutional kernel decoder to adaptively select poles and basis functions for efficient approximation of PDE solution spaces. Its training objective combines RKHS reconstruction loss, feature-map consistency loss, and a holomorphic training loss to capture the smoothness and analytic structure of target functions. Theoretically, the authors establish bounded error, RKHS existence, and convergence guarantees under this framework. Experimentally, AFDONet demonstrates superior accuracy and computational efficiency over existing neural operators in benchmark problems."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a well-motivated and theoretically grounded framework that bridges Adaptive Fourier Decomposition theory with neural operator design, offering clear interpretability and mathematical rigor. The architecture is elegantly structured, combining a VAE backbone, latent-to-RKHS mapping, and dynamic AFD-based decoder, each justified both theoretically and empirically. Theoretical analysis is solid, providing provable error bounds, RKHS existence, and convergence guarantees. Experiments are comprehensive and reproducible, demonstrating consistent superiority across diverse PDE benchmarks. Overall, the work establishes a principled paradigm for explainable and efficient neural PDE solvers on manifold domains."}, "weaknesses": {"value": "The evaluation in this paper is limited to synthetic PDE benchmarks, lacking validation on real-world or noisy datasets."}, "questions": {"value": "1. In line 416, “space for the the Helmholtz equation” — one instance of “the” should be removed.\n2. In Table 3, “latent-to-RHKS” under “Full AFDONet” should be corrected to “latent-to-RKHS.”\n\nThe paper is comprehensive and well-presented; I have no further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T4mIC7HzFM", "forum": "945vCIUZWt", "replyto": "945vCIUZWt", "signatures": ["ICLR.cc/2026/Conference/Submission19105/Reviewer_jRw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19105/Reviewer_jRw1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760889254301, "cdate": 1760889254301, "tmdate": 1762931130796, "mdate": 1762931130796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AFDONet, which is a VAE-based neural operator that integrates Adaptive Fourier Decomposition (AFD) theory. AFDONet employs a VAE to first map PDE data into a latent space. A latent-to-RKHS network is then proposed to map the latents to kernels of AFD which lies in a reproducing kernel Hilbert space (RKHS). The kernels are then orthogonalized for AFD operation. Finally, the decoder constructs the solution given the generated kernels. AFD theory guarantees the approximation/existence/convergence of AFDONet. Experiment results on three PDE families demonstrate the methods' capacity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally easy to follow. \n2. Solid proof is provided to characterize AFDONet's behavior.\n3. Strong experimental results and comprehensive ablation studies to evaluate AFDONet."}, "weaknesses": {"value": "1. The complexity of AFDONet possibly brings heavy computational burden. Also, the incorporation of Gram-Schmidt orthogonalization during the forward process could be instable for ill-conditioned kernels.\n2. Although AFDONet is claimed to be advantageous for PDEs on manifolds, the experimental comparisons are mainly against Euclidean-domain models such as FNO and DeepONet. The datasets themselves cover only a small subset of manifold settings.\n2. The paper's wording could be further improved. For example, the cross-correlation operation is used in Eq. 9 but is only defined after Eq. 10."}, "questions": {"value": "1. I am interested in the runtime behavior of AFDONet. From the method section, the introduction of Gram–Schmidt orthogonalization appears potentially expensive, yet Figure 2 shows AFDONet as faster than FNO. Could the authors explain how this efficiency is achieved in practice (e.g., parallelization, reduced resolution, or approximate orthogonalization)?\n2. Could the authors provide results on larger-scale or more realistic datasets (for example, CFD benchmarks)? Given the “orthogonal reproducing kernel” operation, I am concerned about the method’s scalability and computational overhead as resolution and sample size grow.\n3. I notice in Table 2 that AFD-type decoder performs better than other variants on all datasets beside Possions dataset. Could the authors offer an explanation for this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x2jho8pEvF", "forum": "945vCIUZWt", "replyto": "945vCIUZWt", "signatures": ["ICLR.cc/2026/Conference/Submission19105/Reviewer_1H54"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19105/Reviewer_1H54"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516699285, "cdate": 1761516699285, "tmdate": 1762931130353, "mdate": 1762931130353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AFDONet, which is a neural operator that combines adaptive Fourier decomposition (AFD) with reproducing kernel Hilbert spaces (RKHS) to solve PDEs, even on manifolds. It first uses a VAE to learn latent representations of input functions, then maps them through an MLP to construct data-dependent reproducing kernels. The decoder performs an adaptive, orthogonal decomposition based on AFD theory. A holomorphic loss enforces analytical consistency. Compared to models like FNO or DeepONet, AFDONet achieves higher accuracy on manifold PDEs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is the first to introduce AFD into the study of neural operators.\n- It enables learning operators for physical systems defined on arbitrary manifolds.\n- Using multiple physical systems, the proposed method achieves higher-accuracy simulations than baseline methods."}, "weaknesses": {"value": "- While interpretability is referred to as one advantage of the proposed method, there is no concrete analysis or discussion. Although explicitly obtaining basis functions is beneficial, is it possible to conduct discussions related to scientific insights?\n- There were some unclear points in the manuscript (see Questions)."}, "questions": {"value": "- Although the holomorphic loss is important, the paper does not clearly specify how the derivatives $\\nabla^I u(x,\\cdot)$ of the ground-truth data are approximated, which raises concerns about practical implementation and stability during training.\n- Equation (7) defines the local kernel at a point on the manifold. This kernel is weighted as in Equation (10) using basis functions to predict solutions. If there are few data points, can the manifold structure be captured effectively? Conversely, if there are many data points, does the computational costs become prohibitively large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a0rLDHGpiw", "forum": "945vCIUZWt", "replyto": "945vCIUZWt", "signatures": ["ICLR.cc/2026/Conference/Submission19105/Reviewer_B3HL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19105/Reviewer_B3HL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723917025, "cdate": 1761723917025, "tmdate": 1762931129659, "mdate": 1762931129659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}