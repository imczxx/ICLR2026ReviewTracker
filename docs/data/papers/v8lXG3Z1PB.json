{"id": "v8lXG3Z1PB", "number": 20407, "cdate": 1758305649120, "mdate": 1759896979175, "content": {"title": "Reliable Detection of Autism Spectrum Disorder in Children Using Conformal Prediction", "abstract": "Autism Spectrum Disorder (ASD) is a neurological condition affecting communication and daily functioning. Early intervention can reduce challenges in learning and behavior, motivating the use of machine learning techniques for ASD detection. Although models with high accuracy and F1 scores may appear promising, they can be misleading in low-prevalence settings. By Bayes’ theorem, low prevalence substantially reduces the positive predictive value (PPV), meaning that even models with strong traditional metrics may yield unreliable predictions in practice. False positive ASD detections can lead to unnecessary psychological stress, including anxiety and depression, while false negatives may delay intervention and make treatment more difficult later. In this paper, we integrated conformal prediction into the classification pipeline. Unlike standard classifiers, conformal methods provide prediction sets that include the true label with a specified confidence level ($1-\\alpha$), thereby reducing the risk of false predictions. Results show that Conformal predictors occasionally left cases unpredicted, thereby abstaining in situations where reliability could not be guaranteed. Among the evaluated models, SVM achieved the best performance with 86% correct predictions and 14% abstentions, followed by Logistic Regression (84% correct, 16% abstentions). These results demonstrate that conformal prediction offers a more trustworthy approach for ASD screening.", "tldr": "Conformal prediction, a machine learning framework, enables Autism Spectrum Disorder detection with zero false predictions", "keywords": ["Conformal Prediction", "Machine learning", "Autism spectrum disorder", "Logistic regression", "SVM", "Random  forest"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e5293246b09dd338491104efc8fb5572f1819ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the application of conformal prediction (CP) to improve the reliability of autism spectrum disorder (ASD) screening in children. The authors argue that conventional classifiers—while showing high accuracy—can be unreliable in low-prevalence clinical settings because of low positive predictive value (PPV). To address this, they integrate conformal prediction into standard classifiers (SVM, Logistic Regression, Random Forest, Naive Bayes, and KNN) using the Q-CHAT-10 toddler screening dataset from Kaggle."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors correctly identify a real and clinically important issue—the unreliability of binary classification in low-prevalence medical screening tasks—and propose uncertainty quantification via conformal prediction as a potential solution. The motivation to avoid false positives and negatives in ASD screening is conceptually strong and ethically relevant."}, "weaknesses": {"value": "1. The paper does not introduce any new method, adaptation, or theoretical insight into conformal prediction. It merely applies existing off-the-shelf CP to a small Kaggle dataset with standard classifiers (SVM, LR, RF, NB, KNN).\n2. The dataset is the small Q-CHAT-10 (around 1,000 samples) from Kaggle, which is not a clinical dataset, lacks validation, and is not suitable for making claims about reliable detection.\n3. The reported performance (accuracy and abstention rate) lacks baseline comparisons.\n4. The authors claim “trustworthy detection” but provide no quantitative measure of reliability."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u8ANZRmT59", "forum": "v8lXG3Z1PB", "replyto": "v8lXG3Z1PB", "signatures": ["ICLR.cc/2026/Conference/Submission20407/Reviewer_aw9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20407/Reviewer_aw9U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901773233, "cdate": 1761901773233, "tmdate": 1762933850932, "mdate": 1762933850932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim at improving the calibration of a classification model for ASD using conformal prediction."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Improving calibration is detection tasks with low prevalence is challenging and important in practice, especially is medical settings such as the one considered by the authors."}, "weaknesses": {"value": "This is purely an application paper that uses standard methods and a single dataset that can be found in the public domain. As presented, it highly unlikely suitable for publication at ICLR, for precisely the same reasons: there is no methodological innovation, the experiments only consider one dataset and provide no insights neither methodological or medical."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QmDXiwt7xq", "forum": "v8lXG3Z1PB", "replyto": "v8lXG3Z1PB", "signatures": ["ICLR.cc/2026/Conference/Submission20407/Reviewer_R6Eu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20407/Reviewer_R6Eu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995373171, "cdate": 1761995373171, "tmdate": 1762933850567, "mdate": 1762933850567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates early detection of Autism Spectrum Disorder (ASD) in children by combining machine-learning classifiers with a conformal-prediction framework. Using Q-CHAT-10 questionnaire data, several models (logistic regression, SVM, random forest, naive bayes, KNN) were trained on a training set, and a separate calibration set was used to compute nonconformity scores from model confidence values. Applying split-conformal prediction, the authors selected a quantile threshold to ensure a target coverage level $1-\\alpha$. For each test case, the method outputs either a single label (ASD or non-ASD) or an abstention when confidence is low. Experiments showed, for example, that an SVM conformal predictor achieved about 86 % correct single-label predictions and 14 % abstentions.\nContribution: The study demonstrates how conformal prediction can make ASD-screening models more trustworthy by producing prediction sets—with the option to abstain—rather than forcing uncertain single decisions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents a meaningful attempt to apply conformal prediction to early screening for Autism Spectrum Disorder (ASD), introducing the idea that “uncertain cases can be withheld rather than forced into a decision.” This approach is valuable for real clinical screening, where reducing harm from false positives or missed diagnoses is essential. The study demonstrates how conformal prediction can support safer and more cautious decision-making in medical applications. Such an approach suggests the potential for future systems that provide not only predictions but also reliability and uncertainty information, offering greater practical support to clinicians and researchers."}, "weaknesses": {"value": "1. Lack of coverage reporting:\nThe paper does not clearly state the target confidence level or how the reported 86% correct predictions and 14% abstentions relate to it. Since the main reason to use conformal prediction is to provide prediction sets that meet a desired coverage level, this omission makes it impossible to confirm whether the method actually achieved valid coverage.\n\n2. Unclear definition of the nonconformity score:\nThe description of the score function as “measuring how unusual the observed outcome is compared to the learned pattern” is too vague. The paper should give an exact mathematical definition of how this score is computed from model outputs.\n\n3. Unclear data split and evaluation design:\nThe authors mention a 70–30 split between training and calibration data, but there is no mention of a separate test set. Without an independent test set, it is unclear how the reported results were obtained, and this may affect the reliability of the conclusions.\n\n4. Uncertain reliability of the reported performance:\nThe paper does not specify which dataset was used to produce the final accuracy and abstention results. If these results were measured on the calibration data itself, there is a risk of data leakage, making the performance estimates overly optimistic.\n\n5. Ambiguous claim of “valid coverage for each class”:\nThe statement that the method provides valid coverage for each class is not supported by evidence. To justify such a claim, class-specific thresholds or separate calibration procedures should be presented, but these are not included in the paper.\n\n6. No practical handling of data imbalance:\nAlthough the paper mentions low disease prevalence and the problem of low positive predictive value as motivation, it does not include any analysis or adjustment for class imbalance. There is no attempt to reweight by real-world prevalence or check label-conditional coverage."}, "questions": {"value": "1. Target coverage:\nWhat was the target coverage (confidence level) used for the reported results? Please clarify the value of $1-\\alpha$ that guided the selection of the quantile threshold in the conformal prediction procedure.\n\n2. Test data:\nWhich dataset was used to obtain the reported performance results? Was a separate test set employed apart from the training and calibration data to ensure an unbiased evaluation?\n\n3. Nonconformity score function:\nHow exactly was the nonconformity score computed? Please provide the mathematical formula or definition used to derive the score from the model outputs.\n\n4. Valid coverage for each class:\nWere coverage results calculated separately for each class? If so, how were the class-specific thresholds or conditional calibration procedures defined and applied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ak5GeMM4Wu", "forum": "v8lXG3Z1PB", "replyto": "v8lXG3Z1PB", "signatures": ["ICLR.cc/2026/Conference/Submission20407/Reviewer_uzfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20407/Reviewer_uzfJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762492500242, "cdate": 1762492500242, "tmdate": 1762933849861, "mdate": 1762933849861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}