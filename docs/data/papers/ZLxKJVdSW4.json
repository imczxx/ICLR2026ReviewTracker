{"id": "ZLxKJVdSW4", "number": 19399, "cdate": 1758295919273, "mdate": 1759897041334, "content": {"title": "Reducing Cognitive Overhead in Tool Use via Multi-Small-Agent Reinforcement Learning", "abstract": "Recent progress in multi-agent systems highlights the promise of specialized agents that collaborate through a division of labor. In contrast, most tool-augmented reasoning systems still adopt a single-agent paradigm, where one large model must interleave high-level reasoning with fine-grained tool operations—a process that often leads to cognitive-load interference and unstable outputs. We propose MSARL (Multi-Small-Agent Reinforcement Learning), a novel framework that explicitly decouples reasoning from tool execution and interpretation. In MSARL, a dedicated reasoning agent focuses on strategic problem decomposition and planning, while a specialized tool agent processes long and complex tool outputs, acting as an adaptive condenser to bridge information gaps. This role-specific separation not only reduces cognitive interference but also accelerates the information flow. To enable effective collaboration, we introduce a hierarchical reinforcement learning approach that uses role-specific and collaboration-based rewards, providing granular feedback to the tool agent and a holistic, trajectory-level signal to the reasoning agent. On mathematical problem-solving with code execution, MSARL achieves more stable reasoning and higher final-answer accuracy than strong single-agent baselines. Our findings indicate that this dual-agent architecture significantly mitigates hallucinations and boosts tool invocation tendencies, thereby improving overall robustness. Our method provides a scalable blueprint for building specialized multi-agent system that can tackle complex reasoning tasks. The code for our method is available at: https://anonymous.4open.science/r/msarl-D50D/.", "tldr": "We propose MSARL, a dual-agent framework that effectively solves cognitive load interference in single-agent systems by decoupling high-level reasoning from low-level tool interpretation.", "keywords": ["Multi-Agent Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b6f1a4b6354c2b3abfcaba9c07237d48bdb8017.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper is argues that single-agent, tool-augmented reasoning systems suffer from \"cognitive-load interference\" when interleaving high-level reasoning with fine-grained tool operations, which often leads to unstable outputs. To address this, the authors propose a framework that explicitly decouples these responsibilities into a dedicated Reasoning Agent and a specialized Tool Agent. The Reasoning Agent focuses on problem decomposition and planning, while the Tool Agent processes complex tool outputs and acts as an adaptive condenser to manage information flow. To enable collaboration, the agents are trained using a hierarchical reinforcement learning approach with role-specific and collaboration-based rewards, specifically utilizing normalized advantages for the Tool Agent and aggregated advantages for the Reasoning Agent. On mathematical problem-solving tasks involving code execution, MSARL achieves more stable reasoning and higher final-answer accuracy than single-agent baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper does a good job motivating the need for multi-agent systems by showing the limitations of single-agent methods in section 3\n- The proposed framework demonstrates good performance across math benchmarks with higher accuracy and stability improvements.\n- The reward design seems well justified for providing a dense learning signal"}, "weaknesses": {"value": "- While the cognitive motivation is strong, the architectural decomposition into a high-level planner and a low-level executor/interpreter is not particularly novel, bearing strong resemblance to hierarchical RL or classic Task and Motion Planning paradigms.\n- The study relies exclusively on the Qwen family of models, limiting the generalizability of the findings due to concerns about test-set leakage in proprietary/heavily fine-tuned models.\n- The current ablations are too high-level; specifically, the paper is missing a key comparison of the specialized, collaboration-oriented reward mechanism against a standard outcome-only RL approach (e.g., vanilla PPO/GRPO applied globally to both agents).\n- While the current experiments use code as a tool, exploring whether this framework extends to a distinct tool-use case like search APIs would provide better evidence that this framework scales and is robust outside of math/code problems."}, "questions": {"value": "- Can the authors provide additional details about the overall distinction between this framing and traditional TAMP frameworks or other perspectives in hierarchical RL?\n- Can the authors try their approach with other LLMs to mitigate concerns about data leakage from qwen?\n- Can the authors please provide an additional ablation study that directly compares the effectiveness of the collaboration-oriented, normalized advantage reward against a simpler reinforcement learning approach where both agents are trained using only the sparse, final-outcome reward, applying it uniformly to every step in the trajectory?\n- How well does this approach work in multi-tool use scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ipp0oYFpvl", "forum": "ZLxKJVdSW4", "replyto": "ZLxKJVdSW4", "signatures": ["ICLR.cc/2026/Conference/Submission19399/Reviewer_SjcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19399/Reviewer_SjcT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677010434, "cdate": 1761677010434, "tmdate": 1762931318726, "mdate": 1762931318726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MSARL, a multi-small-agent reinforcement learning framework that separates reasoning and tool use into two collaborating agents to reduce cognitive interference in LLMs. Experiments on mathematical reasoning tasks show that MSARL significantly improves accuracy and stability compared to single-agent RL baselines. Overall, it offers a simple effective architecture for enhancing tool-augmented reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dual-agent decomposition is conceptually simple yet effective. The reasoning–tool separation mirrors cognitive modularity and provides a clean interface for information flow.\n- MSARL-1.5B achieves 55.9 Pass@1, outperforming both larger (7B) and fine-tuned baselines by up to 5.9 points. Ablations show consistent improvements and stable training dynamics. The inclusion of Pass@8 / Maj@8 metrics demonstrates stability gains.\n- The implementation details (datasets, decoding settings, batch, tool-call limits) are thorough and reproducible."}, "weaknesses": {"value": "- All experiments are in mathematical reasoning with code execution. Although the method claims to generalize to “multi-tool” settings, this is not empirically demonstrated.\n- Dual-agent training introduces additional overhead. The actual wall-clock or compute cost vs. single-agent RL baselines is missing."}, "questions": {"value": "- How sensitive is MSARL performance to the hyperparameter C (maximum tool calls)?\n- Could the Helper agent be frozen while only the Reasoner is RL-trained?\n- What is the training cost compared to single-agent GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D5je7BkctM", "forum": "ZLxKJVdSW4", "replyto": "ZLxKJVdSW4", "signatures": ["ICLR.cc/2026/Conference/Submission19399/Reviewer_SfFB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19399/Reviewer_SfFB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774515426, "cdate": 1761774515426, "tmdate": 1762931318283, "mdate": 1762931318283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dual-agent framework that decouples reasoning from tool execution/interpretation for math problem solving with code. A “Reasoner” plans, while a “Helper” parses long tool outputs and returns condensed signals. Training uses SFT on MATH traces, followed by GRPO with role-specific and collaboration rewards."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing is clear.\n2. The experimental setup is easy to follow."}, "weaknesses": {"value": "1. The “cognitive overhead” evidence is narrow (three small backbones, N=5 samples, one judge), limiting generality.\n2. The method is only demonstrated at 1.5B scale; larger models are not reported.\n3. The abstract claims to mitigate hallucinations and to boost tool-invocation tendencies, but there is no quantitative hallucination metric and invocation is capped, preventing analysis of tendency."}, "questions": {"value": "1. Cognitive offload from tool calls may vary with model capacity—do stronger models still require this design?\n2. Why choose a multi-agent split instead of a single policy that reads raw tool outputs and emits a compact parse? Please add a single-agent-with-parser baseline on the same backbone.\n3. In Table 1, MSARL-1.5B is listed under the “7B-Base” block?\n4. The approach appears weak on the challenging AIME benchmark.\n5. Does the approach hold across different model scales and architectures?\n6. Please include stronger TIR baselines on the same backbone."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZKB5u7MKOt", "forum": "ZLxKJVdSW4", "replyto": "ZLxKJVdSW4", "signatures": ["ICLR.cc/2026/Conference/Submission19399/Reviewer_52CB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19399/Reviewer_52CB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931208064, "cdate": 1761931208064, "tmdate": 1762931317887, "mdate": 1762931317887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Proposes MSARL, a dual‑agent framework that decouples high‑level reasoning (Reasoner) from low‑level tool execution/interpretation (Helper) to reduce cognitive interference in tool‑augmented reasoning \n\n- Introduces a collaboration‑oriented, hierarchical RL scheme (GRPO):\n   - Helper gets token‑level advantages from n alternative interpretations per tool use (subgroup rewards).\n   - Reasoner gets a trajectory‑level advantage, pooled over those interpretations.\n   - Removes KL to a reference model for simpler training \n\n- On math with code execution, MSARL‑1.5B improves Pass@1 vs. single‑agent TIR baselines and shows stronger Pass@8/Maj@8 stability; an “untrained” dual‑agent also helps modestly, suggesting architectural gains beyond learning"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n- Clear role separation (planning vs. tool interpretation) with a concrete collaboration reward shaping—a principled take on multi‑agent RL for tool use \n\nQuality\n- End‑to‑end RL implementation with grouped rollouts, per‑role objectives, and practical engineering (idle‑time mitigation via C) \n\n- Competitive results across AIME24/25, MATH500, Olympiad, AMC23; helpful ablation (trained vs. untrained dual‑agent) \n\nClarity\n- System diagram, roll‑out prompts, and pseudocode (Alg. 1) make the pipeline easy to follow \n\nSignificance\n- Demonstrates that decoupling tool interpretation can stabilize and improve math/tool reasoning without scaling model size; suggests a scalable blueprint for multi‑tool settings"}, "weaknesses": {"value": "Complexity–performance trade-off\n- The dual-agent MSARL pipeline introduces substantial architectural and operational complexity (role separation, inter-agent messaging, subgrouped rollouts, tool interpreter integration, global batch size of 1, and a hard cap on tool calls C) without a commensurate accounting of its costs. The paper reports accuracy gains (e.g., +5.9 Pass@1 over the strongest single-agent TIR baseline) but does not quantify end-to-end training/inference latency, GPU utilization/idle time due to agent handoffs, tool-call rates, or cost per correct answer. Without compute/throughput and robustness metrics, it is unclear whether the added engineering, maintenance burden, and potential failure modes (reward gaming via formatting, tool-output brittleness) justify the observed improvements versus simpler single-agent RL baselines with process-level rewards or better tool-use scheduling. Please add wall-clock, GPU-hours, throughput, inference latency, tool-call counts, and multi-seed variance to make the ROI of MSARL’s complexity concrete\n\nEvidence scope and fairness\n- Heavily focused on math/code; limited evidence for non‑code tools (retrieval, calculators, web APIs) or non‑verifiable tasks \n\n- Fairness of comparisons unclear: many baselines are single‑agent; report whether they share the same tool budget (C=1), identical prompts, and identical tool runtimes \n\n- Single‑seed reporting; no CIs/variance. Improvements of a few points may be within training variance"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z5I83yznKD", "forum": "ZLxKJVdSW4", "replyto": "ZLxKJVdSW4", "signatures": ["ICLR.cc/2026/Conference/Submission19399/Reviewer_5ACH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19399/Reviewer_5ACH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932454866, "cdate": 1761932454866, "tmdate": 1762931317356, "mdate": 1762931317356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}