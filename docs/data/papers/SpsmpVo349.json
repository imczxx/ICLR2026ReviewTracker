{"id": "SpsmpVo349", "number": 8993, "cdate": 1758106366165, "mdate": 1759897749506, "content": {"title": "Samples Are Not Equal: A Sample Selection Approach for Deep Clustering", "abstract": "Deep clustering has recently achieved remarkable progress across various domains. However, existing clustering methods typically treat all samples equally, neglecting the inherent differences in their feature patterns and learning states. Such redundant learning often drives models to overemphasize simple feature patterns in high-density regions, weakening their ability to capture complex yet diverse ones in low-density regions. To address this issue, we propose a novel plug-in designed to mitigate overfitting to simple and redundant feature patterns while encouraging the learning of more complex yet diverse ones. Specifically, we introduce a density-aware clustering head initialization strategy that adaptively adjusts each sample's contribution to cluster prototypes according to its local density in the feature space. This strategy mitigates the bias towards high-density regions and encourages a more comprehensive attention on medium- and low-density ones. Furthermore, we design a dynamic sample selection strategy that evaluates the learning state of samples based on the feature consistency and pseudo-label stability. By removing sufficiently learned samples and prioritizing unstable ones, this strategy adaptively reallocates training resources, enabling the model to consistently focus on samples that remain under-learned throughout training. Our method can be integrated as a plug-in into a wide range of deep clustering architectures. Extensive experiments on multiple benchmark datasets demonstrate that our method improves clustering accuracy by up to $\\textbf{6.1}$\\% and enhances training efficiency by up to $\\textbf{1.3$\\times$}$. $\\textbf{Code is available in the supplementary material.}$", "tldr": "We recognize that not all samples contribute equally to training a deep clustering model, so we select the most important ones for efficient training.", "keywords": ["Deep Clustering", "Clustering", "Sample Selection"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40a118b461239a4f078ae9ebc7fccafbe37e5f84.pdf", "supplementary_material": "/attachment/ff94e6308a3e6fb8194640a81449aab86f5810d1.zip"}, "replies": [{"content": {"summary": {"value": "The paper identifies a key problem in deep clustering: existing methods treat all samples equally, causing them to overfit to simple, redundant feature patterns found in high-density regions of the feature space. To solve this, the authors propose a two-part plug-in designed to be integrated into existing deep clustering models:\n* Density-Aware Clustering Head Initialization (DACHI): This strategy addresses the initialization bias. Instead of a standard prototype (which is just the average feature of all samples in a cluster and is thus dominated by high-density samples ), it computes a density-weighted prototype.\n* Dynamic Sample Selection (DSS): This strategy adaptively manages training resources. It identifies and temporarily removes \"sufficiently learned\" samples from training batches. This allows the model to reallocate its capacity toward more \"unstable\" or under-learned samples."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The dynamic sample selection (DSS) strategy provides a novel and sensible method for curriculum learning in a fully unsupervised setting.\n2. The proposed method successfully improves both clustering accuracy and training efficiency.\n3. The method is evaluated as a plug-in for four different deep clustering baselines, demonstrating its general applicability. The ablation studies validate the contribution of both DACHI and DSS."}, "weaknesses": {"value": "1. The paper claims a training speedup of 1.3, but this only accounts for the reduced batch size during the model update step. It fails to discuss the significant overhead of the selection mechanism itself.\n2. The method introduces new and sensitive hyperparameters. The DSS strategy's pruning threshold, $\\epsilon$, is particularly problematic. As shown in Table 5, performance is highly dependent on this value: moderate pruning ($\\epsilon=0.1-0.3$) helps, but aggressive pruning ($\\epsilon=0.5$) degrades performance. The paper provides no clear heuristic for setting this critical value, which was manually set to 1e-1 or 1e-2 depending on the dataset.\n3. The 6.1% gain on CC is large, the improvement on the most recent and powerful baseline, CDC, is a more modest 1.1% average.\n4. The paper repeatedly uses the term \"overfitting\"  to describe the model's bias toward high-density samples. This is a slightly imprecise use of the term."}, "questions": {"value": "1. Related to weakness 1, could you provide a detailed analysis of the total wall-clock time per epoch, including the significant computational and memory overhead introduced by DSS? Specifically, what is the cost of calculating and storing prediction consistency histories for all $N$ samples at every epoch? The DACHI requires a k-nearest neighbor search within each initial cluster. How does this initialization step scale with very large datasets and high-dimensional features?\n2. Related to weakness 2. Is there a more adaptive or heuristic-based method to set this threshold?\n3. Have you investigated the interplay between your two modules? e.g. does the improved DACHI initialization lead to a larger or smaller set of samples being pruned by DSS later in training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lteDlNXdj5", "forum": "SpsmpVo349", "replyto": "SpsmpVo349", "signatures": ["ICLR.cc/2026/Conference/Submission8993/Reviewer_K4YF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8993/Reviewer_K4YF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593731728, "cdate": 1761593731728, "tmdate": 1762920722942, "mdate": 1762920722942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the problem that existing deep clustering methods treat all samples equally, resulting in excessive focus on simple redundant features in high-density areas and neglect of complex and diverse features in low-density areas, the paper proposes a plug-in module, which includes a density-aware clustering head initialization strategy (adaptively adjusting the contribution to the clustering prototype according to the local density of the sample to reduce bias in high-density areas) and a dynamic sample selection strategy (evaluating the sample learning status based on feature consistency and pseudo-label stability, and prioritizing resources to samples that have not been fully learned); this module can be seamlessly integrated into a variety of deep clustering architectures. Experiments on benchmark datasets show that clustering accuracy improves, and the performance gain in medium- and low-density areas (complex features) is more significant."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method proposed in the article is implemented as a plug-in module. It can be seamlessly integrated into a variety of mainstream deep clustering architectures without requiring significant modifications to the original model's core structure. The integration process only requires replacing the initialization step (density-aware clustering head initialization) and the embedded training loop (dynamic sample selection), thereby significantly reducing the application cost in existing systems and providing strong adaptability.\n2. Based on the local density of samples (k-nearest neighbor distance), the contribution weight of samples to the cluster prototype is adaptively adjusted to reduce the dominance of high-density samples on the prototype, retain the clustering structure of medium and low-density areas (complex and diverse features), and avoid bias in the initialization stage."}, "weaknesses": {"value": "1. Density-aware clustering head initialization relies on features extracted by a pre-trained encoder. The document uses MoCo-v2 to pre-train ResNet-34 but does not discuss the impact of the pre-trained encoder's quality on the method. If the feature representations extracted by the pre-trained encoder are of poor quality (e.g., low discrimination for complex samples), subsequent density calculations and K-Means initial clustering based on these features will be biased, thereby affecting the accuracy of cluster prototype initialization.\n2. In the early stages of training, the model has not yet fully learned, and the accuracy of pseudo-labels is low. At this time, screening samples based on pseudo-label stability may mistakenly judge \"complex samples that really need to be learned\" as \"unstable and need to be retained\", resulting in unreasonable allocation of early training resources.\n3. The experiment only verifies the effectiveness of the method on the image dataset, and does not involve other modal data such as text, speech, and time series. The adaptability of the method to non-image modalities has not been verified, and the applicable scenarios are limited."}, "questions": {"value": "1. One key technique is initializing the clustering head, but this is highly dependent on the model's pre-training. How to deal with non-pre-trained models?\n2. While this method improves overall training efficiency, the dynamic sample selection process requires additional computation and tracking: generating weak/strong augmentation views for each sample, calculating prediction consistency (cosine similarity), and tracking changes in second-order differences and pseudo-labels over nearly three epochs. How can this additional overhead be quantified?\n3. There are fluctuations in consistency in the early stages of training. How to judge the credibility of the consistency constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t1yyFxdUKH", "forum": "SpsmpVo349", "replyto": "SpsmpVo349", "signatures": ["ICLR.cc/2026/Conference/Submission8993/Reviewer_8obk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8993/Reviewer_8obk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644780246, "cdate": 1761644780246, "tmdate": 1762920722555, "mdate": 1762920722555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a sample selection approach for deep clustering that addresses the problem that most clustering models treat all samples equally, even though samples in high-density regions are often redundant and simple, while those in low-density regions are complex and informative.\nBy relying more on samples in sparse regions (based on kNN distances), the tested models learn more effectively. The authors propose two main components:\n\n1. Density-Aware Clustering Head Initialization (DACHI) – adjusts each sample’s weight when computing cluster prototypes, so that clusters are not dominated by redundant high-density samples.\n\n2. Dynamic Sample Selection (DSS) – uses prediction consistency and pseudo-label stability to identify well-learned samples and temporarily remove them from training, letting the model focus on under-learned samples. \n\nThe method is plug-and-play and can be integrated into existing deep clustering algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "S1) Sampling strategies have not been researched enough on in the deep clustering area. The author's method can be combined with a lot of different deep clustering methods, potentially advancing the field broadly. \n\nS2) The methods are quite simple and intuitive. \n\nS3) By discarding some samples in each epoch the methods are even accelerated."}, "weaknesses": {"value": "W1) Experimental evaluation could be in more depth: how does the improvement of results depend on \n\na) the model complexity\n\nb) the number / sizes of datasets\n\nc) the number of concepts per class/cluster. \n\nI'm missing some synthetic experiments here. Also, it would be interesting what happens for datasets with consistent densities throughout the datasets, e.g. COIL 20. \nFurthermore, I would be interested in how well the methods works on, e.g., tabular data, as the paper focuses on image data.\n\nW2) Sampling strategies are a major object of research in Active Learning. Setting the method into this context would improve the paper. \n\nW3) How much runtime do the computations for the sampling need? Is the runtime acceleration dependent on properties of the data that can be predicted? \n\nW4) Using k-Means as initlal clustering should be discussed more. It is not clear to me how the value of k for the initial k-Means clustering was chosen. What happens if the data does not follow typical assumptions fitting k-Means, e.g., for video data?\n\nEspecially the first three pages have quite some redundant text that could be shortened in order to tackle the above weaknesses or answer the questions below. \n\nMinor stuff: \n- Table 3 appears way before it is referred to in the text which hinders the reading flow.\n- Type in line 90"}, "questions": {"value": "Q1) How does your method perform on data with consistent density, e.g. COIL20?\n\nQ2) How does your method perform on tabular data?\n\nQ3)  How is your sampling strategy related to Active Learning strategies? What could we learn from there, what are similarities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E2R5mAZqQz", "forum": "SpsmpVo349", "replyto": "SpsmpVo349", "signatures": ["ICLR.cc/2026/Conference/Submission8993/Reviewer_37QQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8993/Reviewer_37QQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922194985, "cdate": 1761922194985, "tmdate": 1762920722170, "mdate": 1762920722170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes improvements to the self-labeling stage of deep clustering. First, it introduces a density-aware clustering head initialization that downweights redundant high-density samples and upweights rare/low-density samples when forming prototypes. Second, it proposes a dynamic sample selection strategy that prunes stable samples based on pseudo-label stability and consistency between weakly and strongly augmented views, allowing training to focus on unstable or underlearned samples. Overall, this leads to improved clustering results across several standard image clustering benchmarks and results in better training efficiency, since fewer samples are actively optimized in later epochs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper targets an important stage used in state of the art deep clustering algorithms: self-labeling. The motivation is clearly laid out and depicts a practical issue: overfitting to dense/easy samples while underfitting rare samples.\n- The method improves both clustering accuracy and efficiency across several standard datasets and multiple baselines, suggesting it is broadly useful. The authors also provide a hyperparameter study on (\nα,\nk, \nϵ) and show that the approach is generally robust to \nα and k."}, "weaknesses": {"value": "- Training is always stopped after a fixed 100 epochs. Unsupervised stopping criteria remain an open problem in the self-labeling stage. The proposed framework (especially with dynamic pruning) could, in principle, offer a stopping signal, but this is not analyzed.\n- The evaluation does not clearly report model selection details or variance across seeds. Because self-labeling inherently involves noise and instability (due to training on pseudo-labels), averaged results and standard deviations are important to properly interpret the reported gains."}, "questions": {"value": "- How are models selected and tuned? What are the standard deviations across runs, e.g. for 5 to 10 seeds. How stable is the method overall?\n- You run for 100 epochs, but given that you explicitly track stability and prune stable samples, could those same signals (e.g., the fraction of samples no longer changing) be used as a stopping heuristic?\n- What happens if you continue training far beyond 100 epochs (e.g., 500+)? Do you eventually prune almost all samples, or does the model start overfitting the small subset of rare or ambiguous samples that never get pruned?\n- At the moment a sample is pruned as stable, how often is it actually assigned the correct ground-truth class? Showing that would address the concern that you might be confidently pruning wrong assignments and locking in errors. That could, however, have the benefit of reducing error propagation by no longer learning on those errors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5p3GbIbsbi", "forum": "SpsmpVo349", "replyto": "SpsmpVo349", "signatures": ["ICLR.cc/2026/Conference/Submission8993/Reviewer_WmJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8993/Reviewer_WmJa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933968654, "cdate": 1761933968654, "tmdate": 1762920721596, "mdate": 1762920721596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}