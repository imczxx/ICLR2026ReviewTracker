{"id": "Tp70ig4iKN", "number": 80, "cdate": 1756728527827, "mdate": 1759898276571, "content": {"title": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection", "abstract": "Detecting AI-generated images with multimodal large language models (MLLMs) has gained increasing attention, due to their rich world knowledge, common-sense reasoning, and potential for explainability.\nHowever, naively applying those MLLMs for detection often leads to suboptimal performance.\nWe argue that the root of this failure lies in a fundamental mismatch: *MLLMs are asked to reason about fakes before they can truly see them.*\nFirst, **they do not really see**: existing MLLMs' vision encoders are primarily optimized for semantic-oriented recognition rather than the perception of low-level signals, leaving them insensitive to subtle forgery traces. Without access to reliable perceptual evidence, the model grounds its judgment on incomplete and limited visual observations.\nSecond, existing finetuning data for detection typically uses narrow, instruction-style formats, which diverge sharply from the diverse, heterogeneous distributions seen in pretraining.\nIn the absence of meaningful visual cues, the model therefore exploits these linguistic shortcuts, resulting in catastrophic forgetting of pretrained knowledge (even the basic dialogue capabilities).\nIn response, we advocate for a new paradigm: *seeing before reasoning*. We propose that MLLMs should first be trained to perceive artifacts—strengthening their artifact-aware visual perception—so that subsequent reasoning is grounded in actual observations. \nWe therefore propose **Forensic-Chat**, a generalizable, explainable, and still-conversational (for multi-round dialogue) assistant for fake image detection.\nSpecifically, we first refine the vision encoder only via self-reconstruction while freezing the LLM, sensitizing it to artifacts without sacrificing pretrained knowledge (Stage 1).\nThen, we construct a multi-round dialogue finetuning data for detection, which is designed to progressively guide the model from artifact perception to common-sense reflection, enabling dialectical reasoning about *why an image is fake* and *what a real version should look like* (Stage 2).\nWe also propose **ExplainFake-Bench**, a benchmark tailored for the evaluation of the MLLM's explainability for image forensics from five key aspects.\nExtensive experiments show the superiority of generalization and genuinely reliable explainability.", "tldr": "We propose a unified MLLM-based framework that simultaneously perceives low-level artifacts and reasons dialectically about high-level plausibility, without reliance on external detectors.", "keywords": ["AI-Generated Image Detection", "MLLM", "Media Forensics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f0450b32e796e0cde2b002e3c20ad8a749d6c10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a two-stage MLLM fine-tuning method for fake image detection. In the first stage, the method freezes the LLM and only finetunes the image encoder using autoregressive loss and yes/no or fake/real target (binary classification). In the second stage, the image encoder is frozen and the LLM is finetuned using image-text pairs. For the first stage, pseudo-fake images are generated using VAEs; the reconstructed image is used to train the image encoder, as it only includes fine-grained artefacts. For the second stage, Gemini 2.5 is used to generate the text descriptions based on what Gemini sees (eg an image of a truck with weird proportions) vs what it knows about the object (eg trucks are symmetric). Finally, the paper proposes a benchmark for the evaluation of MLLMs and their explainability capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very well written, and it was very easy to follow the motivation and method. The method is intuitive; the two-stage finetuning approach has been used previously in other tasks (BLIP, LlaVa in general MLLM) and, as expected, yields good results here as well.\nThe experiments are well executed and convincing, particularly as we see the cross-dataset performance.\nIn addition, the ablation study shows the importance of each component."}, "weaknesses": {"value": "The main weaknesses are related to the motivation and experimental setup. More specifically:\n- The authors use Gemini to annotate the data in stage 2, without human annotations. As such, the method at stage 2 is more of a distillation method (ie, distilling Gemini knowledge into Qwen on the downstream task of fake image detection)\n- Related to this, it is hard to assess the usefulness of the method, as we have no context into how well Gemini performs (both in the dataset creation and also in the wild), as well as how well the base method performs without any training\n- In terms of catastrophic forgetting, the way the paper is written, it looks like this is attributed to the two-stage training; however, avoiding catastrophic forgetting is a well-known property of LoRA adaptors (that are used here). As a result, the claim on catastrophic forgetting is overstated (LoRAs are an implementation detail, not a key methodological contribution), and to properly assess this claim, we would need to see how Forensic-Chat is performing when fully finetuned (without adapters).\n- Human evaluation of the explanations is not included.\n- The novelty of the two-staged approach is a little overstated, in the sense that it has been done in other tasks and is pretty standard for MLLMs, so it would be best to recognise this in the text.\n\nSome minor weaknesses and errata:\n- End-to-end training performance is not shown in the ablations\n- Lines 300-301 are unclear. Without looking into the appendix, the reader simply does not understand what has happened there.\n- Lines 121 & 125: \"and etc\" is added, but it seems that it's better to list exactly what is done, as this is the introduction, and the reader needs a clear understanding.\n- In the list of contributions, the last bullet point is reiterating points 1 & 2.\n\nI am willing to raise my rating if these concerns are addressed, although a key concern is whether there is novelty and even motivation to distil knowledge from a very large model (LLM) into a task-specific MLLM."}, "questions": {"value": "- How does Gemini perform in terms of accuracy and explainability?\n- How does Forensic-Chat perform on the general MLLM understanding (Table 7) when fully finetuned (NOT using LoRA)?\n- How do descriptions generated by Gemini and ForensicChat generated descriptions resonate with human evaluators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2TljhgFobf", "forum": "Tp70ig4iKN", "replyto": "Tp70ig4iKN", "signatures": ["ICLR.cc/2026/Conference/Submission80/Reviewer_AYcS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission80/Reviewer_AYcS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission80/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761157063707, "cdate": 1761157063707, "tmdate": 1762915447283, "mdate": 1762915447283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main contributions of this paper are in the following three aspects:  \n1. Model training. A two-stage training approach is proposed. In the first stage, only the visual encoder is trained to enhance its ability to perceive artifacts. In the second stage, high-quality multi-turn dialogues are used to improve the model’s fake reasoning capabilities.  \n2. Data annotation. A high-quality data annotation process tailored to the two-stage training process is designed and constructed.  \n3. Evaluation: A new evaluation benchmark, ExplainFake-Bench, is proposed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper establishes a novel data construction pipeline and evaluation protocol, and the research work is relatively solid."}, "weaknesses": {"value": "The paper has significant issues in its writing, as it fails to provide sufficient effective information within the limited space of the main text.  \n1. In Section 3 (Method), a large portion of the content still focuses on discussing the motivation of the method rather than the paper’s technical details. The training method in Section 3.1 (autoregressive loss + selective freezing and training) is technically quite common, yet it is discussed at great length. In contrast, the data engineering part in Section 3.2— a key technology that determines whether the model can achieve performance improvement—is not clearly described. Readers must refer to Appendix E to understand the paper’s key innovations.  \n2. As a new evaluation protocol, ExplainFake-Bench only lists five evaluation dimensions in the main text. Readers need to consult the appendix to understand the details of the scoring criteria.  \n3. Important experimental settings used in the study (e.g., model initialization, source of training data) are not clearly specified in the main text.  \n  \nIt is necessary to remind the authors that reviewers are under no obligation to read appendices. Unlike general technical reports, formally published and accepted papers must ensure that all important technical details are included in the main text, rather than over-relying on information added in appendices."}, "questions": {"value": "1. Generalization. What is the source of the images in the training dataset used for reporting experimental results? I would like to know whether the performance reported in Tables 1–4 corresponds to intra-dataset or cross-dataset evaluations.  \n2. Lack of data ablation. The new data construction pipeline is the key to the model’s performance improvement. However, to prove the effectiveness of this pipeline—especially the special design of \"visual evidence + corresponding commonsense rule\" in dataset P2—it is necessary to construct a baseline dataset with a scale comparable to P2. In this baseline dataset, the seed annotations of visual evidence should be directly converted into multi-turn dialogues, and the model should be trained on this dataset as a baseline. This will help determine the source of the performance improvement.  \n3. Data engineering is crucial for improving the performance of current MLLMs. Do the authors have plans to open-source the training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ybvQCrnZH1", "forum": "Tp70ig4iKN", "replyto": "Tp70ig4iKN", "signatures": ["ICLR.cc/2026/Conference/Submission80/Reviewer_pX5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission80/Reviewer_pX5C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission80/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404750575, "cdate": 1761404750575, "tmdate": 1762915447133, "mdate": 1762915447133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Forensic-Chat, a two-stage pipeline for AIGI detection using MLLMs. Stage 1 fine-tunes only the vision encoder on reconstruction-style data to sensitize the model to low-level artifacts, while freezing the LLM. Stage 2 fine-tunes only the LLM on multi-turn dialogue data designed to promote “dialectical reasoning,” i.e., connecting perceived artifacts to commonsense expectations of what a real image should look like. The topic is timely and clearly relevant to a top-tier venue like ICLR. However, I think the current draft is not ready for acceptance. The core concerns are novelty compared with existing practice, incomplete grasp of previous related works based on MLLMs, underspecified experimental controls, and unclear and weak evaluation for explainability. \n\nPlease see details below."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper has a clear problem framing. The paper identifies two coupled failure modes of current MLLM-based AIGI detectors: (a) vision encoders optimized for semantics instead of forensic artifacts, and (b) instruction-style finetuning with templated Q/A both induce shortcut learning and cause catastrophic forgetting.\n\n2. The training recipe composed of stages 1&2 is simple and modular.\n\n3. The authors evaluate on GenImage, GenImage++, AIGI-Holmes, WildRF, and AIGI-Bench. They also compare general vision-language ability on public multimodal benchmarks.\n\n4. Table 5 shows Baseline vs. Stage 1 vs. Stage 2 vs. Stage 1+2, and breaks out where each stage helps."}, "weaknesses": {"value": "1. The paper claims to first propose an entirely new paradigm..., where the model should first perceive the artifacts so that the reasoning process is truly based on the seen cues. However, the two-stage schedule “tune perception module first, then align reasoning head” is very reminiscent of standard staged adaptation/adapter tuning for VLMs. This is an incremental design choice, not obviously a fundamentally “new paradigm.”\n\n2. The proposed Dialectical Fine-Tuning in Stage 2 is described as constructing multi-turn dialogues that progressively ask for deeper analysis and counterfactual reflection on what a ‘real’ image should look like. This is more of a structured chain-of-thought data curation rather than a new learning algorithm. Besides, can the authors provide quantitative proof indicating the conversation is actually diaretical?\n\n3. Stage 1 uses dataset P1, which consists of pairs (I_real, I_recon) where I_recon is reconstructed by a pretrained VAE decoder, producing “pseudo-fake” images. In this part, the model is trained to output tokens like “real” / “fake,” with only the vision encoder updated and the LLM frozen. Stage 2 uses dataset P2, where each image is annotated with multi-turn dialogues generated by an LLM.\nWhat are the specific details, like size, diversity, or quality-control protocol, for P1 or P2? Besides, how is the quality controlled? Are there any human verifications? How did the hallucination in LLM-generated conversations get prevented? \n\n4. The discussion attributes Stage2’s gain to “moving beyond reliance on fragile, low-level artifacts” and leveraging “robust features resilient to real-world distortions,” but does not quantify which distortions Stage2 is robust to.\n\n5. The fatal flaw of this paper is the employed evaluation measures and the reported performance. \n- First, there is no calibration/confidence reliability reporting. Since the method is presented as an interactive forensic assistant, its confidence statements must be trustworthy. Reporting only a generator-averaged accuracy sidesteps this completely. There is also no reliability diagram or analysis of whether high-confidence judgments are actually more trustworthy than low-confidence ones. This is concerning because a system that produces confident-sounding natural language explanations while being poorly calibrated is arguably more dangerous than a simple classifier: it can mislead users into over-trusting an incorrect decision. The metric does not meaningfully support the paper’s real-world generalization claims.\n\n- Second, the reported “macro accuracy” is not standard. According to the results in the tables, it is the unweighted average of per-generator accuracies instead of the actual overall performance on each dataset, while generator subsets can themselves be class-imbalanced. \n\n- Third, the paper argues strong robustness and “generalization to unseen generators / real-world distortions.”. But the reported score is a single scalar averaged across generators, with no visibility into which generators or conditions are actually weak.\n\n6. Regarding the explanability aspects, there are also several concerns.\n- The ExplainFake-Bench evaluation uses an “LLM-as-Judge framework,” with GPT-4o as evaluator, to rate each model along five dimensions, then average the scores. It is well-known that LLM judges show bias toward certain wording styles and safety disclaimers. This can systematically advantage models that share similar language patterns or formatting conventions. The paper does not acknowledge or control for this known bias. LLMs are used to generate the training data, and you also use LLMs to judge them. Wouldn't this cause a cycle-like paradox?\n\n- There is no human evaluation or spot-audit to check factual grounding beyond automated scoring. There is no guarantee that the explanations faithfully correspond to actual image features, and the paper does not propose mechanisms to mitigate such hallucinations. In other words, what is the true clue or judgment to tell an image is fake or real? To what level does the model understand its visual artifacts for reasoning?\n\n- Although text-level measures are employed, they only check textual similarity to pre-written ground-truth explanations, not whether the explanation is factually grounded in actual visual evidence. The model could hallucinate a plausible-sounding reason that matches reference wording but does not correspond to real image features. The evaluation does not include factual grounding metrics.\n\n7. Why would the authors explicitly assume that the MLLMs detect AIGIs by visible visual cues instead of invisible patterns left by specific generators that are generally employed in conventional CNN-based detectors? Is there any empirical proof for this employed assumption? To my knowledge, the research on this aspect is still vacant now.\n\n8. The authors argue that prior work either (a) co-fine-tunes vision+LLM and degrades general skills, or (b) uses an external expert detector and risks “just copying” that expert. However, I don’t see an ablation where you do co-fine-tune both vision and LLM end-to-end on P1+P2 and show that this harms general knowledge benchmarks dramatically more than Stage1+2. I either don’t see a teacher-distillation baseline.\n\n9. The draft currently has noticeable grammatical issues and typos, such as subtal, first percept the artifacts, still-conversational assistant, etc. Some claims are hard to parse (“difficulty mismatch — simple prompts paired with elaborate answers — that encourages shortcut learning”). I highly suspect it as a cue of heavy LLM usage.\n\n10. Stage 2 is referred to as “Dialectical Fine-Tuning (DFT),” and later you call it “Domain-Following Tuning (DFT)” in the ExplainFake-Bench discussion. What is the standard naming on earth? This is very confusing."}, "questions": {"value": "Please see the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eCTeHVLuOD", "forum": "Tp70ig4iKN", "replyto": "Tp70ig4iKN", "signatures": ["ICLR.cc/2026/Conference/Submission80/Reviewer_Rb3x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission80/Reviewer_Rb3x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission80/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928773283, "cdate": 1761928773283, "tmdate": 1762915446934, "mdate": 1762915446934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Forensic-Chat and ExplainFake-Bench. Forensic-Chat is a MLLM-based DeepFake reasoning model that makes use the ExplainFake-Bench data for supervised fine-tuning. ExplainFake-bench was curated by generating explanations from large MLLMs (proprietary models) like Gemini-2.5-Pro."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper attempts to address a very real threat to modern media: protection against DeepFakes, which moves beyond binary real vs. fake classification and provides reasoning as to why the model thinks the provided image is real or fake.\n- ExplainFake-Bench is a good attempt at making reasoning data publicly available for future models to train on. However, the scale of the data (~800 fake images) and the method used for generating the explanations at this scale is largely insufficient."}, "weaknesses": {"value": "- The paper claims that Forensic-Chat is actually seeing before reasoning and not using \"shortcut learning\". However, there are no qualitative evidence for the same. For example, attention heatmaps to show that when the model is talking about the face or the hands, the attention heatmap actually points to those features of the image.\n- The paper claims to be able to detect subtle anomalies within the images, however, the evaluations are only on fully AI-generated images (generated using models like Midjourney, SD, etc.) There are no experiments performed on face-manipulated data like DD-VQA, where the images originate from FaceForensics++ with manipulations only in the face, and the rest of the image being real. Most methods that work with FF++ crop the face as a preprocessing step, assuming a frontal human face is available. But when the full frame is passed, it would be important to check whether the Forensic-Chat model is able to look past the real components in the images (like the background, hands, etc.) and only focus and analyze the face. Only then can Forensic-Chat be called a truly generalized reasoning model.\n- The two-stage training approach used does not actually ensure that the model is \"seeing before reasoning\". The visual encoder is tuned in stage-1 and LLM is tuned in stage-2, which essentially boils down to simple SFT techniques. Unless specific loss functions are devised to force the MLLM to ground its reasoning in visual cues the contribution in the architecture/method is extremely limited. As correctly pointed out by the paper, MLLMs indeed are built for high level semantics, and cannot capture subtle clues present in DeepFake images, however, such a method for training is not helping the MLLM. Intelligent design of the data protocol of each stage isn't enough to make a pretrained MLLM suddenly look for subtle clues and perform perfectly generalized DeepFake detection and reasoning.\n- The evaluation of explainability relies on a new benchmark, ExplainFake-Bench, which uses GPT-4o as a judge. This \"LLM-as-a-Judge\" approach is inherently subjective and may carry the biases of the evaluator model, making the explainability scores difficult to verify independently. Accuracy has been the primary metric for comparison in the paper, which does not, at all, tell us whether the reasoning generated are meaningful or logical or hallucinated. Even NLG metrics like BLEU, ROUGE, etc. were not used which are popularly used in previous methods for DeepFake explainability. This makes it impossible to verify the claim that the model indeed provides good reasoning especially in cross-data or cross-generator settings."}, "questions": {"value": "- Could the authors provide a more detailed analysis of the model's failures, particularly for the generators where it performed poorly (e.g., SD XL, Firefly)? What specific image characteristics or artifact types does the model struggle to detect?\n- If MLLMs are actually as bad in subtle manipulation detection, why has the benchmark dataset been constructed using another (albeit bigger) MLLM like Gemini-2.5-Pro (which is a closed source proprietary model). And if models like Gemini-2.5-Pro indeed is able to generate good reasoning/explanations annotations for real and fake images, why would one need the Forensic-Chat framework to reason on DeepFakes? And if Gemini-2.5-Pro isn't actually generating good explanations, it makes the data to train Forensic-Chat significantly noisy, which again isn't helping the model learn meaningful features."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iaw0wDwjwG", "forum": "Tp70ig4iKN", "replyto": "Tp70ig4iKN", "signatures": ["ICLR.cc/2026/Conference/Submission80/Reviewer_9LRS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission80/Reviewer_9LRS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission80/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967263044, "cdate": 1761967263044, "tmdate": 1762915446650, "mdate": 1762915446650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}