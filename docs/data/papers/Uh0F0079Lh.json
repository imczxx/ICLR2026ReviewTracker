{"id": "Uh0F0079Lh", "number": 24982, "cdate": 1758362764085, "mdate": 1759896739302, "content": {"title": "A Concept Level Energy-Based Framework for Interpreting Black-Box Large Language Model Responses", "abstract": "The widespread adoption of proprietary Large Language Models (LLMs) accessed strictly through closed-access APIs has created a critical challenge for their reliable deployment: a fundamental lack of interpretability. In this work, we propose a model-agnostic, post-hoc interpretation framework to address this. Our approach defines an energy model that quantifies the conceptual consistency between prompts and the corresponding LLM-generated responses. We use this energy to guide the training of an interpreter network for a set of target sentences. Once trained, our interpreter operates as an efficient, standalone tool, providing sentence-level importance scores without requiring further queries to the original LLM API or energy model. These scores quantify how much each prompt sentence influences the generation of specific target sentences. A key advantage is that our framework globally trains a local interpreter, which helps mitigate common biases in LLMs. Our experiments demonstrate that the energy network accurately captures the target LLM's generation patterns. Furthermore, we show that our interpreter effectively identifies the most influential prompt sentences for any given output.", "tldr": "We propose a framework for training a model-agnostic interpreter that identifies influential prompt components for black-box LLM responses by leveraging a global energy-based training objective.", "keywords": ["Black-box large language models", "Post-hoc interpretation", "Energy based models", "Model-agnostic feature attribution"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fe1cd0809cb3fc900af2d9f036c3bb1c6025418.pdf", "supplementary_material": "/attachment/0c4d9fac4a52f639267b5d3384ffeafa76fafed6.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a framework for interpreting black-box large language models using a concept-level energy-based model (EBM) as a surrogate to approximate the model’s behavior. The EBM is trained on prompt–response pairs to learn an energy landscape that distinguishes authentic model outputs from mismatched or corrupted samples. An interpreter network is then trained, guided by the EBM, to identify which sentences in a prompt most influence a given output. Experiments include validating the EBM’s ability to differentiate valid from invalid pairs and a qualitative case study on sentiment analysis where the interpreter highlights key sentences associated with the model’s output sentiment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tries to tackle an important problem: interpreting black-box large language models at a concept level rather than through token-level attributions.\n\n2. The proposed joint optimization between the EBM and the interpreter reflects an innovative attempt to train both models in a single framework."}, "weaknesses": {"value": "While the paper proposes an interesting idea, the experimental section is not convincing to substantiate the claims.\n\n1. The paper assumes that GPT-4o-mini responses are always superior and uses them as “positive” examples, while human-written or GPT-2 responses are treated as “negative.” This assumption is problematic since GPT-4o-mini outputs are not guaranteed to be more accurate or coherent, and human answers are not necessarily worse. The resulting supervision may capture stylistic or distributional differences rather than genuine semantic or factual correctness. The authors should justify this choice or adopt a quality-based labeling scheme instead of relying purely on model origin.\n\n2. The proposed framework introduces multiple components such as the concept space projection, joint EBM-interpreter optimization, and contrastive pretraining, without providing theoretical motivation or ablation studies. It is unclear why the concept space projection is necessary on top of frozen Sentence-BERT embeddings, why joint optimization improves over independent training, or why InfoNCE contrastive loss is preferable to simpler alternatives. Without empirical evidence or rationale, these design choices appear ad hoc and unsubstantiated.\n\n3. The experiments do not include comparisons with standard or simplified baselines, for example, a straightforward supervised transformer trained on the same data or existing post-hoc attribution methods (e.g., LIME, SHAP, or other EBM-based frameworks). Without such baselines, it is impossible to assess whether the proposed method provides any advantage in accuracy, interpretability, or efficiency.\n\n4. The core claim that the interpreter effectively identifies influential prompt sentences is not empirically supported. The only quantitative result (Figure 4) measures energy separation during EBM pretraining, not the quality of interpretations. The IMDB sentiment case study is entirely qualitative and anecdotal, offering no metrics such as precision, recall, or correlation with human-annotated importance scores. Consequently, the interpretability claims remain unverified.\n\nOverall, the paper’s experiments seem too limited to substantiate the strong claims made about interpretability and generalization. The presented results focus on verifying internal consistency rather than demonstrating practical usefulness or faithfulness. The work reads more like a proof-of-concept or early-stage project proposal than a fully validated research contribution. Substantial additional experiments like quantitative evaluations and ablations are needed to establish credibility."}, "questions": {"value": "1. Could the authors provide a more formal definition of the “concept space”? How does it differ from a standard sentence embedding?\n\n2. The paper lacks experiment and implementation details. For example, given the alternating updates between the EBM and the interpreter, how stable was the training process? Did you observe mode collapse, oscillations, or sensitivity to hyperparameters?\n\n3. It would also strengthen the paper if the authors explicitly analyzed when and why the method fails."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fxUXlJUH3t", "forum": "Uh0F0079Lh", "replyto": "Uh0F0079Lh", "signatures": ["ICLR.cc/2026/Conference/Submission24982/Reviewer_5awW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24982/Reviewer_5awW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615969457, "cdate": 1761615969457, "tmdate": 1762943272860, "mdate": 1762943272860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage, concept-level interpretability framework for black-box LLMs. First, an Energy-Based surrogate model is trained on sentence embeddings to assign low energy to authentic ⟨prompt, response⟩ pairs and high energy to negatives. Second, a lightweight interpreter guided by the EBM returns a binary mask of input sentences deemed most influential for a target output sentence."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It is a meaningful attempt to perform interpretability analysis on black-box models."}, "weaknesses": {"value": "- The paper equates sentence-level influence selection with explaining LLM reasoning, but does not validate faithfulness or causal alignment to the LLM’s actual inference. The EBM demonstrates pairwise consistency discrimination not a mapping to the LLM’s reasoning steps. \n- The only interpreter results are a few sentimental examples with highlighted sentences. There are **no quantitative metrics** and **no comparisons to any baselines**.\n- While the architecture is described, the paper does not provide a testable definition linking the learned energy to reasoning processes or a decomposition connecting energy terms to human-interpretable concepts. The EBM is essentially an attention-pooled scoring network trained with InfoNCE.\n- The interpreter is shown only on sentiment summarization, no evidence is given for multi-step reasoning, factuality, or chain-of-thought-like tasks."}, "questions": {"value": "Authors are advised to give more experimental evidence to support this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pIVBkEZH22", "forum": "Uh0F0079Lh", "replyto": "Uh0F0079Lh", "signatures": ["ICLR.cc/2026/Conference/Submission24982/Reviewer_9M1y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24982/Reviewer_9M1y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656784508, "cdate": 1761656784508, "tmdate": 1762943272532, "mdate": 1762943272532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for attributing a model’s output to specific input sentences within a prompt. By training a transformer-based energy network over the sentences of both input and output, the framework learns the semantic relationships between them in the embedding space. This enables the identification of the most influential subset of input sentences that contribute to the model’s response."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper extends attribution-based interpretability to the sentence level, which is a meaningful direction beyond token-level attribution. The proposed method is conceptually straightforward and well-motivated.\n\nExperiments are conducted to demonstrate the interpreter model’s ability to identify meaningful input–output relationships, successfully distinguishing authentic from fabricated examples. The sentiment analysis case study further illustrates the interpreter’s capacity to attribute responses to interpretable input sentences. However, the case study remains relatively simple compared to real-world tasks such as chain-of-thought (CoT) reasoning.\n\nOverall, this paper presents a novel and interesting approach for attributing model outputs to input sentences."}, "weaknesses": {"value": "- The problem statement is not clearly articulated until Section 3, making it difficult for readers to grasp the main objective early on.\n- The phrase “concept-level attribution” may be misleading—this work primarily addresses sentence-level attribution rather than conceptual abstraction.\n- Influence function studies should be discussed as related work, as they are closely connected to input–output attribution and could help position this work more precisely within the interpretability literature.\n- Lines 189–190: The term “indices of the target sentences in the output” is unclear. If this refers to parts of the input, it should be formally defined in the interpreter’s function.\n- Lines 470–471: The claim about diagnosing model failures such as hallucination and internal bias seems overstated given the current experimental scope."}, "questions": {"value": "- Section 3.1: When sentences are segmented using Sentence-BERT, how are their embeddings used for input and output? It appears they are treated sequentially as embeddings, but this should be clarified when first introduced.\n- Section 3.3: Is the number of sentences in $\\mathbf{x}$ fixed to allow elementwise multiplication? If so, please clarify this assumption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rrPEetGjHy", "forum": "Uh0F0079Lh", "replyto": "Uh0F0079Lh", "signatures": ["ICLR.cc/2026/Conference/Submission24982/Reviewer_fzd1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24982/Reviewer_fzd1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799362003, "cdate": 1761799362003, "tmdate": 1762943272233, "mdate": 1762943272233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a post-hoc, model-agnostic framework for interpreting LLMs. It trains a transformer-based energy model (EBM) to approximate the LLM’s reasoning process and identify which prompt sentences most influence specific output sentences. By learning sentence-level attributions across multiple prompts, the method provides global, concept-level explanations that capture the LLM’s overall behavior and biases, offering a more interpretable alternative to local or token-level attribution methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a creative, post-hoc, model-agnostic method for interpreting black-box LLMs, which does not require access to model internals.\n2. The use of a transformer-based EBM as a differentiable surrogate to approximate LLM reasoning is methodologically interesting."}, "weaknesses": {"value": "1. The framework depends on how accurately the EBM captures the target LLM’s internal logic. If the EBM fails to approximate the reasoning process well, the entire interpretation may be unreliable.\n2. The two-stage training (EBM pretraining + interpreter fine-tuning) with alternating optimization increases computational cost and implementation complexity, limiting practical usability.\n3. All experiments are conducted on GPT-4o mini and a sentiment analysis task. The lack of multi-domain or multi-model validation limits claims of generalizability and model-agnosticism.\n4. The pre-training uses only 12K–20K samples from the HC3 dataset, which is small compared to standard LLM-scale interpretability settings. Additionally, only movie reviews are used for final interpretation results, lacking diversity in domains or prompt types."}, "questions": {"value": "1. The paper claims that the Energy-Based Model (EBM) captures the generation dynamics of the target LLM. How do the authors ensure that the learned energy function truly reflects the reasoning or decision-making process of the black-box LLM rather than simply modeling surface-level text similarity?\n2. The paper emphasizes the widening energy gap between positive and negative samples as evidence of better modeling. However, is there empirical evidence that a larger energy gap directly translates to more accurate or meaningful interpretation?\n3. Since the framework involves both an EBM (up to 181M parameters) and an interpreter with attention layers, how does the computational cost compare with simpler post-hoc methods? Is it practical for real-world interpretability settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YIo6pApgja", "forum": "Uh0F0079Lh", "replyto": "Uh0F0079Lh", "signatures": ["ICLR.cc/2026/Conference/Submission24982/Reviewer_KyCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24982/Reviewer_KyCJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897915175, "cdate": 1761897915175, "tmdate": 1762943271943, "mdate": 1762943271943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}