{"id": "0xHWd4CUaX", "number": 25618, "cdate": 1758369519915, "mdate": 1759896712907, "content": {"title": "Contrastive Code Graph Embeddings for Reinforcement Learning-Based Automated Code Refactoring", "abstract": "We propose a novel reinforcement learning (RL) framework for automated code refactoring that uses contrastive pre-trained code graph embeddings to overcome the limitations of the traditional heuristic-based reward functions. The key challenge is balancing the implementation of syntactic improvements - while maintaining the semantics of the code being refactored - something that necessarily requires the existing RL approaches to accomplish and that most often do last year because of the handcrafted nature of their metrics. Our approach presents a syntax-guided contrastive encoder that acquires structural invariant representations of code graphs by relating structurally augmented variants under a self-supervised objective. These embeddings are then combined with standard measures of code quality in a composite reward function, allowing the RL agent to reason about both low-level changes to the syntactic structure as well as high-level changes in the semantic structure. The policy network itself, which takes the form of a graph attention network, runs on the joint representation space directly, which models dependency on the context on the code structure.", "tldr": "", "keywords": ["Code Refactoring"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04fca20531f3055d6a923ceecac8bb98608a947e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript strongly resembles AI-generated content and may have been produced as an internal test for prospective AI researchers. If so, it suggests that the current state of such roles remains immature and requires further development."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I believe this paper was generated by AI. If not, please let me know."}, "weaknesses": {"value": "I believe this paper was generated by AI. If not, please let me know."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LijHT48073", "forum": "0xHWd4CUaX", "replyto": "0xHWd4CUaX", "signatures": ["ICLR.cc/2026/Conference/Submission25618/Reviewer_BKDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25618/Reviewer_BKDb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793531068, "cdate": 1761793531068, "tmdate": 1762943496678, "mdate": 1762943496678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s main contribution is introducing a contrastive pre-training and reinforcement learning (RL) framework for automated code refactoring. It proposes a syntax-guided contrastive encoder that learns structural-invariant code graph embeddings using transformations like subtree masking, edge rewiring, and identifier shuffling. These embeddings are combined with conventional code-quality metrics (e.g., cyclomatic complexity, coupling) into a composite reward function, enabling the RL agent to balance syntactic improvements with semantic preservation. A graph attention network (GAT) policy operates on this joint representation space, with an embedding-guided exploration strategy that biases actions toward high-reward latent states. Experiments on multiple code-refactoring benchmarks show improved syntactic improvement, semantic preservation, and generalization compared with rule-based, learning-based, and previous RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Originality: Creative combination of syntax-guided contrastive pre-training on code graphs with RL for refactoring, including semantics-preserving augmentations and an embedding-guided exploration bias, an interesting twist beyond hand-crafted rewards alone.\n\n\nClarity: The paper lays out a modular pipeline and presents core equations for message passing and contrastive loss, which helps readers follow the design even if prose needs polish."}, "weaknesses": {"value": "Contrastive pre-training is under-specified.\nThe paper names three augmentations and gives an InfoNCE objective, but omits crucial details: augmentation probabilities/ratios, semantic-preservation checks for positives, projection head design, optimizer/LR schedule, and negative sampling strategy. Actionable: add a “Pretraining Details” appendix with full recipe.\n\nNo standalone evaluation of the learned embeddings.\nPre-training quality is inferred only indirectly via downstream ablation; there’s no linear probe, retrieval, clustering, or probe tasks on code-understanding benchmarks. Actionable: add pretraining-only evaluations before RL. The current Experiment section list does not include a pretraining evaluation subsection.\n\nComposite reward lacks operational clarity.\nEq. (5) defines a fused reward but does not specify the exact metrics vector, normalization windows, or how weights (w_q, α, β, γ) were chosen/tuned; no sensitivity study is reported. Actionable: precisely enumerate metrics (and implementations), describe normalization (per-project or global), report a small grid/sensitivity study, and include per-component ablations on at least one dataset.\n\n\nPositioning/novelty vs. close baselines is thin.\nThe approach is compared to rule-based, learned, and RL baselines, but there’s no GraphCodeBERT + identical RL head baseline to isolate the value of this contrastive pretraining recipe. Actionable: add baselines where you swap in public code graph encoders (e.g., GraphCodeBERT) under the same RL stack, and discuss where your gains come from (augmentations? reward fusion?)."}, "questions": {"value": "Contrastive Pre-training Clarity:\nCould the authors provide full details of the contrastive pre-training process, specifically the architecture of the projection head, augmentation ratios and constraints, optimizer settings, and how semantic equivalence between augmented graph pairs was validated? The current description (Section 4.1) lists transformations but lacks implementation-level transparency.\n\nEvaluation of Learned Embeddings:\nHow do you verify that the pre-trained embeddings capture meaningful structural and semantic code relationships before reinforcement learning fine-tuning? No standalone embedding evaluation is presented, making it unclear whether contrastive pre-training itself contributes to downstream performance.\n\nNovelty and Baseline Comparison:\nGiven that models like GraphCodeBERT already perform syntax-guided contrastive pre-training for code representation, what are the novel aspects of your method beyond integrating the embeddings into the RL reward? Have you compared your approach directly against those encoders under identical RL settings to isolate the impact of your proposed pre-training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qmK0wpQx0m", "forum": "0xHWd4CUaX", "replyto": "0xHWd4CUaX", "signatures": ["ICLR.cc/2026/Conference/Submission25618/Reviewer_qRvF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25618/Reviewer_qRvF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945936596, "cdate": 1761945936596, "tmdate": 1762943496291, "mdate": 1762943496291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for automated code refactoring utilizing reinforcement learning (RL). The core contribution is a composite reward function that guides an RL agent, intended to replace traditional handcrafted metrics. This function combines (1) contrastive pre-trained code graph embeddings, (2) standard code quality metrics, and (3) a differential test-based semantic preservation score. The agent's policy is represented by a graph attention network  that operates on code graph representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and practical problem: the automated refactoring of code to improve quality and reduce technical debt. The core motivation to move beyond purely heuristic-based reward functions and incorporate learned, semantically-aware representations  is a valid and promising research direction. The proposed framework, which integrates contrastive pre-training, a graph-based policy network, and explicit semantic checks, is in concept, comprehensive."}, "weaknesses": {"value": "1. Feasibility: The most significant weakness is the methodological soundness of the semantic preservation component. The claim of using symbolic execution —an algorithm known for its high computational complexity—as a \"lightweight\" reward signal in an RL loop  is extraordinary and requires substantial justification. Without a detailed explanation of how this is achieved tractably, the entire experimental results section  is called into question.\n2. Unclear Reward Formulation: The rationale for including \"Embedding dynamics\" as a positive reward component is not well-justified. The paper does not provide a clear theoretical or empirical argument for why a large magnitude of movement in the latent space should inherently correspond to a high-quality refactoring step. This component feels arbitrary and is insufficiently motivated.\n3. Poor Presentation: The text is replete with syntactically awkward constructions, and unclear sentences, which severely impede comprehension. This lack of clarity obscures the paper's technical details and makes a thorough evaluation difficult."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PgTHKLNz4Z", "forum": "0xHWd4CUaX", "replyto": "0xHWd4CUaX", "signatures": ["ICLR.cc/2026/Conference/Submission25618/Reviewer_4u9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25618/Reviewer_4u9X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762393862269, "cdate": 1762393862269, "tmdate": 1762943495870, "mdate": 1762943495870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called Contrastive Code Graph Embeddings (CCGE) for reinforcement learning-based automated code refactoring. The authors aim to overcome the limitations of hand-engineered reward functions by leveraging contrastive pre-training on code graphs and incorporating the learned embeddings into a composite reward function that also accounts for syntactic and semantic correctness. The policy network is implemented as a graph attention network (GAT) operating on the learned joint representation.\nExperiments are conducted on several code refactoring datasets (Refactory, CodeRef, BigCloneBench), showing improvements over traditional static, learning-based, and prior RL baselines in terms of syntactic improvement, semantic preservation, and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Timely and relevant problem – Automated refactoring is an active topic bridging ML and software engineering.\n\nMethodologically plausible – Uses established concepts (contrastive learning + RL) in a novel context.\n\nComprehensive experimental evaluation – Multiple datasets, baselines, and ablations.\n\nEmpirical gains are consistent and supported by reasonable qualitative examples.\n\nClear problem formulation with interpretable reward components and graph structures."}, "weaknesses": {"value": "Writing quality is poor, with several unnatural or incorrect phrases (possibly LLM-edited text).\n\nLimited novelty – the method extends existing ideas (contrastive pre-training, embedding-based rewards) without strong theoretical or algorithmic innovation.\n\nLack of detailed methodology – unclear how exploration and semantic testing modules are implemented; no algorithm listing or complexity analysis.\n\nEvaluation metrics are not standardized – “Syntactic Improvement” and “Maintainability Gain” are not well defined or justified.\n\nNo discussion of failure cases or negative transfer during cross-language testing.\n\nThe “Use of LLM” statement at the end suggests possible overreliance on AI-assisted writing without thorough review."}, "questions": {"value": "How is the balance between traditional metrics and embedding dynamics in Eq. (5) determined? Are α, β, γ tuned manually?\n\nAre the embeddings fine-tuned during RL or kept frozen after pre-training?\n\nHow are the semantic equivalence tests generated—through symbolic execution or data-driven inference?\n\nWhat is the computational cost of contrastive pre-training and how scalable is it for multi-language corpora?\n\nCould this framework extend to code optimization or bug repair beyond refactoring?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PWioz0ha92", "forum": "0xHWd4CUaX", "replyto": "0xHWd4CUaX", "signatures": ["ICLR.cc/2026/Conference/Submission25618/Reviewer_EV4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25618/Reviewer_EV4W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762912297901, "cdate": 1762912297901, "tmdate": 1762943495718, "mdate": 1762943495718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}