{"id": "JOkqoUpF0c", "number": 18589, "cdate": 1758289302965, "mdate": 1759897093873, "content": {"title": "AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided Autoregressive Perspective", "abstract": "Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. To ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text/audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized phoneme-frame causal attention mask to support multi-step parallel acceleration. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.", "tldr": "", "keywords": ["talking-head animation", "autoregressive modeling", "visual generation", "generative models", "multimodality"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a13bc1ec2847fcf8d4d37e023d958e499d93cb06.pdf", "supplementary_material": "/attachment/533c7525be819538f1ee3fa5da500c555ebf8491.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an autoregressive generation framework for the talking-head video generation task. To alleviate the one-to-many relationship between speech and motion, as well as the flicker issue in previous autoregressive methods, the framework converts audio input into phonemes and then adopts a two-stage generation strategy: the first stage generates key frames, and the second stage generates interpolated frames between the key frames. Experiments demonstrate that the proposed method achieves promising performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper employs an Autoregressive Method (instead of the commonly used diffusion methods). Such an attempt is relatively rare, and the method achieves excellent efficiency and quality.\n- The paper conducts extensive experimental demonstrations, covering multiple aspects of metrics, which is quite comprehensive.\n- The paper is written very clearly and is easy to understand."}, "weaknesses": {"value": "- The proposed two-stage \"key frame + interpolation\" method lacks novelty, as this is actually an existing generation strategy (the \"Two temporal res.\" mentioned in [1]). The authors need to clarify the core differences between this two-stage generation strategy and previous methods.\n- Using phonemes may lead to information loss, such as emotion-related information and stress-related information. This may result in the lack of vividness in head movements, eye blinks, and facial expressions. The authors need to demonstrate the necessity of using phonemes (e.g., through ablation studies comparing with the commonly used audio embedding features in previous works) or provide theoretical proof that this modification is essential.\n- Additionally, the paper lacks subjective metrics, such as user studies. It is recommended that the authors supplement such experiments or explain the reasons for being unable to do so.\nReference\n\n\n[1] Harvey, William, et al. \"Flexible diffusion modeling of long videos.\" Advances in neural information processing systems 35 (2022): 27953-27965."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hUmNshgy1l", "forum": "JOkqoUpF0c", "replyto": "JOkqoUpF0c", "signatures": ["ICLR.cc/2026/Conference/Submission18589/Reviewer_1bVj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18589/Reviewer_1bVj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760885857963, "cdate": 1760885857963, "tmdate": 1762928310043, "mdate": 1762928310043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AvatarSync, an autoregressive framework for talking-head animation that leverages phoneme representations and a two-stage generation strategy to produce high-quality, lip-synchronized facial videos from a single image and audio input."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "（1）Structurally, the paper is well-organized, with clear explanations of the methodology, theoretical analysis of inter-frame flicker, and illustrative figures.\n（2）In terms of quality, extensive experiments on two benchmark datasets demonstrate clear improvements in both quantitative metrics (e.g., FID, FVD) and qualitative results.\n（3）The paper introduces a novel autoregressive framework that effectively leverages phoneme representations for talking-head generation, with the phoneme-frame causal attention mask and two-stage hierarchical generation strategy being both creative and well-motivated."}, "weaknesses": {"value": "（1）The method primarily focuses on comparisons with strong baselines like Hallo, while insufficient attention is given to including newer approaches such as VASA-1. It also lacks direct comparisons with other autoregressive methods (e.g., VideoPoet, Teller) in terms of speed and resource consumption. Although comparisons with diffusion models demonstrate certain advantages, they fail to adequately establish its leading position within the autoregressive paradigm.（2）The experimental evaluation mainly concentrates on overall video quality and lip synchronization, while lacking specialized assessment and analysis of subtle facial expressions such as natural blinking and lip pursing.\n（3）The approach relies heavily on multiple high-performance GPUs, which may limit its reproducibility for users with consumer-grade hardware. Demonstrating its performance on more accessible computing resources would be valuable."}, "questions": {"value": "（1）How well does the model scale with longer phoneme sequences? Does performance degradation occur when generating extended videos?\n（2）The paper states that the interpolation module can process different keyframe pairs in parallel. Does this imply that the entire second stage is fully parallelizable? If so, what is the theoretical speedup ratio? Please elaborate on the specific parallelization strategy and its practical benefits in real-world deployment.\n（3）Have you considered incorporating more diverse scenarios in experiments, such as adding noise, to further test the model's robustness in real-world environments?\n（4）The phoneme-frame causal attention mask forces each keyframe to focus solely on its corresponding phoneme. Could this strict one-to-one mapping limit the model's ability to learn coarticulation effects across adjacent phonemes? Would this limitation affect the fluency and naturalness of the resulting lip movements?\n（5）The experiments involve a 4× super-resolution preprocessing step on the CMLR dataset. Is there any quantitative evidence demonstrating the specific contribution of this operation to the final model performance compared to using the original data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EdHOWd885u", "forum": "JOkqoUpF0c", "replyto": "JOkqoUpF0c", "signatures": ["ICLR.cc/2026/Conference/Submission18589/Reviewer_rEVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18589/Reviewer_rEVM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540356773, "cdate": 1761540356773, "tmdate": 1762928309008, "mdate": 1762928309008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AvatarSync, an autoregressive (AR) framework on phoneme presentations with a two-stage pipeline : (1) Facial KeyFrame Generation (FKG) module that maps phoneme tokens (from text or audio) to sparse keyframes under Phoneme-Frame Causal Attention Mask, and (2) a tilmestep-aware, selective state-space modeling, which enables temporally coherent inference. The paper argues autoregressive framework mitigates flicker issues and improves efficiency. Extensive experiments cover performances on both CMLR (Chinese) and HDTF (English)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Phoneme-conditioned keyframes and timestamp-aware interpolation is a clean split, and one-to-one causal attention is motivated and ablated. \n2. The paper cross-lingual settings. \n3. Attention variants and composite losses are well-documented."}, "weaknesses": {"value": "1. Baseline Coverage \nTable1 includes several known baselines, but newer strong systems (e.g., StableAvatar, HunyuanAvatar, Hallo 3) are missing. \n\n2. Missing human evaluation and demos. \nThe paper presents no user study and no supplementary videos, leaving perceptual claims (realism, temporal stability, lip sync, identity) unsupported by human evidence or visual inspection. As results rely solely on automatic metrics, the basis for performance judgment is insufficient. \n\n3. Metric weakness\nIdentity/facial similarity are used as losses, but test-time identity-preservation metrics are not surfaced alongside the metrics used in the main paper. Also, the paper emphasizes \"responsive user experience in the real-world setting\", but does not report the metrics of autoregressive efficiency. Please provide complementary identity-related metrics (e.g., ArcFace, CSIM), and efficiency-related metrics (e.g., FPS, latency, VRAM). \nTable2 omits any lip-sync metrics, and across the paper LSE-C (Sync-C) is largely absent too. Please explain the omission. \n\n4. Attribution gap (what component actually helps?) \nIt is unclear how much each stage contributes. While the paper ablate the attention mechanism within FKG, there is no ablation isolating the timestamp-aware adaptive strategy in the inter-frame module;  hence the relative contributions of FKG vs. the temporal module are not established. Please add a controlled ablation that (i) disables timestamps, (ii) replaces the adaptive strategy with simpler temporal heads and (iii) final version (timestamp-aware adaptive strategy).\n\n5. Phoneme timing and coarticulation.\nA one-to-one causal mask may ignore coarticulation or context span. There is no analysis of context window , lip sync or forced alignment quality. \n\n6. Robustness gaps \nWould the authors comment on whether any cases involving occlusions, accessories (e.g., hats, glasses), pronounced head-pose variation, or mouth-region artifacts (e.g., teeth, tongue) were observed? If so, a brief analysis or examples would be helpful."}, "questions": {"value": "I would appreciate responses to the questions in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A6KsZparea", "forum": "JOkqoUpF0c", "replyto": "JOkqoUpF0c", "signatures": ["ICLR.cc/2026/Conference/Submission18589/Reviewer_ERg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18589/Reviewer_ERg9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826159495, "cdate": 1761826159495, "tmdate": 1762928308500, "mdate": 1762928308500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the inter-frame flickering and slow inference problems of existing GAN and diffusion models, and proposes an autoregressive framework that leverages a many-to-one mapping from text/audio to phonemes and a two-stage strategy to enhance temporal coherence and visual smoothness. Experimental results show improved temporal stability and efficiency of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The phoneme tokenization is inspiring, and the representation is easy to follow.\n2. Empirical results show improved video quality, temporal coherence, and efficiency, which support the claims of the paper."}, "weaknesses": {"value": "1. The paper only considers one reference image for the framework, while in practice, multiple reference images with different facial angles might be provided. How will the framework scale to this case? And will the diversity among reference images introduce noise into the tokens?\n2. Humans are sensitive to inter-frame flicker, but there is a lack of user study in the experiments.\n3. Providing synthesized video examples is recommended to further support the claims of the paper."}, "questions": {"value": "Please answer the questions in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1DTW0rjulw", "forum": "JOkqoUpF0c", "replyto": "JOkqoUpF0c", "signatures": ["ICLR.cc/2026/Conference/Submission18589/Reviewer_7vpw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18589/Reviewer_7vpw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762496044346, "cdate": 1762496044346, "tmdate": 1762928307901, "mdate": 1762928307901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AvatarSync, a novel framework for audio-driven talking-head animation that proposes a paradigm shift away from the prevailing GAN and diffusion-based approaches. The authors identify critical limitations in existing methods: GANs often suffer from visual artifacts and instability, while diffusion models, despite their high fidelity, are plagued by slow inference speeds and a characteristic inter-frame flicker that breaks temporal consistency.\nTo address these issues, AvatarSync reframes the task as a phoneme-guided autoregressive sequence generation problem, inspired by recent successes of large language models in video synthesis. The core of the method is a clever two-stage generation strategy designed to balance quality and efficiency:\nFacial Keyframe Generation (FKG): An autoregressive model generates a sparse set of high-quality facial keyframes conditioned on phoneme sequences extracted from the input audio/text. A key innovation here is the \"Phoneme-Frame Causal Attention Mask,\" which enforces a strict alignment between phonemes and their corresponding visual frames, ensuring precise lip synchronization.\nInter-Frame Interpolation: A second, lightweight module then fills in the intermediate frames between the generated keyframes. This module uses a timestamp-aware strategy to ensure the final video is temporally coherent, smooth, and computationally inexpensive to render.\nThe authors provide extensive experimental validation on the CMLR and HDTF datasets, demonstrating that AvatarSync achieves state-of-the-art results across multiple metrics, including visual fidelity (FID, FVD), lip-sync accuracy (Sync-D), and identity preservation. Crucially, the method is shown to be significantly faster than diffusion-based competitors and qualitatively free from the inter-frame flicker problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper does not merely propose an incremental improvement but \"rethinks\" the entire problem. By identifying the fundamental limitations of GANs and diffusion models for this specific task and proposing a coherent autoregressive alternative, the authors make a significant conceptual contribution. The framing is clear, well-motivated, and timely.\n2. The FKG + Interpolation architecture is a highly intelligent solution to the quality-efficiency trade-off. It leverages the power of a large autoregressive model for the most critical parts of the animation (the keyframes) while delegating the more repetitive task of in-betweening to a faster, specialized module. This design is a key reason for the method's success.\n3. A major strength of AvatarSync is its ability to generate temporally stable videos. The autoregressive generation of keyframes inherently builds upon previous frames, and the interpolation further ensures smoothness. The flicker visualization in Figure 5(b) is a powerful and convincing piece of evidence that directly showcases the superiority of this approach over diffusion models in eliminating distracting artifacts."}, "weaknesses": {"value": "1. The paper's core motivation rests heavily on the premise that diffusion-based methods are inherently slow due to their iterative denoising process. While this is true for traditional DDPM/DDIM samplers, this argument is becoming outdated. Recent advancements in diffusion model distillation have enabled real-time, one-step generation. For instance, [1] demonstrate a diffusion-based approach that achieves real-time performance. The paper's efficiency claims would be much stronger if they were contextualized against these modern, distilled diffusion models, rather than just standard, slow samplers. Without this discussion, the claimed efficiency advantage feels overstated.\n2. The paper introduces a two-stage generation process but fails to compare against other highly relevant and state-of-the-art two-stage methods. A critical omission is AniPortrait [2], a very popular and powerful framework that also employs a two-stage approach (first generating keyframes/poses and then rendering the video). Given the conceptual similarity in using a two-stage pipeline to tackle the problem, a direct comparison—or at least a detailed discussion—is essential to properly situate AvatarSync's contribution. Without it, it is difficult to assess whether the novelty and performance gains come from the autoregressive formulation itself or simply from adopting a two-stage strategy.\n\nReference:\n[1].Guo, Hanzhong, et al. \"Real-time One-Step Diffusion-based Expressive Portrait Videos Generation.\" arXiv preprint arXiv:2412.13479 (2024).\n[2].Wei, Huawei, Zejun Yang, and Zhisheng Wang. \"Aniportrait: Audio-driven synthesis of photorealistic portrait animation.\" arXiv preprint arXiv:2403.17694 (2024)."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5XM1dCYVnO", "forum": "JOkqoUpF0c", "replyto": "JOkqoUpF0c", "signatures": ["ICLR.cc/2026/Conference/Submission18589/Reviewer_AupQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18589/Reviewer_AupQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762835511805, "cdate": 1762835511805, "tmdate": 1762928307318, "mdate": 1762928307318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}