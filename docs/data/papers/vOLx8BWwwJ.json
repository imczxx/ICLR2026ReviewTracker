{"id": "vOLx8BWwwJ", "number": 17590, "cdate": 1758277910725, "mdate": 1759897166084, "content": {"title": "Rethinking Regularization in Federated Learning: An Initialization Perspective", "abstract": "In federated learning, numerous regularization methods have been introduced to alleviate local drift caused by data heterogeneity. While all share the goal of reducing client drift, their effects on client gradients and the resulting features learned by local models differ. \nOur comparative analysis shows that among the tested regularization methods, FedDyn is the most effective, \nachieving superior accuracy-to-round while simultaneously reducing inter client gradient divergence and \npreserving global model features during local training.\nNevertheless, regularization methods, including FedDyn, are only approximations of an ideal scheme that would completely remove local drift and guarantee convergence to the global stationary point. In practice, deviations from this ideal give rise to side effects and, together with the additional computational and communication costs, limit their practicality. Since the performance differences among federated learning algorithms diminish once models are well-initialized, it is more efficient to restrict regularization to the pre-training phase, where its benefits outweigh these drawbacks. Our study of pre-training strategies for FedAvg demonstrates that FedDyn provides the most effective initialization, a property tied to its convergence behavior near the global stationary point. Extensive experiments across both cross-silo and cross-device settings confirm that applying FedDyn solely for pre-training yields faster convergence and reduced overhead compared to maintaining regularization throughout the entire training process.", "tldr": "We show that using FedDyn solely as an initialization strategy, rather than throughout the entire training process, achieves faster convergence and lower cost than standalone regularization methods.", "keywords": ["federated learning", "data heterogeneity", "regularization", "initialization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1677494ae7c35e0f4e613bc106ad64790de59259.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper points out that existing SoTA methods, such as FedDyn, may impose too strong the regularization that impedes from reaching better global minima. The authors propose an intriguing trick: switch back to vanilla FedAvg. With analysis from gradient diversity, cosine similarity, and feature cluster analysis, this work shows improvement over constrained setup (only Table 1 and Figure 1 seems to be serious quantitative results to me). I encourage the authors to provide additional experiments to better understand this algorithm-switch phenomenon to better understand and strengthen their observations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The algorithm-switch phenomenon is interesting and Figure 1 is specifically convincing to me.\n2. The perspective and comparison with learning decay is important to ensure the performance gain is not similar to learning rate scheduler."}, "weaknesses": {"value": "1. Limited algorithm choice\n\nAll the main quantitative analysis focuses on FedDyn, and there are no metrics regarding FedNTD or Scaffold. The authors also should consider more recent algorithms such as FedAlign, FedExp, FedProto, FedMD, or others. It is hard to demonstrate how robust, consistent, and reproducible this method is empirically without a thorough comparison. I strongly encourage the authors to include an additional quantitative table comparing at least three additional federated learning algorithms using the same setup as in Table 1. \n\n2. What is the main contribution of algorithm-switching?\n\nI am not fully convinced that \"regularization impedes global feature learning\" is the best explanation for the working mechanism of the proposed method. My intuition is that changing (or switching) the minimization goal prevents the model from getting stuck at a local minimum, because that is effectively what the proposed method is doing. I would appreciate it if the authors could conduct additional experiments to prove this hypothesis right or wrong. For example, instead of switching from FedDyn to FedAvg, can the authors try switching from FedAvg to FedDyn? Another interesting setup would be to switch between FedDyn and FedAvg every 20 rounds and evaluate the performance."}, "questions": {"value": "1. Can the authors explain why in Figure 1b there are some abrupt drops of diversity but in Figure 1c the cosine similarities increase monotonically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fXllfPRMwo", "forum": "vOLx8BWwwJ", "replyto": "vOLx8BWwwJ", "signatures": ["ICLR.cc/2026/Conference/Submission17590/Reviewer_NfTo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17590/Reviewer_NfTo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761366392990, "cdate": 1761366392990, "tmdate": 1762927448443, "mdate": 1762927448443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes the use of regularization in federated learning, arguing that its primary benefit is in the early training phase. Through novel gradient and feature-level analyses, the authors show that while FedDyn effectively reduces client drift, its benefits diminish over time and it introduces side effects. They propose a two-stage strategy: use FedDyn for pre-training to find a strong initialization, then switch to standard FedAvg for fine-tuning. Experiments show this FedDyn -> FedAvg approach often achieves superior accuracy and faster convergence than using either method alone, while also reducing computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper shifts the focus from finding the \"best\" single algorithm to understanding when an algorithm is most beneficial, introducing a valuable \"initialization vs. fine-tuning\" paradigm.\n2. This paper provides deep, quantitative insights into how regularization methods work and where they fail by considering more metric like gradient diversity and feature interaction tensors.\n3. The proposed two-stage strategy is simple, effective, and computationally efficient."}, "weaknesses": {"value": "1. Lack of Theoretical Proof: The paper lacks a formal convergence proof for the proposed two-stage FedDyn -> FedAvg algorithm, relying instead on empirical results and a non-practical switching criterion.\n2. Switching Point: The effectiveness of the method depends critically on the switching point, which is chosen heuristically in the experiments. The paper offers no practical guidance on how to determine this crucial hyperparameter.\n3. Limited Generality: The conclusion that only FedDyn serves as a good initializer is not fully explained, limiting the generality of the \"regularization for initialization\" principle to other methods.\n4. This paper reads more like an experimental report than an academic paper. Its contribution and innovation seem somewhat weak for a conference like ICLR."}, "questions": {"value": "1. How can the optimal switching point be determined in a principled and adaptive way for new tasks?\n2. What is the fundamental reason that FedDyn succeeds as an initializer while a similar method like SCAFFOLD fails?\n3. Is the two-stage training principle generalizable to other combinations of FL algorithms beyond FedDyn -> FedAvg?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T0WCEb7QtB", "forum": "vOLx8BWwwJ", "replyto": "vOLx8BWwwJ", "signatures": ["ICLR.cc/2026/Conference/Submission17590/Reviewer_PREy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17590/Reviewer_PREy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809411493, "cdate": 1761809411493, "tmdate": 1762927448008, "mdate": 1762927448008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-examines the role of regularization in federated learning (FL) from an initialization perspective. Through a comparative analysis of client gradients and learned features, the authors find that FedDyn is the most effective regularization method for mitigating client drift caused by data heterogeneity. However, they argue that all practical regularization methods, including FedDyn, are imperfect approximations of an ideal scheme, leading to side effects and additional overhead that diminish their benefits in later training stages. Based on the observation that FL is less sensitive to heterogeneity when well-initialized, the paper proposes a two-stage training strategy: using FedDyn for pre-training to provide a strong initialization, followed by standard FedAvg for fine-tuning. Experiments in both cross-silo and cross-device settings demonstrate that this approach achieves faster convergence and higher accuracy compared to using regularization throughout the entire training process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides a good observational analysis of how different regularization methods impact client gradients and learned features in federated learning.\n* The writing is clear and the paper is well-structured.\n* The analysis is supported by abundant experimental figures, providing a multi-faceted view of the regularization effects."}, "weaknesses": {"value": "* The paper claims that regularization encourages local models to learn features that better align with the global model, but this claim is not supported by any theoretical convergence analysis.\n* The \"side effects\" of regularization are not clearly explained. The paper asserts that the server control variate in FedDyn \"does not accurately approximate the gradient of the global objective function\", but the reasoning is not fully developed. Furthermore, the claim of significant \"additional computational cost\" is not quantified. The overhead of adding a regularization term, which is often just a vector operation, seems marginal compared to the cost of model training.\n* The algorithms discussed (FedAvg, FedDyn, etc.) are all well-established. The main contribution appears to be the proposed two-stage training strategy, which is a combination of existing methods. The novelty of this contribution seems limited.\n* The paper provides a formal criterion for switching from FedDyn to FedAvg, but its practical application is unclear. The criterion depends on the term Ct, which seems difficult to compute in a real experiment. How is the switching point determined in the experiments in real experiments? The compution of Ct in the experiment is inpractical. A sensitivity analysis on the switching point would be beneficial."}, "questions": {"value": "* In Section 3, why is the analysis based on the \"pseudo-gradient\" instead of the exact gradient?\n* How to understand Figure 7 (c) and (d)? It seems the proposed algorithm is not competitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1s48nYGWEx", "forum": "vOLx8BWwwJ", "replyto": "vOLx8BWwwJ", "signatures": ["ICLR.cc/2026/Conference/Submission17590/Reviewer_c19g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17590/Reviewer_c19g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954358772, "cdate": 1761954358772, "tmdate": 1762927447230, "mdate": 1762927447230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a compelling argument for rethinking the role of regularization in FL. Its core thesis is that while regularization methods like FedDyn are highly effective at mitigating client drift, they are computationally expensive and their benefits diminish after the model is well-initialized. The authors' key proposal—a two-stage training strategy that uses FedDyn only for pre-training before switching to standard FedAvg—is novel, practical, and well-supported by experimental evidence from gradient and feature-learning perspectives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper goes beyond mere accuracy plots. The analysis from gradient perspective (diversity, cosine similarity) and feature perspective (interaction tensor) provides a much deeper, mechanistic understanding of why FedDyn works better than other methods. This is a major strength.\n2. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. The paper states that FedDyn has \"side effects\" (e.g., the server control variate inaccurately approximates the global gradient, negatively impacting features). However, this is not demonstrated as clearly as its benefits.\n2. The analysis focuses heavily on FedDyn, with SCAFFOLD, MOON, and FedNTD as comparisons. While justified by FedDyn's performance, a broader discussion of why this two-stage strategy might or might not work for other state-of-the-art methods (e.g., FedProx) would strengthen the generalizability of the claim."}, "questions": {"value": "Please refer to weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lDl1tT9Nvk", "forum": "vOLx8BWwwJ", "replyto": "vOLx8BWwwJ", "signatures": ["ICLR.cc/2026/Conference/Submission17590/Reviewer_GRHu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17590/Reviewer_GRHu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007210724, "cdate": 1762007210724, "tmdate": 1762927446887, "mdate": 1762927446887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}