{"id": "xJT4fXJr1Q", "number": 15301, "cdate": 1758250064499, "mdate": 1759897314915, "content": {"title": "Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification", "abstract": "Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful contents, such as fact hallucinations or dangerous instructions. This misalignment  with human expectations, referred to as \\emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. \nWe extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective.", "tldr": "", "keywords": ["Misbehavior Detection", "Large Vision-Language Model", "Evidential Theory", "Uncertainty"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04292e1b2f4f962956658b4c62ddd79a1edc6ee0.pdf", "supplementary_material": "/attachment/66f6b7c0a9e58d2428ab11341432dea9402300b3.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Evidential Uncertainty Quantification (EUQ), a method to detect misbehaviors in Large Vision-Language Models (LVLMs) by quantifying two types of epistemic uncertainty: conflict (CF) and ignorance (IG). Using Dempster-Shafer Theory, EUQ models output-layer features as evidence, enabling efficient detection of hallucinations, jailbreaks, adversarial attacks, and OOD failures in a single forward pass. Extensive experiments on four LVLMs show that EUQ outperforms existing baselines in AUROC and AUPR, with insights into layer-wise uncertainty dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Good writing.\n2. The proposed method appears to have promising performance.\n3. The proposed indicators were effective across different series, validating the effectiveness of the method."}, "weaknesses": {"value": "1. The different types of epistemic uncertainty (CF and IG) quantified are all effective for hallucination detection.\n2. There is a lack of baseline data for some hallucination detection in 2025.\n3. Some work on the detection of LVLMs using evidence theory has not been discussed fully.\n4. It is recommended to test on a larger model, such as 72B, because there are often inconsistencies between small and large models.\n5. Can CF and IG complement each other to improve the final detection performance?\n6. It's best to separate citations from the main text, for example, using \\citep."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OF5BWnpEjW", "forum": "xJT4fXJr1Q", "replyto": "xJT4fXJr1Q", "signatures": ["ICLR.cc/2026/Conference/Submission15301/Reviewer_RBSV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15301/Reviewer_RBSV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751847882, "cdate": 1761751847882, "tmdate": 1762925597164, "mdate": 1762925597164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Evidential Uncertainty Quantification (EUQ), a method for detecting misbehaviors in LVLMs including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution failures. The authors also investigate the conflict and ignorance uncertainty, and argue these two forms of epistemic uncertainty are sources for the misbehaviors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Interesting paper to read as it classifies different types of misbehaviors in VLMs and it is observed that CF/IG can be used to distinguish different types of misbehaviors in VLMs."}, "weaknesses": {"value": "1. Although Figure 4 and the appendix visualizations distinguish misbehavior types, there is no deeper linguistic or visual semantic analysis explaining why certain errors yield high CF or IG.\n2. Thresholding (which could vary across LVLMs, datasets, or misbehavior categories) would have to be determined externally. Additionally, since the authors propose metrics to evaluate misbehaviors in VLMs and make observations, the size of the datasets and the chosen VLMs (four VLMs with â‰¤ 8B parameters) appear somewhat small. What model is evaluated in Figure 5 for model size analysis is not clear.\n3. I also think this paper would benefit from including more baselines for hallucination/jailbreak detection. Predictive entropy is a quite simple baseline.\n\nMinor: typos on line 428 for 'adn'"}, "questions": {"value": "See weakness.\n1. Can you provide more linguistic or visual semantic analysis explaining why certain misbehaviors yield high CF or IG?\n2. Can you compare IG and CF with more baselines besides SE (e.g., POPE, HiddenDetect...) / add more datasets/LVLMs to validate the generalization ability of these two metrics and the validity of the observations?\n\n[Evaluating Object Hallucination in Large Vision-Language Models](https://aclanthology.org/2023.emnlp-main.20/) (Li et al., EMNLP 2023)\n\n[HiddenDetect: Detecting Jailbreak Attacks against Multimodal Large Language Models via Monitoring Hidden States](https://aclanthology.org/2025.acl-long.724/) (Jiang et al., ACL 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mWnjIDQgO0", "forum": "xJT4fXJr1Q", "replyto": "xJT4fXJr1Q", "signatures": ["ICLR.cc/2026/Conference/Submission15301/Reviewer_kLDs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15301/Reviewer_kLDs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871432118, "cdate": 1761871432118, "tmdate": 1762925596759, "mdate": 1762925596759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a training-free method to interpret the uncertainty of a VLM by computing the per-token conflict and ignorance uncertainty in the model's predictions. The authors show that their method outperforms other uncertainty interpretation methods with moderate compute overhead. The paper's methods are evaluated across hallucination, jailbreaking, and out-of-distribution generalization tasks at different model scales."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The main strengths of the paper are:\n1) Disaggregating the uncertainty into conflict and ignorance uncertainty to interpret the uncertainty of a VLM. This allows the authors to measure uncertainty in different contexts (e.g. hallucination, jailbreaking, out-of-distribution generalization).\n2) The experiments are thorough and conducted on multiple model families and expressly evaluated at many scales."}, "weaknesses": {"value": "There are no major weaknesses in the paper. However, in Figure 1 in the paper, the authors show an illustrative example of measuring uncertainty in chain-of-thought reasoning. It would be useful to see examples of how the authors' proposed method can identify uncertainty in these reasoning traces. Currently the authors only evaluate their method on benchmarks which often only measure uncertainty on shorter token sequences."}, "questions": {"value": "1) The paper does not explicitly introduce any mechanism to focus uncertainty on key or semantically important tokens. Would this make it unsuitable for tasks such as dense image captioning or long reasoning traces where only some tokens are key to measuring uncertainty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "u7NA6KWXIK", "forum": "xJT4fXJr1Q", "replyto": "xJT4fXJr1Q", "signatures": ["ICLR.cc/2026/Conference/Submission15301/Reviewer_TyT1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15301/Reviewer_TyT1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020634382, "cdate": 1762020634382, "tmdate": 1762925596430, "mdate": 1762925596430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies detecting the misbehaviors of VLMs, in particular, adversarial inputs, Jailbreak inpus, OOD inputs, and model hallucination. The method then extracts the logits from the VLM and process it using \"Evidence Theoryt\" and get an uncertainty quantification score. The authors then utilize this score to perform misbehavior detection and observe improved performance compared with baseline approaches on. a variety of methods.\n\nI have to say I really cannot understand the method. The paper's technique involves too many concepts I have never heard about:\n\n- Dempster-Shafer Theory\n- Aasic belief assignment, BBA\n- Degree of conflict\n- conflict (CF) and ignorance (IG)\n- Least Commitment Principle (LCP) \n\nI don't think this is the author's problem since this is all due to my lack of knowledge. However, my reviews may not provide lots of useful inputs. I would also appreciate it if the authors could explain things in simpler language, e.g. through an algorithm block, such that I could understand how things are implemented in practice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method seems to be very mathematically rigorous\n\n- The paper considered lots of datasets and models."}, "weaknesses": {"value": "- The proposed method does not seem to have too much novelty; I can't see why the proposed method is specific to VLM or why it cannot be applied on e.g. BERT, ResNet, LLM.\n\n- It is unclear if the method is applicable to closed-source model since the method requires access to logits.\n\n- The choice of baseline is little bit confusing, semantic entropy is for uncertainty quantification over free form generation, but many tasks here only require a single word as the output (if I understand correctly). What is the point of performing clustering here?"}, "questions": {"value": "- Why is the column title near lines 393 and 394 \"Method\" rather than \"model\"?\n\n- \"For multiple-choice and yes/no tasks, correctness is assessed using ROUGE-L Lin (2004) (threshold > 0.5).\" I don't understand why ROUGE is needed here; isn't accuracy applicable?\n\n- The prompt forces the model to hallucinate (line 1229)\n```\nPlease check whether the following description matches\nthe picture content. Just answer yes or no without explanation.\n<image caption>, \n```\nHas the author tried providing the model with the option to reject? Some recent work (e.g. https://arxiv.org/abs/2505.11804) shows that if prompted properly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "me5NH66EaU", "forum": "xJT4fXJr1Q", "replyto": "xJT4fXJr1Q", "signatures": ["ICLR.cc/2026/Conference/Submission15301/Reviewer_iGLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15301/Reviewer_iGLE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762298133246, "cdate": 1762298133246, "tmdate": 1762925595859, "mdate": 1762925595859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}