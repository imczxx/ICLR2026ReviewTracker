{"id": "zvzrDQTk1n", "number": 22569, "cdate": 1758332951440, "mdate": 1759896858946, "content": {"title": "AtomWorld: Benchmarking Spatial Reasoning in Large Language Models on Crystalline Materials", "abstract": "Large Language Models (LLMs) excel at textual reasoning and are beginning to\ndevelop spatial understanding, prompting the question of whether these abilities\ncan be combined for complex, domain-specific tasks. This question is essential in\nfields like materials science, where deep understanding of 3D atomic structures\nis fundamental. While initial studies have successfully applied LLMs to tasks\ninvolving pure crystal generation or coordinate understandings, a standardized\nbenchmark to systematically evaluate their core reasoning abilities across diverse\natomic structures has been notably absent. To address this gap, we introduce\nthe AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic\nInformation Files (CIFs), a standard structure representation format. These tasks,\nincluding structural editing, CIF perception, and property-guided modeling, reveal\na critical limitation: current models, despite establishing promising baselines,\nconsistently fail in structural understanding and spatial reasoning. Our experiments\nshow that these models make frequent errors on structure modification tasks, and\neven in the basic CIF format understandings, potentially leading to cumulative\nerrors in subsequent analysis and materials insights. By defining these standardized\ntasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale\nmodeling, crucial for accelerating materials research and automating scientific\nworkflows.", "tldr": "", "keywords": ["Large Language Models", "Spatial Reasoning", "Benchmark Evaluation", "Materials Science", "Crystalline Materials", "Geometric Operations"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a90abc819278319174faea7c8ac53044e7af3330.pdf", "supplementary_material": "/attachment/f48a0505f95b0d1eeb42be5d396cc37cbe027612.pdf"}, "replies": [{"content": {"summary": {"value": "The paper presents the AtomWorld benchmark to evaluate LLMs on tasks based on Crystallographic Information Files (CIFs), including structural editing, CIF perception, and property-guided modeling. The authors built this standardized benchmark to systematically evaluate the core reasoning abilities of LLMs across diverse atomic structures, hence bridging the gap for applying LLMs for materials science domains. The benchmarking results reveal that current LLMs consistently fail in structural understanding and spatial reasoning, frequently making errors on structure modification tasks, and even in basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis. As the future goal, AtomBench is expected to play a foundational role in both testing and developing the understanding of 3D CIF environments in the next generation of LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors built the AtomWorld, which lays the ground for advancing LLMs toward robust atomic-scale modeling. In contrast, previous work mainly focused on text-based or property-prediction tasks; this benchmark fills a major gap in assessing spatial understanding, which is essential for advancing agentic materials discovery.\n\n1. Beyond AtomBench, the authors add complementary tests, namely PointWorld, CIF-Repair, CIF-Gen, Chemical Competence Score (CCS), and StructProp, each of which targets a different level of reasoning. This design provides a systematic view of where LLMs fail: syntax vs. materials reasoning.\n\n1. The authors provide a few insightful discussions: distinguish between spatial reasoning and following CIF syntax; identify that combining both of them significantly increases difficulty; and point out that tool-augmented and multimodal approaches are needed for progress."}, "weaknesses": {"value": "1. The actions are designed to resemble the real-world structural modifications; however, the modifications may not be consistent with material physics and chemistry. An arbitrary addition/removal/moving doesn't necessarily result in a scientifically meaningful structure. Therefore, the benchmark is more about evaluating the LLM's knowledge of CIF grammar than materials science.\n\n1. The structures in AtomBench are sampled from the Materials Project, which may limit diversity, especially for complex or defective systems.\n\n1. The paper identifies failures but lacks mechanistic explanations of why models fail, which may limit the insight of the work."}, "questions": {"value": "1. For the samples per action, are they drawn from a broad range of chemistries and symmetries (e.g., metals, oxides, perovskites), or mostly from a small subset of Materials Project data entries?\n\n1. Have the authors tested prompt robustness of LLMs? If models are highly prompt-sensitive, AtomWorld might measure instruction-following ability more than spatial reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iyZetxi8FG", "forum": "zvzrDQTk1n", "replyto": "zvzrDQTk1n", "signatures": ["ICLR.cc/2026/Conference/Submission22569/Reviewer_4UhP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22569/Reviewer_4UhP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790829156, "cdate": 1761790829156, "tmdate": 1762942283347, "mdate": 1762942283347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Atom World, a benchmark for evaluating the spatial reasoning capabilities of Large Language Models (LLMs) in the context of atomic structures. The benchmark tests various spatial transformations (move, swap, rotate) on structures represented in CIF (Crystallographic Information File) format. While the problem is relevant, the paper has methodological gaps and presents results that may contradict prior work in the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important question about LLMs' spatial reasoning capabilities in scientific domains\n- Several thoughtful design choices in benchmark construction"}, "weaknesses": {"value": "### 1. Missing Discussion of Prior Work \n\nThere has been prior work on the abilities to model geometric properties with LLM. One of them is [MatText](https://arxiv.org/abs/2406.17295), which was followed up by a work from Andrew Gordon Wilson’s group](https://arxiv.org/abs/2404.02444).\n\n\n### 2. Missing Experimental Details\n\nFor example, the \"move\" operation lacks essential specifications (perhaps I missed them):\n- What is the distance of movement?\n- How are movement distances sampled?\n- Does the distribution of sampled distances correlate with error magnitudes?\n- The maximum distances shown are relatively small—why?\n\n### 3. Insufficient Analysis of Structural Factors\n\nThe paper does not examine how performance relates to:\n- Number of atoms in the structure\n- Structural complexity\n- Symmetry properties\n- Other important structural characteristics\n\n### 4. Questionable Choice of Evaluation Metric\n\nThe current metric may not capture what matters most. For molecular/atomic structures, **preserving intramolecular distances** may be more important than successfully translating the entire structure. The benchmark should weight internal geometric consistency appropriately.\n\n### 5. Oversimplified Treatment of CIF Format\n\nThe CIF dictionary contains numerous fields and is extensible by design. The paper presents the problem as if there is one canonical way to represent structures, glossing over significant flexibility in the format. A cleaner experimental design would:\n- Compare atom-only CIF syntax vs. full CIF syntax\n- Control for the number of atom types\n- Control for which atom types are present\n- Systematically vary these factors\n\n(Not saying that this needs to be covered in experiments, but perhaps it should be reflected in writing). \n\n### 6. Anthropomorphization of LLMs\n\nThe authors anthropomorphize LLMs in the introduction and discussion, referring to \"thinking traces\" that LLMs obviously do not have.\n\n### 7. Puzzling and Inconsistent Results\n\nSeveral results are difficult to interpret:\n\n- **Swap performance is surprisingly poor** - this seems anomalous and lacks explanation (the paper only says “failed surprisingly”)\n- **Move results may be artifacts** of how the benchmark is constructed rather than genuine capability measurements (depends on the distance the atom is being moved)\n- What happens when the entire structure is moved uniformly?\n\nAs the authors themselves seem to acknowledge (“The reality is likely...”), it is hard to obtain deep insights from the current benchmark design --- what are the underlying limitations of the models that cause problems?"}, "questions": {"value": "1. How do you reconcile your conclusions with the findings of MatText and Gruver et al.?\n2. Can you provide complete specifications for all transformation operations, particularly movement distances and their sampling procedures?\n3. Can you provide an analysis broken down by:\n   - Number of atoms\n   - Structural complexity metrics\n   - Symmetry groups\n4. Have you considered metrics that prioritize preservation of intramolecular distances?\n6. Why is swap performance so poor, and is this a genuine model limitation or an artifact?\n7. What happens when you translate entire structures uniformly (rigid body translation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NkRJRQpzc9", "forum": "zvzrDQTk1n", "replyto": "zvzrDQTk1n", "signatures": ["ICLR.cc/2026/Conference/Submission22569/Reviewer_HUw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22569/Reviewer_HUw1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902969965, "cdate": 1761902969965, "tmdate": 1762942283043, "mdate": 1762942283043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AtomWorld, a benchmark designed to assess large language models (LLMs) on spatial reasoning tasks involving crystalline materials. The benchmark focuses on atomic-level manipulations defined in Crystallographic Information Files (CIFs), encompassing ten operations such as atom addition, rotation, substitution, and supercell construction. The authors also design complementary tasks (PointWorld, CIF-Gen, CIF-Repair, Chemical Competence Score, StructProp) to isolate reasoning versus syntax-following abilities. Experiments across GPT, Gemini, Llama, DeepSeek, and Qwen models demonstrate that current LLMs can handle simple operations but struggle with 3D geometric reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark pipeline is implemented correctly, and the evaluation metrics (success rate, mean max distance) are well-defined. Includes PointWorld, CIF-Repair, CIF-Gen, and StructProp, offering a multidimensional view of LLM capability. Applies to multiple frontier models (Gemini, GPT-o4, DeepSeek, Llama, Qwen). The framework could support reinforcement learning or tool-augmented evaluation in future."}, "weaknesses": {"value": "Only ~10K samples despite millions of public CIFs available (e.g., COD, Materials Cloud, OQMD, NOMAD). This limits generality and robustness. Claims of LLM “spatial reasoning failure” are not quantified beyond accuracy; no correlation analysis between task difficulty and model architecture/training. No uncertainty estimates, variance reporting, or ablation analysis. Related Work misplaced after Discussion; results buried within tables and dense text.\n\nWhile the paper is motivated by the goal of testing spatial understanding in scientific LLMs, the benchmark’s data scale and challenge level are limited — only ~10K samples drawn from a small subset of the Materials Project, despite the availability of millions of publicly accessible CIFs (e.g., NOMAD, OQMD, AFLOW, and COD).\n\nAs a result, the benchmark does not meaningfully probe the limits of current models. Most models perform moderately well on all but the hardest operations, suggesting the dataset may lack sufficient complexity or novelty to reveal qualitative differences in spatial reasoning. The organization of the paper also makes comprehension difficult: the Related Work section appears after Discussion and before Conclusion, breaking logical flow.\n\nBelow papers can be considered for comparison:\nhttps://arxiv.org/abs/2312.00111\nhttps://openreview.net/forum?id=Q2PNocDcp6\nhttps://arxiv.org/abs/2506.13051"}, "questions": {"value": "Why was the dataset limited to ~10K CIF actions when large-scale CIF repositories (e.g., NOMAD, OQMD, COD) contain millions of structures?\n\nHow are CIFs sampled—are they balanced across crystal systems and chemistries?\n\nWould scaling AtomWorld to include defects, polymorphs, or molecular crystals yield a more meaningful challenge?\n\nHave you compared the benchmark’s discriminative power (e.g., ability to separate model tiers) to datasets like ChemBench (arXiv:2506.13051)?\n\nWhy does the paper place Related Work after Discussion? Could you restructure to better contextualize contributions earlier?\n\nHave you explored tool-augmented baselines (e.g., with ASE or Pymatgen API access) to distinguish reasoning from symbolic manipulation errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tIM6LjdQUQ", "forum": "zvzrDQTk1n", "replyto": "zvzrDQTk1n", "signatures": ["ICLR.cc/2026/Conference/Submission22569/Reviewer_RAHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22569/Reviewer_RAHg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940699279, "cdate": 1761940699279, "tmdate": 1762942282854, "mdate": 1762942282854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed AtomWorld is a benchmark designed to evaluate spatial reasoning capabilities of LLMs in the domain of crystalline materials. The authors assume that LLMs possess the capability to understand and manipulate 3D atomic structures for crystal design. They designed a series of cognitive tasks in materials discovery and benchmarked several state-of-the-art LLMs, including Gemini 2.5 Pro, GPT-o3, GPT-o4-mini, Deepseek Chat, Llama-3 70B, and Qwen-3. All benchmark tasks uses CIF format as textual input of the LLMs along with a natural language action prompt, requiring certain actions to edit the CIF in order to output a valid modified CIF structure. The benchmark results suggest high success rates on simple operations like adding or changing single atoms, while failures on rotations and atom swapping. The work try to disentangle different failure modes, but the analysis suggests LLMs are tackling the tasks as pattern-matching rather than physics-informed reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work aims at building a comprehensive benchmark that assesses whether LLMs can handle mechanical operations and higher-level cognitive tasks. It is working at building the foundation for future material discovery with LLM capabilities. \n2. Multiple LLMs across different model families are benchmarked in this work. And it reported an interesting observation on how all LLMs consistently fail at complex spatial reasoning tasks."}, "weaknesses": {"value": "1. The benchmarking data are oversimplified. This work evaluates LLMs exclusively on simple prototype crystals with at most three elements, which raises questions about whether the findings generalize to multi-component systems or more complex lattice structures. \n2. In addition, the benchmark rely on extremely small sample sizes often tens or hundreds. E.g. CIF-Repair was tested on 22 samples, CIF-Gen on 20 samples, and only 10 samples ran DFT in StructProp. Therefore the conclusions drawn from these tests about model capabilities or reasoning quality are not reliable.\n3. The StructProp benchmark design is intuitive but fundamentally flawed, as it assumes LLMs understand the property concepts being tested without validating this assumption. Interpretation of the StructProp results can only be regarded as bias from memorized patterns."}, "questions": {"value": "1. All demonstrated examples use relatively simple crystals with at most three elements. Have you benchmarked LLMs on materials with higher compositional complexity or complex crystal systems, e.g. from the Materials Project database? \n2. The CIF format contains many redundant information and can produce lengthy token sequences. Have you evaluated the LLMs' performance on token length of the CIFs? How does the token counts distribute for each task? How does performance correlate with the length of the input CIF file? Does the context window limitations affect performance of the LLMs?\n3. Specifically when atom coordinates involved, have you tested whether generating a structure where the target atom appears early or late in the CIF sequence lead to different results? \n4. For StructProp, have you tested whether models can correctly identify which property is being referenced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3syyfPKI04", "forum": "zvzrDQTk1n", "replyto": "zvzrDQTk1n", "signatures": ["ICLR.cc/2026/Conference/Submission22569/Reviewer_iYfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22569/Reviewer_iYfW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129249664, "cdate": 1762129249664, "tmdate": 1762942282662, "mdate": 1762942282662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}