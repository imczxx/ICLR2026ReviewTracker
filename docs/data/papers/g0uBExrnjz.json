{"id": "g0uBExrnjz", "number": 16224, "cdate": 1758261960871, "mdate": 1763718322808, "content": {"title": "Excessive Reasoning Attack on Reasoning LLMs", "abstract": "Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling.\nHowever, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking).\nIn this work, we expose a novel threat: crafting adversarial inputs to exploit excessive reasoning behaviors.\nHowever, directly optimizing for excessive reasoning is non-trivial because reasoning length is non-differentiable. \nTo overcome this, we introduce a proxy framework that approximates the long reasoning objective and shapes token-level behavior:\n(1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs;\n(2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and\n(3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs.\nWe optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. \nEmpirical results demonstrate a 3x to 6.5x increase in reasoning length with comparable utility performance.\nFurthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, GPT-OSS, DeepSeek-R1, and QWQ models.\nOur findings highlight an emerging efficiency-oriented vulnerability in modern reasoning LLMs, posing new challenges for their reliable deployment.", "tldr": "", "keywords": ["Adversarial Examples", "Reasoning LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/621ca89145a74ce49354c339228915c850a79faa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates a new type of adversarial vulnerability in reasoning large language models (LLMs), referred to as Excessive Reasoning Attack. The authors argue that reasoning-oriented models, such as DeepSeek-R1 and Qwen variants, tend to perform unnecessary or redundant reasoning that can be exploited to increase inference-time computation. To formalize this, they propose a proxy optimization framework consisting of three components: (1) Priority Cross-Entropy Loss to emphasize informative tokens, (2) Excessive Reasoning Loss to encourage repeated or branched reasoning, and (3) Delayed Termination Loss to postpone output completion. Using the GCG optimization method, they generate short adversarial suffixes that substantially extend reasoning trajectories on GSM8K and ORCA benchmarks. Experiments show increased reasoning length, latency, and energy usage, with minimal change in accuracy, and demonstrate partial transferability across several commercial LLMs (o3-mini, GPT-OSS, DeepSeek-R1, QWQ)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces Excessive Reasoning Attack, a novel adversarial attack that differs from prior works focusing solely on content manipulation or refusal-based safety issues. It specifically targets the reasoning process of LLMs, exposing a new dimension of vulnerability related to inference efficiency. \n2. The paper proposes three complementary differentiable proxy losses—Priority Cross-Entropy (PCE), Excessive Reasoning (ER), and Delayed Termination (DT)—which effectively address the non-differentiability of reasoning length.\n3. The experiments yield several meaningful insights, such as the observation that optimized adversarial suffixes remain effective across models sharing the same tokenizer."}, "weaknesses": {"value": "1. White-box Assumption and Limited Practicality\n\nThis paper introduces Excessive Reasoning Attack, a novel adversarial attack targeting reasoning LLMs. I acknowledge that such an attack poses a more substantial threat to online LLM services (e.g., OpenAI, Google, and Alibaba Cloud) than to open-source models. However, the proposed method relies on a white-box assumption, requiring full access to model weights and gradients. This dependency makes it inapplicable to black-box commercial models. Although the results indicate certain transferability, the optimization mechanism still depends heavily on white-box access, which considerably limits the real-world practicality of this attack.\n\n2. Demand on Computational Resources\n\nThe experiments are conducted on 7–8B models (LLaMA and Qwen), which can be handled with a single A100 80GB GPU. However, since the method requires gradient access—a memory-intensive operation—it entails significant computational overhead, which would\nbecome more severe for larger LLMs. It is recommended that the computational cost and GPU memory consumption of the proposed framework be quantitatively analyzed to clarify its scalability and practicality.\n\n3. Limitations of Experimental Design\n\n- Stronger Baselines are Needed\n\nThe baselines in Table 1 are not well-aligned with the stated objective of this paper, inadvertently amplifying the effectiveness of the proposed method. Most compared methods are not designed to extend the reasoning length of LLMs. For instance, Engorgio Prompt primarily targets general DoS attacks, not reasoning-specific ones, while CatAttack introduces distractive sentences that mainly add semantic noise rather than lengthening reasoning. As shown in Table 1, these baselines barely increase reasoning length compared to the original setting. Therefore, stronger baselines are needed to empirically evaluate the performance of the proposed method.\n\n- Limitation of Transferability Experiment\n\nIt would be beneficial to test the transferability of the proposed adversarial attack on more advanced reasoning LLMs, such as GPT-5, Claude, and Gemini, rather than only on open-source or relatively weak models listed in Table 4. Such experiments would better demonstrate how different reasoning capabilities affect robustness against this attack."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QMhmwMsfpl", "forum": "g0uBExrnjz", "replyto": "g0uBExrnjz", "signatures": ["ICLR.cc/2026/Conference/Submission16224/Reviewer_Cuh6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16224/Reviewer_Cuh6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718948511, "cdate": 1761718948511, "tmdate": 1762926383838, "mdate": 1762926383838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a novel denial-of-service (DoS) style threat against reasoning-focused large language models (LLMs), which are prone to inefficient behaviors like redundant or overly long reasoning chains. The authors propose the \"Excessive Reasoning Attack,\" which crafts adversarial suffixes to compel the target model to generate excessively long reasoning trajectories, thus significantly increasing inference latency, energy consumption, and overall computational cost without degrading task accuracy. To achieve this, they introduce a composite loss function comprising three components: Priority Cross-Entropy (PCE) Loss, Excessive Reasoning (ER) Loss, and Delayed Termination (DT) Loss, optimized using a gradient-based search (GCG). The results show a substantial increase in reasoning length (3x to 6.5x) and resource usage, demonstrating transferability across various commercial and open-source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel and Practical Attack Objective:** The paper identifies and exploits a highly relevant vulnerability: the efficiency and resource consumption of reasoning LLMs. Unlike attacks targeting answer correctness, this focuses on *economic* and *operational* damage (computational overhead), which is a critical, underexplored threat in commercial LLM deployment (akin to a DoS attack).\n2.  **Strong Empirical Validation and Transferability:** The attack demonstrates high efficacy, successfully increasing reasoning length by several multiples across different model architectures (LLaMA and Qwen variants). Furthermore, the strong transferability of the adversarial suffixes to black-box commercial models like 03-mini and others highlights the generalized nature of this efficiency vulnerability in the reasoning mechanisms themselves."}, "weaknesses": {"value": "1.  **Ambiguity in Causality of Performance Gain:** The paper observes that the attack, while lengthening reasoning, sometimes increases task accuracy. The analysis attributes this to increased capacity allocation, but a more in-depth exploration of *why* the attack's specific, lexically biased reasoning leads to *better* answers is needed to fully understand the mechanism.\n2.  **Tokenizer Dependency in Transferability:** The transferability analysis, particularly the difference between the LLaMA and Qwen optimized suffixes on the target models, points to a strong dependence on tokenizer alignment. This suggests the attack's universality may be bounded by tokenization schemes, a limitation that should be discussed more explicitly in the conclusion."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zNqh6jBXu4", "forum": "g0uBExrnjz", "replyto": "g0uBExrnjz", "signatures": ["ICLR.cc/2026/Conference/Submission16224/Reviewer_zwXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16224/Reviewer_zwXP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811774022, "cdate": 1761811774022, "tmdate": 1762926383412, "mdate": 1762926383412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a white-box adversarial suffix attack that increases the reasoning token length (and thus latency/energy) of reasoning-oriented LLMs without substantially degrading task accuracy. Because reasoning length is non-differentiable, the authors optimize three differentiable proxy losses: (i) Priority Cross-Entropy (PCE) that reweights supervision toward prompt-dependent tokens, (ii) Excessive Reasoning (ER) that boosts the probability of sentence-initial deliberation tokens, and (iii) Delayed Termination (DT) that suppresses end and beginning of sentence tokens. They search adversarial suffixes with GCG. On GSM8K/ORCA, the attack increases reasoning tokens by 3 to 6.5X, with large latency and energy overhead; adversarial suffixes also transfer to several commercial/closed models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The decomposition into PCE, ER, DT is well-motivated by the goal (longer reasoning). Ablations isolate each piece (Table 5; Table 11), showing monotone increases when objectives are combined.\n\n2) On LLaMA-8B-R1-distill (GSM8K greedy), reasoning tokens rise from 668 to 1914 and latency 24.3s to 54.9s with only 10-token suffixes (Table 1). Similar or larger effects on Qwen-7B-R1-distill. The designed attack works experimentally.\n\n3)Cross-model tests (Table 4) show the attack is not brittle. This connects to prior literature on universal triggers and GCG-style suffixes shaping model behavior."}, "weaknesses": {"value": "1) Only 50 examples * 2 datasets * 3 runs = (approx.) 300 evaluations. No statistical test on accuracy differences. For GSM8K (app. 8.5 k train / 1 k test), using 50 samples risks > +- 3 points variance; hence “no degradation” claims are statistically inconclusive.\n\n\n2) The threat model assumes full gradient access (white-box), while commercial systems (o1/o3-mini) are black-box API only. Transferability results (Tab. 4) are modest (<600 tokens gain) and could arise from stochastic sampling noise.\n\n3) The paper assumes that maximizing the PCE/ER/DT composite correlates with reasoning-token count, yet this link is never formally validated. Equation (3) weights by prompt-sensitivity (Δ log p), but the proof that this expectation increases generation length is missing.\nA simple counterexample exists: tokens with high prompt-sensitivity can be function words (e.g., “Therefore”) that appear early without elongating reasoning.\nA more principled surrogate, e.g., REINFORCE or length-aware policy gradient as in Wu et al., 2025 (When More is Less), would better justify optimization correctness.\n\n\n4) (not a major weakness) I noticed that related work has been provided in Appendix, which is very non-standard. This is not grounds for rejection, but I think including some of the related work would be helpful.\n\n**References:**\n[1] Wu Y, Wang Y, Ye Z, Du T, Jegelka S, Wang Y. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266. 2025 Feb 11."}, "questions": {"value": "1) GSM8K and ORCA are math-only; do you expect similar behavior on symbolic-reasoning datasets (e.g., MATH Bench)?\n\n2) How do you verify that longer reasoning corresponds to distinct reasoning trajectories, not repetition? ANy qualitative study?\n\n3) What happens if $\\beta$ is reduced from 50 to 1? Does effect vanish?\n\n4) Please confirm whether the importance score (Eq2.) is correct. I might be wrong but current formula appears mathematically reversed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "56qRXvVtpH", "forum": "g0uBExrnjz", "replyto": "g0uBExrnjz", "signatures": ["ICLR.cc/2026/Conference/Submission16224/Reviewer_uiUg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16224/Reviewer_uiUg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076487802, "cdate": 1762076487802, "tmdate": 1762926382878, "mdate": 1762926382878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}