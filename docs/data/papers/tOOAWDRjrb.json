{"id": "tOOAWDRjrb", "number": 5020, "cdate": 1757835002613, "mdate": 1763711761064, "content": {"title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers", "abstract": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for visual generation. \nRecent observations reveal Massive Activations (MAs) in their internal feature maps, yet their function remains poorly understood. \nIn this work, we systematically investigate these activations to elucidate their role in visual generation. \nWe found that these massive activations occur across all spatial tokens, and their distribution is modulated by the input timestep embeddings. Importantly, our investigations further demonstrate that these massive activations play a key role in local detail synthesis, while having minimal impact on the overall semantic content of output.\nBuilding on these insights, we propose Detail Guidance (DG), a MAs-driven, training-free self-guidance strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG constructs a degraded detail-deficient model by disrupting MAs and leverages it to guide the original network toward higher-quality detail synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG), enabling joint enhancement of detail fidelity and prompt alignment.\nExtensive experiments demonstrate that our DG consistently improves fine-grained detail quality across various pre-trained DiTs (e.g., SD3, SD3.5, and Flux).", "tldr": "", "keywords": ["Massive Activations", "Diffusion Transformers", "Visual Detail Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4696622b90f02cf27314f3149250da0cc189308.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study the role of massive activation (MA) in DiT models. Massive activation is a common observation in transformer models. The authors attribute massive activations in DiT to the generation of local detail. Experiments show that disruption of MA degrades local information. The authors propose Detail Guidance (DG) to enhance CFG on local details. The manuscript is clearly written. The analysis of MA is comprehension. The experimental design is legit. Overall, it is a high-quality study. Though, I note several unclear parts below and suggest several controls to support the effectiveness of DG."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Massive activation is a common observation in transformer models across modalities. Its functional role is less known in DiT models. The analysis provided by the study is comprehensive (Figure 3-5). The finding of MA’s role of synthesizing is novel and supported. Leveraging this finding, Detail Guidance seems to be an effective, though simple, way to enhance local details."}, "weaknesses": {"value": "**DG control**\n\nAs the authors mentioned, Karras et al 2024 proposed using an under-trained version to contrast the condition path. DG works in a similar way. Method in Karras et al 2024 should be used as a control. Or, if MA-disruption is really only about disrupting local detail features, there should be another control that uses blurred conditional path as the unconditional signal.\n\n**MA-disruption control**\n\nIn “Non-MA disruption”, it is hard to say that zeroing non-MA dims is a fair control. The amount of perturbation in “Non-MA disruption” is smaller than in “MA disruption”. To be a fair control, the same total magnitude of disruption should be the same.\n\n**Missing information, or not emphasized in the main text**\n\nAre MA studied in the conditional path (D_/theta(z,t,c) in eq 1) or unconditional path (D_/theta(z,t) in eq 1)? From eq. 2, it seems the state is taken from the unconditional path, but in eq 4, the MA is studied/manipulated in the conditional path. The interpretation of the results heavily depends on where MA are found and manipulated. Authors should clarify where the hidden states are taken from in Figure 2-5.\n\nFigure 2 misses information on which layer and time step the activations are taken from."}, "questions": {"value": "**Spatial map of MA**\n\nIn the ViT studies, the MA often appears in only several background tokens in the image. If the role of MA in DiT is to synthesize details, then does it mean the MA is supposed to be in all the tokens, or only in tokens with plenty of details? If only disrupting several tokens’ MA, is the details only missing locally? The special activation map of MA dim is not shown. Showing an activation map of MA dim would be helpful to understand its actual role."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p9MaTe2fSq", "forum": "tOOAWDRjrb", "replyto": "tOOAWDRjrb", "signatures": ["ICLR.cc/2026/Conference/Submission5020/Reviewer_SiyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5020/Reviewer_SiyV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709442484, "cdate": 1761709442484, "tmdate": 1762917824769, "mdate": 1762917824769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the role of massive activation (MA) in DiT models. Massive activation is a common observation in transformer models. The authors attribute massive activations in DiT to the generation of local detail. Experiments show that disruption of MA degrades local information. The authors propose Detail Guidance (DG) to enhance CFG on local details. The manuscript is clearly written. The analysis of MA is comprehension. The experimental design is legit. Overall, it is a high-quality study. Though, I note several unclear parts below and suggest several controls to support the effectiveness of DG."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Massive activation is a common observation in transformer models across modalities. Its functional role is less known in DiT models. The analysis provided by the study is comprehensive (Figure 3-5). The finding of MA’s role of synthesizing is novel and supported. Leveraging this finding, Detail Guidance seems to be an effective, though simple, way to enhance local details."}, "weaknesses": {"value": "**DG control**\n\nAs the authors mentioned, Karras et al 2024 proposed using an under-trained version to contrast the condition path. DG works in a similar way. Method in Karras et al 2024 should be used as a control. Or, if MA-disruption is really only about disrupting local detail features, there should be another control that uses blurred conditional path as the unconditional signal.\n\n**MA-disruption control**\n\nIn “Non-MA disruption”, it is hard to say that zeroing non-MA dims is a fair control. The amount of perturbation in “Non-MA disruption” is smaller than in “MA disruption”. To be a fair control, the same total magnitude of disruption should be the same.\n\n**Missing information, or not emphasized in the main text**\n\nAre MA studied in the conditional path (D_/theta(z,t,c) in eq 1) or unconditional path (D_/theta(z,t) in eq 1)? From eq. 2, it seems the state is taken from the unconditional path, but in eq 4, the MA is studied/manipulated in the conditional path. The interpretation of the results heavily depends on where MA are found and manipulated. Authors should clarify where the hidden states are taken from in Figure 2-5.\n\nFigure 2 misses information on which layer and time step the activations are taken from."}, "questions": {"value": "**Spatial map of MA**\n\nIn the ViT studies, the MA often appears in only several background tokens in the image. If the role of MA in DiT is to synthesize details, then does it mean the MA is supposed to be in all the tokens, or only in tokens with plenty of details? If only disrupting several tokens’ MA, is the details only missing locally? The special activation map of MA dim is not shown. Showing an activation map of MA dim would be helpful to understand its actual role."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p9MaTe2fSq", "forum": "tOOAWDRjrb", "replyto": "tOOAWDRjrb", "signatures": ["ICLR.cc/2026/Conference/Submission5020/Reviewer_SiyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5020/Reviewer_SiyV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709442484, "cdate": 1761709442484, "tmdate": 1763746427435, "mdate": 1763746427435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically studies the massive activations in the DiT (Diffusion Transformer) image generation framework. The authors find that these activations are highly correlated with detail synthesis. Based on this observation, they propose a guidance (DG) strategy to steer the generation process toward better texture synthesis. Experiments demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation is well-organized and easy to follow.\n2. The study on massive activations is detailed and systematic.\n3. The proposed DG strategy is effective and novel."}, "weaknesses": {"value": "1. The paper lacks comparison with advanced CFG strategies, such as PAG [1], FA-CFG [2], and Semantic-CFG [3]. While such comparisons are not essential, including them would strengthen the experimental validation and enhance the robustness of the evaluation if avaialbe.\n\n2. In Figure 2, the authors claim that massive activations appear at fixed dimensions across all patch tokens. Is this dimension consistent across all layers?\n\n[1] Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance.\n[2] FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling.\n[3] Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "67s76rWYLR", "forum": "tOOAWDRjrb", "replyto": "tOOAWDRjrb", "signatures": ["ICLR.cc/2026/Conference/Submission5020/Reviewer_RLWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5020/Reviewer_RLWh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760763690, "cdate": 1761760763690, "tmdate": 1762917824397, "mdate": 1762917824397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically studies the role of Massive Activations in image generation and discovers that they play a crucial role in generating fine-grained local details. Building on this observation, the authors propose a simple, training-free method that constructs a “detail-deficient” model by deliberately disrupting these Massive Activations. This degraded model is then used as a negative reference to guide the original network (similar to CFG) to generating images with better detail fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated and well-structured. It first provide a clear analysis of Massive Activations in Diffusion Transformers, and then proposes a simple and effective method based on this observation to improve image generation quality.\n\n- The proposed approach is simple and training-free. It only involves disrupting the massive activations within a pretrained model to construct a degraded variant, which is then used as a negative reference through a CFG-style guidance to enhance fine-grained visual details.\n\n- Strong results. The proposed method shows strong performance gains and visual improvements"}, "weaknesses": {"value": "- DG relies on fixed-dimension activation patterns and the AdaLN scaling mechanism specific to DiTs. It is unclear whether the approach generalizes to non-transformer or hybrid architectures. Moreover, several recent works have suggested that AdaLN may not be the most optimal solution due to its parameter overhead and have proposed lighter alternatives. It would be valuable to discuss whether DG can be adapted to other schemes.\n\n- No ablation on computational overhead or inference latency. Since the proposed method requires constructing and utilizing a degraded model, it would be helpful to provide a comparison of GPU memory usage and inference time with and without DG to better understand its efficiency trade-offs.\n\n\n- It is good to also discuss the failure cases and limitations for the method. For example, while the finding that disrupting MAs mainly affects local details rather than semantics is compelling, such disruptions could also introduce unintended side effects. For exmaple I am gussing over-sharpening or loss of texture consistency. Additional experiments or qualitative examples discussing limitations would make the paper more comprehensive."}, "questions": {"value": "- Can it be adapted to non-DiT or hybrid architectures?\n- What is the computational overhead and inference latency of DG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zs5g2jamPm", "forum": "tOOAWDRjrb", "replyto": "tOOAWDRjrb", "signatures": ["ICLR.cc/2026/Conference/Submission5020/Reviewer_sJzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5020/Reviewer_sJzY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5020/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875056481, "cdate": 1761875056481, "tmdate": 1762917824210, "mdate": 1762917824210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}