{"id": "v0STwakyk5", "number": 1106, "cdate": 1756841213921, "mdate": 1763715663745, "content": {"title": "Non-Euclidian Gradient Descent Occurs at the Edge of Stability", "abstract": "The Edge of Stability (EoS) is a phenomenon where the sharpness (largest eigenvalue) of the Hessian converges to $2/\\eta$ during training with gradient descent (GD) with a step-size $\\eta$. Despite violating classical smoothness assumptions, EoS has been widely observed in deep learning, but its theoretical foundations remain incomplete. We propose a framework for analyzing EoS of non-Euclidean GD using directional smoothness (Mishkin et al., 2024), which naturally extends to non-Euclidean norms. This approach allows us to characterize EoS beyond the standard Euclidean setting, encompassing methods such as $\\ell_{\\infty}$-descent, Block CD, Spectral GD, and Muon without momentum. We derive the appropriate measure of the generalized sharpness under an arbitrary norm. Our generalized sharpness measure includes previously studied vanilla GD and preconditioned GD as special cases.  Through analytical results and experiments on neural networks, we show that non-Euclidean GD also exhibits progressive sharpening followed by oscillations around the threshold $2/\\eta$. Practically, our framework provides a single, geometry-aware spectral measure that works across optimizers, bridging the gap between empirical observations and deep learning theory.", "tldr": "We derive the sharpness measure for non-Euclidean gradient descent, including Block GD, Spectral GD, Sign GD.", "keywords": ["edge of stability", "optimization", "deep learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c900d84848ebea33dfb133b405f417ce7d94dbcc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper extends the *Edge of Stability* (EoS) phenomenon to non-Euclidean gradient descent. It defines *sharpness* under general norms via directional smoothness and shows that several non-Euclidean optimizers (ℓ∞, spectral, block ℓ1,2) also exhibit EoS behavior. A simple theoretical analysis on convex quadratics demonstrates convergence when sharpness < 2/η and divergence otherwise."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Defines a generalized notion of sharpness under arbitrary norms, conceptually clear and well-motivated.  \n2. Demonstrates EoS in various non-Euclidean settings, suggesting generality beyond Euclidean GD."}, "weaknesses": {"value": "1. Experiments too simple – limited to small models and datasets, lacking the breadth of prior EoS studies (e.g., Cohen et al.).  \n2. Theory too shallow – results proven only for convex quadratics; lacks generalization to non-convex deep learning settings.  \n3. Insufficient depth – does not explain the mechanism behind non-Euclidean EoS or analyze differences among norms."}, "questions": {"value": "Is it possible to prove the theory in a setting that is closer to real neural networks, rather than on simple convex functions?\n\nAlso see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OzxM2nA0W2", "forum": "v0STwakyk5", "replyto": "v0STwakyk5", "signatures": ["ICLR.cc/2026/Conference/Submission1106/Reviewer_ybaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1106/Reviewer_ybaT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628177030, "cdate": 1761628177030, "tmdate": 1762915679859, "mdate": 1762915679859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate whether a wider class of gradient based methods typically occurr at the EoS.\nThey show that they do."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work is very interesting. The message is clear, clean, and nice (maybe you could put (10) in page 1 so that one gets the message straight away). I really liked the paper.\nI also think (as a person working on the topic) this is something someone had to do! So good job!\n\nI think there are a few weaknesses which necessitate a little further work. If the authors will be able to pull this work, the paper will be great and I will gladly support the acceptance!"}, "weaknesses": {"value": "### **On the quantities being certificates and not causes**\n\nIn searching for a quantity that gets at the edge of stability, if we want to construct a theory in the fashion of self-stabilization of Damian et al. (2023) or Central Flow of Cohen et al. (2024), I believe we want a curvature quantity which does not depend on the weights oscillating, it's just a curvature feature of the points along the central flow. I don't know if I am explaining myself correctly. \nFor instance, for GD, $\\lambda_{\\max}$ is not a property of the gradients or the weights, it is a property of the Hessian in the point in the middle of the oscillations. Importantly your quantities depend on a direction too, this is a directional curvature as Lee and Jang (2023). If we believe the self-stabilization/central flow story the curvature quantity needs to be a value independent of the gradient/weights directions. This is not the case for these quantities, so I wonder if these are just *certificates* of EoS happening but are not *causal*.\n\n*Meaning these geometrical properties may be consequences of EoS, not EoS being a consequence of these geometrical properties.*\n\nCan you please comment on this? Happy to develop further. \n\nIn the case of Andreyev and Beneventano (2024) for SGD I am convinced that the *\"real, causal,\"* quantity, if any, could be an expectation of a Rayleigh quotient, because in the mini-batch case the gradients *will not* align along a precise direction. But for deterministic methods, I expect the gradients/weights to align with the eigenvector of a certain quantity, I want that quantity which is independent of alignment (as $\\lambda_{\\max}$), not a quantity subject to alignment as a Rayleigh quotient.\n\nDoes this make sense? Is there a reason why you think that my point is wrong? \n\nImagine I have a quadratic, one eigenvalue is bigger than $2/\\eta$ and the other is smaller. I expect Muon to adapt the gradients in a way *due to* that eigenvalue, in that case when I develop a self-stabilization theory for Muon I would use that eigenvalue, not the Rayleigh quotient. Right? Or do you think it is not self-stabilization the machinery that induces EoS?\n\nThanks a lot in advance for considering to clarify, I'm genuinely interested in what you think!\n\n### **The proofs in the appendix**\n\nI feel like those proofs in the appendix are unnecessary, they either exist already or are not needed in this article. Just cut them, it will be better and shorter.\n\n### **More experiments?**\n\nEven more in the light of the comment above on certificate vs cause, I believe I would like you to show more experiments both on quadratics, both on Cifar10 or other dataset. I believe you should go bigger both in the models and in the datasets to have this result properly established! Papers as Cohen et al. (2021), Andreyev and Beneventano (2024), Cohen et al (2024) have several dozen of pages of Appendix in which they show different things. I believe it would be beneficial to do the same. For instance, checking also how the trace goes up or other eigenvalues, etc, would help having a cleaner picture. My acceptance grade will depend on this too."}, "questions": {"value": "See weaknesses too :)\n\n1 ) The directional smoothness sounds a lot like the Rayleigh quotient of gradients and Hessian. The fact that that Rayleigh quotient stabilizes better than $\\lambda_{\\max}(H)$ for GD was already established by Lee and Jang (2023) and possibly by Cohen et al. (2021). Can you please shed light on their relation? I feel like this thing of the generalized shaprness seen as a Rayleigh quotient was known for GD, am I wrong?\n\n2) In case, even the quantity of Andreyev and Beneventano (2024) is a form of Rayleigh quotient, did you consider running experiments on the  expected mini-batch landscape as they did?\n\n3) Cohen et al. (2022) computes AEoS for Adam. Adam in practice is meant to work approximately similarly to a sign descent. You however, notice that sign descent work differently. Do you have any idea to bridge these two results? It would be interesting!\n\n4) Can you give me some better intuition of the sudden drops of which you speak in lines 318-320? \n\n5) I believe the preoscillatory regime is standard, it should be the case also of GD, am I wrong? For instance if $\\eta = 1.5/\\lambda_{\\max}$. Or at least, in parameter space there should be oscillation as you cross $1/\\lambda_{\\max}$, am I wrong? I am extremely curious of what your intuition is for this phase if you think it is not that!\nPS: At line 772 you start without capitalizing.\n\n6) It would be intersting to see if something like what happen in GD with the second eigenvalue happens. What is the second quantity that goes up? What the third? Is there a nice characterization of this? I believe there is, happy to throw my idea at you during rebuttal!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UfBMhLq4J7", "forum": "v0STwakyk5", "replyto": "v0STwakyk5", "signatures": ["ICLR.cc/2026/Conference/Submission1106/Reviewer_QE9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1106/Reviewer_QE9g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983800163, "cdate": 1761983800163, "tmdate": 1762915679394, "mdate": 1762915679394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the Edge of Stability (EoS) phenomenon, where sharpness hovers near $2/\\eta$, and generalizes it from standard Gradient Descent (GD) to a broad class of non-Euclidean steepest-descent methods defined with respect to different norms (e.g., $\\ell_\\infty$-descent/SignGD, Block Coordinate Descent, Spectral GD/Muon). Using the notion of directional smoothness, the authors show that if the loss decreases and gradient norms remain roughly stable, this quantity (and the corresponding generalized sharpness) increases toward $2/\\eta$ and then oscillates around it, extending the EoS picture beyond the $\\ell_2$ setting. The proposed generalized sharpness $S_{\\|\\cdot\\|}$ recovers the Euclidean and preconditioned cases as special instances. However, computing it exactly is generally intractable, and a Frank–Wolfe routine is used to estimate it. The authors further prove convergence for $\\eta<2/S_{\\|\\cdot\\|}$ and demonstrate divergence for $\\eta>2/S_{\\|\\cdot\\|}$ on simplified quadratic models. Empirically, across MLPs, CNNs, and Transformers under multiple geometries, $S_{\\|\\cdot\\|}$ indeed tracks the $2/\\eta$ threshold."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tUnderstanding the training dynamics of algorithms for deep learning, for example in EOS regime, is an interesting research question.\n-\tThe paper provides a unified analysis of multiple optimization algorithms under different norms and geometries for the Edge of Stability (EoS) phenomenon, which is interesting and appears to be novel.\n-\tThe paper is overall well-written and easy to follow."}, "weaknesses": {"value": "-\tThe theoretical results in Section 5 are derived only for a simplified quadratic model, and the results for general norms are weaker than those for the $\\ell_2$ norm.\n-\tEstimating the generalized sharpness is computationally expensive and requires approximation."}, "questions": {"value": "-\tThe discussion at the bottom of page 3 does not seem to fully explain the progressive sharpening observed in the EoS regime and is somewhat confusing. The argument presented appears to apply only to the case where the loss no longer decreases (e.g., near a minimum). However, progressive sharpening occurs well before convergence, as also shown in the paper’s plots. It would be helpful to provide a clearer explanation of this behavior.\n-\tIn Section 3, is the reported sharpness computed using the full Hessian or a layer-wise Hessian approximation?\n-\tSeveral plots (e.g., Transformer experiment in Figures 2 and 4, and CNN experiment in Figure 5) appear more unstable and deviate from the $2/\\eta$ threshold compared with other methods. Could the authors provide an intuitive explanation for this difference?\n-\tThe experimental setups seem inconsistent across methods: for example, some CNN experiments use CIFAR-10 while Transformer experiments use Tiny Shakespeare. What motivates these choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ROA4s9Cx4I", "forum": "v0STwakyk5", "replyto": "v0STwakyk5", "signatures": ["ICLR.cc/2026/Conference/Submission1106/Reviewer_XK99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1106/Reviewer_XK99"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986196685, "cdate": 1761986196685, "tmdate": 1762915679206, "mdate": 1762915679206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}