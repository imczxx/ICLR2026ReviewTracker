{"id": "z3bntsNwG0", "number": 19877, "cdate": 1758300172021, "mdate": 1759897014767, "content": {"title": "Description-Only Supervision: Contrastive Label–Embedding Alignment for Zero-Shot Text Classification", "abstract": "Zero-shot text classification (ZSC) aims to assign labels without task-specific annotation by exploiting the semantics of human-readable labels. In practice, embedding-based ZSC often falls back on training a linear probe, reintroducing annotation costs. We propose \\emph{description-only supervision}, a simple, compute-efficient alternative that requires only a handful of natural-language descriptions per label. We lightly finetune a base embedding model with a contrastive objective that pulls each label verbalizer toward its associated descriptions while pushing it away from others, using a multi-positive formulation to capture the many-to-one label–description relation. Across four benchmarks (topic, sentiment, intent, emotion) and ten encoders (22M–600M parameters), as few as five descriptions per label yield consistent gains, improving macro-F1 by +0.10 on average over zero-shot baselines. Compared to a few-shot SetFit baseline with 8 examples per class, our method attains higher mean performance with substantially lower variance across 20 runs, indicating improved stability in low-data regimes. The approach preserves the dual-encoder advantage (pre-encodable labels/documents), avoids labeled documents entirely, and adds minimal engineering overhead.", "tldr": "We show that lightly finetuning embedding models on a handful of natural-language label descriptions via contrastive alignment greatly improves zero-shot text classification.", "keywords": ["Zero-shot text classification", "description-only supervision", "contrastive learning", "embedding models", "label–description alignment", "multi-positive InfoNCE", "text embeddings", "annotation efficiency"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cbad95e096db5ea1e4d34fd2dbbbb63dce056c8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a contrastive learning framework for zero-shot text classification using only label verbalizers and a small set of natural language descriptions per label. The method applies a dual InfoNCE objective to align verbalizers and descriptions, claiming to improve performance without using any labeled documents. Experiments across multiple datasets and embedding models show consistent macro-F1 gains over zero-shot baselines, and improved stability compared to few-shot SetFit."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Conceptually simple and lightweight method, requiring no labeled training documents.\n\n* Empirical results are broad and convincing, covering 10 encoders and 4 datasets.\n\n* Demonstrates consistent performance improvements and low variance across random runs."}, "weaknesses": {"value": "* Novelty concern:\nThe idea of using natural language descriptions for label supervision in zero or few-shot classification has been explored in prior work. This paper applies a contrastive objective to align label verbalizers with their descriptions, but this formulation may not represent a substantial conceptual advance. The core component, which are label descriptions, contrastive learning, and dual encoders, are already widely used.\nFurthermore, it is not clear how description-only supervision fundamentally differs from conventional label supervision. Although the authors emphasize that no labeled documents are used, the descriptions are manually written for each label, and thus, still reflect explicit labeling information. In this sense, the line between using label supervision and using label descriptions remains blurred and requires clearer theoretical or empirical justification.\n\n* No ablation studies: The contribution of each component (e.g., dual InfoNCE, verbalizer vs. description) is not isolated. How much gain comes from descriptions alone versus contrastive finetuning?\n\n* Poorly annotated appendix:\nTables and figures in the appendix are presented without explanatory text, reducing clarity and reproducibility."}, "questions": {"value": "* Novelty and supervision scope:\nThe proposed method builds on well-known components (verbalizers, natural language label descriptions, and contrastive objectives) all of which have been explored in prior work on zero or few-shot classification. While the paper emphasizes that no labeled documents are used, the manually authored descriptions per label still constitute explicit supervision.\nWhat exactly distinguishes this setup from conventional label supervision?\nAnd what is the substantive novelty beyond recombining existing elements in a contrastive learning framework? A clearer conceptual delineation is needed to justify the claimed contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eHUKasRmRN", "forum": "z3bntsNwG0", "replyto": "z3bntsNwG0", "signatures": ["ICLR.cc/2026/Conference/Submission19877/Reviewer_gUCW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19877/Reviewer_gUCW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529196064, "cdate": 1761529196064, "tmdate": 1762932041485, "mdate": 1762932041485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes description-only supervision for zero-shot text classification by aligning label verbalizers with small sets of human-written label descriptions using a dual-direction contrastive loss (row-wise and column-wise InfoNCE). Experiments on four common classification datasets with ten embedding models show improvements over naive zero-shot baselines, while claiming lower supervision costs than few-shot alternatives."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Uses only label descriptions, preserving dual-encoder efficiency (cacheable embeddings; label scaling). Clear deployment upside vs. cross-encoders/ICL. Clean, principled objective: The row-wise InfoNCE + column-wise multi-positive formulation captures one-to-many label–description relationships and adaptively reweights positives. The derivation and gradients are explicit. \n\n2.  +0.10 macro-F1 on average across 10 encoders × 4 tasks; especially large relative lifts for small models. Comprehensive per-family analysis. \n\n3. The uniformity criterion is a neat, inexpensive heuristic to prevent collapse in small-data contrastive tuning. \n\n4. Early stopping, small description sets, and minimal engineering make replication/deployment feasible.\n\n5. The visualized results are visually appealing."}, "weaknesses": {"value": "1. Banking77 is restricted to six card-related intents, which may understate difficulty relative to the full 77-class benchmark; generality to large label spaces remains partially untested in this paper’s main results.\n\n2. The uniformity-based LR selection samples pairs from the test subset of the target domain, which can blur the line between tuning and evaluation (even though labels are not used). A cleaner protocol (dev split) would avoid potential leakage. \n\n3. Authors fix five generic descriptions per class and postpone quality optimization; robustness to noisy/misaligned descriptions is not systematically ablated. \n\n4. While SetFit is a fair few-shot comparison, contemporary description-driven ZSC methods (e.g., NLI-style label entailment or richer definition-based approaches) aren’t exhaustively compared under identical dual-encoder constraints. \n\n5. The paper states that Figure 1 demonstrates the core idea (Line 131), yet the figure mainly illustrates UMAP embeddings for AGNews rather than providing a conceptual or architectural depiction of the proposed framework.\n\n6. Poor writing, formatting, and referencing quality.   Inconsistent formula numbering: some equations are labeled, while others are not.   Reference formatting is not standardized, and several citations contain extra parentheses (e.g., Lines 45, 50, 64).  Overall layout lacks polish."}, "questions": {"value": "1. You compute uniformity on pairs sampled from the test subset (labels unused). Could you report results using a separate validation split for uniformity selection to rule out any subtle overfitting and quantify the gap (if any)? \n2. How does performance scale from 1→3→5→10 descriptions per label, and how sensitive is the method to noisy or partially off-topic descriptions? An ablation would help practitioners budget description effort. \n3. Have you tried full Banking77 (77 classes) or other datasets with dozens–hundreds of labels? How does the O(DL) batch construction behave in memory/latency, and does the column-wise term remain stable when K varies widely across labels? \n4. Many zero-shot methods compare documents to labels via entailment or rich label definitions. Could you include a dual-encoderized NLI/definition baseline (not cross-encoder) to isolate the value of the proposed contrastive training? \n5. You argue lower uniformity correlates with better F1. Can you provide per-dataset correlation plots across more models or show cases where the correlation breaks, to bound the reliability of this selection rule? (Some plots are in the appendix; more discussion would help.) \n6. Since descriptions are lightweight to write, have you evaluated cross-domain transfer (e.g., train verbalizer/description alignment on one domain and test on another) or multilingual zero-shot where label descriptions are translated?\n7. What clear conceptual advancement distinguishes this method from prior description- or entailment-based ZSC work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3HYIuaC82q", "forum": "z3bntsNwG0", "replyto": "z3bntsNwG0", "signatures": ["ICLR.cc/2026/Conference/Submission19877/Reviewer_87iw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19877/Reviewer_87iw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790825134, "cdate": 1761790825134, "tmdate": 1762932040877, "mdate": 1762932040877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of poor performance of embedding models in zero-shot text classification (ZSC). Existing ZSC methods often have limited performance or require reintroducing annotation costs (e.g., training a linear probe).\nTo solve this problem, the paper proposes a new method called \"Description-Only Supervision\".\nExperiments across 4 benchmark datasets and 10 different encoders show that this method brings an average improvement of +0.10 in Macro-F1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Originality:\n\n1.The core contribution of this paper is highly novel and concise: it proposes a method to align \"verbalizers\" using only \"descriptions\" as a supervision signal, without relying on any labeled documents.\n\n2.The \"multi-positive\" InfoNCE loss function, Lcols, is a clever design for handling the \"many-to-one\" label-to-description relationship.\n\n3.The \"label-free uniformity criterion\" (Luni) proposed in Section 3.1 is a very smart innovation. It addresses the tricky problem of selecting a learning rate (LR) in the ZSC setting (which lacks a validation set), cleverly avoiding data leakage.\n\nQuality:\n\n1.The experimental quality is very high. The paper conducts validation on 10 different embedding models (with parameters ranging from 22M to 600M) and across different types of tasks (topic, sentiment, intent, emotion). This extensive testing strongly demonstrates the method's generalizability.\n\n2.The qualitative analysis is excellent. The UMAP visualization in Figure 1 very clearly demonstrates the method's mechanism—pulling the \"verbalizers\" (stars) back to the center of their \"document cloud\" and \"description cloud,\" which greatly enhances the intuitive understanding of the paper.\n\n3.The Reproducibility statement is very thorough, promising to release all code, data, and models.\n\nClarity:\n\n1.The paper's writing is (mostly) clear. The elaboration of the methodology in Section 3 is well-executed; the mathematical formulas and \"geometric intuition\" complement each other, making it easy for readers to grasp the core idea.\n\nSignificance:\n\n1. This paper holds extremely high practical value. It provides a method that is computationally efficient (retaining the dual-encoder advantage) and has a very low annotation cost (only requiring a few descriptions to be written), yet significantly improves ZSC performance.\n\n2. The stability comparison in Figure 2(a) is one of the most important findings of this paper. It shows that compared to relying on specific few-shot samples (SetFit) 28, this paper's method (Ours) is far more robust (exhibiting minimal variance). This is crucial for deploying reliable models in the real world."}, "weaknesses": {"value": "There is a major contradiction regarding the experimental method for LR selection: This is the biggest weakness of this paper. The authors claim in Section 3.1 that they use the \"uniformity loss\" (Luni) to select the LR, because \"lower (uniformity) values correlate with stronger downstream performance\".\nHowever, the paper's own data (Appendix D, Figure 3) largely contradicts this core claim.\nFor example, on the gte-modernbert-base model, the correlation between Luni and Macro-F1 is not significant on all 4 datasets (p-values of 0.722, 0.672, 0.808, 0.0743, respectively). Qwen3-Embedding-0.6B also shows extremely weak correlation (p-values of 0.890, 0.767).\nThis creates a key contradiction: If the criterion used to select the LR is ineffective on many SOTA models, how were the excellent results for these models in Table 1 achieved? This severely calls into question the rigor of the experiments.\n\nLack of key ablation study: The paper proposes a framework composed of multiple novel components (\"verbalizer + description\", \"row-wise + column-wise\" loss), but provides no ablation studies to demonstrate the necessity of these design choices. We cannot know if Lrows and Lcols are both indispensable.\nWe also cannot know if using the \"Verbalizer\" as the inference anchor is truly superior to other (potentially simpler) alternatives.\nInsufficient sensitivity analysis on \"description quality\": The paper states they wrote 5 descriptions for each class and \"did not tune them\".\n\nWhile this simplifies the experiment, it also evades an important question: To what extent does the method's performance depend on the quality, quantity, and diversity of these descriptions? How would performance change if the descriptions were poorly written, or if only 1-2 descriptions were provided? Although the authors mention this in \"future work\", it is a clear limitation of the current study.\n\nTypesetting Issues: The submitted PDF manuscript has severe typesetting problems. Many pages have large vertical blank spaces, which seriously affect the reading experience and does not meet the conference's formatting standards."}, "questions": {"value": "Here are the key questions I hope the authors will clarify during the Rebuttal phase:\n\n(Most important question) \n\nRegarding the contradiction in the LR selection criterion: \n\nMy biggest concern is the apparent contradiction between Section 3.1 and Appendix Figure 3. You claim to use the \"uniformity loss\" (Luni) to select the LR, and claim a correlation exists between the two.\nHowever, the data in Figure 3 shows that for many of the stronger models (such as gte-modernbert-base and Qwen3-Embedding-0.6B), this correlation is not statistically significant (p-values are very high).\n\nPlease clarify:\n\nFor these models where the correlation was not significant, how exactly did you select the final LR for Table 1? Did you still use this (ineffective) criterion, or did you pick the best-performing LR on the test set (which is not allowed in a ZSC setting)? This must be clarified.\n\nRegarding the ablation of the loss function: \n\nYour symmetric loss L = 1/2Lrows + 1/2Lcols is core to the method. Can you provide an ablation study showing the performance when using only Lrows and only Lcols, respectively? This is crucial for understanding the individual contributions of these two components.\nRegarding the ablation of the inference anchor:\nYou use the \"label verbalizer\" vy as the anchor during inferenc. What would the performance be if you instead used the mean embedding vector of the set of \"label descriptions\" Dy as the inference anchor? Providing this comparison would help justify the necessity of vy as an \"intermediate anchor\".\n\nRegarding the ablation of \"Verbalizer\" vs. \"Label Word\":\n \nWhy did you choose to use a full \"label verbalizer\" (vy, e.g., \"This...is about sports.\") as the alignment target, instead of directly using a simpler \"label word\" (e.g., \"Sports\") to align with the \"descriptions\" Dy? Can you provide an experiment comparing the effectiveness of these two anchor choices?\n\nRegarding the typesetting issues:\n\nThe submitted PDF manuscript contains a large amount (on almost every page) of vertical whitespace. Will this be corrected in the final version?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uS1rBmTuet", "forum": "z3bntsNwG0", "replyto": "z3bntsNwG0", "signatures": ["ICLR.cc/2026/Conference/Submission19877/Reviewer_hnXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19877/Reviewer_hnXM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919233998, "cdate": 1761919233998, "tmdate": 1762932040396, "mdate": 1762932040396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}