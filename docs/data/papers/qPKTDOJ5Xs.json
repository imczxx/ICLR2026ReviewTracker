{"id": "qPKTDOJ5Xs", "number": 20231, "cdate": 1758303935929, "mdate": 1759896989491, "content": {"title": "Closed-form $\\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\\ell_p$ bias", "abstract": "For overparameterized linear regression with isotropic Gaussian design and minimum-$\\ell_p$ interpolator $p\\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\\\{ \\lVert \\widehat{w_p}  \\rVert_r \\\\}_{r \\in [1,p]} $ with sample size.\n\nWe solve this basic,  but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal \n*spike* and a *bulk* of null coordinates in $X^\\top Y$,  yielding closed-form predictions for (i) a data-dependent transition \n$n_\\star$ (the \"elbow\"), and (ii) a universal threshold $r_\\star=2(p-1)$ that separates $\\lVert \\widehat{w_p}  \\rVert_r$'s which plateau from those that continue to grow with an explicit exponent. \n\nThis unified solution resolves the scaling of *all* $\\ell_r$ norms within the family $r\\in [1,p]$ under $\\ell_p$-biased interpolation, \nand explains in one picture which norms saturate and which increase as $n$ grows. \n\nWe then study diagonal linear networks (DLNs) trained by gradient descent. \nBy calibrating the initialization scale $\\alpha$ to an effective $p_{\\mathrm{eff}}(\\alpha)$ via the DLN separable potential, \nwe show empirically that DLNs inherit the same elbow/threshold laws, \nproviding a predictive bridge between explicit and implicit bias. \n\nGiven that many generalization proxies depend on $\\lVert \\widehat {w_p} \\rVert_r$, \nour results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.", "tldr": "Closed-form scaling laws characterize how every $\\ell_r$ norm ($r\\in[1,p]$) of minimum-$\\ell_p$ interpolators scales with sample size, in linear regression and diagonal linear networks.", "keywords": ["Norm", "scaling law", "deep linear nerual network", "linear regression"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a3e03b1aab657a1bf5a8d03eb9ba6858b5b1ad6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to characterize one interesting term in overparameterized linear (models) regression: the behavior of $p$-norm minimum interpolator $||\\hat{w}_{p}||_r$ against the number of data for different $r$ given $p\\in(1, 2]$. To achieve this, the authors propose a novel \"dual-ray\" analysis to derive the corresponding scaling laws. Then they identify a data-dependent transition point and a universal threshold. They conduct some numerical experiments to verify their theoretical claims."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I think the most significant strength of this paper is the new \"dual-ray\" proof technique. I find it direct and intuitive, providing a better approach for the problem studied in this paper than approaches like GMT that might be too complex to be applied.\n\nFurthermore, the derived $r$-norm of $p$-norm solution $ || \\hat{w}_p ||_r $ as a function of several meaningful constants is also interesting. This form can somewhat support the authors' motivation \"using norm-based bounds or proxies should be cautious\"."}, "weaknesses": {"value": "However, I still have several concerns, and I will list them below.\n\n1. I find the primary objective of this paper a bit unconvincing: studying the $r$-norm of the $p$-norm interpolator solution. There lacks a compelling motivation for this focus. Because it seems that it is the $p$-norm of the $p$-norm interpolator solution that matters, beyond the cautioning that generalization bounds may depend on the choice of $r$. This makes the scope of this paper a bit narrow. \n\n2. The theoretical analysis heavily depends on the isotropic Gaussian features and $p \\in (1, 2]$. This does not explore the validity of the findings that violates this assumption, e.g., perhaps the most direct case $p = 1$.\n\n3. Theorem 3.1 is based on a general $w_{\\star}$ (neither single spike nor flat), but the corresponding scaling laws are only studied in two very extreme case (single spike and flat) to discuss the corresponding laws. I understand that this might be due to the clarity reason, but a thorough discussion for how the scaling laws behave for arbitrary $w_{\\star}$ should also be crucial. This also leads to the next weakness.\n\n4. The actionable guidance (line 463 - 467), together with the implications for practice (line 81-85), is a significant overclaim. The theoretical results are built upon linear regression, and there even lacks discussion (both theoretical and empirical) for general $w_{\\star}$ in this task, then I think claiming the corresponding actionable guidance for general settings is very misleading. \n\nMinor: I think this paper should be reviewed in a venue for statistical learning theory or statistics, rather than ICLR (the connection with diagonal linear networks seems unnecessary because the authors directly convert the diagonal linear network to a minimizer of certain norm, which almost has nothing to do with the model itself)."}, "questions": {"value": "1. Why the authors do not include $p = 1$ (and other values of $p$) in the analysis?\n\n2. How can the authors make their actionable guidance more reliable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gUa4U9LTwN", "forum": "qPKTDOJ5Xs", "replyto": "qPKTDOJ5Xs", "signatures": ["ICLR.cc/2026/Conference/Submission20231/Reviewer_ksga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20231/Reviewer_ksga"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758675791, "cdate": 1761758675791, "tmdate": 1762933726956, "mdate": 1762933726956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proves a closed form for the $\\ell_r$ norm of the min $\\ell_p$-norm interpolator under certain standard assumptions. Through experiments, the paper demonstrates that the trends predicted by the theory for linear regression, carry over to a slightly broader class of predictors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Very well-motivated, and clearly written.\n- The mathematical derivations are sound and the application of the proof technique is, to my knowledge, novel in the context of analyzing min-norm interpolators."}, "weaknesses": {"value": "- Theorem 3.1 is quite dense and could perhaps be split into the closed form result in eq 2, and the result characterizing the different sample size regimes\n- It is not particularly easy to grasp what the main terms that determine $||\\hat{w}_p||_r$ are. Would it be possible to give an interpretation of the quantities in equation 2?\n- To further demonstrate the significance of the result, it would be nice to have a concrete example (perhaps supported by experiments) where one can take advantage of the relationship between $r$, $p$, $n$ and $d$ proved in Theorem 3.1. \n\n**Minor remarks**\n- It would help carry the message across more easily if one could visualize the motivation in lines 55-66 with the help of a teaser plot (perhaps a condensed version of figures 1-2?).\n- Some of the markers are not visible in the figures e.g. dotted line in fig 1a."}, "questions": {"value": "- Is there an explanation for the non-monotonical trend of the $\\ell_{1.1}$ norm in figure 1b?\n- What is $d$ for figures 1-2? does varying $\\kappa=d/n$ change the trends in the figures? also would it be possible to indicate for each figure the predicted transition scale $\\n^\\star$?\n- In [1] the authors try to extend the observations from Donhauser et al, 2022 to inductive biases present in non-linear models. Would a similar extension be applicable to the results of this paper?\n\n[1] - https://arxiv.org/pdf/2301.07605"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S2PrzVGkIl", "forum": "qPKTDOJ5Xs", "replyto": "qPKTDOJ5Xs", "signatures": ["ICLR.cc/2026/Conference/Submission20231/Reviewer_4hrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20231/Reviewer_4hrx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946836859, "cdate": 1761946836859, "tmdate": 1762933725946, "mdate": 1762933725946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the ell_r norm of the minimum ell_p norm interpolator for regression over gaussian data in the overparametrized setting where number of data n >> number of dimensions d. the setting assumes a sparse ground truth w^star.\n\nIn particular, they focus on how the ell_r norm grow when n grows when d/n has a limit kappa > 1.\n\n\nThe main finding (theorem 3.1) is that for r <= 2 ( p - 1 ), the ell_r norm of the min ell_p norm interpolator grows, while r > 2 (p -1), the same quantity plateaus.\n\nAnother finding is a \"phase transition\" from spiky-to-bulk dependency in terms of the number of samples (denoted n_star)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The theory is well-supported as seen in Figure 1. The provable behavior from the linear model seems to also carry over empirically to diagonal linear networks."}, "weaknesses": {"value": "Can the authors give a more explicit reference to previous work that uses ell_r for bounding generalization error where r is not in {1,2}? I'm not convinced whether ell_r for r not in {1,2}. A more specific reference to a paper + lemma number would be helpful.\n\nThe legend of figure 1 looks to suggest that there should be error bars/confidence bands, but it's not visible in the plot. Is the result replicated?"}, "questions": {"value": "While i understand the spiky-vs-bulk part of X^TY, it's not clear to me how why the RHS of eqn (4) is said to be \"spike-dominated\". By contrast, in eqn (5), kappa_bulk clearly shows up on the RHS, so that makes sense.\n\nLine 303 says middle panel exhibits a clear elbow near the predicted n_star. but it's not clear to me where is n_star on the middle panel plot.\n\nThe definition of t_star in eqn (1) is confusing. since it is a \"definition\", why is there a \"w.h.p\" at the end?\n\nMinor:\n\nLine 229 equation (8) links to the appendix when it could link to eqn (3) instead. Same issue in the caption of figure 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fj4064vnjl", "forum": "qPKTDOJ5Xs", "replyto": "qPKTDOJ5Xs", "signatures": ["ICLR.cc/2026/Conference/Submission20231/Reviewer_MDQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20231/Reviewer_MDQs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027963418, "cdate": 1762027963418, "tmdate": 1762933725437, "mdate": 1762933725437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified theoretical and empirical study of how the parameter norms of overparameterized linear regression models scale with data under an ℓp-biased interpolation. Specifically, for minimum-ℓp interpolators with $p \\in (1,2]$, the authors derive closed-form, high-probability laws describing how the family of norms $\\{\\lVert \\widehat{w}_p \\rVert_r\\}$ for $r\\in[1,p]$ changes with sample size $n$. A central insight is the identification of a data-dependent transition point n^\\star (the “elbow”) separating bulk- and spike-dominated regimes, and a universal threshold $r^* = 2(p-1)$ that determines which norms plateau and which continue to grow. Using “dual-ray” analysis, the paper provides explicit expressions for these quantities and verifies the predictions empirically across both explicit $\\ell_p$ regression and diagonal linear networks (DLNs) trained by gradient descent. By calibrating the DLN initialization scale $\\alpha$ to an effective $p{\\text{eff}}(\\alpha)$, the authors demonstrate that DLNs inherit the same scaling laws, revealing a consistent connection between explicit and implicit bias. Overall, the work offers a clear, closed-form picture of how different norm measures behave with data growth and highlights that the predictive power of norm-based generalization proxies crucially depends on which ℓr norm is chosen."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "See summary. The paper has a very clear and concrete contribution. The paper’s main strength lies in its clear and unified theoretical characterization of how all $\\ell_r$ norms scale under $\\ell_p$-biased interpolation. It successfully bridges explicit and implicit bias through an effective p_{\\text{eff}}(\\alpha) mapping in diagonal linear networks, and its empirical results convincingly validate the theoretical predictions across multiple regimes, offering practically valuable insights into the sensitivity of norm-based generalization measures."}, "weaknesses": {"value": "I do not seen any major weaknesses of the study. The paper addresses a clear question. One minor weakness is that the connection to generalization is still less clear to me. It would be good to expand on this, and point out other work that also share the same weaknesses."}, "questions": {"value": "Can authors comment on the Gaussian design choice? Do they expect Gaussian universality, or can the results significantly change under other distributions, and how under other natural distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o5UvG2Txdt", "forum": "qPKTDOJ5Xs", "replyto": "qPKTDOJ5Xs", "signatures": ["ICLR.cc/2026/Conference/Submission20231/Reviewer_cAzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20231/Reviewer_cAzm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244300828, "cdate": 1762244300828, "tmdate": 1762933724411, "mdate": 1762933724411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}