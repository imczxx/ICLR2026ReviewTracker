{"id": "zuPxAZgT9F", "number": 15264, "cdate": 1758249500277, "mdate": 1763714047371, "content": {"title": "STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning", "abstract": "In vision–language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial–temporal video grounding (STVG). Prior approaches typically focus on enhancing visual–textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation task, achieving a SOTA 47.3% J&F on MeViS.", "tldr": "", "keywords": ["spatial-temporal video grounding", "vision language model", "reinforcement learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9e2fed369110f0cf2fad09005cb82bfd5843260.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new visual prompt paradigm without alignment coordinates, which transforms frame by frame coordinate prediction into instance level ID recognition. For the first time, reinforcement learning has been introduced into spatial-temporal video grounding, and task driven reward joint optimization of temporal, spatial, and format constraints has been designed. In multiple benchmark tests, refreshing SOTA resulted in a 20.9% improvement in mIoU for HCSTVG-v2."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing of the paper is clear, and the illustrations in the intro section effectively explain the core contribution points of this paper. In addition, the drawing of Figures 2 and 3 is also quite intuitive.\n\n2. The problem studied in this article (cross modal alignment) is one of the core issues in the field of video grounding, and the alignment effect between the spatial-temporal dimensions directly determines the accuracy of grounding in these two dimensions.\n\n3. The experiment used four commonly used indicators and demonstrated the accuracy of the model on the dataset used."}, "weaknesses": {"value": "1. The last sentence of the second paragraph of the intro should be a description of the core idea, but this sentence is not clear. What does' a compact and interpretable formulation 'refer to.\n\n2. Insufficient contribution in model design, only introducing pre-segmentation and reinforcement learning for video objects, lacking new method design for this problem.\n\n3. The commonly used datasets in the field of Video Grounding include VidSTG and HC-STVG. This article only uses HC-STVG, and the effect on VidSTG is unknown, especially the results of ablation experiments."}, "questions": {"value": "See [Weaknesses]."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wur9l1JhTX", "forum": "zuPxAZgT9F", "replyto": "zuPxAZgT9F", "signatures": ["ICLR.cc/2026/Conference/Submission15264/Reviewer_enhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15264/Reviewer_enhd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761290194583, "cdate": 1761290194583, "tmdate": 1762925564034, "mdate": 1762925564034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Brief Summary: The paper tackles the task of video spatio-temporal localization where given a video and a corresponding query, the model should output the bounding boxes + temporal timestamps. The authors propose visual prompting using a combination of yolov12 detector (for object detection) + sam2 (for visual tracking) and overlaying the information on the image itself and providing particular object ids which are also overlayed. Given the visual-prompted video, a Qwen2.5-VL model is trained via GRPO to predict the target id (which has associated bounding box) + time-range. Experiments on spatio-temporal datasets like HCSTVGv1, v2, and temporal grounding like charades-sta show that proposed STVG-R1 outperforms competitive baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Pros:\n\n1. The paper poses a nice application of combining spatio-temporal understanding with VLMs. Spatio-temporal understanding is an important sub-topic in video understanding and how to best leverage VLMs for this task is a well motivated problem. \n\n2. The proposed method is conceptually simple in re-using existing detection and tracking pipelines to utilize VLMs inherent understanding of vision without additional tokens or requiring VLM to do additional bounding box predictions. \n\n3. Authors provide visualization (in appendix A.2) and ablation on various prompt designs. In particular, Table 8 is very interesting, that pure SFT without GRPO training leads to worse results."}, "weaknesses": {"value": "Cons:\n\n1. It seems the absolute improvement over previous baselines is marginal? For instance on hcstvg-v1, performance matches with space-vllm and on v2, it is slightly improved over TA-STVG. On ST-Align, it is same as LLava-ST-7B.\n\n2. The core novelty is slightly limited, the paper suggests doing visual-prompting + grpo training works. This is good to know, but unclear what are the main challenges here. \n\n3. One issue with the visual prompting (assuming the visualization at face value), it is unclear how the approach would tackle things with text (OCR). If you overlay a color bounding box, the text is completely lost. So the model cannot answer questions like \"when did the person look at <some_text>\". \n\n4. The model is essentially restricted to detection quality and classes of yolov12 and the quality of sam2. As such, there is no direct way to leverage VLMs internal association capability. Further dependence on separate models would lead to worse inference times requiring heavy video encoding/decoding. \n\n5. Table 9 in Appendix A.3 seems to suggest direct visual prompting is in fact worse? That seems like a major drawback? \n\n6. (Minor) It would be interesting to see results on more diverse videos, such as some ego-centric datasets (such as ego4d) or movie datasets (such as grounded-vidsitu [Ref1]).\n\n7. The main comparison to baselines is somewhat unfair. The proposed model is able to leverage external models for tracking while baseline models need to do the prediction on their own? I could be missing something obvious here. \n\n8. (Minor) I am slightly confused on the r_s reward, why is it not simply 3d-iou? \n\n9. Authors should show additional downstream tasks which gain from such visual prompting.\n\n---\n\n[Ref1]: Khan, Zeeshan, C. V. Jawahar, and Makarand Tapaswi. \"Grounded video situation recognition.\" Advances in Neural Information Processing Systems 35 (2022): 8199-8210.\n\n---\n\nOverall Rating: 4/10\nThe paper proposes grpo RL training with appropriate visual prompting for spatio-temporal video grounding. The scope however is somewhat narrow and not shown to be applicable for other video understanding tasks, and seems to slightly degrade temporal grounding. The proposed visual prompting itself might interfere with ocr reasoning, and comparison to baselines is not strictly fair."}, "questions": {"value": "Q1. In general, it is advisable to do an initial cold-start before GRPO, but I don't see any references on that in the paper. Did the authors try it and didn't give good results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W4hV7npGJT", "forum": "zuPxAZgT9F", "replyto": "zuPxAZgT9F", "signatures": ["ICLR.cc/2026/Conference/Submission15264/Reviewer_bVvE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15264/Reviewer_bVvE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969057395, "cdate": 1761969057395, "tmdate": 1762925563621, "mdate": 1762925563621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the misalignment between textual descriptions and visual coordinates , which often induces hallucinations in Vision-Language Models (VLMs) for the Spatial-Temporal Video Grounding (STVG) task. The authors propose an \"object-centric visual prompting paradigm\" : via a pre-processing pipeline of detection , tracking , and ReID , each object is assigned a unique, temporally consistent ID. This reformulates per-frame coordinate prediction into a compact instance-level identification problem. Building on this, the paper introduces STVG-R1 , the first reinforcement learning framework for STVG , which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization . Experiments show the approach achieves SOTA and exhibits strong zero-shot generalization on the unseen multi-object referring video object segmentation task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tNovelty: Reformulating STVG from dense per-frame coordinate prediction into a \"compact instance-level identification task\", a novel idea that effectively avoids the difficult problem of VLMs handling coordinate prediction.\n\n•\tNovel RL Framework: Proposing STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to optimize the VLM's reasoning.\n\n•\tState-of-the-art Results: Achieves new SOTA performance on multiple STVG benchmarks.\n\n•\tStrong Generalization: Exhibits SOTA zero-shot performance on the unseen multi-object referring video object segmentation task (MeViS), highlighting the method's robust generalization ability."}, "weaknesses": {"value": "**Regarding the nature and robustness of the \"visual prompting\" pipeline:**\n\n•\tThe pipeline is essentially a complex, training-free data pre-processing pipeline reliant on external SOTA models such as YOLO, SAM2, and ReID, rather than a novel model component.\n\n•\tThe robustness of this pipeline is not discussed. For example, what happens when detection, tracking, or ReID fail? Many critical details of the pipeline, such as the arbitration logic between components, are missing, which hinders reproducibility.\n\n•\tThe visual prompts themselves introduce 'visual pollution' and occlude critical information. Embedding ID characters into video frames is a lossy operation that can obscure key details of an object (e.g., facial expressions, specific markings), thereby hindering the model's understanding. Experiments also show the method is sensitive to hyperparameters such as font size, which further confirms the risk of introducing visual interference.\n\n•\tThe paper does not quantify the additional computational and storage overhead introduced by this complex pipeline. How much (per-video) computational and memory overhead does running YOLO/SAM2/ReID add before using the VLM? This is crucial for assessing the method's practical usability.\n\n**Lack of ablation studies:**\n\n•\tThe paper provides no ablation study for the three components （r_t, r_s r_f） of the RL reward function R(o), making it impossible to determine the key factors driving the performance improvement.\n\n•\tLack of ablation on the necessity of each component in the pre-processing pipeline.\n\n**Contribution Positioning:**\n\n•\tThe paper's RL algorithm (GRPO) is heavily borrowed from DeepSeek-R1, which is more of an \"application-level innovation\" rather than an \"algorithmic innovation\", and this should be clearly stated in the manuscript.\n\n**Clarity and Justification of Key Mathematical Formulations:**\n\n•\tRegarding the spatial reward function r_s(o) in Equation (5): This reward function is designed as a sparse, binary (0 or 1) signal, which prevents the model from receiving \"partially correct\" feedback (e.g., for predicting a spatially adjacent but incorrect ID). The authors should justify why this sparse reward was chosen over a smoother, continuous reward that could reflect the spatial proximity of the predicted ID to the ground truth, and discuss the considerations for RL training stability and efficiency.\n\n•\tRegarding the majority voting rule in Equation (3): This formula determines the global target ID A via majority voting, which is a strong heuristic assumption. This assumption may fail for queries describing transient events (e.g., \"a person who flashes by\"), potentially leading to incorrect training labels. The paper should discuss the limitations of this design and its potential impact on performance.\n\n•\tRegarding the total reward function R(o) in Equation (6): The paper combines temporal and spatial rewards via simple addition, mathematically treating them as independent optimization objectives. However, the evaluation metric for spatio-temporal grounding (vIoU) is inherently coupled. The authors need to explain why a decoupled reward was chosen for training and whether there is a potential misalignment between this design and the final evaluation goal."}, "questions": {"value": "See the questions above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hnEFGsnmrN", "forum": "zuPxAZgT9F", "replyto": "zuPxAZgT9F", "signatures": ["ICLR.cc/2026/Conference/Submission15264/Reviewer_cMwu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15264/Reviewer_cMwu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000659712, "cdate": 1762000659712, "tmdate": 1762925563058, "mdate": 1762925563058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}