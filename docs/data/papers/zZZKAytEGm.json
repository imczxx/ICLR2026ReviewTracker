{"id": "zZZKAytEGm", "number": 21570, "cdate": 1758319135664, "mdate": 1759896914670, "content": {"title": "Why Some Models Resist Unlearning: A Linear Stability Perspective", "abstract": "Machine unlearning—the ability to erase the effect of specific training samples without retraining from scratch—is critical for privacy, regulation, and efficiency. However, most progress in unlearning has been empirical, with little theoretical understanding of when and why unlearning works. We tackle this gap by framing unlearning through the lens of asymptotic linear stability to capture the interaction between optimization dynamics and data geometry. The key quantity in our analysis is data coherence - the cross‑sample alignment of loss‑surface directions near the optimum. We decompose coherence along three axes: within the retain set, within the forget set, and between them, and prove tight stability thresholds that separate convergence from divergence. To further link data properties to forgettability, we study a two‑layer ReLU CNN under a signal‑plus‑noise model and show that stronger memorization makes forgetting easier: when the signal‑to‑noise ratio (SNR) is lower, cross‑sample alignment is weaker, reducing coherence and making unlearning easier; conversely, high‑SNR, highly aligned models resist unlearning. For empirical verification, we show that Hessian tests and CNN heatmaps align closely with the predicted boundary, mapping the stability frontier of gradient‑based unlearning as a function of batching, mixing, and data/model alignment.  Our analysis is grounded in random matrix theory tools and provides the first principled account of the trade-offs between memorization, coherence, and unlearning.", "tldr": "", "keywords": ["Linear stability", "Optimization theory", "Optimization", "Unlearning", "data geometry"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f016a2e316090e4f2606debb18ed10332cc3232.pdf", "supplementary_material": "/attachment/2c7f7e7c2ce57204f81992b7538824f7bb3c8a04.pdf"}, "replies": [{"content": {"summary": {"value": "The paper analyzes gradient descent-ascent based machine unlearning, through Linear stability. They provide a theoretical analysis, based on linear stability, that demonstrates when the data coherence between the forget and retain sets prevents machine unlearning, with clear bounds. Subsequently, they also provide experimental evaluations that display this effect of data coherence in machine unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The papers strong point is the theoretical analysis, it is an interesting extension of existing linear stability results to unlearning methods, descent ascent, that require different datasets and where the two datasets are treated differently in the optimization process. \n\nSection 2 provides tight ( by order) bounds for the coherence of the dataset, over the bound the models manage to unlearn and escape the minimum to which the original solution was bound and below which they remain stable around that solution.\nThe subsequent discussion about the implications of this result for different unlearning methods is also interesting and important\n\nSection 3 provides a very interesting result that correlates the signal to noise ratio with a models ability to move away from the local minimum in which it started. The result provides an interesting observation between data correlations and a models ability to unlearn."}, "weaknesses": {"value": "My main concerns are the following:\n\n1. A primary concern is the general phrasing for the result. The result to the best of my knowledge refers to ascent based machine unlearning, where gradient ascent is applied to the forget set. This is an important distinction from other unlearning methods that don't utilize this technique, such as [1],[2],[3] and as a result I believe some rephrasing in the introduction and related works is important.\n\n2. For related works, [4] also does an analysis for machine unlearning, where the authors discuss similar concerns for Ascent based machine unlearning and showcase in a different setting that correlation impacts machine unlearning. The two works rely on different tools and establish different results.\n\n2. The references don't work, in the version of the pdf that is uploaded, which is a huge issue, as there is no link from the text to the corresponding reference. This is strange as in the iclr template references by default work.\n\n3. In line 151 there is a typo and $f_r$ should be $n_r$\n\n4. In line 260 there is an e missing.\n\n[1] Rewind-to-Delete: Certified Machine Unlearning for Nonconvex Functions  ,Siqiao Mu, Diego Klabjan\n\n[2] Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning  Eli Chien, Haoyu Wang, Ziang Chen, Pan Li\n\n[3] Attribute-to-Delete: Machine Unlearning via Datamodel Matching Kristian Georgiev, Roy Rinberg, Sung Min Park, Shivam Garg, Andrew Ilyas, Aleksander Madry, Seth Neel\n\n[4] Ascent Fails to Forget Ioannis Mavrothalassitis, Pol Puigdemont, Noam Itzhak Levi, Volkan Cevher"}, "questions": {"value": "My primary question has to do with the linear stability of SGD analysis itself. In lines 127 and 128 you utilize a first order Taylor expansion and eliminate the constant factor with respect to $\\delta$ due to the fact that $w^* $ is a local minimum of the loss function $L$ and as a result $\\nabla L(w^*)=0$. In line 137 you do the same for the stochastic gradient $g_t$, which would correspond to sample $k$ from the dataset. Now in general it is not necessary for the sample to have a gradient that is zero on the local minimum, however in equation (1), line 139 you take it to be zero. I wanted to ask. Am I missing something? Is there an additional assumption that is not stated there?\n\nI only have the question above which is crucial as all of the results rely on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S6qOEgcqXF", "forum": "zZZKAytEGm", "replyto": "zZZKAytEGm", "signatures": ["ICLR.cc/2026/Conference/Submission21570/Reviewer_MYQ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21570/Reviewer_MYQ6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521378949, "cdate": 1761521378949, "tmdate": 1762941840111, "mdate": 1762941840111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the relation between memorization of noise and coherence across samples as they relate to unlearning.  It developed a theoretical framework based on linear stability under which standard unlearning techniques would diverge/converge.  It then conducted an experimental study to observe the link between forgetting, coherence, and memorization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Strong points of the paper include the soundness of the framework, innovation towards utilizing linear stability to enhance ML unlearning using forget and retain sets, and relatively easy-to-follow organization."}, "weaknesses": {"value": "The main pitfall of the papers lies in the minimum space allocated to discussing the experiment which is a large portion of the discussed findings for this paper, leading to confusion and currently inconclusive results (as they appear in paper).\n\nThere is not enough discussion for the results of the experiment, and the graphs are hard to read when considering what they are trying to convey.  Going more in depth to describe them would be helpful for readers comprehension.\n\nWhile the main purpose of the experiment is to demonstrate the effects of lower and higher SNR’s, the provided experimental study failed to serve the purpose, since noise appears to be set to a fixed value according to section 3.2.  If this is not the case it would be helpful to go more in depth into how noise is varied between tests.\n\nReadability: some words appear to be placed for the purpose of enhanced vernacular; however, they are confusing at times (due to appearing incorrect) and could be replaced with better words to convey the appropriate idea.  One such example is “more spurious noise.”"}, "questions": {"value": "How is noise varied between tests, as in the explanation provided in section 3.2, it appears to be fixed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "carYRLz4wG", "forum": "zZZKAytEGm", "replyto": "zZZKAytEGm", "signatures": ["ICLR.cc/2026/Conference/Submission21570/Reviewer_j7DZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21570/Reviewer_j7DZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769818883, "cdate": 1761769818883, "tmdate": 1762941839807, "mdate": 1762941839807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel and valuable theoretical framework for machine unlearning, grounding the process in linear stability analysis. The core concepts of \"data coherence\" and the stability frontier are significant contributions. The insight that stronger memorization (via low SNR data) can facilitate unlearning is particularly provocative and insightful. However, the paper's practical relevance is constrained by strong, potentially unrealistic assumptions and an overly simplified experimental validation. While theoretically elegant, the work suffers from a gap between its theoretical claims and the complexities of real-world unlearning scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's primary strength is its novel application of linear stability analysis to machine unlearning. Framing unlearning as a problem of escaping a local minimum, rather than standard training, is a powerful conceptual shift. This provides a principled lens to analyze the underlying dynamics, moving the field away from purely empirical observations.\n2.  The introduction of \"data coherence\" as a measure of Hessian alignment is a key contribution. It provides an intuitive and quantifiable way to understand the critical interaction between the \"retain\" and \"forget\" sets. This concept elegantly explains why unlearning is difficult when the data to be forgotten is structurally similar to the data being kept.\n3. The paper's most impactful finding is the rigorously argued link between memorization and forgettability (\"the more you memorize, the easier you forget\"). By connecting memorization to low signal-to-noise ratio (SNR) data, which in turn leads to low coherence, the authors establish a surprising and deep connection between unlearning, generalization, and data geometry. This is a significant insight with broad implications."}, "weaknesses": {"value": "1. The Linear Approximation is a Strong Limitation: The entire framework is built upon a local linear approximation of the loss landscape around a minimum (w*). This assumption is fragile in deep learning. The unlearning process, especially when successful (divergent), inherently moves parameters far from this local region, invalidating the approximation. Furthermore, it fails to account for the flat, wide minima common in modern networks where the Hessian may be ill-conditioned.\n2. Definition of \"Successful Unlearning\" is Impractical: The paper equates successful unlearning with the divergence of model parameters (E[||w_k||^2] → ∞). This is a mathematical abstraction that does not align with practical goals. A useful unlearning algorithm must not only forget specific data but also converge to a new state where utility on the retain set is preserved. The current framework provides no guarantees or analysis regarding this crucial aspect.\n3.  The central metric relies on analyzing per-sample Hessians. For any large-scale, modern neural network, computing these Hessians is computationally infeasible. This severely limits the theory's applicability as a practical diagnostic or predictive tool, rendering it more explanatory than prescriptive.\n4. The validation is performed on a synthetic dataset with a simple two-layer CNN. This \"toy\" setup, while clean for validating the theory, leaves a significant open question about whether the findings generalize to complex, high-dimensional datasets (e.g., ImageNet) and deep architectures (e.g., ResNets, Transformers).\n5. The experiments confirm the paper's theoretical bounds but do not benchmark against any state-of-the-art (SOTA) approximate unlearning algorithms. It is unclear how the dynamics predicted by this theory relate to the practical performance (e.g., efficacy, efficiency, utility preservation) of widely-used unlearning methods.\n6. The key finding—\"the more you memorize, the easier you forget\"—is derived and tested under a specific signal-plus-noise data model. This is a narrow definition of memorization. It is not clear if this insight holds for other forms of memorization, such as the verbatim memorization of sequences in LLMs, which may arise from different mechanisms."}, "questions": {"value": "1.\tAcknowledge the limitations of the linear stability assumption and clarify that the theory is best suited to explain the onset of divergence rather than the entire unlearning process.\n2.\t Provide at least preliminary experiments on a more realistic benchmark (e.g., ResNet on CIFAR-10) to demonstrate if the theoretical predictions hold in more complex settings.\n3.\tDiscuss how the theoretical condition of divergence could be connected to practical unlearning goals, such as membership inference attack success or the preservation of retain set accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sfZKbFA9xG", "forum": "zZZKAytEGm", "replyto": "zZZKAytEGm", "signatures": ["ICLR.cc/2026/Conference/Submission21570/Reviewer_G3DN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21570/Reviewer_G3DN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917455517, "cdate": 1761917455517, "tmdate": 1762941839481, "mdate": 1762941839481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to understand the unlearning guarantees of the unlearning algorithms based on gradient ascent. In particular, there are many unlearning algorithm that operates based on the following update rule\n\nw_{k+1} = w_k - η * ( (1-α) * (1/B) * sum_{i in Br_k} ∇ℓ_i(w_k) - α * (1/B) * sum_{j in Bf_k} ∇ℓ_j(w_k) )\n\nThen, the authors use linear approximation for the gradient which implies that g_k approx H_k w_k. Using this approximation it is easy to obtain a closed-form expression for the update rule given above. Then, the authors define a measure of unlearning which is based on having ||w_t|| to infinity or ||w_t|| becomes constant. The intuition is that if ||w_t|| becomes infinity it means that the model can escape from the minima that it learnt from both train set and forget set.\n\nTheir main contribution is a measure that predicts whether ||w_t|| is staying constant or it diverges . Their “unlearning coherence” is a scalar that quantifies how aligned the retain and forget curvature directions are near the trained solution. Intuitively, high unlearning coherence means retain and forget curvatures point in similar directions, so ascent on forget is canceled by descent on retain and the updates tend to stay near the old minimum (harder to “unlearn” in their stability sense); low σ means the directions are incoherent, cancellation is weak, and the iterates can escape the old minimum more easily."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "I think the perspective provided in this work seems very interesting and it would lead to a prescriptive theory related to unlearning using gradient ascent and descent. I think the unlearning coherence metric that explains when ascent on forget is neutralized by descent on retain seems an actionable metric."}, "weaknesses": {"value": "I am not fully convinced with the definition of unlearning proposed in the paper. Consider a scenario where all per-example gradients (and effectively the per-sample curvatures) are aligned; adding or removing a sample doesn’t change the optimization path or the final solution, so by a deletion standard the model is already “unlearned” without any unlearning step, yet the paper’s coherence lens would label this case as resistant (high coherence ⇒ no escape), which contradicts the intended notion of unlearning—this exposes a scope mismatch between “ability/need to move under mixed ascent/descent” and “deletion is satisfied.” \n\nA second weakness is that success is proxied by divergence of ||w_t|| (and, empirically, by forget-loss increases) rather than by deletion equivalence to retraining. I think the right empirical metric is membership inference based test. I don't think forget-loss increase is an interesting measure."}, "questions": {"value": "please discuss the weakness raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NFwumuzMPw", "forum": "zZZKAytEGm", "replyto": "zZZKAytEGm", "signatures": ["ICLR.cc/2026/Conference/Submission21570/Reviewer_3vnF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21570/Reviewer_3vnF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127906717, "cdate": 1762127906717, "tmdate": 1762941839139, "mdate": 1762941839139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}