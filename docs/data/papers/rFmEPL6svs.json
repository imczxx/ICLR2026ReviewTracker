{"id": "rFmEPL6svs", "number": 3373, "cdate": 1757412581988, "mdate": 1759898093328, "content": {"title": "Learning Semantic Anchors for Continual Generalized Category Discovery", "abstract": "Continual Generalized Category Discovery (C-GCD) aims to address the dual challenges of continual learning and generalized category discovery in open environments. This task requires the model to incrementally recognize new classes while resisting catastrophic forgetting of old classes. Existing C-GCD methods do not effectively balance the stability-plasticity dilemma, primarily due to ineffective semantic utilization and knowledge preservation, leading to poor recognition of new classes and catastrophic forgetting of old classes. To address these issues, we propose a novel C-GCD method that leverages textual descriptions and visual features to construct and optimize semantic anchors, and freeze image and text encoders to preserve general pre-trained knowledge. Extensive experiments on several datasets demonstrate that our method significantly outperforms existing C-GCD methods, effectively balancing the stability-plasticity dilemma to achieve enhanced new classes recognition and mitigated forgetting of old classes. Code is provided in the supplementary materials.", "tldr": "", "keywords": ["Continual Learning", "Generalized Category Discovery"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62e5811aa09ca10857b8906af4379be5f9292ec2.pdf", "supplementary_material": "/attachment/80b5416b7a4f5dda13cad2592ebd8ad77806311b.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenging task of Continual Generalized Category Discovery (C-GCD), which jointly considers continual learning and open-world category discovery. The authors propose to leverage CLIP’s pre-trained image and text encoders to provide “semantic anchors,” using textual descriptions and visual features to guide incremental learning without catastrophic forgetting. Both the image and text encoders are frozen to preserve general semantic knowledge, and only the semantic anchor module is optimized across incremental stages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies that most existing C-GCD methods struggle to balance the stability–plasticity trade-off and fail to effectively use pre-trained semantic knowledge.\n\n2. The idea of constructing and optimizing semantic anchors while freezing the CLIP backbone is conceptually simple and practically feasible."}, "weaknesses": {"value": "1. Using CLIP as the feature extractor is problematic because CLIP itself already performs zero-shot classification based on extensive pre-trained knowledge. This introduces additional external information, which contradicts the core goal of C-GCD — discovering and recognizing unknown categories from data rather than relying on prior semantics.\nMoreover, it is not evident whether the proposed approach actually learns or adapts from new data in the continual setting, limiting its adaptability and novelty compared to existing works.\n\n2. The semantic anchors are built from an arbitrarily defined textual vocabulary. This design introduces strong human bias and heavy prior knowledge, and it cannot guarantee coverage of all possible categories, which weakens the claim of “generalized” category discovery.\n\n3. No experiment removes the Semantic Anchor Optimization component. Without such an ablation, it is impossible to verify its actual contribution. The current results could simply reflect CLIP’s inherent zero-shot classification ability rather than the proposed method’s innovation.\n\n4. The paper does not include comparisons with several recent and relevant works, such as PromptCCD and VB-CGCD, which makes it difficult to assess the claimed superiority and novelty.\n\n5. The provided code appears inconsistent with the method described in the paper, which is confusing and raises concerns about reproducibility and reliability.\n\nOverall, the work shows limited innovation beyond using CLIP within the C-GCD framework. Compared to this paper [1], it lacks convincing evidence of novel contribution.\n\n[1] Yuxin Fan, Junbiao Cui, Jiye Liang. Learning Textual Prompts for Open-World Semi-Supervised Learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 14756-14765"}, "questions": {"value": "1. Could you conduct an ablation study to show the performance change after removing the Semantic Anchor Optimization module? Specifically, how does the model perform when directly using CLIP features and the constructed anchors for classification without optimization?\n\n2. Could you compare your method with more recent and relevant baselines, such as PromptCCD[1] and VB-CGCD[2]?\n\n3. How does your method handle the situation where new classes emerge that are not covered by any of the constructed anchors?\n\n[1] Cendra, F.J., Zhao, B., Han, K.. PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category Discovery. ECCV 2024. \n[2] Hao Dai, Jagmohan Chauhan. Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective. Proceedings of the 42nd International Conference on Machine Learning, PMLR 267:11884-11903, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fBWuPYIBcM", "forum": "rFmEPL6svs", "replyto": "rFmEPL6svs", "signatures": ["ICLR.cc/2026/Conference/Submission3373/Reviewer_5ARW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3373/Reviewer_5ARW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650684255, "cdate": 1761650684255, "tmdate": 1762916693788, "mdate": 1762916693788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SAC-GCD, a CLIP-based approach for Continual Generalized Category Discovery that builds semantic anchors from text prompts, optimizes them with visual features, freezes the image/text encoders, and performs bimodal label propagation at inference. Experiments on CIFAR-100, ImageNet-100, Tiny-ImageNet, and CUB report notable gains over prior works and large efficiency improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important problem in continual generalized category discovery (C-GCD) using a simple and modular framework that combines frozen encoders with learnable anchors.  \n2. The proposed method achieves strong performance across multiple datasets and stages, showing improved new-class accuracy, reduced forgetting, and significantly shorter training time.  \n3. The writing and figures are clear and easy to follow, and the task setting is well described and motivated."}, "weaknesses": {"value": "- The main components, including prompt or anchor learning for VLMs and label propagation, are already well studied. The paper would benefit from a clearer positioning relative to existing prompt-learning methods, as well as recent CLIP-based label propagation approaches, rather than only citing them. It remains unclear what is fundamentally new in the optimization beyond existing prompt-tuning and distribution-balancing techniques.\n- SAC-GCD leverages CLIP ViT-B/16 with frozen encoders; not all baselines do. Ensure apples-to-apples comparisons (same backbone/pretraining, frozen vs. trainable policies) and include CLIP-based strong baselines with prompt tuning. Some reported gains (e.g., very high new-class accuracy and drastic speedups) look unusually large and warrant careful controls.\n- Some algorithmic details are missing. For example, how exactly is Eq. (4) optimized per batch/epoch? Provide algorithmic steps, runtime, and ablation without that constraint, the practical procedure for anchor selection and expansion, and how memory and computation scale as classes increase."}, "questions": {"value": "- Can you report comparisons against CLIP-prompt baselines (e.g., learnable prompts / visual prompts) under identical frozen-encoder setups?\n- What is the per-stage memory footprint as SemanticDB grows, and how does inference latency scale with the number of anchors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V222EmVSKi", "forum": "rFmEPL6svs", "replyto": "rFmEPL6svs", "signatures": ["ICLR.cc/2026/Conference/Submission3373/Reviewer_6Bo9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3373/Reviewer_6Bo9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929701723, "cdate": 1761929701723, "tmdate": 1762916693578, "mdate": 1762916693578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a vision-language-based framework named SAC-GCD. The goal is to address the stability-plasticity dilemma in continual learning scenarios where new categories emerge over time (Generalized Category Discovery).\nThe method constructs semantic anchors by leveraging class textual descriptions projected via a pre-trained CLIP model. These anchors are optimized using visual features to better align the visual-text space. To prevent forgetting, both the image and text encoders are frozen, and previously learned semantic anchors are reused across stages. Experiments on standard datasets (CIFAR100, IN-100, Tiny-ImageNet, CUB) reportedly outperform prior C-GCD baselines in both new-class recognition and mitigating forgetting, while improving efficiency by avoiding retraining of large encoders."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper provides a coherent description of the C-GCD problem setting and the associated challenges of balancing plasticity and stability.\n+ Freezing CLIP encoders and optimizing only semantic anchors significantly reduces training time and computational cost, which is practically appealing for open-world scenarios.\n+ Evaluations across multiple datasets with comparisons with several baselines demonstrate consistent performance gains in both accuracy and efficiency."}, "weaknesses": {"value": "- The C-GCD task itself has been well established. The presented framework mostly reuses existing components, such as frozen CLIP embeddings and semantic projections, without introducing a fundamentally new formulation or learning objective.\n- The core mechanism (semantic anchors optimized with visual features) is conceptually similar to existing proxy-based or anchor-based representation learning methods, for example, Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery, Kim et al., ICCV 2023. The primary modification is the inclusion of CLIP-derived text features, which is a rather straightforward extension rather than a novel learning principle.\n- The paper does not sufficiently disentangle the contribution of each design choice, (1) the use of CLIP text features vs. standard learned anchors, (2) the necessity of freezing encoders, (3) the specific anchor optimization loss. A quantitative ablation study is needed.\n- The text flow is often repetitive, overly verbose, and lacks linguistic smoothness. Citations are sometimes incorrectly formatted (missing parentheses, inconsistent year styles), and certain sentences are grammatically awkward or redundant. This affects clarity and readability."}, "questions": {"value": "1. How does the proposed “semantic anchor optimization” differ algorithmically and conceptually from prior proxy/anchor-based GCD methods, particularly Kim et al. (ICCV 2023) and Park et al. (ECCV 2024)? A direct comparison or visualization of the optimization process would help justify novelty.\nKim et al. Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery, Park et al. Online Continuous Generalized Category Discovery\n2. Can the authors include results showing performance with and without CLIP text features (or with different VLMs) to quantify the contribution of text-guided semantic information?\n3. Since CLIP embeddings are frozen, how does the method adapt when domain shift occurs, for example, medical or industrial datasets where CLIP’s prior fails? Does the performance drop significantly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AxlPRaouEB", "forum": "rFmEPL6svs", "replyto": "rFmEPL6svs", "signatures": ["ICLR.cc/2026/Conference/Submission3373/Reviewer_MGMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3373/Reviewer_MGMg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973313977, "cdate": 1761973313977, "tmdate": 1762916692993, "mdate": 1762916692993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles Continual Generalized Category Discovery (C-GCD) and proposes SAC-GCD, a CLIP-based pipeline. At every stage, the method (i) builds a class-description database from internet class names and CLIP-style templates, (ii) uses current-stage data to pick the top $|C_u^t|$ descriptions, (iii) turns them into semantic anchors and optimizes them by matching CLIP text similarities to learnable visual anchors with a KL loss, and (iv) finally performs bimodal label propagation over test features and anchors for prediction. The authors claim consistent gains over C-GCD baselines on CIFAR-100, ImageNet-100, Tiny-ImageNet, and CUB compared to other C-GCD methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. **Clear motivation**: the paper claims that current C-GCD methods rarely exploit class-level semantics and often retrain multi-layer encoders, which can hurt old classes. SAC-GCD is a multimodal alternative. \n2. **Lightweight training**: freezing CLIP and only learning stage-wise semantic anchors is attractive from a deployment perspective."}, "weaknesses": {"value": "**(A) Fairness of comparison is not established.**\nAlthough the paper says “we follow the partitioning in Happy … to ensure fairness,” the *actual* pipeline is much stronger than many of the reported baselines: it uses a frozen CLIP as backbone and at test time concatenates test features with semantic anchors and runs bimodal label propagation (LP). Most baselines in Table 2 were not conducted in CLIP or designed  for this inference mode, and the paper appears to reuse their reported numbers rather than rerunning them under “frozen CLIP + LP.” This makes it hard to tell whether gains come from the proposed semantic-anchor machinery or simply from (i) using CLIP, and (ii) using a transductive LP head. A minimally fair comparison would require:\n\n* baselines rerun on the **same CLIP-ViT encoder**,\n* baselines evaluated with the **same LP inference**, or\n* an ablation “SAC-GCD w/o LP”.\n  Without this, the headline table is not persuasive.  \n\n**(B) Description design is underspecified and risks leaking unknown-class semantics.**\nSection 3.2 says the description pool is built by “collecting a large number of class names from the internet and generating textual descriptions using templated statements,” and then, for a stage t, the method counts which descriptions are most similar to current data and picks the top  $|C_u^t|$.  This raises two problems:\n\n1. If the global pool already contains *true* or *near-synonym* names of test-time classes, then the model is effectively allowed to *recover class names from unlabeled data* via CLIP, which is a form of label leakage.\n2. The paper does **not** show any concrete examples of descriptions, does **not** report pool size, source sites, or filtering rules, and does **not** run the obvious ablation “remove exact/near class names from the pool.”\n\n**(C) Writing / technical precision issues.**\nThe core distribution-optimization step (Eq. (4)) enforces\n$\\sum_j z_{i,j} = \\frac{1}{N_l + N_u^t}$\nand then calls this “probability normalization.” But that is *not* a standard per-sample normalization (one would expect it to be 1); it is then “fixed” later by multiplying back $(N_l + N_u^t)$ before pseudo-labeling. This writing is very confusing. The text around this part also mixes $z_{i,j}$ (line 236), $Z$ (line 256) without re-introducing symbols, and many references are cited in a slightly informal way (e.g., line 28). Moreover, 3.2 says class numbers are estimated (line 215); implementation says they are known (line 374). Overall, the paper feels like a workshop draft.\n\n**(D) Limited novelty relative to CLIP-based GCD / prompt-learning lines.**\nThe paper itself notes that the proposed optimization is “conceptually similar to prompt learning” and that it simply learns a set of stage-wise semantic vectors and aligns them to CLIP-induced distributions.  This is a reasonable adaptation to the C-GCD protocol, but the ingredients — CLIP feature frozen, text prompts/descriptions, KL alignment to visual prototypes, class-balance constraints, and transductive LP — are all known [1, 2]. What is new is mainly *where* the authors apply it (C-GCD) and that they choose to *reuse* anchors across stages. This is on the thin side unless the empirical section is watertight (which, per (A)–(B), it is not yet).\n\n**(E) Missing ablations for every critical component** (semantic optimization, balanced assignment, LP, expansion).\n\n**(F) No final loss function.**\n\n**(G) No hyperparameter analysis.**\n\n[1] Learning textual prompts for open-world semi-supervised learning.\n\n[2] GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery"}, "questions": {"value": "1. You select top (|C_u^t|) descriptions by counting matches over current-stage unlabeled data. How do you guarantee that the global description pool does **not** contain exact or quasi-exact names of the incoming “unknown” classes? Please provide concrete examples of descriptions and a “no-true-name” ablation. \n2. Were the baselines in Table 2 **re-run** with frozen CLIP and the **same** label-propagation inference? If not, can you add such a table, or at least SAC-GCD w/o LP? \n3. How sensitive is your method to the size and noise level of the description database (e.g. 1k vs 10k candidates; generic vs fine-grained names)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xm9uWctum8", "forum": "rFmEPL6svs", "replyto": "rFmEPL6svs", "signatures": ["ICLR.cc/2026/Conference/Submission3373/Reviewer_JiZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3373/Reviewer_JiZD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974041694, "cdate": 1761974041694, "tmdate": 1762916692723, "mdate": 1762916692723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}