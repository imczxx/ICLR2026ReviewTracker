{"id": "DUr48hxO2h", "number": 8335, "cdate": 1758078946604, "mdate": 1763706641069, "content": {"title": "Incentivizing Consistent, Effective and Scalable Reasoning Capability in Audio LLMs via Reasoning Process Rewards", "abstract": "The role of reasoning in Audio Large Language Models remains widely underexplored, as introducing a reasoning process often degrades rather than improves performance during inference, a phenomenon we term test-time inverse scaling, where longer reasoning chains yield progressively worse results. We demonstrate that this stems not from fundamental limitations of reasoning itself, but from inadequate training: models without proper guidance for the reasoning process produce hallucinatory, inconsistent reasoning that accumulates errors over longer chains. To address these challenges, we introduce CESAR (Consistent, Effective, and Scalable Audio Reasoners), shifting from outcome verification to rewarding the reasoning process. Our online reinforcement learning framework employs Group Relative Policy Optimization with a multi-faceted reward suite that incentivizes not only correctness and format but also consistency, structured analytical patterns, causal reasoning, domain-knowledge integration, and calibrated reasoning depth. CESAR resolves test-time inverse scaling, transforming reasoning from  detriments into gains while revealing model-specific \"reasoning sweet spots\", where performance peaks during test-time scaling. We achieve state-of-the-art results on MMAU Test-mini, substantially outperforming Gemini 2.5 Pro and GPT-4o Audio, and near-human-level performance on MMSU reasoning tasks. Through AI-as-judge evaluations and qualitative comparisons, we provide both quantitative and qualitative validation of our improved reasoning quality. Importantly, enhanced reasoning creates synergistic effects, simultaneously improving multimodal reasoning and perception capabilities. Overall, CESAR establishes a principled method for developing robust and scalable reasoning in Audio LLMs.", "tldr": "We reveal why audio reasoning fails—models generate inconsistent, hallucinatory, and unstructured reasoning process that degrades performance and can't scale. Our process-oriented RL training resolves this, enabling effective and scalable reasoning.", "keywords": ["Reinforcement Learning", "Audio Large Language Models", "Test-Time Inverse Scaling", "Process-Oriented Rewards", "Reasoning Length Calibration", "Multimodal Reasoning", "Reasoning Scaling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75f315bb052b37d2dea39b577a4100378b0b36a9.pdf", "supplementary_material": "/attachment/db9312a2de248b7fb096302d637514aaf91667e5.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents CESAR, a reinforcement learning framework that improves reasoning in Audio LLMs by rewarding the reasoning process rather than only the final answer. The authors identify a new phenomenon \"test-time inverse scaling\", where longer reasoning chains hurt performance, and attribute it to the lack of process-level supervision. CESAR uses Group Relative Policy Optimization (GRPO) with multiple reward components to train models that reason coherently and efficiently. Experiments on MMAU and MMSU show state-of-the-art results, outperforming GPT-4o Audio and Gemini 2.5 Pro, and reveal “reasoning sweet spots” that improve with controlled reasoning depth."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) The authors identify and analyze test-time inverse scaling in Audio LLMs, a previously undocumented failure mode.\n\n(2) The authors propose a process-oriented RL reward design that explicitly improves reasoning quality and depth."}, "weaknesses": {"value": "(1) The notion of test-time inverse scaling is introduced conceptually but not formally defined or quantified. The paper does not provide a measurable coefficient or slope to characterize this phenomenon. Moreover, there is no analysis of computational cost or latency as a function of reasoning length, which would clarify efficiency trade-offs in scaling reasoning depth.\n\n(2) The paper focuses on reasoning accuracy but ignores word-error rate (WER) or fluency degradation, which could affect real-world usability. No fluency-aware or perceptual rewards are explored.\n\n(3) The comparisons include Qwen2.5-Omni, Ke-Omni-R, GPT-4o Audio, and Gemini 2.5 Pro, but omit other competitive open-source models such as Qwen2-Audio [1], Audio Flamingo 3 [2], and Baichuan-Omni-1.5 [3]. Adding these baselines would better contextualize CESAR’s gains and clarify the limits of proprietary systems.\n\n(4) The GPT-4o Audio “AI-as-Judge” evaluation lacks statistical rigor. The paper does not report confidence intervals, sample sizes, or randomization procedures, and it relies solely on GPT-4o Audio without human verification or alternative evaluators. Using multiple judgment models or human raters would help mitigate potential bias and confirm alignment with human judgment.\n\n\n(5) Experiments are conducted primarily on MMAU Test-mini, which is a small subset of MMAU-Pro [4], and on MMSU. Although the paper briefly mentions testing on the full MMAU-Pro [4] benchmark, detailed results are not reported. The representativeness of the subset is unclear, which weakens claims of scalability and generalization. Evaluating CESAR on additional benchmarks with Audio-CoT [5], SoundMind [6], CoTA [7] would substantiate its robustness.\n\n\n(6) The paper omits key implementation details, including training steps, learning rates, sampling temperature, and optimizer configuration. Moreover, no GRPO stability curve or convergence analysis is provided, making it difficult to assess the reliability and reproducibility of the proposed reinforcement learning setup.\n\n**References**:\n\n[1] Qwen2-Audio: https://arxiv.org/pdf/2407.10759\n\n[2] Audio Flamingo 3: https://arxiv.org/pdf/2507.08128\n\n[3] Baichuan-Omni-1.5: https://arxiv.org/pdf/2501.15368\n\n[4] MMAU-Pro: https://huggingface.co/datasets/gamma-lab-umd/MMAU-Pro\n\n[5] Audio-CoT: https://huggingface.co/datasets/liuhuadai/AudioCoT\n\n[6] SoundMind: https://huggingface.co/datasets/SoundMind-RL/SoundMindDataset\n\n[7] CoTA: https://huggingface.co/datasets/zhifeixie/Audio-Reasoner-CoTA"}, "questions": {"value": "I believe this paper has strong merit and makes an original contribution to the study of reasoning in Audio LLMs. If the authors can address the following questions and provide the requested analyses or experiments, I will raise my overall score and support acceptance of this paper.\n\n(1) Could the authors formally define test-time inverse scaling and provide a quantitative measure (e.g., slope or scaling coefficient) to characterize this phenomenon? Please also include an analysis of computational cost or latency as a function of reasoning length to clarify efficiency trade-offs.\n\n(2) How does CESAR affect word-error rate (WER) and audio fluency during reasoning generation? Have the authors considered introducing fluency-aware or perceptual rewards to mitigate potential degradation in real-world applications?\n\n(3) Could the authors extend the experimental comparison to include recent open-source audio reasoning models with Qwen2-Audio [1], Audio Flamingo 3 [2], and Baichuan-Omni-1.5 [3]? Including these baselines would better contextualize CESAR’s gains and clarify its dependence on proprietary systems.\n\n(4) Current experiments are primarily restricted to multiple-choice QA datasets (MMAU Test-mini and MMSU). To convincingly demonstrate CESAR’s generalization ability, could the authors evaluate the model on Audio-CoT [5],  SoundMind [6],  and CoTA [7]. Reporting results on these benchmarks would directly support the paper’s claims about scalability and robustness across diverse reasoning tasks.\n\n(5) The AI-as-Judge evaluation currently relies solely on GPT-4o Audio. Would the authors consider adding results from multiple evaluators (e.g., Gemini 2.5 or human raters) to assess consistency and potential bias? Reporting confidence intervals and sample-size statistics would further improve reliability.\n\n(6) Could the authors provide detailed hyperparameters (training steps, learning rates, sampling temperature, etc.) and include a GRPO stability curve to illustrate convergence behavior and training reliability?\n\n\n**References**:\n\n[1] Qwen2-Audio: https://arxiv.org/pdf/2407.10759\n\n[2] Audio Flamingo 3: https://arxiv.org/pdf/2507.08128\n\n[3] Baichuan-Omni-1.5: https://arxiv.org/pdf/2501.15368\n\n[4] MMAU-Pro: https://huggingface.co/datasets/gamma-lab-umd/MMAU-Pro\n\n[5] Audio-CoT: https://huggingface.co/datasets/liuhuadai/AudioCoT\n\n[6] SoundMind: https://huggingface.co/datasets/SoundMind-RL/SoundMindDataset\n\n[7] CoTA: https://huggingface.co/datasets/zhifeixie/Audio-Reasoner-CoTA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OnFmNr93WN", "forum": "DUr48hxO2h", "replyto": "DUr48hxO2h", "signatures": ["ICLR.cc/2026/Conference/Submission8335/Reviewer_Rzte"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8335/Reviewer_Rzte"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760660135797, "cdate": 1760660135797, "tmdate": 1762920256967, "mdate": 1762920256967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles \"test-time inverse scaling\" in Audio LLMs, where longer reasoning chains degrade performance. The authors propose CESAR, an RL framework that rewards the \"reasoning process\" itself, not just the final answer. This method incentivizes consistency, structured logic, and penalizes \"overthinking\". CESAR successfully resolves this issue , achieving SOTA results on the MMAU benchmark and surpassing Gemini 2.5 Pro."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is technically sound. Its claims about solving \"test-time inverse scaling\" are strongly supported by extensive experiments, including state-of-the-art (SOTA) comparisons, comprehensive ablation studies, and \"test-time scaling\" analysis.\n2. The paper is exceptionally well-organized with a clear, logical narrative. It provides a high degree of transparency, including pseudocode and detailed keyword tables, which significantly aids reader understanding and the ability to reproduce the results.\n3. The work solves a core problem for Audio LLMs, offering a new, engineerable paradigm for reasoning. Its most significant impact may be diagnostic: by solving reasoning, it clearly identifies the field's next major barrier—the \"perceptual bottleneck\"—which provides crucial guidance for future research."}, "weaknesses": {"value": "1. The proposed multi-faceted reward suite introduces five new reward weights ($\\alpha_j$) that must be balanced and tuned. While the authors provide a simple, effective ratio, this still represents an added layer of complexity compared to simpler reward models. \n2. The readability of Figure 1 is poor. The text within the \"Qualitative Comparison of Reasoning Process\" section , which is meant to show examples of reasoning failures and improvements, is far too small. This makes it very difficult for the reader to understand these crucial illustrative examples, undermining the figure's purpose."}, "questions": {"value": "1. How were the final reward weights (e.g., $\\alpha_1=5.0$ for accuracy, $\\alpha_{2-5}=1.0$ for process rewards) determined? How sensitive is the model's performance to variations in these weights, and is there a systematic approach to balancing these distinct reward objectives?\n2. Since the CESAR reward suite lacks explicit ASR or WER metrics, how does the training impact the model's ASR or WER? Does the \"synergistic effect\" seen in perception tasks (Table 9) also translate to a measurable reduction in WER?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CRUygM0WAm", "forum": "DUr48hxO2h", "replyto": "DUr48hxO2h", "signatures": ["ICLR.cc/2026/Conference/Submission8335/Reviewer_WEba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8335/Reviewer_WEba"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751501393, "cdate": 1761751501393, "tmdate": 1762920256543, "mdate": 1762920256543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning framework, CESAR, to address performance degradation from chain-of-thought reasoning in Audio LLMs. To solve this \"test-time inverse scaling\" problem, the method shifts from rewarding only the final outcome to rewarding the quality of the reasoning process itself, using a combination of reward objectives for consistency and structure. The model is trained with Group Relative Policy Optimization (GRPO) and achieves state-of-the-art performance on the MMAU and MMSU benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The problem statement is clear and the method is sound.\n2. The paper is very neatly written and easy to read and follow.\n3. The results on the datasets used show good performance improvements.\n4. The Appendix (Supplementary Material) is very thorough and contains very relevant discussions."}, "weaknesses": {"value": "1. The ablation study shows the quantitative impact of each reward component, but a qualitative analysis is missing. It would be helpful to see examples of the reasoning text to understand how the Keywords Reward changes the model's thinking. Furthermore, the selection of the keywords feels a bit arbitrary. A clearer explanation for why these specific keywords were chosen would strengthen this part of the method.\n2. The reward function has several weighted parts. The paper provides one set of weights but does not analyze how sensitive the results are to these choices. Understanding how performance changes when the weights are adjusted would be valuable and provide better practical guidance for implementing the method.\n3. The paper's narrative is a bit too attached to Ke-Omni-R1. I believe the paper should rely on it's contributions to stand more on it's own methodology."}, "questions": {"value": "1. The reasoning length \"sweet spot\" seems quite small (around 40 tokens). In other domains like complex mathematics, reasoning chains can be much longer (in the thousands). Is this shorter optimal length a characteristic of the audio reasoning benchmarks used (i.e., the tasks do not require longer chains of thought), or does it reflect a potential limitation of the current method where longer reasoning chains still risk performance degradation, albeit much less severely than in baseline models?\n2. Regarding the reward weights (the alphas in Eq. 3): How sensitive is the model's performance and behavior to these weights? For example, the ablation shows removing the Keywords reward has a significant impact, but what happens if its weight is increased to be on par with the accuracy reward? (all alphas to 1)\n3. How sensitive is the method to different prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IObRjWeo0i", "forum": "DUr48hxO2h", "replyto": "DUr48hxO2h", "signatures": ["ICLR.cc/2026/Conference/Submission8335/Reviewer_D7P9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8335/Reviewer_D7P9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901507058, "cdate": 1761901507058, "tmdate": 1762920256168, "mdate": 1762920256168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CESAR, a method that addresses a phenomenon the authors call test‑time inverse scaling in Audio LLMs, that means longer chain‑of‑thought (CoT) at inference often hurts accuracy. They argue this is due to models being forced to reason without being trained how to reason. They propose CESAR, an online RL framework using GRPO with a multi‑facet reward suite: (i) correctness and format, (ii) process rewards for reasoning–answer and reasoning–question consistency, explicit reasoning patterns/keywords , and an over‑thinking penalty that linearly penalizes long CoT. They used Qwen2.5‑Omni‑7B for experiments. CESAR reports SOTA on MMAU Test‑mini and higher MMSU averages than several baselines. Ablations attribute most gains to the Keywords and Consistency components."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well structured with a clear framework diagram\n- Moving beyond outcome‑only RL to process‑oriented rewards for Audio LLM reasoning is a good and practical direction.\n- Consistent gains with the proposed approach"}, "weaknesses": {"value": "- The paper argues that introducing CoT at inference degrades accuracy in Audio LLMs unless the reasoning process is explicitly trained. However, AURELIA [1] reports improvements by injecting structured, step‑by‑step reasoning into AV‑LLMs at test time without additional training, which appears to contradict the generality of this claim.  A head‑to‑head comparison (e.g., an AURELIA‑style inference‑only reasoning condition) would clarify boundaries of the inverse‑scaling effect the authors report.\n- The “AI‑as‑judge” uses GPT‑4o Audio only; there is no human‑study calibration or alternative judges to validate preference robustness.\n- The scope of evaluation ids limited. Only one model (Qwen) is used to validate the proposed approach. Also, MMAU evaluation is on Test‑mini (1k items), and there are no error bars or seed variability.\n- The linear over‑thinking penalty (Eq. 8) and fixed Lmax_output=256 are plausible but unvalidated; no sensitivity is shown for α‑weights, group size K, or the penalty shape.\n\nReferences:\n\n[1] Sanjoy Chowdhury and Hanan Gani et al. \"AURELIA : Test‑time Reasoning Distillation in Audio‑Visual LLMs\". ICCV 2025."}, "questions": {"value": "- How do win‑rates change with a different judge (e.g., a text‑only LLM judging rendered transcripts, or an open model)?\n- What is the training‑time overhead (GPU‑hours) and inference‑time cost per query at the reported sweet spot (~35–40 tokens) vs non‑reasoning mode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4uv9ZtUtD6", "forum": "DUr48hxO2h", "replyto": "DUr48hxO2h", "signatures": ["ICLR.cc/2026/Conference/Submission8335/Reviewer_59TJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8335/Reviewer_59TJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948608922, "cdate": 1761948608922, "tmdate": 1762920255625, "mdate": 1762920255625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}