{"id": "f5MC1G6XhB", "number": 12157, "cdate": 1758206028862, "mdate": 1759897528392, "content": {"title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "abstract": "The {\\em stop gradient} and {\\em exponential moving average} iterative procedures are commonly used in non-contrastive approaches to self-supervised learning to avoid representation collapse, with excellent performance in downstream applications in practice. This presentation investigates these procedures from the dual viewpoints of optimization and dynamical systems. We show that, in general, although they {\\em do not} optimize the original objective, or {\\em any} other smooth function, they {\\em do} avoid collapse. Following [Tian et al. 2021], but without any of the extra assumptions used in their proofs, we then show using a dynamical system perspective that, in the linear case, minimizing the original objective function without the use of a stop gradient or exponential moving average {\\em always} leads to collapse. Conversely, we characterize explicitly the equilibria of the dynamical systems associated with these two procedures in this linear setting as algebraic varieties in their parameter space, and show that they are, in general, {\\em asymptotically stable}. Our theoretical findings are illustrated by empirical experiments with real and synthetic data.", "tldr": "A theoretical study of stop gradient and exponential moving average training procedures in self-supervised learning from the dual viewpoints of optimization and dynamical systems", "keywords": ["Deep learning", "representation learning", "self-supervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0bb077af92657c54e377cd0d82cdf106c07e0e2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates two popular optimization approaches in non-contrastive learning: exponential moving average (EMA) and stop-grad (SG), both of which are employed in practice to prevent the model from feature collapse (that is, learning the trivial solution where model output is always zero). This question has been previously studied, starting from the work of Tian et al (2021), but the present work weakens some assumptions by relying on classic results from dynamical systems.\n\nThe main contributions of this paper are:\n- to show EMA/SG does not optimize the original loss function and may not optimise any well defined function\n- assuming the models are linear, the paper characterises the equilbria of the dynamical systems associated with EMA and SG, and proves that these equilibria are asymptotically stable"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "EMA and SG are often deployed in optimising deep learning models to prevent collapse. While the focus of this paper is on non-contrastive models (where these techniques are always used), there is a general need for understanding EMA/SG, and how these techniques can lead to desirable/undesirable solutions.\n\nIn particular, this paper provides theoretical/empirical evidence that EMA/SG do not minimise the original objective and the parameters need not necessarily converge, but the equilibria can be characterised as an (asymptotically stable) algebraic variety. These characterisations can provide a more informed understanding of when to (not) apply EMA/SG in deep learning or derive theoretically sound fixes."}, "weaknesses": {"value": "1. The interesting results (Section 3) assume linear models: I note this first because it's a typical grudge about many theory papers. However, it's important to make two observations:\n- The linearity is assumed with respect to trainable parameters. Notably, one could replace the linear non-contrastive models $f_\\theta, f_\\xi$ in Section 3 by random feature models or deep networks in lazy-training regime. This can be easily fixed in the paper by redefining $[xx^\\top], [xy^\\top]$. I believe this would not change any analysis. So, in principle, the analysis holds for models that are nonlinear with respect to input data.\n- The linearity with respect to training parameters is necessary with our current theoretical understanding of EMA/SG. It is evident from the paper that interesting conclusions, which were not previously known, can still be derived assuming linearity. In summary, this \"weakness\" is just to explicitly refute any other possible review that criticises the linearity assumption in the paper\n\n2. Vagueness of some proofs and assumptions. The presentation of some of the proofs is not formal enough (a few less-critical concerns are listed in questions). A bit more critical concern is the lack of clarity of loss in Section 3. Although not clearly stated, the loss is assumed to be a squared distance between the representations from the two models. Many of the proof steps rely on this fact. Here are a few observations:\n- While the analysis of EMA/SG is motivated by papers on BYOL/SimSiam, both consider cosine-based loss functions and not exactly squared distance between (linear) model outputs. The normalisation plays a crucial role in these works, which cannot be explained by the current analysis.\n- Some of the claims in Section 3 cannot extend to other losses. A simple example is the following case of SG: $\\theta(0) = \\xi(0) = 0; \\dot{\\theta}(t) = \\cos(t); \\dot{\\xi}(t) = \\theta(t) - \\xi(t)$. In this case, WolframAlpha gives the solution: $\\theta(t) = \\sin(t)$ but $\\xi(t) = (e^{-t} + \\sin(t) - \\cos(t))/2$. Hence, in this case, the continuous-time dynamics suggested in Equation (5) is an incorrect approximation to SG in discrete time. Similar examples can be constructed for EMA as well, raising doubt for the continuous-time dynamics.\n- The proofs of Lemma 3.2 and Proposition 3.5 assume that the parameter $C$ follows $A$ (since it holds for SG in discrete time), but shouldn't these proofs include the dynamcis of $C$?"}, "questions": {"value": "The main questions are in point-2 of weakness. Several other minor comments are listed below:\n\n1. Description of SG: I think the SimSiam paper eventually uses a symmetrized version of the loss, and hence, SG gets applied to both models. This is not considered in the present paper, where SG is applied only on $\\xi$. It would be good to clarify this. Also line 99 (SG step-(a)) is confusing if one directly replaces $\\xi_{t-1}$ by $\\theta_{t-1}$ (because the update of second model is not clearly specified). The description of EMA with $\\alpha_t=0$ is more clear.\n\n2. lines 227-234: u,v are not defined and $\\lambda$ is missing for inline expressions for $\\nable \\Omega$.\n\n3. Proposition 3.5: Assumption $A$ is full rank is incorrect because it is a rectangular matrix. It is better to assume $S$ is strictly positive definite. By the way, is it possible to characterise when (if at all) is $S$ strictly positive at equilibria? This also may depend on the data, and may not hold generally.\n\n4. Line 368: \"Such a system admits $2^{m(m+1)/2}$ solutions.\"  It is not clear to me why this should hold. Please provide a reference or proof.\n\n5. Theorem 3.8: Replace $A$ with a different notation (since $A$ is used in the model)\n\n6. Lemma 3.4: The result that $B^\\top B = AA^\\top$ asymptotically is reminiscent of several results on gradient descent for deep linear networks (dating back to 1990s). It would be good to include a short discussion on this. Perhaps the asymptotic equality is a consequence of EMA/SG in this setting. \n  \n7. Line 665: \"There is a priori no reason ..\" I don't believe this is a formal way to prove a statement. You need to provide a concrete counter-example. Consider a simple case of a data and loss function to argue that a critical point of $F$ may not be a critical point of $E$. \n\n8. Line 687-688: Same issue as above. Use the linear case (discussed after the proof ends) and assume some simple fixed data to make your point. Perhaps assuming the augmentation is $y = x + \\epsilon$ could work here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pqwontLAUL", "forum": "f5MC1G6XhB", "replyto": "f5MC1G6XhB", "signatures": ["ICLR.cc/2026/Conference/Submission12157/Reviewer_sCRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12157/Reviewer_sCRT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478266041, "cdate": 1761478266041, "tmdate": 1762923112635, "mdate": 1762923112635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the common self-supervised learning procedures of **stop gradient** and **exponential moving average**, which are used to prevent representation collapse. \n\nThe authors show that while these iterative procedures do not optimize the original or any other smooth objective function, they are effective at avoiding collapse. \n\nUsing a dynamical systems perspective in the linear case, the study demonstrates that minimizing the original objective *without* these procedures always leads to collapse, and it explicitly characterizes the **asymptotically stable equilibria** for both procedures as algebraic varieties, which are illustrated with empirical experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a clear theoretical foundation explaining why stop-gradient and EMA prevent representation collapse.\n2. Offers rigorous analysis from both optimization and dynamical systems perspectives.\n3. Empirical experiments effectively support the theoretical findings on real and synthetic data."}, "weaknesses": {"value": "1. The statement in Lines 156–157 (“The SG and EMA training procedures have been designed to avoid collapse in self-supervised learning”) is not entirely accurate. Prior work, such as SimSiam, has already shown that Stop-Gradient alone can prevent collapse without EMA. Moreover, BYOL’s EMA inherently includes a Stop-Gradient operation, and the true collapse-prevention factor lies in the asymmetric *predictor* component — without it, SG or EMA alone would fail.\n\n2. While the paper provides some theoretical analysis of non-contrastive learning, the discussed methods are now outdated compared to more recent and stronger approaches such as MoCov3, DINO, iBOT, and MAE-based self-supervised models. Several studies [1–3] have already offered deeper theoretical insights into both contrastive and non-contrastive frameworks, making the contribution of this paper less timely and impactful.\n\n3. The experimental validation is weak, relying heavily on limited and synthetic data without robust quantitative evaluation on standard benchmarks. This weakens the credibility of the theoretical claims — more comprehensive and realistic experiments are needed to meet the expectations for publication.\n\n[1] *How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-Supervised Contrastive Learning*, ICLR 2022\n\n[2] *Understanding Collapse in Non-Contrastive Siamese Representation Learning*, ECCV 2022\n\n[3] *Contrasting the Landscape of Contrastive and Non-Contrastive Learning*, AISTATS 2022"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bWsR39tFZb", "forum": "f5MC1G6XhB", "replyto": "f5MC1G6XhB", "signatures": ["ICLR.cc/2026/Conference/Submission12157/Reviewer_f64b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12157/Reviewer_f64b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594707415, "cdate": 1761594707415, "tmdate": 1762923112174, "mdate": 1762923112174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, paper provides a theoretical study of two core techniques used in non-contrastive self-supervised learning (SSL) methods - stopping Gradient (SG) and Exponential moving Average (EMA) - that are essential to prevent representation collapse in algorithms such as BYOL and SimSiam. From a dual perspective of optimization theory and dynamic systems, the authors investigate why these methods have been empirically successful in the absence of negative samples and explicit regularization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The topic of the paper is good. It tries to explain non-contrastive self-supervised methods like BYOL/SimSiam from both optimization theory and dynamical systems theory, which \"learn good representations without seeming to have an objective function.\" This combination is innovative in the research of SSL theory. In particular, the perspective of dynamical systems (continuous-time analysis, equilibrium points, and stability proofs) provides new insights into the dynamics of self-supervised training. The authors verify that the non-convergence properties of SG and EMA are consistent with the theory on video classification tasks. Authors also conduct toy experiments to illustrate the trajectory of the dynamical system. Although the experiment is small-scale, it serves as an intuitive verification of the theoretical behavior."}, "weaknesses": {"value": "1. The paper introduces several symbols on the same page, and these symbols represent different meanings in different contexts. This high density of symbolic definitions makes readers need to frequently backtrack and compare with the previous texts during reading, which increases the burden of understanding and is not conducive to quickly grasping the core deduction logic. \n2. I understand the authors’ intention to introduce examples of SG and EMA directly in the introduction. However, such a structure can be risky if not carefully managed. The current presentation makes it challenging for readers to grasp the main content of the paper, the problem being addressed, and its practical significance.\n3. The experiments only include two video classification tasks with low-dimensional toy simulations. There is a lack of comparison with other theoretical assumptions and a lack of quantitative metrics showing the extent to which the original objective function is not minimized.\n4. In Proposition 2.1 and 2.2, \"in general\", \"in practice\", \"it is unclear whether...\" appear several times. The authors lack clarity on which cases are valid and which are exceptions.\n5. Authors must not assume that all readers are familiar with technical terms, so authors need to approach the introduction of their work from the general ML audience perspective."}, "questions": {"value": "1. In proving that P, Q is not a gradient field, the author uses the term \"in general\" several times, but does not explicitly state the specific conditions under which it applies. As a reader, it is difficult to judge whether there are exceptions to this conclusion. If there are such special cases, can you give an intuitive example or a theoretical explanation?\n2. The analysis in Chapter 3 assumes that both encoder and predictor are linear maps, which leads to conclusions about the asymptotic stability of the dynamical system under this condition. But in real networks are usually nonlinear. Then, can this stability analysis obtained in the linear case be extended to nonlinear models?\n3. Since α is assumed to be constant in the analysis, do the authors explore the impact of dynamic changes in α on stability or convergence?\n4. After all the proofs and discoveries, how can this be implemented technically (like a solution or new method the authors propose based on the observation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oS1kDnCCAQ", "forum": "f5MC1G6XhB", "replyto": "f5MC1G6XhB", "signatures": ["ICLR.cc/2026/Conference/Submission12157/Reviewer_dqoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12157/Reviewer_dqoh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629576112, "cdate": 1761629576112, "tmdate": 1762923111689, "mdate": 1762923111689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the role of two key components: stop gradient and Exponential Moving Average (EMA) for non-contrasitve from both optimization and dynamical systems perspectives. From the optimization perspective, they consider a general setting and prove that neither SG nor EMA can be interpreted as minimizing a well-defined smooth loss function, but they can avoid the representation collapse. From the dynamical systems perspective, they prove with SG and EMA that the dynamic system is asymptotically stable (assuming a linear setting). They also conduct real-world and simulation experiments to support their discussion."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* This work clearly shows the influence of SG and EMA from the theoretical perspective and conducts experiments to support their results."}, "weaknesses": {"value": "* It would be better to provide a proof sketch for Proposition 2.2, which makes it easier for readers to understand why with SG and EMA, the $\\bar{P}$ and $\\bar{Q}$ are not the gradient fields of any smooth function.\n\n* We know that the analysis for the nonlinear setting is hard. However, it would be better to discuss how to extend to a nonlinear NN (even a 2-layer Softmax NN)."}, "questions": {"value": "Question: \n\nQ1: Why does the proof of this work require weaker assumptions compared with Tian et al?\n\nMinor Comments: It would be better to use more formal writing, such as lines 247-248, and some experimental settings can be moved to the appendix. Then, there is more space for the proof sketch."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hfPrOwaZyk", "forum": "f5MC1G6XhB", "replyto": "f5MC1G6XhB", "signatures": ["ICLR.cc/2026/Conference/Submission12157/Reviewer_PrY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12157/Reviewer_PrY7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996854835, "cdate": 1761996854835, "tmdate": 1762923111315, "mdate": 1762923111315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}