{"id": "JVyPNQCT8E", "number": 1062, "cdate": 1756832251516, "mdate": 1759898230160, "content": {"title": "Exponential Low-Rank Adapters", "abstract": "Low rank adaptation (LoRA) is a standard parameter efficient fine tuning method, but its updates are rank limited and act as local additive perturbations of the weights.  We introduce exponential low rank adapters (ELRA), which replace LoRA’s additive update $\\Delta W=AB$ with a multiplicative transformation $W_{\\mathrm{new}}=\\exp(\\eta AB)\\,W_0$, where $A\\in\\mathbb{R}^{d\\times r}$ and $B\\in\\mathbb{R}^{r\\times d}$ define a low rank generator and $W_0$ is the frozen pretrained weight. The matrix exponential lifts the generator to a full rank, invertible map that acts coherently on $W_0$. Geometrically, ELRA traces curves on $GL(d)$; when the generator is normal, the path $t\\mapsto \\exp(tG)W_0$ is a constant speed, locally energy minimizing geodesic under the left invariant trace metric. This motivates ELRA\\text{-}PSD, which constrains $G$ to be symmetric positive semidefinite, stabilizing training by enforcing geodesic flows with an energy control on path length. To couple stability with expressivity, we propose ELRA-Hyb, which interpolates between PSD and general generators, starting stable and progressively unlocking capacity. Experiments on diverse language and vision benchmarks show that ELRA-Hyb consistently outperforms state of the art LoRA variants under matched parameter budgets.", "tldr": "", "keywords": ["LoRA", "LLM fine-tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0901547696c792950b09ccbaab9b88f066f2e2a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Exponential Low-Rank Adapters (ELRA), a PEFT method that addresses the inherent rank limitations of standard Low-Rank Adaptation (LoRA). Instead of LoRA's additive update (W_new = W_0 + AB), ELRA uses a multiplicative update with a matrix exponential: W_new = exp(G)W_0, where G = ηAB is a low-rank generator matrix. This lifts the low-rank generator to full-rank allowing for greater expressivity while maintaining a parameter budget comparable to LoRA. ELRA is comparable to previous PEFT algorithms that use a non-linear function applied on top of the PEFT weights to increase its rank: SinLoRA [1], LieRA [2] (concurrent work).\n\nThe paper introduces two variants:\n    - ELRA-PSD: Constrains the generator to be symmetric positive semi-definite (G = ηAA^T) to guarantee stable, geodesic updates.\n    - ELRA-Hyb: Interpolates between the stable ELRA-PSD and the more expressive general ELRA generator, aiming to combine initial training stability with high final expressivity.\n\nExperiments across natural language generation, understanding, vision, and reasoning tasks show that ELRA-Hyb outperforms a wide range of LoRA variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a theoretically grounded way to achieve full-rank updates from a low-parameter generator. The connection to the geometry of matrix Lie groups appears to be a good theoretical strength although I did not verify the validity of the mathematical derivations.\n\n    - The experimental validation is thorough. The authors evaluate their methods on a diverse set of models and tasks demonstrating the performance gains of ELRA-Hyb. \n\n   - The inclusion of geometric stability diagnostics (Figure 1) visually confirm the smoother training dynamics of ELRA-Hyb, reinforcing the theoretical motivation"}, "weaknesses": {"value": "Strong full rank PEFT performers are omitted in the main body of the manuscript. This includes SineLoRA [1] (highly related to ELRA), Krona [3] or RandLoRA [4] (the citation for which is wrong). These should be included as well as their multiplicative variants to reinforce the motivation for multiplicative updates and promote stronger experimental performance of exponential functions over a sin function (SineLoRA) and compared to SOTA full-rank methods (Krona, RandLoRA). I understand that not all methods can be compared with but the current SOTA selection appears weak. I appreciate the intuitive explanations in Appendix A but they don't hold much value compared to experimental results.\n\n[1] Ji, Yiping, et al. \"Efficient learning with sine-activated low-rank matrices.\"  International Conference on Learning Representations (ICLR) 2025.\n\n[2] Si, Chongjie, et al. \"Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group Transformations.\" International Conference on Computer Vision (ICCV) 2025\n\n[3] Edalati, Ali, et al. \"KronA: Parameter-Efficient Tuning with Kronecker Adapter.\" Enhancing LLM Performance: Efficacy, Fine-Tuning, and Inference Techniques. NeurIPSW 2025.\n\n[4] Albert, Paul, et al. \"RandLoRA: Full-rank parameter-efficient fine-tuning of large models.\" International Conference on Learning Representations (ICLR) 2025."}, "questions": {"value": "- Why is an exponential function preferable to a sin function as used in SineLora ?\n- Are multiplicative updates better for all kinds of PEFT strategies or only for exponential ones ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KDaoIHiSqY", "forum": "JVyPNQCT8E", "replyto": "JVyPNQCT8E", "signatures": ["ICLR.cc/2026/Conference/Submission1062/Reviewer_hbrv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1062/Reviewer_hbrv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761179540052, "cdate": 1761179540052, "tmdate": 1762915667641, "mdate": 1762915667641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for adapting language models to new tasks by using exponential low-rank adapters. Instead of the standard low-rank adapters (LoRAs) parameterized as $W_\\mathrm{new} = W_0 + AB$, the authors propose to use exponential low-rank adapters (ELRAs), which are defined by $W_\\mathrm{new} = \\mathrm{exp}(\\eta AB)W_0$. The authors then prove a series of propositions, showing that ELRAs:\n- have a $\\mathcal{O}(drk)$ forward-pass, \n- reproduce standard LoRAs to the first order,\n- perservse $W_0$ rank, and\n- admit a lower bound on the condition number of the adapted weight matrix.\n\nThen, the authors analyze the geometric properties of ELRAs, acting on $\\mathrm{GL}(d)$. They show that for a fixed $G = AB$, the ELRA path parametrized by $\\eta$ is a geodesic in $\\mathrm{GL}(d)$, given that $G$ is *normal*. Motivated by this, the authors propose a variant of the method, called ELRA-PSD, where $G$ is parameterized as $G = AA^\\top$, which is symmetric positive semidefinite and hence normal. Additionally, the authors propose a variant of the method, called ELRA-Hyb where $G$ is parameterized as $G = \\eta(\\alpha AA^\\top + (1-\\alpha) AB)$, with $\\alpha \\in [0, 1]$, decaying from 1 to 0 over the course of training, which is intedend to stabilize training while maintaining expressivity.\n\nFinally, the authors evaluate the proposed methods on a range of tasks, including natural language generation/understanding, question answering, and image classification. The results are compared against several LoRA variants and PEFT methods, showing consistent moderate improvements."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **(S1)** The base method proposed in the paper (ELRA) is simple, generic, efficient, and naturally extends LoRAs to the second-order.\n- **(S2)** All methods proposed in the paper are well-motivated from a theoretical standpoint and have several appealing properties, including preserving the rank of the weight matrix and having a lower bound on the condition number of the adapted weight matrix.\n- **(S3)** The experimental evaluation is broad and covers a variety of tasks, datasets, and models."}, "weaknesses": {"value": "- **(W1)** While the initial motivation for ELRA, detailed in Section 3.3 of the paper, is sound and appealing. The motivation for the geometric analysis, detailed in Section 4, is lacking. More specifically, the authors claim that the geodesic results that they prove (Proposition 6) imply a smoother and balanced traversal of weight space.\n  > unlike the additive update $W_0 + AB$, which can follow irregular trajectories, the exponential path evolves with uniform speed, ensuring smoother and more balanced traversal of the weight space.\n\n  But, this is only with respect to $\\eta$, not $AB$, and in all variants of the method, $G$ is varied as well. For ELRA-Hyb, the paper proposes an additional term for the loss, controlling the Frobenius norm of $G$: $\\mathcal{L}_\\mathrm{hyb} = \\mathcal{L}_\\mathrm{task} + \\lambda \\cdot \\mathrm{tr}(GG^\\top)$, but this can be done for any PEFT method. \n\n- **(W2, minor)** While improvements are consistent, they are often mild, and since the paper does not include confidence intervals or standard errors, it is hard to be sure of statistical significance."}, "questions": {"value": "- **(Q1)** The authors define $\\phi(Z) = \\sum_{n \\geq 1} \\frac{Z^n}{n!}$ and claim (after Proposition 1) that evaluating $\\phi(\\eta BA)$ is $\\mathcal{O}(d^3)$. Why is this the case? More generally, how do the authors compute $\\phi_1(G)$ in practice (the definition involves an infinite sum)? How do they backpropagate through it? I've had trouble accessing the attached code to check this myself.\n\n- **(Q2)** A large portion of the paper is dedicated to the analysis of the geometric properties of ELRAs, which are based on the fact that $\\eta \\mapsto \\mathrm{exp}(\\eta G)$ is a geodesic in $\\mathrm{GL}(d)$ for any normal matrix $G$. However, during training, $\\eta$ and $G$ are varied. How does this affect the geometric properties of the ELRA path? Other than adding a regularization term to the loss, is there a way to ensure that the ELRA path is a geodesic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DHBQyNNS7k", "forum": "JVyPNQCT8E", "replyto": "JVyPNQCT8E", "signatures": ["ICLR.cc/2026/Conference/Submission1062/Reviewer_xCWx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1062/Reviewer_xCWx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575657848, "cdate": 1761575657848, "tmdate": 1762915667520, "mdate": 1762915667520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper under review introduces exponential low rank adaptors (ELRA) within the context of low-rank fine tuning. This method applies a matrix exponential map to the low rank adaptors of LoRA thereby producing a full rank invertible matrix adaptor that can then be fine tuned. Since the exponential map defines locally energy minimizing geodesics under the left invariant trace metric the authors use this to develop ELRA-PSD, which constrains the generator of the exponential map to be symmetric positive semi-definite (PSD) and which the authors claim helps to stabilize training. They then introduce ELRA-Hyb which interpolates between PSD and general generators. On a variety language and vision benchmarks they show that ELRA-Hyb outperform previous fine tuning methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:** The papers idea of using an exponential map applied to the adaptors of low rank fine tuning to map the product adaptors into the Lie group if invertible matrices is new and I have not seen that done before. However, there have been many cases in the literature where techniques have been applied to the adaptors to yield a high-rank matrix adaptor that has shown to be better for fine tuning on a variety of benchmarks. One such example I found when doing a Google search that is pertinent to this work and uses a similar idea is the work \"Efficient Learning With Sine-Activated Low-rank Matrices\" ICLR 2025 by Ji et al., that applies a sinusoid function, with different frequencies, elementwise to the product of the adapters in LoRA. The authors show that this increases the rank of the adapters yielding to better fine tuning and in fact pre-training when low-rank matrices are used. One can see this current work of the authors as another way to do what Ji et al. did. Instead of using a function element-wise, the authors in this paper use a matrix function, namely the matrix exponential that maps from the Lie algebra of invertible matrices (i.e. the space of matrices) to the Lie group of invertible matrices. I was a bit surprised the authors didn't cite this work since it was published recently in ICLR 2025 and would have given the authors work some more context. \n\n**Novelty:** The idea of the paper is novel in that it gives a new way to produce a high rank adapter. The adapters during training are now invertible matrices. Furthermore, the authors show that by constraining the generator of the image of the exponential map to by symmetric positive definite they get better stability which is a nice approach."}, "weaknesses": {"value": "**Presentation:** I did feel that the presentation of the paper could be improved and positioned in a better way within the broader context of fine tuning. For example, in the second paragraph the authors mention the paper RandLoRA by Karimi et al., 2025. When I went to the references in the paper they cite Randlora: A random basis perspective on parameter-efficient fine-tuning. In ICLR 2025 by Karimi et al. However, I cannot find this paper anywhere. When I put that paper into a Google search it doesn't show. When I put it into Google Scholar it also does not show. Can you please provide a link to the paper? Instead I have managed to find the paper: RandLoRA: Full-rank parameter-efficient fine-tuning of large models by Albert et al., ICLR 2025. But this paper I found is not by Karimi et al., in fact it has none of the authors on the cited RandLoRA paper the authors put in their references for their citations which causes me to possibly think they have used an LLM to generate a citation? Please clear this up for me. On lines 61-65 you speak about the work LieRA by Si et al., and mention that it has a problem in that it limits global feature mixing. What do you mean by this? Could you please give a more detailed explanation of what you mean by that paragraph. In section 2, related work section, of the paper you mention works that are closest to your work. I don't think those works are the closest to your work. Upon reading those works and doing a Google search I found that the work  \"Efficient Learning With Sine-Activated Low-rank Matrices\" ICLR 2025 by Ji et al., was closer in spirit in part to what you are trying to do in your paper. Furthermore, the paper has several instances where it states things that are simply obvious. For example, Proposition 3 states that exp(G) is invertible which is a standard fact and can simply be referenced from a book for the proof. In fact that whole proposition is obvious and does not need to be stated. Similarly with Corollary 2. These are basic statements from Lie Group theory and can be just referenced from a standard text book. Similarly I noticed several times in the appendix you prove statements that are obvious. For example, on p.18 you actually prove equation (48) by going through equation (47) which is completely unnecessary. This is a standard fact that you can just reference.\n\n**Clarity:** This is related to the above comment on presentation. The authors have a tendency to over clarify statements in the paper either stating propositions/corollaries that are straightforward in Lie group theory and can be found in textbooks and prove things that can simply be referenced from a textbook. This gives the false sense that the authors have carried out an extensive mathematical analysis to support their work when in reality they are re-stating and re-proving statements that are already well known in Lie group theory.\n\n**Significance:** I don't think the paper will add much value to the already extensive collection of fine tuning methods present in the literature. While their experiments are extensive and probably the highlight of the paper I would say the theory is lacking and certainly over played to exhibit any real understanding of why taking the adaptors as invertible matrices is leading to such performance."}, "questions": {"value": "There are several issues I found with the presentation of the paper when I started to read it. For a start I have made a list of the following questions that I hope can clear up some things.\n\n1. Please could you provide a link to the reference: Randlora: A random basis perspective on parameter-efficient fine-tuning In ICLR 2025 by Karimi et al., that you cite in your paper, talk about in your introduction and also talk about on p. 15 of your paper where you explain why your method is better than Randlora by Karimi et al.\n\n2. Since you spoke about the work Randlora apparently by Karimi et al., why did you not compare against this approach in your experiments? On p. 15 of the paper you explain why your approach is better than Randlora by Karimi et al., yet you don't even compare against their method in your experiments. I found this very strange.\n\n3. In the experiments you mention (see Section 5) that for all methods the rank = 8 and that for ELRA-PSD the rank = 16. If applying an matrix exponential to the adaptors produces a full-rank matrix that is invertible then this learning is full-rank learning. So shouldn't you be able to use a lower rank say rank 4 and produce better results than the baselines using say rank 16? Even though it might mean your method has less parameters, it would give your method an even better advantage than the others in that it can also use less parameters.\n\n4. In the second paragraph of the introduction you mention (last sentence) \"how can we retain parameter efficiency while overcoming rank bottleneck\". Then in the third paragraph you say \"these limits point to a complementary strategy: keep parameter budget fixed and improve expressiveness by reparameterizing the update itself rather than increasing rank\". The work \"Efficient Learning With Sine-Activated Low-rank Matrices\" ICLR 2025 by Ji et al., actually shows how to keep the parameter budget, reparameterize the adaptors and increase rank. I found this paper when I was trying to google and find the paper RandLoRA. Did you compare against their work? \n\n5. On lines 61-65 you speak about the work LieRA by Si et al., and mention that it has a problem in that it limits global feature mixing. What do you mean by this? Could you please give a more detailed explanation of what you mean by that paragraph.\n\n6. In section 4.2.2 you mention that ELRA-Hyb interpolates between PSD and general forms starting with a stable geodesic flow and gradually allowing richer dynamics. During optimization how do you see this? The paper doesn't actually show that optimization is better by using ELRA-Hyb.\n\n7. Lines 266-290 made no sense to me. What are you trying to say here? Are you saying that optimization depends on the condition number and spectral collapse would be bad?\n\n8. I would imagine applying the matrix exponential to the adapters would increase the total number of FLOPs. Is this the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OVGcP3fJDV", "forum": "JVyPNQCT8E", "replyto": "JVyPNQCT8E", "signatures": ["ICLR.cc/2026/Conference/Submission1062/Reviewer_Fo85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1062/Reviewer_Fo85"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727172973, "cdate": 1761727172973, "tmdate": 1762915667339, "mdate": 1762915667339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aim to improve the expressive power of LoRA by replacing the additive update proposed by LoRA into a multiplicative transformation W_new = exp (eta A B) W_0. With exponential, exp(AB) could be full rank. Authors perform some mathematical analysis of the propose method ELRA, and come up with some practical variants ELRA-PSD and ELRA-Hyb for training stablization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written.\n    - The problem this paper aim to solve is important."}, "weaknesses": {"value": "- While the idea of making AB full rank is interesting, it’s unclear why this is necessary. With a fixed number of parameters, the overall degrees of freedom remain unchanged. Even if the update is full rank, the representational capacity is still constrained—so which parts of the space become newly accessible?\n- I would expect a more fine-grained analysis to clarify this point. For example, when approximating a full update with a low-rank one versus using exp⁡(ηAB)W0−W0, does the latter actually yield a better approximation? Some quantitative analysis, such as approximation error versus number of trainable parameters, would make the argument much stronger.\n- Overall, it remains unclear whether the proposed method provides a meaningful advantage over standard low-rank approximations. The experiment results seem good, but I wish this author need to carefully explain why the core idea works."}, "questions": {"value": "See the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kcSvALEC0v", "forum": "JVyPNQCT8E", "replyto": "JVyPNQCT8E", "signatures": ["ICLR.cc/2026/Conference/Submission1062/Reviewer_XNiy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1062/Reviewer_XNiy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976081290, "cdate": 1761976081290, "tmdate": 1762915667208, "mdate": 1762915667208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}