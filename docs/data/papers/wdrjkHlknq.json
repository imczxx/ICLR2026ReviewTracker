{"id": "wdrjkHlknq", "number": 7952, "cdate": 1758045581491, "mdate": 1759897820096, "content": {"title": "BPRL: A Behavioral Approach to State Representation in Reinforcement Learning", "abstract": "Deep Reinforcement Learning (DRL) often suffers from poor sample efficiency and limited generalization in complex, high-dimensional environments. A key challenge is designing effective state representations, which typically requires manual, domain-specific feature engineering. We propose Behavioral Programming Reinforcement Learning (BPRL), a framework that automates the construction of compact, semantically rich state representations. BPRL leverages Behavioral Programming (BP)$-$a scenario-based modeling paradigm$-$to specify the environment's dynamics. The core contribution is that the very same BP model used to define the environment’s logic is also used to $\\textit{automatically derive}$ the state representation for the DRL agent. This dual use of the BP model eliminates the need for manual feature design while ensuring that the extracted representations capture both high-level symbolic structure and temporal dependencies. By combining BP's modularity with structured observations, BPRL simplifies environment modeling and enhances agent learning. Experiments across multiple DRL algorithms on MiniGrid benchmarks demonstrate that BPRL substantially improves sample efficiency and asymptotic performance over standard baselines.", "tldr": "We introduce BPRL, a framework that automatically derives structured state representations from environment logic using Behavioral Programming to enhance reinforcement learning performance.", "keywords": ["Reinforcement Learning", "Behavioral Programming", "Symbolic Representation", "Representation Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/147b2d3327448a1e4f85508de115f77b7190c521.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present BPRL, a framework for integrating various tools from Behavioral Programming (BP) into Deep RL agents. The authors first present a method for distilling environments represented using BP into something usable by a learning agent by virtue of programmatically providing state observations that are not immediately derivable from the BP environment logic. The authors then compare agents trained on these observations to multiple baselines, including a set with image-based observations and additional “BP-strategies” that contain heuristics for environment solutions.\n\n1. **What is the specific question and/or problem tackled by the paper?**\n    \n    The problem of training DRL agents on environments specified by Behavioral Programs.\n    \n2. **Is the approach well motivated, including being well-placed in the literature?**\n    \n    The first part of the paper describing a method for deriving state observations from a BP environment is sensible, but I don’t believe the second half of the paper, on integrating BP-Strategies is well-placed in the literature.\n    \n3. **Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.**\n    \n    It’s unclear whether the results meaningfully and convincingly support the claims made in the paper. Specifically, there are issues with the third claim the authors make:\n    \n    > We empirically demonstrate that BPRL significantly improves learning efficiency and performance across multiple RL algorithms and environments, addressing key challenges in scaling RL to complex, high-dimensional tasks.\n    > \n    \n    IIUC, the authors made a slightly unusual choice to not do a parameter sweep over each agent configuration, so agents with potentially different-sized observation spaces will use the same hyperparameters. It is possible that agent performance would be different after a parameter sweep, which would contradict the claim made by the authors. Furthermore, it is not clear that “key challenges in scaling RL to complex, high-dimensional tasks” are genuinely addressed.\n    \n4. **What is the significance of the work? Does it contribute new knowledge and sufficient value to the community?**\n    \n    It potentially contributes new knowledge to the Behavioral Programming and BP-DRL community, though the value of its contribution is unclear."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The work is connected nicely to the field of Behavioral Programming, the wording and explanations of things are general clear and easy to follow, parts of the work are original (especially WRT deriving states from BP environment specifications)."}, "weaknesses": {"value": "The contribution of the work is only somewhat significant, the paper meanders between two different topics: getting state observations from BP environments and specifying advice using BP-strategies, the results are not particularly compelling or well-described/presented, and the work is poorly situated in the broader field of leveraging symbolic knowledge in DRL.\n\nThe paper is trying to do too many things at once, and does each of them poorly. There are significant methodological and interpretability issues, from making hyperparameter choices that may obfuscate results to making apples to oranges comparisons in reward tables. Finally, the work is extremely poorly situated in the broader RL community outside of the BP niche."}, "questions": {"value": "The differences in performance between various agents are quite opaque in table 1 especially for the PPO agents. It’s not immediately clear that there are substantial differences in performance. Furthermore the table ignores the temporal aspect of learning by merely reporting the final rewards achieved in environments.\n\nThere are potentially too many things in this table, I’m not sure it’s useful to compare all these things. The comparison between original observation, frame-stacked observation, and the BP-derived observation and BP-derived observation + original is sensible, since these are designed to essentially provide ground-truth information about the underlying world state. However, the inclusion of the two BP-strategies agent in this table is unusual. Comparing these agents means something very different, since the BP-strategies agent contains additional heuristics. It’s genuinely hard to make heads or tails of what is useful here to compare, it’s currently comparing apples to apples to oranges to guavas.\n\nExamples of the BP-Strategies (in full code) in the main paper would be useful. At the very least in the appendix, but there are none.\n\nThe discussion on Neuro-symbolic RL is seriously lacking, with only a two-sentence paragraph making vague reference to existing methods which integrate symbolic reasoning into DRL. There are numerous structured languages used to provide supplemental or total advice about decision processes, e.g. LTL, PDDL, RDDL, RLang, Policy Sketches (from Andreas et al. 2017) just to name a few, and there are even more works which leverage those languages in learning agents. This work is incomplete without making reference to both the languages used by the community and the works that integrate those languages into learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kww2Brg4JT", "forum": "wdrjkHlknq", "replyto": "wdrjkHlknq", "signatures": ["ICLR.cc/2026/Conference/Submission7952/Reviewer_V7yu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7952/Reviewer_V7yu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270469040, "cdate": 1761270469040, "tmdate": 1762919970066, "mdate": 1762919970066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel framework for modeling environment dynamics in reinforcement learning (RL), centered around the use of Behavioral Programming (BP). BP is a paradigm for constructing reactive systems by specifying behavioral threads that define what the system may, must, or must not do. This approach enables a modular, rule-based encoding of the environment's behavior.\n\nA key contribution of the work is an automated method for deriving a structured state representation of the environment. As I understand it, each behavioral rule is compiled into a deterministic automaton (called LTS), and the overall environment state is formed by concatenating the current states of these individual automatons.\n\nIn addition to modeling the environment, the paper demonstrates how BP can be used to inject prior knowledge into the RL agent. These BP rules do not need to fully specify the environment's dynamics; instead, they can encode partial knowledge that the designer considers useful for the agent. Each rule is again compiled into an automaton, and the agent's state is augmented with the current state of these automatons, effectively integrating domain knowledge into the learning process.\n\nThe framework is evaluated in the MiniGrid environment, where the features derived from the BP-based representation outperform the standard baseline features, which are typically based on the agent’s raw RGB observations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This is a clear and well-written paper that successfully bridges reinforcement learning (RL) and the Behavioral Programming (BP) literature. The work is technically sound and introduces a novel approach to encoding environment dynamics in RL. While the effectiveness of BP as a general-purpose tool for modeling such dynamics may be open to debate, the idea of using a formal language from which state representations can be automatically extracted is promising. This could be particularly beneficial for new RL practitioners, who often struggle to define a suitable Markovian state space for their problems. In the long term, I see potential in learning behavioral threads (b-threads) from experience, which could help RL agents better manage partial observability and improve generalization."}, "weaknesses": {"value": "**Critical Comments**\n\nThe paper does not discuss why the BP-derived observations outperform the original observations in MiniGrid. I suspect the core reason is that BP-based observations contain _privileged information_ that the agent ideally should not have access to. A key aspect of MiniGrid is its partial observability: the agent can only perceive its immediate surroundings, which makes it a realistic benchmark for RL agents operating under uncertainty.\n\nBy contrast, BP-derived observations appear to remove this constraint. Although the paper does not explicitly detail what information is included in the BP-based state representation, the method's description suggests it likely includes comprehensive environment details—such as the agent’s exact location, the status of doors (open or closed), and the positions of keys outside the agent’s field of view. This effectively transforms the problem into a fully observable one, making the task significantly easier and less realistic. For example, a robot trained in simulation using BP-derived observations would not have access to such privileged information when deployed in the real world.\n\nThis is a crucial point that is currently missing from the paper and deserves further discussion.\n\n---\n\n**Conceptual Concerns**\n\nThe paper also does not address a core challenge in RL. Once the full environment dynamics are programmed, deriving a proper state representation is relatively straightforward—any Markovian state would suffice. The only scenario in which this becomes difficult is when the designer lacks a basic understanding of RL principles.\n\nOne could argue that some Markovian states are more informative than others, and perhaps BP-derived features are optimal in some sense. However, the paper provides no evidence to support this claim. The only comparison made is between BP-derived features (which are Markovian) and raw RGB observations (which are intentionally non-Markovian to preserve realism in MiniGrid). In such a comparison, it is unsurprising that the Markovian features perform better.\n\nA more compelling evaluation would compare BP-derived features against alternative Markovian representations, or formally demonstrate that BP-derived features are superior in general.\n\n---\n\n**Minor Concerns**\n\n- **Section 3.1** states: \"_As is standard in DRL, manual implementation of the environment is a prerequisite_.\" This is not universally true. Sometimes, RL agents learn directly from interaction with the physical environment. Moreover, in most simulation-based research, the environment is already implemented—whether it's a physics engine or a video game—so manual implementation is not always required.\n\n- **Limitations of BPRL**: The paper does not discuss the limitations of the proposed approach. In particular, BPRL may not be suitable for modeling continuous domains, which are common in robotics and other impactful RL applications. Since BP relies on labeled transition systems (LTS), representing continuous variables would require an infinite number of states. Although the paper briefly mentions extending BPRL to continuous domains as future work, it does not address the implications or challenges of doing so.\n\n- **Formal Languages in Reinforcement Learning**: Consider including a discussion of prior work that has incorporated formal languages into reinforcement learning beyond BP [e.g., 1-6].\n\n[1] Littman, M. L., Topcu, U., Fu, J., Isbell, C., Wen, M., & MacGlashan, J. (2017). Environment-independent task specifications via GLTL. arXiv preprint arXiv:1704.04341.\n\n[2] Icarte, R. T., Klassen, T., Valenzano, R., & McIlraith, S. (2018). Using reward machines for high-level task specification and decomposition in reinforcement learning. In International Conference on Machine Learning (pp. 2107-2116). PMLR.\n\n[3] De Giacomo, G., Iocchi, L., Favorito, M., & Patrizi, F. (2018). Reinforcement learning for LTLf/LDLf goals. arXiv preprint arXiv:1807.06333.\n\n[4] Jothimurugan, K., Alur, R., & Bastani, O. (2019). A composable specification language for reinforcement learning tasks. Advances in Neural Information Processing Systems, 32.\n\n[5] Vaezipoor, P., Li, A. C., Icarte, R. A. T., & Mcilraith, S. A. (2021). Ltl2action: Generalizing ltl instructions for multi-task rl. In International Conference on Machine Learning (pp. 10497-10508). PMLR.\n\n[6] Yalcinkaya, B., Lauffer, N., Vazquez-Chanlatte, M., & Seshia, S. (2024). Compositional automata embeddings for goal-conditioned reinforcement learning. Advances in Neural Information Processing Systems, 37, 72933-72963."}, "questions": {"value": "1. What specific information is included in the BP-derived observations, and why do these features lead to better performance compared to the original MiniGrid features? \n\n2. Do the BP-derived observations guarantee Markovian properties with respect to the environment's transition dynamics?\n\n3. Is it feasible to extend the BPRL framework to continuous domains? Given that Behavioral Programming relies on discrete labeled transition systems, it is unclear how it would handle continuous variables without requiring an infinite number of states. Could the authors elaborate on the challenges and potential solutions?\n\n4. Are the BP-derived features optimal in any formal or empirical sense? Has any analysis been conducted to evaluate whether these features offer advantages over other Markovian representations, either theoretically or through comparative experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vp2kKW6nVm", "forum": "wdrjkHlknq", "replyto": "wdrjkHlknq", "signatures": ["ICLR.cc/2026/Conference/Submission7952/Reviewer_Jpnj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7952/Reviewer_Jpnj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512558785, "cdate": 1761512558785, "tmdate": 1762919969561, "mdate": 1762919969561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "## On Full Observability in MiniGrid\nA concern was raised that our BPRL-derived observation ($o_{BP}$) provides an unfair advantage. We must clarify that this is a misunderstanding of our baseline. As defined in our Baselines (Section 4.1), the 'Original' observation is *not* the standard partially-observable agent view. Instead, to provide a much stronger baseline, we use the **fully observable** $H \\times W \\times 3$ grid, which encodes the entire environment's contents, not as RGB but as a more high-level representation (object type, color, and state). \n\nThus, both our BPRL agent and our baseline agent receive the *exact same* global information. The superior performance of BPRL (seen in Figure 3 and Tables 1 and 2) is therefore not due to receiving more information, but from receiving a superior **representation** of that same information. using the baseline, the agent must learn symbolic concepts from a raw grid tensor (not an RGB-like representation, but a high-level one), whereas with our representation, the agent receives the compact, symbolic state ($o_{BP}$) derived automatically from the environment's formal specification (Section 3.2).\n\n## Partial observability and continuous domains\n\nReviewers raised valid concerns about extensibility to partially observable (PO) and continuous domains, which we explicitly identify as key directions for future work (Section 6). \n\nFor **visual-based PO domains** (e.g., using raw pixels), our \"multi-modal state representation'' (Line 53) and **dual-branch network architecture** (Section 4.2, Appendix B) are designed to fuse raw visual input (via a CNN) with our symbolic $o_{BP}$ vector (via an FCN). Our BP observation, along with original experiments, validates this multimodal concept. Handling partial observability at the **behavioral level** is a more complex, non-straightforward research challenge, as it requires the BP model to reflect limited information, and is a key part of our future work.\n\nRegarding **continuous control**, we must distinguish our *experiments* (MiniGrid) from the *formalism*. The BP formalism is mature and well-established in continuous domains, with published work demonstrating its use in controlling physical robotic platforms and its integration with SMT solvers (like Z3) to manage real-valued variables and complex numerical constraints (relevant papers are in our related work section 5.2). Extending our automatic state derivation method to these domains is a primary goal of our future work (Section 6).\n\n## On Scalability via Non-Intrusive Extensibility\nA key feature of the BP formalism that enables scalability is \"non-intrusive extensibility''. This is a dominant feature of BP that allows system specifications to scale. We provide a concrete example of this in our paper (Section~3.1 and Listings 1--2). We first build the MiniGrid-Empty environment using a modular set of b-threads (wall, goal, etc.). To create the far more complex MiniGrid-DoorKey environment, we do *not* refactor or modify the existing code; we **incrementally add** new, independent b-threads (e.g., *door_unlock_with_key* b-thread). This ability to add complex behaviors without altering existing modules (e.g non-intrusively) is the core of BP's scalability. This translates directly to a scalable representation: when a new b-thread is added to the environment model, our BPRL framework automatically expands the state vector to include this new behavior."}, "title": {"value": "Answering Shared Concerns Of The Reviewers"}}, "id": "qzEYHljUw9", "forum": "wdrjkHlknq", "replyto": "wdrjkHlknq", "signatures": ["ICLR.cc/2026/Conference/Submission7952/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7952/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7952/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763656357680, "cdate": 1763656357680, "tmdate": 1763656568423, "mdate": 1763656568423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on state representation learning for deep reinforcement learning. It identifies a key challenge in existing DRL methods: the lack of compact, time-aware, semantically structured state representations. To tackle these problems, this paper proposes BPRL, which leverages Behavioral Programming to implement environments and automatically derive a state representation for RL. Each behavioral thread is treated as a labeled transition system; at every step, the set of current synchronization states across threads is encoded and concatenated into a fixed-size representation. This representation can be fused with raw inputs in a dual-branch policy network. Experiments on MiniGrid demonstrate that BPRL accelerates learning, improves final returns, and enhances sample efficiency over non-BP baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow, presenting clear code listings and a step-by-step construction of b-threads that make the proposed method concrete and reproducible.\n\n2. Similar to rule-based approaches, the behavioral programming method in this paper offers greater interpretability than learning-based state representation methods.\n\n3. This paper also demonstrates the effectiveness of the method, achieving substantially higher sample efficiency, as shown in Figure 3."}, "weaknesses": {"value": "1. My primary concern lies in the limited scalability and applicability of this paper. The BP-based methods, such as the b-threads illustrated in Listings 1 and 2, appear to rely on prior knowledge of the environment’s operational rules and still have the need for manual feature engineering. Consequently, it may be difficult to extend these methods to more complex or unknown environments. In my view, the b-threads resemble logic-based rules that function as language-like instructions. While this design enhances interpretability, it does not embody a learning-driven approach and therefore struggles to generalize to diverse benchmarks where agents lack prior knowledge of the environments.\n\n2. Moreover, this paper do not compare with other popular learning-based state representations such as bisimulation metric and BP-based methods mentioned in the related work. And all experiments are only conducted on the MiniGrid benchmark, with the original observation being a fully observable grid. This leaves open the method’s scalability to richer visual inputs, partial observability, and continuous-control benchmarks.\n\n3. The overall quality of writing is relatively weak. For example, the term non-intrusive extensibility mentioned in Line 63 is unclear and requires further explanation. Similarly, the repeated use of modular and incremental is not well defined or adequately elaborated within the text."}, "questions": {"value": "1. When the rules are unclear or relatively complex, how should we write b-threads? For example, in continuous control (e.g., DMControl or MuJoCo), how do we map continuous trajectories and actions into discrete events and how to design synchronization points?\n\n2. BP code seems to be environment-specific. Is there a more automated and generalizable way to write BP code? For example, Section 3.1 mentions that LLMs can translate requirements into BP code, could you provide a concrete workflow?\n\n3. What exactly does the term multi-modal state representation in Line 53 refer to? Does the author provide any corresponding experiments or empirical results to validate this concept?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AyvCbNjF6m", "forum": "wdrjkHlknq", "replyto": "wdrjkHlknq", "signatures": ["ICLR.cc/2026/Conference/Submission7952/Reviewer_RXZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7952/Reviewer_RXZR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795936812, "cdate": 1761795936812, "tmdate": 1762919969078, "mdate": 1762919969078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the use of Behavioral Programming (BP), a scenario-based modeling paradigm, for constructing structured environment representations in Deep Reinforcement Learning (DRL). The key idea is that the same BP model that specifies the environment can also provide semantically rich state representations for RL agents. The authors test this idea on MiniGrid benchmarks, comparing PPO and A2C agents trained on raw, frame-stacked, and BP-derived observations. They report that BP-based representations yield higher sample efficiency and faster convergence.\n\nThe paper is well-intentioned and attempts to connect the formal methods community with DRL by introducing structured modeling principles into representation learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of leveraging a formal modeling framework (Behavioral Programming) to generate structured representations for DRL opens an interesting interdisciplinary direction.\n- Experiments show meaningful gains in sample efficiency and learning speed, suggesting that structured representations indeed help DRL performance.\n- The paper correctly highlights the mismatch between real-world structured systems and unstructured end-to-end learning, motivating the need for scenario-based abstractions.\n- BP’s event-based structure could provide interpretability benefits, though this aspect is not deeply explored."}, "weaknesses": {"value": "- The paper does not adequately justify why BP is a particularly suitable modeling formalism for RL. Other structured paradigms such as Hierarchical RL, Options, Programmatic RL, or Recursive RL address similar goals, yet the paper neither contrasts nor situates BP within this landscape. This makes the contribution feel incremental or somewhat arbitrary.\n- The technical section assumes significant prior knowledge of BP concepts (e.g., b-threads, synchronization points, request/block/wait semantics). Without a concise self-contained introduction, the paper risks alienating much of the ICLR audience unfamiliar with this formalism.\n- It is not well explained what the RL agent is optimizing in a BP-based environment (reward modeling), or how stochasticity and nondeterminism in BP models are handled.\n- All experiments are conducted on MiniGrid, a relatively simple benchmark. It is not clear whether the evaluated environments involved any stochasticity in their transition dynamics or were entirely deterministic. \n- The paper could be strengthened by an analytical discussion of *why* BP-based representations help (e.g., by measuring reduction in state entropy, improved temporal abstraction, or compositional generalization). Without this, the results, though promising, remain somewhat anecdotal."}, "questions": {"value": "- How does the system handle stochastic environments or partial observability?\n- Could BP representations be automatically inferred, or are they manually designed?\n- What are the expressiveness or scalability limits of BP compared to hierarchical, programmatic, or recurisve RL?\n- Can the proposed method generalize beyond MiniGrid to more realistic or continuous-control settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tc8t6UvE1H", "forum": "wdrjkHlknq", "replyto": "wdrjkHlknq", "signatures": ["ICLR.cc/2026/Conference/Submission7952/Reviewer_PEux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7952/Reviewer_PEux"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150342665, "cdate": 1762150342665, "tmdate": 1762919968634, "mdate": 1762919968634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}