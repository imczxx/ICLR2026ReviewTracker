{"id": "JEGDp1E4OH", "number": 13995, "cdate": 1758226610400, "mdate": 1763682377034, "content": {"title": "MC-Search: Evaluating and Enhancing Multimodal Agentic Search with Structured Long Reasoning Chains", "abstract": "With the increasing demand for step-wise, cross-modal, and knowledge-grounded reasoning, multimodal large language models (MLLMs) are evolving beyond the traditional fixed retrieve-then-generate paradigm toward more sophisticated agentic multimodal retrieval-augmented generation (MM-RAG). Existing benchmarks, however, mainly focus on simplified QA with short retrieval chains, leaving adaptive planning and multimodal reasoning underexplored. We present MC-Search, the first benchmark for agentic MM-RAG with long, step-wise annotated reasoning chains spanning five representative reasoning structures. Each example specifies sub-questions, retrieval modalities, supporting facts, and intermediate answers, with fidelity ensured by HAVE (Hop-wise Attribution and Verification of Evidence), resulting in 3,333 high-quality examples averaging 3.7 hops. Beyond answer accuracy, MC-Search introduces new process-level metrics for reasoning quality, stepwise retrieval and planning accuracy. By developing a unified agentic MM-RAG pipeline, we benchmark six leading MLLMs and reveal systematic issues such as over- and under-retrieval and modality-misaligned planning. Finally, we introduce Search-Align, a process-supervised fine-tuning framework leveraging verified reasoning chains, showing that our data not only enables faithful evaluation but also improves planning and retrieval fidelity in open-source MLLMs.", "tldr": "", "keywords": ["Multimodal", "RAG", "Vision-Language", "Agent", "Benchmark"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f5c29454f70093a1079616b51c71736338ecd80.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MC-SEARCH, a comprehensive benchmark designed to evaluate and advance agentic multimodal retrieval-augmented generation (MM-RAG) systems, particularly focusing on structured, stepwise long-horizon reasoning. MC-SEARCH comprises 3,333 high-quality, HAVE-verified examples encompassing five distinct multi-hop reasoning topologies (including serial, parallel, and cross-modal forks), each annotated with granular sub-questions, modalities, evidence, and intermediate answers. The paper also proposes novel process-level evaluation metrics that go beyond traditional answer accuracy, enabling fine-grained analysis of stepwise retrieval, planning, and reasoning. Extensive benchmarking is conducted on six leading MLLMs, and a process-supervised fine-tuning scheme (SEARCH-ALIGN) leveraging the new dataset is presented, showing notable improvements for open-source models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Rigorous Dataset Construction:** MC-SEARCH addresses a clear gap in existing multimodal RAG benchmarks by providing long, structured, and verified reasoning chains spanning five rich reasoning topologies. The dataset construction meticulously filters for non-redundancy and necessity at each reasoning step using the HAVE protocol, resulting in high annotation quality\n\n**Process-Level Evaluation:** The paper moves beyond answer-level benchmarking by introducing stepwise “Hit per Step,” Rollout Deviation, and LLM-as-a-Judge metrics, directly quantifying the chain of reasoning, retrieval fidelity, and error types. This is a step change from prior black-box evaluation paradigms.\n\nNew Method: They propose SEARCH-ALIGN, a process-supervised fine-tuning framework for MLLMs that leverages verified reasoning chains to align model behavior"}, "weaknesses": {"value": "- Although the model coverage is broad, several open-source baselines used are not state-of-the-art for their respective modalities, which may overstate SEARCH-ALIGN’s improvements. The paper does not sufficiently justify the exclusion of other competitive open-source MLLMs or recent RAG architectures such as UniRAG and MIRAGE from direct comparison.\n\n- While Table 3 highlights clear performance gains from SEARCH-ALIGN, the contribution of individual components—planning, retrieval, and modality selection—is analyzed only qualitatively. A more rigorous ablation is needed to determine whether the model genuinely learns better planning strategies or simply imitates supervised reasoning steps.\n\n- Given the use of LLMs for data construction, filtering, and evaluation, to what extent might there be annotation artifacts or overfitting to the verifier’s error modes? How do the authors ensure the robustness of results in light of potential circularity?"}, "questions": {"value": "- I’m a bit confused about why the reasoning process is represented as a graph rather than a chain.\n\n- Why didn’t you include ROUGE and MRFS scores [1] in your evaluation?\n\n- Why didn’t you incorporate retrieval-related metrics in your benchmark analysis?\n\n- How does your method Search-ALign perform compared to other RL-based agentic optimization approaches?\n\n\n\n[1] Pan Z, Luo H, Li M, et al. Chain-of-action: Faithful and multimodal question answering through large language models[J]. arXiv preprint arXiv:2403.17359, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CQMK91eLqE", "forum": "JEGDp1E4OH", "replyto": "JEGDp1E4OH", "signatures": ["ICLR.cc/2026/Conference/Submission13995/Reviewer_mT9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13995/Reviewer_mT9g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760457633044, "cdate": 1760457633044, "tmdate": 1762924491824, "mdate": 1762924491824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MC-SEARCH, a new benchmark for evaluating multimodal agentic Retrieval-Augmented Generation (MM-RAG). The authors argue that existing benchmarks are too simple, focusing on short reasoning tasks. To address this, they make four main contributions: \n\n1. The MC-SEARCH benchmark, a dataset of 3,333 examples featuring long reasoning chains (avg. 3.7 hops) organized into five distinct reasoning topologies. \n\n2. A data filtering process called HAVE (Hop-wise Attribution and Verification of Evidence) to ensure each reasoning step is necessary and non-redundant. \n\n3. New process-level metrics (e.g., Hit per Step, Rollout Deviation) to evaluate intermediate reasoning steps, not just the final answer. \n\n4. The SEARCH-ALIGN framework, a process-supervised fine-tuning method that uses the benchmark's annotations to improve the capabilities of open-source models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strengths lie in its thoughtful benchmark design and its focus on process-level evaluation.\n\nStructured Benchmark Design: The introduction of five explicit reasoning topologies is a significant contribution. It moves beyond simply creating \"long\" chains and provides a structured way to diagnose specific model failures (e.g., a model failing on Parallel Forks but succeeding on Linear Chains). This offers a more granular analysis than existing benchmarks.\n\nRigorous Data Curation: The HAVE process is a commendable effort to improve benchmark quality. By programmatically filtering out trivial or redundant reasoning steps, the authors have likely created a more challenging and reliable testbed for genuine reasoning abilities, addressing a common weakness in synthetically generated datasets.\n\nProcess-Oriented Evaluation: The push for metrics beyond final answer accuracy is timely. In complex agentic tasks, intermediate failures are often hidden. Metrics like Hit per Step (HPS) and Rollout Deviation (RD) provide a clearer view of how and where models fail, which is critical for future development.\n\nDemonstrated Training Utility: The inclusion of the SEARCH-ALIGN framework is a strong point. By showing that the detailed annotations can be used for process-supervised fine-tuning to improve open-source models, the authors prove that MC-SEARCH is a valuable resource for both evaluation and model development."}, "weaknesses": {"value": "The paper's contributions are undermined by significant weaknesses, primarily an overstatement of novelty and key methodological limitations.\n\nNovelty overstatement — The paper's central claim of being the \"first benchmark for agentic MM-RAG with long, step-wise annotated reasoning chains\" is not well-supported. Prior work such as Dyn-VQA benchmark was specifically designed for dynamic, multi-hop questions requiring complex, adaptive retrieval and also introduced a self-adaptive planning agent. The paper must reposition its contribution not as being the first, but as providing uniquely structured and verified reasoning chains, and it needs to properly differentiate itself from Dyn-VQA and other related works like WebQA, MRAG-Bench, and MMSearch.\n\nPotential model bias — While the HAVE pipeline partially mitigates bias via cross-model filtering (Qwen2.5-VL and Gemini-Pro), Gemini models still dominate generation, filtering, and evaluation phases. This may tune the benchmark toward Gemini’s reasoning style.\n\nSimplified retrieval setting — The evaluation uses a top-1 retrieval setup, which does not reflect realistic RAG conditions where irrelevant documents co-exist. The conclusions regarding over/under-retrieval and SEARCH-ALIGN efficacy may not fully generalize to top-k retrieval.\n\nLimited justification for reasoning topologies — Although the five topologies are intuitive, the paper does not provide empirical or theoretical justification for their selection (e.g., frequency in real-world tasks). Including such evidence would improve the framework’s validity.\n\nMetric rigidity — The Hit per Step (HPS) metric relies on exact evidence matching, which could penalize models finding semantically equivalent alternatives. The authors do mention a semantic alignment mechanism for structural comparison, but an integrated soft-matching variant could make evaluation fairer."}, "questions": {"value": "How does MC-SEARCH differ empirically and conceptually from Dyn-VQA and MMSearch beyond having predefined topologies? Could the authors quantify the added diagnostic value of these structures?\n\nHow do the authors ensure benchmark neutrality given that Gemini models are used for both generation and evaluation? Have they tested whether non-Gemini models (e.g., GPT-4o, Claude, QwenVL) are unfairly disadvantaged?\n\nHow might the conclusions about SEARCH-ALIGN change under a top-k retrieval setting where irrelevant evidence must be filtered dynamically?\n\nWhat criteria guided the selection of the five reasoning topologies? Are these empirically grounded (e.g., observed in task distributions) or designed heuristically?\n\nCould HPS be complemented by a semantic similarity–based variant, ensuring agents that retrieve equivalent but non-identical evidence are not penalized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CM3VCRAG1e", "forum": "JEGDp1E4OH", "replyto": "JEGDp1E4OH", "signatures": ["ICLR.cc/2026/Conference/Submission13995/Reviewer_YFCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13995/Reviewer_YFCi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906601275, "cdate": 1761906601275, "tmdate": 1762924491427, "mdate": 1762924491427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a multimodal retrieval-augmented generation benchmark for agentic paradigm. It has 3333 examples averaging 3.7 hops with sub-questions, retrieval modalities, supporting facts, and intermediate answers ensured by HAVE. It also proposes some process-level metrics to judge the reasoning quality and retrieval performance and planning fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. this submission is well-prepared, especially in figures, tables, and appendix.\n\n2. the key contribution of this submission is obvious and makes sense.\n\n3. the process metrics are actually needed things in multi-step reasoning tasks. \n\n4. the experiments and analysis are comprehensive and high-quality."}, "weaknesses": {"value": "Many related work may help authors to enhance the completeness of the submission:\n\n1. evaluation on robustness of resisting harmful information is also interesting in RAG-based agentic framework [1].\n\n2. \"multi-modality\" may also extend to SQL-based database, query-rewriter-based web-search, and even more [2].\n\n3. token usage (input and output) and the number of retrieval callings are also helpful to enhance the benchmark [3].\n\n\n\n[1] Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain\n\n[2] Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models\n\n[3] RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems"}, "questions": {"value": "1. how to evaluate and ensure that Qwen2.5-VL-7B is good at judging each hop is both necessary and non-redundant? is there any mannual double-check and fine-tuning methods or?\n\n2. please refer to weakness section for more contents."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DdKpgBP5Ge", "forum": "JEGDp1E4OH", "replyto": "JEGDp1E4OH", "signatures": ["ICLR.cc/2026/Conference/Submission13995/Reviewer_nveM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13995/Reviewer_nveM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989141650, "cdate": 1761989141650, "tmdate": 1762924490855, "mdate": 1762924490855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "While I appreciate the clear problem formulation and insightful process-level metrics, I believe the paper requires several revisions. My primary concerns are the questionable novelty of the pipeline (Sec.3) and the contradiction between the claim of a \"long\" benchmark and its actual length. Additionally, crucial methodological details regarding data filtering, utility calculation, and the justification for your similarity metrics are insufficiently explained."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors clearly identify a critical gap in existing research: the lack of benchmarks for evaluating long-chain, structured, and agentic multimodal reasoning. Current benchmarks are often limited to 1-2 hop retrievals, which is insufficient for testing advanced agentic capabilities. The paper moves beyond simple final-answer accuracy by introducing insightful process-level metrics like Hit per Step (HPS) and Rollout Deviation (RD)."}, "weaknesses": {"value": "1. Novelty of Sec.3.1. Could the authors elaborate on the novelty of \"Agentic MM-RAG Pipeline\" compared to [OmniSearch (Sec.4.2)](https://arxiv.org/pdf/2411.02937)?\n2. The authors claim to introduce the \"first benchmark with **long**, step-wise annotated instructions.(Line101)\" However, the provided average length of 3.7 steps seems to contradict the descriptor \"long.\"\n3. Line180, Equation 2. The calculation of $Util(t)$ will become expensive if $T$ is large or the base model is large. (Did I miss some clever way to efficiently calculate $Util(t)$?)\n4. Line197: The logic of marking a sample as *redundant* seems not rigorous enough. E.g., if a redundant and useless entity appears twice in a trajectory, which is likely when models make a mistake, the data will still be marked *not redundant* because $Nav(t)$ equals 1.\n5. Need more elaboration on Data Filtering (Line175). What happens if a step is marked as redundant? Is it removed directly? How do you handle the connection of context of this removed step? Is direct concatenation likely to cause incoherent logic?"}, "questions": {"value": "1. Eq.4 How do you define the equality of $\\hat{r}_{t'}$ and $r_{t}$? (E.g., exact match?) The evidence could be sequences of tokens.\n\n2. Line233 The paper describes each reasoning graph as \"a sequence of retrieval-augmented reasoning steps indexed by t.\" This phrasing suggests a strictly linear, sequential structure, essentially a reasoning chain. However, the term graph typically implies the possibility of branching, merging, or more general topological structures beyond a simple sequence.\n    - Could the authors clarify whether the \"reasoning graphs\" in this work are always linear chains (i.e., sequences), or whether there are scenarios in which they exhibit genuine graph-like structures (e.g., multiple predecessors/successors for a step, parallel reasoning paths, or dynamic step dependencies)?\n    - If all reasoning graphs are indeed sequences, would it be more precise to refer to them as reasoning chains to avoid potential confusion? Conversely, if non-linear graph structures do arise, please elaborate on (1) under what conditions such structures occur, and (2) how the model's inference procedure generates them.\n\n3. Could you please elaborate on why applying *maximum-weight bipartite matching* is a good enough way to model the similarity of two trajectory graph."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2qgxQjuRcr", "forum": "JEGDp1E4OH", "replyto": "JEGDp1E4OH", "signatures": ["ICLR.cc/2026/Conference/Submission13995/Reviewer_HAWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13995/Reviewer_HAWQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051270531, "cdate": 1762051270531, "tmdate": 1763003556004, "mdate": 1763003556004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}