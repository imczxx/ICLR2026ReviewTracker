{"id": "RlPVSeKjoc", "number": 1735, "cdate": 1756913179390, "mdate": 1759898191129, "content": {"title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embedding", "abstract": "So far, expensive finetuning beyond the pretraining sequence length has been a prerequisite to effectively extend the context of language models (LM). In this work, we break this key bottleneck by ***Dro**pping the **P**ositional **E**mbeddings of LMs after training (DroPE)*. Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely *removed after pretraining* following a short recalibration phase. Empirically, DroPE yields seamless *zero-shot* context extension *without any long-context finetuning*, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary position embedding scaling methods.", "tldr": "We extend the context of pretrained LMs by dropping their positional embeddings after training.", "keywords": ["LMs", "Long Context", "Positional Embeddings", "Architecture"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2f93791fad0fe34d194ccd61d426416e9adde34.pdf", "supplementary_material": "/attachment/79763d84373105fbf59273fac15109656c41e472.zip"}, "replies": [{"content": {"summary": {"value": "The paper challenges the conventional methodology by bravely removing positional encoding at serving time (NoPE), but the model had been initially trained using RoPE and recalibrated with continuous pretraining in a later phase without positional encoding. \n\nFor the background, it is well-known that NoPE\n1. can learn positions of tokens due to causal mechanisms in attention;\n2. but underperforms in terms of negative log-likelihood when comparing with most existing positional encodings.\nAlthough NoPE is known to have good generalization properties in terms of context length, it is generally dismissed as the widespreading intuition is that the positional information is \"hard to learn\" or even \"lost\". \n\nThe paper provides an explanation of why NoPE has difficulties in learning positional information. On the other hand, it also provides some observations to explain why RoPE, though learning the positional information fast, has trouble with length generalizations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. Simplicity.\n2. The idea is fairly novel.\n3. Results are fairly compelling with good theoretic analysis to back them up.\n4. Potential impact. I personally think this inspires the search of better positional encoding by not only looking at architecture manipulations, but also the training process."}, "weaknesses": {"value": "- One of the biggest concern to widely adopt this in LLM training is a rigorous study to confirm potential risks. There are two aspects that I'm particular concerned, despite the good numbers on academic benchmarks\n  1. The loss spike and cooling down, does it hurt further learning dynamics? What does the spike do to other layers' parameters, parameter/grad norms, etc. Would it make further SFT different?\n  2. This is less important as to a \"weakness\", but how does it fare on other more up-to-date LLM evaluations? This might be relevant if we want to understand if it is genuinely without regression on all tasks, or whether there are some issues that we need to understand.\n- There seems to be a lack of experimental rigorousness. For this I still very much want to understand all these comparison setups, what are held as control variables and how the confounders are ruled out. Also there seems to be a sneaky modification of architecture in Appendix C.1, even though they \"look minor\", introducing unwanted variable makes experiments shaky. See my questions below."}, "questions": {"value": "It's possible that I miss certain things already in the appendix, but below are the questions I want to understand better.\n\n- I don't understand Figure 9 and Table 4: are you comparing the original SmolLM with SmolLM + extra 30/60/120B token's finetuning? Or did you match an equi-token setup and further finetune SmolLM for the same amount of tokens? If the former, it is perhaps not entirely fair and it still bears the question as to how much the original model would trend higher given the additional training. Also, do you have certain measurement of confidence interval on those eval accuracies? \n- In Table 2, what are the exact setup for SmolLM+PI/NTK/YaRN, and which token budget for the DroPE did you use?\n- In Appendix C.1, you mentioned a \"simple QKNorm\". Does that mean you modified the architecture slightly after dropping the positional encoding? What happened without? Are we able to see a detailed analysis on ablating all these variables?\n\nI will read more later and write more questions if something arises.\n\nI am tentatively giving it a 6 as I think it is very intriguing work and the authors have very good math foundations and taste. But experimental rigorousness is crucial. **I may revise the score higher or even lower** as I understand more about the details of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lz47auZM7a", "forum": "RlPVSeKjoc", "replyto": "RlPVSeKjoc", "signatures": ["ICLR.cc/2026/Conference/Submission1735/Reviewer_fTZx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1735/Reviewer_fTZx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428551014, "cdate": 1761428551014, "tmdate": 1762915873527, "mdate": 1762915873527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DroPE, a simple and effective method for extending the context length of pretrained language models without expensive long-context finetuning. The core idea is to remove the positional embeddings (PEs) from a fully trained model and then perform a short \"recalibration\" training phase. The authors argue that PEs are a crucial scaffold for efficient pretraining but inherently limit generalization to longer sequences. By dropping them post-training, DroPE achieves significant zero-shot context extension. Empirical results on a 0.5B parameter model and the SmolLM model show that DroPE outperforms established RoPE scaling methods on long-context tasks while preserving performance within the original context window."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The central idea of dropping positional embeddings to achieve context length extrapolation is interesting. It reframes PEs as a temporary training aid rather than a permanent architectural component.\n\n2. The proposed DroPE method is simple to implement and demonstrates strong empirical performance, simply but effectively alleviating the OOD issue of RoPE."}, "weaknesses": {"value": "1. The paper's central claims are not validated at a scale that reflects the current state of long-context LLMs. The experiments are limited to relatively small models (under 0.5B parameters) and a modest 2x context extension (e.g., 2048 to 4096 tokens). This is a significant limitation, as the most pressing need for context extension exists in much larger models (7B+) and for vastly longer sequences (e.g., 32k, 128k, and beyond). It is unclear if the method's effectiveness and training stability would hold when extending context by 10x or 100x, where challenges like attention dilution and loss of positional signal become far more severe. The strong claims of the paper require evidence at a more demanding scale to be fully convincing.\n\n2. While the paper provides extensive analysis, a notable portion of it lacks novelty and feels disconnected from the proposed method. The analysis in Section 4, which details the failure of RoPE-scaling methods due to the compression of low frequencies, largely reiterates well-established findings from the original papers on YaRN, NTK-RoPE, and LongRoPE. Furthermore, these theoretical insights do not directly inform the specific design of the DroPE recalibration process, such as the required duration or optimal hyperparameters. The theory explains why a problem exists but offers little guidance on how to best implement the proposed solution.\n\n3. The theoretical justifications in Section 3, intended to prove the necessity of PEs during training, rely on arguments that feel trivial. For example, Proposition 3.2 proves that a NoPE transformer's gradients vanish on an artificial sequence of identical tokens. This is an unrealistic edge case that has little bearing on training with diverse, real-world data. Spending significant space on such formalisms seems to over-justify a widely accepted premise (that PEs help training) and detracts from what could have been a more focused empirical investigation into the DroPE method itself."}, "questions": {"value": "1. How do you expect DroPE to perform when scaling to much larger models (e.g., 7B+) and extending the context by a much larger factor (e.g., from 4k to 128k)? Do you anticipate any new optimization challenges?\n\n2. Have you evaluated DroPE on tasks that are highly sensitive to precise token positions (e.g., Passage reranking in HELMET)? Is there a trade-off where long-context generalization comes at the cost of fine-grained positional awareness?\n\n[1] HELMET: How to Evaluate Long-context Language Models Effectively and Thoroughly\n\n3. What is the intuition for what the model learns during recalibration? Is it primarily learning to infer relative positions from the causal mask, and have you observed corresponding changes in attention patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lxEQvX9gHN", "forum": "RlPVSeKjoc", "replyto": "RlPVSeKjoc", "signatures": ["ICLR.cc/2026/Conference/Submission1735/Reviewer_PvVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1735/Reviewer_PvVC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848442562, "cdate": 1761848442562, "tmdate": 1762915873244, "mdate": 1762915873244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method to extend the context length of pretrained language models by removing RoPE after pretraining and conducting a short recalibration phase. The authors argue that positional embeddings are crucial for training convergence but harms zero-shot generalization to longer contexts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel and counterintuitive hypothesis where positional embeddings might not be necesssary throughout a model's lifecycle.\n2. Treating PEs (Positional Embeddings) as \"training scaffolds\" that can be removed is elegant and very good for downstream use cases such as long context fine-tuning and kv-cached inference performance.\n3. Good theoretical contributions on why PEs are necessary during training and why they can be removed after.\n4. Strong potential for practical impact because of how simple this method is. Furthermore, reduces the inductive biases of LLMs after removing RoPE."}, "weaknesses": {"value": "1. Experiments on small models might not scale well to larger models. Could be possible to take existing large LMs and recalibrate them with DroPE.\n2. No ablations or thorough experimentation on the recalibration phase. Naively removing PEs might work but given the theory there should be better ways. Furthermore, recalibration cost is not explored either.\n3. Dubious claim that YaRN cannot extrapolate to longer contexes, directly contradicting the original paper and results from the industry (DeepSeek R1, Qwen3, GPT-OSS).\n4. Lack of comparisons against more recent length generalization work (LongRope 2, sparse attention, etc.)\n5. Only test at 2k context lengths, most methods now test at 128k+.\n6. Specific solution to autoregressive LLMs, does not generalize to diffusion models."}, "questions": {"value": "1. Why was YaRN not able to extrapolate to 2x context length? This might be a mistake since Qwen3 uses YaRN without finetuning and has a perfect score in the needle-in-a-haystack benchmark.\n2. Do you know if DroPE is better than YaRN because it has more training time during the recalibration phase? Can you compare it against YaRN but with the same recalibration phase?\n3. Do you think that gradually removing the RoPE during the recalibration phase be better than outright removing it in a single step? It's highly likely that DroPE is equivalent to setting a RoPE Linear scaling to infinity.\n4. What happens with DroPE's performance at higher context scaling lengths? 4x, 8x, etc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bc76Y6K1W6", "forum": "RlPVSeKjoc", "replyto": "RlPVSeKjoc", "signatures": ["ICLR.cc/2026/Conference/Submission1735/Reviewer_iW8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1735/Reviewer_iW8i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953063580, "cdate": 1761953063580, "tmdate": 1762915873090, "mdate": 1762915873090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DroPE, a method that removes RoPE after pretraining to address its limitations. The authors demonstrate that while RoPE provides crucial inductive bias for rapid convergence during training, its explicit positional encoding hinders zero-shot generalization to longer sequences. They analyze why existing RoPE-scaling techniques fail in zero-shot scenarios by showing how they distort low-frequency attention heads essential for long-range semantic understanding. The core innovation is DroPE, which eliminates RoPE and employs a brief recalibration phase, compelling the model to rely on implicit positional cues from the causal mask and data patterns. In experiments, this approach achieves robust zero-shot extrapolation when extending the context length to twice the training length, significantly outperforming complex RoPE-scaling methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper shows that RoPE trained from scratch outperforms NoPE: NoPE has higher perplexity and exhibits gradient-vanishing issues during training.\n2. The authors propose a new way to obtain a NoPE base model by “dropping” RoPE from a well-trained RoPE model (Drop RoPE).\n3. They evaluate on selected datasets (NIAH and four LongBench subsets) and empirically demonstrate that RoPE→NoPE bases achieve length-generalization benefits for limited extrapolation ranges (mainly around 2×)."}, "weaknesses": {"value": "1. To me, the manuscript reads more like a blog post than a formal paper. The central claim—that DropRoPE provides a generalization advantage—feels weak across several experimental dimensions: the variety of base models tested, the extrapolation distances considered, the number and comprehensiveness of benchmarks, and the choice of baselines.\n2. The observation that performance can be restored by continued training after dropping positional encodings has been reported elsewhere. For example, Table 5 in [1] shows smollm-135M can recover performance via continued training even when positional encodings are fully or partially removed.\n\n[1] Towards Economical Inference: Enabling DeepSeek’s Multi-Head Latent Attention in Any Transformer-based LLMs (https://arxiv.org/pdf/2502.14837v1)"}, "questions": {"value": "1. Can NoPE extrapolation methods such as [2] be integrated with DropRoPE?\n2. A 2× extrapolation advantage is likely of limited practical significance. Can you evaluate generalization over a wider range of lengths (for example, from 2× up to 8×, as in [2])?\n3. Why were only four LongBench subsets selected rather than the full LongBench suite?\n4. Can the method be validated on widely used LLMs (e.g., LLaMA, Qwen)?\n\n[2] Length Generalization of Causal Transformers without Position Encoding (https://arxiv.org/pdf/2404.12224)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8CaWqM3nS1", "forum": "RlPVSeKjoc", "replyto": "RlPVSeKjoc", "signatures": ["ICLR.cc/2026/Conference/Submission1735/Reviewer_Zi8P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1735/Reviewer_Zi8P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762594065538, "cdate": 1762594065538, "tmdate": 1762915872797, "mdate": 1762915872797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}