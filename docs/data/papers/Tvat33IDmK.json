{"id": "Tvat33IDmK", "number": 9476, "cdate": 1758123827419, "mdate": 1763705468256, "content": {"title": "On The Expressive Power of GNN Derivatives", "abstract": "Despite significant advances in Graph Neural Networks (GNNs), their limited expressivity remains a fundamental challenge. Research on GNN expressivity has produced many expressive architectures, leading to architecture hierarchies with models of increasing expressive power. Separately, derivatives of GNNs with respect to node features have been widely studied in the context of the oversquashing and over-smoothing phenomena, GNN explainability, and more. To date, these derivatives remain unexplored as a means to enhance GNN expressivity. In this paper, we show that these derivatives provide a natural way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN (HOD-GNN), a novel method that enhances the expressivity of Message Passing Neural Networks (MPNNs) by leveraging high-order node derivatives of the base model. These derivatives generate expressive structure-aware node embeddings processed by a second GNN in an end-to-end trainable architecture. Theoretically, we show that the resulting architecture family's expressive power aligns with the WL hierarchy. We also draw deep connections between HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For computational efficiency, we develop a message-passing algorithm for computing high-order derivatives of MPNNs that exploits graph sparsity and parallelism. Evaluations on popular graph learning benchmarks demonstrate HOD-GNN’s strong performance on popular graph learning tasks.", "tldr": "We present High Order Derivative-GNN (HOD-GNN), a method that leverages the input derivatives of a base MPNN to enhance the expressive power of a downstream GNN by injecting  derivative-informed node features.", "keywords": ["Graph Neural Networks", "GNNs", "Expressivity", "Message Passing", "Geometric deep learning", "Differential geometry", "Symmetry"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c1cf9ed0fadd3bfcde384e57e05da731f2914b6.pdf", "supplementary_material": "/attachment/dd95e6295df88da85a9adcbcf07ed07746b82a01.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes HOD-GNN, a framework that enhances GNN expressiveness by computing high-order derivatives of a base MPNN with respect to node features and using them as structural encodings. Experiments on graph classification and regression benchmarks demonstrate competitive performance, with particular success on larger graphs where full-bag subgraph GNNs fail."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel approach to GNN expressivity. Unlike prior work that uses derivatives for analysis, this paper uses derivatives as input features to enhance expressiveness. The connection between Taylor expansion of marking and derivatives provides clear intuition.\n\n2. The efficient derivative computation algorithm is a notable technical contribution. Exploiting graph sparsity through message-passing-like updates makes the approach practical, as demonstrated by favorable memory usage and successful scaling to Peptides datasets where subgraph GNNs fail.\n\n3. The experimental evaluation is thorough for k=1 case, covering diverse benchmarks, multiple baseline families."}, "weaknesses": {"value": "1. Despite the paper's title and emphasis on \"k-HOD-GNN\", all experiments use k=1. The core theoretical claim that `k-HOD-GNN matches k-OSAN expressivity for general k` is  unvalidated. No experiments test k=2 or higher, no comparison with k-OSAN for k>1, and no demonstration of distinguishing graph pairs that require k>1. This makes the paper's main contribution more claimed than demonstrated.\n\n2. Theorem 4.2 only proves superiority over RWSE+MPNN for ReLU, not k-WL equivalence. The paper fails to adequately address this gap between theoretical claims and practical implementation. The k-WL distinguishability claim in the introduction is therefore misleading for the models actually tested."}, "questions": {"value": "1. Can you provide experiments validating k-HOD-GNN for $k \\ge 2$? Even a simple synthetic task distinguishing graph pairs requiring 2-node marking would substantially strengthen the main contribution. Without this, the paper's scope should be revised to focus on 1-HOD-GNN."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DAKi1Qhfsf", "forum": "Tvat33IDmK", "replyto": "Tvat33IDmK", "signatures": ["ICLR.cc/2026/Conference/Submission9476/Reviewer_nnEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9476/Reviewer_nnEj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760624210817, "cdate": 1760624210817, "tmdate": 1762921062784, "mdate": 1762921062784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HOD-GNN, an architecture that enhances GNN expressivity by computing derivatives of a base MPNN with respect to input node features and using these derivatives as additional structural information to input to the second GNN.  The paper proves that k-HOD-GNN can approximate k-OSAN subgraph GNNs, which is strictly more expressive than MPNNs with random walk structural encodings, and achieves favorable computational complexity for sparse graphs with shallow base MPNNs by exploiting an efficient message-passing-like derivative computation algorithm. Empirically, HOD-GNN consistently ranks in the top two tiers across seven benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a fresh perspective on enhancing GNN expressivity through derivatives. The intuition is clearly explained through the connection to marking-based GNNs and Taylor expansion. I think the presented approach is conceptually elegant. \n\n2. The theoretical analysis provides a complete expressivity characterization by proving k-HOD-GNN approximates k-OSAN subgraph GNNs while offering constructive separation examples that precisely identify when the method is efficient.\n\n3. The three-component pipeline from base MPNN, intermediate derivative computation, and the downstream GNN is clear and easy to follow."}, "weaknesses": {"value": "1. The claimed \"inductive bias toward derivative-aware representations\" is mentioned but never explained.\n\n2. The triangle counting example in the Motivation section is thin as a driver of the whole method. This motivating toy showing first-order derivatives recover triangle counts uses a very special linear stack, i.e., identity activations, and does not show whether derivatives remain informative for typical non-linear, normalized, or regularized MPNN on real tasks. Thus, it’s illustrative but not compelling evidence that derivatives are broadly the right signal. Also, in the theoretical analysis, the separation beyond k-WL depends on analytic activations that  iss fine for softplus/tanh, but not ReLU.\n\n3. Flattening $D^{out}$ could be large in practice. \n\n4. For $U^{node}$, a 2-IGN is powerful but can be heavy. The paper claims scalability via sparsity, but constants can bite (derivative channels $\\times$ feature dims $\\times$ orders). \n\n5. I didn’t see normalization for $D^{out}$ or $D^{(T)}$. Some stabilization may be necessary because  derivatives can vary wildly with feature scaling.\n\n6. I’m confused about the rationale for computing two derivative tensors that target different functions. Specifically, $D^{(T)}$ computes derivatives of the final node embeddings, while $D^{out}$ computes that based on the graph-level output. Why is it necessary to encode both, rather than using only node-level derivatives with an invariant head, or using only graph-level derivatives if the target is graph classification?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2zOBGHQUdW", "forum": "Tvat33IDmK", "replyto": "Tvat33IDmK", "signatures": ["ICLR.cc/2026/Conference/Submission9476/Reviewer_986R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9476/Reviewer_986R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459543957, "cdate": 1761459543957, "tmdate": 1762921062455, "mdate": 1762921062455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the expressive power of GNN from a derivative perspective. Combining high-order derivatives, the authors enhance the expressivity of GNN."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In this paper, the authors consider the expressive power for the derivatives of GNN, an innovative and interesting perspective to analyze expressive ability. By leveraging to high-order node derivatives, the authors enhance expressive power of GNN. The analysis seems reasonable and the conclusion seems reliable."}, "weaknesses": {"value": "While this is an interesting work, this finding is not surprising. Normally, the node feature vectors can be viewed as the zero-order information, i.e., the function value $h_v$. High-order derivatives can be seen as the high-order information for the function value. Combining much more high-order information can distinguish many graphs doesn't seem surprising to me. Instead, I am more interested in uncovering deeper insights from the derivative perspective, as such insights could provide a more fundamental understanding of the expressive power of GNNs."}, "questions": {"value": "1.I am interested in a more detailed characterization of the relationship between the k-WL test and GNN derivatives. For example, on a high level, can the k-WL test be equivalently expressed using derivatives of order less than k? Such a connection could directly elucidate the source of enhanced expressive power.\n\n2.I would like to clarify the impact of different choices for $\\alpha$ on expression ability in k-HOD-GNN? Theorem 4.1 only states existence, but it remains unclear: whether any possible choice of $\\alpha$ in k-HOD-GNN has stronger expression power than the k-WL test, when k and m are fixed. Then, it would be valuable to clarify the conditions on $\\alpha$ that guarantee stronger expression power. In addition, a systematic method for selecting the optimal $\\alpha$ is also an important theoretical justification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RaOsio2LaF", "forum": "Tvat33IDmK", "replyto": "Tvat33IDmK", "signatures": ["ICLR.cc/2026/Conference/Submission9476/Reviewer_S9Dt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9476/Reviewer_S9Dt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699951560, "cdate": 1761699951560, "tmdate": 1762921062169, "mdate": 1762921062169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces new class of Graph Neural Networks that aims to enhance expressivity by leveraging derivatives of MPNNs with respect to node features. The authors argue that derivatives naturally encode structural information and can thus extend the expressive power of base GNNs, drawing parallels with Weisfeiler–Lehman hierarchies, subgraph GNNs, and positional encodings. The paper includes theoretical expressivity proofs, an efficient message-passing algorithm for derivative computation, and experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper is well written and easy to follow. The idea is clear, simple but not trivial, and the conceptual move of translating the use of derivatives, which are widely explored in other contexts, into the domain of expressivity is interesting. \n\n2) It is particularly interesting that k-HOD-GNN can approximate any k-OSAN model to arbitrary precision while remaining computationally more efficient. This means HOD-GNN advances the field in a desirable direction, i.e., toward architectures that are highly expressive yet computationally tractable.\n\n3) Generally, the theoretical exposition is coherent and convincing, and it is always clear what is being done and why. Overall, the contribution feels clean: the narrative is consistent, and the motivation for using derivatives is well justified."}, "weaknesses": {"value": "**Minor issues:**\n\n1) There are a few small notational inconsistencies:\n\n-Line 150: $\\mathbf{X}$ should be bold, and h should carry a subscript (v for node-level, out for global as the authors define), since it’s unclear otherwise which representation is being referred to.\n\n-Line 158: when mentioning the final node representation $h_v$, it would be more consistent to add the superscript ($T$) as done earlier.\n\n-Line 125: you use $V^k$, which is understandable but never formally defined. Similarly, what exactly a “model” $\\mathcal{M}$ is could be defined more formally, though the meaning is still clear.\n\n2) The overview paragraph (lines 191–199) could be made clearer. In particular, point (4) says that $\\mathcal{T}$ is applied to “derivative-informed features”, but since $\\mathcal{T}$ is itself a GNN, the phrasing might mislead the reader to think that it acts only on those features, rather than on the full graph. Moreover, earlier in the paragraph $\\mathcal{T}$ is only called “downstream network,” which increases potential confusion. For such a crucial overview section, absolute clarity would help.\n\n**Conceptual concerns:**\n\nI think the paper could engage more critically with the current stage of the field. The literature on GNN expressivity went through a strong wave of works proposing models with expressive power going beyond 1-WL. However, in recent times the community has started to question whether such additional expressivity is truly useful in practice [1]—especially on feature-rich datasets, where node attributes already break most of the symmetries that 1-WL cannot distinguish. One could argue that even if 1-WL failures are rare in real-world datasets, developing models that go beyond the WL limit is still valuable, as it guarantees the ability to distinguish as many input as possible, regardless of whether such cases are frequent or not. However, several recent works have shown that higher expressivity often comes at the cost of worse generalization [2,3,4]. In this sense, proposing a model that only aims to surpass the WL boundary, without discussing why this added expressivity is beneficial or how it impacts generalization, feels a bit outdated relative to how the field has evolved. It would strengthen the paper to address questions like: 1) Do you have any hints about the generalization behavior of your model? 2) Why might HOD-GNN preserve generalization compared to other expressive models? 3) Can you provide an intuition for how derivative-based features may induce better inductive bias? A discussion about this would make the contribution feel more in tune with the current research conversation.\n\n\n**Experimental completeness:**\n\nIf results for certain baselines were not reported in prior papers, they were not rerun here. This leads to some missing entries, particularly for two datasets. It’s not necessarily a major issue, but it would be useful to explain the reason for the omission explicitly. For instance, were those models prohibitively expensive to run?\n\n\n[1] Jogl, Fabian, Pascal Welke, and Thomas Gärtner. \"Is Expressivity Essential for the Predictive Performance of Graph Neural Networks?.\" NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning. 2024.\n\n[2] Franks, Billy Joe, et al. \"Weisfeiler-Leman at the margin: When more expressivity matters.\" Forty-first International Conference on Machine Learning.\n\n[3] Maskey, Sohir, et al. \"Graph Representational Learning: When Does More Expressivity Hurt Generalization?.\" arXiv preprint arXiv:2505.11298 (2025).\n\n[4] Carrasco, Martin, et al. \"Rademacher Meets Colors: More Expressivity, but at What Cost?.\" arXiv preprint arXiv:2510.10101 (2025)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ClSj03WfrX", "forum": "Tvat33IDmK", "replyto": "Tvat33IDmK", "signatures": ["ICLR.cc/2026/Conference/Submission9476/Reviewer_WWrV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9476/Reviewer_WWrV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759481371, "cdate": 1761759481371, "tmdate": 1762921061904, "mdate": 1762921061904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment"}, "comment": {"value": "We thank all the reviewers for their positive and constructive feedback. We underscore 4 key contributions of our paper mentioned by the reviewers:\n\n\n\n\n1. **Strong theoretical contribution:** Reviewers consistently highlighted the strength of our theoretical work, noting that the “theoretical exposition is coherent and convincing,” that our “efficient derivative computation algorithm is a notable technical contribution,” and that our analysis provides a “complete expressivity characterization” along with “constructive separation examples that precisely identify when the method is efficient” (**nnEj**, **WWrV**, **986R**).\n\n\n\n\n2. **Novel and valuable perspective:** Reviewers described our approach as “novel”, “innovative,” “interesting,” and offering a “fresh perspective on GNN expressivity”, further noting that it \"advances the field in a desirable direction\" (**nnEj**, **S9Dt**, **WWrV**).\n\n\n\n\n3. **Thorough and reliable experiments:** Reviewers noted that our experimental section is “thorough” and “reliable,” demonstrating “competitive performance” on graph learning benchmarks, with “particular success on larger graphs” (**nnEj**, **S9Dt**, **986R**).\n\n\n\n\n4. **Clarity and quality of exposition:** Several reviewers highlighted that the paper is “clear”, “intuitive”, “well-written”, and “easy to follow”, and that the contribution “feels clean” (**nnEj**, **WWrV**, **986R**).\n\n\nDuring the rebuttal period, we added several strong improvements to our paper:\n\n\n1. In response to the reviewers’ inquiries regarding experimental validation of k-HOD-GNN for $k > 1$, we conducted new synthetic experiments demonstrating that **2-HOD-GNN can separate 34 pairs of 3-WL–indistinguishable graphs**, thereby directly validating our theoretical results (as requested).\n\n\n\n\n2. In response to questions about the generalization ability of HOD-GNN, we compared the train–test gap of HOD-GNN on MOLHIV and MOLTOX21 to baselines that are both more expressive and less expressive. The results show that **HOD-GNN exhibits stronger generalization** than these baselines. We additionally include an intuition explaining why we believe HOD-GNN generalizes well.\n\n\n\n\n3. Finally, to address inquiries about the exact expressivity gains obtained from using derivative signals, we provide **several complementary theoretical results** that clarify these effects. First, we show that even a 1-HOD-GNN with only output level derivatives can separate graphs that 1-FWL cannot, allowing us to “save” one level of tuple complexity. Second, $k$-HOD-GNN is bounded by $(k+1)$-FWL, showing that this one-level gain is maximal when using node/output level derivatives. Finally, adding edge-wise derivatives lets 1-HOD-GNN distinguish some 2-FWL-indistinguishable graphs, giving a two-level gain under standard sparsity assumptions."}}, "id": "bpjKtl9iF7", "forum": "Tvat33IDmK", "replyto": "Tvat33IDmK", "signatures": ["ICLR.cc/2026/Conference/Submission9476/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9476/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission9476/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763586067202, "cdate": 1763586067202, "tmdate": 1763586067202, "mdate": 1763586067202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revised manuscript upload"}, "comment": {"value": "Dear reviewers, we thank you for your helpful comments and suggestions, and have updated the manuscript accordingly. All changes have been marked in blue for ease of reference. Please note that the inclusion of new material required renumbering several items in the paper. In particular:\n- Theorem 4.2 is now Theorem 4.3\n\n\n- Propositions 4.3 and 4.4 are now Propositions 4.4 and 4.5\n\n\n- Table 7 in the appendix is now Table 8\n\nWe believe these revisions address your feedback and improve the clarity and completeness of the work."}}, "id": "mjgnLdlL3s", "forum": "Tvat33IDmK", "replyto": "Tvat33IDmK", "signatures": ["ICLR.cc/2026/Conference/Submission9476/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9476/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission9476/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705647588, "cdate": 1763705647588, "tmdate": 1763705674114, "mdate": 1763705674114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}