{"id": "JEN4nsDgh9", "number": 7660, "cdate": 1758031013135, "mdate": 1763634587071, "content": {"title": "Do I look like a \"cat.n.01\" to you? A Taxonomy Image Generation Benchmark", "abstract": "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored.\nTo address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models’ abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.", "tldr": "", "keywords": ["Image Generation", "Taxonomy", "Text2Image", "WordNet", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fcc38e64226c110e100b8fac7eb5ee1a746b823.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a comprehensive benchmark for evaluating text-to-image (T2I) models on taxonomy-based concept generation. The benchmark, called Taxonomy Image Generation, measures how well models can visualize WordNet concepts of varying abstraction levels.\nIt includes: three datasets (Easy Concepts, Random WordNet split, and LLM-generated predictions), 12 models (e.g., SDXL, Playground, FLUX, PixArt, etc.), nine metrics including ELO-based pairwise evaluation, reward models, and new taxonomy-driven similarity measures derived from KL divergence and mutual information.\nThe authors use both human and GPT-4-based pairwise evaluations, finding strong correlation (Spearman ≈ 0.9) between them. Results show Playground and FLUX consistently outperform others. The study concludes that T2I models can effectively visualize hierarchical lexical concepts, extending the visual dimension of WordNet beyond ImageNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Novel task definition linking taxonomy and image generation.\n\n2.Methodological depth — integrates human, GPT-based, and reward-model evaluations."}, "weaknesses": {"value": "1.The evaluation heavily depends on GPT-4 judgments, which could introduce unquantified biases.\n\n2.Missing ablation studies (e.g., comparing against pure CLIP-based baselines).\n\n3.Do the authors plan to extend this benchmark to multilingual or cross-lingual taxonomies?\n\n4.How is mutual-information-based similarity normalized across models?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ubu0UC414I", "forum": "JEN4nsDgh9", "replyto": "JEN4nsDgh9", "signatures": ["ICLR.cc/2026/Conference/Submission7660/Reviewer_qiUN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7660/Reviewer_qiUN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709482333, "cdate": 1761709482333, "tmdate": 1762919729492, "mdate": 1762919729492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a “taxonomy-aware” benchmark for text-to-image (T2I) generation: given WordNet synsets (e.g., cat.n.01), models must produce an image that faithfully depicts the target concept in its taxonomic sense. The authors build several test splits (common concepts, random WordNet nodes, and LLM-expanded concepts), evaluate multiple zero-shot T2I systems with prompts with/without textual definitions, and score results via pairwise preference (ELO/BT), reward models, taxonomy-aware CLIP similarities (lemma/hypernym/co-hyponym, specificity), plus FID/IS. Key findings: model rankings differ from mainstream T2I leaderboards; generation generally beats retrieval; adding a textual definition improves preference alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated task: focuses on whether models grasp taxonomic meaning rather than just surface text-image alignment.\n\n- Diverse evaluation protocol that mixes human/GPT-based preference with taxonomy-aware automatic metrics.\n\n- Clear experimental settings (with/without definition) that isolate the impact of definitional context.\n\n- Useful analysis showing generation > retrieval under this task design.\n\n- Potentially valuable artifact: standardized synset prompts/splits that others can reuse."}, "weaknesses": {"value": "- Missing 2025-era baselines: the model panel omits strong recent systems (e.g., Qwen-Image family, the latest GPT multimodal generators, and other 2025-vintage open models). This limits the external validity of the conclusions.\n\n- Single-image, point-accuracy perspective is narrow: current metrics say “right vs. wrong” at the target synset but don’t quantify how wrong. A hierarchy-distance sensitive measure (path-length / information-content similarity such as Wu-Palmer, Jiang–Conrath, Lin, or a taxonomic cross-entropy) would reveal whether errors drift to ancestors vs. co-hyponyms.\n\n- Insufficient error visualization: no compact one-page confusion view for co-hyponyms/siblings, making it hard to see systematic fine-grained failures (e.g., sibling breeds within a genus).\n\n- (Secondary) Retrieval baseline appears underpowered; fairness controls (equal-time/equal-compute) and rank uncertainty (CIs, Kendall τ stability) are under-reported."}, "questions": {"value": "- Model coverage: Can you add at least one 2025 frontier model and one 2025 open-source model (e.g., Qwen-Image-X, latest GPT-image, etc.) and re-report the main tables? If closed models are hard to access, provide an equal-time and equal-compute subset to ensure fair comparison.\n\n- Hierarchy-distance metric: Please report a taxonomic distance–aware score (e.g., average shortest-path or an IC-based similarity from WordNet) and break down errors into ancestor vs. sibling vs. distant categories. A simple plot of depth vs. accuracy and sibling density vs. confusion rate would be very informative.\n\n- One-page error analysis: Add a co-hyponym confusion matrix per super-class (e.g., within feline), with 4–6 representative visual examples annotated by the discriminative attributes that caused confusion (pattern, morphology, context)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rwu2vUAuw0", "forum": "JEN4nsDgh9", "replyto": "JEN4nsDgh9", "signatures": ["ICLR.cc/2026/Conference/Submission7660/Reviewer_zHqq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7660/Reviewer_zHqq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889412577, "cdate": 1761889412577, "tmdate": 1762919728819, "mdate": 1762919728819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes \"Taxonomy Image Generation\" as a novel benchmark task, aiming to evaluate text-to-image (T2I) models on their ability to visualize concise taxonomic concepts from WordNet. Unlike standard T2I tasks that use detailed descriptive prompts, this benchmark focuses on short, often abstract lemmas (e.g., \"landscape\"). The study evaluates 12 models and one retrieval baseline using 9 metrics, including novel taxonomy-specific scores grounded in KL Divergence (Hypernym/Cohyponym Similarity, Specificity)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a gap in current T2I evaluation: standard benchmarks rely on rich, detailed descriptions (like DiffusionDB ), which mask model weaknesses in handling the concise, abstract prompts typical of taxonomic entries (Figure 1 ).\n\n- The attempt to derive taxonomy-specific metrics (Hypernym Similarity, Specificity) grounded in the hierarchical structure of WordNet is a conceptually interesting move beyond standard FID/IS\n."}, "weaknesses": {"value": "- While the authors note that taxonomic prompting is difficult due to brevity (Figure 1 ), their benchmark heavily relies on adding explicit definitions to prompts to achieve good performance. This shifts the task from evaluating whether a model \"knows\" a taxonomic concept (e.g., \"cigar_lighter.n.01\") to evaluating standard instruction following. The significant drop in Human-GPT correlation (0.92 to 0.73) when definitions are removed indicates the definition, not the concept, is carrying the evaluation.\n\n- The human evaluation section is quite sparse. While it mentions using 4 experts in computational linguistics for 3370 image pairs, it lacks critical reproducibility details. It does not specify the recruitment platform, compensation, or crucially, how many raters evaluated each pair. A total of 4 raters for a major benchmark is also arguably too small a pool to ensure diversity of perspective. Details about the task are also missing.\n\n- The proposed 'Specificity' metric appears empirically flawed. Models that perform poorly in human preference (e.g., SD1.5) rank highly in Specificity. A metric that inversely correlates with human judgment and requires \"interpretation\" to be useful is not a strong contribution.\n\n- Several presentation and formatting Issues:\n  - The submission does not follow standard ICLR formatting (e.g., incorrect bottom margins)\n  - Citations are frequently malformed and integrated poorly into the text flow (e.g., missing parentheses or awkward placement)\n  - The results section is extremely short and most of the results were left in the Appendix"}, "questions": {"value": "- How do you validate that your CLIP-based \"taxonomy\" metrics are actually measuring hierarchical understanding rather than just inheriting CLIP's well-documented inability to distinguish fine-grained semantic differences?\n\n- If 'Specificity' ranks human-dispreferred models (like SD1.5) highly, what is its practical utility as a benchmark metric?\n\n- If the core challenge of taxonomic generation is handling concise/abstract prompts (as suggested in Figure 1), why heavily emphasize results that use explicit definitions, which essentially convert the task back into standard descriptive T2I generation?\n\n-  With only 4 assessors for 3370 pairs, how many human judgments were obtained per image pair?\n\n- Why was a web-scale retrieval baseline (like LAION) not included to provide a realistic comparison for the retrieval approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7nS7JyS8Gw", "forum": "JEN4nsDgh9", "replyto": "JEN4nsDgh9", "signatures": ["ICLR.cc/2026/Conference/Submission7660/Reviewer_KcuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7660/Reviewer_KcuZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931596441, "cdate": 1761931596441, "tmdate": 1762919728108, "mdate": 1762919728108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark for Taxonomy Image Generation, aiming to evaluate how well text-to-image models can visually depict concepts from hierarchical lexical databases like WordNet. The authors construct datasets combining common-sense, random, and LLM-generated taxonomy concepts and evaluate 12 models using nine taxonomy-aware metrics and both human and GPT-4 pairwise feedback. Results show that model rankings on this benchmark differ markedly from traditional T2I benchmarks, with Playground-v2 and FLUX performing best overall, while retrieval-based methods perform poorly. The paper also proposes new similarity metrics grounded in KL divergence and mutual information to quantify image–concept alignment. Overall, it shows the potential of automated visual generation to extend and update structured knowledge resources like WordNet."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the first comprehensive benchmark for taxonomy-based image generation, bridging a clear gap between linguistic taxonomies and visual generation evaluation.\n\n- It combines human evaluation, GPT-4 pairwise judging, and nine theoretically grounded metrics to provide a rigorous and well-rounded assessment of model performance. It evaluates 12 diverse text-to-image models across multiple datasets, ensuring broad coverage and fair comparison of model capabilities.\n\n- The findings reveal that taxonomy-based visual understanding differs significantly from standard T2I benchmarks, showing new challenges and directions for semantically grounded generation."}, "weaknesses": {"value": "- The benchmark focuses mainly on WordNet concepts, which may limit generalization to other taxonomies or domains that differ in structure and it may contain outdated or linguistically biased concept definitions that could propagate into the evaluation.\n\n- More qualitative analysis that explains why certain models fail on specific taxonomy levels (e.g., abstract vs. concrete concepts) will be helpful for improving the paper quality.\n\n- Although human evaluation is included, the number of annotators is small, which may limit statistical robustness."}, "questions": {"value": "How well do the proposed taxonomy-specific metrics (like hypernym and cohyponym similarity) correlate with human semantic understanding beyond CLIP-based embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gZRvLFKkKw", "forum": "JEN4nsDgh9", "replyto": "JEN4nsDgh9", "signatures": ["ICLR.cc/2026/Conference/Submission7660/Reviewer_tSGp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7660/Reviewer_tSGp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123033596, "cdate": 1762123033596, "tmdate": 1762919727646, "mdate": 1762919727646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}