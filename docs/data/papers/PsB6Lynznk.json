{"id": "PsB6Lynznk", "number": 5911, "cdate": 1757945778254, "mdate": 1759897945598, "content": {"title": "Long-Context Generalization with Sparse Attention", "abstract": "Transformer-based architectures traditionally employ softmax to compute attention weights, which produces dense distributions over all tokens in a sequence. \n    While effective in many settings, this density has been shown to be detrimental for tasks that demand precise focus on fixed-size patterns: as sequence length increases, non-informative tokens accumulate  attention probability mass, leading to dispersion and representational collapse.\n    We show in this paper that dynamically sparse attention mechanisms using $\\alpha$-entmax can avoid these issues, due to their ability to assign exact zeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows $\\alpha$-entmax with a learnable temperature parameter, allowing the attention distribution to interpolate between sparse (pattern-focused) and dense (softmax-like) regimes.\n    Our empirical evaluation on synthetic tasks and language modeling demonstrates that ASEntmax substantially outperforms softmax, scalable softmax, and fixed-temperature $\\alpha$-entmax baselines, achieving up to 1000$\\times$ length extrapolation on synthetic benchmarks and superior long-context generalization on language modeling while preserving short-context performance, including better perplexity trends and higher retrieval accuracies at 8$\\times$ training length.", "tldr": "", "keywords": ["long-context", "sparse attention", "length generalisation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4b553534f2fd52f9284a1f80c93a971f74d2696.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the long-context generalization limitations of transformer architectures by replacing the traditional softmax attention with ASENTMAX, a differentiable sparse transformation that assigns exact zero attention to irrelevant tokens, achieving up to 1000× length extrapolation and superior long-context perplexity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides theoretical analyses to address core limitations of softmax attention. It formally proves that alpha-entmax maintains non-vanishing attention probabilities.\n2. ASEntmax introduces head-specific, learnable scaling parameters to adjust logits dynamically. \n3. The empirical evaluation is thorough and diverse, covering synthetic tasks (retrieval-focused, memory-dependent, ordering) and real-world language modeling."}, "weaknesses": {"value": "1. It is unclear whether ASEntmax can be integrated with FlashAttention-2, a widely adopted library for efficient softmax attention. Given FlashAttention’s popularity in industrial and academic settings, clarifying ASEntmax’s compatibility with FlashAttention (e.g., modifications needed for sparse logit scaling, runtime overhead comparisons) is critical for its practical adoption.\n2. The paper does not explicitly confirm whether ASEntmax inherits the theoretical properties of alpha-entmax in Section 3. For instance, does the learnable temperature parameter disrupt the thresholding effect of ω-entmax that ensures zero attention to irrelevant tokens?\n3. The theoretical motivation for ASEntmax’s scaling mechanism (Section 4.1) relies on offsetting the range growth of IID Gaussian logits. However, real-world attention logits are unlikely to follow IID Gaussian distributions—they are influenced by semantic relevance, positional biases, and layer-wise transformations. \n4. Proposition 2 claims that alpha-entmax alleviates over-squashing “via stronger gradient signals,” but the reasoning is incomplete. \n5. In the MQMTAR task, ASEntmax achieves 1024× length extrapolation, but the paper relies on standard positional encodings (e.g., NAPE, ALiBi). Existing literature shows that many positional encodings struggle with extreme extrapolation due to degraded positional awareness. The paper does not analyze whether NAPE alone can support 1024× extrapolation or if its performance is entirely dependent on ASEntmax. \n6. Table 4 shows ASEntmax achieves near-perfect performance (97.4%) on S-NIAH-1 (OOD, 16K tokens) but only 25.4% on S-NIAH-2 (OOD, 8K tokens). The paper does not explain this stark disparity. Is S-NIAH-2 is presumably a harder task？\n7. The related works should include a discussion about the broader landscape of sparse attention mechanisms designed for long-context modeling (e.g., [1-7]).\n---\n[1] Efficient streaming language models with attention sinks. ICLR 2024. \n\n[2] MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. NeurIPS 2024. \n\n[3] Core Context Aware Transformers for Long Context Language Modeling. ICML 2025. \n\n[4] XAttention: Block Sparse Attention with Antidiagonal Scoring. ICML 2025. \n\n[5] FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference. ICLR 2025. \n\n[6] Curse of High Dimensionality Issue in Transformer for Long Context Modeling. ICML 2025. \n\n[7] MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention. ICML 2025"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t4l3IEO5Ju", "forum": "PsB6Lynznk", "replyto": "PsB6Lynznk", "signatures": ["ICLR.cc/2026/Conference/Submission5911/Reviewer_7xk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5911/Reviewer_7xk8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071376438, "cdate": 1762071376438, "tmdate": 1762918343877, "mdate": 1762918343877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper makes attention genuinely sparse with α-entmax and then auto-tunes temperature by context length via ASEntmax so the model still locks onto key tokens in very long sequences. The theory shows softmax spreads probability mass as sequences grow while entmax keeps entropy low and prevents weights from vanishing on important positions. The experiments mix controlled synthetic tasks with language modeling and point to steadier long-context behavior without hurting short-context quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper cleanly connects attention dispersion to representation collapse and over-compression with precise definitions and propositions.\n\n2. The method plays nicely with NAPE and existing fast-attention kernels like FlashAttention/AdaSplash, which makes it engineer-friendly. \n\nOverall,I think it is solid, but I'm not fully sure my evaluation is correct."}, "weaknesses": {"value": "1. Entropy bounds rely on bounded-logit or near-Gaussian assumptions that may break on heavy-tailed or high-contrast distributions. \n2. Deployment-critical profiles for memory, throughput, and tail latency are thin, especially for large models and ultra-long inputs. \n3. Interplay with RoPE extensions, retrieval-style sparsity, or SSM hybrids is discussed but not systematically tested. \n4. Robustness beyond NAPE and specific hyperparameters is unclear, so portability to other position encodings or task families needs work."}, "questions": {"value": "1. What happens when the logit distribution departs from IID Gaussian or becomes extremely aligned, and can you show counterexamples and diagnostics. \n2. How should this combine with Top-k, chunk routing, or RAG pruning, and are there simple ordering/budgeting rules to follow. \n3. Can you provide an end-to-end compute profile and an early-exit strategy so systems can auto-tune ASEntmax intensity by sequence length."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2NLNJgRLar", "forum": "PsB6Lynznk", "replyto": "PsB6Lynznk", "signatures": ["ICLR.cc/2026/Conference/Submission5911/Reviewer_5G4R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5911/Reviewer_5G4R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076315824, "cdate": 1762076315824, "tmdate": 1762918343631, "mdate": 1762918343631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Adaptive-Scalable Entmax (ASEntmax), a sparse attention mechanism that improves long-context generalization by \"zeroing out\" attention (in an exact manner) on irrelevant tokens.\nOn multiple datasets and benchmarks, ASEntmax outperforms all baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- a very simple change to existing attention mechanism\n- works well across different datasets and tasks, showing consistent length generalization compared to baselines"}, "weaknesses": {"value": "- there are some understanding problems that i'm facing, refer to the questions"}, "questions": {"value": "- how is adaptive scalable entmax different from both scalable softmax and vanilla entmax ? is there a version of entmax that is only \"adaptive\" and not \"scalable\"? i understand entmax enables to turn of attention on some tokens below a certain threshold; scalable softmax amplifies attention if logits exceed by some amount compared to minimum logit; why can't vanilla entmax work by itself? why is the scaling solution proposed by SSMax required in this paper?\n\n- i didn't understand the motivation behind figure 3. why should scalers learned in figure 3 help in long-context generalization?\n\n- is QK-norm used before attention? can this fix some of the problems faced in long-context due to bounded logits?\n\n- is there any finetuning used before testing on RULER ?\n\n- why do the results vary a lot when using RoPE vs when using NAPE (e.g. in Appendix H) ?\n\nI am definitely willing to improve the score if these are addressed :)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CqhaVnaGro", "forum": "PsB6Lynznk", "replyto": "PsB6Lynznk", "signatures": ["ICLR.cc/2026/Conference/Submission5911/Reviewer_HXz2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5911/Reviewer_HXz2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092658825, "cdate": 1762092658825, "tmdate": 1762918343389, "mdate": 1762918343389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The attention scores, even transformed by a highly distorting function (such as softmax), exhibit one unfortunate trait – they tend to have high entropy and get closer to uniform distribution, if the sequence length grows significantly. This work aims to analyze and offer a solution for  this problem by building on previous literature on a specific family of functions – Entmax – which have favorable properties, most notably sparseness. The authors propose a novel variation of this function, ASEntmax with learnable weights which allow regulating sparseness level with the increase of sequence length. \n\nThe scope of this work is essentially theoretical, as it examines long-range properties of softmax and Entmax family of transforms, successfully proving several and expanding our understanding of softmax limitations and methods to circumvent them. However, the paper also for the first time (as I’m aware of) empirically validates (AS)Entmax activations in causal language modeling on a scale of 420M parameter models.\n\nOverall, I believe this is a solid work and it should be accepted."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* The problem of length generalization of Transformers is well-motivated and relevant.\n\n* The paper is thorough and rigorous in math, although I didn’t check all proofs and derivations. \n\n* The conclusions derived from principled theoretical analysis, are also plausible intuitively and corroborated by evidence.\n\n* The empirical validation is convincing, and the results are promising. Specifically, Entmax and ASEntmax show superior out-of-distribution generalization capabilities as compared to Softmax in causal language modeling on two long-context datasets."}, "weaknesses": {"value": "* It would be helpful to also compare different attention activation functions for language modeling perplexity as in Table 3, but with industry-standard RoPE embeddings. Appendix H.9. does not provide such comparisons, but it seems that the RoPE models are already pre-trained, and you would only need to test their perplexity on these long-context benchmarks. \n\n* The paper could be more self-contained. To fully comprehend the material, it is necessary to familiarize oneself with several previous works about Entmax. The exposition in Appendix A is scarce and for the most part repeats section 2.2. I would appreciate if definitions of Tsallis Entropy, q-expential functions, and a brief recap of sections 2.1-2.2 of Sparsemax paper (https://proceedings.mlr.press/v48/martins16.pdf) and sections 3.1-3.3 of original Entmax paper (https://aclanthology.org/P19-1146.pdf) could be provided.\n\n* The paper doesn’t establish which values of $\\alpha$ and the functional form of $\\tau(\\cdot)$ are used in the experiments.\n\n* There are no comparisons of speed of calculation between the 4 considered algorithms. An earlier work (https://proceedings.mlr.press/v48/martins16.pdf) alludes that run-time complexity for at least some of the Entmax variants is $O(L \\log L)$ instead of softmax’s $O(L)$, so it warrants a test for understanding how real-life throughput is impacted."}, "questions": {"value": "- Can ASEntmax be implemented as an efficient Triton kernel, similarly to AdaSplash (https://openreview.net/forum?id=OWIPDWhUcO), without incurring numerical instabilities? \n\n- Will you open-source the model weights for all variants of Llama pre-trained on DCLM data?\n\n- Proposition 2.1 (Preserved representations) – can the opposite be proved for softmax? If not, the statement of this proposition should be altered to include softmax."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Gk9tEUNzDl", "forum": "PsB6Lynznk", "replyto": "PsB6Lynznk", "signatures": ["ICLR.cc/2026/Conference/Submission5911/Reviewer_xkyo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5911/Reviewer_xkyo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762319237937, "cdate": 1762319237937, "tmdate": 1762918343123, "mdate": 1762918343123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}