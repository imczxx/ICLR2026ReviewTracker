{"id": "vYSxXKGD0a", "number": 17685, "cdate": 1758279219457, "mdate": 1759897160290, "content": {"title": "Observe Anything: Human-Intervened Video Understanding with Adaptive Orbital Memory", "abstract": "Video understanding needs both detecting objects in individual frames and maintaining the object identities across time. However conventional methods separate detection and tracking, leading to failures under long-term occlusion, abrupt appearance changes, and the emergence of novel objects. These challenges are particularly severe in dynamic and open-world environments, where objects frequently disappear, reappear, or evolve in appearance, and once identity is lost, automated systems rarely recover. Consequently, a formulation incorporating human-intervention is required to ensure reliable and adaptive continuity. To alleviate this, we introduce Video Object Observation (VOO), a new task unifying detection, tracking, and hunam-intervention, thereby shifting the focus from frame-level recognition to consistent sequence-level observation. To realize this, in this paper, we propose VOOV (Video Object Observer with human-interVention), the first framework explicitly designed for VOO. VOOV integrates three complementary memory modules, such as Originate, Sequential, and Long Term, that jointly encode semantic identity and temporal context, while an Orbital Deformable Attention mechanism models object motion probabilistically. Sparse human-intervention, including initialization, bounding box correction, and target switching, is systematically incorporated into memory, thereby enabling online adaptation without retraining. Experiments on multiple benchmarks demonstrate that VOOV achieves SotA performance, providing robust and real-time observation across diverse and challenging scenarios.", "tldr": "we present a novel framework for video object observation for video object detection and video object tracking with human-intervention", "keywords": ["video object detection", "video object tracking", "human-in-the-loop", "continuity"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8630b3aa9abcc418e64a784973924cd3ca01e01b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **Video Object Observation (VOO)**, a new task that unifies object detection, tracking, and human intervention to maintain continuous object identity in videos. To address this, the authors propose **VOOV (Video Object Observer with human-interVention)**, a novel framework designed to handle common tracking failures like long-term occlusion and abrupt appearance changes. VOOV's core contribution is its architecture, which features three complementary memory modules—**Originate, Sequential, and Long-Term**—to encode an object's identity and temporal context at different scales. It also introduces an **Orbital Deformable Attention** mechanism that probabilistically models object motion to better predict location. The framework allows sparse human feedback, such as bounding box corrections, to be integrated directly into its memory modules in real-time without needing to retrain the model, enabling corrections to persist and influence future frames. Experiments show that VOOV achieves state-of-the-art performance, providing robust and efficient observation across diverse and challenging scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces Video Object Observation (VOO), a new task that formally integrates detection, tracking, and human intervention for continuous identity preservation.\n- The proposed VOOV framework features an new architecture that combines a hierarchical memory system with a probabilistic, motion-aware attention mechanism for robust temporal modeling.\n- The model efficiently integrates sparse human feedback as lightweight, test-time memory updates, enabling real-time adaptation and persistent correction propagation without retraining.\n- The work is supported by a comprehensive evaluation across five benchmarks, including extensive comparisons, ablation studies, and a user study, which collectively validate the framework's state-of-the-art performance and practical effectiveness."}, "weaknesses": {"value": "- First of all, the paper's title, \"Video Understanding\" and \"Observe Anything,\" is overclaimed. Video understanding encompasses many different levels, but this work focuses only on low-level tracking. This raises doubts about whether the work is framed under the correct category and scope. The title could be significantly improved by including the term \"video object tracking with human intervention,\" which is clearer and fits the work's scope. The reviewer is quite concerned about the intention of using such a broad and high-level title in this submission.\n\n- The proposed task does not seem realistic to me in terms of real-world applications. It is unclear in what scenario the proposed VOOV could actually be used. If a human is required to constantly focus on the video, checking if the algorithm has tracked the wrong target and providing intervention (or correction), what is the actual purpose of the method?\n\n- The paper benchmarks and compares its method with multiple \"detector,\" \"single object tracker,\" and \"multi-object tracker\" methods. All these methods are designed for different tasks and have no ability to leverage human intervention. This raises doubts about the fairness of the comparison, especially when these methods are tested on VOS-type benchmarks.\n\n- Line 361 states: \"Interventions are triggered when predictions fall below IoU 0.3 for five consecutive frames or during occlusion >1s, and are applied identically across methods.\" It is unclear how this intervention is applied to existing methods, such as a detector like YOLOv8.\n\n- It is unclear to me how multiple baselines are implemented, especially methods like SAM2-prompt. I also found a technical statement error regarding SAM2 in L.2058: \"In contrast, SAM2 performs prompt-based inference, requiring a bounding box prompt...Since SAM2 does not maintain a memory state or temporal continuity, any correction must be explicitly reapplied at each frame.\" This is not correct, as SAM2 leverages a memory bank to store object features across the temporal dimension. This incorrect technical statement alone raises a big concern about the validity of the implemented baseline, specifically SAM2-prompt, especially given the missing implementation details.\n\n- Diving further into Table 10, the reported FPS (3.2) for SAM2 shows a tenfold discrepancy with the FPS reported by the official paper (30+ on a single GPU). This adds another extra layer of concern regarding the rigor of the experiments in this paper.\n\n---\n\nConsidering the overclaimed scope in the title, unclear motivations, missing baseline details, technical errors in the paper, concerning experimental results especially incorrect understanding of the baseline method, and large discrepency on the reported FPS, I lean toward rejection."}, "questions": {"value": "- How does the work ensure the experiments are reproducible? Every human can provide the intervention at different timestamp.\n- More details on how the baselines are implenented, especially detector and SAM2.\n- Some other concerns (task motivations, paper title, SAM2...etc) mentioned in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qRRDtz2l7j", "forum": "vYSxXKGD0a", "replyto": "vYSxXKGD0a", "signatures": ["ICLR.cc/2026/Conference/Submission17685/Reviewer_X9jh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17685/Reviewer_X9jh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760588672375, "cdate": 1760588672375, "tmdate": 1762927533058, "mdate": 1762927533058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Video Object Observation (VOO) task and designs the VOOV (Video Object Observer with human-interVention) framework to address it. The core of VOOV lies in its hierarchical memory pipeline (comprising origin, sequence, and long-term memory) and its Orbit Deformable Attention (ODA) mechanism. The methodology aims to achieve retraining-free online adaptation by translating user interventions (e.g., bounding box corrections) into real-time gradient updates of memory embeddings. It is asserted that this approach effectively resolves long-term occlusion and identity drift issues.\n\nExperiments were conducted on multiple video datasets, introducing metrics such as Interaction Efficiency (IE). The results demonstrate that VOOV, operating under a limited intervention budget, outperforms existing automated trackers and Human-in-the-Loop (HITL) benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Rigorous Experimental Validation**: The paper systematically substantiates the performance advantages of VOOV over State-of-the-Art (SOTA) benchmarks through comprehensive ablation studies (Section 4.1), fair comparative analyses (Section 4.2), and detailed failure case analyses (Section 4.4).\n\n**High Interaction Efficiency**: The proposed Interaction Efficiency (IE) metric effectively quantifies the persistent improvements derived from user feedback, demonstrating the method's efficient utilization of sparse interventions.\n\n**Robust Recovery Capability**: The data indicate that VOOV achieves a recovery rate surpassing 90% under conditions of severe occlusion and inter-object overlap, thereby strongly validating the practical efficacy of the memory-propagated interventions."}, "weaknesses": {"value": "Ambiguous Task Demarcation: The VOO task (which unifies detection, tracking, and intervention) is fundamentally a natural extension of existing Human-in-the-Loop (HITL) Tracking, rather than a disruptive new paradigm. The concept of integrating interventions into memory, within the context of memory-based trackers (e.g., MeMOTR), constitutes an incremental improvement.\n\nOmission of Relevant Comparisons: Concurrently, the design of memory mechanisms has been extensively studied in various tracking algorithms, particularly in domains such as Single Object Tracking (SOT) and long-video understanding. The paper fails to provide a comparison with relevant literature in these fields.\n\nImplementation Concerns: The method's dependence on human information is likely to affect its practical implementation difficulty.\n\nRecommendation: It is suggested that a figure be supplemented to illustrate the distinctions between the proposed task and preceding tasks."}, "questions": {"value": "Given the rapid proliferation (or: accelerated development) of Multimodal Large Language Models (MLLMs), which have emerged as a relatively mature technology capable, to a certain extent, of performing tasks traditionally requiring human intervention, is it feasible (or: viable) to substitute the input of human-provided information with MLLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vRmIIk9PKF", "forum": "vYSxXKGD0a", "replyto": "vYSxXKGD0a", "signatures": ["ICLR.cc/2026/Conference/Submission17685/Reviewer_rRMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17685/Reviewer_rRMv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761152979606, "cdate": 1761152979606, "tmdate": 1762927532637, "mdate": 1762927532637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Video Object Observation (VOO), a new task that unifies object detection, tracking, and human intervention to address tracking failures caused by occlusion or appearance changes. The authors propose VOOV, a framework centered on a hierarchical memory system comprising Originate, Sequential, and Long-Term modules. The core mechanism is a \"test-time adaptation\" where human feedback (e.g., a bounding box correction) triggers a lightweight gradient update directly to the model's memory embeddings, rather than requiring full retraining. This allows corrections to propagate to subsequent frames, ensuring continuous identity preservation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces VOO, a novel and well-motivated task. Formally unifying detection, tracking, and intervention addresses a practical limitation of fully automated systems that fail under real-world conditions.\n- The method for handling human feedback is a key strength. Using test-time adaptation to apply a gradient-based update to memory embeddings is a clever and efficient solution that allows user corrections to propagate temporally.\n- The model demonstrates strong empirical performance. The failure case analysis (Table 2), budget-performance tradeoff (Fig. 4b), and the inclusion of a real user study (Table 3a) effectively validate the framework's robustness and the efficiency of its intervention mechanism."}, "weaknesses": {"value": "- I feel the paper does a poor job of reviewing related work. The main literature review is buried in Appendix D, so it's not clear how this builds on prior art. Also, in Section 2, the authors introduce these standard problem formulations like Eq. (1) and (2) without citing *any* of the key papers that proposed or use them. Consider moving the appendix content to the main paper and adding those citations\n- The quantitative evaluation of the VOO task hinges on a *simulated* human user. However, the criteria for this simulation (triggering at \"IoU < 0.3 for five consecutive frames or during occlusion > 1s\" are presented without justification. Could the authors explain how the threshold is chosen, and how occlusion is decided? Do they follow any prior work to establish this as a standard protocol for evaluating human-in-the-loop tracking?\n- Can the proposed framework handle object state changes (such as butter melting, cutting an apple). Some discussion on this direction would be helpful."}, "questions": {"value": "See weaknesses for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6QYYyoXGc9", "forum": "vYSxXKGD0a", "replyto": "vYSxXKGD0a", "signatures": ["ICLR.cc/2026/Conference/Submission17685/Reviewer_fuVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17685/Reviewer_fuVs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870543651, "cdate": 1761870543651, "tmdate": 1762927532126, "mdate": 1762927532126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a persistent challenge in video understanding: the automated object detection and tracking systems on novel objects and long-term occlusion. In addition, existing human-in-the-loop (HITL) systems are insufficient since they predominantly operate at the level of annotation efficiency or detection correction and there is a significant disconnect that limits their capacity. \n\nThe paper then proposes a new task Video Object Observation (VOO) that integrates detection, tracking, and intervention within a single formulation. In specific, it maintains continuous identity across video clips, and the paper claims that interventions (initialization, bounding box correction, target switching) should be embedded into the model’s memory to influence subsequent predictions. The paper also introduces the Video Object Observer with human-interVention (VOOV), to solve the VOO task, which consists of a hierarchical memory system, memory-integrated intervention, and orbital deformable attention (ODA).\n\nThe author conducts extensive experiments for evaluating the performance of the new framework on five major benchmarks against the latest baselines and validates their performance with a user study demonstrating its efficiency and reduced cognitive load in practice."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe paper is well-written and thoroughly formalizes a new task called VOO. The author successfully identifies the current flaw in existing HITL methods, emphasizing the inefficient and addressing the practical gap between tracking and HITL systems.\n\n•\tThe VOOV framework itself is novel and effective: The idea of integrating sparse human feedback directly into memory states is interesting and Equation 6 directly shows how instant update is applied to memory embeddings rather than retraining all model parameters, which allow corrections to propagate forward through multi modules and therefore influencing predictions. In Table 1a, the ablation clearly shows the effectiveness of memory pipelines, showing a huge performance drop without using All memory and without using propagation as stated.\n\n•\tExtensive Experiment and evaluation: The experimental validation is comprehensive. The authors compare VOOV against all relevant models: SAM2-Prompt, ByteTrack, YOLOv8, and DETR. VOOV scales more effectively, recovering from occlusion and identity switches with fewer corrections. In addition, the ablation studies Table 1a are highly effective and clearly demonstrate the necessity of each key component."}, "weaknesses": {"value": "•\tGeneralizability: VOOV is a new yet complex task of combining several critical video understanding problems; it utilizes a DETR-based backbone for frame-level features, three separate memory encoders and aggregators (Originate, Long-Term, Sequential), and ODA. Although they seem to be all effective according to the author’s ablation, how difficult it is to train and learn at an engineering level remains unclear. It limits the reproducibility, and the effort of building the VOOV framework by using these modules might need further explanation.\n\n•\tIntervention failure: The mechanism of memory update helped preserve long-term memory. However, how do robustness and step size affect this gradient update? It’s unclear how this mechanism would behave while the sparse interventions become more frequence or if it conflicts with human intervention? Will frequent gradient updates lead the memory embedding to become unstable?"}, "questions": {"value": "•\tHow did the author evaluate the novel objects in their open world setting in Table. 3b? Can you explain how you prevent leakage and make the experiment OOD?\n\n•\tHow is the scalability for memory while facing frequent target switches which are close to real-world settings? In Appendix B.4, the author designs a multi-object observation where each of the M objects has their own memory pipeline. Will it become a problem if M becomes large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "74AcDm29JT", "forum": "vYSxXKGD0a", "replyto": "vYSxXKGD0a", "signatures": ["ICLR.cc/2026/Conference/Submission17685/Reviewer_wKXF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17685/Reviewer_wKXF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094677031, "cdate": 1762094677031, "tmdate": 1762927531801, "mdate": 1762927531801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}