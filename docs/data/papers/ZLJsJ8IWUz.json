{"id": "ZLJsJ8IWUz", "number": 14580, "cdate": 1758239370235, "mdate": 1759897361381, "content": {"title": "Dynamic Kernel Graph Sparsifiers", "abstract": "A geometric graph  associated with a set of points $P= \\{x_1, x_2, \\cdots, x_n \\} \\subset \\mathbb{R}^d$ and a fixed kernel function $\\mathsf{K}:\\mathbb{R}^d\\times \\mathbb{R}^d\\to\\mathbb{R}_{\\geq 0}$ is a complete graph on $P$ such that the weight of edge $(x_i, x_j)$ is $\\mathsf{K}(x_i, x_j)$. We present a fully-dynamic data structure that maintains a spectral sparsifier of a geometric graph under updates that change the locations of points in $P$ one at a time. The update time of our data structure is $n^{o(1)}$ with high probability, and the initialization time is $n^{1+o(1)}$. Under certain assumption, our data structure can be made robust against adaptive adversaries, which makes our sparsifier applicable in iterative optimization algorithms.   \nWe further show that the Laplacian matrices corresponding to geometric graphs admit a randomized  sketch for maintaining  matrix-vector multiplication and projection in $n^{o(1)}$ time, under sparse updates to the query vectors, or under modification of points in $P$.", "tldr": "", "keywords": ["Kernel", "Geometric Graph", "Theory"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc68c60733277d30ebb40a08d2371e98e867f323.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a fully-dynamic data structure to maintain an $\\epsilon$-spectral sparsifier for a geometric graph. A geometric graph is defined here as a complete graph on a set of $n$ points $P \\subset \\mathbb{R}^d$, where edge weights are given by a kernel function $K(x_i, x_j)$. The dynamic operation considered is the update of a single point's location. This operation is challenging because it changes $O(n)$ edge weights simultaneously, rendering traditional dynamic graph algorithms (based on single edge updates) inefficient.\n\nThe primary contribution is a data structure, DYNAMICGEOSPAR, that achieves a subpolynomial ($n^{o(1)}$) update time and a nearly-linear ($n^{1+o(1)}$) initialization time. The core technical innovation is a new dynamic well-separated pair decomposition (WSPD). The method works by projecting the $d$-dimensional points into an ultra-low $k=o(\\log n)$ dimension using a Johnson-Lindenstrauss (JL) transform. It then maintains a dynamic WSPD (using a compressed quadtree) in this low-dimensional space and uses a novel, efficient resampling technique to update the sparse set of edges corresponding to the original $d$-dimensional bicliques.\n\nThe paper further provides algorithms for maintaining randomized sketches for two related problems, also in $n^{o(1)}$ update time:\n1.  Approximate matrix-vector multiplication with the graph Laplacian ($L_G v$).\n2.  Approximate Laplacian solving ($L_G^\\dagger b$).\n\nFinally, the authors show that under a specific assumption balancing the data's aspect ratio ($\\alpha$) and ambient dimension ($d$) such that $\\alpha^d = O(poly(n))$, the data structure can be made robust against adaptive adversaries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work addresses a problem—fully-dynamic spectral sparsification for geometric graphs under *point location changes*—for which the authors claim no non-trivial algorithms were previously known. The primary technical contribution, a novel dynamic WSPD data structure combined with a \"smooth resampling technique\", seems original.\n\nKernel methods are a cornerstone of machine learning, and this paper provides a foundational tool for making them dynamic. The ability to efficiently maintain kernel-based graph structures under data changes is crucial for iterative optimization algorithms and real-world systems with streaming or evolving data. This paper successfully extends the important static results from (Alman et al., 2020) to the much more challenging dynamic setting."}, "weaknesses": {"value": "- The result for robustness against adaptive adversaries hinges on the assumption that $\\alpha^d = O(poly(n))$, where $\\alpha$ is the aspect ratio and $d$ is the ambient dimension. This assumption appears very restrictive. For any moderately high-dimensional data (e.g., $d > \\log n$), it requires the aspect ratio $\\alpha$ to be extremely close to 1. This significantly limits the applicability of the adversarial-robust result to real-world, high-dimensional ML datasets, which are often not so well-behaved.\n- The results apply to \"(C, L)-multiplicatively Lipschitz\" kernel functions. The paper provides a formal definition but does not explicitly explain whether popular kernels (e.g., Laplacian, polynomial) satisfy this (C, L)-Lipschitz condition. This makes it harder for a general ICLR audience to grasp the scope of the paper's applicability immediately."}, "questions": {"value": "- Could the authors provide a few examples of common machine learning kernels that satisfy the \"(C, L)-multiplicatively Lipschitz\" definition? \n- What are the main technical barriers to extending your approach to kernels that *do not* satisfy the (C, L)-Lipschitz property\n- Could the authors comment on the $\\alpha^d = O(poly(n))$ assumption for the adversarial-robust result? Is there some way to preprocess a dataset to decrease the aspect ratio?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2R2YU3uk06", "forum": "ZLJsJ8IWUz", "replyto": "ZLJsJ8IWUz", "signatures": ["ICLR.cc/2026/Conference/Submission14580/Reviewer_rC5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14580/Reviewer_rC5G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770740235, "cdate": 1761770740235, "tmdate": 1762924967059, "mdate": 1762924967059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studied dynamic algorithms for spectral sparsifiers of geometric graphs with $(C, L)$-Lipschitz kernel distance functions. The problem setting is as follows: we have $n$ data points in $\\mathbb{R}^{d}$, and the distances between the points satisfy some smoothness conditions (the $(C, L)$-Lipschitz property mentioned in the paper). At every time, an adversary comes to move a point $x_i$ to another position in $\\mathbb{R}^{d}$. Our goal is to maintain a spectral sparsifier, i.e., a subset of edges that forms a subgraph $H\\subseteq G$, such that the value of $x^{T} L_{H}x$ is close to $x^{T} L_{G}x$, where $L_{H}$ and $L_{G}$ are the graph Laplacians of $H$ and $G$, respectively.\n\nThe main result of this paper is a dynamic algorithm that maintains a $(1+\\varepsilon)$-approximation for the spectral sparsifier in $n^{1+o(1)}$ pre-processing time and $n^{o(1)}$ amortized update time. Furthermore, when $\\Delta^{d}$ is bounded by some polynomial, the algorithm is able to apply some union bound on a special data structure to argue adversarial robustness of their algorithm. Finally, the algorithm is possible to be generalized to approximate matrix multiplications and Laplacian system solvers, which are example applications of spectral sparsifiers.\n\n**Main techniques.** The main techniques of the paper are based on the well-separated pair decomposition ($s$-WSPD) defined in Callahan and Kosaraju [STOC’92]. At a high level, the decomposition finds partitions of vertices such that the shortest distance is at least $s$ times the diameter of the partitions. Therefore, we will only need to preserve the distances between the partitions to obtain good approximations. For the pre-processing steps, we only need dimensionality reduction + random sampling for such a data structure. For dynamic updates, we need to take advantage of the $(C, L)$-Lipschitz property and an algorithm such that each point $x$ only appears in at most $s^d$ WSPD pairs. The algorithm could be made adversarially robust by using a robust distance estimation data structure as long as the aspect ratio and the dimensions are bounded (for the $\\varepsilon$-net argument to work)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I’m supportive of the paper. The paper studied a well-motivated problem with many downstream applications in numerical linear algebra, data processing, and machine learning. The paper also spent some passages discussing these applications. The intuitions for the techniques are clearly given in the paper, and I could follow most of their ideas. The writing of the paper demonstrates a great breadth of knowledge, and this is the first dynamic algorithm for spectral sparsifier, as far as I know. Therefore, I believe the paper is a solid acceptance."}, "weaknesses": {"value": "I do not see major flaws in the paper. One potential criticism is that the paper contains no experiments; however, I do not think it is an issue for a paper with this amount and quality of results. There are some typos and lower-order clarity issues in the paper (see questions), but I think most of them could be easily fixed."}, "questions": {"value": "Some of the questions are asked in the weakness section. A few additional questions and comments:\n- Your title suggested that the algorithm works for kernel graphs. However, I think the algorithm works for general graphs with $(C, L)$-Lipschitz property, right?\n- In definition 1.2, why do you need to define a graph here? I assume $L_G$ is an arbitrary matrix instead of the graph Laplacian? If not, then what’s the difference between definitions 1.2 and 1.3?\n- If you start with an empty dataset, and get updates for each data point insertion/deletion, and let $n$ be the maximum number of data points we could ever have. In this setting, can your algorithm still get $n^{o(1)}$ amortized update time without any pre-processing time?\n- What is the dependency on $\\varepsilon$ for your update time? There must be some trade-off, no? I think you’re ignoring some log factors or treating $\\varepsilon$ as a constant.\n- Did you make the assumption that the total number of updates is $poly(n)$? You probably need that for union bounding over the events. Did you put such a statement/discussion in the paper?\n- Line 170/171: I think $\\ell_2$ norm is a well-established term, and ‘entry-wise $\\ell_2$ norm’ sounds a bit strange.\n- Line 201-202: we can compute this partition efficiently *with by*  -> with or by\n- Line 217: The notion of the ultra-low dimension JL lemma was mentioned without any explanation or intuition given. I guess the technique is possible to apply due to the problem and distance functions in your setting (in general subspace embedding, $\\Omega(\\log{n})$ should be necessary). Can you give more explanations about this?\n- Line 359: Is there an issue with the notation? Do you mean $d(U^2_1+U^2_2 + \\cdots + U^2_k)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t2yuSpK0cw", "forum": "ZLJsJ8IWUz", "replyto": "ZLJsJ8IWUz", "signatures": ["ICLR.cc/2026/Conference/Submission14580/Reviewer_TZMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14580/Reviewer_TZMZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771349961, "cdate": 1761771349961, "tmdate": 1762924966271, "mdate": 1762924966271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the dynamic maintenance of spectral sparsifiers for geometric graphs. Concretely, given an n-vertex geometric graph G (read a complete graph where each edge weight is defined according to some kernel function among its endpoints) associated with a set of points P lying on a d-dimension Euclidean space, the goal is to dynamically maintain a spectral sparsifier H of G under insertions/deletions of points to P. The main result is a data-structure that supports point updates in n^{o(1)} time, while ensuring that the size of H is n^{1+o(1)} at any time. The paper presents a randomized algorithm and discusses extensions that support adaptive adversarial updates. There are also results involving the maintenance of vector-matrix multiplication using random sketches as well as solutions to Laplacian systems of the underlying geometric graph. This review will mainly focus on the dynamic spectral sparsifier contribution.\n\nOne obvious approach when thinking about this problem is simply using the dynamic spectral sparsifier algorithm [ADK+16] that handles edge updates to the geometric graph in poly-logarithmic time. However, a single point update might trigger \\Omega(n) changes in the complete, geometric graph, which would in turn lead to a linear-time update time guarantee in the worst case. To overcome this issue, the paper leverages the construction of a static, sublinear algorithm for computing geometric spectral sparsifiers [ACSS20].\n\nThe algorithm consists of several building blocks. In the first step, it applies a JL transform on the point set P to bring down the dimension of the space, which results in the point set Q. It then proceeds to build a compressed quadtree T of Q and a well-separated pair decomposition on top of T. For each pair in this decomposition, it randomly samples edges from the complete bipartite graph induced by the pair, and the union over all sampled edges forms the final spectral sparsifier of the geometric graph.\n\nThe main technical contribution of this work is to make each of these building blocks dynamic. For some of these blocks, dynamic algorithms existed prior to this work, e.g., maintaining a well-separated pair decomposition is a classical result in computational geometry. The paper shows how to adapt this for the problem at hand. To the best of my judgment, it seems that their main contribution in the dynamic maintenance is making sure that they update the uniform samples for the bi-cliques efficiently, and this is also the running time bottleneck of their data structure."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Given the importance of geometric graphs and their application in data-driven applications, the problem studied here is natural and very well motivated. The results on the update time are also competitive."}, "weaknesses": {"value": "* The question is not central to classic dynamic data structures, but perhaps relevant in dynamic ML applications"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RTItbiyDGO", "forum": "ZLJsJ8IWUz", "replyto": "ZLJsJ8IWUz", "signatures": ["ICLR.cc/2026/Conference/Submission14580/Reviewer_oZ3V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14580/Reviewer_oZ3V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938595752, "cdate": 1761938595752, "tmdate": 1762924965290, "mdate": 1762924965290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies geometric graph sparsification under the updates that changes a point in the set of input points. They give a sparsification algorithm whose update time is $n^{o(1)}$ and initialization time is $n^{1+o(1)}$. They also give a robust verison of their algorithm under some assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is in my understanding the first algorithm that provides sparisification of geometric graphs. Using standard techniques, they also extend it to Laplacian solver (though that part I did not find that interesting).\n\nThe writing of the paper is good and I could not any weakness or suggestions for the authors to improve their paper. I found it easy to follow, which is a good sign. \n\nI would say that I did not read the proof, but looking at the theorem statement, nothing came out that sounds it is incorrect. I hope the authors have done proper verification of their analysis."}, "weaknesses": {"value": "I did not find any."}, "questions": {"value": "One small quetion: On line 458, you meantion a small caveat: Why is it a problem that x is $n$-dimensional. If I understand correctly, $L_H$ is an n times n matrix, just with few entries. So, if x is sparse, that allows faster multiplication, but its random projection can make it dense and if you are projecting to low-dimensional space, I do not see how matrix-vector multiplication can be done!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uEZRxg8krY", "forum": "ZLJsJ8IWUz", "replyto": "ZLJsJ8IWUz", "signatures": ["ICLR.cc/2026/Conference/Submission14580/Reviewer_oqKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14580/Reviewer_oqKw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762432193014, "cdate": 1762432193014, "tmdate": 1762924964217, "mdate": 1762924964217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}