{"id": "5RATVAQGPx", "number": 15986, "cdate": 1758258106629, "mdate": 1759897268813, "content": {"title": "Jackpot: Align Actor-Policy Distribution for scalable and stable RL for LLM", "abstract": "Reinforcement learning (RL) has become an increasingly important paradigm for improving large language models (LLMs) on alignment, reasoning, and coding tasks, yet it remains extremely costly. The majority of training time is spent on rollouts. Allowing actor and policy distributions to differ could unlock substantial scalability and efficiency benefits, such as supporting large-batch or asynchronous training, and even enabling a lightweight rollout model. However, existing importance sampling–based corrections for distribution mismatch suffer from an inherent trade-off between stability and training performance. To tackle this problem, we propose Jackpot, which leverages Optimal Budget Rejection Sampling to directly reduce the gap between actor and policy distributions. For efficiency and stability in practical training, We introduce an efficient probability estimation strategy based on Top-$K$ logits with batch bias correction, and designs a stabilized Jackpot-PPO loss that jointly accounts for both the importance sampling ratio and the trust-region constraint in PPO. Empirically, our method achieves stable improvements in large-batch and asynchronous training, and in extreme off-policy training it substantially delays the onset of collapse and delivers competitive performance. Specifically, we achieve 20\\% improvement on AMC benchmarks and ~8\\% AIME benchmarks over the off-policy baseline under 128$\\times$ actor-policy update ratio for Qwen3-4B-Base and 64$\\times$ for Qwen3-8B-Base, while achieving greater stability and better performance than prior off-policy RL methods under extreme settings.", "tldr": "", "keywords": ["Large language models", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04204fdb67b57fbaad3a8f67732d7528179e13e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "JACKPOT targets, actor–policy mismatch, the PPO pain point in LLM RL. It fixes the problem at sampling time with Optimal-Budget Rejection Sampling. The naïve OBRS recipe is unusable at scale because it needs full-vocab normalizers, yields unbounded correction weights, and can kill throughput when alignment is too strong. The paper’s value is a practical pipeline: estimate OBRS on a top-k union of actor/policy tokens and calibrate the missing mass with the simple identity that the normalizer equals the expected acceptance rate; then train with a stabilized PPO objective that factorizes the correction and clips each ratio so gradients stay bounded and trust-region friendly. Empirically it demonstrates it shines at where vanilla PPO struggles, large rollout batches at fixed minibatch, and extreme off-policy collection, delivering steadier learning and better scores without changing the base model or reward loop."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Targets PPO’s pain points. It directly attacks actor–policy mismatch in the regimes that break PPO such as huge rollout batches with the same small minibatch and extreme off-policy actors, and shows steadier learning and better scores there.\n2 Not a naïve OBRS port. The paper surfaces why a straight plug-in fails (full-vocab normalizers, unbounded weights, acceptance-throughput collapse) and then fixes each with a practical trio: top-k union for feasibility, acceptance-rate calibration for unbiased scale, and a factorized, clipped correction that keeps updates stable.\n3. Clean narrative. The writing crisply motivates the problem, diagnoses naïve failures, and walks the reader through the recipe and its effects; the algorithm is easy to implement from the description."}, "weaknesses": {"value": "1. Loss clipping bias. The stabilized objective clips two likelihood ratios to keep gradients tame. That’s an intentional bias; it can underweight rare-but-informative tokens and shrink effective step size. The paper argues stability, but it does not quantify when the bias alters policy improvement or exploration.\n2. Actor–policy visitation skew. OBRS keeps tokens the current policy already likes, which shifts the behavior distribution. Alignment lowers variance, but it can also prune rare, hard states that matter for learning. Without coverage/diversity diagnostics, it is unclear whether OBRS quietly narrows what the policy ever sees.\n3. Sequence or block-level OBRS. Tokenwise rejection is simple, but it fragments credit on long sequences where decisions cohere over spans. Span- or block-level acceptance could improve temporal coherence at the same keep-rate (see questions below)."}, "questions": {"value": "1. Clipping bias: compare unclipped vs clipped vs two-sided clipped weights on identical runs; report policy-improvement proxies, gradient norms, collapse rate.\n2. Visitation skew: show state-coverage heatmaps before/after OBRS, effective sample size per update, entropy of the kept policy, and advantage-weighted coverage of rare regions.\n3. Top-k curve: plot performance, gradient norms, and normalizer error versus k; provide a simple k rule or a dynamic k policy tied to a target acceptance band.\n4. Potential KL \"double counting\"?: With OBRS plus the clipped reference ratio acting like a trust region, an explicit KL(pθ‖pref) may over-constrain updates. add curves with and without the explicit KL term to the reference under OBRS; clarify whether the second clipped ratio already suffices or if a reduced KL is still helpful.\n5. Curious to see block-level OBRS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PVxGvx1rB2", "forum": "5RATVAQGPx", "replyto": "5RATVAQGPx", "signatures": ["ICLR.cc/2026/Conference/Submission15986/Reviewer_k9JV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15986/Reviewer_k9JV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717818958, "cdate": 1761717818958, "tmdate": 1762926753809, "mdate": 1762926753809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "We thank all the reviewers [R1 (`ZoQc`)], [R2 (`BCWN`)], [R3 (`43dn`)], [R4 (`k9JV`)].  For their attention to our paper and their highly thoughtful and supportive feedback! We were glad that the reviewers found the work principled [`R1`, `R3`], with empirical promising results [`R1`, `R2`], theoretical guarantees [`R3`], tackling important and concrete problems [`R1`, `R3`, `R4`], and felt the findings are interesting [`R2`]. We are also glad to see that some reviewers find our presentation and narrative clean [`R4`]. \n\nIn order to make sure that we take the reviewers’ comments seriously, we rewrote and modified our paper according to their feedback. We have updated the paper to incorporate constructive suggestions, as shown in the revision. We summarize the major changes:\n\n**Hyper-parameter Robustness and Selection [`R1`, `R2`, `R3`, `R4`].**\n\nJackpot involves several important threshold and hyperparameter selections. We provide a detailed discussion of their role, selection, and sensitivity of the hyperparameters. We conduct comprehensive and large-scale hyperparameter ablation studies, demonstrating that our method is robust across a wide range of settings and generally insensitive to the selection of various hyperparameters. Additionally, we offer concrete recommendations for how to choose them in practice according to different user settings. $\\\\color{blue}{\\\\text{(Appendix C, p.g. 16-19)}}$\n\n**Implementation and Additional Computation Analysis [`R1`, `R2`, `R3`].**\n\nOur method can be seamlessly integrated into existing RL training pipelines and vLLM with minimal engineering effort. Additionally, our method DONOT require additional computation of log_probs and model inferences compared to standard PPO, since we show that we are reusing standard PPO computation outputs directly. In practice, the additional computation introduced by our approach amounts to only about 3% of a standard RL step, making the overhead negligible. Moreover, the total training time can be further reduced by leveraging large-batch training and performing rollouts with more efficient models, both of which significantly accelerate the overall workflow while maintaining performance.  $\\\\color{blue}{\\\\text{(Section 4.5, p.g. 7)}}$\n\n**Motivation and novelty of applying OBRS in RL [`R1`, `R2`].**\n\nWe discussed the motivation for using OBRS in Jackpot to reduce the distribution mismatch that arises in RL training.  Moreover, previous TIS-based methods suffer from a large distribution gap between actor and policy mismatch, as the importance ratio forces the trajectories sampled by the actor of low likelihood, causing a training-inference mismatch. Thus, the issue motivates our method that directly adjusts and modifies the actor’s sampling tokens. Additionally, we also clarified that our use of OBRS is complementary to prior methods, rather than a replacement, of importance-sampling–based corrections (e.g., TIS). $\\\\color{blue}{\\\\text{(Section 1, p.g. 1, Section 2.2, p.g. 3)}}$"}}, "id": "kOyrbjEbiA", "forum": "5RATVAQGPx", "replyto": "5RATVAQGPx", "signatures": ["ICLR.cc/2026/Conference/Submission15986/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15986/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15986/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684558586, "cdate": 1763684558586, "tmdate": 1763684558586, "mdate": 1763684558586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new reinforcement learning framework aimed at improving the stability and efficiency of RL-based fine-tuning for large language models. The central problem addressed is the distribution mismatch between the actor (rollout) model and the policy being optimized, which often arises in large-batch, asynchronous, or off-policy training scenarios. Existing importance sampling corrections for this mismatch tend to suffer from instability or poor performance. To tackle this, the authors propose JACKPOT, a method that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the KL divergence between the actor and policy distributions.\n\nEmpirical results on large-scale language models (Qwen3-4B and Qwen3-8B) demonstrate 20% improvement on AMC and 8% improvement on AIME benchmarks under extreme off-policy settings, as well as significant gains in large-batch asynchronous RL training. The method substantially delays training collapse and outperforms prior off-policy RL methods such as Truncated Importance Sampling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a concrete and important practical bottleneck: distribution mismatch between rollout models and the policy used for updates in RL for LLMs (large-batch, asynchronous, quantized/distilled rollouts, etc.). The motivation is clearly stated and practically relevant.\n2. The paper proposes using Optimal Budgeted Rejection Sampling to directly reduce actor–policy KL, which is a principled move. The paper provides theoretical guarantees: OBRS is the unique budget-optimal token-wise accept/reject rule minimizing KL for a given acceptance budget, and the post-rejection distribution monotonically approaches the target as the scale parameter varies. These formal results strengthen the core idea."}, "weaknesses": {"value": "1. The OBRS acceptance probability for a sampled token requires evaluating the ratio $p_{\\theta_{new}} (a) / p_{inf} (a)$. In practice, this implies scoring candidate tokens under the *current policy* during data collection or otherwise obtaining these probabilities. However, the paper does not fully quantify the extra compute this incurs in real serving setups. This is important because the whole motivation is improving throughput; any extra compute could reduce or eliminate the gains.\n2. The stabilized weight uses two clipped ratios and a stop-gradient on the alignment factor, trading bias for variance. The paper argues this is necessary for stability, but there is limited theoretical quantification of the bias introduced by clipping and the stop-gradient, nor diagnostics showing how much bias affects final performance in less extreme settings. More ablation (e.g., varying c1,c2, removing stop-grad) would help understand the tradeoffs.\n3. The final stabilized Jackpot objective (Section 4.4) is a pragmatic heuristic. The paper admits that it \"trades a small amount of bias (from clipping and approximation) for a massive reduction in variance\". This factorization and clipping move the method away from the clean theoretical guarantees of OBRS. The paper does not provide any theoretical analysis of how this specific, biased re-weighting scheme affects the optimization landscape, convergence, or the final policy."}, "questions": {"value": "1. In Algorithm 1, the acceptance probability $\\alpha$ requires access to $p_{\\theta_{new}}$ at sampling time. How is this computed in practice when rollouts are generated by a separate inference actor? Are logits from $p_{\\theta_{new}}$ queried synchronously (which could double inference cost), or are they approximated, cached, or delayed? Please clarify how this fits into a large distributed RLHF or asynchronous setup, and quantify the extra compute or communication cost.\n2. The stabilized Jackpot loss uses two clipped ratios and a stop-gradient. Could you formalize or empirically show how these choices affect gradient bias and variance? Specifically, how do c1,c2 and the stop-gradient operation trade off between bias and stability? It would help to include plots of gradient variance before and after stabilization.\n3. The method is validated primarily on mathematical benchmarks (AMC, AIME, GSM8K, MATH-500). Evaluating on alignment or preference-based tasks (e.g., Anthropic-HH) would help test generality and robustness beyond numeric reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eTV32rdSCa", "forum": "5RATVAQGPx", "replyto": "5RATVAQGPx", "signatures": ["ICLR.cc/2026/Conference/Submission15986/Reviewer_43dn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15986/Reviewer_43dn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914605485, "cdate": 1761914605485, "tmdate": 1762926193725, "mdate": 1762926193725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to reduce the high rollout cost from large trajectories in reinforcement learning based post-training of LLMs while improving stability and scalability. The key idea is to sample trajectories from a lightweight actor policy different from the main training policy to make RL more efficient. To handle the resulting mismatch, the paper introduces a rejection-sampling–based approach that directly reduces the distribution gap between actor and policy. It further proposes a Top-K approximation with bias correction and a stabilized PPO loss to maintain trust-region stability. Experimental ablations shows promising gain over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of using simple light-weight policy for sampling trajectories is interesting and effective especially for larger policies\n- bounding the KL and using rejection sampling helps in mitigating the distribution shift\n- efficient Top-K probability estimation keeps it usable for large vocabularies.\n- empirical performance are promising especially on AMC and AIME benchmarks compared to off-policy baselines."}, "weaknesses": {"value": "- Although interesting, the idea is extremely similar to several parallel streams including speculative decoding, on policy distillation, weak-strong etc and the key novelty of the approach is not clear\n- The paper ensures the KL divergence between the actor and policy distribution, however how it ensures closeness to the reference policy based on which standard RLHF policies are trained? How do you ensure closeness to that? Can you show a KL plot with the reference?\n- It requires sampling multiple trajectories and computing the ratio of the probabilities, how much time it incurs additionally during training?\n- Also, what kind of light-weight models can help to mitigate this shift in an efficient way - ie does compression or smaller finetuned models or distilled models, how are different models behaviour \n- How sensitive is the approach with the threshold and how is the threshold determined? That will also affect the number of trajectories to generate? It will be helpful to provide the details."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VlQCcr2qqV", "forum": "5RATVAQGPx", "replyto": "5RATVAQGPx", "signatures": ["ICLR.cc/2026/Conference/Submission15986/Reviewer_BCWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15986/Reviewer_BCWN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965292522, "cdate": 1761965292522, "tmdate": 1762926192841, "mdate": 1762926192841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational cost and instability arising from actor-policy distribution mismatch in reinforcement learning for large language models. While allowing these distributions to differ could enable significant efficiency gains—such as large-batch training, asynchronous updates, or using smaller rollout models—existing importance sampling-based corrections face a fundamental trade-off between stability and performance. The authors propose Jackpot, which leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the distributional gap between actor and policy networks. The method introduces three key contributions: an OBRS-based masking mechanism that maintains closer alignment between distributions, an efficient probability estimation strategy using Top-K logits with batch-wise bias correction to handle memory constraints, and a stabilized PPO loss jointly accounting for importance sampling ratios and trust-region constraints. Evaluated on large-batch training (128 mini-batches per rollout) and extreme off-policy scenarios, Jackpot demonstrates 20% improvement on AMC benchmarks and 8% on AIME benchmarks over baseline methods while maintaining stable training dynamics under severe distributional mismatch conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper tackles an important and practically relevant problem in reinforcement learning for LLMs: the gradient estimation bias and training instability caused by distribution mismatch between the actor and policy networks. \n\n- The proposed OBRS-based approach seems to be a principled solution to directly reduce this distributional gap.\n\n-  The empirical evaluation demonstrates promising results, particularly in maintaining training stability."}, "weaknesses": {"value": "- The paper lacks a detailed discussion of the differences between importance sampling ratio-based corrections and the rejection sampling approach, despite both relying on importance sampling ratio calculations. This makes it unclear why the optimal rejection sampling method provides advantages over existing correction techniques like TIS.\n\n- The paper builds upon Optimal Budgeted Rejection Sampling (OBRS) but does not provide sufficient introduction or justification for this choice. Without adequate background and motivation, it is difficult for readers to accept that OBRS is the appropriate approach for addressing the distribution mismatch problem.\n\n- The empirical evaluation lacks comprehensive ablation studies to disentangle the contributions of individual components. It remains unclear whether the OBRS-based masking mechanism, the Jackpot re-weighting strategy, or both components are essential for achieving the reported improvements in stability and performance."}, "questions": {"value": "- The LaTeX format of this paper does not appear to follow the standard ICLR submission template.\n\n- Does the Phase 1 Data Collection with OBRS require modifications to the sampling code in vLLM or other inference frameworks?\n\n- How should the hyperparameters $c_1$ and $c_2$ be chosen? Is there any guidance provided for their selection. Furthermore, is the method robust to different choices of these hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k2oDpMHNTC", "forum": "5RATVAQGPx", "replyto": "5RATVAQGPx", "signatures": ["ICLR.cc/2026/Conference/Submission15986/Reviewer_ZoQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15986/Reviewer_ZoQc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968653470, "cdate": 1761968653470, "tmdate": 1762926192381, "mdate": 1762926192381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}