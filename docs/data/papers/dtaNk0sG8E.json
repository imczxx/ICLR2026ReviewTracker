{"id": "dtaNk0sG8E", "number": 4221, "cdate": 1757641164421, "mdate": 1759898046622, "content": {"title": "What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?", "abstract": "Multimodal reasoning models have recently shown promise on challenging domains such as olympiad-level geometry, yet their evaluation remains dominated by aggregate accuracy, a single score that obscures where and how models are improving. We introduce MathLens, a benchmark designed to disentangle the subskills of multimodal reasoning while preserving the complexity of textbook-style geometry problems. The benchmark separates performance into three components: Perception: extracting information from raw inputs, Reasoning: operating on available information, and Integration: selecting relevant perceptual evidence and applying it within reasoning.\nTo support each test, we provide annotations: visual diagrams, textual descriptions to evaluate reasoning in isolation, controlled questions that require both modalities, and probes for fine-grained perceptual skills, all derived from symbolic specifications of the problems to ensure consistency and robustness. Our analysis reveals that different training approaches have uneven effects: First, reinforcement learning chiefly strengthens perception, especially when supported by textual supervision, while textual SFT indirectly improves perception through reflective reasoning. Second, reasoning improves only in tandem with perception. Third, integration remains the weakest capacity, with residual errors concentrated there once other skills advance. Finally, robustness diverges: RL improves consistency under diagram variation, whereas multimodal SFT reduces it through overfitting. We will release all data and experimental logs.", "tldr": "MathLens provides a disentangled benchmark that isolates perception,reasoning,and integration skills in multimodal mathematical problem solving.", "keywords": ["multimodal reasoning", "benchmark", "mathematical problem solving", "Multi-Modal Language Models (MLLMs)", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff9f117b60fccc47432373575c461202671d4b32.pdf", "supplementary_material": "/attachment/ebebf3badfab687e9f216da140077fbeafe00b30.zip"}, "replies": [{"content": {"summary": {"value": "They introduce MATHLENS, a benchmark designed to disentangle the subskills of multimodal reasoning while preserving the complexity of textbookstyle geometry problems. The benchmark separates performance into three components: Perception: extracting information from raw inputs, Reasoning: operating on available information, and Integration: selecting relevant perceptual evidence and applying it within reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written with demonstrative figures.\n2. The idea of separating the reasoning process of MLLMs is interesting.\n3. The analysis of the result of different methods is detailed and shines light on the inner workings of MLLM reasoning."}, "weaknesses": {"value": "1. The perception probes test for a finite set of atomic facts. A model might correctly identify all the probed facts but fail to perceive another crucial, un-probed visual detail. This perceptual failure would be misclassified as an integration failure.\n2. The primary benchmark, MATHLENS, is composed entirely of geometry problems. It cannot catch the full scope of MLLM reasoning. The add-on MATHLENS-GENERAL cannot maintain the same rigor."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "degCTTVlEN", "forum": "dtaNk0sG8E", "replyto": "dtaNk0sG8E", "signatures": ["ICLR.cc/2026/Conference/Submission4221/Reviewer_sN3K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4221/Reviewer_sN3K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760523305218, "cdate": 1760523305218, "tmdate": 1762917234793, "mdate": 1762917234793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem that aggregate accuracy metrics are insufficient for evaluating and understanding the progress of Multimodal Large Language Models (MLLMs) on complex reasoning tasks. By comparing a model's performance across these tests, the authors can decompose errors into failures of perception, reasoning, or Integration (defined as failing the joint task despite succeeding on perception and reasoning individually). The methodology also involves generating semantic variations of diagrams to test robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Error analysis reveals RL shifts errors to integration, providing evidence-based guidance for future training, unlike aggregate benchmarks.\n- Use of symbolic states ensures equivalence across modalities, supporting valid isolation of subskills with high diagnostic value.\n- The evaluation of robustness using semantic-level diagram modifications, rather than just pixel-level noise."}, "weaknesses": {"value": "- The primary analysis and all major findings are derived exclusively from the geometry domain. While this allows for tight control, it leaves the generalizability of the conclusions.\n- The accuracy of the error decomposition hinges on the assumption that the perception probes are exhaustive.\n- The paper presents many empirical comparisons but does not report confidence intervals or use statistical tests to confirm the significance of the observed differences."}, "questions": {"value": "- Regarding the error decomposition, how did you ensure the set of perception probes for each problem was comprehensive? Is it possible that some errors classified as \"integration\" failures are in fact subtle perceptual failures not captured by the probes?\n- What compute was used for evaluations, and how might nondeterminism affect API models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eJkAss2R4s", "forum": "dtaNk0sG8E", "replyto": "dtaNk0sG8E", "signatures": ["ICLR.cc/2026/Conference/Submission4221/Reviewer_atqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4221/Reviewer_atqw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377613475, "cdate": 1761377613475, "tmdate": 1762917234549, "mdate": 1762917234549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MATHLENS, a benchmark designed to disentangle the subskills of multimodal reasoning—specifically perception, reasoning, and their integration—in the context of geometry problems. \nThe authors argue that existing benchmarks often rely on aggregate accuracy, which obscures the specific capacities where models fail or improve. \nMATHLENS is built from symbolic specifications to generate aligned annotations, including visual diagrams, textual descriptions, perception probes, and multimodal questions, ensuring consistency and robustness. \nThrough controlled experiments on open multimodal models, the paper reveals that different training approaches\nhave uneven effects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MATHLENS provides a rigorous framework for decomposing multimodal reasoning into perception, reasoning, and integration, addressing a gap in existing evaluations. The use of symbolic semantics ensures controlled and reproducible annotations.\n2. The paper thoroughly evaluates multiple model families across diverse settings, including robustness tests with semantic diagram modifications. This allows for nuanced insights into how training strategies affect specific capacities.\n3. MATHLENS demonstrates strong alignment with datasets like MathVista and MathVerse, enhancing its credibility and utility for the community. The release of data and tools promotes reproducibility and further research."}, "weaknesses": {"value": "See Questions."}, "questions": {"value": "1. Have you explored preliminary strategies to improve integration? If so, what were the results? If not, what directions do you prioritize for future work?\n2. MATHLENS relies on geometry problems, which have well-defined symbolic representations. For tasks with ambiguous symbolic mappings, how would you adapt MATHLENS’s decomposition framework to ensure consistent evaluation of perception, reasoning, and integration?\n3. The diagram modifications test structural changes, but how would MATHLENS perform under real-world perturbations like occlusions or lighting variations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FFfAqigD7B", "forum": "dtaNk0sG8E", "replyto": "dtaNk0sG8E", "signatures": ["ICLR.cc/2026/Conference/Submission4221/Reviewer_q8XG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4221/Reviewer_q8XG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755953728, "cdate": 1761755953728, "tmdate": 1762917234284, "mdate": 1762917234284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a math benchmark to evaluate MLMs through different lenses rather than a single one. Specifically, they present and made publicly available a dataset called MathLens for geometry to perform evaluation based on three aspects: perception, reasoning and integration, enabling evaluation beyond a single score accuracy. Authors then did several fine-tuning analysis and made these main observations: (1) RL boosts perception, (2) textual SFT indirectly strengthens perception through reflective reasoning, (3) integration is the weakest skill among all, and finally (4) robustness varies (e.g., RL vs. SFT)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper is well-written and easy to follow. The core idea is interesting and novel.\n\n2. A public release of the dataset for the community is nice and will help future research.\n\n3. Extensive experiments and analyses were provided. For instance robustness analysis investigates changes to the diagram (e.g., rotation, etc.)."}, "weaknesses": {"value": "1. Single data source is just very limited -- only FormalGeo-7K was used as the basis of the data. This makes it hard to trust the findings IMO (e.g., relation between RL and SFT).\n\n2. The core analysis were performed using open-weight models and it's unclear how the fine-tuning of closed-source models such as Gemini would make any difference.\n\n3. Integration measurement is done indirectly -- this could be conflated with other latent failures, like context-length limitations."}, "questions": {"value": "1. As mentioned in the paper, integration is the main bottleneck in structured geometry. Is this true for real-world less-structured data as well? \n\n2. Since integration is the weakest skill, what training or architectural changes can be made to improve it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wCzj0Wn1Zr", "forum": "dtaNk0sG8E", "replyto": "dtaNk0sG8E", "signatures": ["ICLR.cc/2026/Conference/Submission4221/Reviewer_gFmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4221/Reviewer_gFmh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942135802, "cdate": 1761942135802, "tmdate": 1762917234096, "mdate": 1762917234096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a math benchmark to evaluate MLMs through different lenses rather than a single one. Specifically, they present and made publicly available a dataset called MathLens for geometry to perform evaluation based on three aspects: perception, reasoning and integration, enabling evaluation beyond a single score accuracy. Authors then did several fine-tuning analysis and made these main observations: (1) RL boosts perception, (2) textual SFT indirectly strengthens perception through reflective reasoning, (3) integration is the weakest skill among all, and finally (4) robustness varies (e.g., RL vs. SFT)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper is well-written and easy to follow. The core idea is interesting and novel.\n\n2. A public release of the dataset for the community is nice and will help future research.\n\n3. Extensive experiments and analyses were provided. For instance robustness analysis investigates changes to the diagram (e.g., rotation, etc.)."}, "weaknesses": {"value": "1. Single data source is just very limited -- only FormalGeo-7K was used as the basis of the data. This makes it hard to trust the findings IMO (e.g., relation between RL and SFT).\n\n2. The core analysis were performed using open-weight models and it's unclear how the fine-tuning of closed-source models such as Gemini would make any difference.\n\n3. Integration measurement is done indirectly -- this could be conflated with other latent failures, like context-length limitations."}, "questions": {"value": "1. As mentioned in the paper, integration is the main bottleneck in structured geometry. Is this true for real-world less-structured data as well? \n\n2. Since integration is the weakest skill, what training or architectural changes can be made to improve it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wCzj0Wn1Zr", "forum": "dtaNk0sG8E", "replyto": "dtaNk0sG8E", "signatures": ["ICLR.cc/2026/Conference/Submission4221/Reviewer_gFmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4221/Reviewer_gFmh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942135802, "cdate": 1761942135802, "tmdate": 1763399537022, "mdate": 1763399537022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}