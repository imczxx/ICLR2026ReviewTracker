{"id": "xfxmp1PZmV", "number": 23005, "cdate": 1758338029115, "mdate": 1759896836280, "content": {"title": "RAE: A Neural Network Dimensionality Reduction Method for Nearest Neighbors Preservation in Vector Search", "abstract": "While high-dimensional embedding vectors are being increasingly employed in various tasks like Retrieval-Augmented Generation and Recommendation Systems, popular dimensionality reduction (DR) methods such as PCA and UMAP have rarely been adopted for accelerating the retrieval process due to their inability of preserving the nearest neighbor (NN) relationship among vectors. Empowered by neural networks' optimization capability and the bounding effect of Rayleigh quotient, we propose a Regularized Auto-Encoder (RAE) for k-NN preserving dimensionality reduction. RAE constrains the network parameter variation through regularization terms, adjusting singular values to control embedding magnitude changes during reduction, thus preserving k-NN relationships. We provide a rigorous mathematical analysis demonstrating that regularization establishes an upper bound on the norm distortion rate of transformed vectors, thereby offering provable guarantees for k-NN preservation. With modest training overhead, RAE achieves superior k-NN recall compared to existing DR approaches while maintaining fast retrieval efficiency.", "tldr": "A neural network dimensionality reduction method that preserves k-NN relationships by controlling the upper bound, which is analyzed mathematically, of norm distortion rate through regularization.", "keywords": ["dimensionality reduction", "k-NN preserving", "neural network", "vector retrieval"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04c464239ae0ba53cfa0e5fbbb129e76abf1682d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper seeks to develop a dimensionality reduction method that preserves k-NN relationships while at the same time achieving strong compression for the vectors. They use an autoencoder, ie a reconstruction loss, together with a regularization strategy that favorstransformations that maintain neighborhood structures. The paper further contains a theoretical analysis that gives bounds on how well the reduced vectors maintain neighbor relationships, with the upper bound controlled by coefficient $\\lambda$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written and has a easy to follow theoretical analysis. The linear encoder trained is some times better for classification than PCA"}, "weaknesses": {"value": "1) Experimental validation lacking for a top-tier paper: The datasets tested are generally small scale: The ImageNet-Tiny dataset used is a 10-way classification task. Showing results on more common/larger datasets would strengthen the paper (eg ImageNet-1k, Oxford+Paris datasets with distractors, Google landmarks)\n\n2) The presentation of the method in the introduction is misleading:  To me the learning method does not __explicitly__ preserves k-NN relationships (as the paper claims). The training objective is an autoencoder with Frobernius norm regularization, while from the analysis in sec 3 it is clear that, kNN preservation is very implicit.\n\n3) Discussion and comparisons to closely related works like [A,B] that are utilizing contrastive learning objectives are missing. \n[A] Dimensionality reduction by learning an invariant mapping. (CVPR, 2006). \n[B] TLDR: Twin Learning for Dimensionality Reduction (TMLR 2022). \n\n4) Gains over PCA are very small, and in fact PCA is better in many cases.\n\nOther notes:\n- Definition 1 \"k-NN Preservation Task\" does not seem to present some task.\n- The tables should name ImageNet-Tiny with its full name."}, "questions": {"value": "1. How does the method **explicitly** preserves k-NN relationships, as is claimed in the introduction? Can you explain how \"RAE directly targets the preservation of relative distances between vectors and their neighbors through a carefully crafted optimization objective.\"\n\n2. Can this be generalized to non linear transformations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RhpLJF2Tx1", "forum": "xfxmp1PZmV", "replyto": "xfxmp1PZmV", "signatures": ["ICLR.cc/2026/Conference/Submission23005/Reviewer_Scie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23005/Reviewer_Scie"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649191554, "cdate": 1761649191554, "tmdate": 1762942473201, "mdate": 1762942473201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the issue of compressing feature dimensions to improve retrieval efficiency and proposes RAE, a neural network-based feature dimensionality reduction method. During the training process, a Frobenius norm is introduced as a regularization constraint to compress features while preserving the KNN relationships in the high-dimensional space, thereby ensuring retrieval accuracy. In the experimental section, RAE is compared with methods such as PCA, and it demonstrates certain advantages in some scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1)The research problem has strong practical application value.\n2)The proposed method is supported by certain theoretical proofs."}, "weaknesses": {"value": "1）The advantages of the proposed method are very marginal. According to Table 1, the improvement of RAE over PCA (a classic algorithm from many years ago) is limited. On ImageNet, the improvements are all around 0.5 percentage points, and only in Top-5 accuracy. On CelebA, it performs on par with PCA, while in the Euclidean experiment, it is significantly weaker than PCA. Similar situations are observed on IMDb and Flickr30K. Compared to PCA, RAE also requires tuning hyperparameters, which severely impacts its performance.\n2）Using neural networks for feature compression is not a particularly novel idea, but the authors did not compare their work with such existing methods at all. Although the paper provides some explanations, they are insufficient to convince the reviewers.\n3）The scale of the datasets used falls short of the currently promoted billion-scale retrieval benchmarks."}, "questions": {"value": "Please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nHnQyXksi1", "forum": "xfxmp1PZmV", "replyto": "xfxmp1PZmV", "signatures": ["ICLR.cc/2026/Conference/Submission23005/Reviewer_6rBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23005/Reviewer_6rBo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659743476, "cdate": 1761659743476, "tmdate": 1762942472913, "mdate": 1762942472913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a dimensionality reduction technique called RAE. RAE is a linear autoencoder with a Frobenius norm regularization."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Given the nature of the weaknesses described below, I do not find any clear strengths in the paper. The proposed method has a flaw, that unless properly explained, is fatal in my opinion. Moreover, there are other dimensionality reduction methods beyond PCA that should be included in the study."}, "weaknesses": {"value": "The main equation of the paper, Equation (7), is not properly written. Are the authors constraining W_e, W_d, their product, or something else? The matrix W is never properly defined. I would assume from the following discussion that the authors are regularizing W_e. Moreover, this equation cannot be the full training objective, but that one for a specific vector in the dataset.\n\nThe fact that a linear projection into a lower dimensional space has bounded distortions, given by the ratios of the singular values of the matrix is well known in linear algebra and not a new discovery.\n\nMaybe more importantly, the optimal solution to the problem\n$$\nmin_{W_d, W_e} || W_d W_e X - X ||_2^2 ,\n$$\nwhere the columns of X are the data vectors, is given by the singular vectors SVD of the X associated with the top singular values of X. Assuming a hard constraint on W_e, we can set up the new problem\n\n$$\nmin_{W_d, W_e} || W_d W_e X - X ||_2^2  \\quad s.t. \\quad ||W_e||_F^2 < \\epsilon .\n$$\n\nThe solution to this problem will still be the same one as the above problem, but W_e and W_d would be scaled down and up by a scalar, respectively. Since the hard constraint problem is equivalent to (a properly written version of) Equation (7) for some \\lambda, it seems like the authors are re-inventing PCA but with a scalar factor that changes nothing of the overall structure of the problem. From this perspective, it seems like any differences with PCA in the results are just because of optimization used by the authors to compute W_d and W_e. In my mind, unless properly explained, this is a fatal flaw in this work.\n\nThere are previous uses of dimensionality reduction for vector search that the authors should acknowledge and probably compare to. I leave some examples for reference below.\n\n- Jegou, H., Douze, M., Schmid, C. and Perez, P. (2010), Aggregating local descriptors into a compact image\nrepresentation, in ‘IEEE Conference on Computer Vision and Pattern Recognition’, IEEE, pp. 3304–3311.\n\n- Gong, Y., Lazebnik, S., Gordo, A. and Perronnin, F. (2012), ‘Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval’, IEEE transactions on pattern analysis and machine intelligence 35(12), 2916–2929.\n\n- Babenko, A. and Lempitsky, V. (2014b), ‘The inverted multi-index’, IEEE transactions on pattern analysis and machine intelligence 37(6), 1247–1260.\n\n- Wei, B., Guan, T. and Yu, J. (2014), ‘Projected residual vector quantization for ANN search’, IEEE MultiMedia 21(3), 41–51.\n\n- Zhang, H., Tang, B., Hu, W. and Wang, X. (2022), Connecting compression spaces with transformer for approximate nearest neighbor search, in ‘European Conference on Computer Vision’, pp. 515–530.\n\n- Tepper, M., Bhati, I. S., Aguerrebere, C., Hildebrand, M., & Willke, T. L. LeanVec: Searching vectors faster by making them fit. Transactions on Machine Learning Research."}, "questions": {"value": "Please refer to my remarks above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "To4e3BD0Zk", "forum": "xfxmp1PZmV", "replyto": "xfxmp1PZmV", "signatures": ["ICLR.cc/2026/Conference/Submission23005/Reviewer_MoJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23005/Reviewer_MoJ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999653530, "cdate": 1761999653530, "tmdate": 1762942472709, "mdate": 1762942472709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a dimensionality reduction technique with the goal of preserving nearest neighbors. Given a set of embedding vectors the goal is to project them to a lower dimensional space such their k-NN neighbors are preserved. This is achieved using a linear projection from an n to m dimensional space, and the linear projection is learned by minimizing a reconstruction loss with a regularization term.\n\nThe k-NN neighbors are preserved by virtue of preserving the pairwise distances between the vectors. Experimental results show that RAE performs better than other traditional methods including MDS, ISOMAP, UMAP, and PCA."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is very well presented and the core concepts and ideas are clear. The mathematical details seem to be correct and reasonably detailed.\n- Experimental results show that the method is more accurate compared to prior comparable baselines."}, "weaknesses": {"value": "- Linear dimensionality reduction methods are a well studied topic and as such the novelty of this paper is unclear. The method presented here seems to be very similar to the already existing PCA.\n- A higher dimensional vector of dimension n is first projected into a low dimensional subspace of dimension m and projected back to dimension n. This process essentially projects the n dimensional vector in a row-rank subspace of dimension n, which is closely related to PCA. PCA is essentially an orthogonal projection to a low rank subspace and is equivalent to minimizing the reconstruction error between the original and projected vectors. The only difference between the proposed method and PCA seems to be the regularization term.\n- The utility of the method is unclear. In the case of retrieval and search using neural embeddings, why would one not train low dimensional vectors directly or perhaps use Matryoshka style embeddings which natively provide low dimensional embeddings for various use cases."}, "questions": {"value": "- Without the regularization term, the method seems to be closely related to PCA. With the regularization term is it similar to probabilistic PCA with the regularization term being similar to prior on the weights? It is not exactly the same, but perhaps similar?\n- Does minimizing the loss in Eq. 7 without the regularization term, essentially give us PCA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AMG4YcfUR8", "forum": "xfxmp1PZmV", "replyto": "xfxmp1PZmV", "signatures": ["ICLR.cc/2026/Conference/Submission23005/Reviewer_YVfq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23005/Reviewer_YVfq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037536025, "cdate": 1762037536025, "tmdate": 1762942472436, "mdate": 1762942472436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}