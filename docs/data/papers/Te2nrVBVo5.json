{"id": "Te2nrVBVo5", "number": 18455, "cdate": 1758287963296, "mdate": 1759897102389, "content": {"title": "Interpreting Emergent Military Tactics in a General AlphaZero Framework", "abstract": "This paper presents an approach that combines AlphaZero  with convolutional  and transformer-based  neural network architectures to learn strategies in battlefield-inspired gridworld games. These games are designed to balance realism with rapid outcomes, featuring multiple agents organized into competing teams. To encourage effective coordination among agents, we investigate different reward shaping methods and evaluate their impact on emergent teamwork. The learned strategies are analyzed on a tactical level, in an attempt to reveal insights into multi-agent collaboration and competitive behavior. In particular, the framework provides a testbed for studying how military-style strategies can emerge from self-play. Through a series of comparative studies, we further break down the contributions of architectural components and training methodologies to demonstrate the effectiveness of this approach for decision-making in dynamic adversarial settings.", "tldr": "We use AlphaZero to discover military tactics for battlefield scenario's", "keywords": ["reinforcement learning", "alphazero", "military"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f8ba90047512a69357b538ddb3a743977236a0d.pdf", "supplementary_material": "/attachment/ffa602e19ce1600d8a46c3fdd44d97e3e9558006.zip"}, "replies": [{"content": {"summary": {"value": "The authors describe a new multi-agent reinforcement learning environment designed to mimic military tactics. The environment simulates real-world battlefield constraints like ammunition, weapon range etc. The environment is clearly described in the paper along with the game play and state representation. Experiments on the environment were performed investigating different policy architecture, reward shaping etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The environment is very clearly defined and there are potentials of establishing as a new environment.\nEffects of different training factors like reward shaping are thorough"}, "weaknesses": {"value": "The paper lacks novelty in terms of algorithm or modeling improvements.\nTechniques described in the paper are very well established in the literature.\n\nThe environment could potentially be beneficial in the military space, there are limit novelty as a benchmark.\nThere were limited comparisons with existing multi-agent test benches like Google Research Football.\nFurthermore, even the SMAC seems to be more complex and diverse compared to the test bench described."}, "questions": {"value": "* Instead of randomly assigning players' agent on rows, could there be specific scenarios? This can test the generalizations of the policies\n* Have you compared with simple baselines like DQN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DY4y730toL", "forum": "Te2nrVBVo5", "replyto": "Te2nrVBVo5", "signatures": ["ICLR.cc/2026/Conference/Submission18455/Reviewer_Wo4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18455/Reviewer_Wo4W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867813848, "cdate": 1761867813848, "tmdate": 1762928152960, "mdate": 1762928152960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new battlefield gridworld environment and test AlphaZero-inspired automated game playing approaches in it. The authors present the environment and then test the AlphaZero-inspired approaches. They find that they largely work. The authors present a great deal of outputs to characterize the performance in this environment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The primary strength is the new environment. This is fairly novel though there are a number of similar environments, such as MicroRTS and Generals.io. However, there is no originality to the AI approaches presented in the paper. The quality of the work is standard, this is the typical approach any researchers would take to applying these approaches to a new environment. The clarity of the work is very high in terms of both the environment and the AlphaZero implementation. The significance is unfortunately low, as there are similar existing environments and there aren't any surprising results for AlphaZero-like approaches in such environments."}, "weaknesses": {"value": "The fundamental problem with this work is that the contributions are not relevant to an AI venue. There is a great deal of work on automated game playing for similar environments [1,2,3]. None of the approaches applied in this environment are novel in the literature. \n\nTo make this work relevant to an AI venue I'd recommend that the authors consider an analysis that compares their environment to similar existing environments to clarify how they are similar and different, and present experiments to support these claims. \n\n1. Bhatia, Aaditya, et al. \"Generally Genius: A Generals. io Agent Development and Data Collection Framework.\" Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. Vol. 19. No. 1. 2023.\n2. Straka, Matej, and Martin Schmid. \"Artificial Generals Intelligence: Mastering Generals. io with Reinforcement Learning.\" arXiv preprint arXiv:2507.06825 (2025).\n3. Ontañón, Santiago, et al. \"The first microrts artificial intelligence competition.\" AI Magazine 39.1 (2018): 75-83."}, "questions": {"value": "1. How do the authors relate their environment to similar environments like MicroRTS and Generals.io?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QW1krKH6zX", "forum": "Te2nrVBVo5", "replyto": "Te2nrVBVo5", "signatures": ["ICLR.cc/2026/Conference/Submission18455/Reviewer_ZwTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18455/Reviewer_ZwTf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964388197, "cdate": 1761964388197, "tmdate": 1762928152601, "mdate": 1762928152601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on applying self-play and MCTS-based RL methods, previously proposed and successfully applied in Alpha-zero (Silver et. al., 2017), in a grid-world setting. The authors particularly investigated the impact of reward-shaping, model acceptance scheme, and network architecture on the performance of the learned agent."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The Battlefield Game environment could serve as an additional benchmark for multi-agent or MCTS-based research. Overall, the provided source code runs and includes proper docstrings and type hints, albeit with incomplete coverage."}, "weaknesses": {"value": "In my opinion, the paper proposes no novelty. This work instead serves as a toy(-ish) example of MCTS and self-play approaches.\n- The lack of standard benchmarks in the empirical studies (even if the goal is to train an agent solely in the proposed environment) makes the analysis of network architectures (e.g., Transformers vs. CNNs) and reward shaping methods hard to interpret and impossible to compare with prior work; as a result, the conclusions are largely anecdotal and lack external validity."}, "questions": {"value": "- Were the agents on each team run in a decentralized manner?\n- (Minor) Figure 1 is difficult to read."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "I am concerned about the target application of the paper. On multiple occasions, the authors discuss the implications of the paper’s observations for a military setting. I am not sure whether this is considered an ethical issue, but I felt the need to flag it."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hZy7O5mg5Z", "forum": "Te2nrVBVo5", "replyto": "Te2nrVBVo5", "signatures": ["ICLR.cc/2026/Conference/Submission18455/Reviewer_T7VF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18455/Reviewer_T7VF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006321546, "cdate": 1762006321546, "tmdate": 1762928152245, "mdate": 1762928152245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adaptation of the AlphaZero framework to a battlefield gridworld environment, aiming to study the emergence of military-style tactics via self-play reinforcement learning. \nThe authors design a simplified environment featuring limited ammunition, shrinking safe zones, and center-control objectives. \nBased on this, this work investigates the effects of different reward shaping terms and model architectures, and analyze the emergent tactical behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper clearly explains the AlphaZero training pipeline, state representation, and reward shaping mechanism.\n2. As a small-scale testbed, it could be useful for education or for preliminary exploration of the generation of gaming behavior."}, "weaknesses": {"value": "1. The entire methodology is a direct and unmodified (or small modidified) application of AlphaZero to a small toy gridworld. Common techniques such as reward shaping and acceptance threshold tuning are employed, but the paper introduces no novel algorithmic mechanism, theoretical insight, or analytical framework.\n2. There are no baseline comparisons with established MARL frameworks. Moreover, no statistical analysis, ablation, or environmental diversity is presented, and all experiments are restricted to a single 7×7 gridworld.\n3. Overall, the paper reads more like an engineering report documenting the implementation of AlphaZero with minor adjustments, rather than a research contribution offering new understanding or advancement."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wSvYkob9au", "forum": "Te2nrVBVo5", "replyto": "Te2nrVBVo5", "signatures": ["ICLR.cc/2026/Conference/Submission18455/Reviewer_fXEi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18455/Reviewer_fXEi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040416055, "cdate": 1762040416055, "tmdate": 1762928151862, "mdate": 1762928151862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}