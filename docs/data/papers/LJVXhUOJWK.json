{"id": "LJVXhUOJWK", "number": 16298, "cdate": 1758262905101, "mdate": 1759897249287, "content": {"title": "ADSS: Boosting Text-to-Image Diffusion Models via Attention-Driven Seed Selection", "abstract": "Text-to-image diffusion models can synthesize high-quality images, yet the outcome is notoriously sensitive to the random seed: different initial seeds often yield large variations in image quality and prompt--image alignment. We revisit this ``seed effect'' and show that early-stage attention dynamics over prompt core tokens---the content-bearing words---strongly predict final generation quality. Building on this observation, we introduce ADSS—Attention-Driven Seed Selection—a training-free, plug-and-play method that tracks cross-attention to core tokens during sampling to rank and select seeds for a fixed prompt, requiring no finetuning or latent changes and globally ranking the entire seed pool rather than using a fixed threshold. Since it operates purely at inference time, ADSS can also serve as a lightweight add-on preselection step before existing seed-optimization pipelines, enabling additional gains without extra training or code changes. Extensive experiments on three benchmarks show consistent improvements in prompt faithfulness and visual quality across Stable Diffusion variants, as reflected by human preference and alignment metrics. Our results highlight ADSS as a simple and effective route to more controllable generation by leveraging prompt core token attention for robust seed preselection.", "tldr": "", "keywords": ["Text to Image Diffusion Model; Seed selection"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ee4fbe5d095fa3a7957663dffc72de486509edf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Attention-Driven Seed Selection (ADSS), an inference-time procedure for improving the output of text-to-image diffusion models by seed selection. The core idea is to pre-screen a large pool of random seeds by running only the initial steps of the denoising process. The method identifies the most promising seeds by measuring the cross-attention scores on \"body tokens\". Seeds that exhibit higher attention on these tokens early on are selected for the full generation process, leading to a curated set of high-quality outputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The primary contribution of this work is the valuable insight that early-stage cross-attention dynamics on core semantic tokens are a strong predictor of final image quality and prompt alignment. This observation provides a simple yet effective mechanism for filtering out \"bad seeds\" without requiring model retraining or complex optimization pipelines.\n- The writing is clear and the methodology is well-explained."}, "weaknesses": {"value": "- The paper's methodology heavily relies on the accurate identification of \"body tokens,\" yet the process for selecting them is not described. For ADSS to be a truly lightweight and automated method, this selection process is critical. The omission of these details is a significant gap in the paper, hindering reproducibility and a full assessment of the method's overhead.\n- The experiments are conducted on Stable Diffusion 1.x and 2.x variants. While these models are foundational, they are no longer state-of-the-art. The absence of evaluation on more current and capable models makes it difficult to assess the relevance and effectiveness of ADSS.\n- ADSS requires an initial pool of seeds that is much larger than the desired number of final images (e.g., screening 100 seeds to select 50). In practical applications, users are often constrained by hardware and can only generate a small batch of images at a time (e.g., 4 or 8). This raises questions about the method's utility in such restricted scenario.\n- The ablation in Table 3 compares selecting body tokens to adjectives or verbs. However, it lacks baselines such as random seed selection and selecting seeds based on high attention to a randomly chosen subset of tokens. Without this, it is unclear whether the benefit comes specifically from focusing on body tokens or if simply prioritizing any consistent set of tokens in the early stages would yield an improvement over these baselines."}, "questions": {"value": "1. Could you please elaborate on the procedure used to identify \"body tokens\" from a given prompt? Is this a manual or automated process? If automated, what is the associated computational cost, and how critical is the accuracy of this step to the overall performance of ADSS?\n2. The practicality of ADSS seems dependent on the size of the initial seed pool. Have you investigated how performance over the random baseline changes with a smaller initial pool (e.g., screening 8 seeds to select 4)?\n3. In Table 1, the HPS score for ADSS on DrawBench with SD 1.4 is 0.2481, and the IR score is -0.1774. In Table 3, the scores for ADSSbody (the default) are 0.2473 (HPS) and -0.2195 (IR). Could you clarify the reason for this discrepancy in the results for the same method, model, and dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m0cr4D1SbH", "forum": "LJVXhUOJWK", "replyto": "LJVXhUOJWK", "signatures": ["ICLR.cc/2026/Conference/Submission16298/Reviewer_Xf9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16298/Reviewer_Xf9p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298535846, "cdate": 1761298535846, "tmdate": 1762926440399, "mdate": 1762926440399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ADSS (Attention-Driven Seed Selection), a novel, training-free, and plug-and-play inference method. ADSS is based on the key insight that the dynamics of the cross-attention mechanism over core tokens (the content-bearing words in the prompt) during the early stages of sampling strongly predict the final image quality. The method works by running a candidate pool of seeds for a few initial steps, tracking the cross-attention values to the core tokens, and then selecting the seed that ranks highest based on this attention dynamic. Experiments on multiple benchmarks and Stable Diffusion variants demonstrate consistent improvements in prompt faithfulness and visual quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. ADSS is a purely inference-time method, making it exceptionally easy to integrate into existing pipelines without expensive fine-tuning or structural model changes.\n2. Unlike methods that rely on fixing specific golden seeds or using a threshold, ADSS globally ranks a pool of candidate seeds for a given prompt, allowing a principled selection of the best available option.\n3. The method is shown to serve as an effective pre-selection step before more complex seed-optimization pipelines, suggesting it provides orthogonal improvements."}, "weaknesses": {"value": "1. Although the method is training-free, it requires running the generation pipeline for multiple steps (to track attention) for every candidate seed in the pool before one is selected. This computational overhead, especially if a large seed pool is required, must be quantified, and compared rigorously against the cost of a single standard generation run.\n2. The paper fails to explicitly define the mechanism used to identify core tokens (the content-bearing words). Is this a heuristic, a Part-of-Speech (POS) tagger, or an additional LLM module? The effectiveness and plug-and-play nature of ADSS are critically dependent on the robustness of this front-end step.\n3. The core mechanism relies on early-stage attention dynamics. There is no detailed ablation showing which specific timestep(s) or range of steps are optimal for attention tracking. This key hyperparameter is likely model- or sampler-dependent and needs empirical justification.\n4. While tested on Stable Diffusion variants, the method's reliance on cross-attention to text tokens may be specific to the UNet/CLIP architecture. Its generalizability to modern Diffusion Transformer (DiT) models is not discussed and needs clarification.\n5. The performance gains rely entirely on the premise that a good seed exists within the sampled pool. The paper does not analyze the required size of the initial seed pool for achieving consistent quality gain (e.g., if a pool of 5 is used vs. 50), which is crucial for practical deployment.\n6. ADSS is positioned as a pre-selection step for seed-optimization pipelines. However, the paper does not show a direct, head-to-head comparison to complex, but highly effective, full optimization techniques (e.g., reward-based search) to justify its stand-alone cost-benefit ratio."}, "questions": {"value": "There are some areas in this paper that need improvement, which are provided in the weakness section. My primary concerns are about reproducibility and cost-benefit analysis:\n1. A quantitative analysis of the computational overhead (e.g., time or FLOPs) required by ADSS versus a single standard generation, for various seed pool sizes.\n2. A formal definition and ablation of the core token identification process.\n3. An ablation study that rigorously tests the optimal range of timesteps for attention tracking across different samplers and models, justifying the early-stage heuristic.\n4. A discussion or ablation on model generality regarding MM-DiT architectures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "by2qNNiYtr", "forum": "LJVXhUOJWK", "replyto": "LJVXhUOJWK", "signatures": ["ICLR.cc/2026/Conference/Submission16298/Reviewer_ssYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16298/Reviewer_ssYw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381733525, "cdate": 1761381733525, "tmdate": 1762926440067, "mdate": 1762926440067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ADSS (Attention-Driven Seed Selection), a training-free, plug-and-play method to improve text-to-image diffusion models by addressing the well-known \"seed effect,\" where different random seeds can lead to large variations in image quality and prompt alignment. ADSS works by monitoring early-stage cross-attention to core tokens in the prompt during the denoising process, using this signal to rank and select the most promising seeds for a given prompt. Unlike previous seed selection or optimization methods, ADSS requires no additional training, fine-tuning, or external datasets, and operates entirely at inference time. Experiments across multiple Stable Diffusion variants and benchmarks show that ADSS consistently improves prompt faithfulness and visual quality to some extent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ ADSS requires no model retraining or fine-tuning, making it easy to integrate into existing pipelines.\n\n+ Experiments across multiple Stable Diffusion variants and benchmarks show that ADSS consistently improves prompt alignment and image quality.\n\n+ The proposed method operates at inference time with minimal computational overhead, enabling early-stage seed selection.\nGeneralizable: Can be combined with other seed optimization or refinement methods for additive improvements."}, "weaknesses": {"value": "- ADSS heaviliy eelies on early attention signals, resulting in its effectiveness may depend on the reliability of early-stage cross-attention as a predictor, which could vary with prompt complexity or model architecture. Especially for recent methods, LLM prompt rewrite/expansion is a commonly used strategy which will transfer short input prompt to detailed long prompt as input to the model. It is not clear how good ADSS can be applied to those cases with detailed prompt. \n\n- Improvements are marginal over strong baselines e.g. Golden Seeds, especially for highly compositional or diverse prompts.\n\n- ADSS requires multiple generations per prompt and assumes access to a pool of candidate seeds, which may increase inference cost in some settings.\n\n- The proposed method is only limited to seed selection and it does not address other sources of generation errors beyond seed sensitivity."}, "questions": {"value": "Please refer to the detailed questions raised in Weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lLAIKaasKR", "forum": "LJVXhUOJWK", "replyto": "LJVXhUOJWK", "signatures": ["ICLR.cc/2026/Conference/Submission16298/Reviewer_wWrj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16298/Reviewer_wWrj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985097533, "cdate": 1761985097533, "tmdate": 1762926439669, "mdate": 1762926439669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}