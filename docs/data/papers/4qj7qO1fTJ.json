{"id": "4qj7qO1fTJ", "number": 21969, "cdate": 1758324223950, "mdate": 1763712025913, "content": {"title": "BottleneckMLP: Graph Explanation via Implicit Information Bottleneck", "abstract": "The success of Graph Neural Networks (GNNs) in modeling unstructured data has heightened the demand for explainable AI (XAI) methods that provide transparent, interpretable rationales for their predictions. A prominent line of work leverages the Information Bottleneck (IB) principle, which frames explanation as optimizing for representations that maximize predictive information $I(Z;Y)$ while minimizing input dependence $I(X;Z)$. We show that explicit IB-based losses in GNN explainers provide little benefit beyond standard training: the fitting and compression phases of IB emerge naturally, whereas the variational bounds used in explicit objectives are too loose to meaningfully constrain mutual information. To address this, we propose BottleneckMLP, a simple architectural module that implicitly enforces the IB principle. By injecting Gaussian noise inversely scaled by node importance, followed by architectural compression, BottleneckMLP amplifies the reduction of $I(X;Z)$ while increasing $I(Z;Y)$. This yields embeddings where important nodes remain structured and clustered, while unimportant nodes drift toward Gaussianized, high-entropy distributions, consistent with progressive information loss under IB. BottleneckMLP integrates seamlessly with current explainers, as well as subgraph recognition tasks, replacing explicit IB terms and consistently improving predictive performance and explanation quality across diverse datasets.", "tldr": "BottleneckMLP is a general module which implicitly enforces IB, effectively replacing explicit IB loss terms in existing ante-hoc graph explanation frameworks.", "keywords": ["Graph Neural Networks", "Explainability", "Information Bottleneck", "Mutual Information", "Representation Learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6975704b4ccd4e1831892e61eaa7275cc42ba5cb.pdf", "supplementary_material": "/attachment/156fc62ebaf3e5f04c85a6e4102827fc97e1c469.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new architectural approach to improve the interpretability of Graph Neural Networks (GNNs) through implicit enforcement of the Information Bottleneck (IB) principle, without relying on explicit IB loss terms. Although the paper’s title sounds broad, the actual problem scope is much narrower, where they only focus on improving Ante-hoc and IB-based GNNExplainablity. Their goal is to show that explicit IB is unnecessary even within IB-based ante-hoc frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Their experiments demonstrate that adding BottleneckMLP often outperforms using explicit IB loss terms in existing ante-hoc explainers (GSAT, PGIB, TGIB) across multiple datasets. The results are effective. \n2. They provide analysis showing how unimportant node embeddings are pushed toward Gaussian, high-entropy distributions (i.e. “forgetting”), while important nodes remain structured, thereby aligning with the IB principle in a principled manner."}, "weaknesses": {"value": "1. The claim that “a prominent line of work leverages the IB principle” is overclaim. IB-based explainers represent only a small subset of existing methods, and most state-of-the-art explainers (e.g., SubgraphX, GOAt, ReFine, PGExplainer++) do not rely on IB and typically perform better. \n2. By restricting comparisons only to IB-related baselines (GSAT, PGIB, TGIB) and including only one outdated non-IB baseline (PGExplainer, 2020), the experimental validation becomes narrow and unconvincing. Broader comparison with recent non-IB explainers is necessary to justify the contribution. \n3. Explaining temporal GNNs is within their scope, but the evaluation is incomplete. They do not clearly justify the pros and cons of applying IB to TGNNs, and they fail to compare with known temporal explainers like T-GNNExplainer.\n4. They compare only to IB-based explainers, but they should also include other ante-hoc GNN explainers that are not IB-based."}, "questions": {"value": "Why limit baseline to GCN? Why not test on GIN, which is stronger than GCN in many graph tasks? Their experimental setup seems biased by weak baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UWWCvct1QW", "forum": "4qj7qO1fTJ", "replyto": "4qj7qO1fTJ", "signatures": ["ICLR.cc/2026/Conference/Submission21969/Reviewer_Bvh8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21969/Reviewer_Bvh8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760574824957, "cdate": 1760574824957, "tmdate": 1762942002979, "mdate": 1762942002979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the limitations of explicit Information Bottleneck objectives in GNN explainers and proposes a novel architectural module, BottleneckMLP, that implicitly enforces the IB without relying on auxiliary IB loss terms. First, the paper demonstrates that the existing explict IB losses are difficult to satisfy the i.i.d. assumption due to the structural dependencies inherent in graph data. Second, the paper achieves implicit extraction of critical information through importance-weighted Gaussian noise injection and additional MLP layers. Finally, the effectiveness of the proposed module is validated on both real-world and synthetic graph datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper demonstrates that the existing explict IB losses are difficult to satisfy the i.i.d. assumption due to the structural dependencies inherent in graph data.\n2. The paper proposes achieving implicit IB process through weighted noise injection and MLP layers, with certain theoretical guarantees.\n3. The effectiveness of the proposed module is validated on both real-world and synthetic graph datasets."}, "weaknesses": {"value": "1.\tFigure 1 does not illustrate the crucial step that node embeddings are perturbed based on importance weights, the key question is whether the MLP in the subgraph extractor module and the MLP used to predict node importance weights share parameters and why.\n2.\tIn experiment, Tables 1 and 2 lack validation on a broader range of real-world or synthetic datasets, as well as AUC/ROC of explanation subgraph, For example, on datasets such as Alkane-Carbonyl, Fluoride-Carbonyl, and Benzene from [1] (\"Evaluating attribution for graph neural networks\"), among others.\n3.\tThe hyperparameter \\sigma in Eq(6) also plays a crucial role in noise injection, should an additional parameter sensitivity analysis  be included in the experimental section?\n4.\tIn presentation, the text in all figures is too small and causes some difficulty in reading.\n5.\tAdding an efficiency analysis and a detailed algorithmic description would make the method clearer in terms of computational efficiency and implementation details (or alternatively, providing concrete code implementation would be helpful)."}, "questions": {"value": "1. As shown in Figure 2(b), why does the CE loss alone for GNNs not exhibit the two distinct phases  similar to that in DNNs? and why are additional MLP layers introduced after adding weighted noise, and is this architecture necessary for achieving implicit IB? Please provide a detailed analysis rather than merely  based on visualization results.\n2. Figure 3 lacks a more analysis, for example, why does I(Z;Y) and I(X;Z) at shallow layers (1 or 2) first decrease and then increase as training epochs progress? Please provide a more detailed analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tOHxqW4HON", "forum": "4qj7qO1fTJ", "replyto": "4qj7qO1fTJ", "signatures": ["ICLR.cc/2026/Conference/Submission21969/Reviewer_zeJh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21969/Reviewer_zeJh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991844580, "cdate": 1761991844580, "tmdate": 1762942002710, "mdate": 1762942002710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether the Graph Information Bottleneck (GIB) loss term is effective.\nThe authors argue that the i.i.d. assumption and the structural entanglement inherent in graph data violate the conditions required for these information-theoretic bounds to hold.\nTo address this issue, the paper proposes BottleneckMLP, which injects Gaussian noise into node embeddings and causes unimportant nodes to drift toward high-entropy, Gaussianized distributions.\nResults across multiple graph explainers show that BottleneckMLP produces better explanatory subgraphs than GIB."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The novelty of this paper is strong. Although the GIB has been widely applied in graph explainers, there has been little analysis of its actual effectiveness. This work fills the gap.\n2. This paper provides a rigorous theoretical foundation for deriving the proposed BottleneckMLP.\n3. Extensive experiments of this paper support the method proposed in this paper."}, "weaknesses": {"value": "1. The font size in Figure 1 of the paper is too small, and some elements lack legends for annotation.\n2. Can this method be extended to post-hoc explainers, such as V-InFor [1]? Can the performance of post-hoc explainers be compared?\n3. The paper notes the default MLP architecture is $h \\rightarrow h/4 \\rightarrow h$ and that finding the optimal architecture is like hyperparameter tuning. While Appendix G tests other configurations, a brief discussion behind a bottleneck-then-expansion structure versus a purely compressive one (e.g., $h \\rightarrow h/4$) would be beneficial. It's unclear if the expansion phase plays a key role or if the compression is the only necessary component.\n4. The paper reports Fidelity scores but does not provide results for sparsity, which is also an important property for explanatory subgraphs. Since the standard GIB can adaptively select the optimal budget, it remains unclear whether BottleneckMLP possesses this capability as well.\n\nReference:\n[1] Wang, S., Yin, J., Li, C., Xie, X., & Wang, J. (2023). V-infor: A robust graph neural networks explainer for structurally corrupted graphs. Advances in Neural Information Processing Systems, 36, 56469-56487."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fg3tZp7dh0", "forum": "4qj7qO1fTJ", "replyto": "4qj7qO1fTJ", "signatures": ["ICLR.cc/2026/Conference/Submission21969/Reviewer_9HiS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21969/Reviewer_9HiS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997732600, "cdate": 1761997732600, "tmdate": 1762942002387, "mdate": 1762942002387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BottleneckMLP, a new approach to induce information bottleneck (IB) in graph neural network (GNN) ante-hoc explainers (those that are trained together with the GNN downstream task). The paper shows, empirically and theoretically, why IB-based losses in existing ante-hoc explainers fail; then, it presents an theoretically-sound approach to circumvent this limitation (BottleneckMLP); finally, it provides empirical evidence of the success of this circumvention. The paper covers the preliminaries in IB theory, as well as related work, and the theoretical approach is based on proven lemmas and theorems. Overall, the proposed method to induce IB outperforms ante-hoc explainers without it (with and without IB-based losses proposed in previous work) in both explanation quality and accuracy. Additional experiments support additional claims, such as improved representation dynamics of BottleneckMLP over other methods. The paper ends with a summary of future directions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper is stating, measuring and mitigating a relevant limitation from previous work in IB approaches for GNN explainability. This is an important and significant issue in the field. In this case, the paper shows that current IB-based losses to induce better explanations lack soundness, and the paper proposes an alternative that is argued to mitigate this issue, both with theory and experiments to support this claim.\n\nThe paper is also strong in the sense of discussing a deeper theoretical analysis in a formulation that is rather simple: adding Gaussian noise and then using under-parametrised neural networks for compression. The simplicity of the method is also a highlight.\n\nThe paper is well organised and presented, it is very easy to follow the itemised contributions throughout the whole paper structure."}, "weaknesses": {"value": "1. I missed a measure of the actual running time of BottleneckMLP when compared to other methods (with and without IB-based losses). Would it be possible to provide them, please?\n\n2. The justification for the looseness of the variational upper bound (Sec. 4.1) did not convince me. Going away from an abstract approach, for instance, let’s consider GSAT. Which is the loss being used? And why exactly is it “loose”? What does it actually mean to be “loose”? Is it only because it relies on approximation? I believe that approaching these questions would clarify the looseness claim.\n\n3. I don’t want to frame this as an issue of the paper itself as I believe this is actually a discussion about the GNN explainability field, but I miss a discussion in the paper about what *is* a graph explanation. Is it actually true that an information-bottlenecked-sub-graph is the sub-graph that correctly “explains” the downstream decision? What if the model, in its black-box architecture, uses other nodes that “are not meant to be used”? In some sense, this discussion collapses into the post-hoc vs ante-hoc discussion: should one “train” an explanation, which in this case loses the explainability role and becomes another model output?\n\n4. The whole Sec. 4.3.1 develops on why variational bounds are used but in the end it simply says that the assumptions break with graphs. So why all the formulas? It could have been stated from the beginning, if the formulas do not contribute to the statement. I mean, it loses a special space in the paper.\n\n5. When graphs are side-by-side, the default is sharing the same y-axis. Why is this not the case for Fig. 2?\n\n6. The future directions could be more developed. I understand the limitation of space in the submission. Can you please develop more on those?\n\n7. The abstract states that “explicit IB-based losses in GNN explainers provide little benefit beyond standard training: the fitting and compression phases of IB emerge naturally”. However, Fig. 2 supports that compression only happens with BottleneckMLP: “In Figure 2c, I(X; Z) rises early as task-relevant features are captured, then declines in later epochs, reflecting effective compression.” (L341). Can you please clarify? It could also be useful to see Fig. 3 for “Original” and “w/o IB Loss”, in addition to “BottleneckMLP”.\n\n8. Very minor issues: Please take a look at the parenthesis in the citations. Almost all citations are with incorrect parenthesis. E.g., where it is “node classification Luo et al. (2020b)”, it should be “node classification (Luo et al., 2020b)”. Probably easy to change `\\citet{}` to `\\cite{}` in LaTeX. Also, please increase the font size of the text inside the figures. Finally, please take extra care to some parts of the text with typos (e.g., L178, L179, L229, L338)."}, "questions": {"value": "Please refer to the Weaknesses session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EBoTsAjG9S", "forum": "4qj7qO1fTJ", "replyto": "4qj7qO1fTJ", "signatures": ["ICLR.cc/2026/Conference/Submission21969/Reviewer_k24x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21969/Reviewer_k24x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762773831706, "cdate": 1762773831706, "tmdate": 1762942002030, "mdate": 1762942002030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Comment For All Reviewers"}, "comment": {"value": "We thank all the reviewers for their valuable comments. Below we summarize our changes and the new experimental results. \n\n1. New experiments of BottleneckMLP across graph tasks and explainer types\n   1. Subgraph recognition (VGIB) in Appx H.1\n   2. Post-hoc node classification (PGExplainer) in Appx H.2\n   3. Post-hoc graph classification (V-InFoR, ReFine, ProxyExplainer) in Appx H.3\n   4. Non-IB ante-hoc graph classification (GOAt, DIR) in Appx. H.4\n   5. New datasets on IB-based ante-hoc explainers GSAT and PGIB (Alkane-carbonyl, benzene, fluoride-carbonyl) in Section 5.3\n   6. Results on different GNN Backbones (GIN, GAT, GCN, PNA) in Appx R\n2. New sections 5.4 and 5.5 discussing the generalization of BottleneckMLP across explainability methods, hyperparameter sensitivity analysis, and convergence results\n3. Hyperparameter analysis of noise variance sigma (Appx S)\n4. Sparsity analysis (Appx T)\n5. Comparative analysis of information plane dynamics across models (Appx P)\n6. Pseudocode of BottleneckMLP algorithm (Appx Q)\n7. Explanation of why stacking graph layers does not work\n8. Minor fixes including citations, typos, figure readability\n\n**New experiments of BottleneckMLP across graph tasks and explainer types. (Section 5.3, Appx H and R)**\n\nWe find that BottleneckMLP consistently exhibits superior performance across explanation models and tasks.\n\n1. Subgraph recognition in VGIB improves by 3.5-62.5% depending on the dataset\n2. Post-hoc node classification: in PGExplainer accuracy improves by 1.1-4.9% and explanation AUC-ROC by 0.6% and 0.8%. Furthermore, we observe that removing Entropy Loss in the original PGExplainer is detrimental to performance (AUC-ROC drops 98%). In the absence of the entropy loss, BottleneckMLP fully recovers and slightly improves this performance.\n3. Post-hoc graph classification: all metrics in V-InFoR are improved with BottleneckMLP across all datasets. On ReFine, BottleneckMLP exhibits comparable performance to the baseline method by recovering the function of fidelity loss and a 20% accuracy increase on Mutag. On ProxyExplainer, BottleneckMLP mostly improves explanation performance and fidelity across datasets.\n4. Non-IB ante-hoc graph classification: in GOAt, BottleneckMLP improves all metrics across all datasets. On DIR, BottleneckMLP shows improvement in accuracy by 24.2% and ability to recover the lack of the explicit loss in DIR, similar to PGExplainer. BottleneckMLP has slightly lower performance on other metrics.\n5. On new datasets Alkane Carbonyl, Fluorine Carbonyl and Benzene, BottleneckMLP consistently outperforms baseline GSAT and PGIB across all metrics.\n6. On other GNN backbones, GCN + BottleneckMLP outperforms GCN baseline in all datasets across metrics. On GAT, BottleneckMLP improves on MUTAG and NCI1 while showing a slight drop in performance in Proteins. On PNA, we achieve similar or slightly lower performance.\n\n\n**Hyperparameter analysis of noise variance sigma (Appx S)**\n\nWe include analysis of the hyperparameter sigma (gaussian noise variance). This parameter  controls the strength of Gaussianization of node embeddings that we tune with grid search over validation accuracy. We give the full analysis in Appendix S.\n\n**Convergence results (Appx O)**\n\nWe observe that BottleneckMLP accelerates convergence and we include new experiments  along with theoretical rationale on convergence speed-up in Appendix O.\n\n**Sparsity (Appx T)**\n\nWe include a section on sparsity in Appendix T analysing BottleneckMLP performance across different sparsity levels. We evaluate BottleneckMLP using the adopted top-k protocol of each baseline method, ensuring the comparison is fair and aligned with prior work. We note that BottleneckMLP does not directly have a sparsity budget, and sparsity appears as a result of the implicit bottleneck enforced with the method.\n\n**Comparative analysis of information plane dynamics across models (Appx P)**\n\nAs an extension to Figure 3 in the paper, we include a discussion section in Appendix P on the analysis of information planes across models. The information plane reveals MI dynamics across epochs, and we plot I(X;Z) vs I(Z;Y) to observe potential compression and concentration, which is what IB-based models try to achieve. Our analysis shows that compression and concentration is only observed with BottleneckMLP, where the baseline model with and without info loss shows no evidence of compression (minimization of I(X;Z) while retaining or increasing I(Z;Y)).\n\n**Pseudocode of BottleneckMLP algorithm (Appx Q)**\n\nWe have included a pseudocode algorithm of BottleneckMLP’s importance-weighted noise injection training loop in Appx Q."}}, "id": "gxY0dZerS9", "forum": "4qj7qO1fTJ", "replyto": "4qj7qO1fTJ", "signatures": ["ICLR.cc/2026/Conference/Submission21969/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21969/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21969/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763690884360, "cdate": 1763690884360, "tmdate": 1763690884360, "mdate": 1763690884360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}