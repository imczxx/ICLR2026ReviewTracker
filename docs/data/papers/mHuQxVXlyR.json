{"id": "mHuQxVXlyR", "number": 24511, "cdate": 1758357513437, "mdate": 1759896762291, "content": {"title": "Equalized Generative Treatment: Matching $f$-divergences for Fairness in Generative Models", "abstract": "Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification, focusing on balancing probability of generating each sensitive group. We show, both theoretically and empirically, that such criteria are brittle, as they can be satisfied even when different groups are modeled with widely varying quality. To address this gap, we introduce a new fairness definition for generative models, *equalized generative treatment* (EGT), which requires comparable generation quality across all sensitive groups, where this quality is measured via a reference $f$-divergence. We further analyze the trade-offs induced by EGT, showing that fairness constraints necessarily tie global model quality to the hardest group to approximate. Finally, we benchmark several strategies, including min-max optimization and group-conditional training, that directly target this criterion, and demonstrate through image generation experiments that EGT yields fairer outcomes without prohibitive losses in overall performance.", "tldr": "Extending criteria of fairness in generative models to ensure equal treatment", "keywords": ["Fairness", "Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe44f694ee68690ee6bd91ed9eea75545debdff4.pdf", "supplementary_material": "/attachment/9d3f042993f72d26efbc51f627e3d161d37b3b62.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses fairness in generative models and argues that existing fairness notions are brittle, as they can equalize group proportions while still generating lower-quality samples for certain groups. To overcome this, the authors propose Equalized Generative Treatment (EGT), which enforces fairness by requiring parity of f-divergences between real and generated distributions across sensitive groups. They analyze its theoretical properties, highlight the inherent fairness–performance trade-off, and evaluate several training strategies (reweighting, min–max optimization, conditional training) on diffusion models and LLMs, showing reduced inter-group quality gaps without major loss in overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses an important topic: fairness in generative models.\n- The writing is clear and easy to follow.\n- Experiments cover various modalities of data: image and text."}, "weaknesses": {"value": "- **Trivial central claim.** A large portion of the paper demonstrates that proportion-based fairness can still lead to quality disparity across groups, which is a natural and well-known issue. As demonstrated long ago (e.g., in [1]), the difference of intra-group FID already captures this discrepancy without defining new metrics.\n\n- **Straightforward definition.** The proposed EGT metric is a direct restatement of equalizing f-divergences across groups, which is conceptually simple and lacks novelty.\n\n- **No dedicated algorithm.** While the metric is defined, no new optimization method is introduced. Section 5 only reuses existing methods (reweighting, min–max, conditional training), and their relationship to EGT optimization remains unclear.\n\n- **Outdated baseline methods.** The study heavily relies on older fairness strategies (e.g., Choi et al. 2020, classical min–max training [2,3]), limiting its relevance to modern generative frameworks.\n\n- **Weak evaluation metrics.** Precision and recall are insufficient to capture generative capability in a thorough manner; stronger metrics such as density and coverage [4] would provide a more meaningful assessment.\n\n\n\n----\n**References**\n\n[1] A Fair Generative Model Using LeCam Divergence, AAAI 2023.\n\n[2] Minimax Group Fairness: Algorithms and Experiments, Arxiv 2020.\n\n[3] Fairness without Demographics through Adversarially Reweighted Learning, NeurIPS 2020.\n\n[4] Reliable Fidelity and Diversity Metrics for Generative Models, ICML 2020."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "85hAaV80GF", "forum": "mHuQxVXlyR", "replyto": "mHuQxVXlyR", "signatures": ["ICLR.cc/2026/Conference/Submission24511/Reviewer_hajo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24511/Reviewer_hajo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746909477, "cdate": 1761746909477, "tmdate": 1762943108888, "mdate": 1762943108888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies fairness in generative models and argues that proportion-based criteria, e.g. equalized generative odds (EGO) and matching generative odds (MGO) can be satisfied even when generation quality differs substantially across demographic groups. The authors formalize this limitation and introduce Equalized Generative Treatment (EGT), which asks models to deliver comparable quality for each group by equalizing a standard measure of model-versus-data mismatch across groups. They show this effectively concentrates optimization on the worst-served group. They evaluate three practical training strategies (reweighting, a simple min-max objective, and conditional training) on diffusion models (FFHQ faces) and small LLMs (Wikipedia-style biographies), reporting reductions in per-group precision/recall gaps relative to odds-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical result shows you can satisfy odds-based fairness and still be arbitrarily worse for one group. This cleanly formalizes a widely suspected issue.\n2. The paper evaluates multiple model families (EDM-VP, EDM-VE, LLaMA-3.2) across different modalities (images, text) with multiple optimization strategies. The TopP&R framework for evaluation is more principled than standard kNN-based precision/recall.\n3. Group-wise Topological Precision & Recall mitigates brittleness in support estimation and decouples fidelity from coverage. Repeated random projections add robustness."}, "weaknesses": {"value": "1.  EGT is defined with standard divergences, but the paper does not provide a direct, ready-to-use training objective that enforces EGT. Experiments rely on proxies (reweighting, min-max, conditional training).\n2. Table 1 shows conditional training achieves excellent  $\\delta$-FID but significantly worse $\\delta$-PR compared to unconditional methods. This directly contradicts the expectation that specific conditioning improves group-wise control. The paper mentions this result but provides no explanation or investigation.\n3. Figure 4 clearly shows enforcing fairness raises all groups to worst-case performance, and Theorem 4.3 implies this mathematically.\n4. Experiments focus mainly on a binary gender attribute and do not study intersectional groups, limiting external validity.\n5. Most tables lack error bars.\n6. Core evaluation details (TopP&R configuration, feature spaces/oracles, sample sizes), plus many training specifics and full tables, presented in the appendix, not the main paper."}, "questions": {"value": "1. EGT is defined with a generic divergence family. Which one do you recommend for different modalities (images vs. text), and how sensitive are conclusions to this choice?\n2. How do diffusion results change if you swap the FFHQ gender oracle (e.g., a different backbone/threshold)? How often do oracle disagreements flip group-gap conclusions?\n3. Can EGT extend to multiple attributes and their intersections? How does this affect sample efficiency and numerical stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h0JqlmDk5y", "forum": "mHuQxVXlyR", "replyto": "mHuQxVXlyR", "signatures": ["ICLR.cc/2026/Conference/Submission24511/Reviewer_hkXX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24511/Reviewer_hkXX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997523450, "cdate": 1761997523450, "tmdate": 1762943108365, "mdate": 1762943108365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the quality disparity issue of generative models, which is becoming a very crucial area within the field of AI fairness. A main contribution of the paper is to introduce a novel group fairness definition called equalized generative treatment (EGT), which designed to monitor quality gaps among different groups using f-divergence. The paper provides several theoretical insights related to the proposed metrics. In experiments, the paper compares several fairness algorithms in terms of the existing fairness metrics and proposed metric, for both diffusion models and language models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper mainly discusses the quality gap issue of generative models, which becomes a significant area within AI fairness.\n- A novel group fairness metric for quality gaps is proposed, grounded in theoretical insights. This metric offers the advantage of adapting various f-divergence-based measures."}, "weaknesses": {"value": "- The paper could contain more clear explanations on the novelty and effective of their \"methods\".\n   - The algorithms used in the experiments to reduce the f-divergence gap appear to be relatively straightforward adaptations of existing fairness algorithms. They do not seem specifically designed for effectively mitigating the f-divergence gap. In addition, based on the current results and discussions, it remains unclear when each method is relatively more effective for addressing the generation quality gap.\n   - Conditional training demonstrated pretty strong overall results in mitigating the quality gap, which raises the question of whether this improvement comes from the f-divergence-based insight or from the inherent effectiveness of conditional training itself. Although the paper notes that \"conditional training does not automatically improve fairness gaps in terms of EGT\", further investigation using state-of-the-art conditional training methods could more definitively clarify their impact on the quality gap in generative tasks.\n\n- While there's some discussion of existing fairness metrics (both classification-based and quality-based), a more comprehensive correlation study between the proposed metric and the existing ones would further clarify the significance of the new metric.\n\n\nMinor: \n- Typo in Introduction: A missing white space between 1st and 2nd sentences.\n- The paper might consider adding an explanation of the abbreviation EGT in the introduction section."}, "questions": {"value": "The questions are merged in the above sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EjmpzPo0fC", "forum": "mHuQxVXlyR", "replyto": "mHuQxVXlyR", "signatures": ["ICLR.cc/2026/Conference/Submission24511/Reviewer_ATTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24511/Reviewer_ATTV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061563014, "cdate": 1762061563014, "tmdate": 1762943108114, "mdate": 1762943108114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}