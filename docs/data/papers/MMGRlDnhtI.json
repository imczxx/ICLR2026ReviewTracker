{"id": "MMGRlDnhtI", "number": 20098, "cdate": 1758302447736, "mdate": 1759897001849, "content": {"title": "Search Arena: Analyzing Search-Augmented LLMs", "abstract": "Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce \\textbf{Search Arena}, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations and types of cited sources, even when the cited content does not directly support the associated claims, uncovering a gap between perceived and actual credibility. To assess cross-setting performance, we conduct cross-arena analyses by testing search-augmented LLMs in a general purpose chat environment and conventional LLMs in search-heavy settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research.", "tldr": "We introduce the first large-scale human-preference dataset of user interactions with search-augmented LLMs and provide in-depth analyses.", "keywords": ["Large Language Models", "Web Search", "Human-AI Interaction"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e5a05b0f50c51ded493e3c848c335532415522e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Search Arena, a large-scale, multilingual and multi-turn benchmark for search-augmented LLMs that contains 24k real user conversations and 12k human-preference judgements. The authors analyse how response length, number of citations, source types and citation fidelity correlate with user preference, and conduct cross-arena experiments to quantify the gain of web access. The dataset and ranking pipeline are released under CC-BY."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The benchmark itself is a valuable contribution: it is two orders of magnitude larger than prior search-LLM datasets, covers 70+ languages, multi-turn context and nine diverse intents, and provides full system traces (query, retrieved pages, response, human vote).\n- The quantitative analysis offers actionable insights—e.g. users reward more citations even when irrelevant, Wikipedia citations are penalised, and reasoning models filter sources more aggressively—all statistically validated with a Bradley-Terry model."}, "weaknesses": {"value": "- As an evaluation benchmark the dataset is still tied to human preference. Automatic metrics are only sketched (e.g., claim-citation labels), so new models must either run costly human battles or risk LLM-as-a-judge drift. The paper also does not demonstrate any model-training experiments that exploit the released supervision.\n- The main qualitative insights—citation count boosts perceived credibility, users distrust Wikipedia, etc.—largely echo existing insights; the work does not surface novel, counter-intuitive findings.\n- Although ethics are discussed, the entire corpus is mined from users who clicked “accept” once. No right-to-withdraw, no IRB statement, and limited sensitive-content filtering leave lingering privacy concerns."}, "questions": {"value": "- What concrete steps will you take to handle residual PII or sensitive queries that automatic filters missed, and will you offer users a mechanism to retroactively remove their data?\n- For groups that wish to evaluate a new model on Search Arena without running fresh human battles, which fully automatic metric pipeline do you recommend, and how should statistical significance be computed when only LLM judges are available?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KpPcXDd5gI", "forum": "MMGRlDnhtI", "replyto": "MMGRlDnhtI", "signatures": ["ICLR.cc/2026/Conference/Submission20098/Reviewer_Tf64"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20098/Reviewer_Tf64"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210912611, "cdate": 1761210912611, "tmdate": 1762932994474, "mdate": 1762932994474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Search Arena, a crowd-sourced dataset for analyzing search-augmented language models collected by deploying a chat platform to collect in-the-wild user interactions. The dataset contains >24k paired multi-turn conversations and over 12k human preference votes, collected from interactions with 13 different models. This work addresses limitations of existing factoid benchmarks that are focused on single-turn, fact-checking questions. The dataset spans 71 languages and presents a new user intent taxonomy. \n\nThe authors use this dataset to analyze user preferences and find that users prefer longer responses and more citations. An interesting finding is that this preference for more citations holds even when the cited content is irrelevant to the claim. The paper also presents a cross-arena analysis, where they show that search-augmented models perform comparably in general-purpose chat settings, but models relying only on parametric knowledge underperform in search-intensive settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated and easy to follow. It addresses a clear gap, i.e. the lack of large-scale multi-turn and multilingual datasets for evaluating search-augmented LLMs.\n- The Search Arena dataset can be very useful to researchers working on a broad set of topics. Its scale as well as diversity in languages and query intents make it a valuable resource.\n- The analysis of user preferences is interesting and results in some non-obvious findings. The finding that user preference is positively correlated with the number of citations, even when those citations are irrelevant / do not support the claims, is concerning and could affect model design decisions.\n- The cross-arena analysis is interesting as well because it provides strong evidence that search augmentation is beneficial in search-heavy tasks without degrading general chat performance."}, "weaknesses": {"value": "- The paper presents a lot of correlational evidence, for eg, about which response features and cited sources users find important, but it states these findings more definitively than the correlational analysis supports. I think this is a major weakness and I would suggest rewriting these claims with caution.\n\n- The paper relies heavily on LLM-based pipelines for analysis, particularly for user intent classification and citation attribution. For the citation attribution validation, the process is described vaguely. For eg, the paper states that \"Human experts validated a subset of the outputs\" but does not provide inter-annotator agreement scores or a quantitative comparison between the LLM pipeline and human judgments. Given that the finding about irrelevant citations is a significant takeaway, a more robust validation of this pipeline is necessary. \n\n- Relatedly, the citation attribution pipeline was run on just 100 English conversations per intent category due to cost / scraping challenges, which is a pretty small subset. It is unclear if these findings about irrelevant citations generalize across all languages and the full dataset.\n\n- The paper provides limited discussion of some relevant prior work, such as WildChat and WildBench.\n\n- The paper offers limited to no discussion on precisely how future work should utilize the Search Arena dataset."}, "questions": {"value": "- The authors mentioned that reasoning models filter irrelevant content and cite fewer sources. Did you find any difference in the types of sources cited by reasoning vs non-reasoning models?\n- It is unintuitive to me that citing Wikipedia is negatively correlated with user preference. Since it is hard to control the prompt set while just varying the cited sources, I wonder if the coefficient differences between cited sources are actually meaningful to draw conclusions from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rycrk0WLDW", "forum": "MMGRlDnhtI", "replyto": "MMGRlDnhtI", "signatures": ["ICLR.cc/2026/Conference/Submission20098/Reviewer_Uoa4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20098/Reviewer_Uoa4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884186385, "cdate": 1761884186385, "tmdate": 1762932993685, "mdate": 1762932993685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study introduces the first large-scale, crowd-sourced dataset of human interactions with search-augmented LLMs. It analyzes how users engage with search-augmented LLMs and finds that user preferences are strongly influenced by citation quantity and source type (even when citations are irrelevant). A cross-arena experiment shows that web search improves model performance in both search and non-search settings, but models without search capabilities perform poorly when users expect real-time retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "-The study is impressively comprehensive, drawing on a diverse dataset that spans 11,650 users across 136 countries, 13 models, and 70 languages.\n\n-The authors conducted a series of insightful and relevant analyses on human preferences in search-augmented LLMs, such as the “Types of Cited Sources” section. The finding that Wikipedia citations correlate negatively with user preference (while social media and community sources correlate positively) is both counterintuitive and well-explained (i.e., Wikipedia’s breadth and outdatedness often reduce perceived relevance).\n\n-Impactful work: This work releases a large-scale, human-preference dataset for search-augmented LLMs, which acts as a stepping stone for future research works to understand how users evaluate search-augmented LLMs.\n\n-The paper is well-structure and well-motivated, with the intent diversity section drawing clear comparisons to prior datasets. The writing is polished, and the narrative is easy to follow."}, "weaknesses": {"value": "The current premise of search arena relies on human preference signals, but the paper’s own findings cast doubt on whether these signals are reliable indicators of true search quality. In particular, the Citation Attribution analysis shows that users often fail to distinguish between supporting and irrelevant citations and tend to prefer responses with a higher number of citations (regardless of the validity)). Human preference may conflate perceived credibility with actual factual correctness, leading to rankings that reward citation quantity or presentation style rather than true retrieval or reasoning quality.\n\nThe authors could expand their Citation Attribution experiment (currently only limited to ~100 examples) as a more objective evaluation metric. In particular, metrics assessing (i) citation accuracy, (ii) citation relevance (semantic alignment between retrieved source and response content), (iii) recency (temporal freshness of cited information), and (iv) retrieval rarity or specificity (the model’s ability to surface rare but authoritative sources) could provide a more faithful measure for search-augmented LLM performance."}, "questions": {"value": "The reasoning traces shown in Figure 3 are quite interesting (demonstrating multi-document analysis, filtering, and synthesis). Did the authors observe cases where reasoning models misinterpreted their sources? If so, how might such errors in reasoning be detected at a larger scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iruBr2PHD4", "forum": "MMGRlDnhtI", "replyto": "MMGRlDnhtI", "signatures": ["ICLR.cc/2026/Conference/Submission20098/Reviewer_ttYs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20098/Reviewer_ttYs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947938286, "cdate": 1761947938286, "tmdate": 1762932992893, "mdate": 1762932992893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study aims to solve the issue of evaluation benchmark for search-augmented LLM limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. To this end, a crowd-sourced, large-scale, human-preference dataset is proposed together with the analysis and performance accessing across different categories and settings. Overall, this is an important contribution in evaluating retrieval-augmented generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike prior datasets such as SimpleQA and BrowseComp which are static, English-only, single-turn fact-seeking queries, the proposed Search Arena evaluates models in diverse, open-ended, multilingual, and multi-turn settings.\n\n2. The human-preference analysis are detailed, covering number of citations, supportive claims, cited sources, etc. This is the crucial point in terms of the construction principle of Search Arena.\n\n3. A set of detailed experimental results and analysis are provided to cover various aspects."}, "weaknesses": {"value": "1. The reliability of the collected data should be further judged, although the more sophisticated approaches are expensive.\n\n2. The definition of search-augmented LLM and retrieval-augmented generation should be further distinguished if the author discusses them in different context as shown in the related work.  Besides, the conversational search [1,2] is highly related to the proposed Search Arena, i.e., multi-turn human-AI interaction in search setting.\n\n3. Section 2 discuss the difference compared to SimpleQA, BrowseComp, and Text Arena, but lack of the comparison with existing conversational search/RAG datasets, e.g., TopiOCQA [3], Coral [4]. \n\n[1] Neural Approaches to Conversational Information Retrieval. SIGIR 2020.\n\n[2] A Survey of Conversational Search. ACM TOIS 2025.\n\n[3] TopiOCQA: Open-domain Conversational Question Answering with Topic Switching. TACL 2021.\n\n[4] CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation. NAACL 2024."}, "questions": {"value": "1. What is the potential mechanism to improve the reliability of the constructed datasets based on human votes?\n\n2. What is the difference between search-augmented LLM and retrieval-augmented generation (in conversational scenarios)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nh48oUavsn", "forum": "MMGRlDnhtI", "replyto": "MMGRlDnhtI", "signatures": ["ICLR.cc/2026/Conference/Submission20098/Reviewer_7PyT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20098/Reviewer_7PyT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032269734, "cdate": 1762032269734, "tmdate": 1762932992179, "mdate": 1762932992179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}