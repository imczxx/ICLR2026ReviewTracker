{"id": "xrAOvoTfJq", "number": 10293, "cdate": 1758166329910, "mdate": 1758816970039, "content": {"title": "Fast and Simplex: 2-Simplicial Attention in Triton", "abstract": "Recent work has shown that training loss scales as a power law with both model size and the \nnumber of tokens, and that achieving compute-optimal models requires scaling model size and \ntoken count together. However, these scaling laws assume an infinite supply of data and \napply primarily in compute-bound settings. As modern large language models increasingly rely \non massive internet-scale datasets, the assumption that they are compute-bound is becoming \nless valid. This shift highlights the need for architectures that prioritize token \nefficiency.\n\nIn this work, we investigate the use of the 2-simplicial Transformer, an architecture that \ngeneralizes standard dot-product attention to trilinear functions through an efficient \nTriton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves \nbetter token efficiency than standard Transformers: for a fixed token budget, similarly \nsized models outperform their dot-product counterparts on tasks involving mathematics, \ncoding, reasoning, and logic. We quantify these gains by demonstrating\nthat $2$-simplicial attention changes the exponent \nin the scaling laws for knowledge and reasoning tasks compared to dot product attention.", "tldr": "Higher order sliding window attention improves scaling law exponents on reasoning tasks", "keywords": ["higher order attention", "sliding window attention", "triton"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4cae13b8d3b87d375d0af2b6ce6295c514479e3f.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}