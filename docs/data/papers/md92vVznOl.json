{"id": "md92vVznOl", "number": 19620, "cdate": 1758297727977, "mdate": 1759897029810, "content": {"title": "From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning", "abstract": "Large Language Models (LLMs) excel at general tasks but underperform in specialized domains like economics and psychology, which require deep, principled understanding. To address this, we introduce ACER (Automated Curriculum-Enhanced Regimen) that transforms generalist models into domain experts without sacrificing their broad capabilities. ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom’s taxonomy. This ensures systematic topic coverage and progressively increasing difficulty. The resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions.\n\nExperiments with Llama 3.2 (1B and 3B) show significant gains in specialized MMLU subsets. In challenging domains like microeconomics, where baselines struggle, ACER boosts accuracy by 5 percentage points. Across all target domains, we observe a consistent macro-average improvement of 3 percentage points. Notably, ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points, while maintaining stable performance on general reasoning tasks. Our results demonstrate that ACER offers a scalable and effective recipe for closing critical domain gaps in LLMs.", "tldr": "We introduce ACER, a curriculum-driven regimen that sharpens LLM expertise in niche domains like economics without compromising general performance.", "keywords": ["Knowledge Infusion", "Curriculum Learning", "Synthetic Data Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5669f467ecb8983f9bbf3730ebf0ebfa6bcc62a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ACER (Automated Curriculum-Enhanced Regimen), a framework for infusing domain-specific knowledge into large language models (LLMs) through automated generation of textbook-style curricula and Bloom’s taxonomy–guided question–answer (QA) pairs. The synthetic data is used for continual pretraining under various curriculum scheduling strategies, including cognitive progression (from textbooks to easy/hard QA) and persona-based content ordering (high school → researcher). Experiments on Llama 3.2 (1B and 3B) show consistent gains (~3 percentage points macro-average) on five MMLU subdomains where the base models underperform relative to larger teachers (e.g., microeconomics, econometrics). The authors also report modest improvements on non-target MMLU domains (+0.7 points), as well as gains on knowledge-intensive benchmarks like ARC (Clark et al., 2018) and GPQA (Rein et al., 2023), without degradation on general reasoning tasks (e.g., GSM8K, HellaSwag)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem: The paper addresses a genuine limitation of current LLMs—their shallow understanding of specialized domains—highlighted by consistent performance gaps on benchmarks like MMLU (Hendrycks et al., 2021).\n2. Systematic synthesis pipeline: ACER’s multi-stage generation process (domain detailing → outline → textbook → QA pairs) is pedagogically grounded and scalable, drawing on established educational frameworks like Bloom’s taxonomy (Bloom et al., 1956).\n3. Comprehensive evaluation: The authors evaluate across multiple benchmarks (MMLU, ARC, GPQA, AGIEval, GSM8K, HellaSwag) and include ablations over curriculum scheduling strategies.\n4. Reproducibility: Training details, data mixing ratios, decontamination procedures, and prompt templates are thoroughly documented in the appendix."}, "weaknesses": {"value": "1. Lack of comparison to strong synthetic data baselines: The paper does not compare ACER against recent, high-impact synthetic data methods, like Phi-4 (Abdin et al., 2024), which uses multi-agent, multi-stage pipelines to generate diverse, high-quality textbook-like data. Without such comparisons, it is unclear whether gains stem from curriculum structure or simply from high-quality synthetic data.\n2. Marginal and unstable gains from curriculum scheduling: While the “Flat” baseline (random mixing of books and QA) already yields +2.5 points, the best curriculum (Cog+Con) adds only ~0.5 additional points. More concerning, the Interleaved schedule—inspired by Lee et al. (2024)—performs worse than Flat, particularly in mathematics and statistics. This undermines the central claim that structured sequencing is beneficial, and suggests the gains may be schedule-sensitive rather than robust.\n3. Heavy reliance on a powerful external model for data generation: The synthetic corpus is generated using Gemini 2.0 Flash, a proprietary, state-of-the-art model. The paper provides no ablation using weaker or open-source generators (e.g., Llama 3 itself). This raises concerns about generalizability: ACER may essentially be a form of knowledge distillation from a stronger teacher, not a self-contained curriculum learning method.\n4. Limited model scale and cherry-picked domains: Evaluation is restricted to 1B/3B models. Larger models (e.g., 7B/8B) may already encode sufficient domain knowledge, rendering ACER’s marginal gains irrelevant at scale. Moreover, the five target domains are selected based on the largest student–teacher gaps in MMLU—a reasonable heuristic, but one that risks selection bias; the method’s efficacy on domains with smaller gaps remains untested.\n5. Overstated claims about catastrophic forgetting: While non-target MMLU performance improves slightly (+0.7), the 1B model shows degradation on GSM8K (Strict EM: 0.0667 → 0.0591). Although small, this suggests potential capacity interference. The paper lacks more rigorous forgetting metrics (e.g., loss on original pretraining data)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LsMCEVymZb", "forum": "md92vVznOl", "replyto": "md92vVznOl", "signatures": ["ICLR.cc/2026/Conference/Submission19620/Reviewer_WdyZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19620/Reviewer_WdyZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761295686133, "cdate": 1761295686133, "tmdate": 1762931476247, "mdate": 1762931476247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce ACER, a framework that synthesizes textbook-style corpora alongside complementary exam-style question-answer pairs. They also design curriculum learning methods that align both cognitive and content dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper identifies that state-of-the-art LLMs struggle in specialized domains requiring deep, principled understanding."}, "weaknesses": {"value": "- The paper only compares the effectiveness of different curriculum schedules within their own method in Table 1, and lacks comparisons against other baseline approaches in the same domain (e.g., those referenced in lines 53–55).\n\n- The ACER synthesis seems to rely on Gemini 2.0 Flash as the LLM teacher, but this is not explicitly stated in Section 3. Moreover, the paper omits presenting Gemini 2.0 Flash’s performance on the evaluation benchmarks, which is important as it is the teacher model.\n\n- The ACER simulations lack input from external domain knowledge, as the authors’ emphasis on the need for “principled domain expertise.”\n\n- The paper does not include results from larger open-source or proprietary models, even as reference results."}, "questions": {"value": "- Please add results of strong data-curriculum baselines (e.g., self-instruct,and works cited in lines 53–55).\n\n- State explicitly which teacher LLM is used for ACER synthesis (it seems to be Gemini 2.0 Flash).\n\n- Report the teacher's and other LLMs' scores on all benchmarks\n\n- Describe quality controls for factual/conceptual soundness of the synthesized corpus."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rCJIlN58Y9", "forum": "md92vVznOl", "replyto": "md92vVznOl", "signatures": ["ICLR.cc/2026/Conference/Submission19620/Reviewer_911Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19620/Reviewer_911Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815756028, "cdate": 1761815756028, "tmdate": 1762931475678, "mdate": 1762931475678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a new method for improving domain-expertise in LLMs. Unlike prior work where synthetic instruction data or domain-specific pretraining lacked structured progression, the authors propose ACER (Automated Curriculum-Enhanced Regimen), which automatically generates textbook-style content plus question-answer pairs following Bloom’s taxonomy, and uses a curriculum-aware schedule (cognitive difficulty + persona audience progression) to continually pretrain a general LLM. The proposed method is evaluated on subsets of the MMLU benchmark (five niche domains), as well as ARC, GPQA and other tasks. Experiment results show 3 percentage point macro-average improvement in target domains while preserving general capabilities and achieving 0.7 point gains in non-target domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper presents a systematic synthetic corpus generation pipeline that is well-motivated and clearly described.\n2. The curriculum scheduling is a reasonable design choice, enabling ablations that highlight the value of ordering in continual pretraining.\n3. Empirical results demonstrate improvements in both target niche domains and stability on general capability benchmarks."}, "weaknesses": {"value": "1. The impact of the synthesis book corpus generation pipeline is not sufficiently discussed. The experiments centered around using the same pipeline under different curriculum schedules. It's unclear how big a role the generation pipleline plays in the overall performance improvement.\n2. Limited insight was revealed and discussed among the different curriculum schedules, e.g. what makes some schedule works better than the others.\n3. The performance improvement in some domains are rather limited and may well fell within the range of variance, e.g. Econ, psych, and Macro_nt."}, "questions": {"value": "1. What's the impact of the pipeline design for the synthesis book corpus generation?\n2. Any insight on how sensitive are the results to the quality of the synthetic textbook content, e.g. if generated from a weaker model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MjGcSFx0hH", "forum": "md92vVznOl", "replyto": "md92vVznOl", "signatures": ["ICLR.cc/2026/Conference/Submission19620/Reviewer_sBBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19620/Reviewer_sBBz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113513483, "cdate": 1762113513483, "tmdate": 1762931475140, "mdate": 1762931475140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}