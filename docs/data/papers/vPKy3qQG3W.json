{"id": "vPKy3qQG3W", "number": 10037, "cdate": 1758157620537, "mdate": 1759897679459, "content": {"title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment", "abstract": "Learning diverse manipulation skills for real-world robots is severely bottlenecked by the reliance on costly and hard-to-scale teleoperated demonstrations. While human videos offer a scalable alternative, effectively transferring manipulation knowledge is fundamentally hindered by the significant morphological gap between human and robotic embodiments. \nTo address this challenge and facilitate skill transfer from human to robot, we introduce Traj2Action,a novel framework that bridges this embodiment gap by using the 3D trajectory of the operational endpoint as a unified intermediate representation, and then transfers the manipulation knowledge embedded in this trajectory to the robot's actions.\nOur policy first learns to generate a coarse trajectory, which forms an high-level motion plan by leveraging both human and robot data. \nThis plan then conditions the synthesis of precise, robot-specific actions (e.g., orientation and gripper state) within a co-denoising framework. Extensive real-world experiments on a Franka robot demonstrate that Traj2Action boosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short- and long-horizon real-world tasks, and achieves significant gains as human data scales in robot policy learning.Our project website, featuring code and video demonstrations, is available at https://anonymous.4open.science/w/Traj2Action-4A45/.", "tldr": "Traj2Action bridges the human-robot embodiment gap using 3D trajectories as a unified representation. A coarse trajectory plan guides fine-grained actions via a co-denoising framework, outperforming prior methods in real-world robot tasks.", "keywords": ["embodied intelligence", "vision-language model", "human manipulation videos", "VLA"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89887b507341ef163b993d68b038ca902cfe8acc.pdf", "supplementary_material": "/attachment/c77b7a1b768f97e002d27921990dc90530d8b62d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Traj2Action, a simple two-expert policy for transferring manipulation skills from human demonstrations to robots. It uses the 3D end-effector/hand trajectory (positions only) as a unified intermediate representation, first predicting a coarse trajectory from multi-view images and language, then conditioning a robot-only action expert to output precise deltas (position, rotation, gripper) via a joint co-denoising / flow-matching objective. On four Franka tasks, the method reports sizable gains over a VLA baseline and indicates human data can partially substitute for teleoperated robot data, with ablations on human-data scale and trajectory sampling frequency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Conceptual clarity & simplicity. Using a position-only trajectory as the embodiment-agnostic “common divisor” is clean and broadly applicable; the coarse-to-fine decomposition is intuitive.\n- Empirical signal. Real-robot evaluations show consistent improvements over π₀, especially on long-horizon tasks; scaling human data helps, and ablations on FPS alignment are thoughtful.\n- Cost/efficiency angle. Evidence that human demos can replace a portion of costly robot data is valuable for the community’s data economics."}, "weaknesses": {"value": "- Limited baselines beyond ablations. Most results compare to π₀ with or without the trajectory expert; there is no direct comparison to recent human-robot co-training / cross-embodiment approaches (e.g., egocentric human-video VLA pretraining, video-track-to-action, motion-track alignment, etc.). This makes it hard to judge progress relative to state-of-the-art alternatives designed for the same setting.\n- Representation scope. The unified trajectory uses positions only; orientation/grasp semantics are pushed entirely to the action expert. It remains unclear when this abstraction is sufficient (e.g., tool use, in-hand reorientation) and when it fails. A task-type breakdown would clarify boundary conditions.\n- Reporting and statistics. Many deltas are reported without confidence intervals/significance tests across seeds or days; robustness to camera calibration errors and to workspace/domain shifts is not deeply probed."}, "questions": {"value": "- Comparative baselines. Can you add direct comparisons to human-robot co-training or egocentric video VLA approaches and trajectory/track-based transfer (e.g., track-to-act / motion-tracks-style methods)? Matching robot-data budgets would strengthen claims.\n- When do positions suffice? For tasks requiring tight orientation control (e.g., peg-in-hole, tool insertion), does a position-only unified space degrade?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vVHdlrKbmr", "forum": "vPKy3qQG3W", "replyto": "vPKy3qQG3W", "signatures": ["ICLR.cc/2026/Conference/Submission10037/Reviewer_89t7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10037/Reviewer_89t7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760970659501, "cdate": 1760970659501, "tmdate": 1762921442084, "mdate": 1762921442084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This papers proposes Traj2Action, a robot manipulation method to use a pose trajectory as a shared action space for both grippers and human hand. The authors demonstrate that this framework, when using human data, can have a large performance gain over short/long horizon tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper overall is very easy to follow\n2. The human data experiment is very motivating, it both demonstrates that performance can be scaled up even with human data, and human data can lead to more robust policy. \n3. The real-world experiment results are convincing"}, "weaknesses": {"value": "- My main concern is novelty. The design and benefits of intermediate action spaces have been thoroughly studied in numerous papers (not novel). Even when we consider human to robot transfer, MimicPlay (CoRL 2023) already have very similar problem setup and framework, except that this paper is based on a VLM backbone, which is not available at the time. There are also already many papers focusing on human to dexterous hand transfer, where a shared action space is naturally the hand pose. Overall I very much appreciate the efforts that went into this framework and system. But I believe the method itself is not significant enough for a venue like ICLR. \n- This work focus on human data and robot data collected from a single custom workspace setup. The generalization to larger-scale human videos is questionable. \n- The robot tasks are too simple, just pick-and-drop type of tasks. \n- The authors only conduct self-comparison in the experiments, which would be strengthened by more comparisons.\n- The authors collect over 1000 demonstrations for each task, which seems to be an order of magnitude more than conventional standard (~100), I wonder what is the rationale behind his design choice."}, "questions": {"value": "(in weakness)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YF3vsQQOye", "forum": "vPKy3qQG3W", "replyto": "vPKy3qQG3W", "signatures": ["ICLR.cc/2026/Conference/Submission10037/Reviewer_NXTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10037/Reviewer_NXTf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761440412999, "cdate": 1761440412999, "tmdate": 1762921441812, "mdate": 1762921441812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of transferring manipulation skills from human videos to robots by proposing Traj2Action, a framework that bridges the morphological gap between human hands and robot end-effectors using 3D trajectories as a unified representation. The method operates in a coarse-to-fine manner, where a trajectory expert first generates a high-level motion plan from human and robot data, which then guides an action expert to synthesize precise, robot-specific actions via a joint denoising training scheme. Real-world experiments demonstrate that this approach allows higher data collection efficiency and outperforms the $\\pi_0$ baseline."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow. \n2. The idea is well motivated.\n3. The experiment results show that including human data improves the performance."}, "weaknesses": {"value": "1. The method relies on a multi-camera motion capture system with a wrist-mounted ego-view camera to collect human data. This setup is far from scalable. It is impossible to collect in-the-wild data or even learn from internet videos.\n2. The representation of the human hand action as translations of a single 3D point (the thumb-index midpoint) is a significant oversimplification. This abstraction, while effective for the pick-and-place tasks presented, inherently limits the method's applicability. It cannot capture orientation cues from the human wrist or the complex finger coordination required for rotation-critical tasks (e.g., screwing) or contact-rich, dexterous manipulations (e.g., in-hand manipulation, tool using). Consequently, the experimental validation on four variants of basic pick-and-place tasks does not demonstrate the method's efficacy for the broader spectrum of robotic manipulation.\n3. The chosen tasks (PWB, PTT, SRP, SPC) are all structural variations of pick-and-place conducted in a controlled table-top setting.\n4. The paper only compares against $\\pi_0$, but lacks direct comparisons to other recent SOTA cross-embodiment imitation learning methods (e.g., Track2Act, ATM, Im2Flow2Act, EgoMimic).\n5. The paper's claims of generalization are weakly supported. The sole generalization experiment is a simple instructional combination generalization (\"put it in the blue tray\" instead of \"yellow\"), which achieves a low success rate (12%). Crucially, the work lacks evaluation on other critical axes of generalization essential for real-world deployment, such as scene, object, and motion generalization."}, "questions": {"value": "For my main concerns, please refer to the “Weaknesses” part. Here are some minor questions:\n\n1. The paper states that the Trajectory Expert's weights are initialized from the pretrained Action Expert of the $\\pi_0$ model. However, the dimensions differ: actions are 7-dimensional, while trajectories are 3-dimensional. Could the authors clarify the specific architectural modification to accommodate this? Was a simple linear/MLP projection layer added, and if so, how was it initialized?\n2. For the human data collection, the ego-centric view is provided by a camera mounted on the back of the user's hand. Given that this cannot be secured as rigidly as on a robot arm, what measures were taken to ensure the camera's pose remained stable relative to the hand? How do you deal with the potential camera shifting or loosening during demonstrations, which could introduce noise into the visual input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OmlI78WxAc", "forum": "vPKy3qQG3W", "replyto": "vPKy3qQG3W", "signatures": ["ICLR.cc/2026/Conference/Submission10037/Reviewer_oZ1F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10037/Reviewer_oZ1F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838907150, "cdate": 1761838907150, "tmdate": 1762921441526, "mdate": 1762921441526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Traj2Action, a human→robot skill transfer framework that uses the 3D end-effector trajectory as a unified intermediate representation. A Trajectory Expert (trained on human+robot data) predicts a coarse future trajectory, which then conditions an Action Expert (trained on robot data) to output precise deltas in position, orientation, and gripper state via a joint (flow-matching) co-denoising objective. The authors collect human hand trajectories with multi-view capture and an additional wrist/ego view to reduce observation mismatch. On four real-robot tasks (two short-horizon, two long-horizon), Traj2Action improves over a pi_0 baseline, shows scaling gains with more human data, and demonstrates that human demos can replace a sizable fraction of robot demos at comparable performance/cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The strengths of this work are:\n\n- The problem setting is very presented and motivated: human-to-robot skill transfer is important and practical; focusing on mapping human demos into robot-usable signals is worthwhile.\n\n- They conduct real-robot evaluation. Includes on-hardware tests rather than sim-only results, with clear task metrics (e.g., success rate / waypoint progress)."}, "weaknesses": {"value": "The main weaknesses are limited methodological novelty and unfair experimental comparisons.\n\n-  Limited novelty and missing related work in the part of translating human motion to robot: The proposed “unified trajectory space” offers little methodological novelty. The paper omits key position-based retargeting literature—e.g., AnyTeleop [1] and works that map human video motion to robot trajectories for downstream learning [2–5]. The claim that fingertip positions as a unified motion representation is not new at all (see [1-5] and lots of other recent works); moreover, the method ignores hand/gripper geometry by directly mapping the human fingertip midpoint to the robot end-effector pose. What happens under a different gripper or finger lengths? This appears to require ad-hoc offsets and re-tuning. Prior work typically optimizes retargeting to minimize pose error under the robot’s kinematics; while such methods are imperfect (they ignore object dynamics), the proposed heuristic is arguably weaker because it ignores the robot kinematics entirely.\n\n[1]. Qin, Yuzhe, et al. \"Anyteleop: A general vision-based dexterous robot arm-hand teleoperation system.\" arXiv preprint arXiv:2307.04577 (2023).\n\n[2]. Shaw, Kenneth, Shikhar Bahl, and Deepak Pathak. \"Videodex: Learning dexterity from internet videos.\" Conference on Robot Learning. PMLR, 2023.\n\n[3]. Liu, Yangcen, et al. \"Immimic: Cross-domain imitation from human videos via mapping and interpolation.\" arXiv preprint arXiv:2509.10952 (2025).\n\n[4]. Chen, Zoey Qiuyu, et al. \"Dextransfer: Real world multi-fingered dexterous grasping with minimal human demonstrations.\" arXiv preprint arXiv:2209.14284 (2022).\n\n[5]. Bahl, Shikhar, Abhinav Gupta, and Deepak Pathak. \"Human-to-robot imitation in the wild.\" arXiv preprint arXiv:2207.09450 (2022).\n\n- Limited novelty and missing related work in the part of the policy learning: Training the “trajectory expert” on both human and robot data resembles standard co-training strategy [6]. It’s also unclear whether you use an explicit co-training scheme or simply merge datasets. The “action expert,” conditioned on the trajectory expert and trained on robot-only data, amounts to robot-data fine-tuning for higher precision—similar in spirit to prior pipelines (e.g., [2]). Beyond fitting this into a VLA-style backbone, it’s not clear what is conceptually new.\n\n[6]. Maddukuri, Abhiram, et al. \"Sim-and-real co-training: A simple recipe for vision-based robotic manipulation.\" arXiv preprint arXiv:2503.24361 (2025).\n\n- Evaluation design and baseline fairness: VLA methods target open-world, diverse task sets, but your evaluation focuses on a small set of pick-and-place-like tasks derived from human demos. Yet you primarily compare against VLA baselines rather than recent human-demo learning methods. Therefore, unless your tasks align with the breadth and diversity of standard VLA benchmarks, comparing only to VLA is not a fair test of your contributions."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V2Ug5hc8jF", "forum": "vPKy3qQG3W", "replyto": "vPKy3qQG3W", "signatures": ["ICLR.cc/2026/Conference/Submission10037/Reviewer_yabZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10037/Reviewer_yabZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112575673, "cdate": 1762112575673, "tmdate": 1762921441240, "mdate": 1762921441240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}