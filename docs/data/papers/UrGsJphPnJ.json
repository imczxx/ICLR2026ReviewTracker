{"id": "UrGsJphPnJ", "number": 21184, "cdate": 1758314688261, "mdate": 1759896937369, "content": {"title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs", "abstract": "A primary challenge in developing large language models (LLMs) is their onerous pre-training cost. This paper explores a promising paradigm to improve LLM pre-training efficiency and quality by leveraging a small language model (SLM). In particular, this paradigm relies on an SLM to both (1) provide soft labels as additional supervision, and (2) select a small subset of valuable training examples. Put together, this enables an effective transfer of the SLM's predictive distribution to the LLM, while prioritizing specific regions of the training data distribution. Empirically, this leads to reduced LLM training time compared to standard training, while improving the overall quality. Theoretically, we develop a statistical framework to study the utility of SLMs in enabling efficient training of high-quality LLMs. Our framework characterizes how the SLM's seemingly low-quality supervision can enhance the training of a much more capable LLM. Furthermore, it also highlights the need for an adaptive utilization of such supervision, by striking a balance between the bias and variance introduced by the SLM-provided soft labels. We corroborate our theoretical framework by improving the pre-training of LLMs with 2.8B and 8.6B parameters by utilizing smaller LMs on the Pile dataset.", "tldr": "We leverage small LMs as teachers during knowledge distillation to improve large LM pre-training on both quality and training efficiency and rigorously support our methods with novel statistical results.", "keywords": ["Large language models", "knowledge distillation", "data selection", "efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b99d7eeb3e88f7df4654d0c086c5cbbe7d6b866.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SALT (Small model Aided Large model Training), a two-stage pre-training method for Large Language Models (LLMs). It leverages a Small Language Model (SLM) as a teacher for Knowledge Distillation (KD) in the initial stage, followed by standard pre-training. An optional extension $SALT_{DS}$ uses the SLM to select \"challenging yet learnable\" data for the KD phase. The authors provide a theoretical analysis presenting risk bounds for KD in the language modeling context. Empirically, SALT is shown to achieve performance comparable to or better than a standard baseline with fewer training steps (70%), resulting in significant wall-clock time savings (25-29%) for 2.8B and 8.6B models, and also improves performance after SFT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles the critical and practical problem of reducing the high computational cost of LLM pre-training.\n\nIt demonstrates significant wall-clock time savings (25-29%) on large-scale models (2.8B, 8.6B) while maintaining or improving performance over the baseline.\n\nThe paper provides a theoretical analysis by developing risk bounds for knowledge distillation specifically in the autoregressive language modeling context.\n\nThe improvements from SALT pre-training are shown to carry over to downstream tasks after supervised fine-tuning."}, "weaknesses": {"value": "The core components (small-to-large KD, early-stage KD, data selection via small models) are not new. The claimed novelty rests on a specific combination of these ideas, and the theoretical contribution appears to be an application of existing analyses to this specific setting.\n\nThe paper lacks direct empirical comparisons against stronger, contemporary baselines for both data selection and KD scheduling. It is also unclear if the standard baseline was sufficiently tuned."}, "questions": {"value": "Can you clarify the specific novel contributions over prior work (like Qin et al., 2022, Ankner et al. 2024) beyond scale and the specific data selection heuristic?\n\nThe method performed poorly with a 0.5B teacher. What is the explanation for this performance degradation, and what does it imply for the method's practical limits?\n\nIf training step is longer, does the improvement disappear?\n\nIf training data is more recent clean one like fineweb-edu, does the improvement disappear?\n\nHow about the cost of hyper parameter search? And, generalization toward reuse of the selected hyperparameters to other settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6lHlqBM0uy", "forum": "UrGsJphPnJ", "replyto": "UrGsJphPnJ", "signatures": ["ICLR.cc/2026/Conference/Submission21184/Reviewer_SNmq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21184/Reviewer_SNmq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393107429, "cdate": 1761393107429, "tmdate": 1762941589980, "mdate": 1762941589980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a core problem in the large language model (LLM) field: high pre-training costs. The authors propose a novel and efficient training paradigm, leveraging small language models (SLMs) to aid the training of LLMs, which they name SALT (Small model aided large model training).\n\nThe core contributions of this method are twofold:\n1.  **Theoretical Framework:** The paper first establishes a statistical framework to theoretically analyze the application of knowledge distillation (KD) in language models. Specifically, it explores the feasibility of \"reverse distillation\" (using a *weaker* SLM as a teacher to train a *stronger* LLM student). The theory reveals this as a **bias-variance trade-off**: soft labels from the SLM can reduce training variance, but due to its weaker capability, it also introduces bias (especially on \"hard\" samples).\n2.  **SALT Algorithm:** Guided by this theory, the authors designed the SALT algorithm. It is a **two-stage training method**:\n    * **Stage 1 (KD):** In the early phase of training ($n_{KD}$ steps), it uses the SLM as a teacher for knowledge distillation, capitalizing on its low bias and variance-reduction benefits in \"easy\" data regions.\n    * **Stage 2 (Standard):** In the subsequent phase, it switches back to standard (ground-truth-based) next-token prediction training, allowing the LLM to learn the \"hard\" samples that the SLM could not master.\n3.  **$SALT_{DS}$ (Data Selection):** The paper further proposes an extension, $SALT_{DS}$, where the SLM is also used for **data selection**. It uses a scoring function (Eq. 10) to filter for \"**challenging yet learnable**\" training sequences, specifically for the KD in the first stage.\n\n**Experimental Results:**\nThe authors validate their method by training 2.8B and 8.6B parameter LLMs on the Pile dataset (using 1.5B and 2.8B SLMs as teachers).\n* **Efficiency:** SALT-trained LLMs can match (or exceed) the performance of standard-trained (BASELINE) LLMs using **less than 70% of the training steps**.\n* **Performance:** The final SALT models outperform the BASELINE on a wide range of few-shot benchmarks and supervised fine-tuning (SFT) tasks.\n* **Time Savings:** This translates to an estimated **~25% (2.8B) to ~28% (8.6B) wall-clock time saving**."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Addresses a Critical Problem:** The work focuses on reducing LLM pre-training costs, which is a highly important and valuable research direction.\n2.  **Reported Efficiency Gains:** The paper reports significant wall-clock time savings (~25-28%). If robust, this result is of high practical value.\n3.  **Theoretical Framework:** The paper provides a theoretical framework that attempts to explain the \"weak-teacher-strong-student\" distillation mechanism via a bias-variance trade-off."}, "weaknesses": {"value": "1.  **Unclear and Minimal Contribution of $SALT_{DS}$:** This is a **major weakness**. The paper introduces $SALT_{DS}$ (with data selection) as a key extension, yet the experimental results show it offers **no significant or consistent benefit** over the simpler SALT baseline (e.g., 8.6B model avg @100% steps: SALT 52.96 vs $SALT_{DS}$ 52.81). This makes the contribution of the data selection part *nearly void*. The authors need to justify why this more complex method (requiring an extra SLM scoring pass) is proposed if it provides no tangible benefit.\n2.  **Severe Lack of Ablation for Data Selection:** The core of $SALT_{DS}$ is the scoring function in Eq. (10), which relies on a critical hyperparameter $k$ (set to 10) to define \"learnability\". The paper provides **no ablation or sensitivity analysis** on this $k$ value. What happens if $k=1$ or $k=50$? Without this analysis, the effectiveness of this data selection mechanism is **unproven**, and its design appears arbitrary.\n3.  **Unprincipled Choice of $n_{KD}$:** The choice of the transition point $n_{KD}$ (36K steps) seems **arbitrary**. While Appendix K shows robustness for $n_{KD}$ between 20k and 60k, the paper fails to discuss how one might *principally* determine this optimal transition point. Lacking this discussion makes the method feel more like a specific \"recipe\" than a general approach."}, "questions": {"value": "1.  **($SALT$ vs $SALT_{DS}$) Necessity of $SALT_{DS}$:** As noted in the weaknesses, the performance of $SALT$ and $SALT_{DS}$ is very close. Can the authors elaborate on whether the practical benefit of $SALT_{DS}$ justifies its additional complexity (i.e., a full forward pass over the pre-training data by the SLM)? Under what conditions would you expect $SALT_{DS}$ to *significantly* outperform $SALT$?\n2.  **($SALT_{DS}$) Data Selection Hyperparameter $k$:** The choice of $k$ in Eq. (10) seems critical. You used $k=10$. Did you experiment with other $k$ values? For example, how would performance be affected by a very small $k$ (e.g., $k=1$, focusing only on tokens the SLM gets right) or a very large $k$ (e.g., $k=100$)? This is important for understanding the definition of \"learnability.\"\n3.  **(SALT) Choice of Transition Point $n_{KD}$:** The ablation in Appendix K (Table 22) shows $n_{KD}=60K$ has slightly better average performance (47.99) than $n_{KD}=36K$ (47.94). You mention choosing 36K for efficiency. My question is: is there a *principled* way to determine the optimal $n_{KD}$? For example, does it correspond to a point where the SLM teacher's training loss begins to \"saturate,\" or some metric indicating that \"easy\" samples have been sufficiently learned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yLhgtxMyuP", "forum": "UrGsJphPnJ", "replyto": "UrGsJphPnJ", "signatures": ["ICLR.cc/2026/Conference/Submission21184/Reviewer_KDHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21184/Reviewer_KDHw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916302731, "cdate": 1761916302731, "tmdate": 1762941588927, "mdate": 1762941588927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SALT (Small‚Äëmodel Aided Large‚Äëmodel Training), a two‚Äëstage pre‚Äëtraining recipe: early knowledge distillation (KD) from a smaller teacher followed by standard next‚Äëtoken training; an extended variant (SALTDS) uses the SLM to select ‚Äúchallenging yet learnable‚Äù sequences for the KD phase (Algorithm 1, p. 5). A statistical framework (Theorems 3.2 & 3.4) provides excess risk bounds for language modeling under KD, highlighting a bias‚Äìvariance trade‚Äëoff where SLM‚Äëprovided soft labels can reduce variance if teacher‚Äìdata divergence remains small (pp. 3‚Äì4). Empirically, 2.8B/8.6B students pre‚Äëtrained on The Pile with UL2 show that SALT/SALTDS reach or exceed baseline few‚Äëshot performance using 70% of training steps and achieve ~25‚Äì29% wall‚Äëclock savings, with further downstream gains after supervised fine‚Äëtuning (Tables 1‚Äì4, pp. 7‚Äì8; Appendix J, p. 44). A teacher‚Äësize ablation (Table 3, p. 8) shows benefits diminish when the teacher is very small (0.5B)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, simple method with practical payoff. SALT‚Äôs two‚Äëstage schedule is easy to implement and consistently improves overall few‚Äëshot averages at the same step budget, and matches/beat baselines at 70% steps, delivering ~25‚Äì29% time savings in the authors‚Äô TPU setup (Table 2 Table 13).\n\n- Theory tailored to language modeling under KD. The paper gives sequence‚Äëlevel generalization bounds for KD in LMs (Theorem 3.4), relating gains to reduced variance vs teacher‚Äìdata divergence, and motivates selective/early KD (Remark 3.5 & ¬ß3.3). \n\n- Thoughtful diagnostics & positioning. The histograms of  (Fig. 2; Appendix C) empirically support the stability assumption; the bucketed analyses (e.g., Table 5 on XLSum‚ÄëEN,; Appendix M) illustrate that early KD mostly helps ‚Äúeasy‚Äù slices, which fits the theory and design.\n\n- Robustness checks. Ablations on transition strategies and KD duration and on teacher quality (Table 21) give a fuller picture; the method still helps across two student sizes (2.8B & 8.6B)."}, "weaknesses": {"value": "1. Fairness of the efficiency claim at 70% steps. The core headline‚Äî‚ÄúSALT surpasses BASELINE at 146k (~70%) steps‚Äù (Table 2 Table 13)‚Äîis not matched with a BASELINE@70% few‚Äëshot evaluation. The only ‚Äúearly‚Äù baseline shown near the main text is BASELINE@36k (Table 1 & Fig. 3), which is too early to compare to 146k. Without BASELINE@146k, it‚Äôs hard to attribute step‚Äëefficiency to KD rather than general training dynamics. Please include BASELINE evaluated at identical step counts reported for SALT/SALTDS.\n\n2. Theory‚Äìpractice gap in key assumptions. The token‚Äëlevel bound (Theorem 3.4) assumes a finite function class \nŒò and bounded per‚Äëtoken log‚Äëloss (Assumption 3.1), and quantifies bias via TV distance to the data conditional distribution‚Äîa quantity that‚Äôs intractable to estimate in practice. The empirical proxy uses completions from a strong LM ‚Äúoracle‚Äù, which approximates model‚Äëto‚Äëmodel divergence, not teacher‚Äëto‚Äëdata divergence; this weakens the validation of DIV Clarify how the bounds should be interpreted operationally given these gaps.\n\n3. Baselines could be stronger. Reverse KD (RKD) is a useful strawman but not a competitive baseline. Consider curriculum KD top‚Äëk token KD (Appendix A.2, p. 19) with tuned  ùëò or self‚Äëdistillation controls. For SALTDS, a data‚Äëselection baseline that mimics the same selected subset without KD would isolate the data‚Äëselection contribution more fairly.\n\n4. Statistical reporting. Few‚Äëshot results are single‚Äërun with no seeds/variance/confidence intervals. Given small deltas (e. g., +0.62 average points for 2.8B at 100% steps; Table 2), error bars are important.\n\n5. Cost accounting transparency. Wall‚Äëclock savings (25‚Äì29%) depend on specific hardware and rematerialization settings. Reporting FLOPs or a normalized throughput‚Äëadjusted cost would make claims more portable.\n\n6. Scope of evaluation. The Pile is English‚Äëheavy; a brief multilingual check or domain‚Äëshift test (e.g., code/math) beyond MBPP and MATH citations would strengthen generality claims. (You do show strong LAMBADA gains‚ÄîTable 2‚Äîbut broader coverage would help.)"}, "questions": {"value": "1. BASELINE@146k: Can you report few‚Äëshot averages and domain breakdowns for BASELINE at 146k steps (2.8B and 8.6B) to match the SALT reporting? This is critical for the step‚Äëefficiency claim.\n\n2. SALTDS selection: In Eq. (10) (p. 6), you use median of filtered per‚Äëtoken losses with top‚Äëk masking. Did you try mean/trimmed‚Äëmean or per‚Äëdocument entropy to balance ‚Äúchallenging yet learnable‚Äù? Any diversity constraint to avoid duplicative sequences?\n\n## Suggestions\n\n- Add BASELINE@70% few‚Äëshot (and maybe @80%, @90%) to align curves across methods. Also show SALT@70% vs BASELINE@70% on key individual tasks (beyond overall averages)\n\n- Report variability. Provide ‚â•3 seeds for the few‚Äëshot suite with mean¬±std or 95% CIs; likewise for SFT results\n\n- Stronger baselines. Include self‚Äëdistillation and top‚Äëk token KD; try KD weight annealing beyond the tested linear variants (Appendix K, Table 23)\n\n- Wider evaluation. Include multilingual (e.g., TyDi beyond English) or code/math reasoning benchmarks, and a brief data‚Äëshift analysis to test whether SALT‚Äôs gains persist out of distribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pyg16QfhK4", "forum": "UrGsJphPnJ", "replyto": "UrGsJphPnJ", "signatures": ["ICLR.cc/2026/Conference/Submission21184/Reviewer_UdEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21184/Reviewer_UdEv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945829463, "cdate": 1761945829463, "tmdate": 1762941588046, "mdate": 1762941588046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the use of pretrained smaller language model (SLM) to train larger ones via knowledge distillation (KD).\nThe paper first provide theoretical studies of the risk bound including the effects of KD.\nThen, it proposes a two-stage approach (SALT) of pretraining reusing SLM: First, perform KD to train the model (optionally select learnable data to further accelerate training). Then, pretrain the model as usual.\nThis utilize the intuition that SLM can help learning easier tasks, and at the latter stage the capacity of the larger model can be leveraged to further train itself.\n\nThe paper provides experimental validation comparing mainly with from-scratch pretraining and KD (without second stage). Results for post-training are also given."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides both theoretical and empirical contributions to KD with smaller models.\n- Experiments are performed with care, with ablation studies given to justify the hyperparameters used, etc."}, "weaknesses": {"value": "1. Data selection method is not so practical as it involves using early model checkpoint, which is not typically available if one wishes to use off the shelf public models. Moreover, it involves more hyper parameters for tuning.\n2. Lack of baseline: I think the proposed method should be compared to model growth methods as baselines. These methods are quite straightforward compared to SALT (simply stacking or expanding the widths works well; [2405.15319]) and are also reusing small models to accelerate the training of large ones.\n3. While the theory provides some clue on why KD helps, it does not provide concrete guidance on when or how to transition from KD to pretraining without it. Using a two stage training process works well intuitively without the theory as well (weak teacher can provide supervision to a strong but blank-state student only in the beginning), making the theoretical contribution seemingly redundant."}, "questions": {"value": "1. The accuracies seem to be converging with larger train steps. I wonder if at even larger train steps from-scratch pretraining becomes better and KD becomes unnecessary?\n2. The improvement seems to be small compared to from-scratch pretraining. I wonder, when both baseline and KD are compared under equal compute (KD requires extra compute due to the inference of SLM), from-scratch pretraining could perform better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WXNFB38frV", "forum": "UrGsJphPnJ", "replyto": "UrGsJphPnJ", "signatures": ["ICLR.cc/2026/Conference/Submission21184/Reviewer_XJTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21184/Reviewer_XJTj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987545840, "cdate": 1761987545840, "tmdate": 1762941586468, "mdate": 1762941586468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}