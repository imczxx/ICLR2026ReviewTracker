{"id": "cC3TW1s309", "number": 18623, "cdate": 1758289555364, "mdate": 1759897091088, "content": {"title": "Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding", "abstract": "Despite recent progress in vision-language models (VLMs), holistic understanding of long-form video content remains a significant challenge, partly due to limitations in current benchmarks. Many focus on peripheral, \"needle-in-a-haystack\"  details, encouraging context-insensitive retrieval over deep comprehension. Others rely on large-scale, semi-automatically generated questions (often produced by language models themselves) that are easier for models to answer but fail to reflect genuine understanding. In this paper, we introduce $\\textbf{MF$^2$}$, a new benchmark for evaluating whether models can comprehend, consolidate, and recall key narrative information---requiring integration of both visual and linguistic modalities---from full-length movies ($\\textbf{50-170 minutes long}$). MF$^2$ includes over 50 full-length, $\\textbf{open-licensed}$ movies, each paired with manually constructed sets of claim pairs---one true (fact) and one plausible but false (fib), totalling over 850 pairs. These claims target core narrative elements such as $\\textbf{character motivations}$ and $\\textbf{emotions}$, $\\textbf{causal chains}$, and $\\textbf{event order}$, and refer to $\\textbf{memorable moments}$ that humans can recall without rewatching the movie. Instead of multiple-choice formats, we adopt a binary claim evaluation protocol: for each pair, models must correctly identify both the true and false claims. This reduces biases like answer ordering and enables a more precise assessment of reasoning. Our experiments demonstrate that both open-weight and closed state-of-the-art models fall well short of human performance, underscoring the relative ease of the task for humans and their superior ability to retain and reason over critical narrative information---an ability current VLMs lack.", "tldr": "A benchmark for long video understanding", "keywords": ["vision-language models", "long video understanding", "memory consolidation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2fa5171aed3771de3b7cdba3a7ce5fd97b67273e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MF^2 (Movie Facts and Fibs), a new benchmark designed to evaluate the holistic narrative comprehension of Vision-Language Models (VLMs) across full-length movies, which typically last between 50 and 170 minutes. The authors argue that existing benchmarks fail to assess genuine understanding, instead focusing on narrow retrieval tasks that resemble a \"needle-in-a-haystack\" approach. MF2 features over 850 manually constructed contrastive claim pairs (a true \"fact\" and a plausible \"fib\") for 53 open-licensed films, which require reasoning about core elements like character motivations, causal relationships, and emotional arcs. The evaluation protocol demands that models correctly identify both the true and false claims in a pair, and initial experiments reveal a significant performance gap between state-of-the-art models (Gemini 2.5 Pro) and human evaluators."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The core strength of MF^2 is its dedicated focus on evaluating genuine narrative comprehension rather than shallow retrieval or memorization\n- MF^2 contains over 850 manually constructed contrastive claim pairs (fact/fib). This manual effort combats the issues seen in other benchmarks that rely on large-scale, semi-automatically generated questions, which may suffer from model biases\n- The dataset uses 53 full-length movies, with an average duration of 88.33 minutes. This scale is longer than most existing video understanding datasets, which often rely on short clips.\n- The code and dataset for reproduction are included, which is nice\n- The paper is well-written, and the flow is easy to follow"}, "weaknesses": {"value": "- Models and humans evaluate each claim independently without knowledge of the paired (fact/fib) structure during prediction. The challenging pairwise accuracy metric (requiring both fact and fib to be correct) is computed after the predictions are made. I don’t see why such a post-hoc setup is necessary\n- The strategy used to prevent data contamination, while effective, introduces a bias toward a specific style of filmmaking. Added to the fact that the video quality will not be as good since the movies are very old. I am wondering how much of this is to blame for the low performance of some models.\n- The dataset’s evaluation protocol means the task is essentially two separate binary classification tasks, rather than a single task requiring the model to resolve the subtle differences between two highly similar statements. The benchmark thus measures independent truth prediction ability, but not necessarily the model's ability to perform contrastive reasoning based on the textual juxtaposition of the claims themselves"}, "questions": {"value": "See weaknesses plus\n- Would focusing on older films released between 1920 and 1970 be an issue, since video quality was not as good back then, thus increasing the difficulty of the dataset?\n- How is this dataset different from Tropes datasets which also use binary classification and long movies, and which often focus on narrative reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GMMcubkQ9R", "forum": "cC3TW1s309", "replyto": "cC3TW1s309", "signatures": ["ICLR.cc/2026/Conference/Submission18623/Reviewer_KDA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18623/Reviewer_KDA2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816716049, "cdate": 1761816716049, "tmdate": 1762928337677, "mdate": 1762928337677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a Q&A benchmark for long movie understanding. The key novelty of the benchmark is that each Q (850 in total) has two possible answers, one true (fact) and one false (fib) and the goal is to correctly understand both the true and false answers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper touches a very important part in movie understanding, i.e. the ability of MLLMs to understand true facts from counterfactual events that could be plausible or sound realistic. I think this is a very nice ability for the MLLMs to account for. \n\n- The paper is well-written and easy to follow. \n\n- The work offers high-quality human annotations (not automatically done)."}, "weaknesses": {"value": "There are several drawbacks associated with the work.\n\n1/ Evaluation\n\n1i/ Subtitles only. \nThe paper analyses how much subtitles help visual information; however, most recent studies have shown the big modality gap in these MLLMs, which begs the question whether the proposed questions can be answered solely with subtitles. Table 4 further supports this: it shows that subtitles are the ones providing the most information to reach the target performance. In this case, why is video and generally visual information needed? \n\n1ii/ Pairwise accuracy metric. \nAlthough I appreciate proposing a new metric and understand that it’s hard to convince the audience, the pairwise accuracy seems weird and is somehow tailored to artificially lower performance. It is unclear what this metric offers more than simply accuracy.\n\n2/ Experimental setup with the pairwise accuracy computation.\nGiven this metric, it is unclear how logits are being transformed into pairwise accuracy. To my understanding, each option in the Fib & Fact is treated independently. However, I think it would be essential to have more evaluations with different assignments. Some examples, given that only one statement is true in every answer (and one is false), are: \n(i) if the first one is assigned true, the second should be assigned as false (and vice versa);\n(ii) assign the true or false based on the logits;\n(iii) ask the MLLM if it is certain of its choice. \nThis (non-exhaustive) list may help us see whether existing VLMs actually confuse fibs from facts in this setting. \n\n3/ The examples in the sup mat show that aquarium vs bus or appeared happy vs appeared bothered. \nThese counterfactuals are visible by single frames or even simply by text. It is unclear why both modalities are needed. A good way to evaluate this would be to remove the word completely (instead of changing it) and ask the VLM to predict it (this or a synonym), similar to “fill the gap” part. \n\n4/ Inconsistent results between Table 3 and Table 4.  \n\n5/ Data contamination. \nTable 4 shows the zero shot performance to be 66.5% with only movie titles! This clearly shows that there is a strong data leakage. This suggests that no matter the design of evaluation protocols and confusion, the models have seen the data. This makes it hard to know whether performances come from models, params or simply data leakage. \n\n6/ Limited evaluation protocol. \nEvaluating on multiple-choice QA is limited compared to open-ended or generative movie understanding tasks. It is unclear whether performance correlates with genuine scene-level understanding.\n\n7/ Data diversity. \nGiven that all movies come from the 20th century, this limits the diversity of the data. \n\n8/ Dataset scale. \nThe dataset contains 53 movies and approx. 850 Q&A pairs, which makes it small compared to modern multimodal datasets. The scale together with the limited diversity may compromise any drawn conclusions as a model may overfit to these data/domains without actual generalization capabilities. \n\nMinor comment. \nGiven English subtitles, it would be interesting to have a discussion about language bias."}, "questions": {"value": "For questions, it would be great if the authors could address some points from the weaknesses above. Specifically: \n\nQ1 (W1i) It would be useful to have performances by using only subtitles. \n\nQ2 It would be great if the authors showed the need for visual understanding. One way to do this would be by examining whether models actually use the visual stream vs mostly (or solely) the textual one, for example, by using counterfactual visual cues or by shuffling scenes.\n\nQ3 (W1ii, W2) Pairwise accuracy metric.\n\nQ4 It would be great if the authors could provide some failure case analysis, showing reasoning errors or multimodal misalignments, especially to show the need for vision. \n\nQ5 (W5) Given the data contamination observed in Table 4, it would be beneficial if the authors could test this by using fictional or synthetic movie titles or shuffling titles or reusing parts from other movie titles?\n\nQ6 (W7, W8) It would be great if the authors can explain how to prevent overfitting, especially if the dataset is intended for leaderboard-style competition. \n\nQ7 (minor) It would be useful to check if model predictions correlate with the number of frames sampled per movie, for example, by examining if more visual input (ie more frames) improves reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mfr59uBogY", "forum": "cC3TW1s309", "replyto": "cC3TW1s309", "signatures": ["ICLR.cc/2026/Conference/Submission18623/Reviewer_rdkg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18623/Reviewer_rdkg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843058370, "cdate": 1761843058370, "tmdate": 1762928337110, "mdate": 1762928337110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MF2, a novel benchmark designed to evaluate holistic narrative understanding in full-length movies (50-170 minutes) by requiring models to distinguish between manually crafted fact-fib claim pairs targeting core story elements like character motivations and causal chains. The benchmark includes 53 open-licensed films and over 850 contrastive pairs, adopting a binary evaluation protocol to reduce biases associated with multiple-choice formats and emphasize reasoning over retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark's use of open-licensed content and human-annotated claims ensures reproducibility and high-quality labels, addressing gaps in existing datasets prone to copyright issues or automated generation. The contrastive claim design and granular categorization (e.g., single-scene to global reasoning) enable a nuanced assessment of narrative comprehension, with human baselines highlighting the task's feasibility for humans but difficulty for models."}, "weaknesses": {"value": "1.\tThe movies in MF2 are exclusively from 1920–1970 (to avoid data contamination), lacking modern films with contemporary narrative styles, visual effects, or cultural contexts. This limits the generalization of results to real-world scenarios involving recent long videos (e.g., modern films, documentaries). It is suggested that the authors discuss this limitation.\n2.\tAll annotators are co-authors of the paper, rather than independent external annotators. This may introduce subjective biases in claim design (e.g., consistent preferences for certain narrative elements) and quality control, weakening the objectivity of the benchmark.\n3.\tThe experiments only evaluate a narrow set of mainstream VLMs (e.g., Gemini 2.5 Pro, InternVL3). This fails to test the benchmark’s adaptability to diverse model architectures. It is recommended that the authors follow LVBench and evaluate more VLMs."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SyHnNH8tfG", "forum": "cC3TW1s309", "replyto": "cC3TW1s309", "signatures": ["ICLR.cc/2026/Conference/Submission18623/Reviewer_Nok5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18623/Reviewer_Nok5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965714475, "cdate": 1761965714475, "tmdate": 1762928336619, "mdate": 1762928336619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MF², a new benchmark designed to evaluate narrative comprehension in long-form movies (average 88 minutes, 53 movies in total). \n26 humans watched movies and generated paired statements consisting of an accurate factual description (fact) and a minimally modified false statement (fib). \nFor each pair, they specified the required scene granularity (single/multi/global) for answering and selected one or more comprehension dimension from {event/entity understanding, temporal perception, emotion understanding, causal reasoning}. \nIn total, 868 pairs were constructed. \nThis paper provide the result on evaluated performance using both publicly available recent VLM models and proprietary models such as GPT-4o and Gemini 2.5 Pro, alongside human assessments. \nAlso, the results of ablation studies are provided to examine the impact of input modality in (video,  subtitle)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Benchmark datasets are valuable assets for our community. The authors commit to releasing the full dataset, code, and movies, ensuring reproducibility and supporting future research.\n- Movies are long-form video that mostly self-contained stories. MF² focuses on holistic narrative understanding on full-length movies, requiring models to reason about the story’s core elements, unlike previous benchmarks that emphasize “needle-in-a-haystack” details. This work seems to pioneer a new area to deal with holistic understanding and reasoning across the entire narrative in movie-level videos.\n- MF² is expected high-quality, human-annotated benchmark dataset constructed with intensive human labors including full-time watching movies.\n- The multi-faceted analysis—across models, modalities, reasoning types, and comprehension dimensions—is thorough and convincing. The inclusion of human baselines is particularly valuable. It is good information as a baseline for potential users to choose the proposed benchmark dataset."}, "weaknesses": {"value": "- While minimal-edit fibs are effective and annotators filters ambiguous cases, some may be too obvious or, conversely, too subtle, potentially confusing both humans and models.\n- Considering the previous benchmarks, this reviewer do not intend to challenge the current configuration. While the authors directly compares the values from the models and humans presented in Table 3, humans would need to watch the video and subtitles without sound for fair comparison. \n- For multi-scene cases, the range of multiples is not provided. I think it would be helpful to show its distribution, if the values are collected.\n- “global” claims seems to be new and substantial problems to address the overall understanding of the movie. However, it seems there are only 59 cases. Considering the total number of movies, 53, there are only 1.11 claims per a movie. As a main contribution, it looks too small.\n- (Minor) In Figures 3-5, the graph visibility could be improved. Rather than simply distinguishing by color, adding another method would make it more visible in grayscale printout."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zz8dRFsCmF", "forum": "cC3TW1s309", "replyto": "cC3TW1s309", "signatures": ["ICLR.cc/2026/Conference/Submission18623/Reviewer_BnV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18623/Reviewer_BnV2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762271358828, "cdate": 1762271358828, "tmdate": 1762928336085, "mdate": 1762928336085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MF2, a benchmark for evaluating long-form movie understanding in vision-language models. It contains 53 open-licensed movies and 868 fact–fib pairs testing reasoning over causal, emotional, temporal, and event aspects. Using a binary claim evaluation protocol, MF2 avoids multiple-choice biases and better measures narrative reasoning. Experiments show that state-of-the-art models (e.g., GPT-4o, Gemini 2.5 Pro) perform well below humans, especially in global and emotional reasoning, revealing current limitations in long-term narrative understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. MF2 introduces a new evaluation paradigm for long-form narrative video understanding, moving beyond prior benchmarks that focus on short-term visual recall or detail-oriented retrieval. It provides a more realistic and cognitively demanding test of narrative comprehension.\n\n2. All samples are manually constructed by the research team after watching entire movies, ensuring annotation quality and consistency. The dataset covers multiple reasoning granularities (single-scene, multi-scene, global) and diverse comprehension dimensions (event, temporal, causal, and emotional understanding), offering a comprehensive assessment of model capabilities."}, "weaknesses": {"value": "- **Overstated Novelty Compared to Existing Long-Video Benchmarks:** While MF2 leverages full-length movies to evaluate long-term reasoning, there already exist several general long-video understanding benchmarks. The authors’ claim that prior works focus only on \"peripheral or low-level details\" and lack \"abstractive understanding of the central storyline\" is somewhat overstated. For instance, HourVideo also involves hour-long videos and tasks such as causal and counterfactual reasoning, which similarly require comprehensive narrative understanding.\n\n- **Limited Movie Diversity:** The dataset mainly consists of public-domain films from 1920–1970, which ensures openness but results in relatively narrow thematic and stylistic diversity. This may limit the benchmark’s generalizability to modern, culturally diverse, or visually complex narratives found in contemporary media."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n3S37c6eEN", "forum": "cC3TW1s309", "replyto": "cC3TW1s309", "signatures": ["ICLR.cc/2026/Conference/Submission18623/Reviewer_J9hv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18623/Reviewer_J9hv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762450376805, "cdate": 1762450376805, "tmdate": 1762928335551, "mdate": 1762928335551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}