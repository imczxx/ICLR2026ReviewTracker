{"id": "E0ZAcqy9TB", "number": 1629, "cdate": 1756899018692, "mdate": 1763533479428, "content": {"title": "Video-GPT via Next Clip Diffusion", "abstract": "GPT has shown its remarkable success in natural language processing. However, the language sequence is not sufficient to describe spatial-temporal details in the visual world. Alternatively, the video sequence is good at capturing such details. Motivated by this fact, we propose a concise Video-GPT in this paper by treating video as new language for visual world modeling. By analogy to next token prediction in GPT, we introduce a novel next clip diffusion paradigm for pretraining Video-GPT. Different from the previous works, this distinct paradigm allows Video-GPT to tackle both short-term generation and long-term prediction, by autoregressively denoising the noisy clip according to the clean clips in the history. Extensive experiments show our Video-GPT achieves the state-of-the-art performance on video prediction, which is the key factor towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in both video generation and understanding, showing its great generalization capacity in downstream.", "tldr": "Video-GPT treats video as new language for visual world modeling.", "keywords": ["Video; Diffusion; LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ebfe6089c7f9abc727341dbb91fc6aea03b6de3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Video-GPT, a novel foundation model for video generation and understanding, built upon an elegant analogy of treating video as a new language. The core contribution is a \"next clip diffusion\" pretraining paradigm, which ingeniously combines autoregressive modeling and diffusion. By treating video clips as \"visual words,\" the model autoregressively predicts the next clip by denoising a noisy version, conditioned on the history of previously generated clean clips. This self-supervised approach allows for effective pretraining on large-scale unlabeled video data. The pretrained Video-GPT achieves state-of-the-art performance on the Physics-IQ benchmark, demonstrating a strong capacity for world modeling, and shows excellent generalization across six diverse downstream video generation and understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed \"next clip diffusion\" paradigm is a novel and insightful method for unifying autoregressive and diffusion models for video. The concept of conditioning the denoising of a future clip on the clean history of past clips is a clever and distinct approach that effectively leverages the strengths of both modeling families for long-term video prediction.\n2. The paper presents exceptionally strong empirical results. Achieving a state-of-the-art score of 34.97 on the Physics-IQ benchmark, significantly outperforming prior work, is a standout achievement that validates the model's ability to learn physical dynamics. Furthermore, the model's strong performance across a wide array of 6 downstream tasks (including generation and understanding) underscores its quality and versatility as a powerful video foundation model. The ablation studies are thorough and convincingly support the main design choices."}, "weaknesses": {"value": "1. The proposed input formulation, an interleaved sequence of noisy and clean clips [NS(1), CL(1), ..., NS(K), CL(K)], effectively doubles the sequence length processed by the transformer compared to methods that only use historical context. Given the quadratic complexity of attention, this could be a significant computational bottleneck, potentially limiting scalability. A brief analysis of the computational trade-offs would be beneficial.\n2. The paper states that frames are divided into K clips, K∼Uniform{2,3,...,N}. This process is central to the method, but its details are sparse. It is unclear if clips are contiguous blocks or formed differently. The impact of this random clip partitioning strategy on training stability and performance is not ablated, yet it seems like a critical hyperparameter.\n3. While the paper provides a good overview of related work, the discussion on how \"next clip diffusion\" specifically differs from other recent hybrid autoregressive-diffusion models for video (e.g., VideoPoet, SEINE) could be more detailed. A deeper comparative analysis would help to better contextualize the novelty of the proposed conditioning and generation scheme"}, "questions": {"value": "1. The inference process is autoregressive, where the model's own generated (denoised) clips are used as the clean history for subsequent steps. Have you investigated the model's robustness to error accumulation? For instance, does a minor artifact in a generated clip DNS(k)$ degrade the quality of all future clips?\n2. You mention that the model is trained to predict the clean clip directly (x-prediction) rather than the noise (ϵ-prediction) or velocity (v-prediction) to keep the training simple. This is a departure from many modern diffusion frameworks. Could you elaborate on this design choice? Did you experiment with other prediction targets, and did x-prediction yield superior performance?\n3. The progressive training strategy in Table 1, which starts with short clips (effectively next-frame prediction) and gradually increases clip length, is interesting. Could you provide more intuition on why this curriculum is effective? Does learning fine-grained temporal dynamics first provide a better foundation for the model before it tackles longer-range dependencies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K3nhiwPTpG", "forum": "E0ZAcqy9TB", "replyto": "E0ZAcqy9TB", "signatures": ["ICLR.cc/2026/Conference/Submission1629/Reviewer_wpuq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1629/Reviewer_wpuq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380070802, "cdate": 1761380070802, "tmdate": 1762915836545, "mdate": 1762915836545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reply to All Reviewers"}, "comment": {"value": "Dear Reviewers,\n\nWe would like to extend our sincerest gratitude for your time and for providing such insightful and constructive feedback on our paper, \"Video-GPT via Next Clip Diffusion.\"\n\nWe are greatly encouraged that you found our work to have \"**excellent soundness and presentation**\" (Reviewer sVcH), and recognized our core contribution as a \"**creative combination**\" (Reviewer vBbq) and a \"**novel and insightful method**\" (Reviewer wpuq). We are also pleased that you acknowledged the \"**impressive engineering effort**\" (Reviewer 8PYT) and the \"**exceptionally strong empirical results**\" (Reviewer wpuq), particularly our state-of-the-art performance on the Physics-IQ benchmark.\n\nIn the individual responses below, we have addressed every question and concern in detail. To summarize our major updates and clarifications:\n1.  **New Benchmarks:** We added results on **EvalCrafter** to demonstrate our model's visual quality and motion consistency beyond physics modeling.\n2.  **Ablation Studies:** We included extensive ablations on model architecture (Vanilla vs. DiT), VAE choices (2D vs. 3D), and prediction targets ($x$ vs. $v$).\n3.  **Long Video Generation:** We provided qualitative comparisons (1-minute generation) against Open-Sora-Plan to demonstrate temporal coherence.\n4.  **Clarifications:** We refined our terminology (e.g., \"clip-level token\") and provided pseudocode for our sampling strategy.\n\nWe have uploaded a revised manuscript reflecting these changes. If you have any questions please feel free to give us feedback and we will try our best to answer them.\n\nSincerely,\n\nThe Authors"}}, "id": "ooWyd65mb0", "forum": "E0ZAcqy9TB", "replyto": "E0ZAcqy9TB", "signatures": ["ICLR.cc/2026/Conference/Submission1629/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1629/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1629/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763533343935, "cdate": 1763533343935, "tmdate": 1763548869008, "mdate": 1763548869008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Video-GPT, a foundation model for video pre-training based on a \"next clip diffusion\" paradigm. The core idea is to treat video clips as analogous to words in a sentence. The model is trained to denoise a future \"noisy\" clip conditioned on a history of preceding \"clean\" clips, effectively combining an autoregressive structure at the clip level with a diffusion process for content generation within each clip. The authors demonstrate the model's effectiveness on video prediction benchmarks and show its generalization capabilities by fine-tuning it on six diverse downstream video generation and understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The primary strength of this work is the impressive engineering effort demonstrated in building and evaluating a complete system. The model achieves a state-of-the-art score on the Physics-IQ benchmark, suggesting its pre-training paradigm is effective at capturing physical dynamics and motion continuity. Furthermore, the extensive fine-tuning across a wide array of both generation and understanding tasks showcases the versatility and potential of the resulting pretrained model."}, "weaknesses": {"value": "Despite the strong results on specific benchmarks, this paper has significant weaknesses that undermine its contribution as a top-tier research publication. Firstly, the technical novelty is limited; the \"next clip diffusion\" idea is a combination of existing autoregressive and diffusion frameworks rather than a fundamental new technique. Secondly, and more critically, the evaluation feels dated and deliberately avoids direct comparison with the true state-of-the-art in video generation quality. The paper heavily relies on the Physics-IQ benchmark while making no qualitative or quantitative comparisons against contemporary leading models known for their visual fidelity. This positions the work more as a technical report for an existing system than a paper pushing the research frontier, especially given the rapid progress in the field over the past year."}, "questions": {"value": "The central question is regarding the evaluation strategy. Why did the authors choose to focus on the Physics-IQ benchmark and omit direct, qualitative side-by-side comparisons with state-of-the-art open-domain video generation models that are the current de facto standard for assessing generation quality? Without this comparison, the claims of SOTA performance feel narrow and potentially misleading."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "reKZMFXqzj", "forum": "E0ZAcqy9TB", "replyto": "E0ZAcqy9TB", "signatures": ["ICLR.cc/2026/Conference/Submission1629/Reviewer_8PYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1629/Reviewer_8PYT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906779401, "cdate": 1761906779401, "tmdate": 1762915836381, "mdate": 1762915836381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Video-GPT, a concise large video foundation model that unifies autoregressive modeling and diffusion via a novel \"next clip diffusion\" paradigm. Inspired by GPT’s next token prediction, the model treats video clips as \"visual words\" to model spatial-temporal details in the visual world—addressing the limitation of language sequences in capturing such details. The key design involves constructing interleaved sequences of noisy and clean clips, using hierarchical attention masking to leverage historical clean clips for denoising future noisy clips."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The \"next clip diffusion\" paradigm is a creative combination of autoregressive modeling (from GPT) and diffusion (for high-quality generation). Treating clips as visual words and using historical clean clips as context for denoising is a novel adaptation of language modeling to video, filling the gap between discrete text tokens and continuous video data. This hybrid design effectively unifies short-term generation and long-term prediction.\n2. As a unified video foundation model, Video-GPT bridges video generation and understanding tasks, advancing the goal of visual world modeling. Its strong performance on physics-aware prediction (Physics-IQ) indicates progress in learning world knowledge from video.\n3. Its generalization to six downstream tasks highlights its potential as a backbone for diverse video applications."}, "weaknesses": {"value": "1. Insufficient comparison with hybrid baselines: The paper mentions prior works that combine diffusion and autoregressive modeling but lacks a detailed comparison of their core differences. \n2. There is a lack of comparisons with some newer autoregressive + diffusion video generation models, such as self-forcing, apt2.\n3. Limited analysis of architectural choices: The model inherits Phi-3-mini’s architecture and SDXL’s VAE without justifying these choices. There is no comparison with other architectures (e.g., DiT, U-Net) or VAEs (e.g., 3D VAE vs. 2D VAE) to show whether these selections are critical to performance. Additionally, the progressive training strategy’s effectiveness is only validated via frame count ablation, without analyzing how frame interval or clip number affects convergence."}, "questions": {"value": "1. Could you clarify the core differences between Video-GPT’s \"next clip diffusion\" and prior hybrid diffusion-autoregressive models? Specifically, how does your clip-level autoregressive design and hierarchical masking outperform their frame-level or pixel-level combinations?\n2. The paper compares the performance of Video GPT with other models in video prediction, achieving SOTA in Physics-IQ. However, it doesn't examine its performance in other aspects that need to be considered in video generation, such as motion quality and subject consistency.\n3. How does Video-GPT perform on longer videos (e.g., 1 minute or more) in terms of temporal coherence and content consistency? Have you tested it against long-video generation models (e.g., Flexifilm, Open-Sora-Plan) and observed any performance degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8qGmDsDzw6", "forum": "E0ZAcqy9TB", "replyto": "E0ZAcqy9TB", "signatures": ["ICLR.cc/2026/Conference/Submission1629/Reviewer_vBbq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1629/Reviewer_vBbq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980426099, "cdate": 1761980426099, "tmdate": 1762915836265, "mdate": 1762915836265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This Manuscript introduces `video-gpt`, a generative self-supervised solution to represent videos. The main idea is to train video embeddings in a GPT style, where each clip is acting as a token. The training idea is to have interleaved noisy and clean clips, and the training objective is to de-noise the noisy clips."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "State of the results on multiple tasks is the main strength of this paper.\n\nThe idea of interleaved clip level noise/de-noise, although intuitive, is novel IMO.\n\nVideo level self-supervision has been overlooked, IMO. Research like this can bring more attention and opens the road for future works in video domain."}, "weaknesses": {"value": "Some design choices for training is not trivial to me and I need more clarification. (will ask in question section).\n\nI believe such a method works only on single camera and continious (or single scene) videos. If there is a POV change in a video, like Movies or TV shows, I believe that it will break the whole network.\n\nMotion is not modeled very well in this work. I am curious to know how this model can predict videos where there is partly stationary clips (minimal motion) and partly abrupt motion."}, "questions": {"value": "1- Authors propose `Clips as tokens` but later they propose frame-level and patch-level masking. It reads to me as `partial-token` masking. IMO, it would be better not to name Clips as tokens.\n\n2- What is the intuition behind having interleaved noisy and clean clips during training? Why not going with a classic next frame prediction formulation and have `k clean clips` and diffuse the `k+1th noisy clip`? What is the advantage of interleaved modeling? \n\n3- Why `from scratch` model performs poorly in `Tab 6`?\n\n4- In Section 3.3, are all previous K clips (some of them being generated diffused clips) being used to predict the k+1? or there is a limit on K to keep the context window capped?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EBBnYjrMIt", "forum": "E0ZAcqy9TB", "replyto": "E0ZAcqy9TB", "signatures": ["ICLR.cc/2026/Conference/Submission1629/Reviewer_sVcH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1629/Reviewer_sVcH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031245689, "cdate": 1762031245689, "tmdate": 1762915836122, "mdate": 1762915836122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}