{"id": "ATSNt0erIr", "number": 677, "cdate": 1756767282395, "mdate": 1759898247747, "content": {"title": "Learn Bullish Moves via EigenCluster Tokens", "abstract": "Conventional tokenization schemes, such as point-wise and patch-wise methods, are poorly suited for financial time series data due to excessive token counts, sparse distributions, and heightened out-of-vocabulary risks---an issue not explicitly addressed in prior work. This paper introduces a novel tokenization approach for financial time series. By clustering scalar projections of eigenvectors from multi-window Open-High-Low-Close (OHLC) price matrices, our method generates compact and semantically meaningful tokens, enabling Transformer-based models to effectively identify next-day close price increase patterns. Extensive experiments on S\\&P 500 and CSI 300 datasets show our approach outperforms market baselines by 6--9\\% in precision, while reducing token vocabulary size to 51--101 tokens and sequence length by 75\\% versus point-wise.", "tldr": "A clustering-based tokenization method for financial time series that improves bullish signal prediction in Transformers.", "keywords": ["Time Series Tokenization", "Financial Transformer", "Bullish Signal Prediction", "Eigen-Cluster Analysis", "Multi-scale Representation"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fbb3188ba29e5acfc5610ed67ba194c40c749b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EigenCluster Tokens, a clustering-based tokenization method for financial time series that uses eigendecomposition and multi-scale analysis to generate compact, meaningful tokens. It enables Transformers to predict bullish movements with 6–9% higher precision than market baselines, reduces token sequence length by 75%, and achieves superior efficiency and real-world trading performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Originality: Proposes a novel, domain-specific tokenization paradigm—EigenCluster Tokens—by combining spectral decomposition and clustering for financial time series, reframing prediction as bullish cluster identification.\n+ Quality: Methodologically rigorous with sound mathematical formulation and thorough experiments across multiple datasets, baselines, and ablation settings.\n+ Clarity: Well-structured and clearly written, with intuitive visualizations (e.g., multi-scale workflow, cluster separation) and logical progression from problem to solution.\n+ Significance: Highlights tokenization—not architecture—as the key bottleneck in financial forecasting with Transformers. Offers practical benefits: compact representations, faster inference, and actionable trading signals."}, "weaknesses": {"value": "+ Lack of Justification for Key Methodological Choices:\n  + Arbitrary Scalar Projection Function: The function to project high-dimensional features into a scalar (Eq. 6), using a specific combination of $sin()$ and $L2$-norm, lacks theoretical or intuitive explanation. It is unclear why this complex form is superior to simpler, more interpretable alternatives (e.g., using the principal components directly).\n  + Counter-intuitive \"Bullish Cluster\" Criterion: The rule for selecting the \"bullish cluster\" by favoring the \"smallest\" size (Eq. 9) is counter-intuitive. This could lead the model to overfit to a few unrepresentative outliers. A more robust criterion would balance the cluster's size with its predictive precision.\n+ Insufficient and Potentially Misleading Comparisons:\n  + Weak Baseline Models: The comparison is primarily against many weak baseline (e.g. XGBoost, LSTM, Lasso), which are not state-of-the-art baselines. A more convincing evaluation would involve comparing against established time series models like PatchTST or Autoformer.\n  + Overly Simplistic Financial Backtest: The trading backtest is insufficient as it only compares against market indices. It should include stronger quantitative baselines (e.g., Time-Series Momentum) and report standard risk-adjusted performance metrics like the Sharpe Ratio and Maximum Drawdown.\n+ Serious Concerns about Generalizability:\n  + Hand-Tuned Hyperparameters: The formula for selecting the optimal number of clusters $K$ (Eq. 10) and its weights were empirically hand-tuned on a single dataset. This is a classic case of \"hyperparameter fitting\" that severely undermines the method's credibility and claims of generalization, making it feel more like a custom solution than a general framework.\n  + Lack of Online Applicability: The entire tokenization process is performed offline. The paper fails to discuss how the method would adapt to continuously arriving new data in a real-world, online trading scenario, which is a critical practical omission."}, "questions": {"value": "1. My primary concern is the justification for using a deep 8-layer Transformer with an extremely short input sequence of 9 tokens, trained on a limited set of index data. This setup raises significant questions about overfitting and the actual utility of a complex self-attention architecture. To address this, please **provide plots of the training and validation loss curves** from your experiments to demonstrate that the model is learning generalizable patterns rather than memorizing the training data. Additionally, please provide a clear justification—ideally supported by a comparative experiment with a simpler model like an LSTM—for why a deep Transformer is necessary and effective in this short-sequence, low-data regime.\n2. The question to addresses the computational scalability of the multi-scale tokenization. The proposed method requires $n$ separate eigendecompositions on increasingly large matrices, which seems computationally prohibitive for the longer input sequences (e.g., n > 96) common in time series forecasting. To assess the practical viability of your approach, could you please provide both a formal complexity analysis and an empirical study showing **how the tokenization time, training time, and model precision scale as the sequence length $n$ increases**? This is crucial to understand whether the performance benefits justify the potentially exponential growth in computational cost.\n3. The concern about the validity of the baseline comparisons. The paper compares its method against non-standard, discretized versions of *point-wise* and *patch-wise* tokenization. The standard approach in modern time series Transformers is to directly project continuous patches into the embedding space via a linear layer, which is a stronger baseline. To properly validate the contribution, the authors must add an experiment comparing their method against this standard **linear projection baseline** under the same model architecture. Without this, the paper's claims of superiority are not sufficiently substantiated, as it has not competed against the most common and robust alternative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QKn9jDMyk6", "forum": "ATSNt0erIr", "replyto": "ATSNt0erIr", "signatures": ["ICLR.cc/2026/Conference/Submission677/Reviewer_a6aw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission677/Reviewer_a6aw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761309991537, "cdate": 1761309991537, "tmdate": 1762915579594, "mdate": 1762915579594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes eigencluster tokenization, a multi-scale spectral method that discretizes OHLC time series into semantic tokens for Transformer-based next-day up-move prediction. It achieves higher precision and faster inference than point- or patch-wise baselines, with complementary gains from multi-scale design, eigendecomposition, and clustering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a clear and significant research problem, focusing on tokenization and out-of-vocabulary (OOV) challenges in financial time-series modeling, which are crucial for improving model generalization and interpretability.\n\n2. The proposed approach shows good engineering feasibility, as the multi-scale prefix, eigendecomposition, and clustering pipeline forms a concise and implementable framework that reduces token and sequence length while preserving key temporal information.\n\n3. The authors exhibit commendable efforts toward reproducibility by providing code access and detailed implementation descriptions in the appendix, which facilitates transparent verification and future research replication."}, "weaknesses": {"value": "1. The current scalar projection design lacks theoretical grounding and comparative evaluation. It is recommended to test alternative mappings (e.g., first principal component, eigenvector projection, multi-component clustering, or nonlinear functions like tanh) and report robustness across settings.\n\n2. In terms of clustering methodology, automatic selection approaches such as the Bayesian Information Criterion, Akaike Information Criterion, Bayesian Gaussian Mixture Models, or time-series cross-validation should be compared. The paper should also justify the rationale and advantages of the specific clustering method it adopts.\n\n3. There already exist other clustering-based tokenization methods, but the paper does not cite or compare them to demonstrate the advantages of its proposed tokenization approach.\n\n4. The paper does not provide sufficient justification for the effectiveness of the chosen prefix windowing approach, and the selection of the prefix window size lacks rationale and comparative experiments. \n\n5. The baselines include only point- and patch-wise discretization, but not learned tokenizers such as VQ-VAE, discrete autoencoders, or transformer-based quantizers, which would provide a stronger benchmark for the proposed approach.\n\n6. The paper lacks a principled discussion of why eigen-decomposition and 1D clustering should preserve predictive structure or how they relate to temporal-spectral representations."}, "questions": {"value": "1) What is the theoretical motivation for the specific scalar projection design? Have alternative mappings or nonlinear transformations been tested to confirm robustness?\n\n2) What is the principled link between eigendecomposition + 1D clustering and the preservation of predictive temporal–spectral structure in price series?\n\n3) Why was 1D K-means chosen over automatic or probabilistic clustering methods (e.g., BIC/AIC-based selection or Bayesian GMM)? How sensitive are the results to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "j9aN1XcTCk", "forum": "ATSNt0erIr", "replyto": "ATSNt0erIr", "signatures": ["ICLR.cc/2026/Conference/Submission677/Reviewer_U9Fv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission677/Reviewer_U9Fv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713228086, "cdate": 1761713228086, "tmdate": 1762915579452, "mdate": 1762915579452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new financial time series discretization method, Eigencluster Tokenization, which performs multi-scale eigendecomposition on OHLC matrices and clusters eigenvector projections to generate compact and semantically meaningful tokens. The approach aims to overcome three major issues in conventional point-wise and patch-wise tokenization for financial data: excessive token counts, sparse distributions, and out-of-vocabulary (OOV) risks. The authors conduct cross-market experiments on both S&P 500 and CSI 300 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly discusses the challenges of financial tokenization that differ from language modeling (e.g., high variability, OOV risk), and proposes a multi-scale eigendecomposition and clustering approach on OHLC matrices. Made adaptations specific to financial data characteristics, such as using C_t−n+k−1 for normalization.\n\n\n2. The definition of the most bullish cluster enhances model interpretability.\n\n3. The evaluation on both S&P 500 and CSI 300 datasets demonstrates strong performance, along with significant reductions in token count and computational cost."}, "weaknesses": {"value": "1. The explanation of Table 1 is unclear.\n\n2. The clustering method is limited to K-means only; the impact of alternative clustering approaches (e.g., GMM, Spectral Clustering) is not explored.\n\n3. Constituent stocks are part of the indices, and depending on the index composition, certain stocks may have a dominant weight and be strongly correlated with index movements. The paper does not discuss this dependence.\n\n4. The font size in Figure 6 are too small, making it hard to read; figure annotations are insufficient (no explanation of labels for each point).\n\n5. The paper lacks comparison with other recent tokenization methods."}, "questions": {"value": "1. Explain the tables and figures more clearly and make them easier to read.\n\n2. Provide a discussion on the dependence or correlation among the test stocks.\n\n3. Compare the proposed method with other state-of-the-art tokenization approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DIWqcAHS0C", "forum": "ATSNt0erIr", "replyto": "ATSNt0erIr", "signatures": ["ICLR.cc/2026/Conference/Submission677/Reviewer_18H9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission677/Reviewer_18H9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022720643, "cdate": 1762022720643, "tmdate": 1762915579336, "mdate": 1762915579336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an eigen-cluster tokenization method for applying Transformer architectures to financial time series. It argues that conventional point-wise and patch-wise tokenization approaches are ill-suited for financial data because of three structural issues: excessive token cardinality, sparse token representation, and out-of-vocabulary tokens.\n\nTo address these problems, the authors propose a multi-scale spectral clustering pipeline. OHLC matrices are decomposed via eigen decomposition to derive dominant temporal components, and their scalar projections are clustered to produce discrete tokens. The resulting cluster tokens are used in a Transformer to predict next-day price increases. The method reportedly reduces the token vocabulary to around 50–100 while improving precision by 6–9 pp over point- and patch-wise baselines across S&P 500 and CSI 300 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation from tokenization theory: The paper correctly identifies structural mismatches between standard tokenization and financial data — particularly OOV tokens arising from non-stationarity and sparse embedding updates. This diagnosis is compelling and empirically supported with vocabulary size and OOV analyses.\n- Methodological novelty: Using eigen decomposition and clustering as a discrete representation mechanism is conceptually interesting and extends recent ideas from computer-vision token reduction (e.g., Clusterformer, PACAViT). The multi-scale prefix-window representation captures temporal hierarchy, which is intuitively aligned with financial market regimes.\n- Experimental breadth.: The authors conduct cross-market experiments (US/China), ablations, and backtests. The ablation results substantiate that clustering and eigen decomposition both materially affect performance."}, "weaknesses": {"value": "- SMOTE and data-generation bias: The use of SMOTE to balance “bullish clusters” is problematic for financial time-series forecasting. SMOTE interpolates samples in Euclidean space, implicitly assuming smooth similarity structures; yet in asset-price data, temporal autocorrelation and regime-dependence make such interpolation unrealistic. Oversampling can distort minority-class distributions and generate spurious trajectories. The paper acknowledges in appendix that SMOTE materially changes sample counts, but does not demonstrate that synthetic samples preserve realistic dynamics. This undermines the credibility of the reported precision gains.\n- Heuristic cluster selection (Eq. 10): The rule combining three weighted terms (bullish probability, cluster size, and K penalty) is purely empirical with fixed weights (25.0, 0.8, 0.2). No sensitivity or robustness analysis is presented. Because market regimes differ across periods, applying the same weights to all datasets introduces arbitrariness and potential overfitting to the chosen calibration period."}, "questions": {"value": "As noted in the weaknesses, using SMOTE for financial time series is highly unusual. The paper should clarify why this oversampling method was chosen and why it is appropriate, given that SMOTE interpolates data points and may distort temporal and distributional structures essential to financial data.\n\nSimilarly, the weighting scheme in Eq. (10) (25.0, 0.8, 0.2) seems empirically chosen without explanation or sensitivity analysis. The authors should specify how these weights were determined and whether the results remain robust under different settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ROOwgltcnZ", "forum": "ATSNt0erIr", "replyto": "ATSNt0erIr", "signatures": ["ICLR.cc/2026/Conference/Submission677/Reviewer_CR4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission677/Reviewer_CR4w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096545037, "cdate": 1762096545037, "tmdate": 1762915579190, "mdate": 1762915579190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Authors’ Global Response"}, "comment": {"value": "We sincerely thank all reviewers for your thoughtful and high-quality comments. Compared with many other review experiences, we genuinely appreciate the professionalism and strong sense of responsibility you demonstrated during the evaluation of our submission. All of your comments are valuable and have helped us identify important directions for improving the manuscript.\n\nWe have done our best to address your concerns within our capacity, including adding supplementary experiments and providing clearer mathematical explanations. If any of our wording comes across as inappropriate or unclear, we sincerely apologize in advance.\n\nIf there are any further questions or clarifications needed, we warmly welcome additional comments. We plan to consolidate all feedback and corresponding revisions before uploading the final updated version of the manuscript. Please kindly understand our timeline, and we expect to complete the update by **December 1st**.\n\nThank you again for your time and constructive feedback."}}, "id": "KOEsXFo1xd", "forum": "ATSNt0erIr", "replyto": "ATSNt0erIr", "signatures": ["ICLR.cc/2026/Conference/Submission677/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission677/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission677/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763649954290, "cdate": 1763649954290, "tmdate": 1763649954290, "mdate": 1763649954290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}