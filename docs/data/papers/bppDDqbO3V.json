{"id": "bppDDqbO3V", "number": 17734, "cdate": 1758279925528, "mdate": 1759897157419, "content": {"title": "Dissecting the Role of Positional Encoding in Length Generalization", "abstract": "Length generalization (LG) is a persistent challenge for Transformers. Despite recent studies improving the models' LG capability, its underlying mechanisms are still underexplored. To better understand LG, we propose that LG requires alignment of the model’s inductive bias with the task’s computational structure, and validate this view with experiments on Transformers. Focusing on iterative tasks (e.g., Polynomial Iteration, Parity, Binary Copy), we systematically analyze different PEs and find that the misalignment persists for Transformers: the structural bias of softmax attention and computational biases from PEs destabilize LG under extrapolation. Notably, Transformers without positional encoding (NoPE) could show partial LG capability, potentially because implicit position encoding through hidden-state statistics and contextual token distributions preserves the consistent computation in extrapolation, though these signals decay with length, leaving the encoding misaligned with the task. Building on this mechanistic analysis, we introduce a lightweight enhancement—value-side relative coding with logit rescaling—that better aligns inductive bias with task structure. This sustains iterative computation and improves LG, offering insights for future PE design.", "tldr": "Exploring the mechanism of Positional Encoding in Length Generalization on Reasoning Tasks", "keywords": ["Mechanistic Interpretation", "Positional Encoding", "Length Generalization", "Iteration Head", "Reasoning Tasks."], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90529a739b35fa11dcb043a9df324fae67ce0eb1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the impact of positional encoding strategies (ALiBi, APE, FIRE, NoPE, RoPE, T5, YARN) on length generalization of Transformers on iterative tasks (specifically Polynomial Iteration, Parity, Binary Copy). The authors propose that successful LG relies on alignment between the iterative task's computational structure and the inductive bias of the positional encoding. Their mechanistic analysis shows that many popular PEs are misaligned with iterative tasks, helping to explain why they often perform worse than NoPE. Finally, they propose modified PEs aimed at improving alignment with iterative tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The mechanistic analysis is clear, convincing, and interesting. The NoPE statistical analysis nicely complements the constructive argument in Kazemnejad."}, "weaknesses": {"value": "A limitation is studying only iterative tasks. In particular, the Logit controller and Value-side relative PE appear to be specifically designed to improve LG for iterative tasks, but their impact on LG for other types of tasks (such as the many others studied in Kazemnejad) is unclear. To be practically useful, we would hope for PEs that could improve LG on many kinds of tasks, not just a limited subset.\n\nSee also questions."}, "questions": {"value": "Can you more explicitly position the paper relative to Kazemnejad, noting the novel contributions w.r.t. Kazemnejad? Kazemnejad et al. (2023) show the failure of LG and the relative superiority of NoPE over other PEs over a range of tasks (Fig F.5 shows (lack of) LG for Parity for NoPE, T5, ALiBi, APE). Kazemnejad further prove that NoPE can theoretically represent both absolute and relative PEs, e.g. for a specific weight configuration in the first layer, and all subsequent layers, respectively. In my reading, the novelty of the current paper lies in: a specific study of *iterative tasks* only (adding the tasks Polynomial Iteration and Binary Copy to Kazemnejad which already studies Parity), a mechanistic explanation of the specific failure-modes of various PEs for this task, and a new statistical analysis of NoPE’s ability to encode position information (distinct from Kazemnejad’s proof which relies on constructing specific weight matrices). Is this accurate?\n\nStudying 2- and 3-layer Transformers makes sense for the mechanistic analysis where you are looking for particular expected attention patterns, but do you know whether training deeper Transformers (more layers) on the same tasks show the same behavior shown in Figure 3 (i.e. does length generalization still degrade relatively quickly OOD, with NoPE extrapolating better than other choices of PE)? The trend where the LG improves from 2- to 3-layer makes one wonder if it might continue to improve with more depth -- and whether the relative performance of the different PEs might change.\n\nWhat can the study of iterative tasks tell us about other classes of tasks for which LG is desired? Can we expect the Logit controller and Value-side relative PE to improve (or at least not harm!) LG for other classes of tasks with different structure (e.g. arithmetic, etc.)?\n\nMinor notes:\nL17. positional enconding (PE) (abbrev. never introduced)\nL175 Typo “Algins”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WgwA9UZNVq", "forum": "bppDDqbO3V", "replyto": "bppDDqbO3V", "signatures": ["ICLR.cc/2026/Conference/Submission17734/Reviewer_VJmt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17734/Reviewer_VJmt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761015887709, "cdate": 1761015887709, "tmdate": 1762927568768, "mdate": 1762927568768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the role of different positional encoding for length generalization on synthetic tasks like parity, binary copy, and such (typically tasks that can be accomplished by iterative local updates). The paper arguments for the misalignment between inductive biase of position encoding and the task as the key factor harming performance - and tries to propose some fixes that would created a better aligment - e.g. logit control and rescaling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Overall interesting analyses\n* Sound lightweight extensions (ViPE) that shows effective results."}, "weaknesses": {"value": "* Lack of benchmarking of ViPE on realistic benchamrks greatly undermines scope of the paper. \n* While the paper provides some valuable insights, part of it feels somewhat \"obvious\" -- of course, one would think that failure to length generalize is an issue of the lacking the right inductive biase; and adding more task-specific inductive bias, or better invariance-mainetance across length increase, length generalization can be improved. This does not feel like a substantively new insight- although the key strength that redeems the paper is in proposing a potential solution.\n* Similar ideas (in the context of RNNs - but the principles seem to translate) have been also explored here [1]. The benchmarks in [1] (including those from its appendix) could be have been also useful to evaluate on.\n* Even the proposed method still seems to disgracefully degrades around sequence length 43-48 -- suggesting that the generalization may not scale well. \n\n[1] Monotonic Location Attention for Length Generalization - Ray Chowdhury et al."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G8iF2SvLsR", "forum": "bppDDqbO3V", "replyto": "bppDDqbO3V", "signatures": ["ICLR.cc/2026/Conference/Submission17734/Reviewer_KVcN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17734/Reviewer_KVcN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613793578, "cdate": 1761613793578, "tmdate": 1762927568196, "mdate": 1762927568196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanisms behind length generalization in Transformers, proposing that LG depends on the alignment between a model’s inductive bias and the computational structure of the task. Through synthetic experiments on iterative reasoning tasks, the authors analyze various positional encodings, including RoPE,  NoPE, and find that most fail to generalize to longer sequences. They show that NoPE can partially achieve LG via implicit positional signals emerging from hidden-state statistics and contextual token distributions, though these signals decay with length. Building on this analysis, the paper introduces ViPE combining value-side relative coding and logit rescaling, aligning model bias with task structure and substantially improving extrapolation performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a novel explanation of how NoPE implicitly encodes positional information through hidden-state statistics and contextual token distributions, contributing theoretical clarity to understanding Transformers without explicit positional encodings. The proposed method ViPE introduces value-side relative encoding and logit rescaling, significantly improving length extrapolation and demonstrating the practical value of aligning model inductive bias with task structure.\n\n2. The experiments are thorough and clearly presented, covering multiple positional encodings and iterative tasks. The visualization of attention maps and performance degradation effectively supports the paper’s main claims about misalignment and fragility in length extrapolation.\n\n3. The paper offers a fresh view by framing length generalization as an alignment problem between a model’s inductive bias and the computational structure of the task, providing an insightful analytical framework."}, "weaknesses": {"value": "1. All experiments are conducted solely on synthetic iterative tasks, leaving it unclear whether the conclusions generalize to natural language or more complex reasoning tasks. This considerably limits the paper’s practical value. For instance, in general length generalization settings using pretrained models (e.g., Qwen2.5), would the attention maps still exhibit such clear structural patterns? \n\n2. Since the paper focuses exclusively on synthetic tasks, and ViPE appears somewhat tailored to tasks with precise computational structures (is that correct?), I wonder how the authors envision its performance on more typical tasks, including natural language and general length generalization benchmarks. Given resource and time constraints, additional experiments are unnecessary, but I would appreciate the authors’ perspective on this point.\n\n3. The analysis of NoPE seems to show only that NoPE can use statistical information to distinguish positions, but not that it actually does so in practice. The experiments in Section 5.3 merely demonstrate that NoPE encodes absolute and relative positions. Should this be considered only a lower bound on NoPE’s capability? That said, the authors’ analysis is valuable in that it inspired the design of ViPE, which is a positive contribution.\n\nHowever, I’m not fully confident in my own judgment, and I'm willing to adjust my score after seeing other reviewers’ comments and the authors’ rebuttal."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1xn00MlY1d", "forum": "bppDDqbO3V", "replyto": "bppDDqbO3V", "signatures": ["ICLR.cc/2026/Conference/Submission17734/Reviewer_ivHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17734/Reviewer_ivHQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989840870, "cdate": 1761989840870, "tmdate": 1762927567554, "mdate": 1762927567554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an important question in Transformers — why models struggle to generalize to longer sequences than those seen during training. The authors propose that length generalization (LG) depends on how well the model’s inductive bias aligns with the computational structure of the task. The authors study this question on Polynomial Iteration task, which has clear recursive computational rules. The authors examine how various positional encodings (PEs) — including Absolute (APE), Relative (T5, RoPE), and No Positional Encoding (NoPE) — influence LG. The authors find that Transformers can simulate iterative computation when trained on Polynomial Iteration, but the alignment is fragile under extrapolation. The model's inductive biases - computational (from PEs) and structural (from attention) are misaligned with the inherent computational structure of the task. Interestingly, NoPE sometimes generalizes better than explicit encodings due to implicit positional information in hidden-state statistics and contextual token distributions. The authors aim to reduce the two sources of misalignment: (i) structural bias from softmax attention and (ii) computational bias from PEs. They propose ViPE (Value-side relative position encoding with logic rescaling), which stabilizes the computation and improves LG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation that LG emerges from alignment between a model’s inductive bias and the computational structure of the task — is both intuitive and interesting.\n\n2. The findings that NoPE models can exhibit partial LG by implicitly encoding positional information through hidden-state mean/variance and contextual token distributions is interesting."}, "weaknesses": {"value": "1. The evaluation is performed on synthetic tasks. It would be interesting to see how the approach (particularly ViPE) performs on realistic tasks. Overall, limited evaluation that does not cover a good range of tasks. \n\n2. Limited connections with other existing works on PE (e.g., spectral analysis of PEs). \n\n3. ViPE performance is reported only on polynomial iteration task. Since the authors aim to reduce the two sources of misalignment: (i) structural bias from softmax attention and (ii) computational bias from PEs and propose ViPE (Value-side relative position encoding with logic rescaling), it would be good to evaluate this comprehensively. \n\n4. The propositions presented in the paper rely on uniform attention, which is used as a simplifying assumption but it is not clear to what extent does this limit a realistic scenario."}, "questions": {"value": "How would the approach perform on other more realistic tasks, e.g., NLP tasks? \n\nHow does the approach connects to other PEs-based modeling?\n\nHow sensitive are the results to the assumption of uniform attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yvMFEH7FbZ", "forum": "bppDDqbO3V", "replyto": "bppDDqbO3V", "signatures": ["ICLR.cc/2026/Conference/Submission17734/Reviewer_nP5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17734/Reviewer_nP5W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127667093, "cdate": 1762127667093, "tmdate": 1762966309553, "mdate": 1762966309553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}