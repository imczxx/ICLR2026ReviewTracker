{"id": "ViwYnzTMRW", "number": 6123, "cdate": 1757953450587, "mdate": 1759897934333, "content": {"title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery", "abstract": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT) reasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning capabilities, achieving impressive performance in commonsense reasoning and mathematical inference. Despite their effectiveness, Long-CoT reasoning models are often criticized for their limited ability and low efficiency in knowledge-intensive domains such as molecule discovery. Success in this field requires a precise understanding of domain knowledge, including molecular structures and chemical principles, which is challenging due to the inherent complexity of molecular data and the scarcity of high-quality expert annotations. To bridge this gap, we introduce **Mol-R1**, a novel framework designed to improve explainability and reasoning performance of R1-like Explicit Long-CoT reasoning LLMs in text-based molecule generation. \nOur approach begins with a high-quality reasoning dataset curated through **P**rior **R**egulation via **I**n-context **D**istillation (**PRID**), a dedicated distillation strategy to effectively generate paired reasoning traces guided by prior regulations. Building upon this, we introduce **MoIA**, **Mo**lecular **I**terative **A**daptation, a sophisticated training strategy that iteratively combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO), tailored to boost the reasoning performance of R1-like  reasoning models for molecule discovery.\nFinally, we examine the performance of Mol-R1 in the text-based molecule reasoning generation task, showing superior performance against existing baselines.", "tldr": "", "keywords": ["Long CoT Reasoning", "Large Language Models", "Text-based Molecule Discovery"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d81b8b090db6fa7e66948f166f7f120a2514072e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Mol-R1, a framework for generating molecules from text descriptions using reasoning traces.\nThe approach consists of two components: (1) Prior Regulation via In-context Distillation (PRID), which generates initial reasoning traces by prompting GPT-4o with an expert-annotated example and followed filtering, and (2) Molecular Iterative Adaptation (MoIA), which iteratively refines the model through cycles of supervised fine-tuning, reinforcement learning, and rejection re-sampling.\nThe authors evaluate on the ChEBI-20 dataset and report improvements in exact match scores and molecular fingerprint metrics compared to baseline models including DeepSeek-R1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Important problem. Explainability in molecule generation for drug discovery is genuinely valuable, and the motivation for transparent reasoning is well-articulated.\n- The experimental evaluation is reasonably comprehensive in scope. The authors compare against multiple strong baselines, including recent reasoning models like QWQ-32B and DeepSeek-R1, as well as GPT-OSS-120b.\n- Multiple metrics beyond accuracy (fingerprint similarity, validity measures)\n- The case study in Figure 5 provides useful qualitative insight by showing how reasoning traces evolve across training iterations, demonstrating that the model progressively refines its understanding of molecular structure."}, "weaknesses": {"value": "- **Unvalidated reasoning claims.** Title emphasizes \"Explicit Long-CoT Reasoning,\" but Mol-R1 produces ~428-word traces versus 5,337 (QWQ-32B) and 4,518 (DeepSeek-R1)—actually *shorter*, not longer. No analysis of: (1) whether reasoning length correlates with performance, (2) whether traces causally contribute versus merely correlate, (3) generalization to novel molecules.The claimed novelty misrepresents standard techniques.\n - **Misrepresented novelty.** PRID (Figure 11) is standard one-shot prompting of GPT-4o, obscured by terminology. MoIA exactly follows STaR [Zelikman et al., 2022]—iteratively generating traces, filtering correct ones, retraining—and expert iteration [Anthony et al., 2017; Silver et al., 2017], yet neither is cited. Related work omits these foundational papers entirely.\n - **Missing critical ablations.** Need: (1) matched-budget comparison of models trained with/without reasoning (current ablations lack base model trained without reasoning at same budget), (2) reasoning length vs. performance analysis, (3) whether improvements come from reasoning versus more training data. No evaluation of generalization to distribution shifts or novel scaffolds.\n - **Flawed reasoning evaluation.** Consistent-F1 uses Gemma-3-27B as judge without validation—no inter-rater reliability with chemists, no multi-judge comparison. The metric appears to include all traces including failed predictions, mixing signal with noise when it should condition only on successful predictions. Figure 3 shows Mol-R1-RS-G achieves Consistent-F1 ~0.25, beating some MoIA iterations, yet this is never discussed. Additionally, an ablation where reasoning traces are paired with separate but correct SMILES strings would be highly valuable to assess the judge's discernment—if judgment agreement remains high despite mismatched reasoning-molecule pairs, the judge cannot distinguish reasoning quality from answer correctness, invalidating the metric.\n - **Missing experimental details.** Table 1: no error bars, inference procedures, or computational costs. BLEU questionable for SMILES—may just measure canonical notation learning. Mol-R1-RS-G exact match vs. fingerprint discrepancy unexplained. Figure 4 \"inference\" curve undefined; train-inference gap not discussed. \n - **Incomplete related work.** Section 2.2 ignores ChemBERTa, MolGPT, and extensive prior work. Foundational reasoning papers (STaR, ReAct) uncited. Line 102 claim that \"parameter scale creates reasoning capability\" is misleading—o1 and DeepSeek-R1 success comes from RL on top of capable base models, not scale alone.\n - **Unsubstantiated claims.** Lines 64-65: reasoning enables \"human-interpretable justifications\" without human evaluation from chemists. Lines 216-217: existing LLMs \"fail due to lack of molecular knowledge\" without supporting experiments, contradicting successful deployments.\n\n### Additional References\n - Eric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Self-taught reasoner. In Proceedingsof the NIPS, volume 22, 2022.\n - Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learningand tree search. Advances in neural information processing systems, 30, 2017.\n - David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, ArthurGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the gameof go without human knowledge. nature, 550(7676):354–359, 2017."}, "questions": {"value": "1. Can you provide ablations training models with/without reasoning on matched-quality data, controlling for everything except reasoning text presence?\n2. How do you justify \"long-CoT\" when traces are 1/10th baseline length? Does reasoning length correlate with performance?\n3. Can you evaluate on molecules created after GPT-4o and Llama-3.1-8B training cutoffs to rule out contamination?\n4. How is Consistent-F1 computed? Do you include failed predictions? Have you validated Gemma-3-27B as a qualified judge against human chemists?\n5. Why does Mol-R1-RS-G beat MoIA iterations on Consistent-F1 (Figure 3)?\n6. What distinguishes your approach from STaR [Zelikman et al., 2022] and expert iteration [Anthony et al., 2017]?\n7. Can you provide human evaluation from chemists on reasoning interpretability and trustworthiness?\n8. The BLEU scores show interesting patterns—they fluctuate across MoIA iterations rather than monotonically improving, even as exact match scores improve. What explains this discrepancy? Is BLEU actually measuring something meaningful for SMILES strings, or is it primarily capturing whether the model has learned the canonical notation format?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "- **Interpretability risk** Without human‑chemist validation, traces may be plausible but unfaithful, inflating trust.\n - **Responsible release** If releasing models/traces, include misuse policies and filters for hazardous chemistry prompts."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TWpGyP95ei", "forum": "ViwYnzTMRW", "replyto": "ViwYnzTMRW", "signatures": ["ICLR.cc/2026/Conference/Submission6123/Reviewer_uAbq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6123/Reviewer_uAbq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576489480, "cdate": 1761576489480, "tmdate": 1762918480203, "mdate": 1762918480203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Mol-R1, a framework to improve explainability of molecule discovery with LLMs. It presented techniques including PRID and MoIA for training the LLM to deal with pre-regulation and post-regulation problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The figure effectively increases the understandability of the paper.\n- The paper proposes RL fine-tuning reasoning approach for molecular generation."}, "weaknesses": {"value": "- **Lack of baselines and details of experiment**: It is unclear which dataset was used in Table 1. From the description, it seems to be based on the *ChEBI-20* dataset, but the experimental settings are not explicitly stated. If *ChEBI-20* was indeed used, the authors should have compared against established caption-based molecular generation baselines such as MolT5 [1], BioT5 [2], and MSR [3]. Moreover, the reported metrics appear substantially lower than those achieved by these baselines, which raises concerns about the experimental results.\n- **Complexity of terms**: The paper introduces several new terms without clear justification. For example, the definition of PRID is vague—does it merely concatenate expert-annotated contexts with raw training data before querying the LLM? The claimed novelty of the PRID method remains unclear. In addition, although the authors adopt GRPO as their reinforcement learning strategy, they refer to their approach as RPO (only maximizing the reward without KL penalty, which causes unnecessary confusion.\n- **Limited applicability beyond text-based molecular generation task**: The *ChEBI-20* text-based generation task is arguably unrealistic, as many captions explicitly include IUPAC names or overly detailed structural descriptions, making direct molecule reconstruction trivial or uninformative. It would be valuable to demonstrate the proposed framework’s generality on other, more practical tasks.\n\n\n\n[1] Edwards et al., Translation between Molecules and Natural Language, EMNLP 2022.\n\n[2] Pei et al., BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations, EMNLP 2023.\n\n[3] Jang et al., Structural Reasoning Improves Molecular Understanding of LLM, ACL 2025"}, "questions": {"value": "- **Model choice for baselines**: Why did the authors exclusively use Gemma models for rejection sampling? For a fair comparison, shouldn’t the same base model be used across settings? Furthermore, the paper lacks justification for using multiple model sizes—Gemma-27B, Gemma-12B, and Llama-3.1-8B—in different parts of the experiment.\n- **Reward sparsity in GRPO**: Isn’t EM reward too sparse to train GRPO? Have the authors experimented with alternative or complementary reward signals, such as similarity-based metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oGlr14QnKi", "forum": "ViwYnzTMRW", "replyto": "ViwYnzTMRW", "signatures": ["ICLR.cc/2026/Conference/Submission6123/Reviewer_Rokw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6123/Reviewer_Rokw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922433729, "cdate": 1761922433729, "tmdate": 1762918479550, "mdate": 1762918479550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Mol-R1 introduces a reasoning-first framework for molecule generation using natural language descriptions. It generates explicit reasoning traces before outputting a SMILES molecule string. The training pipeline combines reasoning trace distillation from GPT-4 (PRID) and a stable supervised + RL training loop (MoIA). Experiments show strong improvements in generation accuracy and reasoning consistency over existing methods.s"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The model generates high-quality, interpretable reasoning traces, and outperforms larger models on molecule generation accuracy\n\n2. Mol-R1 presents an innovative reasoning-trace data generation via PRID\n\n3. The works enables stable learning through MoIA’s alternating SL and RL\n\n4. The authors defines a consistency metric between reasoning and output"}, "weaknesses": {"value": "1. The reasoning quality is judged by an automated LLM, not human experts\n\n2. The paper only tests the Mol-R1 model on one specific task: generating a SMILES molecule from a natural language description, i.e. text-to-SMILES. It leaves open the question of how well the model would perform on other types of molecular design tasks, such as molecule optimization for certain properties."}, "questions": {"value": "1. Does PRID work effectively with weaker teacher models?\n\n2. How does Mol-R1 generalize to broader molecule design tasks?\n\n3. The paper uses LLM to score how consistent the model’s reasoning is with the output molecule (Consistent-F1). Has the authors conducted any human expert evaluation to verify that the reasoning is actually chemically sound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5dtL3Zhec6", "forum": "ViwYnzTMRW", "replyto": "ViwYnzTMRW", "signatures": ["ICLR.cc/2026/Conference/Submission6123/Reviewer_27sz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6123/Reviewer_27sz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922927477, "cdate": 1761922927477, "tmdate": 1762918479222, "mdate": 1762918479222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a training pipeline for chemical reasoning models based on expert iterations. The SFT data is generated using one-shot in-context distillation, where the single demonstration consists of a human-annotated reasoning trace. In benchmark experiments, the authors demonstrate the effectiveness of their approach, showing that it outperforms baseline models and consistently improves across successive expert-iteration rounds."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* **(S1 - relevance, novelty) - High-quality SFT data as a valuable contribution to the chemical reasoning community.** In-context distillation is a concept well established in the general reasoning domain, and the authors successfully adapt this idea to chemical reasoning scenarios. This adaptation introduces a degree of novelty, particularly because high-quality SFT data is valuable for the community (however, see (W1)).\n\n* **(S2 - relevance, novelty) - Expert iterations demonstrate effectiveness.** Similar to (S1), applying a well-known principle to chemical reasoning tasks is interesting, especially since the authors are able to demonstrate its effectiveness by comparing expert iterations to vanilla GRPO (Figure 4) (however, see (W1))."}, "weaknesses": {"value": "* **(W1 - clarity, quality) - Lack of related work.** The concepts of in-context distillation and expert iterations are well established in the general reasoning community. The authors should elaborate on this in the related work section and clearly state that their main contribution lies in applying these existing concepts to chemical reasoning scenarios.\n\n* **(W2 - quality, significance) - Missing error bars.** All results are presented without error bars or statistical tests. Consequently, it remains unclear to what extent the observed performance gains may have occurred by chance. Since rerunning training might be too costly, the authors could at least report error bars across tasks or samples.\n\n* **(W3 - clarity, minor issue).** At several points, the manuscript’s clarity could be improved:\n    - The terms *pre-regulation* and *post-regulation* are not standard. The authors should consider replacing them or providing clear definitions.\n    - l144ff:  \n      > \"In contrast, our work introduces text-based molecule reasoning generation, instead of simply translating a molecule into a predefined description.\"  \n\n      This statement is misleading, as prior work has already explored this direction [1].\n    - Equation (2) is written in an overly complicated manner. Essentially, the loss corresponds to a standard cross-entropy objective in a next-token prediction setup, where trace and answer tokens do not need to be distinguished.\n\n* **(W4 - clarity, quality) - Lack of information regarding distilled traces.** In-context distillation is performed to improve the quality of the SFT data. However, it remains unclear whether this procedure truly improves data quality (see questions)."}, "questions": {"value": "* Could it be demonstrated that in-context distilled samples are of higher quality compared to standard rejection-sampled data?\n* How diverse are the in-context distilled samples? Are they content-wise close to the expert example? If so, wouldn’t this pose a potential issue? It seems reasonable to assume that different samples may require distinct reasoning strategies.\n\n### Other comments\n* The entropy consideration in Appendix B is correct; however, the implicit conclusion (not explicitly written in the manuscript) that reasoning traces cannot harm but only help is inaccurate. While it is true that increasing the context window allows the model to access more information through the attention mechanism (and never less), this does not imply that additional tokens cannot have negative effects. Intuitively, the model indeed attends to all tokens within the context window, and the entropy argument provides a formal proof of increased accessible information. Nevertheless, intermediate tokens — such as reasoning trace tokens — can still be harmful. For example, imagine a case where the model is provided with a correct molecule in the question but then exposed to an incorrect molecule repeated 100 times within the reasoning trace, making the model effectively forget the original molecule. Admittedly, this example is extreme and constructed, but the same principle can apply to more subtle cases that may arise within real reasoning traces.\n\n* The authors mention at several points (e.g., l146) that reasoning traces ensure interpretability. However, this would only hold if the reasoning traces consistently reflected the model’s actual decision-making process. Since this cannot be guaranteed — and numerous counterexamples have been documented — the claim should be stated with greater caution.\n\n### References\n[1] Narayanan. Training a Scientific Reasoning Model for Chemistry."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A5AEX9zZMs", "forum": "ViwYnzTMRW", "replyto": "ViwYnzTMRW", "signatures": ["ICLR.cc/2026/Conference/Submission6123/Reviewer_R386"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6123/Reviewer_R386"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957026858, "cdate": 1761957026858, "tmdate": 1762918478447, "mdate": 1762918478447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}