{"id": "HPxkDRdgMO", "number": 6816, "cdate": 1757996773033, "mdate": 1762939031595, "content": {"title": "Attention Calibration for Reducing Hallucination in Large Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content. Recent works attribute this issue to an inherent bias of LVLMs where vision token attention map has spurious focus on certain positions, and propose to mitigate this issue by reordering visual tokens. However, we find that different LVLMs exhibit different correlations between attention and spatial position, which makes the existing static solution difficult to generalize to other LVLMs. To begin with, we investigate the attention bias introduced by image tokens through a toy experiment, in which a blank image is fed into the model to capture its position-dependent bias. We then remove this bias from the original attention map, which already leads to a substantial reduction in hallucinations. This proof of concept validates the core intuition behind attention calibration. Building upon this insight, we propose Dynamic Attention Calibration (DAC)—a lightweight, plug-and-play module that leverages contrastive learning to dynamically enforce positional invariance. Unlike static baselines, DAC adapts to different models and inputs in a robust and learnable manner, offering a generalizable solution to mitigate attention-related hallucinations in LVLMs. Comprehensive experiments across multiple benchmarks demonstrate that DAC significantly reduces object hallucination while improving general multimodal alignment. Our method achieves state-of-the-art performance across diverse LVLM architectures on various metrics.", "tldr": "", "keywords": ["Large Vision-Language Models (LVLMs)", "Hallucination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/52b25d2b5f7ef9a40af1f1295f347198cef99338.pdf", "supplementary_material": "/attachment/c827983d0b554bf9fd418a4e1ee72eb6f27bc650.zip"}, "replies": [{"content": {"summary": {"value": "This paper argues that hallucinations in LVLMs arise from positional bias in visual tokens. To address this issue, the authors introduce a calibration technique designed to compensate for such bias. The method is implemented on three LVLMs: LLaVA-1.5, mPLUG-Owl2, and LLaVA-NeXT, and evaluated across four hallucination benchmarks: POPE, MME, CHAIR, and LLaVA-Bench."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This work focuses on two key issues in LVLMs: 1) hallucination 2) positional bias in visual attention.\n- Addressing hallucination through intervention at inference is an efficient choice, compared to techniques that require heavy training.\n- The paper is easy to follow."}, "weaknesses": {"value": "* The opening statement in the abstract seems inaccurate. Current LVLMs are not particularly strong at reasoning — in fact, limited reasoning ability is one of their fundamental weaknesses, independent of their susceptibility to hallucination. (l.011: \"...(LVLMs) exhibit impressive multimodal reasoning capabilities...\")\n\n* In Figure 1, the attention maps (a–c) are derived from a blank image, which contains no meaningful visual information. Consequently, these maps offer limited insight. It would be more informative to visualize attention patterns using real images with meaningful objects — do the attentions remain concentrated in the same regions, or do they shift elsewhere?\n\n* The baseline models used (LLaVA 1.5 and mPLUG-Owl2) are relatively outdated. It would strengthen the paper to evaluate the proposed method on more recent LVLMs such as Qwen-VL 2.5, InternVL 2.5, or BLIP-3.\n\n* The benchmarks employed in this study are not sufficiently robust. For instance, POPE assesses object existence on only 500 images and does not account for other forms of hallucination, such as object attributes or relations. Similarly, CHAIR evaluates just 500 MSCOCO images with limited ground-truth annotations. LLaVA-Bench includes only 30 images and relies on GPT-4 as a judge, which is also unreliable. Overall, these benchmarks provide a weak basis for evaluation.\n\n* Several results appear to be missing. Specifically, there are no results on LLaVA-Bench and MME for LLaVA-Next.\n\n* \"Specifically, LVLMs tend to assign lower attention to tokens corresponding to the top-left region of an image compared to those in the bottom-right region. This asymmetric attention makes LVLMs more susceptible to object hallucination in the top-left region, where visual\ngrounding is weaker.\" -- this statement is not backed by any significant proof. Even the examples shared in Figure 1 do not satisfy this argument.\n\n* There are issues with citation format."}, "questions": {"value": "- Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Yy7YGeyeOT", "forum": "HPxkDRdgMO", "replyto": "HPxkDRdgMO", "signatures": ["ICLR.cc/2026/Conference/Submission6816/Reviewer_scRb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6816/Reviewer_scRb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711971241, "cdate": 1761711971241, "tmdate": 1762919081972, "mdate": 1762919081972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2OcfFhYUzZ", "forum": "HPxkDRdgMO", "replyto": "HPxkDRdgMO", "signatures": ["ICLR.cc/2026/Conference/Submission6816/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6816/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762936218025, "cdate": 1762936218025, "tmdate": 1762936218025, "mdate": 1762936218025, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the persistent problem of object hallucination in LVLMs—cases where a model generates visual descriptions that are misaligned with the actual image content. The authors identify Spatial Perception Bias (SPB), a form of systematic positional bias in the visual attention of LVLMs, as a core cause of hallucination. To investigate this bias, they first conduct a simple test by inputting blank images into different LVLMs, revealing significant position-dependent variations in attention distribution. Building on this finding, the paper first introduces a static correction method, Uniform Attention Calibration (UAC), which removes position-based bias by adjusting attention maps using data from a blank image. Extending this idea, the authors propose Dynamic Attention Calibration (DAC), a lightweight, learnable, and plug-and-play module that employs contrastive learning to enforce positional invariance in visual attention dynamically. DAC is integrated directly into the self-attention layers of LVLM decoders and fine-tuned using paired and augmented image samples. Experiments across several benchmarks show that DAC significantly reduces hallucinations while improving multi-modal consistency and perceptual accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author explores the factors contributing to hallucinations in LVLMs from the perspective of Spatial Perception Bias.\n\n2. The Dynamic Attention Calibration (DAC) mechanism employs contrastive learning to enhance positional robustness, demonstrating significant effectiveness in practical applications.\n\n3. DAC exhibits low computational cost, revealing its potential value as a general-purpose hallucination mitigation module."}, "weaknesses": {"value": "1. The fundamental cause of spatial perception bias requires further analysis.  \n2. The method is relatively simple and lacks novelty. The contrastive learning approach relies excessively on augmented data, which may fail to adequately capture the complexity of visual scenes and spatial relationships.  \n3. There is a lack of visualization analysis, and no attention distribution is presented on real-world data."}, "questions": {"value": "1. Is spatial perception bias primarily derived from the training set?  \n2. What is the difference between SPB and attention? I feel that conceptually they are the same.  \n3. If spatial perception and attention are conceptually consistent, then what is the theoretical difference compared with previous papers that addressed hallucination by altering the attention distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CquqtDVmbI", "forum": "HPxkDRdgMO", "replyto": "HPxkDRdgMO", "signatures": ["ICLR.cc/2026/Conference/Submission6816/Reviewer_mYBw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6816/Reviewer_mYBw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735106360, "cdate": 1761735106360, "tmdate": 1762919081444, "mdate": 1762919081444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dynamic attention calibration approach that uses contrastive learning to adjust vision token attention, aiming to mitigate spatial perception bias in LVLMs. Experimental results show that it helps reduce object hallucination to some extent."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an interesting and underexplored problem, positional perception bias, with clear motivation and visual evidence in figure 1.\n2. The method builds on CCA and is clearly described and easy to follow."}, "weaknesses": {"value": "1. The performance gain on POPE in Table 2 appears marginal, especially for LLaVA-Next (all within 1% gain). It’s unclear whether the improvement is beyond the standard deviation.\n\n2. I’m concerned about the generalizability of the method since the calibration is trained and evaluated entirely on MSCOCO images. The paper would be stronger if it:\n(1) Included POPE results on the GQA dataset, following the same setup as the original POPE paper; and\n(2) Reported CHAIR results for the mPLUG-Owl2 model, which is already included in the POPE evaluation but not shown (even in the appendix).\n\n3. Missing important baselines on attention calibration e.g. [1] from ACL 2025.\n\n[1] Don’t Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models"}, "questions": {"value": "1. Is there any analysis or qualitative result that specifically investigates why the positional perception bias emerges? I find this aspect less well studied and lacking deeper explanation. I agree with the previous CCA paper’s point that long-term decay in RoPE poses challenges for modeling cross-modal interactions over long spatial distances, but this work could be more insightful and stronger if it further analyzed the underlying causes of the positional perception bias.\n\n2. The paper encourages LVLMs to focus more on the objects themselves rather than their absolute positions, however, positional information is still valuable for spatial reasoning. Is there any downside or potential loss of positional cues caused by the proposed calibration? A discussion on this tradeoff, along with extended results on the MME position task across models and baselines, would make the paper stronger"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YppVzhNaTE", "forum": "HPxkDRdgMO", "replyto": "HPxkDRdgMO", "signatures": ["ICLR.cc/2026/Conference/Submission6816/Reviewer_5D7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6816/Reviewer_5D7A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776188913, "cdate": 1761776188913, "tmdate": 1762919081030, "mdate": 1762919081030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies spatial position bias as an imbalance in how vision tokens are attended across spatial regions and considers it as a major cause of hallucination in LVLMs. The authors validate this phenomenon using Uniform Attention Calibration by removing attention bias measured from blank images and show hallucination reduction. Based on this, authors propose Dynamic Attention Calibration (DAC) and considers it as a lightweight plug-and-play module that learns to dynamically adjust attention weights through contrastive learning. Experiments cover LLaVA-1.5, mPLUG-Owl2 and LLaVA-NeXT on evaluations including POPE, CHAIR, MME and LLaVA-Bench."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important issue of object hallucination in LVLMs.\n2. The proposed method DAC is simple, lightweight, and model-agnostic, requiring minimal fine-tuning.\n3. Empirical results demonstrate improvements across multiple benchmarks and LVLM architectures."}, "weaknesses": {"value": "1. Contrastive methods to reduce visual hallucination have been extensively studied in previous works. Related works are not being sufficiently discussed but instead the authors spent a large amount of texts on introducing VLMs even before CLIP, which are not the closest related works. Just to name a few directly related publications: (1) HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding. (2) Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training. A more rigorous discussion on past publications is required to clarify how this method meaningfully differs from (and outperforms) existing hallucination reduction approaches.\n\n2. The evaluation relies heavily on older models like LLaVA-1.5, with minimal inclusion of more recent and representative LVLMs such as the InternVL, Qwen-VL (especially Qwen2.5-VL), Gemma3, or Kimi-VL-A3B series. The effectiveness shown on llava and the older models may not transfer to these current model. Even within the experiments, several evaluations omit LLaVA-Next or mPLUG-Owl2 (e.g., Tables 3–5). Evaluation also focused on old metrics, where benchmark like HallusionBench [1] should be considered. \n\n3. A substantial portion of the paper is dedicated to general background material not directly related to hallucination mitigation. For example, the subsection on the self-attention mechanism (lines 162–173) merely reintroduces textbook content and adds little to the main contribution. Why is it necessary to spend a subsection on introducing the very basic attention mechanism??\n\nIn addition, citation formatting and consistency are highly problematic throughout. Overall, the writing and presentation quality fall short of ICLR standards.\n\n[1] HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ng4NydRUlX", "forum": "HPxkDRdgMO", "replyto": "HPxkDRdgMO", "signatures": ["ICLR.cc/2026/Conference/Submission6816/Reviewer_4JEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6816/Reviewer_4JEV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111673667, "cdate": 1762111673667, "tmdate": 1762919080474, "mdate": 1762919080474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}