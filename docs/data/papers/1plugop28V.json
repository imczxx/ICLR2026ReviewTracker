{"id": "1plugop28V", "number": 15652, "cdate": 1758253610238, "mdate": 1759897291023, "content": {"title": "On the Measurement and Efficient Mitigation of Length Generalization Gaps in Large Language Models", "abstract": "Large Language Models (LLMs) typically train on short text due to the quadratic complexity of their self-attention architectures. \nAs a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts.\nIn this paper, we rigorously establish an upper bound on length generalization in the measurement space and identify two length-related factors that limit performance. \nOur theory explains two recent observations: **_(i)_** out-of-distribution positions in longer contexts reduce length generalization, and **_(ii)_** fine-tuning on entire sequences is not necessary. \nMotivated by these insights, we propose _Virtual-context Learning_ (_VCL_), a flexible method that requires minimal modifications to most fine-tuning approaches.\nExperiments on various tasks show that _VCL_ allows LLMs to generalize to 4 $\\times$ context windows while retaining perplexity and improving performance on downstream tasks such as Passkey Retrieval and LongBench. \n_VCL_ brings substantial efficiency improvements, reducing decoding time and memory usage by up to 50\\% compared with fine-tuning baselines.", "tldr": "Theoretical analysis, implications and inspired solution to length generalization", "keywords": ["large language models", "length generalization"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5115d361fdde16ee7f4465ecf1e5db9a1f478bb6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a measure-theoretic framework for analyzing length generalization in large language models (LLMs). The authors identify two key limiting factors: short-length bias caused by training on limited context windows, and distribution shift between training and inference contexts. They introduce **Virtual-context Learning (VCL)**, a fine-tuning strategy that selectively updates parameters associated with out-of-distribution (OOD) tokens. By leveraging Wasserstein distance to quantify distributional divergence, VCL enables models to extrapolate to context lengths up to four times longer than those seen during training while maintaining strong performance and reducing computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers an intellectually interesting perspective by framing length generalization in large language models through a measure-theoretic lens. This perspective allows the authors to formalize the relationship between training and inference distributions in a mathematically principled way. In particular, the use of Wasserstein distance to characterize distribution shift between short and long contexts provides a coherent theoretical account of why models often fail to extrapolate beyond their training sequence lengths.\n\nBeyond the theoretical contribution, the proposed Virtual-context Learning (VCL) method demonstrates clear empirical value. The approach is practically appealing because it targets only out-of-distribution tokens during fine-tuning, thereby reducing computational overhead while still improving performance. Experimental results on passkey retrieval, language modeling, and LongBench consistently show that VCL enables models to operate effectively on context windows up to four times longer than those seen during pretraining. These results are backed by thorough experimentation and ablation studies, which strengthen the empirical claims."}, "weaknesses": {"value": "The paper is mathematically dense and may be difficult to follow for readers without a strong background in measure theory, which could limit its accessibility to the broader NLP community.\n\nThe experimental scope is relatively narrow since all evaluations are performed on LLaMA-2-7B, leaving uncertainty about the generalizability of VCL to other architectures such as BERT or T5.\n\nThe method is not sufficiently compared to other long-context techniques like ALiBi, NTK-aware scaling, or sparse attention mechanisms, making it difficult to assess relative strengths in terms of efficiency and scalability.\n\nThe evaluation primarily focuses on long-context understanding and retrieval tasks, without examining the impact of VCL on other task categories such as reasoning, summarization, or natural language inference."}, "questions": {"value": "How does VCL compare quantitatively with alternative length generalization techniques such as ALiBi, RWKV, or sparse attention in terms of memory usage, training cost, and runtime efficiency?\n\nCould the proposed theoretical framework and VCL be extended to sparse or linear attention mechanisms, or to encoder-decoder architectures? If so, what modifications would be required?\n\nHow stable is VCL when scaling to extremely long sequences beyond 4× the original context window? Does performance degrade gracefully?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Arg4wec9QS", "forum": "1plugop28V", "replyto": "1plugop28V", "signatures": ["ICLR.cc/2026/Conference/Submission15652/Reviewer_UEnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15652/Reviewer_UEnV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717239401, "cdate": 1761717239401, "tmdate": 1762925910174, "mdate": 1762925910174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of length generalization in transformer-based language models: how well a model trained on sequences of length $N$ can generalize to sequences of length $M > N$ at test time. The paper has two parts: a theoretical contribution and a practical method proposal. In the first part, using measure theory tools and certain assumptions, the paper derives a bound on the Wasserstein distance between the distributions of length-N and length-M attentions, showing that the upper bound does not grow with $M$. Next, the authors propose a simple method, Virtual Context Learning (VCL), to improve length generalization. Experiments show that VCL achieves lower perplexity on longer sequences than those seen during training, especially when combined with existing position interpolation methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper includes a theoretical contribution that motivates the proposed method, offering a perspective useful for understanding length generalization in attention-based models. Detailed proofs and definitions of the mathematical results are provided in the appendix.\n- The remark from the derived bound is interesting: self-attention can generalize to out-of-distribution sequence lengths if the empirical measure of attention embeddings does not shift (in the Wasserstein distance sense).\n- The proposed method is simple to implement, achieves good results when combined with existing position interpolation methods, and improves training efficiency in terms of memory and latency."}, "weaknesses": {"value": "- The presentation of the paper can be significantly improved. While the authors include detailed mathematical proofs and definitions in the appendix, the first half of the paper remains difficult to follow for readers without a background in measure theory. The paper could be made more accessible to a broader machine learning audience by focusing more on intuitive explanations of the assumptions and theorems rather than precise mathematical formalism. In particular, the connection between the theoretical results and the proposed method is weak in the current version and should be further clarified.\n- Citations of previous works are sometimes confusing. For example, in lines 110–114, the paper discusses the works of Zhou et al. and Huang et al., but when mentioning the shortcomings of “these studies,” it cites Han et al. and Press et al., which makes the paragraph unclear.\n- Regarding the argument that not all tokens are needed for fine-tuning: the paper presents an experiment where gradients are applied only to the second half of the sequence, with the first half frozen. However, this does not clearly support the claim that “not all tokens are required,” since all tokens in the second half are still used. Please clarify the reasoning. Also, there seems to be a typo in line 314 (“215” should be “256”).\n- The argument about reducing distribution shift through VCL (lines 346–348) is unclear. In standard language modeling, a sequence of length $M$ has $M$ losses (averaged). With VCL, $l$ out of $M$ losses are dropped, training only on tokens with at least $l$ context tokens. It is not clear why this would reduce distribution shift, please explain.\n- The empirical results are incomplete:\n  - Line 370 mentions full-length fine-tuning as a baseline, but perplexity results (Table 1 and Figure 4) do not include it. Please add full fine-tuning rows for models trained on 8k (and possibly 16k) sequences under the same budget and setup as Table 1. For Figure 4, include full-length fine-tuning results (training on all $M$ tokens of a sequence).\n  - In Section 6.2 (passkey retrieval), the VCL setup is unclear. Please specify the exact values of $l$ and $M$ for the VCL-8k-yarn and VCL-6k-yarn experiments, and report results for VCL alone (without yarn).\n  - In Section 6.3, clarify the experimental setup, including $l$ and $M$.\n  - Table 2 lacks standard deviation values (e.g., from multiple runs with different random seeds). Also, please report VCL performance without PI methods.\n  - The training duration (200 steps) seems too short. Would the improvements persist with longer training? Note that long-context adaptation typically involves a large number of tokens, e.g., [4] increases context from 2k to 8k with an additional 120B tokens.\n  - More recent evaluation metrics, such as RULER [5], are missing.\n  - Since VCL omits losses for tokens with shorter context, this could cause forgetting on regular benchmarks. Please include results on standard pretraining benchmarks to verify that forgetting does not occur.\n- Some related works are missing. It is now common in language model training to use stage-wise or curriculum-based pretraining (e.g., starting with shorter and gradually increasing sequence lengths) to improve long-context performance [1, 2, 3]. Additionally, the paper does not discuss the practical challenge of limited long-context data, which is important for motivating the problem.\n\n[1] Zhu, Tongyao, et al. \"SkyLadder: Better and Faster Pretraining via Context Window Scheduling.\" arXiv preprint arXiv:2503.15450 (2025).\n\n[2] Pouransari, Hadi, et al. \"Dataset decomposition: Faster llm training with variable sequence length curriculum.\" Advances in Neural Information Processing Systems 37 (2024): 36121-36147.\n\n[3] Jin, Hongye, et al. \"Growlength: Accelerating LLMs pretraining by progressively growing training length, 2023.\" URL https://arxiv.org/abs/2310.00576.\n\n[4] Li, Jeffrey, et al. \"Datacomp-lm: In search of the next generation of training sets for language models.\" Advances in Neural Information Processing Systems 37 (2024): 14200-14282.\n\n[5] Hsieh, Cheng-Ping, et al. \"RULER: What's the Real Context Size of Your Long-Context Language Models?.\" arXiv preprint arXiv:2404.06654 (2024)."}, "questions": {"value": "- An interesting consequence of Theorem 4.6 is that $\\mathbb{W}(\\mu, \\nu)$ is the primary factor in length generalization, rather than the sequence length $M$ itself. Could you comment on why there is a performance drop for a synthetic task  like passkey retrieval (Fig. 3)? Based on the intuition in Figure 1, one might expect no distribution shift for such a task even when the sequence length increases.\n- The attention update formulation in line 154 is missing the commonly used output projection in Equation (3). While this might not affect the theoretical analysis, it could be worth noting explicitly.\n- In line 149, it is stated that the Softmax-attention scaling factor $d_{QK}$ is assumed to be 1, but it is unclear where this assumption is used later, since the variable is still retained in the equations. Please clarify.\n- In Definition E.2 (Empirical Measure Mapping), $m(X)$ for $X = \\{x_1, …, x_N\\} \\subset E$ is defined as the average of the Dirac measures $\\delta_{x_t}$. However, this seems inconsistent with Definition E.1, where all $\\delta_{x_t} = 1$ since they belong to $E$. Could you please clarify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gn4evHKRjk", "forum": "1plugop28V", "replyto": "1plugop28V", "signatures": ["ICLR.cc/2026/Conference/Submission15652/Reviewer_Vtsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15652/Reviewer_Vtsn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931129261, "cdate": 1761931129261, "tmdate": 1762925909483, "mdate": 1762925909483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a measure-theoretic framework for analyzing length generalization in LLMs, establishing an upper bound on the Wasserstein distance between attention outputs for sequences of different lengths. The bound depends on two factors:  $\\sqrt{\\ln{N}}$ (where N is the shorter sequence length) and $W(\\mu,\\nu)$ (the distribution shift distance). Based on these insights, the authors propose Virtual-context Learning (VCL), which fine-tunes models by computing loss only on out-of-distribution position tokens. Experiments on language modeling, passkey retrieval, and LongBench demonstrate that VCL achieves comparable or better performance than full-length fine-tuning while reducing memory usage and training time by approximately 50%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The measure-theoretic approach to length generalization for different position embedding and extrapolation strategies is original and mathematically rigorous.\n2. VCL is simple to implement with minimal code changes and delivers substantial computational savings, as is claimed by the authors.\n3. The paper includes diverse tasks (perplexity, passkey retrieval, LongBench) and thorough ablations, demonstrating the method's effectiveness across different settings."}, "weaknesses": {"value": "1. The transfer from theory to application needs more clarification. Specifically, although RoPE-equipped models use absolute position IDs, the PE models relative distance, which creates a less generalization gap than the analysis. When using RoPE, only finetuning on the full length lets the model learn the longest dependency length, which contradicts the authors' claim in Section 5.1. Alibi also largely mitigates this gap by introducing long-range decay. \n2. The performance of VCL on length >4K in Table 1 is suspicious. This Table indicates that VCL training can achieve length extrapolation far beyond the training range and largely improves upon YaRN, which needs further clarification. Does this result suggest that VCL can achieve infinite-length extrapolation (also evidenced by Fig 3)?\n3. The efficiency benefit of the proposed method gradually diminishes when generalizing to extremely long sequence lengths."}, "questions": {"value": "1. How tight is the bound in Theorem 4.6?\n2. An analysis of the validity of Assumptions 4.3 and 4.4 would be beneficial. How well are they satisfied in real-world scenarios?\n3. Please explain the Oracle setting in Section 5.1.\n4. Why the results in Figure 4(a) different from the ones in Table 1?\n5. How does the proposed method compare with other PE-manipulation methods like PoSE or LongRecipe?\n6. Please use \\citep instead of \\citet when necessary.\n7. \"PE\" in line 64 isn't defined before.\n8. Figure 6 has the wrong y-axis title and a typo in its caption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rCR24iPOCL", "forum": "1plugop28V", "replyto": "1plugop28V", "signatures": ["ICLR.cc/2026/Conference/Submission15652/Reviewer_k9iq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15652/Reviewer_k9iq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962362930, "cdate": 1761962362930, "tmdate": 1762925909016, "mdate": 1762925909016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a theoretical analysis for attention distribution shift and proposed a simple method for adapting LLMs to longer contexts by only tuning parameters for later tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A (unsprisingly) simple method for fine-tuning LLMs to longer contexts.\n1. A theoretical perspective for understanding the length extension failure for current LLMs.\n1. Results show efficient efficacy in extending LLMs to longer-context tasks."}, "weaknesses": {"value": "1. Descriptions of past work in the first paragraph in Related Work, \"Length generalization\" are not accurate and look carelessly written.\n1. Unclearly explained how the wassterstein distance between short and long context attention is related to length-generalization. It seems a rather expected phenomenon. The writing hint that \"which is the primary factor driving length generalization failures\" but not clearly explained why so, and why the opposite could not be true.\n1. The proposed method is only distantly related to the theory. The conceptual connection is loos, and little proof is provided on how the solution alleviates the terms in Theorem 4.6\n1. A minor weakness: results are okay for research concept-proving, but not evaluated at the scale of sota LLM models which are already trained on longer context. This limits the downstream impact. I could understand this if it is due to resource limitations (but authors mentioned only 8 A100 GPUs so they might have the resource to do that. Not sure why they didn't evaluate on larger models).\n1. Also, baselines only include those by 2023 so seem limited."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NH5zrX3x4d", "forum": "1plugop28V", "replyto": "1plugop28V", "signatures": ["ICLR.cc/2026/Conference/Submission15652/Reviewer_KCDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15652/Reviewer_KCDj"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972348401, "cdate": 1761972348401, "tmdate": 1762925908666, "mdate": 1762925908666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}