{"id": "1ZmdfDzGE1", "number": 19434, "cdate": 1758296262788, "mdate": 1759897039270, "content": {"title": "DSA: Efficient Inference For Video Generation Models via Distributed Sparse Attention", "abstract": "Diffusion Transformer models have driven the rapid advances in video generation, achieving state-of-the-art quality and flexibility. However, their attention mechanism remains a major performance bottleneck, as its dense computation scales quadratically with the sequence length. To overcome this limitation and reduce the generation latency, we propose DSA, a novel attention mechanism that integrates sparse attention with distributed inference for diffusion-based video generation. By leveraging carefully-designed parallelism strategies and scheduling, DSA significantly reduces redundant computation while preserving global context. Extensive experiments on benchmark datasets demonstrate that, when deployed on 8 GPUs, DSA achieves up to 1.43× inference speedup than the existing distributed method and 10.79× faster than single-GPU inference.", "tldr": "DSA introduces a training-free sparse attention with distributed inference for diffusion-based video generation, cutting redundant computation and achieving up to 10.55× faster inference.", "keywords": ["Distributed System", "Diffusion", "Inference", "Sparsity"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/554a96b1cf818265b54926282b19d485e49a17e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DSA, a system that combines sparse attention mechanisms with distributed inference to accelerate video generation using Diffusion Transformer models. The approach introduces two key components: (1) Mixed Parallelism (MP) that applies different parallelism strategies (spatial sequence parallel vs. temporal sequence parallel) based on attention patterns, and (2) Dynamic Attention Scheduling (DAS) that optimizes computation-communication overlap. Experiments on Wan and Hunyuan-Video models show up to 10.79× speedup over single-GPU inference while maintaining video quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a solid contribution by integrating sparse attention with distributed inference for video generation. The key insight of matching different parallelism strategies to spatial versus temporal attention patterns is well-motivated and novel. The experimental evaluation covers multiple models with comprehensive quality and system performance metrics, demonstrating impressive super-linear scaling for larger models. The training-free nature makes it immediately applicable to existing models."}, "weaknesses": {"value": "1.The paper only reports PSNR, SSIM, and LPIPS metrics without perceptual quality metrics like VBench scores that evaluate specific video generation dimensions such as subject consistency, temporal style, spatial relationships, and overall consistency. These metrics are crucial for assessing whether sparse patterns preserve the semantic and temporal coherence of generated videos.\n\n2.The evaluation does not compare against other sparse attention methods for video generation such as DiTFastAttn, MInference applied to video models, or cache-based methods like PAB. \n\n3.Critical design decisions lack ablation studies. The paper does not validate whether the mixed parallelism strategy outperforms a uniform strategy. The impact of dynamic scheduling versus naive scheduling is mentioned briefly but not thoroughly evaluated across different models and workloads. There is no sensitivity analysis on the sparsity hyperparameters (cs and ct) to show robustness across different efficiency-accuracy trade-offs.\n\n4.The paper provides only two visual examples in Figure 6. More extensive qualitative comparisons across diverse scenarios (minor vs. significant scene changes, rare vs. frequent object interactions) would strengthen the quality claims, especially given the method's high PSNR but potential for subtle temporal artifacts.\n\n5.While the paper claims hardware efficiency through layout transformation, it does not provide detailed kernel-level benchmarks comparing the proposed approach against naive sparse attention implementations across different sparsity levels."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vj0OSIynLJ", "forum": "1ZmdfDzGE1", "replyto": "1ZmdfDzGE1", "signatures": ["ICLR.cc/2026/Conference/Submission19434/Reviewer_RRN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19434/Reviewer_RRN5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799501326, "cdate": 1761799501326, "tmdate": 1762931356507, "mdate": 1762931356507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dynamic Selective Attention (DSA) to accelerate video generation.\n\nDSA adopts the dynamic sparse patterns based on the fixed spatial and temporal sparse patterns.\n\nThis work mainly focus on the contribution on the system level scheduling design.\n\nThe experimental results show that this work achieves good results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work shows how to schedule the attention computation for sparse attention."}, "weaknesses": {"value": "1. This work does not provide the detailed dynamic sparse pattern online generation methods. In Section 4.1, this work only shows that they adopted the spatial and temporal kernels from the work [1], which shows the limited novelty. The dynamic pattern looks like the token-importance based pruning, which is not novel.\n\n2. This work claims dynamic sparse pattern, while it compares to the fixed sparse pattern works like [1]. This work does not compare to dynamic sparse pattern works like [2] \n\n\n---\n[1] Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity\n\n[2] DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance"}, "questions": {"value": "1. Please provide the details of the dynamic sparse pattern generation method.\n\n2. Please provide the overhead (like latency) brought by the dynamic sparse pattern generation.\n\n3. Please provide the comparison to dynamic sparse pattern works in video generation like [1].\n\n---\n[1] DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3nCx3wkrJa", "forum": "1ZmdfDzGE1", "replyto": "1ZmdfDzGE1", "signatures": ["ICLR.cc/2026/Conference/Submission19434/Reviewer_HyRK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19434/Reviewer_HyRK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926556313, "cdate": 1761926556313, "tmdate": 1762931356055, "mdate": 1762931356055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an interesting approach to improving the efficiency of video generation models using distributed sparse attention. However, the lack of supplementary materials, incomplete evaluation, and inconsistent reporting of results hinder the ability to fully assess the effectiveness of the proposed method. Addressing these issues will significantly enhance the paper's quality and credibility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an interesting approach to improving the efficiency of video generation models using distributed sparse attention."}, "weaknesses": {"value": "1. The authors have not provided the MP4 files for the proposed method and comparison methods in the supplementary materials. This makes it very difficult to assess the actual visual quality and temporal consistency of the generated videos.\n\n2. Even without the MP4 files, the authors could have provided code or other means to reproduce the results and evaluate the visual quality.\n\n3. The authors have not tested the full range of metrics from VBench or VBench2.0. Comprehensive evaluation is crucial to understand the strengths and weaknesses of the proposed method.\n\n4. Table 2 includes timing results for USP, but Table 1 lacks corresponding quality metrics for USP. This inconsistency makes it difficult to compare the proposed method with USP comprehensively.\n\n5. The related work section could benefit from a more comprehensive review of pre-trained models for video generation acceleration and sparse/linear attention methods.\n\nIf the authors address the above concerns effectively, particularly by providing supplementary materials and a more comprehensive evaluation, I would be willing to reconsider my assessment and potentially give a more positive score."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jweCbT1WgU", "forum": "1ZmdfDzGE1", "replyto": "1ZmdfDzGE1", "signatures": ["ICLR.cc/2026/Conference/Submission19434/Reviewer_3sqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19434/Reviewer_3sqw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976940961, "cdate": 1761976940961, "tmdate": 1762931355554, "mdate": 1762931355554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed distributed sparse attention (DSA) for DiT-based video generation model and achieved $1.43 \\times$ speed up than the unified sequence parallelism (USP)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By combining sparse attention with distributed strategies, the video generation model achieves improved multi-GPU efficiency while preserving the original generation quality as much as possible.\n\n2. Evaluations were conducted on several mainstream models, including Wan2.1-1.3B, Wan2.1-14B, and Hunyuan-Video, covering video quality metrics (PSNR, SSIM, LPIPS, VBench) as well as system performance metrics (latency and speedup)."}, "weaknesses": {"value": "1. The overall approach resembles a combination of the SVG method and the USP method, with optimized attention scheduling. \n\ntypo: Line 152, \"xx tokens\""}, "questions": {"value": "1. In Table 2, the results of USP + SVG can be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n9whF2TP9S", "forum": "1ZmdfDzGE1", "replyto": "1ZmdfDzGE1", "signatures": ["ICLR.cc/2026/Conference/Submission19434/Reviewer_FLsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19434/Reviewer_FLsG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988707903, "cdate": 1761988707903, "tmdate": 1762931354439, "mdate": 1762931354439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}