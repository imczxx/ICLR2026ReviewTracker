{"id": "UrM4MNRYZm", "number": 15647, "cdate": 1758253566827, "mdate": 1759897291333, "content": {"title": "PonderLM: Pretraining Language Models to Ponder in Continuous Space", "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort.\nIn this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations.\nExperiments across three widely used open-source architectures—GPT-2, Pythia, and LLaMA—and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, our PonderPythia models demonstrate remarkable effectiveness: PonderPythia-2.8B surpasses Pythia-6.9B and rivals Pythia-12B, while our PonderPythia-1B matches TinyLlama-1.1B, a model trained on 10 times more data.", "tldr": "We pretrain language models to ponder within a continuous embedding space.", "keywords": ["language modeling", "pondering language models", "pretraining", "continuous embedding space"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d26193d80d84ab07c396b65f01db0d0b91aec2d.pdf", "supplementary_material": "/attachment/d40add96e6bc2db3ca9ad4a9f35a487a0e8b2417.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel technique (called \"pondering\") to iteratively call the forward pass of an LLM for a single token generation. Typically, a forward pass with an LLM over an input sequence results in a probability distribution over the vocabulary, which is then used for decoding the output token. The pondering approach in this paper utilizes the probability distribution at each token position to compute a weighted sum of the vocab embeddings. The main idea is to leverage such refined embeddings in subsequent forward passes (\"s\" times), followed by the calculation of the CE loss. The results show that the pondering approach improves the FLOPs and data usage efficiency during pre-training of various model architectures and sizes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The methodology is well presented along with clear writing, code, and results.\n\n2. The language modeling and downstream task evaluation results are promising and can influence research on such iterative embedding refinements as a new frontier to explore the scaling of compute."}, "weaknesses": {"value": "1. A major concern I have is about the validity of the scaling law shown in Figure 4 if one were to change the number of pondering steps from 3 to a lower or higher value. Or even if the randomized pondering step approach was employed for that matter. If the scaling laws were similar to figure 4 with a randomized approach of pondering step selection, then that is an even better approach to scale since it is currently unclear how the value of (s=3 steps) was chosen. I believe the message of the paper can be further strengthened if this weakness is addressed.\n\n2. This is a minor concern, but can a model trained to ponder for (s=3) steps be effective when pondering for (s>3) steps during inference? Some early results (either positive/negative) can influence a lot of future work in this direction."}, "questions": {"value": "1. Did the authors analyze any properties of the embeddings after every pondering step? For ex: spectral properties, cosine similarities etc\n\n2. How do the gradient norms change when incorporating pondering vs the vanilla pretraining? \n\n3. Were there any training stability challenges when training from scratch, since I see some spikes in Figure 8 for continued pre-training? How did you address them?\n\n4. An analysis of how the output distributions evolve with iterative pondering steps would be a great addition to the paper. For ex, does the KL divergence of the output distributions change drastically in early pondering steps or later?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1GUHBdwZ6L", "forum": "UrM4MNRYZm", "replyto": "UrM4MNRYZm", "signatures": ["ICLR.cc/2026/Conference/Submission15647/Reviewer_We2v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15647/Reviewer_We2v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464484072, "cdate": 1761464484072, "tmdate": 1762925906637, "mdate": 1762925906637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PonderLM, a self-supervised, architecture-agnostic mechanism that lets a language model “ponder” within a single token step by feeding back a probability-weighted sum of token embeddings for additional forward passes, creating a continuous, fully differentiable inner loop that increases compute per parameter without RL or curated CoT data. Implemented across GPT-2, Pythia, and LLaMA, the approach yields lower pretraining perplexity and strong downstream gains—e.g., PonderPythia-2.8B surpasses Pythia-6.9B and rivals Pythia-12B, while PonderPythia-1B matches TinyLlama-1.1B trained on 10× more data—with performance improving as pondering steps increase. \n\nThe key contributions are: (i) a simple pondering loop that replaces discrete token emission with continuous weighted embeddings; (ii) proof that such behavior emerges via standard next-token pretraining alone; (iii) consistent benefits across model families and scales, especially for smaller models; and (iv) a framing of pondering as a third, orthogonal scaling axis (complementary to parameter and CoT test-time scaling) that may improve parameter knowledge density and reduce communication costs at scale."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a differentiable, self-supervised pondering loop that feeds a probability-weighted embedding back into the model within a single token generation step. This elegant mechanism eliminates the discrete bottleneck imposed by vocabulary spaces during internal computation. By conceptualizing pondering as a third scaling axis, orthogonal to both parameter scaling and test-time CoT scaling, the work offers a novel perspective on model scaling dynamics. Moreover, demonstrating that this behavior can emerge without reinforcement learning or curated CoT supervision substantially relaxes the conventional prerequisites of test-time scaling approaches.\n\n2. The proposed inner loop is straightforward, easily integrable into standard language model architectures, and fully differentiable for end-to-end training using conventional next-token prediction. Empirical evaluations across three architectures (GPT-2, Pythia, and LLaMA) and nine downstream tasks reveal consistent and substantial performance gains—most notably, size-for-size improvements where a 2.8B model outperforms a 6.9B counterpart and approaches the performance of a 12B model. Moreover, the results exhibit monotonic improvements as the number of pondering steps increases."}, "weaknesses": {"value": "1. The paper’s motivation and theoretical foundation appear insufficiently developed. It remains unclear why repeating the forward pass within a single token-generation step should improve performance. The current justification—an analogy to human “slow thinking”—is conceptually interesting but lacks a mechanistic explanation or connection to established findings in neural or cognitive science. Providing a clearer rationale, ideally supported by formal analysis of how the proposed weighted-embedding feedback influences model expressivity, optimization dynamics, or the compute–performance trade-off, would considerably strengthen both the motivation and the overall argument.\n\n2. To substantiate the claim of a latent thinking process, it would be valuable to analyze the model’s internal states and compare them with those from explicit (e.g., CoT) and implicit thinking methods.\n\n3. Several evaluated tasks appear susceptible to data contamination [1]. It would strengthen the paper to quantify the extent of contamination and disentangle its contribution to the observed gains—for example, by applying contamination checks, re-running on decontaminated splits, or reporting performance deltas with/without potentially contaminated items.\n\n\nReferences\n\n1. Koala: An Index for Quantifying Overlaps with Pre-training Corpora. Vu et al. 2023"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ra5lKSnPI7", "forum": "UrM4MNRYZm", "replyto": "UrM4MNRYZm", "signatures": ["ICLR.cc/2026/Conference/Submission15647/Reviewer_w4mh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15647/Reviewer_w4mh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610870557, "cdate": 1761610870557, "tmdate": 1762925905736, "mdate": 1762925905736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper takes the standard output token probabilities and feeds them back into the model a set number of time with gradients attached. The intuition is that this allows the model to \"think\" more before outputting a token."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strong consistent results, good that it only needs general corpus data, comprehensive set of experiments."}, "weaknesses": {"value": "**W1.** Limited novelty - a very simple change and very similar to prior methods. But perhaps this is not a weakness as the results seem good.\n\n**W2.** 4x compute at inference time. With LLMs actually being used now, inference cost is important. I think they should perhaps therefore be compared to 4x larger models which will have the same inference cost. In this case the performance is less strong."}, "questions": {"value": "**Q1.** What is meant by: “potentially reducing communication costs at scale”?\n\n**Q2.** It is unusual for results to be quite this consistent. Are the authors sure there were no other changes that contributed to this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DZthJ68cnE", "forum": "UrM4MNRYZm", "replyto": "UrM4MNRYZm", "signatures": ["ICLR.cc/2026/Conference/Submission15647/Reviewer_Nkod"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15647/Reviewer_Nkod"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769727327, "cdate": 1761769727327, "tmdate": 1762925905046, "mdate": 1762925905046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pondering process to replace the standard single forward pass in transformers. Instead of producing a predictive distribution in one pass, the model refines it iteratively. At each iteration, the model computes the pondering embeddings by using the current predicted distribution to take a weighted sum over all token embeddings. This pondering embedding is then added back to the original token embeddings through a residual connection, and the updated embeddings are fed back into the model to produce a refined output distribution. \n\nThe pondering method shows strong empirical performance across extensive experiments. The pondering-trained Pythia models reach the same performance with significantly fewer parameters and training data, and they consistently outperform their counterparts on 9 downstream benchmarks. Ablation studies on alternative embedding strategies and the number of pondering steps further demonstrate the effectiveness of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea is simple yet effective, and it does not rely on any external supervision. The proposed mechanism is easy to implement and can be plugged into standard Transformer architectures with minimal changes.\n\nThe empirical results are solid and sufficiently demonstrate the effectiveness of the proposed approach. The observation that performance improves monotonically with more pondering steps suggests that the method provides a controllable way to trade compute for performance."}, "weaknesses": {"value": "Since the method introduces additional iterative passes beyond the standard forward pass, it incurs non-trivial training and inference cost, which may become particularly expensive for larger models and longer sequences."}, "questions": {"value": "1. In Eq. (5) you add the pondering embedding via a residual connection. What happens if this residual connection is removed, i.e., if you simply set $E^1=T$. Did you run this ablation? It would help isolate the contribution of the residual pathway itself.\n\n2. Have you examined how the predicted distribution evolves across pondering steps? It would be interesting to see whether each step moves the prediction closer to the target distribution, as this could provide additional interpretability into how the refinement process actually works."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DmjWFJ8Sjg", "forum": "UrM4MNRYZm", "replyto": "UrM4MNRYZm", "signatures": ["ICLR.cc/2026/Conference/Submission15647/Reviewer_Zzsq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15647/Reviewer_Zzsq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794020168, "cdate": 1761794020168, "tmdate": 1762925904407, "mdate": 1762925904407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}