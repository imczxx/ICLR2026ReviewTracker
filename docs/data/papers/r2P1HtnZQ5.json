{"id": "r2P1HtnZQ5", "number": 13029, "cdate": 1758212835353, "mdate": 1759897470049, "content": {"title": "ChemHGNN: A Hierarchical Hypergraph Neural Network for Reaction Virtual Screening and Discovery", "abstract": "Reaction virtual screening and discovery are fundamental challenges in chemistry and material science, where traditional graph neural networks (GNNs) struggle to model multi-reactant interactions.  In this work, we propose ChemHGNN, a hypergraph neural network (HGNN) framework that effectively captures high-order relationships in reaction networks. Unlike GNNs, which require constructing complete graphs for multi-reactant reactions, ChemHGNN naturally models multi-reactant reactions through hyperedges, enabling more expressive reaction representations. To address key challenges—such as combinatorial explosion, model collapse, and chemically invalid negative samples—we introduce a reaction center-aware negative sampling strategy (RCNS) and a hierarchical embedding approach combining molecule, reaction and hypergraph level features. Experiments on the USPTO dataset demonstrate that ChemHGNN significantly outperforms HGNN and GNN baselines, particularly in large-scale settings, while maintaining interpretability and chemical plausibility. Our work establishes HGNNs as a superior alternative to GNNs for reaction virtual screening and discovery, offering a chemically informed framework for accelerating reaction discovery.", "tldr": "This paper introduces ChemHGNN, a hierarchical hypergraph neural network designed to improve virtual screening and discovery of new chemical reactions by modeling multi-reactant interactions more effectively than traditional GNNs.", "keywords": ["Reaction Virtual Screening", "Reaction Discovery", "Hypergraph Neural Network", "Machine Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f3de144b1602876315876045c70c895e5187053.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ChemHGNN, a hierarchical hypergraph neural network designed for chemical reaction virtual screening and discovery. The authors argue that traditional GNNs fail to effectively capture high-order interactions among multiple reactants, which are naturally modeled by hypergraphs. ChemHGNN integrates: \n\n1. A Weisfeiler–Lehman Network (WLN) pretrained for reaction-center prediction at the molecular level, \n\n2. A hypergraph neural network (HGNN) capturing high-order molecule–reaction relations, \n\n3. A reaction-center–aware negative sampling (RCNS) mechanism to generate chemically valid negative examples, and a simulated annealing (SA)–based “sort-out block” for efficient virtual screening.\n\n Extensive experiments on the USPTO-410k dataset (1k, 5k, 10k subsets) show that ChemHGNN consistently outperforms both GNN and HGNN baselines in F1-score, specificity, and generalization across unseen reaction templates. The paper claims improved robustness against model collapse and better chemical plausibility of generated samples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、Novel task formulation.\nThe paper focuses on reaction feasibility prediction (“can a given reactant set react?”), which differs from traditional product-prediction tasks and is conceptually interesting.\n\n2、Clear and modular architecture.\nThe hierarchical design is logically consistent and well-motivated."}, "weaknesses": {"value": "1.  Several figures in the main text and appendix suffer from extremely low resolution, making them difficult to read (e.g., Figs. 4–7). In addition, there are clear labeling and content errors( for instance, in Fig. 7, the title of the third sub-figure appears to be incorrect, and the accompanying color bar seems unrelated to the plotted image). These presentation issues significantly hinder readability.\n\n2. The experimental comparison is limited to general-purpose graph models such as GCN,, and HGNN, while omitting more closely related and competitive approaches specifically designed for chemical reaction modeling. In particular, methods like Rxn Hypergraph[1], DLGNet[2],raction graph[3], and more recent graph neural networks or graph transformer architectures should have been included.\n\n3. The “Sort-Out Block” introduced in Section 3.3 seems not to be used in the main experiments or ablation experiments presented in the paper. It functions only as an auxiliary, post-hoc component for the reaction-discovery demonstration rather than as part of the core ChemHGNN framework. Therefore, including it in the main methodology section may be misleading; this component would be more appropriately presented as a supplementary or application section instead of within the primary model description.\n\n4. The “bond formation/breaking conservation” loss (Eq. 12) is over-interpreted. It is unclear how r_i relates to real bond changes, since the model neither learns explicit bond transformations nor receives bond-level supervision. The claimed chemical meaning is unsubstantiated; this term functions only as a simple regularization and should be mentioned briefly in the appendix rather than highlighted in the main text.\n\n5. see question.\n\n[1]Rxn Hypergraph: a Hypergraph Attention Model for Chemical Reaction Representation\n\n[2] DLGNET: HYPEREDGE CLASSIFICATION THROUGH DIRECTED LINE GRAPHS FOR CHEMICAL REACTIONS\n\n[3] Reaction Graph: Toward Modeling Chemical Reactions with 3D Molecular Structures"}, "questions": {"value": "1. The t-SNE visualization in figs7 does not clearly support the claimed superiority of ChemHGNN—if anything, the HGNN baseline appears to show more distinct clustering. Could the authors clarify how this figure validates the proposed method?\n\n2. Why does RCNS not outperform SNS in Tables 7, 8, 13, and 18?  The proposed RCNS method seems ineffective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5uUrfO3Kz8", "forum": "r2P1HtnZQ5", "replyto": "r2P1HtnZQ5", "signatures": ["ICLR.cc/2026/Conference/Submission13029/Reviewer_XhcC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13029/Reviewer_XhcC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298449571, "cdate": 1761298449571, "tmdate": 1762923763758, "mdate": 1762923763758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **ChemHGNN**, a hierarchical framework for chemical reaction screening, arguing that standard GNNs fail to model multi-reactant interactions. ChemHGNN addresses this by representing molecules as nodes and reactions as hyperedges. The model fuses features from a pre-trained GNN (a WLN) with a hypergraph neural network (HGNN) to learn both molecular and relational patterns. A novel, domain-aware negative sampling (RCNS) strategy is also proposed. Experiments on USPTO subsets show ChemHGNN outperforms GNN baselines, avoids the \"model collapse\" that plagues them, and generalizes to unseen reaction types."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated hierarchical design.** Using a pre-trained WLN to extract molecule/bond features and feeding them into a hypergraph-level model is a natural way to combine local (bond-level) and global (reaction-level) context.\n2. **Insightful empirical analysis.** Experiments diagnose the “model collapse” failure mode of some GNNs and demonstrate ChemHGNN’s robustness.\n3. **Generalization to unseen templates.** Results suggest the model captures transferable chemical patterns that apply to novel reaction templates.\n4. **High-impact application.** Tackles an important problem, reaction screening and molecular discovery, with direct practical relevance."}, "weaknesses": {"value": "1. **Questionable novelty / outdated components.** The core architecture mainly combines existing components (WLN + a basic HGNN). The paper does not convincingly justify why these choices were preferred over more expressive, modern GNN/HGNN designs. The technical contribution risks reading as an engineering assembly rather than a novel network design.\n2. **Dataset limitations.** Evaluation is confined to subsets of the USPTO dataset. Additional reaction corpora are needed to substantiate claims of robustness and generalizability.\n3. **Missing technical detail & ablations.** The WLN pre-training procedure is under-specified. Crucially, there is no ablation comparing a frozen WLN versus fine-tuning it end-to-end, an important design choice that should be justified experimentally.\n4. **Hypergraph representation concerns.** The work models reactions as undirected hypergraphs and does not explore directed hypergraph formulations, which may be more natural for reactant → product transformations.\n5. **Insufficient baseline coverage.** The only HGNN baseline used is an older (2019) model. The evaluation would be stronger if compared against more recent, competitive hypergraph and direction-aware graph methods."}, "questions": {"value": "1. **Modeling choices & baselines** Can the authors justify combining a WLN with a basic HGNN rather than using more expressive modern architectures? For example, how would work ChemHGNN with the use of a more strong method such as Dir-GNN[1] or Neural Sheaf Diffusion [2], or to strong hypergraph operators like AllSet [3] or ED-HNN [4] ?\n\n2. **Hypergraph directionality**\nThe current hypergraph formulation is undirected and includes only reactants. Have the authors considered directed hypergraph representations to capture the intrinsic reactant → product transformation? In particular, how might a directed-hyperedge approach, methods like GeDi-HNN [5], affect expressivity and performance?\n\n3. **Experimental coverage & comparative evaluation** Why were recent graph and hypergraph methods (e.g., Dir-GNN, Neural Sheaf Diffusion, AllSet, edge-centric hypergraph approaches) excluded from the comparisons? Please clarify whether including these baselines would alter the conclusions about state-of-the-art performance.\n\n4. **Pre-training and fine-tuning** Please provide full details of the WLN pre-training (pretext task, training data, labels, optimization hyperparameters). Crucially, did you evaluate fine-tuning the WLN end-to-end on the reaction screening task, and if so, how did fine-tuning affect accuracy, stability, and the reported “model collapse” behavior?\n\n5. **Dataset scope and generalization** Experiments are restricted to USPTO subsets. How do the authors expect ChemHGNN to generalize to other reaction benchmarks or to datasets with substantially different reaction-type distributions? Can you provide cross-dataset or transfer evaluations, or justify why these were not attempted?\n\n\n\n[1] Rossi, Emanuele, et al. \"Edge directionality improves learning on heterophilic graphs.\" Learning on graphs conference. PMLR, 2024.\n\n[2] Bodnar, Cristian, et al. \"Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns.\" Advances in Neural Information Processing Systems 35 (2022): 18527-18541.\n\n[3] Chien, Eli, et al. \"You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks.\" International Conference on Learning Representations.\n\n[4] Wang, Peihao, et al. \"Equivariant Hypergraph Diffusion Neural Operators.\" The Eleventh International Conference on Learning Representations.\n\n[5] Fiorini, Stefano, et al. \"Let there be direction in hypergraph neural networks.\" Transactions on Machine Learning Research (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n6U9vsry5q", "forum": "r2P1HtnZQ5", "replyto": "r2P1HtnZQ5", "signatures": ["ICLR.cc/2026/Conference/Submission13029/Reviewer_WQKT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13029/Reviewer_WQKT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747242745, "cdate": 1761747242745, "tmdate": 1762923763119, "mdate": 1762923763119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ChemHGNN, a hierarchical Hypergraph Neural Network (HGNN) for reaction virtual screening. Inputs are predefined candidate reactant sets (hyperedges) drawn from positives in the dataset and negatives created by Negative Sampling (NS) heuristics: Sized (SNS), Motif (MNS), Clique (CNS), and a new Reaction-Center Negative Sampling (RCNS). The model fuses: (a) a frozen Weisfeiler–Lehman Network (WLN) pretrained for reaction-center prediction to pool atom features into molecule embeddings, (b) a two-layer HGNN over the reaction hypergraph, and (c) a cross-attention fusion, followed by an Multi-Layer Perceptron (MLP) classifier. Training uses binary cross-entropy plus a zero-sum Mean Squared Error (MSE) regularizer on the sum of per-molecule embeddings. The paper adds a Simulated Annealing (SA) “sort-out” stage that searches over reactant combinations using a hand-crafted objective (the Euclidean norm of the sum of learned molecular vectors. Experiments use USPTO-410k subsets at 1k, 5k, and 10k; ChemHGNN reports gains over HGNN, Graph Convolutional Network (GCN), Graph Attention Network (GAT), and Neural Overlapping Community Detection (NOCD) on several metrics, with the largest gains at 10k."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The motivation for higher-order modeling with hypergraphs is clear, though it is not novel given a substantial literature. The pipeline is tidy: a reaction-center-pretrained Weisfeiler–Lehman Network (WLN) produces molecule embeddings that feed the hypergraph model. On USPTO, the method shows gains across multiple negative sampling schemes (NS). Ablations indicate that removing the WLN features, using something other than simple sum aggregation, or dropping the mean squared error regularizer (MSE) degrades performance."}, "weaknesses": {"value": "The model does not natively propose or score arbitrary reactant sets; it only classifies provided hyperedges. Candidate sets come from dataset positives and NS-generated negatives; the only attempt to explore new combinations is SA, which is is an untrained block that is \"external\" to the proposed neural net. This weakens the “discovery” narrative and entangles performance with SA design choices. External validity is thin: all results are USPTO-based; no independent datasets or real screening case studies. Baselines are limited for hyperlink prediction; stronger modern hypergraph predictors are not compared.\n\nLastly, I think baselines are missing, including:\nHyper-SAGNN: a self-attention based graph neural network for hypergraphs: https://openreview.net/forum?id=ryeHuJBtPH\nNHP: Neural Hypergraph Link Prediction: https://dl.acm.org/doi/10.1145/3340531.3411870 -- this is cited in the paper, but no comparisons to it are made.\nPrincipled Hyperedge Prediction with Structural Spectral Features and Neural Networks: https://arxiv.org/abs/2106.04292\nA Hypergraph Neural Network Framework for Learning Hyperedge-Dependent Node Embeddings: https://arxiv.org/abs/2212.14077\nHypergraph contrastive attention networks for hyperedge prediction with negative samples evaluation: https://www.sciencedirect.com/science/article/abs/pii/S0893608024007317\nLink Prediction with Relational Hypergraphs: https://arxiv.org/html/2402.04062v2"}, "questions": {"value": "Can the model score any arbitrary set of molecules without predefining a hyperedge and without SA?\nWhy does SA optimize the vector-sum norm surrogate rather than the classifier score? Can you compare to this option?\nWhat is the impact of SA's hyperparameters and iteration budgets on the results?\nMaybe I missed something, but why should the sum of per-molecule embeddings be near zero in general chemistry?\nDo you have extra results beyond those on USPTO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ryxNXVLCiV", "forum": "r2P1HtnZQ5", "replyto": "r2P1HtnZQ5", "signatures": ["ICLR.cc/2026/Conference/Submission13029/Reviewer_v4aA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13029/Reviewer_v4aA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128166590, "cdate": 1762128166590, "tmdate": 1762923762735, "mdate": 1762923762735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}