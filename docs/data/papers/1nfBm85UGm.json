{"id": "1nfBm85UGm", "number": 11699, "cdate": 1758203175066, "mdate": 1759897560073, "content": {"title": "WAN3DNS: WEAK ADVERSARIAL NETWORKS FOR SOLVING 3D INCOMPRESSIBLE NAVIER-STOKES EQUATIONS", "abstract": "The 3D incompressible Navier-Stokes equations model essential fluid phenomena, including turbulence and aerodynamics, but are challenging to solve due to nonlinearity and limited solution regularity. Classical solvers are costly, and neural network-based methods typically assume strong solutions, limiting their use in underresolved regimes. We introduce WAN3DNS, a weak-form neural solver that recasts the equations as a minimax optimization problem, enabling learning directly from weak solutions. Using the weak formulation, WAN3DNS circumvents the stringent differentiability requirements of classical physics-informed neural networks (PINNs) and accommodates scenarios where weak solutions exist, but strong solutions may not. We evaluate WAN3DNS's accuracy and effectiveness on three benchmark cases: the Kovasznay, Beltrami, and 3D lid-driven cavity flows. Furthermore, using Galerkin's theory, we conduct a rigorous error analysis and show that the $L^{2}$-training error is controllably bounded by the architectural parameters of the network and the norm of residues. It implies that for neural networks with a small loss, the corresponding $L^{2}$-error will be small as well. This work bridges the gap between weak solution theory and deep learning, offering a robust alternative for complex fluid flow simulations with reduced regularity constraints.", "tldr": "", "keywords": ["Generative models; Computational Fluid Dynamics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e059336c57670e3b182a4289243c164a80f9360.pdf", "supplementary_material": "/attachment/7d2cacbd5120ed3f9c8999c911b89f1930304bf5.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript introduces the WAN3DNS framework, which is a weak-form adversarial NN framework to solve 2D/3D incompressible N.-S. equations. In contrast to classical PINNs that rely on differentiable strong solutions, the proposed WAN3DNS framework operates directly on the weak formulation of the governing equations. It reformulates the PDE system as a min-max optimization problem, where the primal network predicts velocity and pressure, and the adversarial network generates divergence-free test functions to enforce the PDE constraints. Incompressibility is implicitly enforced under this new formulation. A theoretical error bound links the L2 training error to physical accuracy. Empirical validation on three canonical benchmarks shows that WAN3DNS achieves higher accuracy than DeepXDE, NSFnets, and WAN-Biharmonic."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel weak-form adversarial formulation**: This manuscript extends the legacy WAN networks from scalar to vector-valued PDEs..\n- **Theoretical guarantees**: This manuscript provides an explicit error bound connecting the network’s residuals to the L2 solution error. The use of Galerkin theory to justify network error behavior is mathematically sound.\n- **Experimental rigor**: The benchmarks cover steady, unsteady, and singular flows. The experimental results show clear accuracy gains over prior methods."}, "weaknesses": {"value": "- **Insufficient physical complexity**: The test cases only involve low-Reynolds-number, laminar flows. WAN3DNS’s scalability to turbulent or high-Re regimes remains unclear.\n- **No discussions on computational efficiency**: While the adversarial training improves expressivity, it will increase the computational cost and instability, yet runtime and scalability analyses are not thoroughly discussed. \n- **Adversarial optimization sensitivity**: The adversarial training process suffers from high sensitivity to hyperparameters (e.g., learning rates, penalty coefficients), which could hinder reproducibility or robustness.\n- **No ablation on modern architectures**: Only MLPs are tested. It is unclear how WAN3DNS would perform with more modern architectures (e.g., neural operators, graph neural networks, etc.)."}, "questions": {"value": "- How would the proposed WAN3DNS framework scale with domain size and sampling density?\n- How are test functions in the adversarial network initialized or regularized? How do the authors ensure numerical stability during the training phase?\n- Would replacing the MLPs with operator-learning architectures (e.g., neural operators, graph neural networks) further improve generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GQxBaaEgEz", "forum": "1nfBm85UGm", "replyto": "1nfBm85UGm", "signatures": ["ICLR.cc/2026/Conference/Submission11699/Reviewer_fsru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11699/Reviewer_fsru"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760730958650, "cdate": 1760730958650, "tmdate": 1762922747242, "mdate": 1762922747242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WAN3DNS, a deep learning framework for solving the 3D incompressible Navier-Stokes (NS) equations based on the Weak Adversarial Network (WAN) approach. Traditional Physics-Informed Neural Networks (PINNs) rely on the strong formulation of PDEs, requiring high solution regularity and automatic differentiation. In contrast, WAN3DNS leverages the weak (integral) formulation, which relaxes these regularity requirements.\n\nThe core methodology recasts the weak form of the NS equations (including the momentum equations and the divergence-free constraint) into a minimax optimization problem. A primary network approximates the velocity and pressure fields, aiming to minimize the residuals in the appropriate dual norms. Simultaneously, an adversarial network generates test functions to maximize these residuals, akin to a GAN training dynamic. This work extends the WAN approach, previously limited to scalar PDEs or 2D NS via stream functions (WAN-Biharmonic), to the full 3D velocity-pressure formulation."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-motivated approach with several significant strengths in the domain of scientific machine learning for fluid dynamics:\n\n1. Originality and Significance: The extension of the WAN framework to the 3D incompressible Navier-Stokes equations in the velocity-pressure formulation is novel and non-trivial. Prior WAN approaches for NS (e.g., WAN-Biharmonic) relied on the 2D stream function formulation, which does not generalize easily to 3D. Successfully handling the coupled vector system and the divergence-free constraint within an adversarial weak-form setting is a valuable methodological contribution.\n\n2. Robustness via Weak Formulation: By leveraging the weak formulation, the method inherently relaxes the differentiability requirements needed by strong-form PINNs. This advantage is clearly demonstrated in the 3D lid-driven cavity (LDC) experiment (Table 3, Figure 9), where WAN3DNS handles the corner singularities much more robustly than the PINN baseline, addressing a known limitation of strong-form approaches.\n\n3. Strong Empirical Performance: The numerical results are compelling. WAN3DNS generally outperforms the SOTA baselines across the benchmarks. The accuracy of the pressure field in the Beltrami flow (Table 2) is particularly impressive, showing orders of magnitude improvement over the baselines (e.g., 0.011 error for WAN3DNS vs. 10.54 for DeepXDE).\n\n4. Theoretical Effort: The inclusion of a theoretical error analysis (Theorem 1) that aims to bound the L2 error by the training residuals is valuable, although the applicability of the proof has critical issues (see Weaknesses)."}, "weaknesses": {"value": "Despite the promising results, the paper has significant weaknesses that need to be addressed, particularly concerning the theoretical claims and the rigor of the empirical comparison.\n\n1. Critical Discrepancy Between Theory and Experiments (Major Concern): The theoretical error analysis presented in Appendix B contains a critical flaw regarding its applicability to the numerical experiments. Line 671 explicitly states, \"we consider the NS equation with periodic boundary conditions.\" The definition of the boundary residual also relies on periodicity. However, all numerical experiments (Kovasznay, Beltrami, and LDC) utilize Dirichlet boundary conditions. The proof techniques, particularly the handling of boundary terms arising from integration by parts (e.g., Eqs. 18 and 19), are heavily dependent on the boundary conditions. This mismatch fundamentally undermines the relevance of Theorem 1 to the presented experimental results and severely impacts the soundness of the theoretical claims.\n\n2. Fairness and Rigor of Baseline Comparisons: For the Beltrami flow experiment, the paper states (L375) that the comparison was conducted \"using the same hyperparameters\" for WAN3DNS, NSFnets, and DeepXDE. This approach is methodologically flawed, as optimal hyperparameters vary significantly across different methods and training strategies (e.g., PINN vs. WAN). A fair comparison requires comparing the best-tuned version of each method.\n\n3. Baseline Specificity: In the 3D LDC experiment (Table 3), the comparison is restricted to a vague \"PINNs\" baseline, rather than the specific SOTA methods (DeepXDE, NSFnets) used elsewhere. The specific implementation must be detailed for reproducibility.\n\n4. Training Stability and Dynamics: The WAN methodology relies on adversarial training (a minimax game), which is known to be unstable and difficult to tune. While a sensitivity analysis is provided (Appendix E), the paper lacks a discussion on the stability of the training process itself (e.g., convergence plots for both networks, the ratio of training steps, or specific stabilization strategies employed).\n\n5. Limited Scope and High Regularity Assumptions: The experiments are limited to relatively low Reynolds numbers (e.g., Re=400 for LDC). The performance and advantages of WAN3DNS in more challenging, high-Re regimes, where solution regularity is lower, remain unevaluated. Furthermore, the theoretical analysis (Theorem 1) requires strong regularity assumptions on the exact solution, which somewhat contradicts the motivation of using the weak form for low-regularity scenarios."}, "questions": {"value": "I request the authors address the following critical questions during the rebuttal phase:\n\n1. Can the authors address the critical discrepancy regarding the boundary conditions used in the proof of Theorem 1 (periodic BCs mentioned in L672) versus the Dirichlet BCs used in the experiments? Does the proof hold for Dirichlet boundary conditions? If so, please detail how the boundary terms arising from integration by parts (e.g., in Eq. 18 and 20) are rigorously handled or bounded in the non-periodic case.\n\n2. a) For the comparisons in Tables 1 and 2, can you confirm if the baselines (DeepXDE, NSFnets) were independently optimized, rather than just using the hyperparameters tuned for WAN3DNS? If the latter, the comparison is not fair. b) Which specific implementation and architecture were used for the \"PINNs\" baseline reported in Table 3?\n\n3. In the Beltrami flow (Table 2), the pressure errors for DeepXDE and NSFnets are exceedingly high. Can you elaborate on why the strong-form baselines fail so dramatically in pressure recovery in this specific case, and how the weak adversarial formulation specifically aids in accurate pressure reconstruction?\n\n4. Could the authors provide more insight into the stability of the adversarial training process? A visualization of the loss curves for both the primary and adversarial networks would be helpful. Were specific techniques required to stabilize the minimax optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WtfFlmy9wM", "forum": "1nfBm85UGm", "replyto": "1nfBm85UGm", "signatures": ["ICLR.cc/2026/Conference/Submission11699/Reviewer_cvjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11699/Reviewer_cvjj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956240153, "cdate": 1761956240153, "tmdate": 1762922746524, "mdate": 1762922746524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a neural network method to solve the incompressible Navier-Stokes equations (NSE). The specific method is a weak adversarial approach, in which the weak form of the equations is considered, and a min-max problem is formulated: minimization or the residuals with respect to the trial functions, and maximization of the residuals with respect to the test functions. This method had been introduced in Zhang et al (2020) and is cited in this manuscript. This work extends the method of Zhang et al.to vector valued problems (in particular, NSE). The authors present a brief theoretical analysis of the errors, and test their method on three well known benchmark problems, and compare against WAN, DeepXDE and NSFNets methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The strengths of this method derives from the strength of the parent method (Zhang et al (2020)). Zhang et al (2020)'s paper presents an insightful mathematical formulation of the PDE weak formulation into a min-max problem and using a network and an adversarial training algorithm. The current paper applies that method to the incompressible Navier-Stokes equations.\n\n* This paper is readable. But there are weaknesses (see below).\n\n* And the authors have presented results on various examples of the Navier-Stokes equations.\n\n* A theoretical analysis on the boundedness of the error is presented.\n\n* Comparison with a few other methods is provided."}, "weaknesses": {"value": "* No new insight.\n\n* Some notations are not properly introduced. It is possible that the paper assumes that the reader is familiar with the original method (Zhang et al. (2020)) and the notation therein. In that case, this must be explicitly stated.\n\n* This paper is readable. But the writing needs to be improved. The mathematical statements (such as Theorem 1) need to be precise, all symbols must be explained, and must have their role to play\n\n* The authors presented a theorem on the bounds of the errors, but did not show any numerical result relating to the estimates, at least not in the main text.\n\n\n$\\textbf{Suggestions}$:\n\n* Line 210: If $\\mathbf{u}$ is a vector valued function, it is recommended to use the notation $\\mathbf{u}:[0,T]\\to H^1_0(\\Omega, \\mathbb{R}^m)$ or $\\mathbf{u}:[0,T]\\to [H^1_0(\\Omega)]^m$.\n\n* Line 217: It would be better to explicitly state $d=2$ or 3.\n\n* Line 228: Should be $u_i$ on the RHS."}, "questions": {"value": "* Eq 10 (and in many subsequent instances): $\\mathcal{A}_t[u]$ is now only a function of $u$, and not of $[u,p]$, why?\n\n* Line 245: is $p$ included in $u_\\theta$?\n\n* Theorem 1: what is $q_\\theta,\\ R_s,\\ R_{PDE}$.. ?\n\n* Theorem 1: I am assuming that the theorem must have a constraint on the viscosity parameter. Is that correct? And did the authors consider this?\n\n* Theorem 1: In the case that the authors assumed a high viscosity regime, did the authors invoke the classical inf-sup condition of the weak form? And did the authors attempt to obtain a similar error estimate on the pressure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g2ClQY0y1M", "forum": "1nfBm85UGm", "replyto": "1nfBm85UGm", "signatures": ["ICLR.cc/2026/Conference/Submission11699/Reviewer_7Bz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11699/Reviewer_7Bz7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008387660, "cdate": 1762008387660, "tmdate": 1762922746128, "mdate": 1762922746128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}