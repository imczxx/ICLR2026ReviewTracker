{"id": "VwdOWNsZ3D", "number": 17122, "cdate": 1758272484686, "mdate": 1763034536020, "content": {"title": "RoboFace: Face Restoration Made Robust via Implicit and Explicit Textual Guidance", "abstract": "Existing blind face restoration methods often struggle with out-of-distribution degradations. While high-level latent spaces like discrete codebooks offer some robustness, they frequently introduce unnatural artifacts under severe corruption; meanwhile, alternative approaches depend on costly degradation scaling and large-scale diffusion model retraining. In this paper, we propose RoboFace, a novel framework that achieves robust face restoration by prompting a pre-trained diffusion model with dual textual guidance. Given low-quality inputs, RoboFace constructs a structured, semantic-aligned space through two complementary guides: implicit guidance from CLIP latent features to preserve visual fidelity and identity, and explicit guidance from natural text prompts for flexible, user-interactive control. These guides are seamlessly integrated via a thoughtfully designed Decoupled Cross-Attention (DCA) module, which adaptively aligns them\nwith the pretrained diffusion model. Extensive experiments demonstrate that RoboFace is exceptionally robust across a wide spectrum of degradations, delivering state-of-the-art results even on challenging low-quality surveillance faces. Our results highlight the promise of semantic guidance as a reliable and flexible paradigm for robust face restoration.", "tldr": "", "keywords": ["Computer Vision", "Blind Face Restoration", "Degradation Robustness", "Textual Guidance"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ad0471b84221d51c66d187f1615bd0f1eac1f0f6.pdf", "supplementary_material": "/attachment/66fdc842383eba6ffd5a7345363fde0df7377546.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RoboFace, a diffusion-based framework for face restoration under real-world degradations. The method introduces a dual textual guidance mechanism consisting of (1) implicit guidance, where CLIP image embeddings are mapped into the text space and denoised via a Remover network, and (2) explicit guidance, where natural-language descriptions generated by LLaVA are injected via a decoupled cross-attention (DCA) design. The goal is to enhance both robustness and controllability while preserving identity consistency. Experiments on multiple benchmarks demonstrate strong visual quality and robustness to out-of-distribution (OOD) degradations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Combining explicit (language-driven) and implicit (CLIP-driven) semantic signals in a decoupled attention order (“explicit → implicit”) is a technical contribution.\n\n2. The method is clearly explained, and the system pipeline (dual textual paths + DCA) is easy to follow."}, "weaknesses": {"value": "1. I am quite confused why  CLIP-based priors is treated as implicit condition. The text encoder of clip can explicitly encode semantic information of the input text. I think it is similar to direct text input.\n\n2. The core components (CLIP-based priors, text conditioning, and correlation-aware loss) have already been explored in recent studies such as DiffBIR, DifFace, and correlation-regularized diffusion approaches.\n\n3. The manuscript does not sufficiently position its contribution against newer 2025 methods like LAFR (2025), TD-BFR (2025), HonestFace (2025), OSDFace (2024), and DiffusionReward (2025), which report stronger fidelity, efficiency, and identity consistency.\n\n4. The experiments omit several very recent, high-performing diffusion-based face restoration methods (e.g., LAFR, TD-BFR, HonestFace, DiffusionReward, VSPBFR). Without these, it is hard to assess the relative advantage of RoboFace, especially in terms of efficiency and identity preservation.\n\n5. The performance may be affected by LLaVA’s description quality under extreme degradations. The paper briefly notes this but does not analyze failure cases or ablations on prompt noise."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pSEXhT9I0o", "forum": "VwdOWNsZ3D", "replyto": "VwdOWNsZ3D", "signatures": ["ICLR.cc/2026/Conference/Submission17122/Reviewer_RDH7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17122/Reviewer_RDH7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889334013, "cdate": 1761889334013, "tmdate": 1762927120906, "mdate": 1762927120906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "koUU7HMYjf", "forum": "VwdOWNsZ3D", "replyto": "VwdOWNsZ3D", "signatures": ["ICLR.cc/2026/Conference/Submission17122/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17122/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763034535351, "cdate": 1763034535351, "tmdate": 1763034535351, "mdate": 1763034535351, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focus on the problem of face restoration and propose a novel blind face restoration framework that leverages dual textual guidance to enhance robustness against diverse degradations.  Specifically, the proposed method integrates implicit guidance from CLIP latent features and explicit guidance from natural language prompts through a Decoupled Cross-Attention (DCA) module, ensuring high-fidelity and user-controllable restorations. Extensive experiments show superior performance over state-of-the-art methods, particularly in handling severe and out-of-distribution degradations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes a dual textual guidance framework that leverages implicit CLIP features and explicit natural-language prompts, using semantic space as an intermediate representation to enable blind face restoration without explicit degradation modeling.\n2. A Decoupled Cross-Attention (DCA) module with an explicit-then-implicit fusion order is introduced to preserve diffusion priors while enhancing identity consistency."}, "weaknesses": {"value": "1. LLaVA-generated descriptions may misidentify facial attributes under severe degradation, and such errors are injected as explicit guidance into the diffusion model, potentially causing identity drift.\n2. RoboFace has a large model size (2590M parameters) and inference latency (4.32s), limiting its practicality in resource-constrained settings.\n3. Explicit prompts rely on an intermediate restoration from Stage I as input to LLaVA, introducing a two-stage pipeline that increases complexity and hinders end-to-end deployment, as final quality depends heavily on the intermediate result."}, "questions": {"value": "1. Have the authors tried generating explicit text directly from the raw LQ input? If not feasible, could LLaVA outputs be post-processed or equipped with a confidence mechanism to mitigate erroneous guidance?\n2. The DCA module uses an explicit-then-implicit fusion order, but the rationale for not using parallel or implicit-then-explicit fusion is unclear. While Table 6 shows performance differences, the underlying mechanism lacks analysis.\n3. As shown in Table 10, RoboFace has 2590M parameters and 4.32s inference time. Can the architecture be simplified for deployment? Does the current design strike a reasonable trade-off between speed and quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1EAlAhaEpr", "forum": "VwdOWNsZ3D", "replyto": "VwdOWNsZ3D", "signatures": ["ICLR.cc/2026/Conference/Submission17122/Reviewer_dLaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17122/Reviewer_dLaE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892224009, "cdate": 1761892224009, "tmdate": 1762927120444, "mdate": 1762927120444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RoboFace, a novel blind face restoration (BFR) framework that leverages dual textual guidance (implicit CLIP features and explicit natural language prompts) to address out-of-distribution (OOD) degradations. The approach is innovative, well-motivated, and demonstrates strong empirical results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Robustness: The method generalizes exceptionally well to OOD degradations, as validated on synthetic (CelebA-Test) and real-world (SCface, LFW, WIDER) datasets. The explicit-then-implicit guidance order is justified through ablations.\n\nRigorous Evaluation: Extensive comparisons with 7 SOTA methods across multiple degradation levels and metrics (FID, LPIPS, CLIPIQA) convincingly demonstrate superiority. The t-SNE analysis and text-similarity experiments provide insightful validation of the proposed textual-space denoising.\n\nControllability: The framework supports user-interactive restoration via explicit text prompts (e.g., modifying facial attributes), adding practical value."}, "weaknesses": {"value": "Questionable Validity of OOD Problem Formulation: The paper's central argument—that existing BFR methods struggle with out-of-distribution (OOD) degradations—relies heavily on evaluating low-quality face cases using FID scores. However, the experimental setup may overstate the practical relevance of the OOD scenario. For instance, the last row of Figure 3 shows that the so-called \"LQ faces\" are almost pure noise, which shifts the problem from faithful restoration to noise-to-image generation. In such extreme cases, it is inherently difficult to guarantee the authenticity of reconstructed facial identities, potentially undermining the core objective of BFR. This raises doubts about whether the proposed OOD benchmark aligns with real-world restoration needs.\n\nInsufficient Comparison with Contemporary Diffusion-Based Methods: While the authors compare RoboFace with several established methods (e.g., CodeFormer, GFPGAN), the literature review overlooks recent advances in diffusion-based BFR. For a fair and comprehensive evaluation, the following works should be discussed and compared:\n\nDR2(Wang et al., 2023): A diffusion-based degradation remover designed for robust blind face restoration.\n\nDiffBIR(Lin et al., 2024): Leverages generative diffusion priors for real-world blind image restoration.\n\nAuthFace(Chen et al., 2025): Focuses on authentic face restoration with face-oriented diffusion priors."}, "questions": {"value": "Please refer to the paper weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wUz2aAcGeX", "forum": "VwdOWNsZ3D", "replyto": "VwdOWNsZ3D", "signatures": ["ICLR.cc/2026/Conference/Submission17122/Reviewer_TMN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17122/Reviewer_TMN5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917522966, "cdate": 1761917522966, "tmdate": 1762927119105, "mdate": 1762927119105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RoboFace, a blind face restoration (BFR) framework that couples CLIP-derived features mapped into the text-conditioning space—and explicit textual guidance—natural-language prompts—inside a Decoupled Cross-Attention (DCA) conditioning scheme for a pretrained diffusion model. The approach aims to improve robustness to out-of-distribution (OOD) degradations without retraining a large diffusion prior. Experiments on synthetic CelebA splits (in-range, partially OOD, fully OOD) and real datasets (LFW-Test, WIDER-Test, SCface) show strong perceptual quality and improved FID on challenging settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Implicit CLIP-to-text tokens (Mapper→Remover) plus explicit prompts, fused via explicit-then-implicit DCA to first set global semantics and then refine identity—a coherent coarse-to-fine logic. On the hardest CelebA split (FID≈262), RoboFace attains the best overall perceptual metrics and competitive FID vs. diffusion and codebook baselines; on SCface, it outperforms alternatives on average FID and maintains high CLIPIQA. Studies dissect effects of (i) implicit vs. explicit guidance, (ii) DCA ordering, (iii) text extractor choice (LLaVA vs. Qwen-VL), and (iv) token count, supporting design choices."}, "weaknesses": {"value": "1. During inference, explicit prompts come from LLaVA run on an intermediate restoration (Stage I) rather than the LQ image; errors or biases here may propagate. The paper acknowledges occasional attribute drift.\n2. Training uses second-order degradations; benchmarks focus on blur/noise/JPEG and surveillance. Fewer results on other real pipelines (e.g., heavy compression artifacts beyond JPEG ranges, motion blur, color shifts).\n3. Model size is large (due to SD 2.1 + ControlNet + dual guidance); while inference time is reported as reasonable, parameters/FLOPs are substantially higher than some baselines. A stronger efficiency-quality tradeoff analysis would help."}, "questions": {"value": "1. How sensitive is performance to LLaVA prompt errors? Can you quantify restoration quality as you inject noise or replace LLaVA with weaker captions?\n2. What is the gap between explicit prompts generated from the Stage-I image vs. directly from the LQ input, especially on SCface?\n3. Can you include tests on motion blur, non-Gaussian noise, color/band-limited artifacts, or smartphone pipeline degradations"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ToIS6nmkH6", "forum": "VwdOWNsZ3D", "replyto": "VwdOWNsZ3D", "signatures": ["ICLR.cc/2026/Conference/Submission17122/Reviewer_yqMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17122/Reviewer_yqMg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762249963471, "cdate": 1762249963471, "tmdate": 1762927118501, "mdate": 1762927118501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}