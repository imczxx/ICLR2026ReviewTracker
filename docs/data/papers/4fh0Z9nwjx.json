{"id": "4fh0Z9nwjx", "number": 23892, "cdate": 1758349919596, "mdate": 1759896791994, "content": {"title": "Exploring Expert Failures Improves LLM Agent Tuning", "abstract": "Large Language Models (LLMs) have tremendous potential as agents, excelling in tasks requiring multiple rounds of decision-making. \nFor large-scale deployment, a smaller LLM is commonly fine-tuned by learning from teacher-model trajectories and subsequently improving itself via interaction with the environment.\nA key challenge is that many complex training tasks never yield a successful trajectory (zero reward): the teacher's trajectories fail to solve them, and the student’s limited exploration cannot discover one despite many attempts. \nWithout reward signals during training, the student is unlikely to solve similarly difficult test tasks.\nApplying Rejection Sampling Fine-Tuning (RFT) to WebShop highlights the issue: GPT-4 (the teacher) may succeed on only 36\\% of the training tasks, and RFT inherently favors actions drawn from those successes. \nAs a result, the student cannot complete most complex tasks for which the teacher does not provide a direct solution because these tasks require more advanced action sequences. \nTo discover reward signals in these complex tasks, we examined the failed teacher trajectories on these challenging tasks, and found that teacher's trajectories often contain valuable guidance—such as plans and key actions—that student seldom used during its exploration. \nMotivated by this insight, we introduce Exploring Expert Failures (EEF), which uses expert actions to improve the exploration during training and carefully incorporates them into the training by masking out potentially harmful actions to prevent contamination of the learning process.\nThis further allows us to let our student model utilize additional weaker yet more cost-efficient teachers, such as GPT-3.5 Turbo, without inheriting the weaker teacher's suboptimal behaviors.\nConsequently, EEF successfully resolves many previously unsolvable tasks and significantly enhances agent performance on test tasks.\nNotably, our approach achieved a remarkable 62\\% win rate in WebShop, surpassing both RFT (53.6\\%) and GPT-4 (35.6\\%). \nTo the best of our knowledge, this establishes a new state-of-the-art, achieving a score of 0.81 on WebShop and 81/100 on SciWorld, two widely used and challenging tasks for evaluating LLM agents.", "tldr": "Our method, EEF,  leverages beneficial actions from failed expert trajectories to enhance LLM agents, achieving SOTA performance on challenging tasks like WebShop and SciWorld.", "keywords": ["LLM agent", "Finetuning", "Imitation Learning", "Reinforcement learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82a33de43b8a2721da75944224a1e2c4695b47b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LLM agent training relies on successful expert trajectories. When an expert fails, there is no learning signal, which limits generalisation. However, there may still be useful sub-sequences even within failed expert trajectories, which are currently discarded. The paper proposes a method to reuse these segments to improve learning, identifying beneficial vs harmful actions, and then using selected positive trajectories for fine-tuning. With their new method, they achieve SOTA on a few benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "It's a good idea, learning from failure, or rather extracting value even from failed trajectories. I always wondered why approaches like q-learning which only calculate their score at the very end are so wasteful. This new method is not exactly very complex, but I would count that as an advantage, rather than a disadvantage. The prove is in the results, which seem to confirm the efficacy of their idea, outperforming GPT-4, and showing robustness for different model bases."}, "weaknesses": {"value": "I am not quite sure whether the WebShop and ScienceWorld benchmarks are really enough to show generalisation to open-domain or real-world agent tasks. If the authors truly believe in their paradigm, I would like them to add more benchmarks. Is that feasible as a revision? Would lead me to probably increase my recommendation.\n\nAlso, the masking and identification of the beneficial actions are more of a heuristic than a deeper analysis. A bit more reflection would likewise upgrade the paper. This would also help against the possible critique of the missing more thorough hyperparameter sensitivity analysis, especially given the rather small set of benchmarks."}, "questions": {"value": "See weaknesses: Both listed weaknesses could be addressed, questions are written there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UONKZTtPjd", "forum": "4fh0Z9nwjx", "replyto": "4fh0Z9nwjx", "signatures": ["ICLR.cc/2026/Conference/Submission23892/Reviewer_jRTw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23892/Reviewer_jRTw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761332621228, "cdate": 1761332621228, "tmdate": 1762942843208, "mdate": 1762942843208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses learning from failed expert trajectories when using rejection fine-tuning (RFT). The authors propose EEF (Exploring Expert Failures): run the student from intermediate expert states, identify important states, find trace segments that lead to success when started from those states, mask harmful earlier steps, and add the successful segments to supervised fine-tuning. EEF is simple to add to RFT and yields substantial gains on two agentic benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Tackles a practically important problem: exploration in long-horizon, sparse-reward tasks.\n\n- The method is simple and practical to implement on top of existing RFT pipelines.\n\n- The paper includes ablations and diagnostics that help connect the method to the observed performance gains.\n\n- Writing and running examples are clear and help explain the idea."}, "weaknesses": {"value": "Here are some weaknesses that if addressed, can prove EEF’s effectiveness, robustness, and practicality.\n\n- Missing success-rate statistics: The paper does not report how often simulations started from intermediate expert states actually find successful continuations (vs. starting from $s_0$). Without these frequencies it’s unclear whether EEF’s core mechanism is generally effective or only works in selected cases.\n\n- There’s no controlled ablation comparing fine-tuning on $s_0$-only, $s_r$-only, and both. Training only on recovery segments could probably reduces performance from initial states. It could be interesting to understand how much of the performance improvements comes from finetuning on $D_{s_0}$ vs $D_r$.\n\n- Since the paper shows gains using cheaper GPT-3.5 traces, a clearer discussion or an explicit cost breakdown (compute / API dollars / token counts) would help readers weigh spending on higher-quality demonstrations versus more simulation budget. A recommendation for budget allocation under a fixed cost constraint might be especially useful.\n\n- Evaluation covers only two benchmarks; adding another long-horizon domain would help determine whether EEF addresses a general failure mode or a domain-specific phenomenon."}, "questions": {"value": "- Could you provide approximate compute / costs (e.g., API dollars or token counts) for the EEF GPT-4 runs, the RFT×6 baseline, and the mixed GPT-3.5+GPT-4 variant in Table 3?\n\n- Have you observed cases where EEF reduces performance (for example by overfitting to brittle recovery actions)? If so, how common are those cases?\n\n- On “simplicity bias”: is there prior empirical work you can cite? If not, would it be possible to quantify how often failed trajectories in your datasets seem explained by simplicity bias?\n\n- How does RFT perform worse than SFT POS in table 3? Isn’t RFT with one iteration equivalent to SFT POS? Since the paper reports the best model across RFT iterations, RFT should in principle perform at least as well as SFT POS. Could you clarify this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y7vxxiR3RQ", "forum": "4fh0Z9nwjx", "replyto": "4fh0Z9nwjx", "signatures": ["ICLR.cc/2026/Conference/Submission23892/Reviewer_sBXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23892/Reviewer_sBXW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768194694, "cdate": 1761768194694, "tmdate": 1762942842883, "mdate": 1762942842883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Exploring Expert Failures (EEF), a fine-tuning method that enhances the performance of LLM-based agents on complex, multi-step tasks where even expert models (e.g., GPT-4) frequently fail. EEF can expand exploration by reusing expert actions, acknowledging that failed trajectories frequently encode useful signals. The method is evaluated on the WebShop and ScienceWorld benchmarks, demonstrating its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is written well and easy to follow. \n\nThe idea of reusing valuable actions from failure trajectories to expand exploration sounds promising.\n\nThe experiments on the WebShop and ScienceWorld benchmarks demonstrate the effectiveness of the method."}, "weaknesses": {"value": "The novelty of the proposed method appears limited, as the key concept of utilizing failure trajectories is not new. It should be noted that similar ideas have been investigated in prior works, including IPR [1], LEMA [2], and STeCa [3].\n\nThe experiments are not sufficiently comprehensive. (1) Several related works are not compared against, such as IPR [1], LEMA [2], and STeCa [3]. (2) Although LLAMA3-8B Instruct and Mistral-7B-v0.3 were used as base models, the results for Mistral-7B-v0.3 are not presented in sufficient detail. Moreover, since Mistral-7B-v0.3 was released a year ago, it is recommended to conduct experiments using Qwen3-8B as an additional base model. (3) In the experiments, a fair comparison should be made (with Iter=3) against the method that only training on solutions from the initial state. (4) It is necessary to compare EEF with different solution selection strategies, such as the randomly selecting strategy. (5) The individual effects of the two types of important states, $D_{s_0}$ and $D_r$, should be verified to understand their respective impacts on the experiment. (6) The analysis experiments currently focus only on WebShop. Similar analyses should be extended to the ScienceWorld benchmark. (7) In the Efficiency Analysis, a brief discussion on the trade-off between the simulation budget (M) and performance gains would be helpful.\n\nRegarding the methodology, harmful states are identified through agent simulation. Could sampling issues potentially lead to inaccurate judgments in this process?\n\n[1] Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement, EMNLP 2024 \\\n[2] Learning From Mistakes Makes LLM Better Reasoner, 2023  \\\n[3] STeCa: Step-level Trajectory Calibration for LLM Agent Learning, 2025.2"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VKVXDdgcb8", "forum": "4fh0Z9nwjx", "replyto": "4fh0Z9nwjx", "signatures": ["ICLR.cc/2026/Conference/Submission23892/Reviewer_VEFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23892/Reviewer_VEFG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816017620, "cdate": 1761816017620, "tmdate": 1762942842558, "mdate": 1762942842558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method called Exploring Expert Failures (EEF) to improve the fine-tuning of LLM agents. The authors address a key limitation of standard techniques like Rejection Sampling Fine-Tuning (RFT), where agents learn only from successful expert demonstrations and thus fail to master complex tasks where the expert often fails. The core idea of EEF is to salvage useful information from the expert's failed trajectories by simulating the agent's performance from intermediate steps. By identifying segments of a failed path that can lead to success, EEF incorporates these \"beneficial actions\" into the training data, demonstrably improving performance on benchmarks like WebShop and SciWorld."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem studied in the paper is very interesting, especially the analysis of RFT, although it seems that this problem has not been well solved."}, "weaknesses": {"value": "The methodology's innovation is arguably incremental. It builds directly upon the existing RFT paradigm, and its primary contribution is a more sophisticated data filtering and augmentation strategy rather than a fundamentally new approach to agent learning. The effectiveness of EEF is heavily dependent on extensive simulation and sampling. The process requires re-simulating trajectories from numerous states within failed expert attempts to identify useful sub-paths, raising concerns about its computational expense and scalability. The performance gains are achieved through what is essentially a more guided, brute-force exploration of the expert's failure space. In essence, the method refines the \"guess-and-check\" nature of sampling-based tuning by adding a more targeted \"check\" phase, but it does not move beyond this data-intensive framework.\n\n\n1. What if the model consistently fails to sample a successful trajectory?\n\n2. Is there an analysis of the sampling efficiency? Specifically, what proportion of the explored trajectories are ultimately found to be useful for training?\n\n3. As I mentioned in the summary, this method relies on extensive sampling and is confined to Supervised Fine-Tuning.\n\n4. From another perspective, the method like GRPO also samples a large number of rollouts but then uses Reinforcement Learning to optimize the model, rather than SFT. This appears to be a more logical approach."}, "questions": {"value": "Stated in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eRgYdPYqVu", "forum": "4fh0Z9nwjx", "replyto": "4fh0Z9nwjx", "signatures": ["ICLR.cc/2026/Conference/Submission23892/Reviewer_m5Mf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23892/Reviewer_m5Mf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982108938, "cdate": 1761982108938, "tmdate": 1762942842249, "mdate": 1762942842249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}