{"id": "qA42mWsnbl", "number": 9983, "cdate": 1758154385041, "mdate": 1763677427330, "content": {"title": "Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time", "abstract": "We study in-context learning (ICL) of linear regression in a deep linear self-attention model, characterizing how performance depends on various computational and statistical resources (width, depth, number of training steps, batch size and data per context). In a joint limit where data dimension, context length, and residual stream width scale proportionally, we analyze the limiting asymptotics for three ICL settings: (1) isotropic covariates and tasks (ISO), (2) fixed and structured covariance (FS), and (3) where covariances are randomly rotated and structured (RRS). For ISO and FS settings, we find that depth only aids ICL performance if context length is limited. Alternatively, in the RRS setting where covariances change across contexts, increasing the depth leads to significant improvements in ICL, even at infinite context length. This provides a new solvable toy model of neural scaling laws which depends on both width and depth of a transformer and predicts an optimal transformer shapes as a function of compute.", "tldr": "A theory of scaling laws for ICL regression that predicts optimal width/depth shapes.", "keywords": ["Deep Learning", "scaling laws", "in-context learning", "transformers", "attention"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fa93267ecb7d8e2c72550de9e435d335f2ce396.pdf", "supplementary_material": "/attachment/d8d4c9f5bae7f23ed120f52d40e0136dc93771c0.zip"}, "replies": [{"content": {"summary": {"value": "This paper theoretically investigates in-context learning for linear regression using deep linear self-attention models, analyzing performance based on width (N), depth (L), context length (P), pretraining time (t), and data structure. Examining isotropic (ISO), fixed structured (FS), and randomly rotated structured (RRS) covariance settings, the authors find that depth primarily benefits ICL in the ISO and FS settings only when context length is limited; for infinite context length in these settings, increasing depth beyond L=1 offers no advantage. However, in the more complex RRS setting where covariances vary, depth significantly improves performance even at infinite context length. For this RRS case, the paper derives a Chinchilla-like scaling law and predicts a compute-optimal shape scaling, linking optimal architecture to task data properties."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The paper provides a comprehensive theoretical analysis of multi-layer linear self-attention models for in-context linear regression across three distinct covariate settings (ISO, FS, RRS).\n- This paper rigorously characterizes the training dynamics using gradient flow analysis, revealing how the model learns under different data structures and providing an interpretation of the learned estimator as implementing multi-step gradient descent with optimal step sizes.\n- The derivation of a Chinchilla-like neural scaling law incorporating time, width, depth, and context length for the RRS setting in the context of linear regression with power-law features is a significant theoretical contribution.\n- The application of Dynamical Mean Field Theory (DMFT) to derive a two-point deterministic equivalent for the loss landscape under random rotations represents a novel technical approach for analyzing complex learning dynamics in this asymptotic regime."}, "weaknesses": {"value": "-The presentation of detailed proofs and derivations within the appendix could be improved for clarity and accessibility, making it challenging to fully verify the technical steps."}, "questions": {"value": "- Could the authors elaborate on the necessity of employing DMFT to derive the closed-form loss expression in Result 7? Is this approach required because directly analyzing the gradient flow dynamics in (13) is intractable, perhaps due to the lack of a known closed-form solution for the ODE governing $\\gamma(t)$ in the randomly rotated setting with finite width N?\n- The current analysis focuses heavily on the proportional asymptotic regime. Are the techniques employed amenable to deriving non-asymptotic results that might provide insights into the behavior of the system with sizes?\n- The derivation of the two-point deterministic equivalent using DMFT in Appendix A introduces notation that appears distinct from the parameters used in the main text to describe the Transformer model and its dynamics and it's hard to directly match (22) with the main result. Could the authors provide a clearer mapping between the DMFT variables/order parameters and the model parameters/dynamics described earlier in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cGFWBa69Tw", "forum": "qA42mWsnbl", "replyto": "qA42mWsnbl", "signatures": ["ICLR.cc/2026/Conference/Submission9983/Reviewer_NdPw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9983/Reviewer_NdPw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537857663, "cdate": 1761537857663, "tmdate": 1762921416997, "mdate": 1762921416997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank the reviewers for their careful reading and constructive feedback of this work. We have made many improvements to the paper in response to the reviews and would appreciate the reviewers to consider improving their evaluation of the paper. Below we outline some common concerns raised in many of the reviews and what steps we've taken to address them. \n\n### Common Concerns\n\nWe now discuss some common concerns which appeared in many of the reviews and the steps we have taken to address them in the new draft. We have made significant updates to the paper to address these concerns and hope that the reviewers will consider improving their scores in light of these updates.\n\n#### Looped transformer vs untied weights\n\nMany reviewers were concerned that we focused our analysis on a model with tied weights across layers ($\\Gamma^\\ell = \\Gamma$ for all $\\ell$) instead of having distinct parameters for each attention block.\n\nThe new version of our draft **directly addresses this question** in the new Section 5.1 (see new Result 9) with supporting calculations the new Appendix G.1 and simulations in Figure 8.\n\nThe key result is that for **ISO** or **RRS** ICL covariates and an initial condition that is symmetric and isotropic across layers $\\Gamma^\\ell(0) = \\gamma(0) I$, there is **no difference** between the dynamics and scaling law that we derived for the looped transformer and the decoupled depth $L$ model. A consequence of this result is that **our looped transformer theory** exactly describes the dynamics of a deep model when training on RRS covariates. We verify this empirically in the new Appendix Figure 8.\n\n#### RRS setting is not the most general ICL data distribution\n\nSome reviewers pointed out that our focus on the RRS setting does not cover the most general possible ICL distribution shift. We appreciate this critique and to respond we would like to emphasize the following points\n\n1. We are primarily interested in showing that **covariance diversity** during pretraining is important to utilize depth to discover an ICL solution that generalizes across data covariances. The RRS setting is but **one example** of a (especially tractable) data distribution that has this property. \n2. The nice thing about the RRS model is that it enables analytical calculations through connections to free probability of random matrices which enables us to extract interesting scaling laws. This is not to say that interesting depth scaling laws could not arise for other ICL distributions with diverse covariances.\n3. In general, we can easily allow for varying spectra $\\Lambda$ across contexts too, but the key effect needed for depth separation is random rotation. \n\nIn response to this critique we added some additional discussion in Limitations and Future Directions.\n\n#### Does our theory only hold in proportional asymptotic regime? \n\n**No** our theory can also describe arbitrary spectra and large but finite $N,P,D$. In fact, the most interesting application of the theory should hold for dimension free covariates ( where $\\sum_{k=1}^\\infty \\lambda_k < \\infty$ as $D \\to \\infty$ ). In this case, our theoretical predictions for the risk provide an accurate approximation of the average loss rather than an exact description of the dynamics. \n\nWe added a new Appendix section F.1 which studies the deterministic equivalent for general $D,P,N$ (all large but finite). We now use this formulation in the scaling law part of the theory. We also describe this in more detail before Result 7. \n\n#### Limitations of Current Architectures and Ablations\n\nSome reviewers pointed out that additional experiments on more realistic architectures (incluidng Transformers with nonlinear MLP layers) would be useful. \n\nIn response to this, we are running additional experiments varying the number of attention heads, adding or subtracting MLP blocks, and examining softmax vs linear attention. We have added some initial new experiments in Figure 10. \n\n#### Issues with writing and clarity\n\nWe have tried improving our mathematical notation and clarity throughout based on the feedback from the reviewers. With the additional page we have also expanded some of the explanation of certain results. \n\nFor example, we have \n\n1. Simplified result 1 and included the label noise term in that expression.\n2. Provided more text explaining the result in Section 4 and how it doesn't require a proportional scaling limit.  \n3. Added more motivation for the analysis in Appendix A in the new subsections titled **Intuitive Motivation for this Section** and **Dynamical Mean Field Theory Illustrative Example**.  \n4. Discussed in more detail the connection between general deep models and the looped setting we analyze mathematically for RRS covariates. \n5. Tried to make mathematical notation more consistent ($\\left< \\right>$ for averages, reserving the symbol $i$ for $\\sqrt{-1}$ instead of an index, etc.)."}}, "id": "30rZx72hqA", "forum": "qA42mWsnbl", "replyto": "qA42mWsnbl", "signatures": ["ICLR.cc/2026/Conference/Submission9983/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9983/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9983/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763677557152, "cdate": 1763677557152, "tmdate": 1763677557152, "mdate": 1763677557152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies deep linear self-attention trained on the in-context linear regression task, characterizing how performance depends on width, depth, number of training steps, batch size and data per context."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an interesting and relevant problem of training a deep linear self-attention model on in-context linear regression tasks, considering different in-context task structures. Extensive prior work has examined one-layer linear self-attention trained on in-context linear regression with isotropic task vectors. Analyzing deep models and non-isotropic task vectors represents meaningful progress."}, "weaknesses": {"value": "- My main concern is the looped transformer assumption, $W_i^l=W_i^{l'}, i\\in \\{k,q,v\\}$, which means that all layers have the same weights. The expressivity and optimization properties of a deep self-attention model can differ significantly with or without this assumption. Why would the scaling limits derived under this constrain reflect those of a real multi-layer attention model, which typically learns different weights across layers?\n\n- In the reduced model in Equation (4), all weight matrices within a layer appear to be merged into a single trainable matrix $\\Gamma$, effectively making the self-attention layer \"shallow\". Since gradient descent dynamics and the loss landscape are sensitive to such reparameterization, the true loss landscape probably differs from those shown in Figure 2 and Figure 4b. If this is indeed the case, it would be helpful to explicitly highlight this distinction.\n\n- I suggest that the authors perform another round of proofreading and polishing. There are presentation issues that make the paper unnecessarily difficult to read smoothly. I list some below.\n\n  It appears that manual vertical spacing commands have been used in several places of the paper. The formatting on page 14 seems irregular.\n\n  The clarity of Equation (3) could be improved by specifying the dimensionality of weight matrices. \n\n  The authors use inner-product and transpose notations interchangeably. The expectation notation $\\mathbb E(\\cdot)$ and the angle brackets notation $\\langle \\cdot \\rangle$ are also used interchangeably. Adopting consistent notations throughout would enhance readability.\n\n  The symbols $\\boldsymbol X, \\boldsymbol y$ in Equation (4) seem to be undefined.\n\n  The symbol $i$ is used inconsistently: sometimes as an index and sometimes as the imaginary number. In particular, it is undefined in Result 7. Clarifying its meaning in each context would avoid confusion.\n\n  In Equations (13), (15), it appears that a scalar is being added to a matrix, e.g., $(1-L^{-1}\\gamma\\Lambda), (i\\omega+\\Psi\\Lambda)$. Please check these terms for dimensional consistency. Additionally, the trace operator $\\text{tr}$ is used without parentheses, which could be ambiguous."}, "questions": {"value": "The problem considered in this paper is interesting and potentially important. However, recurring issues with presentation and clarity make it difficult to fully assess the contributions. Improving the clarity would make the results more accessible for proper evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wR2Gozthtc", "forum": "qA42mWsnbl", "replyto": "qA42mWsnbl", "signatures": ["ICLR.cc/2026/Conference/Submission9983/Reviewer_qeR8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9983/Reviewer_qeR8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833814253, "cdate": 1761833814253, "tmdate": 1762921416717, "mdate": 1762921416717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle the problem of how should we allocate depth, width, context length, and training compute when scaling transformers for in-context learning (ICL) on regression tasks. The paper analyzes a deep loop linear attention transformer trained (via SGD) to do linear regression in context, without finetuning at test time. It studies three task regimes: (i) isotropic data, (ii) fixed but structured covariance, and (iii) randomly rotated structured covariance (task distribution shifts every context), and derives dynamical equations and asymptotic scaling laws linking pretraining time, width, depth, and context length. The contribution lies in a unified, theoretically grounded scaling law that distinguishes the roles of depth, width, context length, and training time in ICL."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I appreciate that the paper gives a concrete, theoretically grounded answer to a question that is widely discussed in practice: how should depth vs. width vs. context length scale for in-context learning? Instead of treating “bigger model = better,” it isolates when depth specifically matters, and ties that to properties of the task distribution (shared vs. varying covariance across contexts). Specifically, the model decouples network width $N$ from the problem dimension $D$ when studying the scaling law, and the use of loop transformer ensures that the total number of parameters does not go up with the number of computes---which I believe provides a decoupled and generic test bench that adds to similar work studying scaling with linear model.\n\n2. A demonstration that the usefulness of depth is task-distribution dependent: If all tasks share the same covariance, depth is asymptotically unnecessary (long enough context suffices, as the model weights can \"encode\" the covariate information). If covariances vary across contexts, depth is fundamentally valuable, even with infinite context (reflecting the philosophy of test-time compute). \n\n3. A good match between theories and experiments."}, "weaknesses": {"value": "1. The entire analysis is built around linear regression tasks solved via in-context learning with (mostly) loop linear attention. I didn't find much discussion surrounding the use of loop attention block. One benefit I could imagine is the decoupling between total model weights and the depth. However, the use of loop could potentially restrict the model's expressiveness, where model could possibly implemented higher-order optimization algorithm, e.g., Newton's step rather than gradient descent [1], and the model might demonstrate different scaling behavior, especially in depth. \n\n2. The key conceptual result is that depth becomes essential when task covariances vary across contexts (“randomly rotated structured,” RRS). But the diversity they model is very specific: random orthogonal rotations of a shared spectrum. That’s mathematically nice but arguably still a stylized shift. Real heterogeneity looks more like mixture of domains, sparsity structure, nonstationary label noise, hierarchical latent factors, etc. It’s not obvious that random rotations is the right stand-in for natural distribution shift.\n\n3. The paper argues its results are relevant to large-scale LLM design, but the experiments cap out at synthetic regression and relatively small controlled transformers. There’s no ablation on modern-scale architectures (residual blocks with MLPs, nonlinear attention heads, long-context finetuning) to show even qualitative alignment. So the significance for frontier models is still somewhat speculative.\n\n[1] Giannou, A., Yang, L., Wang, T., Papailiopoulos, D., & Lee, J. D. (2024). How Well Can Transformers Emulate In-context Newton’s Method? arXiv preprint arXiv:2403.03183."}, "questions": {"value": "1. Your theory and experiments focus on linear regression tasks with (mostly) linear attention, Gaussian feature distributions, and controlled covariance structure. How confident should we be that the same depth–width scaling conclusions hold for nonlinear Transformers trained on natural language, vision, or multimodal data? \n\n2. You mostly analyze single-head linear attention plus residual depth. How do you expect multi-head structure and MLP blocks (i.e., actual transformer blocks) to affect the depth vs. width story? For example, could significantly more attention heads (possibly also scales with D) also benefit the learning process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PXkXZj12OH", "forum": "qA42mWsnbl", "replyto": "qA42mWsnbl", "signatures": ["ICLR.cc/2026/Conference/Submission9983/Reviewer_B4Ac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9983/Reviewer_B4Ac"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877056894, "cdate": 1761877056894, "tmdate": 1762921416451, "mdate": 1762921416451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies deep linear self-attention by analyzing the corresponding solvable model. The authors systematically investigate how depth of the model, width, context length, and training steps affect the solution of the model. Specifically, the authors focus on three distinct types of data (termed as ISO, FS, and RRS), where the authors reveal that depth is unnecessary for long contexts on ISO and FS, along with a series of other results that characterize the gradient flow dynamics. Furthermore, the authors derive a separable scaling law for the RSS setting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well written and well organized. The presentation is very clear and the flow of this paper is consistent, which I think can allow the readers to easily appreciate the core contributions of this work (the summarized theoretical results along with immediate numerical experimental results). \n\n   The proof is also easy to follow (though I did not check all the details): first transforming the gradient learning dynamics to an equivalent linear model, which has a simpler dynamics, then diving to this new dynamics under different conditions of the covariance. Although this general idea is not new, incorporating the depth to the analysis is novel.\n\n2. The derived theoretical results indeed consider attention in the multi-layer case, which I believe is an improvement over prior works. The authors demonstrate when and why the depth can be necessary. The summarized results are indeed novel and interesting."}, "weaknesses": {"value": "While the results are interesting by considering the depth, I think the setting is still not significantly novel compared to prior works. In particular:\n\n - While equation (3) indicates a dependence on the layer depth $l$, the induced parameter $\\Gamma$ in fact does not depend on $l$, because the matrices $W_i$'s are treated equally for different layers given one specific $i$. Instead, this dependence on $l$ is replaced by a simple summation over $l$ in $\\Gamma$. As a result, the corresponding analysis in fact does not provide significantly novel analysis compared to prior works in this line of research, i.e., studying the gradient flow dynamics of the induced parameter $\\Gamma$ (which is still a linear regression) as a proxy of the true weight matrices of the attention model. It remains unclear whether doing so can really capture the effects of depth.\n\n- The RSS is positioned as the most general case, but the randomly rotated and structured across contexts covariance still cannot effectively capture  the essence of task diversity. In addition, the theoretical framework is built on taking a very specific join limit where $P, K, B, D, N \\to \\infty$ with fixed ratios. While convenient for analytical tractability, this obscures efects that are relevant at finite scales. \n\n- Due to the aforementioned limitations, the generality of the derived results remains unclear.\n\n\nMinor: As this paper considers solvable models and scaling laws of attention, I think the related work [1], which also studies a solvable model for attention and its scaling laws, could be discussed a bit in the related work.\n\n[1]. Lyu et al.  A Solvable Attention for Neural Scaling Laws. ICLR 2025."}, "questions": {"value": "1. As I'm mostly concerned about the role of the depth, which plays an important role in the novelty of this work, can the authors justify the validity of assuming equal weight matrices across layers and the corresponding generality?\n\n2. Furthermore, can the authors discuss the difficulty brought by varying weight matrices w.r.t layers and how the current framework can still be applied in that case?\n\n3. Is taking the joint limit of $P, K, B, D, N$ necessary for the results presented in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1eU5fIM1gn", "forum": "qA42mWsnbl", "replyto": "qA42mWsnbl", "signatures": ["ICLR.cc/2026/Conference/Submission9983/Reviewer_KSDs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9983/Reviewer_KSDs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928786398, "cdate": 1761928786398, "tmdate": 1762921416051, "mdate": 1762921416051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}