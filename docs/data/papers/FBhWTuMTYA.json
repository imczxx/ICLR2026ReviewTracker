{"id": "FBhWTuMTYA", "number": 8656, "cdate": 1758093855863, "mdate": 1759897771500, "content": {"title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models", "abstract": "Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines. Code is available at https://anonymous.4open.science/r/VCRL-BD7E.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Launguage Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fd4f0d8c936690624efec187e73a87dd22e2232.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Variance-based Curriculum Reinforcement Learning (VCRL), a novel framework designed to improve the training efficiency of Large Language Models (LLMs) with policy-based reinforcement learning. The central claim is that existing rollout-based RL methods, like GRPO, are suboptimal because they do not consider the difficulty of training samples relative to the model's current capabilities. The authors propose using the variance of rewards from multiple rollouts of a single prompt as a dynamic measure of its difficulty. The core hypothesis is that samples with high variance are the most valuable for learning. The authors conduct experiments on several mathematical reasoning benchmarks with Qwen3 models, demonstrating that VCRL outperforms strong RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The concept of using group reward variance as a dynamic difficulty metric is intuitive. It provides a way to implement curriculum learning without needing pre-defined difficulty labels.\n2. VCRL shows consistent state-of-the-art performance across five different mathematical reasoning benchmarks and two model sizes, outperforming strong baselines like GRPO, DAPO, and GSPO.\n3. The paper provides a theoretical argument that VCRL's training should be more stable than GRPO's from a policy gradient norm perspective."}, "weaknesses": {"value": "1. While the method is framed as \"curriculum learning,\" the paper does not provide any direct visualization of this curriculum.\n2. All experiments are conducted on mathematical reasoning tasks, which typically have a binary reward structure."}, "questions": {"value": "1. To strengthen the \"curriculum\" claim, could you provide a visualization showing how the curriculum in the training batch evolves?\n2. The paper's introduction claims that existing methods \"fail to explicitly consider LLMs' learning ability for samples of different difficulty levels\". Could you provide more direct evidence for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LfJiWYJBft", "forum": "FBhWTuMTYA", "replyto": "FBhWTuMTYA", "signatures": ["ICLR.cc/2026/Conference/Submission8656/Reviewer_CFiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8656/Reviewer_CFiz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667745046, "cdate": 1761667745046, "tmdate": 1762920476702, "mdate": 1762920476702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a variance-based curriculum reinforcement learning method to order the samples for training the mathematical reasoning LLMs. The method measures the difficulty of samples based on the group reward variance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The writing is easy to follow;\n- The experiments look intensive;"}, "weaknesses": {"value": "- Curriculum RL (CRL) has been extensively studied [1], but the paper does not discuss the literature;\n- Due to the lack of CRL literature, the baseline does not include any CRL method for comparison;\n- Aware of the CRL literature, the proposed variance-based curriculum RL is not novel or interesting;\n- The abstract includes too many acronyms. \n\n[1] Portelas, Rémy, et al. \"Automatic curriculum learning for deep rl: A short survey.\" arXiv preprint arXiv:2003.04664 (2020)."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sio5xkGPUQ", "forum": "FBhWTuMTYA", "replyto": "FBhWTuMTYA", "signatures": ["ICLR.cc/2026/Conference/Submission8656/Reviewer_pPKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8656/Reviewer_pPKY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675492800, "cdate": 1761675492800, "tmdate": 1762920476308, "mdate": 1762920476308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at improving the mathematical reasoning ability of LLMs with a curriculum learning framework. The proposed method is based on a simple but interesting observation: under the sparse reward setting, the agent's performance preserve low variance on both easy and hard tasks, while high with medium difficulty ones. Hence, the appropriate difficulty of experiences are determined as those with high group reward variance (Variance-based Dynamic Sampling). This work also proposes to use the high-variance samples to compose the experience replay (Replay Learning). The experiments are relatively extensive, conducted on five mathematical benchmarks with two variations of *Qwen3* as the base models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea is conceptually simple but useful. The description of the method is for the most parts clear and the motivation is well explained. The empirical results on mathematical problem solving are promising. Compared to the reviewed related works, the variance-based criterion is simpler than those that requires an extra learnable module, which is easier to implement and potentially generalize to other tasks."}, "weaknesses": {"value": "1. The work does not compare to other curriculum learning methodologies (e.g., Those mentioned in the related works: Hammoud et al. 2025, Feng et al. 2025a, etc.).\n   \n2. There is a mismatch between the claimed contributions and experimental setting. At the end of the introduction, the claim of contribution is quite general, indicating that the framework can be used for general RL tasks. However, the previous introduction and latter experiments both only focus on mathematical reasoning with LLMs. If the framework is claimed to be generalizable, other experiments should be conducted; Otherwise, the extension to other tasks should be held for merely a discussion, and the scope in the claim of contribution should be narrowed down."}, "questions": {"value": "* **TECHNICAL QUESTIONS:**\n\n1. The proposed framework is rather generalizable, with more potential with other tasks than merely mathematics. Do the authors have any insight or further results on other language tasks?\n     \n2. Is it possible to evaluate the token efficiency with the proposed method? If I understand correctly, after using the Replay Learning scheme proposed in this work, the RL can be more efficient. It would be interesting to evaluate that.\n\n* **PRESENTATION SUGGESTIONS:**\n\n1. Line 17, the acronym 'RLVR' was used before properly introducing the full term.\n    \n 2. Line 98, A verb is missing here. For example, 'a value model *estimated/calculated* by GAE' is better.\n     \n3. Line 132: 'clipping range' is better as 'clipping bound'?\n     \n4. If I understand correctly, the two points made in section 3.1 are conceptually the same thing: without curriculum learning, the current model parameter mismatches current training sample difficulty, rendering efficient learning difficult. The motivation to introduce curriculum learning is good and well-delivered from the text, but the writing can be more concrete.\n     \n5. The description around equation 7 can be clearer. Specifically, it is better to mention that the difference of the probabilities are $\\\\epsilon-$bounded, because $\\\\epsilon$ may also indicate a random noise or other meanings.\n     \n6. The authors should give a formality description of what a group of training samples that pertains to the same query is mathematically denoted.\n     \n7. Line 216: it is better to rephrase as 'the estimator achieves the maximum value of'\n     \n8. In Equation 12, should the $\\\\sigma\\_{i,max}²$ be $\\\\sigma\\_{max}²$, because it is the group-wise maximum of variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2HLWKMiI1y", "forum": "FBhWTuMTYA", "replyto": "FBhWTuMTYA", "signatures": ["ICLR.cc/2026/Conference/Submission8656/Reviewer_jpnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8656/Reviewer_jpnt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009356988, "cdate": 1762009356988, "tmdate": 1762920475871, "mdate": 1762920475871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a curriculum strategy for reinforcement learning that prioritizes training examples based on how inconsistent the model’s rewards are across multiple rollouts of the same sample. High-variance problems are treated as most informative because the model sometimes succeeds and sometimes fails on them, indicating useful learning signal. The method filters for such problems at each step, supplements batches with a replay buffer that stores previously identified high-value items, and keeps the underlying GRPO-style optimization unchanged. Experiments on several math reasoning benchmarks with Qwen3 models demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The studied problem is important. This paper is well-motivated.\n\n2. The theoretical analysis and gradient-norm plots suggest the proposed method has smoother updates than GRPO are provided.\n\n3. Different LLM RL methods are included in the experiments, showing the generalizability of the proposed method."}, "weaknesses": {"value": "1. The biggest problem is that this paper does not compare against existing curriculum-learning methods for (LLM) reinforcement learning, despite a growing body of existing works [1,2,3,4,5,6]. The reported baselines are non-curriculum algorithms (e.g., vanilla GRPO), which makes it hard to compare the proposed variance-based curriculum signal and other signals. In addition, the related-work section does not adequately position the method within prior curriculum strategies on LLM RL. A better evaluation should includes more existing RL curriculum learning methods. It is also suggested to expand the related-work discussion to clarify what is new versus known in curriculum design for (LLM) RL. The high level idea that uses a signal to represent the learning difficulties and design the curriculum based on the difficulties is not new and are widely used in the listed existing works. It might need more discussion to justify the novelty and the superiority of the variance based signals. What is the fundamental differences and advantages comparing to these existing works? \n\n2. The experiments use only Qwen models on math reasoning benchmarks. Although Qwen is widely-used open-sourced model family, recent works point out that it may have exposure to some math benchmark data [7], which could influence results. Please broaden coverage to additional domains (e.g., coding) and include more model families to make the empirical results more comprehensive.\n\n3. The method appears to discard some rolled-out samples and then fetch replacements from a memory bank, which likely increases per-step compute and latency relative to GRPO. The paper does not quantify this cost. Please report wall-clock time to a target accuracy comparing to baselines, the ratio of discarded to reused rollouts, sensitivity to the number of rollouts and to the difficulty threshold, and a breakdown of memory-bank overhead. Efficiency claims should be supported under the total compute/time cost, rather than only step numbers or fixed-step comparisons.\n\n\n[1] Zhang et al., Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation. EMNLP 2025.\n\n[2] Tzannetos et al., Proximal Curriculum for Reinforcement Learning Agents\n\n[3] Parashar et al., Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning\n\n[4] Chen et al., Self-Evolving Curriculum for LLM Reasoning\n\n[5] Wang et al., DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training\n\n[6] Bae et al., Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning\n\n[7] Wu et al., Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LMeD0dLUHb", "forum": "FBhWTuMTYA", "replyto": "FBhWTuMTYA", "signatures": ["ICLR.cc/2026/Conference/Submission8656/Reviewer_6HFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8656/Reviewer_6HFJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060869554, "cdate": 1762060869554, "tmdate": 1762920475415, "mdate": 1762920475415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}