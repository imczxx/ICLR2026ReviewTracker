{"id": "SjBgxTsQYu", "number": 11064, "cdate": 1758188430891, "mdate": 1762923179319, "content": {"title": "Context-Aware Alignment: Adapting Large Language Models to Individual Historical Data", "abstract": "Aligning large language models with human preferences is essential for ensuring their effectiveness, utility, and safety in real-world applications. While much of the current research focuses on aligning LLMs with generalized human values such as fairness, transparency, and ethical behavior, limited attention has been given to aligning LLMs with the preferences and characteristics of individual users. In this paper, we propose a novel approach that leverages individual historical context to achieve personalized alignment, adapting LLMs to align with the unique traits and preferences of specific users. Our method focuses on extracting persona-related representations—abstract features encapsulating conversational style, tone, and preferences—from past user interactions. These representations guide the model in generating responses tailored to the user's individual characteristics. Experimental results demonstrate that our approach significantly outperforms existing baselines, improving the model's ability to reflect individual personas while maintaining contextual appropriateness. This research opens new possibilities for more personalized, context-aware, and user-centric applications of LLMs.", "tldr": "", "keywords": ["Individual Alignment", "Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/948317364a2d3eceea8be26da9a9250cadc0c799.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DPORE (Direct Preference Optimization with Representation Regularization), a novel approach for individual alignment of large language models (LLMs) that adapts models to user-specific preferences using historical conversational data. The core idea is to extract persona embeddings from past user interactions through a contrastive learning framework, ensuring embeddings capture abstract stylistic, tonal, and preference-related traits. These user representations are then integrated into DPO training via a representation regularization term, which encourages alignment between generated responses and user embeddings for preferred outputs while penalizing similarity for rejected ones. Experiments using the ALOE benchmark (3,000 multi-turn dialogues across 3,310 personas) demonstrate that DPORE significantly outperforms baselines (SFT, DPO) in individual alignment while maintaining comparable general-purpose performance on AlpacaEval and MT-Bench. A case study further shows that DPORE produces richer, persona-consistent dialogue compared to DPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper attempts to solve a relevant gap between general and individual alignment. Loss formulations and embedding pipeline are well defined. \n2. The paper highlights the importance of integrating user context into preference learning.\n3. Reproducibility details and open-source commitment are commendabl"}, "weaknesses": {"value": "1. All experiments rely on the ALOE benchmark, a fully synthetic dataset where both user personas and dialogues are generated by GPT models. While this setting allows controlled testing, it is not a realistic proxy for human personalization. Synthetic personas are typically coherent and idealized, lacking noise, ambiguity, or the inconsistencies that characterize real user histories. Thus, it is unclear whether DPORE would remain stable or meaningful on authentic human conversation logs, where linguistic variation and sparse feedback are dominant.\n2. The contrastive embedding model is central to DPORE, but the paper provides no analysis of what these embeddings represent. There are no visualizations, clustering analyses, or qualitative examples demonstrating that they meaningfully separate users by personality, style, or topic preferences. Without interpretability studies, it remains unclear whether the embeddings encode genuine persona features or simply artifacts of text style.\n3. The paper mentions human evaluation but provides no information about how it was conducted. How were evaluators recruited (e.g., expert annotators, crowd workers, internal team)? How many annotators participated, and were there any quality-control measures (inter-rater agreement, majority voting)? What evaluation questions or rubrics were used (e.g., were raters judging helpfulness, persona consistency, or both)? Without this information, it is impossible to assess the reliability, statistical validity, or replicability of the human results. This omission significantly weakens the empirical section.\n4. GPT-4 is used to judge the outputs, but GPT-generated data were also used to train the synthetic personas and possibly influence the embedding extractor. This creates evaluation bias: models that align more closely with GPT’s writing style may appear better according to GPT’s own preferences. It's OK to use LLM as a judge, but it's not OK to have no human evaluation on human-LLM agreement. \n5. The entire study uses only one model architecture (LLaMA-3 8B) for both training and evaluation. This makes it unclear whether the approach is model-agnostic or if the reported gains depend on a specific model’s embedding space and behavior. Testing across at least two different base models (e.g., Qwen, Mistral, Phi) would have made the claims more convincing."}, "questions": {"value": "1. How exactly were human evaluators recruited, trained, and instructed? Please clarify the evaluation protocol and criteria.\n2. How does DPORE perform when applied to a different base model?\n3. Can the method generalize to new users with very limited history (few-shot scenarios)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i8cqe025t4", "forum": "SjBgxTsQYu", "replyto": "SjBgxTsQYu", "signatures": ["ICLR.cc/2026/Conference/Submission11064/Reviewer_B1Pe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11064/Reviewer_B1Pe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761173929286, "cdate": 1761173929286, "tmdate": 1762922241278, "mdate": 1762922241278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely appreciate the time and effort you have dedicated to reviewing our submission. Your valuable feedback has provided us with significant insights, which will be invaluable as we continue to refine our work.\n\nAfter careful consideration, we have decided to withdraw our paper from the conference. We believe this decision will allow us to further improve the quality of our research before presenting it to the community.\n\nThank you once again for your understanding and support."}}, "id": "ERvCqF9AjR", "forum": "SjBgxTsQYu", "replyto": "SjBgxTsQYu", "signatures": ["ICLR.cc/2026/Conference/Submission11064/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11064/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762923177967, "cdate": 1762923177967, "tmdate": 1762923177967, "mdate": 1762923177967, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an important yet underexplored direction in LLM alignment: personalization to individual user preferences rather than only general human values. The authors propose a method that learns persona-aware representations from users’ historical interactions and uses them to guide response generation, achieving stronger individual alignment without compromising contextual coherence. Experiments on standard benchmarks show clear improvements over existing baselines, suggesting promising avenues for user-centric LLM deployment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach is technically sound and presents a valid strategy for personalized alignment by integration of persona-aware signals into the DPO framework\n2. The paper is well-written, clearly structured, and easy to follow."}, "weaknesses": {"value": "1.\tThe abstract is unclear and provides only a high-level, cursory description of the method—particularly regarding the extraction of persona-related representations. A more precise and informative summary would better convey the technical contribution.\n2.\tEvaluation is limited to the ALOE benchmark with only 100 samples, which is insufficient to assess the generalizability of the approach. The authors should include additional personalized alignment benchmarks (e.g., LongLaMP [1], PERSONA [2], PREFEVAL [3], or others) to demonstrate robustness across diverse settings.\n3.\tKey implementation details are missing: the architecture and hyperparameters of the encoder G are not specified, and equation numbering appears to be inconsistent (e.g., equations after Eq. 1 are unnumbered).\n4.\tThe core idea—injecting user-specific representations into DPO via a regularizer—is relatively straightforward and closely follows existing alignment frameworks. Given the limited experiments, the novelty and contribution appear modest."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tAKrSMLtcb", "forum": "SjBgxTsQYu", "replyto": "SjBgxTsQYu", "signatures": ["ICLR.cc/2026/Conference/Submission11064/Reviewer_m543"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11064/Reviewer_m543"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974864482, "cdate": 1761974864482, "tmdate": 1762922240285, "mdate": 1762922240285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DPORE, a method for context-aware personalization of large language models that extends DPO by incorporating persona embeddings learned from historical conversations. These embeddings serve as stylistic and preference representations that guide alignment through a representation-regularized loss. The paper presents a clear motivation and shows moderate improvements on the ALOE benchmark with a narrow evaluation. However, the paper lacks critical baselines and generalization tests, which weakens the paper’s empirical foundation. While the idea of embedding-based personalization is promising, the evidence does not convincingly demonstrate that DPORE provides substantial or generalizable gains over simpler alternatives. As such, the broader claims of effective and generalizable personalization are not substantiated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important and emerging problem: scalable, individualized adaptation of LLMs.\n- Methodologically sound extension of DPO with representation regularization.\n- Results suggest some improvements wrt persona consistency and stylistic coherence in multi-turn dialogue."}, "weaknesses": {"value": "- Limited evaluation. The model is tested on a single synthetic dataset with only 100 evaluation cases, offering a weak empirical basis for general conclusions about personalization performance.\n- Missing baselines. The paper omits an in-context personalization baseline (e.g., adding user profile information to the prompt) which is essential for judging whether fine-tuning offers meaningful improvement beyond prompting.\n- Lack of generalization analysis. The paper does not examine how a persona-tuned model generalizes to unseen personas or to users without profiles, leaving unclear whether DPORE overfits to training identities or degrades in default use."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dm76G2e0yq", "forum": "SjBgxTsQYu", "replyto": "SjBgxTsQYu", "signatures": ["ICLR.cc/2026/Conference/Submission11064/Reviewer_kBmr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11064/Reviewer_kBmr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102140687, "cdate": 1762102140687, "tmdate": 1762922239503, "mdate": 1762922239503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses individual alignment, where user-specific preferences may differ from the general preferences learned through RLHF. To achieve this, the authors propose a user embedding learning step using a contrastive learning objective, encouraging dialogue excerpts from the same persona to be closer in representation space and farther apart otherwise. They further incorporate a persona-based regularization term into the DPO objective, based on the similarity between response representation and the learnt user embeddings. Experiments are conducted on the ALOE benchmark, comparing against base models, SFT on preferred responses, and vanilla DPO."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important and underexplored problem—aligning LLMs to individual rather than collective preferences, which is a known limitation of current alignment frameworks such as DPO.\n\nThe proposed method of learning user embeddings and integrating them as a regularization term is conceptually simple, intuitive, and shows improved results over vanilla DPO without degrading general language understanding or reasoning performance.\n\nThe paper is clearly written and easy to follow."}, "weaknesses": {"value": "The proposed persona-based regularization lacks theoretical grounding. There is no analysis explaining why this specific regularization form benefits DPO the most or how it affects its optimality. The design choices are intuitive but unsupported by deeper reasoning or ablations.\n\nThe method assumes user identity/persona labels are available for all dialogue excerpts to train the contrastive objective. In real-world scenarios, such annotations are rarely accessible. It is also unclear whether the learned persona embeddings generalize to unseen users at inference time, as no cross-user generalization experiments are provided.\n\nBoth the contrastive learning and evaluation are performed on the ALOE dataset, with overlapping persona pools. This raises a concern about data leakage. If the same personas appear in both stages, the model’s apparent ability to “recover” personas may simply reflect memorization.\n\nThe experiments only include basic ablations (Base, SFT, and vanilla DPO). The authors do not follow the full evaluation setup of ALOE, omitting alignment-level metrics and normalized IR scores. The regularization should, in principle, be applicable to other RLHF methods (e.g., PPO), yet no such experiments are presented to demonstrate generality.\n\nThe performance gain on AlpacaEval requires more explanation. Case studies suggest that DPORE tends to generate longer responses to appear more “personal,” which could inflate scores due to length bias rather than genuine personalization."}, "questions": {"value": "What data is used for DPO training in DPORE? Are the training data and persona pools in the contrastive learning stage the same as those used in evaluation?\n\nCould you provide the original evaluation metrics from ALOE for comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cM6SeUKuFb", "forum": "SjBgxTsQYu", "replyto": "SjBgxTsQYu", "signatures": ["ICLR.cc/2026/Conference/Submission11064/Reviewer_G9oY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11064/Reviewer_G9oY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762598314714, "cdate": 1762598314714, "tmdate": 1762922238501, "mdate": 1762922238501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}