{"id": "LqrWNdceum", "number": 14247, "cdate": 1758231150335, "mdate": 1763752529309, "content": {"title": "EMBridge: Enhancing Gesture Generalization from EMG Signals Through Cross-modal Representation Learning", "abstract": "Hand gesture classification using high-quality structured data such as videos, images, and hand skeletons is a well-explored problem in computer vision. Alternatively, leveraging low-power, cost-effective bio-signals, e.g. surface electromyography (sEMG), allows for continuous gesture prediction on wearable devices. In this work, we aim to enhance EMG representation quality by aligning it with embeddings obtained from structured, high-quality modalities that provide richer semantic guidance, ultimately enabling zero-shot gesture generalization. Specifically, we propose EMBridge, a cross-modal representation learning framework that bridges the modality gap between EMG and pose. EMBridge learns high-quality EMG representations by introducing a Querying Transformer (Q-Former), a masked pose reconstruction loss, and a community-aware soft contrastive learning objective that aligns the relative geometry of the embedding spaces. We evaluate EMBridge on both in-distribution and unseen gesture classification tasks and demonstrate consistent performance gains over all baselines. To the best of our knowledge, EMBridge is the first cross-modal representation learning framework to achieve zero-shot gesture classification from wearable EMG signals, showing potential toward real-world gesture recognition on wearable devices.", "tldr": "We proposed a cross-modal representation learning framework, EMBridge, to align EMG representations with more structured Pose representations. EMBridge enables zero-shot classification on unseen gestures and achieved superior generalization..", "keywords": ["EMG", "Zero-shot Gesture Classification", "Cross-modal", "Representation Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd2140ba036411543fddfef186b06a5e7d81582b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a cross-modal framework that aligns EMG signals with hand pose embeddings to enable zero-shot gesture recognition. The approach is technically solid and yields consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The proposed solution represents a well executed and effective integration of recent state-of-the-art multimodal representation learning approaches, such as BLIP-style Q-Formers, into the EMG–pose alignment setting. The framework adapts these techniques in a way that is coherent and practically meaningful for wearable gesture recognition.\n2.The experimental results are comprehensive and show consistent improvements across multiple datasets and evaluation protocols, including both in-distribution and zero-shot gesture classification, demonstrating the practical effectiveness of the proposed method."}, "weaknesses": {"value": "1.While the proposed Community-Aware Soft Contrastive Learning objective introduces a soft alignment mechanism, the community structure itself is derived from a hard K-means clustering, which inherently imposes discrete partitions on the pose embedding space. This feels contradicting the goal of modeling continuous semantic similarity. Should a probabilistic model such as a Gaussian Mixture or kernel-based affinity estimation better capture soft neighborhood structures?\n2.The construction of communities purely from geometric proximity in the pose latent space overlooks available semantic priors about gesture types. Integrating gesture semantics (e.g., coarse action categories) or using supervised/semi-supervised clustering could produce more meaningful communities and stronger cross-modal supervision.\n3.While the paper is well structured and the experimental settings are clearly described, the background context for this specific application domain remains somewhat underdeveloped. In particular, the work would benefit from a clearer definition and motivation of the pose modality (e.g., what exactly constitutes a “pose” sample, how gestures are defined, and what practical application scenarios are targeted). For such a specialized domain, providing stronger domain background and contextual grounding would make the contribution easier to interpret and appreciate."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hnDwKefp4G", "forum": "LqrWNdceum", "replyto": "LqrWNdceum", "signatures": ["ICLR.cc/2026/Conference/Submission14247/Reviewer_fYtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14247/Reviewer_fYtB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760715335453, "cdate": 1760715335453, "tmdate": 1762924701640, "mdate": 1762924701640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the task of hand gesture recognition. The authors propose a cross-modal representation learning framework that bridges the semantic gap between EMG representations and poses to achieve zero-shot gesture generalization. Specifically, they introduce a Q-Former, a masked pose reconstruction loss, and a community-aware soft contrastive learning objective to enhance EMG representation learning. Extensive experiments are conducted to validate the effectiveness of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed cross-modal representation learning method for hand gesture recognition is a novel and interesting attempt.\n\n2. The proposed cross-modal representation learning strategy effectively improves the quality of EMG representations.\n\n3. The authors proposed a community-level structural similarity framework for soft contrastive learning.\n\n4. The authors performed zero-shot classification on both in-distribution and unseen gesture categories."}, "weaknesses": {"value": "1. The proposed community-aware soft contrastive learning mainly stems from previous work on contrastive learning. However, the motivation for introducing community-level structural similarities is not well explained.\n\n2. The proposed method is only validated on the pose–EMG paired data. Could the proposed method also be applied to RGB–EMG data?"}, "questions": {"value": "1. In Appendix A.6, the authors claim that batch size does not affect the proposed EMBridge, while it degrades the performance of CPEP and Q-Former. Do these conclusions come from experimental results?\n\n2. The evaluation is conducted on the emg2pose and NinaPro datasets. Only four gestures are used in the unseen setting, would this setup reduce the difficulty of the zero-shot evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WWCItfdyfU", "forum": "LqrWNdceum", "replyto": "LqrWNdceum", "signatures": ["ICLR.cc/2026/Conference/Submission14247/Reviewer_jAkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14247/Reviewer_jAkZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761037342126, "cdate": 1761037342126, "tmdate": 1762924701198, "mdate": 1762924701198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on gesture generalization from EMG signals and pose. The authors propose EMBridge, a cross-modal representation learning framework consisting of three components: (1) a querying transformer that extracts pose-informative features from EMG signals and aligns them with pose features using an asymmetric setup, (2) a masked pose reconstruction loss to enrich the pose representations, and (3) a community-aware soft contrastive learning objective to account for varying pose similarities that are not captured by standard contrastive learning. The authors conduct a detailed ablation study on different components of the framework, evaluate performance on both in-distribution and unseen gesture classification tasks, and demonstrate improvements over baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, with clear motivation and challenges. It addresses an important and interesting problem in EMG-based gesture generalization to achieve zero-shot gesture classification. The proposed framework is original, well thought out, and carefully designed to meet domain-specific needs, such as incorporating pose communities for community-aware soft contrastive learning, which is particularly interesting. The experimental section is detailed and includes ablation studies on different components of the framework, along with analyses on three EMG datasets."}, "weaknesses": {"value": "I do not see any major weaknesses in this work. However, a more detailed discussion of the limitations and potential future directions would strengthen the paper. While reproducibility details are sufficient, sharing the code in the future would further benefit the community."}, "questions": {"value": "- What is the motivation behind using an asymmetric setup with the pose estimator and EMG encoder? Have the authors experimented with a setup where the pose estimator is also trained during the contrastive learning process (in the Query Transformer)?\n\n- Regarding the relationship between paired gestures in experiments, are they independent or combinatorial? If they are combinatorial (for example, open hand + down), is this relationship captured in the pose similarity matrix, or could it be included as prior knowledge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TBRwfYbZSV", "forum": "LqrWNdceum", "replyto": "LqrWNdceum", "signatures": ["ICLR.cc/2026/Conference/Submission14247/Reviewer_VnDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14247/Reviewer_VnDF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937266970, "cdate": 1761937266970, "tmdate": 1762924700670, "mdate": 1762924700670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}