{"id": "0TPKIbihMF", "number": 15672, "cdate": 1758253728352, "mdate": 1759897289921, "content": {"title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "abstract": "As AI tools become increasingly common for research ideation, robust evaluation is critical to ensure the validity and usefulness of generated ideas. We introduce ScholarEval, a retrieval-augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness—the empirical validity of proposed methods based on existing literature, and contribution—the degree of advancement made by the idea across different dimensions relative to prior research. To evaluate ScholarEval, we introduce ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas and reviews, comprising 117 ideas across four disciplines: artificial intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows that ScholarEval achieves significantly higher coverage of points mentioned in the human expert annotated rubrics in ScholarIdeas compared to all baselines. Furthermore, ScholarEval is consistently preferred over our strongest baseline o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI, in terms of evaluation actionability, depth, and evidence support. Our large-scale user study also shows that ScholarEval significantly outperforms deep research in literature engagement, idea refinement, and usefulness. We will openly release our code, dataset, and ScholarEval tool for the community to use and build on.", "tldr": "", "keywords": ["research idea evaluation", "literature-grounded", "multi-domain", "ai4science"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d209107acc86de56311c706b30a0ad7f092bc876.pdf", "supplementary_material": "/attachment/abb63d654357f69cde04a8cc792cf25d4c407a92.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a retrieval-augmented evaluation framework named SCHOLAREVAL to evaluate the research ideas by the soundness and contribution, and an expert-verified dataset named SCHOLARIDEAS of 117 research ideas and 1076 review rubrics from four disciplines. The framework adopts a multi-stage pipeline, starting with finding relevant papers of a research idea and synthesizing the existing findings, then measuring the soundness and contribution by evaluating the method components in existing work and comparing the contribution to related work. The experiments show that the proposed framework evaluates the ideas more similarly to human reviews on the proposed dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper contributes a framework to evaluate research ideas and a dataset of ideas and reviews verified by human experts. The framework evaluates the ideas from two aspects and also provides feedback. It shows superiority in multiple aspects over several baselines.\n\n2. The dataset includes human reviews collected from public platforms and covers four disciplines: artificial intelligence, neuroscience, biochemistry, and ecology. \n\n3. The experiments evaluate the proposed framework using various LLMs to show its effectiveness across LLMs. The evaluation measures on various metrics and includes human verification."}, "weaknesses": {"value": "1. The evaluation of soundness and contribution is dependent on the performance of LLMs in retrieving and comparing with prior work, which might be tied to LLM capacity and undermine the evaluation of soundness for ideas of new tasks or novel evaluation.\n\n2. The soundness evaluation works by extracting the method components and evaluating their effectiveness in prior work. This module seems to overlook the connections or interplay between the method components or the significance of model components in different contexts or assumptions.\n\n3. It's unclear if unsuccessful ideas are included in the proposed datasets in Sec 3.1 and whether the framework is capable of distinguishing them from successful ones."}, "questions": {"value": "1. How to ensure an exhaustive search of relevant literature in the framework? As AI papers might be open-sourced largely, does it apply to other disciplines to retrieve papers?\n\n2. How is the efficiency of the framework to evaluate a research idea?\n\n3. The standard deviation of coverage for the proposed method in Table 1 seems to be larger than the baselines across disciplines and variants. Does that indicate an unstable performance of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rLmoCRlLsY", "forum": "0TPKIbihMF", "replyto": "0TPKIbihMF", "signatures": ["ICLR.cc/2026/Conference/Submission15672/Reviewer_btKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15672/Reviewer_btKk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506629037, "cdate": 1761506629037, "tmdate": 1762925926677, "mdate": 1762925926677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ScholarEval, a retrieval-augmented generation (RAG) framework designed to automatically evaluate research ideas. The framework divides evaluation into two dimensions: \"Soundness\" (the empirical validity of the method) and \"Contribution\" (progress relative to previous research). To evaluate the system, the authors constructed a new expert-annotated dataset, ScholarIdeas, which contains 117 research ideas (extracted from published papers) from 4 disciplines (including AI and life sciences) and their corresponding review rubrics.\n\nThe main advantages and contributions of the paper are claimed to lie in: (1) proposing a multi-stage evaluation framework that can provide dense and actionable feedback; (2) constructing a new evaluation benchmark, ScholarIdeas; (3) experiments show that ScholarEval significantly outperforms baselines including general LLMs and 04-mini-deep-research in both automatic metrics (covering expert review points) and human evaluation (User Research)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem addressed by the paper—automatically evaluating the quality of research ideas—is a core challenge in the field of AI scientific discovery and holds high value.\n2. The paper focuses on generating dense, literature-supported, actionable feedback rather than just a single assessment score. This focus on actionability is commendable.\n3. Section 5 of the paper, \"Expert User Research,\" is its most rigorous part. Different from the main body of the paper (which is based on ScholarIdeas evaluation), this study adopted the correct methodology (real experts evaluating fundamental pre-execution ideas) and demonstrated that the system outperforms a key baseline in practical applications."}, "weaknesses": {"value": "- The crisis of effectiveness in the core dataset (ScholarIdeas).\n    - Task misalignment: The paper claims to evaluate pre-execution creativity, but the dataset was extracted ex post facto from already completed papers. This represents a fundamental task misalignment.\n    - Narrow evaluation scope: The automatic evaluation of the paper relies entirely on ScholarIdeas. The lack of testing on other real-world data (such as ACL 2025, NeurIPS 2025, etc.) makes it impossible to prove the generalization ability of this framework.\n    - Timeliness issue: How can we ensure that this data will still be valid next year? Also, isn't the evaluation of 117 ideas too few?\n- The baseline comparison is weak and misleading.\n    - Baseline is seriously insufficient: The experimental comparison in the paper is seriously inadequate. It avoids comparison with real academic competitors (i.e., other published creativity assessment/novelty assessment tools).\n    - Omission of key literature: The paper completely overlooks directly relevant contemporary frameworks such as DeepReview (with its 'deep thinking process') or THE-Tree ('historical evolution'), which are designed to deepen evaluation and reasoning.\n    - \"Straw Man\" Argument: The paper chooses to compare with a general LLM and a commercial Black box system (04-mini-deep-research). This is a \"straw man\" style of argument and does not prove its superiority over other research work in the field.\n- Inherent bias against breakthrough innovation: This framework is entirely \"literature-based,\" which means it is methodologically unable to fairly evaluate truly novel, interdisciplinary, or paradigm-challenging ideas that lack literature precedents.\n- Invalid and contradictory evaluation protocols.\n    - **Automated evaluation of the loop**: The primary metric \"Coverage\" measures the degree of match between the system (an LLM pipeline) and the \"gold standard\" (a benchmark extracted by another LLM), rather than aligning with the true human ability to evaluate creativity.\n    - **Unreliable LLM-as-judge**: The paper uses LLM-as-judge (Figure 4) to claim its superiority. However, the validation conducted by the authors themselves in the appendix (Table 6) shows that the agreement between LLM-judge and human annotators is extremely low (e.g., Cohen's Kappa for the \"Depth\" dimension is -0.0161), rendering its conclusion invalid."}, "questions": {"value": "1. Regarding the dataset and evaluation scope:\n    * Can the author explain the rationale for retrospectively extracting \"ideas\" from published papers to construct ScholarIdeas? This seems to be contrary to the goal of evaluating \"pre-execution\" ideas.\n    * Why is the automatic evaluation limited to the ScholarIdeas dataset? Have the authors considered testing on other contemporary, real-world \"testbeds\" (such as data from ACL 2025, NeurIPS 2025) to verify the generalization ability of the framework?\n2. Regarding the baseline comparison: Could the author explain why all relevant academic baselines (such as DeepReview, THE-Tree, etc.) were omitted from the experimental comparison? Without such a comparison, how can the superiority of ScholarEval over the domain SOTA be verified?\n3. Regarding the validity of the evaluation protocol: Given that the appendix (Table 6) shows extremely low agreement (Kappa value close to 0) between LLM-as-judge and human evaluators, why do the authors still use it as the core evidence to support the superiority of the system in Figure 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c2a0v1iOlx", "forum": "0TPKIbihMF", "replyto": "0TPKIbihMF", "signatures": ["ICLR.cc/2026/Conference/Submission15672/Reviewer_uuD6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15672/Reviewer_uuD6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895757935, "cdate": 1761895757935, "tmdate": 1762925926345, "mdate": 1762925926345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SCHOLAREVAL, a retrieval-augmented framework designed to evaluate the quality of research ideas. The proposed framework consists of two modules to assess ideas: a soundness module for the empirical validity and a contribution module for novelty. Each module synthesizes the review following a retrieval-augmented way. To validate the system, the authors create a new dataset of 117 expert-annotated research ideas and detailed review rubrics from OpenReview and eLife. Experiments show that SCHOLAREVAL outperforms LLM baselines. An expert study is done to highlight its strength."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The paper is well written. The structure of the paper is well organized.\nS2: The paper contributes a new dataset SCHOLARIDEAS for evaluation of the idea evaluation works.\nS3: The proposed framework SCHOLAREVAL outperforms the baselines. An expert study is also conducted and shows a preference for SCHOLAREVAL on all measured dimensions."}, "weaknesses": {"value": "W1: The construction of the dataset needs more consideration. The research ideas used for evaluation are extracted from published papers by LLMs, not preliminary ideas before execution. The extraction is made by removing execution sections of papers to simulate research ideas at an early stage. The gap between the actual ideas and the extracted ones cannot be ignored.\nW2: The paper highly relies on LLMs providing evaluation. More is expected on validation of the LLM-based metrics.\nW3: The compared baselines are weak. The proposed framework is compared against multiple general LLMs and only one deep research system (o4-mini-deep-research). The Related Work section mentions several papers specifically designed for research idea evaluation but they are not used in the experiments. This makes the experimental results less convincible."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BJvYacvqVl", "forum": "0TPKIbihMF", "replyto": "0TPKIbihMF", "signatures": ["ICLR.cc/2026/Conference/Submission15672/Reviewer_EwMQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15672/Reviewer_EwMQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992877025, "cdate": 1761992877025, "tmdate": 1762925925754, "mdate": 1762925925754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ScholarEval, a novel, retrieval-augmented framework designed for comprehensive evaluation of research ideas, a critical and underexplored area in AI co-science. The system assesses ideas based on two core, literature-grounded criteria: Soundness (empirical validity of proposed methods) and Contribution (advancement across multiple dimensions relative to prior work). The pipeline involves targeted search, information extraction from the full text and abstracts of scholarly literature (Semantic Scholar), and synthesis of dense, actionable, and cited feedback. To benchmark the system, the authors introduce ScholarIdeas, the first expert-annotated, multi-domain dataset for idea evaluation. ScholarEval significantly outperforms strong baselines, including a state-of-the-art agentic system, in terms of review coverage, depth, evidence support, and overall usefulness, as validated by automatic metrics and an expert user study."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. ScholarEval fills a critical gap by offering a system that provides comprehensive, multifaceted evaluation across both methodological soundness and multi-dimensional contribution, moving beyond one-dimensional scoring or sparse feedback.\n2. The explicit use of retrieval-augmentation ensures that all claims and suggestions are supported by direct evidence from the literature via citations, resulting in feedback that is demonstrably more actionable, valid, and useful for idea refinement compared to baselines.\n3. The system significantly outperforms the strongest baseline (o4-mini-deep-research) by a large margin in coverage of expert review points and is consistently preferred by human experts in terms of depth, actionability, and literature engagement.\n4. The introduction of ScholarIdeas (117 ideas, 1076 rubrics across four disciplines) is a major, high-quality contribution to the community, enabling future standardized benchmarking in this area."}, "weaknesses": {"value": "1. The framework's core strength is also its primary vulnerability. It relies entirely on retrieved literature, meaning it may misjudge the effectiveness of a truly novel method not yet represented in the literature or misrepresent the contribution if relevant contrasting papers are missed.\n2. The reported high cost (up to $3) and time (around 12 minutes) per evaluation could pose a significant barrier to adoption for large-scale or batch ideation workflows.\n3. Dependency and Sensitivity to LLM Backbone: The entire multi-stage process, from method extraction and summarization to synthesis, is highly dependent on the capability of the underlying LLM (M). The paper does not fully isolate the performance gain attributable to the pipeline structure versus the quality of the specific LLM used."}, "questions": {"value": "1. The paper acknowledges that a failure to retrieve relevant papers can misrepresent soundness or contribution. What is the measured recall or F1 score for the crucial \"Context Retrieval\" step in the Soundness module? Could the authors provide an ablation study that systematically quantifies the impact on final coverage when retrieval is limited (e.g., reducing the number of papers/snippets retrieved) to better understand the system's robustness to retrieval noise or omissions?\n2. Given the multi-stage reliance on the LLM, please provide an ablation where the full ScholarEval pipeline is executed using the same (presumably weaker) LLM backbone used for the \"deep-research\" baseline16. This is crucial to definitively isolate the performance gain stemming from the novel pipeline architecture versus the improved capabilities of the underlying language model (M)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CC21JqIayp", "forum": "0TPKIbihMF", "replyto": "0TPKIbihMF", "signatures": ["ICLR.cc/2026/Conference/Submission15672/Reviewer_qnee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15672/Reviewer_qnee"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762256861449, "cdate": 1762256861449, "tmdate": 1762925925274, "mdate": 1762925925274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}