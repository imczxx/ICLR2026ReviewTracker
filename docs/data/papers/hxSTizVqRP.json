{"id": "hxSTizVqRP", "number": 11360, "cdate": 1758197348998, "mdate": 1759897580154, "content": {"title": "VISA: Preserving Fine-Grained Perception in MLLMs via Visual Semantic Anchoring", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in general-purpose visual understanding. However, their training paradigm faces a fundamental bottleneck: the challenge of learning high-fidelity visual representations from indirect, text-based objectives alone. This inefficient process leads to a phenomenon we term semantic attenuation, where internal visual representations lose critical, high-fidelity details, hindering performance on tasks requiring fine-grained perception. To resolve this core representation learning challenge, we propose **VIsual Semantic Anchoring (VISA)**, a novel and general training framework that introduces a direct, vision-native supervisory signal into the MLLM's intermediate layers. By anchoring the MLLM's representations to the rich feature space of a pretrained Vision Foundation Model (VFM) through representation alignment, VISA ensures its visual pathway learns and maintains a detailed and structured understanding of the visual world. Our composite loss, which enforces both point-wise semantic alignment and structural consistency, makes this process highly effective. Extensive experiments on diverse benchmarks and model backbones demonstrate that by fostering more robust internal representations, VISA significantly enhances fine-grained reasoning, improves factual grounding against hallucinations, and accelerates training convergence, establishing a new and effective paradigm for developing more perceptually robust MLLMs. Our code is open-sourced via \\url{https://anonymous.4open.science/r/anonymous_VISA-D482/}", "tldr": "We propose VISA, a training framework that uses a powerful VFM as a semantic anchor to provide direct visual supervision to MLLM's intermediate layers, counteracting the loss of fine-grained detail caused by indirect text-only training.", "keywords": ["Multimodal Large Language Models", "Fine-Grained Perception", "Semantic Drift", "Plug-and-Play Module"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af43aa3b8425f2e97efaee64e1c1aead05957bac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a technique to improve the internal representation of features extracted by a visual encoder within a MLLM. The authors propose to use an independently trained visual foundation model (DINOv2) as a regularization while training the MLLM by extracting the visual tokens from an intermediate layer of the LLM, transforming them using a simple MLP and comparing them to the visual tokens that DINOv2 extracts. This technique, although not presented like that in the work, is basically a distillation from DINOv2 to the MLLM implemented by feature matching. The result of applying this technique is that the resulting MLLM is able to retain more high frequency characteristics within the visual features which in turn result in improved performance on some visual understanding benchmark compared to the same MLLM model trained without DINOv2 regularization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The core observation that visual features “degrades” over training and over passing through the LLM due to the use of a mainly text driven supervision is quite interesting and to the best of my knowledge novel. \n\n+ The proposed solution is simple and well motivated. Implementation seems straightforward and easy to reproduce, but also quite effective in the reported results\n\n+ Presentation is clean and easy to follow."}, "weaknesses": {"value": "## Major\n\na. **Missing motivational experiment**: Given the premises of the paper I would have expected to see an experiment similar to Fig. 2 but taking into account how the feature transformation operated by the intermediate LLM layers improve/downgrades the high frequency content of the feature over training iterations. The core premise of this work is that it is the text only supervision used in the IT SFT phase for MLLM that pushes models to learn transformation for visual tokens that “forget” high frequency features the deeper the tokens are through the model. To verify that, I would have expected this phenomena to be studied as a function of training iterations of the MLLM, but the authors instead rely on Fig. 2 (which is an experiment done on a fully trained MLLM) as their only motivating experiment.\n\n\n## Minor\n\nB. **Claims could be revised**: I think what the authors proposed is 2 things: (i) a method to retain high frequency details in the intermediate features of an MLLM and (ii) a way of using this in a distillation setting to learn from a VLM teacher. (i) is clearly called out in the paper, (ii) is not and should be made more clear. The evidence for this is in my opinion in Tab. 2, the “Self-anchor” experiment is enforcing only claim (i) and has a somewhat modest improvement compared to baseline, most improvements come from  the “Vision-native anchor” that combines (i) + (ii). Presenting this work under this light will make the result equally interesting but somewhat less surprising: there are several works in the literature showing that combining visual encoder can boost the performance of MLLMs [1, 2, 3]. According to (ii) this work is basically combining the feature extraction capabilities of CLIP with the one of DINOv2 in a clever novel way and without requiring running both at inference time. This is cool but should be made more clear.\n\nC. **Missing potential experiments**: Following up on weakness B I think it would have been interesting to test what would have been the performance of a MLLM trained with both CLIP and DINOv2 encoder (as simple as concatenating the features ina longer sequence). Other experiments that would have made the work more complete would be considering other VFM teachers besides DINOv2 and CLIP. Finally I would have liked to see an ablation on the usefulness of the weighting term in Eq. 2.\n\n## References\n\n1. [Cao, Jiajun, et al. \"Move-kd: Knowledge distillation for vlms with mixture of visual encoders.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.](https://arxiv.org/pdf/2501.01709)\n2. [Chung, Jihoon, et al. \"Unifying specialized visual encoders for video language models.\" arXiv preprint arXiv:2501.01426 (2](https://arxiv.org/pdf/2501.01426)\n3. [Kar, Oğuzhan Fatih, et al. \"Brave: Broadening the visual encoding of vision-language models.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.](https://arxiv.org/pdf/2404.07204)"}, "questions": {"value": "1. Can you comment on weakness B? \n2. I could not find in the paper on what the different models are trained and starting from what. Can you clarify?\n3. Have you run any experiment unfreezing the visual encoder? How would that impact the behavior of the visual features?\n4. How would you implement VISA if the VFM and the MLLM do not have the same number of visual tokens?\n5. Following up on D, can you provide more information on the CLIP-Text experiment? It is not clear from the manuscript how that would work. Eq. 2 and Eq. 3 imply that the dimensionality N of the number of tokens should match between whatever is going through the MLLM and the VFM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rtn2ZDjnVx", "forum": "hxSTizVqRP", "replyto": "hxSTizVqRP", "signatures": ["ICLR.cc/2026/Conference/Submission11360/Reviewer_yvqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11360/Reviewer_yvqy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761946242, "cdate": 1761761946242, "tmdate": 1762922492096, "mdate": 1762922492096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Visual Semantic Anchoring (VISA), a training framework designed to enhance the fine-grained visual perception of Multimodal Large Language Models (MLLMs). The core problem identified is \"semantic attenuation,\" where MLLMs lose critical high-fidelity visual details because they are trained primarily with an indirect, text-based language modeling objective. VISA resolves this by introducing a direct, vision-native supervisory signal into the MLLM's intermediate layers. This signal anchors the MLLM's internal visual representations to the rich, unbiased feature space of a pretrained Vision Foundation Model (VFM) (like DINOv2). The method uses a composite loss that enforces both point-wise semantic alignment and structural consistency between the MLLM's features and the VFM's features. Extensive experiments confirm that VISA significantly enhances fine-grained reasoning, improves factual grounding, and maintains a higher degree of decodable high-frequency visual information across the MLLM's deep layers"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Motivation: The paper provides a clear diagnosis and empirical evidence for semantic attenuation, a critical issue where the text-centric loss degrades fine-grained visual feature integrity. This is a valuable theoretical contribution, and the proposed VISA framework directly targets this architectural weakness.\n\n2. Effective and General Solution: VISA is demonstrated to be a flexible and effective training framework that significantly improves performance on fine-grained tasks (e.g., counting and spatial reasoning) across diverse, modern MLLM backbones (Vicuna, Qwen2.5-VL, InternVL3.5-8B). The concept of using a vision-native VFM (like DINOv2) to provide a less-biased semantic anchor is well-justified.\n\n3. Strong Empirical and Qualitative Support: The work includes robust quantitative evidence from diagnostic probing experiments showing higher decodable information in VISA-enhanced models across all layers. This is further supported by compelling frequency-domain analysis, which visually confirms that VISA preserves substantially more high-frequency information essential for detailed perception."}, "weaknesses": {"value": "1. Dependency on VFM Choice and Feature Bias: The success relies on the assumption that the chosen VFM (DINOv2) provides a universally stable and \"unwavering\" anchor. However, the paper does not fully investigate the sensitivity of VISA to the VFM choice. If the VFM's feature space contains specific biases (e.g., an over-representation of certain textures or colors), VISA would enforce these biases onto the MLLM.\n\n2. Missing Quantification of Training Overhead: While the paper claims improved training efficiency and stability, the introduction of a dense, token-wise representation alignment loss across intermediate layers likely imposes a non-trivial computational overhead during training. The paper should explicitly quantify the increase in training time/memory cost per step compared to the baseline."}, "questions": {"value": "1. Semantic Adapter Architecture Details: The semantic adapter ($P_{\\pi}$) is a lightweight 3-layer MLP17. Please provide the full architectural details (e.g., specific layer dimensions, activation functions, and the training objective for $P_{\\pi}$). Given that the VFM features are frozen, how is $P_{\\pi}$ constrained to ensure that the MLLM's features are not overly distorted simply to satisfy the alignment loss with the VFM's unique embedding space?\n\n2. Sensitivity Analysis on Loss Weights: While stability around default hyperparameters ($\\alpha=0.5$, $\\lambda=1.0$) is noted, given that $\\mathcal{L_{point}}$ and $\\mathcal{L}_{struct}$ have complementary task-specific benefits (MME vs. GQA/MMVP), is there any evidence that different downstream tasks (e.g., counting vs. spatial relation) benefit from task-specific optimal values of $\\alpha$? A deeper discussion on tuning $\\alpha$ and $\\lambda$ for diverse task sets is recommended."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kRG2ykZXVe", "forum": "hxSTizVqRP", "replyto": "hxSTizVqRP", "signatures": ["ICLR.cc/2026/Conference/Submission11360/Reviewer_dsgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11360/Reviewer_dsgs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898942580, "cdate": 1761898942580, "tmdate": 1762922491706, "mdate": 1762922491706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that prevailing MLLMs are trained with indirect, text-centric objectives, causing a “semantic attenuation” phenomenon at inference time: salient visual details are gradually lost, hampering fine-grained downstream tasks. To counteract this, the authors propose to use a pre-trained visual foundation model (VFM) as a semantic anchor and align the deep representations of the MLLM with those of the VFM. Empirical results on benchmarks verify the idea."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The approach is conceptually clear and easy to implement."}, "weaknesses": {"value": "1. **Superficial connection to distillation literature.**  \n   The proposed alignment is essentially a form of representation distillation, yet the paper omits a systematic discussion of prior distillation works. Moreover, advanced techniques—such as attention-based distillation [1] or multi-layer feature fusion [2]—are not explored; only rudimentary point-wise and structural losses are examined, making the method appear overly naive.\n\n2. **Insufficient experimental footprint.**  \n   Evaluations are restricted to a narrow set of tasks. Comprehensive benchmarks that reasoning and fine-grained perception (e.g., MMMU, MMBench, POPE) are absent, leaving generalizability in question.\n\n3. **Lack of ablation and theoretical insight.**  \n   The authors observe that distilling solely shallow or deep layers yields marginal gains, whereas mid-layer alignment works best. No further analysis—empirical or theoretical—is provided to explain why, forcing practitioners to rely on grid-search and limiting the method’s applicability to backbones of varying depths."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uDEk0BF8hJ", "forum": "hxSTizVqRP", "replyto": "hxSTizVqRP", "signatures": ["ICLR.cc/2026/Conference/Submission11360/Reviewer_niVD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11360/Reviewer_niVD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916037679, "cdate": 1761916037679, "tmdate": 1762922491278, "mdate": 1762922491278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and addresses a key limitation in MLLMs: semantic attenuation, where text-only supervision degrades fine-grained visual representations. The authors propose VISA (Visual Semantic Anchoring), a general training framework that anchors intermediate MLLM layers to vision-native representations from pretrained VFMs (e.g., DINOv2). A composite loss combining (i) point-wise semantic alignment and (ii) structural consistency is used to regularize the visual pathway. VISA claims consistent improvements across multiple backbones (Vicuna, Qwen2.5-VL, InternVL3.5)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and well-organized, with a strong motivation around the semantic attenuation problem in MLLMs.\n\n- The idea of diagnosing semantic attenuation through layer-wise probing is an interesting and valuable analytical direction. While the setup lacks methodological details, it highlights a systematic attempt to quantify internal representation degradation\n\n- Simple yet principled approach. VISA’s anchoring mechanism is well-designed, sound, and easy to integrate into any existing MLLMs.\n\n- The ablations are extensive covering optimal layer choice, anchor source, and loss components. The inclusion of probing and frequency-domain studies further supports their claims along with few qualitative samples.\n\n- The proposed approach is demonstrated across multiple backbones with limited hyper-parameter sensitivity, suggesting generality and practicality."}, "weaknesses": {"value": "- Insufficient details for the diagnostic probe. The paper (sec 3.2) lacks crucial information on dataset source, #samples, training details, and evaluation used for “layer-wise probe accuracy.” Without these details, the evidence for semantic attenuation remains inconclusive and difficult to reproduce.\n\n- The reported scores for Qwen 2.5-VL, InternVL 3.5, and LLaVA-1.5 in Table 1 differ substantially from the officially published results, casting doubt on the claimed improvements and overall experimental reliability. The authors must clarify these discrepancies.\n\n- Missing discussion of reconstruction-based MLLM training. The work omits discussion of closely related methods that use reconstruction objectives to preserve high-frequency details, such as X-Former [2] and Reconstructive Visual Instruction Tuning [3]. Prior analyses [1] have shown that self-supervised reconstruction methods (e.g., MAE) capture richer fine-grained and high-frequency visual representations. These approaches share VISA’s motivation but differ in the type of supervision employed. A comparative analysis or at least discussion is necessary to better position VISA among these existing paradigms.\n\n- The method currently anchors exclusively to DINOv2. An ablation comparing anchors (DINOv2 vs. SigLIP vs. MAE) would strengthen the paper by showing how different VFMs affect fine-grained perception. \n\n- It would further improve the paper to evaluate VISA on explicit fine-grained perception (GVTBench[4]) tasks such as object counting and multi-class identification benchmark [4], to directly measure improvements in fine-grained visual understanding.\n\n\n[1] What Do Self-Supervised Vision Transformers Learn? Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, Sangdoo Yun. ICLR 2023\n\n[2] X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs. Sirnam Swetha, Jinyu Yang, Tal Neiman, Mamshad Nayeem Rizve, Son Tran, Benjamin Yao, Trishul Chilimbi, Mubarak Shah. ECCV 2024\n\n[3] Reconstructive Visual Instruction Tuning. Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Zhaoxiang Zhang. ICLR 2025\n\n[4] What Makes for Good Visual Tokenizers for Large Language Models? Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, Ying Shan"}, "questions": {"value": "- Were baseline model scores obtained via official evaluation checkpoints ? why do they differ from official reported scores ?\n- What datasets and protocols were used for the diagnostic probing experiments shown in Fig. 2 ?\n- Could VISA leverage multiple anchors or frequency-complementary models ?\n\n\nIf the authors are able to clarify the discrepancies in the baseline model scores, provide complete details of the probing setup, and better position this work relative to existing reconstruction-based methods, I would be inclined to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aj2PUJXzAs", "forum": "hxSTizVqRP", "replyto": "hxSTizVqRP", "signatures": ["ICLR.cc/2026/Conference/Submission11360/Reviewer_BibD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11360/Reviewer_BibD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981938814, "cdate": 1761981938814, "tmdate": 1762922490694, "mdate": 1762922490694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}