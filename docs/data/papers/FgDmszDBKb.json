{"id": "FgDmszDBKb", "number": 25527, "cdate": 1758368907192, "mdate": 1763738900658, "content": {"title": "StaQ: a Finite Memory Approach to Discrete Action Policy Mirror Descent", "abstract": "In Reinforcement Learning (RL), regularization with a Kullback-Leibler divergence that penalizes large deviations between successive policies has emerged as a popular tool both in theory and practice. This family of algorithms, often referred to as Policy Mirror Descent (PMD), has the property of averaging out policy evaluation errors which are bound to occur when using function approximators. However, exact PMD has remained a mostly theoretical framework, as its closed-form solution involves the sum of all past Q-functions which is generally intractable. A common practical approximation of PMD is to follow the natural policy gradient, but this potentially introduces errors in the policy update. In this paper, we propose and analyze PMD-like algorithms for discrete action spaces that only keep the last $M$ Q-functions in memory. We show theoretically that for a finite and large enough $M$, an RL algorithm can be derived that introduces no errors from the policy update, yet keeps the desirable PMD property of averaging out policy evaluation errors. Using an efficient GPU implementation, we then show empirically on several medium-scale RL benchmarks such as Mujoco and MinAtar that increasing $M$ improves performance up to a certain threshold where performance becomes indistinguishable with exact PMD, reinforcing the theoretical findings that using an infinite sum might be unnecessary and that keeping in memory the last $M$ Q-functions is a practical alternative to the natural policy gradient instantiation of PMD.", "tldr": "We study a variant of PMD that keeps in memory the last M Q-functions, showing that it does not bias convergence and retains the averaging of error effect of PMD", "keywords": ["reinforcemnt learning; entropy regularization; policy mirror descent; function approximators"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e87ca7b3f3d76ac91ca8b9326d2d83f546e996a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors observe that the exact Policy Mirror Descent (PMD) algorithm produces a policy that depends on the sum of all past Q-functions. However, under function approximation, this accumulation introduces error over time. To address this issue, the authors restrict their analysis to the discrete action space and propose a PMD-like algorithm that retains the most recent M Q-functions. In this formulation, for a sufficiently large but finite M, the policy update error diminishes, while the averaging effect in policy evaluation remains (up to error terms that decay exponentially with respect to M). Furthermore, the authors propose a practical implementation of their algorithm to investigate the benefit of stacking past M Q-functions in practice."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "From a theoretical standpoint, it is both important and interesting to explore whether incorporating the past M Q-functions can provide benefits in the function approximation setting. However, I am not convinced that the findings in the paper translate to the practical setting."}, "weaknesses": {"value": "- The presentation of the paper is at times very difficult to follow.\n- The paper is positioned as an investigation into whether the theoretical foundations of PMD translate effectively into practice. However, all empirical results are deferred to the appendix without a clear guideline on what to look for, which limits the reader’s ability to assess the validity of the hypotheses. After carefully reviewing the appendix, I have several concerns:\n    - The theoretical analysis focuses on the discrete action setting, yet the experiments start with continuous control tasks where the policy is parameterized as a diagonal Gaussian distribution. \n    - For MuJoCo environments, Soft Actor-Critic (SAC) is widely regarded as a strong baseline, but it is not included in the results. Conversely, DQN and M-DQN, which are designed for discrete action spaces, are evaluated on MuJoCo tasks.\n    - In the Atari experiments, DQN and M-DQN use $\\epsilon$-greedy exploration, but it is unclear whether the proposed method uses the same exploration strategy.\n    - It is not specified whether the authors relied on a standard implementation framework (e.g., spinning up or stable-baselines) for the proposed method, which makes reproducibility and fair comparison difficult to assess."}, "questions": {"value": "In the preliminaries, the policy is defined as  $\\pi \\propto \\exp(Q(s, \\cdot))$. However, based on the standard formulation of PMD, shouldn’t this instead be $\\pi_t \\propto \\pi_{t-1} \\exp(Q^{t-1}(s, \\cdot))$? This would reflect the iterative update of the policy relative to the previous one, which, when applied recursively, results in the sum over all passed Q-functions?\n\nIn the function approximation setting, the early Q-functions are typically of low quality. Could you elaborate on the intuition behind why storing these past Q-functions would be beneficial? This design choice seems somewhat counterintuitive. Additionally, could you clarify which policy is used to sample actions during interaction with the environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zp6SY9jeGX", "forum": "FgDmszDBKb", "replyto": "FgDmszDBKb", "signatures": ["ICLR.cc/2026/Conference/Submission25527/Reviewer_f4Z1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25527/Reviewer_f4Z1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761347931278, "cdate": 1761347931278, "tmdate": 1762943462270, "mdate": 1762943462270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "StaQ is a finite-memory variant of Policy Mirror Descent for \\emph{discrete} actions that averages only the most recent \\(M\\) Q-functions. A stacked-Q implementation keeps computation efficient. As \\(M\\) increases, StaQ closely matches exact PMD and yields more stable, higher returns on discretized MuJoCo and MinAtar."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides guarantees that the truncation error from keeping only $M$ past Q-functions rapidly diminishes as $M$  grows, so the policy-update bias effectively vanishes. This bridges the gap between an idealized infinite-history PMD and a practical algorithm, giving users a clear knob $M$ to trade memory for accuracy.\n\n2. The stacked-Q architecture enables parallel evaluation of multiple historical Q estimates in a single forward pass, avoiding extra policy-optimization steps. In practice, this keeps wall-clock overhead modest even for large \\(M\\), which makes the method easy to scale on standard GPUs.\n\n3. Across several discrete or discretized benchmarks, increasing \\(M\\) consistently improves return and reduces variability across random seeds. For sufficiently large \\(M\\) (e.g., hundreds), performance is nearly indistinguishable from exact PMD, supporting the theoretical claims with tangible gains."}, "weaknesses": {"value": "1 .The method and analysis focus on  discrete actions; extensions to continuous control are not developed. Given that many modern RL tasks are continuous, this limits immediate applicability and may require nontrivial adaptations of both the algorithm and proofs.\n2.  Several experiments rely on discretizing originally continuous-control environments, which can alter the dynamics and policy landscape. Conclusions drawn in these settings may not directly transfer to canonical continuous-control benchmarks without further validation."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rbk9asRUxb", "forum": "FgDmszDBKb", "replyto": "FgDmszDBKb", "signatures": ["ICLR.cc/2026/Conference/Submission25527/Reviewer_VUeC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25527/Reviewer_VUeC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827546240, "cdate": 1761827546240, "tmdate": 1762943461959, "mdate": 1762943461959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Policy mirror descent (PMD) algorithms can average out policy-evaluation errors, but their exact implementation is impractical since it requires storing all past Q functions. To address this, the paper proposes a finite-memory variant, called StaQ, which only requires retaining the last $M$ Q functions. The paper shows that the policy update error vanishes for sufficiently large $M$ and the number of iterations, and the averaging effect in PMD can be obtained by introducing a KL divergence penalization during policy update."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The finite-memory formulation is well-motivated, enabling a practical implementation of the PMD algorithm without requiring infinite storage or recomputation.\n\nThe paper discusses the similarities and differences between the proposed method and related works, mainly in Section 2, and further connects relevant prior studies throughout later sections where appropriate.\n\nThe work is supported by theoretical results. Theoretical analysis for the convergence of Entropy-regularized Policy Mirror Descent (EPMD) is provided in Section 4, to connect EPMD to finite memory EPMD and improve over existing analysis on the convergence rate of EPMD. Theorem 5.1 extends the convergence analysis to the finite memory setting.\n\nThe paper considers the applicability of the algorithm by providing practical implementation for EPMD in Section 4.3, and for finite memory EPMD in Section 5.1 (StaQ).\n\nThe paper empirically investigates how the number of stored Q functions, $M$, affects StaQ’s performance, demonstrating when the finite-memory approach approximates the exact EPMD’s behavior.\n\nThe method is empirically evaluated across a wide range of tasks, and detailed parameter settings are reported to support reproducibility."}, "weaknesses": {"value": "The paper could be strengthened by quantifying memory usage and explicitly analyzing the trade-off between memory consumption and learning efficiency. StaQ lies between natural policy gradient methods (e.g., TRPO) and PMD. While StaQ reduces storage and computation by keeping only $M$ recent Q functions compared to PMD, it may still require higher memory usage compared to other baselines like TRPO. It may be worth comparing the memory usage of these methods and their learning curves.\n\nThe runtime in Table 1 provides limited insight into StaQ’s advantages. It does not capture one of the key benefits of StaQ, which lies in maintaining stable and accurate policy updates under limited memory. Table 1 mainly suggests that varying the number of stored Q functions does not have a large effect on the running time. Including learning curves in the main paper, to highlight StaQ’s learning efficiency, might make the empirical case more convincing.\n\nThe method is limited to discrete action space tasks. For continuous control tasks, the paper discretizes the action space, which may lead to loss of precision and suboptimal policies.\n\n**Small thing:**\n\nLine 324-325: “...over states sampled from _some a_ predefined initial state distribution.” You may want to remove _some_."}, "questions": {"value": "The choice of $M$ may depend on the environment. Do you expect the observed pattern, where StaQ with M=300 approximates PMD, to generalize to other environments, such as those with higher stochasticity or delayed rewards?\n\nIn Figure 2, Humanoid-v4, M=500 performs worse than M=300. Similarly, in Table 2, M=300 occasionally outperforms the exact PMD. This result seems to contradict the intuition that a larger M approximates the exact PMD better. Where do you think the StaQ’s performance advantage may come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p9twfiRG7u", "forum": "FgDmszDBKb", "replyto": "FgDmszDBKb", "signatures": ["ICLR.cc/2026/Conference/Submission25527/Reviewer_UZAB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25527/Reviewer_UZAB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957823401, "cdate": 1761957823401, "tmdate": 1762943461649, "mdate": 1762943461649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies policy mirror descent (PMD) with a finite memory of past Q-functions in discrete action spaces. It proposes StaQ, an optimization-free policy update rule implemented through a stacked neural network.Policy mirror descent algorithms usually include a distance term, such as KL divergence, to enforce a trust region and ensure stable updates. \n\nTheoretically, the paper shows that if there is no policy evaluation error and the memory size is large enough, the finite-memory update converges to the same policy as exact policy mirror descent. It also retains the error-averaging effect, with extra terms that decay exponentially with memory size.\n\nThe analysis covers both value iteration and policy iteration, providing explicit bounds that separate the effects of evaluation error and truncation. Empirically, the results show that increasing memory size improves performance up to a plateau, matching the exact-PMD baseline within the training horizon on discretized MuJoCo and MinAtar benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem formulation and principled update:\n  - The finite-memory PMD policy is\n    $$\n    \\xi\\_{k+1} = \\beta\\\\xi\\_k + \\alpha q\\_k + \\frac{\\alpha \\beta^M}{1-\\beta^M}\\big(q\\_k - q\\_{k-M}\\big),\n    $$\n    is derived cleanly and highlights both the deletion of the oldest $q$ and the mild overweighting of the latest $q$.\n- Theoretical extensions to Vieillard et al. (2020a) and careful bounds:\n  - The paper correctly qualifies that if policy evaluation error vanishes and $M$ satisfies the stated condition, the update introduces no additional policy-update error, i.e., the algorithm converges to the optimal entropy-regularized policy.\n- Practical relevance and engineering:\n  - The stacked-neural-network (SNN) implementation exploits batched evaluation of many frozen $Q$-networks on GPU, making large $M$ feasible in practice. Precomputing logits for the replay buffer further amortizes cost during policy evaluation.\n  - Wall-clock data (Table 1) show minimal runtime increase with $M$ on mid-scale tasks, supporting the feasibility claim.\n- Empirical evidence aligns with their theorems:\n  - Across discrete-action MuJoCo and MinAtar, increasing $M$ generally improves performance until diminishing returns in some environments; large $M$ becomes empirically close to the exact-PMD variant (that never deletes within the 5M-step window).\n\n**References**\n- Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, Remi Munos, and Matthieu Geist. Leverage the average: an analysis of kl regularization in reinforcement learning. In Advances in Neural Information Processing Systems, 2020a."}, "weaknesses": {"value": "- The main results and implementation are limited to discrete action spaces. While the paper acknowledges this, a discussion of algorithmic options for continuous actions (e.g., approximate sampling or parameterized policies) would strengthen the practical impact.\n- Empirical analysis gaps:\n  - Improvement with $M$ is not monotone across all tasks. The paper frames the trend as \"beneficial effects with diminishing returns”, but the figures/tables show non-monotonic or even worse behavior on some environments (e.g., Hopper-v4, Breakout-v1, Freeway-v1, HalfCheetah-v4). The analysis could more explicitly discuss when/why performance can dip at intermediate $M$.\n  - Baselines: Mirror Descent Policy Optimization (MDPO) is a closely related comparator for policy mirror descent with approximate updates; including it would position StaQ more clearly against the most directly related policy-optimization methods.\n  - Sensitivity and interpretability: Although $\\epsilon$-softmax exploration is used uniformly in the main study and a “sticky” behavior policy is shown to unlock MountainCar in a focused analysis, a broader sensitivity study of behavior policy design and its interaction with error averaging would be valuable.\n- Benchmark choice: Discretized MuJoCo is not ideal; full Atari or other naturally discrete benchmarks would be more appropriate."}, "questions": {"value": "1. The paper’s methods and theory are limited to discrete action spaces. How difficult would it be to extend StaQ to continuous actions? Could re-parameterized actions such as Gumbel softmax policies preserve similar theoretical guarantees? \n2. The paper assumes vanishing policy evaluation error for convergence proofs. How sensitive is StaQ to nonzero evaluation errors in practice, and could the analysis be extended to handle this more realistic case?\n3. The improvement with memory size (M) is not monotonic across tasks. Can the authors explain why intermediate (M) sometimes leads to worse performance? Does truncation interact with learning dynamics or noise in Q-function estimates?\n4. The experiments lack baselines such as Mirror Descent Policy Optimization (MDPO). Why was MDPO omitted, and how might StaQ compare to it theoretically and empirically?\n5. The paper uses only one behavior policy variant (ε-softmax) except for a small analysis on sticky policies. Could more diverse behavior policies change StaQ’s error-averaging behavior or its convergence?\n6. Given the strong theoretical results but limited empirical exploration, how confident should we be that StaQ’s convergence properties translate to real-world deep RL settings?\n7. Are there scenarios where finite memory could harm performance due to outdated Q-functions being overweighted or reintroduced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "thHzdqM7BD", "forum": "FgDmszDBKb", "replyto": "FgDmszDBKb", "signatures": ["ICLR.cc/2026/Conference/Submission25527/Reviewer_E3vT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25527/Reviewer_E3vT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965728896, "cdate": 1761965728896, "tmdate": 1762943461374, "mdate": 1762943461374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}