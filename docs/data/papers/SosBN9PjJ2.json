{"id": "SosBN9PjJ2", "number": 20265, "cdate": 1758304237720, "mdate": 1759896987442, "content": {"title": "Insight-RAG: Enhancing LLMs with Insight-Driven Augmentation", "abstract": "Retrieval Augmented Generation (RAG) frameworks have shown significant promise in leveraging external knowledge to enhance the performance of large language models (LLMs). However, conventional RAG methods often retrieve documents based solely on surface-level relevance, leading to many issues: they may overlook deeply buried information within individual documents, miss relevant insights spanning multiple documents, and struggle to support tasks beyond traditional question answering without significant additional customization.   In this paper, we propose Insight-RAG, a novel framework designed to address these issues. In the initial stage of Insight-RAG, instead of using traditional retrieval methods, we employ an LLM to analyze the input query and task, extracting the underlying informational requirements. In the subsequent stage, a specialized LLM---trained on the document database---is queried to mine content that directly addresses these identified insights. Finally, by integrating the original query with the retrieved insights, similar to conventional RAG approaches, we employ a final LLM to generate a contextually enriched and accurate response. Using two scientific paper datasets, we created evaluation benchmarks targeting each of the mentioned issues and assessed Insight-RAG against traditional RAG pipeline.  Our results demonstrate that the Insight-RAG pipeline successfully addresses these challenges, outperforming existing methods by up to 60 percentage points. Supported by our comprehensive ablation studies---including the performance of each component and the quality of the identified insights---these findings suggest that integrating insight-driven retrieval within the RAG framework not only enhances performance but also broadens its applicability to tasks beyond conventional question answering.  We will release our dataset and code.", "tldr": "We introduce Insight-RAG, a novel framework that improves traditional RAG by adding an intermediate insight extraction step, significantly outperforming standard RAG on our newly created benchmarks targeting RAG's key limitations.", "keywords": ["retrieval-augmented generation", "domain adaptation", "information extraction"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/749f80529b80ecb82af6c35a537d051684fe870b.pdf", "supplementary_material": "/attachment/0ca7711d5d9da3239a662282c8c543340b90e797.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a RAG framework (Insight-RAG) including an insight identifier, an insight retriever, and a response generator. The insight identifier involves a prompt for generating “sentence fragment that can be completed by the LLM trained on the dataset”. The insight retriever is essentially an LM fine-tuned in-domain to complete the sentence fragments. The question combined with the “insights” are inputs to the response generator, which produce the final answer. They also construct two new datasets by extracting triplets (subject, relation, object) from 5,000 papers, and converting the triplets into natural language questions. Results show that Insight-RAG is very effective in the proposed datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method shows strong performances on the proposed dataset."}, "weaknesses": {"value": "1. The experimental details are not clear. (1) It is not clear how the insight retriever is trained. How the retrieval is done is also unclear. From my understanding it’s just generating new tokens based on the insight, but then L372-373 mention recall @ 10. How do you sample 10 insights? (2) It is unclear how the train / dev / test split is constructed. Is the insight retriever trained on testing data? (3) How to extract triples from the papers is unclear. The prompt is not provided.  \n2. The datasets seem to be created for the method and are very synthetic. The results might not generalize to real-world datasets. Why not use existing RAG datasets? The fact that you fine-tuned on triplets, and the questions are constructed on the triplets is very concerning. \n3. It is unclear where the performance gain comes from. It seems to me that most of the benefits come from fine-tuning the insight retriever on in-domain triplets (including those of the test data). The insight retriever is only memorizing the triplets and giving the correct objects during test time. If that is the case, I do not think this method is useful in practical scenarios. A quick way to find out if this is the case, you could fine-tuned an LM on the data you train the insight retriever on, and evaluate the performance of that LM with simple prompting. I think this should be a baseline to be included. \n4. The presentation could be improved. (1) It is unclear from the main text what insights and questions look like. (I only understand when I look at the prompt in the Appendix.) It would be very helpful if you could include some examples in the main text. (2) Insight retriever is not actually a retriever. The naming should explain it clearly: it is simply a model fine-tuned in-domain. (3) Some results should not be in the appendix. For example, if you would like to refer to ColBERT and Self-RAG results, you should not put them in the appendix. (4) L176-L193 seem unnecessary."}, "questions": {"value": "1. How is the insight retriever actually trained? What are the inputs and outputs? How do you train it on triplets? Is it trained on the triplets of the test data as well? \n2. How do you construct train / development / test splits? \n3. What is the prompt for extracting the triplets? \n4. L214 you mentioned Recall @ k. How do you obtain top-K results? \n5. L246-247 you mention the questions may be ambiguous or erroneous. Why even consider these questions? Why not filter them out?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qer70SgXRL", "forum": "SosBN9PjJ2", "replyto": "SosBN9PjJ2", "signatures": ["ICLR.cc/2026/Conference/Submission20265/Reviewer_pcc5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20265/Reviewer_pcc5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877053455, "cdate": 1761877053455, "tmdate": 1762933746459, "mdate": 1762933746459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Insight-RAG, a three-stage pipeline: (1) an LLM Insight Identifier converts a query/task into sentence-fragment insights; (2) an Insight Retriever (Llama-3.2-3B with LoRA continual pretraining on the target corpus and extracted triples) completes those fragments to surface content; (3) a final LLM generates the answer. The authors build bespoke benchmarks from AAN and OpenCorpus abstracts by extracting triples with GPT-4o-mini, translating them into questions, and evaluating deeply buried, multi-document, and a non-QA matching task. They report large gains over vanilla RAG and Self-RAG in some settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear articulation of perceived RAG failure modes (deeply buried facts, cross-doc synthesis, beyond-QA tasks).\n2. Sensible modular decomposition and ablations on the two new components.\n3. Some effort toward component analysis (identifier similarity scoring, retriever metrics) and token-budget sensitivity."}, "weaknesses": {"value": "1. Benchmarks and labels are built from GPT-extracted triples taken from the same corpus used to continually pretrain the retriever, so training and evaluation are not cleanly separated.\n2.Comparisons omit strong modern RAG setups (hybrid lexical+dense, reranking, trained Self-RAG/Iter-RetGen, HyDE, RePlug, etc.), making the reported large gains unconvincing.\n3. The Insight Retriever appears to generate text rather than retrieve evidence/passages, making the comparison to passage-retrieval RAG apples-to-oranges and undermining attribution/grounding.\n4. Synthetic, LLM-generated questions over abstracts only; brittle EM/F1 scoring without human adjudication or significance tests; constrained context budgets that handicap RAG."}, "questions": {"value": "These are some suggestions-\n1. Create document-level train/dev/test splits with strict exclusion of test docs (and their extracted triples) from CPT. Release the splits and scripts now.\n2. Evaluate on established QA and multi-doc datasets (e.g., NQ-Open, HotpotQA, Multidoc2Dial, BioASQ/PubMedQA/LegalBench-RAG) and at least one real non-QA decision task with human labels.\n3. Replace the Insight Identifier with query rewriting/keyword extraction; replace the Insight Retriever with (a) top-k passage retrieval + reranker; (b) CPT-only generator without the identifier; (c) non-parametric triple retrieval."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tae7j0hZ7W", "forum": "SosBN9PjJ2", "replyto": "SosBN9PjJ2", "signatures": ["ICLR.cc/2026/Conference/Submission20265/Reviewer_fDW5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20265/Reviewer_fDW5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014081047, "cdate": 1762014081047, "tmdate": 1762933746171, "mdate": 1762933746171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Insight-RAG, introducing a dual intermediary insight extraction component in the RAG pipeline to address deeply buried information in knowledge bases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive assessment of the methodology with multiple SOTA LLMs\n- Authors provided a computational complexity analysis with cost implications and prompt sensitivity"}, "weaknesses": {"value": "- Lack of methodology motivation and benchmark choice explanations with respect to the paper objective (retrieve deeply buried information) - which otherwise remains overall vague. \n- The authors claim that the contribution is represented by the coupling between the identifier and the retriever, yet these appear to be separately “prompted/trained” components \n- Triples not defined, examples would be welcome\n- (nit) Figure 6 does not contribute much in itself - Z scores are not very readable, could be moved in text / table"}, "questions": {"value": "- Are there other relevant RAG variants worth including in the comparison?\n- How does the proposal compares to a fine tuned reranker? With respect to ColBERT, Figure 9 and 10 could benefit from direct comparison with the authors methodology (main text)\n- What is the rationale of CPT for an insight retriever? Why not other (SFT, RL) approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FzPSNhqvsb", "forum": "SosBN9PjJ2", "replyto": "SosBN9PjJ2", "signatures": ["ICLR.cc/2026/Conference/Submission20265/Reviewer_w3b3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20265/Reviewer_w3b3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139037948, "cdate": 1762139037948, "tmdate": 1762933745831, "mdate": 1762933745831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional retrieval mechanisms often fail to capture nuanced insights required for complex tasks and may neglect relevant insights dispersed across multiple documents. To deal with this problem, the authors propose a new method, Insight-RAG, that firstly identifies necessary insights to solve a task and then feeds the identified insights to a large language model (LLM). Experimental results on the newly created benchmark based on conventional two scientific paper datasets show that Insight-RAG can outperform strong baselines in accuracy with much less contextual information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method, Insight-RAG, contains components prepared specifically for capturing nuanced insights and handling multiple documents, addressing the fundamental issues in the conventional RAG.\n- The authors created a new benchmark based on two conventional scientific paper datasets.\n- The experiments use various large language models (LLMs).\n- The experimental results show the large gains by using Insight-RAG.\n- The analysis shows the contribution of each component."}, "weaknesses": {"value": "- Even though Insight Retriever is fine-tuned by LoRA, the RAG baselines use retrievers without fine-tuning. This is an unfair comparison.\n- The datasets used in the experiment are restricted to the scientific paper domain.\n- The increased runtime caused by Insight Retriever is not reported."}, "questions": {"value": "Does Insight Retriever work without fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AvU1i6Hobv", "forum": "SosBN9PjJ2", "replyto": "SosBN9PjJ2", "signatures": ["ICLR.cc/2026/Conference/Submission20265/Reviewer_fcRV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20265/Reviewer_fcRV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762192265454, "cdate": 1762192265454, "tmdate": 1762933745321, "mdate": 1762933745321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}