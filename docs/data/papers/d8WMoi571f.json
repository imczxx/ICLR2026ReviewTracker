{"id": "d8WMoi571f", "number": 8994, "cdate": 1758106420652, "mdate": 1759897749438, "content": {"title": "Revisiting Confidence Calibration for Misclassification Detection in VLMs", "abstract": "Confidence calibration has been widely studied to improve the trustworthiness of predictions in vision-language models (VLMs). However, we theoretically reveal that standard confidence calibration inherently _impairs_ the ability to distinguish between correct and incorrect predictions (i.e., Misclassification Detection, MisD), which is crucial for reliable deployment of VLMs in high-risk applications. In this paper, we investigate MisD in VLMs and propose confidence recalibration to enhance MisD. Specifically, we design a new confidence calibration objective to replace the standard one. This modification theoretically achieves higher precision in the MisD task and reduces the mixing of correct and incorrect predictions at every confidence level, thereby overcoming the limitations of standard calibration for MisD. As the calibration objective is not differentiable, we introduce a differentiable surrogate loss to enable better optimization. Moreover, to preserve the predictions and zero-shot ability of the original VLM, we develop a post-hoc framework, which employs a lightweight meta network to predict sample-specific temperature factors, trained with the surrogate loss. Extensive experiments across multiple metrics validate the effectiveness of our approach on MisD.", "tldr": "", "keywords": ["Vision-language models", "Confidence calibration", "Misclassification Detection"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50be4873c57100664b43f7d28241dc7b21198c1f.pdf", "supplementary_material": "/attachment/771b57b17575cf79f00064fd16fe6e25bd8200b0.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses misclassification detection (MisD) in vision-language models like CLIP. The author first shows that the objective of confidence calibration is misaligned with MisD. Then, the authors theoretically analyze the limitations of standard calibration objectives in differentiating correct from incorrect predictions. To solve the issue, they design a differentiable surrogate loss to increase separation between correct and incorrect predictions. Extensive experiments across six datasets demonstrate that LMN can improve MisD performance compared to recent calibration or OOD detection methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation is well-structured. The paper first clearly distinguishes the goal of confidence calibration from misclassification detection, and show that a well-calibrated model can be worse for MisD than a poorly-calibrated model.\n2.\tThe theoretical analysis is clear. The author links the area under/above the curve in the reliability diagram to the precision of detecting correct/incorrect predictions, and show that even a perfectly calibrated mode has limited MisD precision, bounded by the conditional expectation of confidence.\n3.\tThe experimental results are technologically sound and generally support the author’s claim. The author show that previous calibration can not bring significant improvement in MisD and the proposed method LMN shows consistent improvements across diverse datasets."}, "weaknesses": {"value": "1.\tLack of quantitative analysis for the motivation. The paper provides a strong conceptual illustration of the failure of calibration in Figure 1. However, it lacks a quantitative experiment to directly quantify the MisD performance after calibration (e.g., TS[1] or ProximityBias [2]).\n2.\tThe connection between the proposed method and VLM is weak. The proposed LMN relies only on output logits. Consequently, the method appears applicable to any classifier rather than being inherently tied to VLMs. The author carefully discusses the role of the proposed LMN in VLM.\n3.\tMissing comparison with other failure detection methods like [3]. The experiments only compare with general calibration or OOD detection methods for VLM. \n4.\tUndefined loss notation. $L_{LDA}$ in Appendix E.2 is not defined anywhere in the paper. It looks like a typo for $L_{SUR}$.\n\n[1] On calibration of modern neural networks. ICML, 2017.\n\n[2] Proximity-informed calibration for deep neural networks. NeurlPS, 2023.\n\n[3] Rethinking confidence calibration for failure prediction. ECCV, 2022."}, "questions": {"value": "Questions:\n1.\tHow does LMN affect overall calibration performance?  It is unclear whether LMN compromises or preserves global calibration alignment.\n2.\tHow does DOR perform in the open-vocabulary setting? Table 2 presents open-vocabulary results for the proposed method. However, it does not compare with open-vocabulary calibration methods like DOR [4].\n\n[4] Understanding and mitigating miscalibration in prompt tuning for vision-language models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Ty0claBtv", "forum": "d8WMoi571f", "replyto": "d8WMoi571f", "signatures": ["ICLR.cc/2026/Conference/Submission8994/Reviewer_x3hT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8994/Reviewer_x3hT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381418812, "cdate": 1761381418812, "tmdate": 1762920723402, "mdate": 1762920723402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the connection between confidence calibration and misclassification detection (MisD), revealing that standard calibration methods limit misclassification detection (MisD) performance. The authors propose a new reliability curve designed to improve the precision of detecting both correct and incorrect predictions by better separating them. To address the non-differentiability of this new curve, a surrogate loss is introduced, and a lightweight post-hoc framework employing a meta network for sample-specific temperature scaling is developed to preserve the predictive power of Vision-Language Models (VLMs). Both theoretical analysis and experiments demonstrate that this approach consistently enhances MisD performance without sacrificing model accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and yet complex challenge in any classification network. The authors focus on VLMs and misclassification performance. The paper nicely combines a mathematical founded treatment and empirical experiments to validate their findings. This provides a robust backdrop for the presented scientific approach which can be followed with some investment for any reader. Moreover, the topic it addresses is of interest to practical applications of VLMs.\n\nFurther:\n- Publicly available, reproducible code.\n- Comprehensive theoretical analysis supported by empirical results.\n- Clear originality: reframing the relationship between calibration targets and separability of correct vs. incorrect predictions.\n- Consistent improvements demonstrated across datasets and settings.\n- Post-hoc design maintains the zero-shot generalization ability of VLMs while enhancing reliability."}, "weaknesses": {"value": "The paper suffers from very imprecise language while presupposing a lot of knowledge from the reader. Moreover, does the style of the paper recall a blog post at many points. The mathematical treatment of the subject eliviates this to some extent as it provides a rigorous means of communication. However, especially towards the end of the report where the empirical findings are meant to support the theoretical treatment, the paper lacks scrutiny (missing uncertainty assessment of the presented results in Table 1 and 2 as well as no uncertainty estimates in figure 3 and 4). This renders the presentation of evidence weak as many of the presented performance metrics only differ by singular percentages to the competition.\n\nFurther:\n- The originality of the surrogate loss formulation is somewhat unclear. The core novelty may primarily lie in the use of a sigmoid-shaped target curve, and it would help to clarify how this extends beyond a standard activation-based reformulation.\n- The distinction between the *original calibration method* (theoretical objective) and the *post-hoc meta-network implementation* is often ambiguous, especially in the Experiments and Evaluation sections.\n- The choice of (r=0.5) in defining the new target calibration curve (Figure 2) should be theoretically or empirically motivated.\n- Some notations (especially those connecting the reliability curve to the surrogate penalty) are heavy, making it hard for readers to see how the calibration diagonal relates to the standard loss.\n- The paper does not explore implications for related uncertainty-based tasks (e.g., OOD detection) or visualize the distribution of confidence scores.\n- Key summary metrics such as **ECE** and **Accuracy** are missing from the results tables, though both are important for assessing calibration and interpreting AUROC.\n- Some conclusions slightly overstate empirical findings (e.g., \"but also empirically corroborate the theoretical analysis, validating the advantages of the normalized sigmoid reliability curve in MisD.\" - based on the calibration curve in Figure 4, which is not defined enough for me to agree that this corroborates the theoretial analysis).\n- Information about dataset size, accuracy and diversity are missing, making it hard to understand how generalizable these findings are"}, "questions": {"value": "- line 32: \"easily deployed\" please remove such heuristic andcolloguial statements\n- line 35: \"the reliability of VLMs is often overlooked\" this is true for many deep learning models, please remove\n- line 36: \"has emerged as a key research direction for building reliable VLMs.\" this is true for many deep learning models, please remove\n- line 44-49: \"To address the former, ...\" such relative statements are hard to follow. Please reformulate these lines for better readability and clarity\n- page 2, Fig 1:\n    + many coloers in the plot, this hinders readability\n    + remove superfluous backgrounds (grey)\n    + remove emojis, the reader should make interpretations not the figure\n    + axis title of fig 1b, should be confidence*s*\n    + the threshold in fig 1b is very small\n    + remove superflous boxes with rounded corners (this distracts the very complicated figure)\n    + caption: remove the literal reference to concrete confidence scores, these should go into a table if important\n    + line 67-71: this is one long sentence, please reformulate\n- line 75-80: \"To remedy\" is one long sentence, please split it up for readibility\n- line 87-96: the bullet points repeat the text above, please keep only one to avoid confusion and leave space for other improvements\n- line 106: `sim` is not explained anywhere\n- line 108: Start sentence, \"Where the \"{class}\" is filled.\n- line 148: \"diagonal (y=x)\" which should be \"diagonal (acc = s)\", possibly repeat that you mean accuracy versus confidence\n- page 3: lemma 3.1 appears to be independent of `r` - if I confused this, please consider reformulating as the math suggests as much\n- page 4, equation 5: $\\lambda$ should be included in the function definion left of $\\Psi$ to indicate a dependency\n- page 5, line 254/255: \"and is more flexible than perfect calibration\" this appears to be unwarranted claim, please remove or reformulate\n- page 6, line 283-292: very hard to follow paragraph. Please consider reformulating it to highlight central ideas.\n- Table 1: no error bars or statistical treatment which elucidates competitiveness, not discussion of variability of metric results in the text too\n- line 360: \"we adopt widely used metrics\" this is jargon, please remove.\n- figure 3: only small differences visible, an uncertainty treatment would make this plot stronger\n- line 413: very general claim about universality of your approach, as the data can be consdired weak, please remove\n- table 2: no error bars or uncertainty treatment"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r1zHGvUG0E", "forum": "d8WMoi571f", "replyto": "d8WMoi571f", "signatures": ["ICLR.cc/2026/Conference/Submission8994/Reviewer_Mme6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8994/Reviewer_Mme6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838015172, "cdate": 1761838015172, "tmdate": 1762920723060, "mdate": 1762920723060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies misclassification detection (MisD) for VLMs and argues that standard confidence calibration can hinder MisD. Specifically, the author argues that the objective of aligning confidence with accuracy is theoretically at odds with the goal of MiSD, which requires a clear ranking between correct and incorrect predictions. To address this issue, this paper proposes a recalibration objective based on a normalized sigmoid reliability curve, which is designed to maximize the separation between correct and incorrect predictions at every confidence level. Since this objective is non-differentiable, the authors introduce a differentiable surrogate loss. To preserve the original VLM's capabilities, they implement this via a post-hoc, lightweight meta-network that predicts instance-specific temperature scaling factors. Empirical experiments on multiple datasets demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core argument that perfect calibration inherently limits MiSD performance is technically sound.\n2. The empirical results demonstrate the effectiveness of the proposed method.\n3. The paper is well written."}, "weaknesses": {"value": "1. The parameter $\\lambda$, which controls the sharpness of the sigmoid curve, is crucial for balancing separation strength and stability. While a parameter analysis is provided, the paper could offer more concrete guidance on how to select $\\lambda$ in practice for a new dataset, rather than treating it purely as a hyperparameter to be tuned.\n2. Results focus on CLIP ViT-B/32 (plus prompt variants). Including B/16, L/14, or OpenCLIP variants, and at least one non-CLIP VLM would benefit this paper.\n3. The “lightweight” claim is plausible, but concrete numbers for training/inference overhead would still benefit the paper."}, "questions": {"value": "1. In the surrogate loss (Eq. 8), the fusion function $\\phi$ can be summation or multiplication. Table 4 shows it is set to \"summation\" for most datasets but \"multiply\" for RESICS45 and CUB. What is the intuition behind this choice? Is there a clear rule or characteristic of a dataset that would guide this selection?\n2. In Table 1, the performance of the proposed method on Flowers102, RESICS45, and CUB seems marginal. Could you analyze the reason? And does this mean the effectiveness of the proposed method is dataset dependent?\n3. The meta-network uses features from both image and text modalities. Could you provide an ablation or insight to understand the relative importance of each modality's features (image embedding, text embedding, logits) for predicting the effective temperature factor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ueQuDMPU7", "forum": "d8WMoi571f", "replyto": "d8WMoi571f", "signatures": ["ICLR.cc/2026/Conference/Submission8994/Reviewer_jRJV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8994/Reviewer_jRJV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850080875, "cdate": 1761850080875, "tmdate": 1762920722825, "mdate": 1762920722825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses misclassification detection in VLMs, showing that standard confidence calibration—though improving ECE can theoretically and empirically hurt MisD. The authors analyze reliability diagrams, proving that standard calibration caps MisD precision. They introduce a new reliability curve, a normalized sigmoid, to reshape calibration toward better MisD separation, and design a differentiable surrogate loss to optimize it. Finally, they propose a lightweight post-hoc meta-network (LMN) that predicts sample-specific temperature coefficients, ensuring compatibility with pretrained CLIP models. Experiments on six datasets show MisD gains across metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is well-stated and supported with some empirical observations.\n\n2. The paper provides theoretical proofs (Appendix B) showing the new curve’s precision dominance and lower entropy."}, "weaknesses": {"value": "1. The paper mostly compares against calibration (FeatureClipping, DOR) and an OOD method (SCT), while noting FSMisD and ViLU without quantitative results. This leaves open whether LMN is competitive for the actual MisD task.\n\n2. All core results use CLIP ViT-B/32 as the base model. It remains unclear whether LMN generalizes to modern VLMs.\n\n3. While the method is motivated by calibration–MisD tension and uses reliability-curve theory, the paper does not directly report calibration metrics for LMN (ECE/NLL/Brier) on the main setups.\n\n4. The paper fixes 16-shot calibration and evaluates on DTD, Flowers102, EuroSAT, RESICS45, MNIST, CUB. This is a useful starting point but too narrow to support general claims.\n\n5. Given that dealing with distribution shift is important to a calibration or MisD method, please also include setups to show the robustness of proposed method under distribution shift. For example, training on data from ImageNet-Val, and evaluate the ECE and MisD on ImageNet-Sketch and ImageNet-A etc."}, "questions": {"value": "1. Please include some recent MisD-oriented methods as the comparison baselines.\n\n2. Add SigLIP (e.g., SigLIP-B/16) or other popular VLMs as the backbone to show the generalization of the proposed method.\n\n3. Beyond the performance on MisD, please also include the calibration performance (ECE and Brier).\n\n4. To train a lightweight neural network, it would be interesting to show the sensitivity against data quantity of the proposed method. Also, would LMN benefit from having more data instances?\n\n5. Train/Calibrate on ImageNet-val, then evaluate MisD + calibration on ImageNet-Sketch and ImageNet-A; report AUROC/AUPR/FPR90, ECE/NLL/Brier."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nOyerWHTWw", "forum": "d8WMoi571f", "replyto": "d8WMoi571f", "signatures": ["ICLR.cc/2026/Conference/Submission8994/Reviewer_GAVj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8994/Reviewer_GAVj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911136112, "cdate": 1761911136112, "tmdate": 1762920722402, "mdate": 1762920722402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}