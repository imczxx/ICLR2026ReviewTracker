{"id": "a1d2smwmBS", "number": 12965, "cdate": 1758212203653, "mdate": 1759897473765, "content": {"title": "Financial fraud collusion among generative AI agents in social networks", "abstract": "In this work, we investigate the risks of collective financial fraud in large-scale multi-agent systems, driven by large language model (LLM) agents. We examine whether agents can collaborate in fraudulent activities, amplify the risks of such behaviors, and identify factors critical to fraud success. To facilitate this research, we introduce MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online behaviors. The benchmark includes 21 typical online fraud scenarios, covering the full fraud lifecycle across both public and private domains. We explore the dynamics of fraud operations by analyzing interaction depth, hype-building effects, and collaboration failures. Finally, we propose two potential mitigation strategies: the development of monitor agents to block malicious agents and fostering group resilience through information sharing. Our findings highlight the real-world dangers of multi-agent fraud and suggest measures for reducing associated risks.", "tldr": "This paper explores the risk of collective financial fraud in agent society and proposes strategies to mitigate it.", "keywords": ["financial fraud", "multi-agent system", "agent society"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4f4f6ff695bdcb9434765a2f4e93a0484c823e9.pdf", "supplementary_material": "/attachment/0b6530253a849d9ddc6cb7af6fb4cc4926128f4a.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the risks of **financial fraud collusion** in multi-agent systems (MAS) driven by LLMs. The authors propose *MultiAgentFraudBench*, a benchmark covering the **entire fraud lifecycle** (public lure, private trust-building, final transfer), and evaluate 16 mainstream LLMs under 21 scenarios. Results show that stronger models have higher fraud success rates, and that **collusion channels significantly amplify harm**. The paper also proposes preliminary mitigation strategies, including *monitor agents* that detect and block malicious agents, and *group resilience* mechanisms encouraging benign agents to share warnings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Originality**: Introduces one of the first systematic benchmarks to study financial fraud collusion in MAS.\n* **Quality**: Experiments are comprehensive, covering 16 LLMs and multiple fraud scenarios. Ablation studies (scale, ratios, collusion vs. non-collusion) add credibility.\n* **Clarity**: Writing is mostly clear, with strong use of figures/tables to illustrate fraud dynamics.\n* **Importance**: Highlights a timely and socially important problem—the potential misuse of MAS for coordinated fraud—and provides a foundation for further work in mitigation."}, "weaknesses": {"value": "* Scenarios are fully synthetic (LLM-generated posts), which weakens external validity. More discussion or comparison with real-world fraud cases is needed.\n* The alignment failure is tested by forcing malicious system prompts, which may exaggerate risks compared to real-world deployment.\n* Proposed defenses (debunking, banning, resilience) are interesting but technically shallow. Stronger contributions could involve mechanism design at the agent or system level.\n* I recommand you change your bench's name, it's too big as you mainly discuss financial fraud."}, "questions": {"value": "* Can you clarify how realistic they expect *MultiAgentFraudBench* to be when extrapolated to real-world fraud ecosystems?\n* Could benign agents develop **emergent self-defense behaviors** without being explicitly prompted, instead of relying only on prescribed resilience mechanisms?\n* How would fraud success interact with recommender system biases in real platforms? Could this be modeled more systematically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IePNp1OoFP", "forum": "a1d2smwmBS", "replyto": "a1d2smwmBS", "signatures": ["ICLR.cc/2026/Conference/Submission12965/Reviewer_U9v1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12965/Reviewer_U9v1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533945805, "cdate": 1761533945805, "tmdate": 1762923721505, "mdate": 1762923721505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates collective financial fraud behaviors among large language model (LLM) agents within multi-agent social systems. The authors propose MultiAgentFraudBench, a large-scale benchmark simulating 21 realistic online fraud scenarios across public and private domains. Using this framework, they analyze how interaction depth, model capability, and agent collusion amplify fraud success. Experiments involving 17 major LLMs show that stronger reasoning models often exhibit higher fraud effectiveness, while current safety alignments fail to prevent malicious compliance. The study also reveals that collusive communication channels significantly magnify harm and that prolonged dialogues increase vulnerability. Finally, the authors explore mitigation strategies at content, agent, and society levels—such as debunking, banning, and collective resilience—to reduce fraud propagation. Overall, the work highlights critical safety risks in autonomous multi-agent systems and provides a valuable foundation for studying malicious coordination and countermeasures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work raises a novel and important question: whether multi-agent systems can engage in collusive fraudulent behaviors through coordinated actions.\n2. Building on this problem, the paper introduces a large-scale benchmark covering 21 full-cycle financial fraud scenarios, enabling systematic evaluation of fraud emergence and safety vulnerabilities.\n3. The proposed evaluation metrics effectively verify the existence of collaborative fraud and reveal weaknesses in current safety mechanisms.\n4. Two mitigation strategies are designed to alleviate collusive fraud, enhancing the security and ethical deployment of collaborative AI systems in social contexts."}, "weaknesses": {"value": "1. The study lacks validation in real-world settings. Although the framework covers 21 simulated scenarios, it simplifies financial fraud into a binary opposition between “benign” and “malicious” agents. In reality, user behaviors are more diverse, and such simplification may limit the framework’s realism and generalizability.\n2. The agent design is overly simplistic. Each agent is defined by five positive personality traits, which may not accurately reflect the heterogeneity and psychological diversity of real populations. Moreover, individuals differ in their awareness and resistance to financial scams, but these variations are not modeled in the simulation.\n3. The proposed mitigation strategies also have limitations. The agent-level banning assumes clear detection without considering deceptive or disguised malicious behavior and may incur high computational costs in large-scale deployment. Meanwhile, the society-level collective resilience strategy relies on voluntary cooperation rather than systematic enforcement, which may not consistently hold in real-world environments and thus limits reliability."}, "questions": {"value": "1. How consistent are the simulation results with real-world financial fraud behaviors? Further validation in realistic environments would strengthen the study’s credibility.\n2. Have the authors considered testing agents with adverse or malicious personality traits? Incorporating agents with varying levels of vigilance or susceptibility to fraud could better reflect real-world population diversity.\n3. Could the authors provide more detailed case analyses of both successful and failed fraud attempts to illustrate behavioral dynamics more clearly?\n4. How would the proposed mitigation strategies perform under real adversarial conditions, where malicious agents actively adapt or disguise their behaviors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KULdrTlxsD", "forum": "a1d2smwmBS", "replyto": "a1d2smwmBS", "signatures": ["ICLR.cc/2026/Conference/Submission12965/Reviewer_c5hA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12965/Reviewer_c5hA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728929333, "cdate": 1761728929333, "tmdate": 1762923720951, "mdate": 1762923720951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript presents a systematic investigation into the potential risks of collusive financial fraud conducted by multi-agent systems powered by large language models (LLMs) on social platforms. Building upon the OASIS social simulation framework, the authors develop the first comprehensive benchmark, MultiAgentFraudBench, that covers both public and private domains across the full fraud lifecycle (hooking, trust building, and payment request). Through large-scale simulations involving hundreds of benign and malicious agents, the study quantitatively evaluates major LLMs in terms of conversion and propagation rates, revealing a consistent trend that more capable models exhibit higher fraud risks. It further examines the differential effectiveness of content-level, agent-level, and social-level mitigation strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This study leverages the advanced LLM-based multi-agent framework OASIS to investigate a topic of substantial practical significance, representing a noteworthy contribution.\n- The manuscript includes a comprehensive range of large language models, reflecting considerable research effort.\n- The broad coverage of fraud patterns and the simulation design are rigorous and well-founded.\n- Extensive experiments are conducted, yielding several interesting findings, for example, that small social circles achieve higher fraud success rates than large ones when the number of interaction rounds is limited, and that more capable LLMs can successfully deceive less capable ones in adversarial settings between fraudsters and victims."}, "weaknesses": {"value": "- Some conclusions appear somewhat trivial, such as an increase in normal users reduces the overall fraud success rate.\n- Although collusion is emphasized as the core fraudulent mechanism, it is defined only by the existence of private communication among fraudsters, which seems insufficient. The paper lacks deeper methodological design and experimental analysis focused on collusion itself; a clearer and more robust definition of collusion would strengthen the work.\n- Finally, the findings do not strongly highlight the distinct advantages or necessity of using LLMs, i.e. similar conclusions might plausibly be obtained with traditional generative AI models serving as agents."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "edYFEcX8SC", "forum": "a1d2smwmBS", "replyto": "a1d2smwmBS", "signatures": ["ICLR.cc/2026/Conference/Submission12965/Reviewer_1afb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12965/Reviewer_1afb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751900115, "cdate": 1761751900115, "tmdate": 1762923720541, "mdate": 1762923720541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to assess the risks of collaboration among malicious agents for fraud. The methodology relies on running a pure-agent society where agents have distinct personalities and individual goals. Agent societies consist of multiple agents with individual goals and characteristics.\n\nThe MultiAgentFraudBench extends the OASIS framework, which simulates a social media platform with LLM agents. The simulation follows time steps. Every agent acts in every time step. MultiAgentFraudBench extends OASIS to the private domain by adding private messaging. The benchmark consists of 100 benign agents (simulating platform users) and 10 malicious agents.\n\nGeneration of benign agents (users): Benign agents are generated with an age uniformly between 18-65, and personality as drawn from normal distributions on the Big Five dimensions (Openness to Experience, Conscientiousness, Extraversion, Agreeableness, Neuroticism). Furthermore, users hold sensitive information (social security number, Bank Card, PIN) and assets (bank balance).\n\nAction space: Both benign/malicious agents have the same action space (e.g. \"like\", \"post\", \"comment\" and \"private message\"), and act with the same distribution to avoid trivial detection.\n\nObservation space: benign and malicious agents see the same posts (\"the public domain\") and their own private messages (\"the private domain\"). Furthermore, malicious agents know about each other.\n\nThe fraud success is evaluated with two metrics: 1) $R_{conv}$ Conversation-level fraud success (successful persuasion percentage of private chats) 2) $R_{pop}$ Population-level fraud impact (defrauded percentage of benign population.)\n\nWith this setup, the paper considers a range of LLM-models for malicious agents and finds that fraud success is correlated with general capability, with DeepSeek-R1 reaching a maximum of $41.0$ for $R_{pop}$ and Gemini-Flash-2.5 reaching $76.5$ for $R_{conv}$. They inspect what affects fraud success and find that longer interactions (higher interaction depth) increase fraud success, as well as public domain collusion.\n\nThe authors offer three methods for mitigating fraud. \n1) Debunking: at midpoint of run, all malicious posts and private methods are tagged with risk labels. They find that $R_{pop}$ decreases but $R_{conv}$ doesn't. \t\n2) Banning: add a monitoring LLM-agent that classifies and removes agents exceeding a threshold of maliciousness. Both $R_{pop}$ and $R_{conv}$ go down significantly, and the model achieves detection accuracy of 1.0 \n3) Group Resilience. Benign actors may make awareness of posts in the public domain. This reduces both $R_{pop}$ and $R_{conv}$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Benchmark\nThe paper offers a comprehensive simulation of a social media platform with posting, liking, commenting, and private messaging. The fraud types included are broad and realistic.\n\n2) Findings\nFindings are interesting and relevant to the research questions.\n\na) LLMs rarely refuse actions, even when the malicious intent is obvious. The finding suggests that interactive settings are an effective way to \"jailbreak\" models and have them act maliciously.\n\nb) Ablation: Collusion of malicious agents almost doubles the fraud success rates. This underlines the risk of collaboration in fraud tasks.\n\nc) User Banning by an LLM detection model performs very well. This is a promising result for fighting fraud."}, "weaknesses": {"value": "1) Key assumption of human emulation: \n\nFor this experiment to be an accurate benchmark of the risk, it is assumed that the targets (the benign LLM-agents) emulate the susceptibility of human (real users). However, this assumption may be questionable especially because the model used to simulate benign users (Qwen-2.5-32B-Instruct) is of lower general capability than some of the malicious models (like DeepSeek-R1).\n\n2) Collaboration: \n\nThe amount of collusion (collaboration between malicious agents) is not quantified. I would like to see statistics on the number of agents working together for a typical fraud success in the appendix, and more elaborate statistics on the amount of collusion in the public domain as in section 5.2.\n\n3) Bias to action: \n\nAs mentioned in Appendix C.1, the activation probability in a time step is $1$. Does this cause an \"action bias\" to benign actors? If at every time step they are \"forced to act\", are they more likely to interact with malicious actors? Many typical social media users are \"lurkers\", and this may decrease fraud susceptibility.\n\n4) Relationship Network Connection:\n\nThe relationship between users is generated by an ER random graph with $p = 0.1$. However, social media network connectivity can be more realistically modelled by a Bárábasi-Albert scale-free model, as in OASIS.\n\n5) Debunking as a mitigation method: \n\n•\tThe debunking model (labelling all posts with perfect knowledge as \"fraudulent\") is simplistic.\n\n•\tIn general, this method assumes perfect knowledge of maliciousness. However, this should be done with a detector model.\n\n•\tPrivacy concerns in debunking in the private domain.\n\n6) Group resilience as a mitigation method: \n\nHow well does the result of group resilience (i.e. posting fraud awareness posts) generalize to human users? In LLMs, putting fraud awareness in the context window seems likely to decrease their susceptibility, but humans may have more selective attention spans and ignore awareness posts. This again raises the question of whether using LLMs to judge humans' fraud susceptibility is appropriate."}, "questions": {"value": "- It is interesting to see that malicious agents can effectively coordinate to defraud other LLMs, but I am not convinced that a 32B LLM is a realistic emulation of a human acting on a social media platform. Can you address this concern?\n\n- Can you quantify the increase in fraud success through collaboration over solo-acting malicious agents?\n\n- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZL5YRczM3v", "forum": "a1d2smwmBS", "replyto": "a1d2smwmBS", "signatures": ["ICLR.cc/2026/Conference/Submission12965/Reviewer_1Neb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12965/Reviewer_1Neb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936166514, "cdate": 1761936166514, "tmdate": 1762923720060, "mdate": 1762923720060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}