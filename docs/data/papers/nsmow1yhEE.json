{"id": "nsmow1yhEE", "number": 13423, "cdate": 1758217642726, "mdate": 1759897438503, "content": {"title": "Is Extending Modality The Right Path Towards Omni-Modality?", "abstract": "Omni-modal language models (OLMs) aim to integrate and reason over diverse input modalities—such as text, images, video, and audio—while maintaining strong language capabilities. Despite recent advancements, existing models, especially open-source ones, remain far from true omni-modality, struggling to generalize beyond the specific modality pairs they are trained on or to achieve strong performance when processing multi-modal inputs.\nWe study the effect of extending modality, the dominant technique for training multimodal models, where an off-the-shelf language model is fine-tuned on target-domain and language data.\nSpecifically, we investigate three key questions: (1) Does modality extension compromise core language abilities? (2) Can model merging effectively integrate independently fine-tuned modality-specific models to achieve omni-modality? (3) Does omni-modality extension lead to better knowledge sharing and generalization compared to sequential extension?\nThrough extensive experiments, we analyze these trade-offs and provide insights into the feasibility of achieving true omni-modality using current approaches.", "tldr": "", "keywords": ["Omni-Modality", "Modality Extension", "Model Merge"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3170ad523a7ee29005d6520a0cfcca5d6ba08550.pdf", "supplementary_material": "/attachment/4ceda7094bf2d23ac1e95f1fdec2229187de2f1d.zip"}, "replies": [{"content": {"summary": {"value": "This paper examines whether extending the number of modalities in large multimodal models inherently leads to stronger performance. The authors propose a systematic evaluation framework that incrementally expands modalities from text-only (T) to text–vision (T+V), text–vision–audio (T+V+A), and full multimodal (T+V+A+Vd) setups. Two quantitative metrics—Direct Modality Gain (DMG) and Cross-Modality Synergy (CMS)—are introduced to measure the marginal benefit of each modality and the interaction among modalities. Experiments across multiple benchmarks (MMBench, MMMU, VideoMME, AudioSetQA) and model families (LLaMA, Qwen2.5, and LLaVA-Video-Qwen2) show that while visual input offers large improvements over text-only baselines, adding audio and video modalities yields diminishing or even negative returns. The authors conclude that **alignment quality and fusion efficiency matter more than the sheer number of modalities**. Overall, the work provides a clear, structured analysis that challenges a widely held assumption in multimodal learning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **Novel and insightful research question.**\n   The paper tackles a fundamental yet underexplored problem in multimodal learning — whether adding more modalities inherently leads to stronger models. This question is both theoretically meaningful and practically important, especially as large multimodal models (e.g., GPT-4o, Gemini) continue to expand their modality scope.\n\n2. **Systematic and interpretable experimental design.**\n   The incremental setup (T → T+V → T+V+A → T+V+A+Vd) provides a clear and reproducible framework for measuring the marginal gains and interactions of each modality. The proposed quantitative metrics, Direct Modality Gain (DMG) and Cross-Modality Synergy (CMS), are simple yet effective tools to analyze modality contributions in a principled way."}, "weaknesses": {"value": "1. **Reduced data exposure and suboptimal multimodal convergence.**\n   When additional modalities are introduced, the effective data exposure per modality decreases. As a result, the multimodal fusion training may not fully converge to its optimal state, especially when the data ratio among modalities is not carefully balanced. Properly adjusting exposure and sampling across modalities is crucial for achieving stable multimodal alignment.\n\n2. **Insufficient model capacity for multimodal integration.**\n   The backbone used in this paper (7B parameters) is relatively small for complex multimodal fusion, especially when integrating three or more modalities. Larger backbones may exhibit different behavior and could potentially overturn the current conclusions about diminishing returns from modality expansion.\n\n3. **Tokenizer design may fundamentally affect cross-modal alignment.**\n   The paper does not explore the impact of tokenizer design, yet a *unified tokenizer* shared across modalities can substantially enhance cross-modal representation learning and alignment. The absence of such a unified tokenization scheme might significantly influence the observed performance trends after modality fusion."}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M9Fxb3dsy0", "forum": "nsmow1yhEE", "replyto": "nsmow1yhEE", "signatures": ["ICLR.cc/2026/Conference/Submission13423/Reviewer_GhJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13423/Reviewer_GhJ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761138743734, "cdate": 1761138743734, "tmdate": 1762924047276, "mdate": 1762924047276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a comprehensive empirical investigation of the effects of modality fine-tuning and model merging on large language models. The paper's novelty is very weak, the analysis is descriptive rather than explanatory, and the proposed techniques are heuristic and without deeper insight or stronger innovation, which is lower the bar of the ICLR’s main track."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically analyzes two major strategies for achieving omni-modality, covering text, image, video, and audio. \n\n2. Clear introduce of used multimodal benchmarks,and well-chosen metrics."}, "weaknesses": {"value": "1. The paper's novelty is largely lower than the bar of the top-tier conference ICLR, which does not draw out some insight and novel conclusion. **The answers to the three main questions listed in the abstract even have already been studied by the previous relevant works**. For example,\n* [a] analyzed and conducted experiments to demonstrate that multimodality does not enhance the model's language capability (RQ1);\n* [a] also examined whether multimodal fine-tuning leads to better knowledge sharing and generalization (RQ3). [a] showed that synergy sometimes emerges not only between modalities but also between comprehension and generation capabilities. To some extent, [a] even provides a more thorough and in-depth analysis.\n\n*Reference: [a] On Path to Multimodal Generalist: General-Level and General-Bench, ICML' 25.*\n\n2. The paper mainly reports performance changes without analyzing why these trade-offs arise. No theoretical or mechanistic understanding of modality interference or reasoning degradation is provided.\n\n3. The study concludes that omni-modality fine-tuning is inefficient but does not provide a concrete alternative or improved solution.\n\n4. The paper does not analyze the model's performance in robustness and out-of-distribution behavior, as well as not discussing the scalability when using different strategies.\n\n5. In the paper, the experiments are primarily conducted on LLaVA and Qwen, which makes the conclusions and arguments less convincing. The authors should evaluate more models to strengthen the generality of their findings; otherwise, the observed phenomena may not be broadly applicable.\n\n6. Finally, the authors should cite [a] in their paper and explain the differences between them, as it seems that [a] is very highly-related to this submission.\n\n7. Unclear experimental setup of model merging and omnimodality fine-tuning; the authors should introduce the implementation detalis about these two strategies more clearly."}, "questions": {"value": "Can you provide a mechanistic explanation for why modality fine-tuning most severely impacts reasoning and instruction-following capabilities?\n\nCan you provide  training  efficiency analysis in the all main tables, such as the number of used GPUs and the total training time. This will guaratee the fairness when comapring the two different training strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9n0KZX3Iwr", "forum": "nsmow1yhEE", "replyto": "nsmow1yhEE", "signatures": ["ICLR.cc/2026/Conference/Submission13423/Reviewer_k6At"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13423/Reviewer_k6At"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761483056658, "cdate": 1761483056658, "tmdate": 1762924046959, "mdate": 1762924046959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores three key research questions on the road toward omni-modality:(1) Does modality extension compromise core language abilities? (2) Can model merging effectively integrate in\u0002dependently fine-tuned modality-specific models to achieve omni-modality? (3) Does omni-modality extension lead to better knowledge sharing and generalization compared to sequential extension? Through extensive experiments on Qwen2-VL and LLaVA models, the authors find that modality extension enhances visual knowledge but typically weakens reasoning, instruction following, and safety. They propose a weighted model-merging strategy and further explore small-step fine-tuning on merged models. Their main conclusion is that modality extension alone is may not sufficient for achieving omni-modality, while merging followed by limited fine-tuning may offer a better solution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Deep empirical insight into multimodal learning trade-offs: The paper provides one of the most systematic investigations into how modality extension reshapes an LLM’s internal balance between language, reasoning, and multimodal understanding. By quantifying these effects across diverse benchmarks, it gives the community a clearer, evidence-based understanding of why multimodal expansion may harm reasoning, offering diagnostic insight rather than only performance metrics."}, "weaknesses": {"value": "1. The paper’s conclusions are based mainly on Qwen2-era models, and newer architectures such as Qwen3 already show that the observed trade-offs between language and multimodal performance may not hold universally. This limits how far the conclusions can be generalized.\n2. The paper does not provide a theoretical explanation for why omni-modality fine-tuning or model merging work or fail. Without an analytical view of optimization dynamics or representation sharing, it is difficult to know whether the observed effects are fundamental or just artifacts of specific training settings.\n3. The paper does not discuss how future omni-modal models could move beyond the current limitations. It identifies existing problems but does not provide concrete insights or directions for how the next generation of models might improve.\n4. The presentation of figures and tables could be improved. If each figure caption clearly summarizes the finding, readers can understand the results more quickly."}, "questions": {"value": "1. Do newer models challenge the paper’s conclusion? In Qwen3-VL, multimodal training achieves equal or even better performance than the text-only Qwen3 model on several reasoning and instruction-following benchmarks. This raises the question of whether the trade-offs identified in this paper are fundamental or just reflect the limitations of earlier training paradigms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fXxnoObqhl", "forum": "nsmow1yhEE", "replyto": "nsmow1yhEE", "signatures": ["ICLR.cc/2026/Conference/Submission13423/Reviewer_R61M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13423/Reviewer_R61M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486820529, "cdate": 1761486820529, "tmdate": 1762924046634, "mdate": 1762924046634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates three questions for multi-modal models, 1) whether language capabilities are compromised with modality extension, 2) whether model merging preserves the language capabilities, 3) impact of omni-modality fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles important questions that when well executed would be useful for the overall community. The paper has done a good job in identifying the benchmarks for language capabilities and considered a range of models as well. The methodology of the paper was overall easy to read but the experiments can be improved as discussed below."}, "weaknesses": {"value": "My major concern is the lack of evidences for the claims throughout the paper. For instance,\n* **Visual modality extends the knowledge scope:** The paper highlights that Qwen-VL-Instruct obtains 5% improvement compared with LLaVA. But this inteprertation is based on MMLU-Pro. There is no difference in performance on MMLU. This is further conflated by the use of 1.4T paired samples by Qwen and only around 10M by LLaVA. It is thus also unfair to make claims based on the efficiency of vision over audio as audio only uses 520K samples.\n* **Harms of modality fine-tuning:**\n     * The paper highlights the negative impact of instruction fine-tuning in various aspects. But as the paper also mentions, this has been shown in many prior studies. Thus, making the positioning of the work among prior works unclear. \n     * It should further be explicitly clarified whether the base model was the multi-modal model with frozen LM and modality encoders trained or only the base model. This is important to support the claim about fine-tuning failing to preserve the core language skills. This is also important to dissect the performance from the merging counterpart where the models are frozen as well.\n* **Video enhances long context:** This is unclear a well because we do see a decrease in LLaVA-video which is not discussed in the results. Audio might also long content, but the deterioration from audio is not discussed in the results.\n\nFollowing are points I did not consider for my review but should be fixed or elaborated on:\n* Section 3 can be renamed to Problem setup for modality extension and the subsection titles 3.1 and 3.2 can be omitted. \n* The paper uses different encoders for different modalities but encodes every modality using F in the notations which is confusing. \n* Modality fine-tuning and first paragraph of section 4 contains redundant information that can be merged. \n* Cite the studies on L182 page 4\n\nOverall, I believe the paper highlights claims that are not completely supported by the empirical results. Therefore I recommend rejection in the current state of the paper."}, "questions": {"value": "Please refer to my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LiF82kqC41", "forum": "nsmow1yhEE", "replyto": "nsmow1yhEE", "signatures": ["ICLR.cc/2026/Conference/Submission13423/Reviewer_Cri7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13423/Reviewer_Cri7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979156151, "cdate": 1761979156151, "tmdate": 1762924046244, "mdate": 1762924046244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all the reviewers for their dedicated suggestions. Our paper’s core contributions lie in the analysis of quantified trade-offs of different modality extension methods, the mechanistic understanding of these trade-offs, and we also introduce the weighted average merging method as a promising and concrete solution.\n\n- **For the revision of our submission:**\nWe add the Qwen3-VL series (released in October) experiments and the results are shown below. The results also align with our claims on Qwen2 that 1) ​​visual modality extends the scope of parametric knowledge, 2) Modality fine-tuning harms core language abilities, 3) video modality may enhance the long context ability.\n\n|                      | MMLU | MMLU-Pro | IFEval  | PR  | ZeroScrolls | GPQA | MATH | HumanEval+ |        |         | MMMLU | HarmBench |\n|----------------------|------|----------|---------|-----|-------------|------|------|------------|--------|---------|-------|-----------|\n|                      | Acc  | Acc      | Average | Acc | Acc         | Acc  | Acc  | Pass@1     | Pass@5 | Pass@10 | Acc   | ASR ↓     |\n| Qwen3-4B-Instruct    | 69.3 | 42.8     | 89.2    | 100 | 47.6        | 12.6 | 71.8 | 81.3       | 94.4   | 96.0    | 56.3  | 0.0052    |\n| Qwen3-VL-4B-Instruct | 69.7 | 44.0     | 86.2    | 100 | 76.2        | 12.6 | 66.3 | 80.0       | 89.2   | 93.4    | 55.9  | 0.0057    |\n\n- **Paper revision:**\n    1. We rename Section 3 to \"Problem Setup...\" and remove the subsection titles.\n    2. We correct the typo 'F' (L139) to '$E_{m_{i}}$' to align with our defined notation.\n    3. We merge the redundant descriptions in Section 3.1 and the start of Section 4.\n    4. We add the appropriate citations on L182 to credit prior work.\n    5. We add conclusions in the captions of Table 3 / 4 and Figure 2 to clarify the presentation of these charts.\n    6. We add the citation reviewers mentioned."}}, "id": "urMWtMPjnp", "forum": "nsmow1yhEE", "replyto": "nsmow1yhEE", "signatures": ["ICLR.cc/2026/Conference/Submission13423/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13423/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission13423/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763761142441, "cdate": 1763761142441, "tmdate": 1763761142441, "mdate": 1763761142441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}