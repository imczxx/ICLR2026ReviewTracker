{"id": "eIF1QvC94Z", "number": 5666, "cdate": 1757926376041, "mdate": 1763718064157, "content": {"title": "Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression", "abstract": "While zero-shot diffusion-based compression methods have seen significant progress in recent years, they remain notoriously slow and computationally demanding. This paper presents an efficient zero-shot diffusion-based compression method that runs substantially faster than existing methods, while maintaining performance that is on par with the state-of-the-art techniques. Our method builds upon the recently proposed Denoising Diffusion Codebook Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by sequentially choosing the diffusion noise vectors from reproducible random codebooks, guiding the denoiser’s output to reconstruct the target image. We modify this framework with *Turbo-DDCM*, which efficiently combines a large number of noise vectors at each denoising step, thereby significantly reducing the number of required denoising operations. This modification is also coupled with an improved encoding protocol. Furthermore, we introduce two flexible variants of Turbo-DDCM, a priority-aware variant that prioritizes user-specified regions and a distortion-controlled variant that compresses an image based on a target PSNR rather than a target BPP. Comprehensive experiments position Turbo-DDCM as a compelling, practical, and flexible image compression scheme.", "tldr": "We present Turbo-DDCM, a zero-shot diffusion-based image compression method up to an order of magnitude faster than state-of-the-art, while maintaining competitive compression quality. We also introduce prioritized regions compression for our method.", "keywords": ["image compression", "diffusion models", "diffusion-based image compression", "zero-shot diffusion-based image compression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43fa42dc5b499a82c21984baec0e40a8f8f8643c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the issues of slow speed and high computational resource requirements of DiffC by proposing the Turbo-DDCM method. As a continuation of DiffC methods based on the RCC theory, TurboDDCM well inherits advantages such as zero-shot capability and achieves improvements in speed and performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is reasonably elaborated and well-supported in the subsequent methods and experiments. The time issue is a key concern in the field of image diffusion research based on diffusion architectures, and the authors have properly proposed a new noise reconstruction method to tackle this.\n2. The structure of the paper is clear and well-written."}, "weaknesses": {"value": "1. The authors only conducted time comparisons within the RCC field. It would be valuable to know the speed comparison with diffusion compression methods based on condition introduction and fine-tuning. Specifically, whether RCC-based image compression has time advantages over fine-tuning-diffusion-based image compression, especially given the emergence of many one-step diffusion image compression methods (e.g., StableCodec).\n2. Following the first point, for fewer time steps (e.g., 5 steps or even a single step), does the performance of the proposed method degrade significantly?\n3. The performance improvement of the final results seems not particularly significant, and the comparisons are insufficient. The baselines selected by the authors appear to be consistent with those of DiffC, but DiffC is a work from ICLR25. It is suggested that the authors include more comparisons with recent works from the past year, such as DiffEIC and StableCodec.\n---\nref: \n\n[1] Li Z, Zhou Y, Wei H, et al. Towards extreme image compression with latent feature guidance and diffusion prior[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2024.\n\n[2] Zhang T, Luo X, Li L, et al. StableCodec: Taming One-Step Diffusion for Extreme Image Compression[J]. arXiv preprint arXiv:2506.21977, 2025. (ICCV25 accepted)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y6sPj1cgHa", "forum": "eIF1QvC94Z", "replyto": "eIF1QvC94Z", "signatures": ["ICLR.cc/2026/Conference/Submission5666/Reviewer_f616"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5666/Reviewer_f616"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761197261483, "cdate": 1761197261483, "tmdate": 1762918183972, "mdate": 1762918183972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an efficient zero-shot diffusion-based image compression method, which is based on the Denoising Diffusion Codebook Models (DDCMs) compression scheme. The paper modifies DDCM with Turbo-DDCM and introduces two flexible variants of Turbo-DDCM."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to read.\n2. The paper proposes a method which has a faster speed than existing zero-shot diffusion-based image compression methods."}, "weaknesses": {"value": "1. The novelty and contribution are weak. The author just modified the DDCM, but there is no ablation study to analyze the proposed components.\n2. The presentation is incomplete. There is a lack of quantitative comparison and ablation study. And there are no tables in the whole paper."}, "questions": {"value": "1. What are the backbones of all zero-shot diffusion-based methods? For a fair comparison of speed, the proposed method should have the same backbone as these methods.\n2. Does the author use Turbo-Lora to accelerate Stable Diffusion 2.1?\n3. What is the speed of non-diffusion-based methods? The author should also compare with them.\n4. How to process higher resolutions?\n\nPlease reply to the Weaknesses and Questions. Based on the author's response, I will adjust my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pSLKnL6HhQ", "forum": "eIF1QvC94Z", "replyto": "eIF1QvC94Z", "signatures": ["ICLR.cc/2026/Conference/Submission5666/Reviewer_QvxK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5666/Reviewer_QvxK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762308921901, "cdate": 1762308921901, "tmdate": 1762918183679, "mdate": 1762918183679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Turbo-DDCM proposes a new approach to zero-shot diffusion-based compression. In the original Denoising Diffusion Codebook Models (DDCM) approach, at each denoising timestep, the compression algorithm selects the next noise vector to add to the image from a codebook of K noise vectors, y selecting the one which moves the noisy image closest to the target image. In Turbo-DDCM, the authors instead encode a noise vector by selecting a subset of M of those K random vectors, and taking a linear combination of them, with coefficients either -1 or 1. This effectively allows the authors to select from a combinatorially larger set of noise vectors, getting closer to the target image with each step. With this improved guidance, they can steer the diffusion model to produce the target image in many fewer steps. They empirically show that this method allows for zero-shot image compression using diffusion models which is multiple times faster than prior approaches."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This is a clever idea for improving the computational efficiency of noise-vector selection in zero-shot DDPM-based compression. It clearly works quite well."}, "weaknesses": {"value": "I consider all of these zero-shot methods (DiffC, DDCM, Turbo-DDCM) to be modifications of Theis et al's original DiffC proposal. They all, to various degrees, trade off theoretical elegance/rigor for real-world usability. In the original 2022 algorithm, there's a precisely defined relationship between the diffusion model's log probability of an image and the number of bits needed to compress that image to a certain noise level. Turbo-DDCM achieves state-of-the-art real-world performance, but trades away these theoretical guarantees."}, "questions": {"value": "The other available zero-shot diffusion compression methods (DDCM, and DiffC) have hyperparameters for speeding up their runtime by trading off against their rate/distortion performance. Primarily this means the number of denoising steps performed. It would be very interesting to see how these methods fare as you decrease the number of denoising steps to match Turbo-DDCM.\n\nIt's not obvious to me that the z_t* vectors selected by turbo-DDCM should yield x_{t-1} vectors which are in the same distribution that the diffusion model was trained on? Like the diffusion model was trained to expect specific linear combinations of real images + gaussian noise, at different ratios, for each time step. But when you generate z_t* according to this fancy process, does it still have the same statistics that random noise does, from the perspective of the diffusion model? Like you are selecting these vectors to be correlated with the target image, so I would naively expect them to have different statistics from random noise. But I'm not confident in this assessment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9Dya6kz5wr", "forum": "eIF1QvC94Z", "replyto": "eIF1QvC94Z", "signatures": ["ICLR.cc/2026/Conference/Submission5666/Reviewer_NNvr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5666/Reviewer_NNvr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762337616512, "cdate": 1762337616512, "tmdate": 1762918183184, "mdate": 1762918183184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fast and flexible zero-shot diffusion-based image compression method (i.e., Turbo-DDCM), which reduces the number of required denoising operations and maintains the performance with an improved encoding protocol. Moreover, Turbo-DDCM presents a priority-aware variant that prioritizes regions of interest and a distortion-controlled variant that compresses an image based on a target PSNR. Experiments are performed on Kodak and DIV2K datasets to investigate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Turbo-DDCM has a competitive performance with recent methods in terms of the rate-distortion-perception tradeoff, and it achieves up to an order of magnitude speedup over existing methods."}, "weaknesses": {"value": "1.\tThe experimental section lacks comprehensive quantitative results, such as BD-rate or BD-PSNR.\n2.\tThe workflow description of the proposed method is not concise enough.\n3.\tAlthough the proposed method is faster than the comparison algorithm, it does not offer a performance advantage.\n4.\tThe paper does not analyze the advantages of diffusion-based image compression methods compared to other types of image compression models, such as GAN, CNN, and RNN.\n5.\tNo results tables are provided for better indicating its superiorities over other methods."}, "questions": {"value": "1.\tWhat is the meaning of the symbol C in Eq. (8)?\n2.\tThe author should provide a detailed explanation of the encoding and decoding process in Figure 2.\n3.\tWhat is the meaning of log2(K M) in Eq. (14)?\n4.\tIt is difficult to distinguish the differences among the decoded images in the first row of Figure 1.\n5.\tIn Figure 4, when comparing with other methods, the authors should also provide the results on the DIV2K dataset.\n6.\tThe proposed method should be compared with GAN-based algorithms.\n7.\tWhen comparing computational complexity, the authors should also provide GPU memory usage.\n8.\tThe author should analyze the impact of the values of hyperparameters T, K, and C on performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "983BRqKNVp", "forum": "eIF1QvC94Z", "replyto": "eIF1QvC94Z", "signatures": ["ICLR.cc/2026/Conference/Submission5666/Reviewer_4uEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5666/Reviewer_4uEv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762569648772, "cdate": 1762569648772, "tmdate": 1762918182956, "mdate": 1762918182956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a zero-shot algorithm based on a pre-trained diffusion model to achieve image compression at extremely low bit rates. In essence, this method represents an efficient enhancement of the DDCM approach, transforming the original MP method into MULTI-ATOM and proposing a new bit protocol to further reduce the redundancy in the bitstream generated by the MULTI-ATOM method. The paper presents a plethora of experiments and evidence to validate the effectiveness and reliability of the algorithm. The experiments demonstrate that this approach achieves excellent performance metrics in zero-shot diffusion-based schemes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, with clear expression and concise yet precise explanations of the motivation and methodology.\n2. The theoretical explanations are solid, providing a high level of theoretical underpinning for the approach.\n3. The experiments are comprehensive, comparing all zero-shot diffusion-based image compression baselines and achieving satisfactory performance even with a several-fold acceleration"}, "weaknesses": {"value": "1. Some modules lack sufficient explanations, especially those borrowed from other articles. Providing necessary explanations can help readers better understand the operational mechanisms of the entire algorithm.\n2. The lack of striking innovativeness in the algorithm may be noted, as the entire solution builds upon the pipeline of DDCM and does not surpass DiffC in terms of performance.\n3. I believe confining the baselines to the zero-shot domain is inappropriate. While zero-shot algorithms show promise and are worth researching in diffusion-based image compression schemes, all zero-shot algorithms fundamentally sacrifice rapid inference to reduce or eliminate training costs, whereas compression tasks are sensitive to inference latency. Recently, some solutions based on few-step pre-trained diffusion models [1,2] have achieved outstanding performance with minimal latency. Overall, I suggest that the method should be compared against some non-zero-shot diffusion baselines or explore the potential application of this algorithm in fine-tuning diffusion priors, which could enhance the practical contributions and persuasiveness of the paper.\n\n[1] [TCSVT] RDEIC: Accelerating Diffusion-Based Extreme Image Compression with Relay Residual Diffusion\n\n[2] [ICCV 2025] StableCodec: Taming One-Step Diffusion for Extreme Image Compression"}, "questions": {"value": "1. The article's overview of the overall encoding and decoding process is somewhat vague. It would be beneficial to include a pseudocode  to provide readers with a clearer understanding of the algorithm's encoding and decoding processes.\n2. The use of ROI in the article for flexible bitrate allocation is commendable. However, the paper lacks details on the application of this technology. For instance, is ROI specific to this algorithm only? Can it be integrated with DDCM or other zero-shot schemes? What is the specific process of implementing ROI? Does ROI introduce additional inference latency? These aspects should be further elaborated.\n3. The article showcases the Round Trip Time. What does this specifically refer to? In a compression algorithm, it's essential to separately compare encoding and decoding times.\n4. The article employs a 512 central crop for the Kodak dataset, which is relatively uncommon in the compression field where full-size Kodak testing is more prevalent. Is there a specific reason for this approach? Is it related to high GPU memory usage by the algorithm?\n5. Testing FID on the Kodak dataset may not be appropriate as Kodak comprises only 24 images, and even when divided into 64-pixel patches, reliable FID metrics may not be guaranteed.\n6. CLIC20 is a commonly used dataset in image compression. It would enhance the algorithm's reliability to include CLIC20 in the main experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ybBF5b3qi7", "forum": "eIF1QvC94Z", "replyto": "eIF1QvC94Z", "signatures": ["ICLR.cc/2026/Conference/Submission5666/Reviewer_4dvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5666/Reviewer_4dvB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762650426820, "cdate": 1762650426820, "tmdate": 1762918182555, "mdate": 1762918182555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}