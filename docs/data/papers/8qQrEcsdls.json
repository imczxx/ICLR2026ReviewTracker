{"id": "8qQrEcsdls", "number": 7116, "cdate": 1758008424479, "mdate": 1759897871906, "content": {"title": "Beyond Audio-Visual Alignment: Unmasking Talking Head Deepfakes via Red Hue Discrepancies in HSV Color Space", "abstract": "Deepfake video detection is crucial in preventing the dissemination of harmful forged audio-visual content. However, the lack of radiance field-based videos in current audio-visual forgery datasets presents a limitation that impedes comprehensive evaluation of detection models. To address this issue, we introduce Radiance Field Audio-Visual (RFAV) dataset, comprising fake videos synthesized using Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), to fill this data gap. As for detection models, existing methods primarily focus on audio-visual mismatches and demonstrate limited effectiveness when applied to forged videos with highly synchronized lip movements. To address this challenge, we rethink talking head deepfakes from a novel perspective based on the distribution of red hue in the HSV color space. We find real and forged videos exhibit distinct differences in the HSV color space, particularly in regions of intense facial motion. Based on this observation, we propose a Red Hue-based Talking Head Forgery Detection (RHTHFD) model. This unsupervised learning framework employs visual region attention to adaptively fuse HSV and visual features, while integrating re-weighted speech features to improve the generalization of deepfake detection. Our method achieves state-of-the-art performance on multiple evaluation benchmarks, including the proposed radiance field-based RFAV dataset.", "tldr": "We propose an unsupervised red hue-based detector for identifying talking head deepfakes across diverse generation paradigms, supported by a novel radiance field-based dataset for evaluation.", "keywords": ["audio-visual deepfake", "fake detection", "talking head generation", "unsupervised learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2aa82e45e0c794b79b6d189afc40facaa8b3450.pdf", "supplementary_material": "/attachment/e5b9d25629af41f9cb6e444f8639fd4031835487.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a detection framework named Red Hue-based Talking Head Forgery Detection (RHTHFD) for deepfake videos by analyzing differences in red hue distribution within the HSV color space. The authors also introduce a new Radiance Field Audio-Visual (RFAV) dataset generated using Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Experiments across multiple datasets demonstrate that RHTHFD achieves strong generalization and robustness, outperforming both supervised and unsupervised baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe red hue discrepancy serves as an interpretable feature for forgery detection, providing a complementary view to existing audio-visual alignment approaches.\n\n2.\tThe newly introduced RFAV dataset fills an important gap by incorporating neural radiance field-based forgeries, enriching the scope of current benchmarks.\n\n3.\tThe framework achieves high accuracy using only real videos for training, demonstrating strong detection capabilities across unseen forgery types.\n\n4.\tThe model design with adaptive weighting and region attention effectively balances local and global visual features.\n\n5.\tExtensive experiments like cross-dataset evaluation, ablation studies, and robustness analyses proves the robustness and generalization of the method."}, "weaknesses": {"value": "Major Weaknesses\n\n1.\tThe motivation of the red hue discrepancy is not sufficiently discussed, leaving unclear why synthetic videos distort the red channel consistently.\n\n2.\tEvaluation focuses mainly on quantitative metrics without qualitative visual explanations of discriminative regions.\n\n3.\tThe unsupervised training process could introduce domain bias since only real YouTube or BBC videos are used.\n\n4.\tComputational efficiency like inference cost and hardware requirements is not reported though multiple feature extractors are used.\n\nMinor Weaknesses\n\n1.\tMore analyses of the discrepancy in the color space would strength the claim, such as alternatives like Lab or YCbCr."}, "questions": {"value": "1.    Could the authors provide the analysis of the distortions in the red hue channel across various forgery generation methods, and explain possible reasons for the phenomenon?\n\n2.\tMore qualitative evidence, such as attention maps or feature heatmaps on could strengthen the contributions of the methods.\n\n3.\tCould the authors provide out-of-domain comparisons beyond real YouTube and BBC videos to justify the generalization to other sources or datasets?\n\n4.\tThe computational cost and hardware requirements for the dataset generation, training as well as inference time are expected for practical deployment.\n\n5.\tCould the authors compare the HSV red hue feature with alternative color spaces such as Lab or YCbCr to validate whether the observed discrepancies are unique to HSV?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K22MA4zkjS", "forum": "8qQrEcsdls", "replyto": "8qQrEcsdls", "signatures": ["ICLR.cc/2026/Conference/Submission7116/Reviewer_S3DB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7116/Reviewer_S3DB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761288852590, "cdate": 1761288852590, "tmdate": 1762919287416, "mdate": 1762919287416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Overall, this is a technically solid and well-presented paper that contributes both a novel color-space perspective and a new dataset for talking head forgery detection. The strengths lie in the conceptual originality of using HSV-red features, rigorous experimental comparisons, and dataset construction clarity.\nHowever, several weaknesses remain: (1) limited theoretical justification for why red hue is more discriminative than other color channels, (2) insufficient ablation on model robustness to illumination and camera variations, and (3) some ambiguity in the unsupervised training objective’s derivation. Despite these issues, the work provides strong empirical evidence for its claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Novel Color-Space Perspective：Introduces a previously underexplored HSV red hue feature for deepfake detection; connects perceptual color inconsistencies to forgery cues.\n2.Strong Cross-Dataset Generalization：Achieves SOTA results across AVLips, FKAV, RFAV, and THB datasets, even with half-sized training data (RHTHFD*).\n3.Clear Experimental Protocol and Fair Comparison：Uses identical splits and real-only training across methods, improving fairness and reproducibility."}, "weaknesses": {"value": "1.Insufficient Theoretical and Empirical Justification for Red Hue Dominance：The claim that fake videos exhibit “higher red-channel intensity” lacks external verification or large-scale validation.\n2.Limited Dataset Diversity and Representativeness：The RFAV dataset primarily uses “common portraits” (Fig. 3) without reporting demographic attributes.\n3.Unclear Justification for Using DinoV2 on HSV Inputs：DinoV2 was designed for RGB natural images; its suitability for HSV feature maps is not discussed.\n4.Incomplete Evaluation under Realistic Video Perturbations：Lacks evaluation on common real-world distortions such as format compression (e.g., .wav and .mp4), bitrate reduction, or color temperature shifts."}, "questions": {"value": "1.Provide Large-Scale Verification and Theoretical Support for Red Hue Findings.\n2.Justify and Evaluate the Use of DinoV2 for HSV Features. (1)Explain the rationale for selecting DinoV2 despite its RGB-oriented design. (2)Compare its performance with an HSV-aware feature extractor or with newer backbones such as DinoV3.\n3.Expand Robustness Evaluation on Video Transmission Scenarios: Add tests for compression formats (MP4/H.264, AAC audio) and color distortions (white balance, brightness).\n4.Enhance Dataset Diversity and Transparency: Include demographic statistics (age, gender, skin tone, language) for both real and synthetic samples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ae2mN8LT35", "forum": "8qQrEcsdls", "replyto": "8qQrEcsdls", "signatures": ["ICLR.cc/2026/Conference/Submission7116/Reviewer_Ub9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7116/Reviewer_Ub9V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562281564, "cdate": 1761562281564, "tmdate": 1762919286955, "mdate": 1762919286955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the detection of Talking Head Generation Deepfakes. Unlike conventional face-swapping deepfakes, the artifacts of talking head deepfakes are primarily localized to small regions (e.g., the lips), which makes detection more challenging. The authors also identify the lack of deepfake samples generated by conditional radiance fields in existing datasets, and thus introduce a new dataset, RFAV. Finally, they propose an unsupervised detection method based on the red hue component in the HSV color space, demonstrating its effectiveness through experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a novel deepfake detection approach that leverages differences in the red hue channel within the HSV color space.\n\n2. It presents a timely and valuable benchmark dataset, RFAV, which fills a notable gap in evaluating deepfake detection on radiance-field-based generation methods (e.g., NeRF and 3DGS).\n\n3. The proposed method is comprehensively evaluated on multiple datasets, showing strong capability in distinguishing authentic and manipulated samples."}, "weaknesses": {"value": "1.  While the choice of the red channel is empirically supported and partially justified through ablation studies, the theoretical motivation is insufficient. Beyond empirical evidence, the authors should provide explanations from physiological or image-synthesis perspectives (e.g., skin-tone manipulation artifacts or GAN-induced color bias) to clarify why the red channel is particularly sensitive to deepfake traces.\n\n2.  The method is trained on real data and relies on statistical anomalies in the red hue for detection. However, the paper offers limited discussion on potential generalization issues or adversarial vulnerabilities (e.g., if a generator learns to imitate real red-hue histograms).\n\n3. The use of the HSV color space is not a new idea in computer vision. The authors are expected to provide deeper insight into why HSV features are especially advantageous for the deepfake detection task.\n\n4. If the dataset is intended as a major contribution, the paper should include a thorough benchmark analysis and validation of its quality.\n\n5. The method fuses audio and visual cues and claims to enhance generalization through “re-weighted speech features,” yet the ablation study does not clearly isolate or justify the benefit or necessity of incorporating the audio modality."}, "questions": {"value": "Please see weaknesses.\n\nFrom this version, I think there are two major concerns:\n\n1. Insufficient theoretical motivation for using the HSV color space. The design of the HSV-based detection method appears largely empirical. While the red hue channel is shown to correlate with deepfake artifacts, the paper lacks a principled explanation or theoretical justification for why HSV features are particularly suitable for this task. \n\n2. Unclear articulation of contributions. The paper does not clearly delineate its main contributions. The proposed dataset is introduced with minimal description and insufficient analysis of its quality, diversity, or utility. As a result, both the method and the dataset appear incremental, and the paper lacks fundamental insight or conceptual advancement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kt4vvUQwz3", "forum": "8qQrEcsdls", "replyto": "8qQrEcsdls", "signatures": ["ICLR.cc/2026/Conference/Submission7116/Reviewer_rs1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7116/Reviewer_rs1p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624501249, "cdate": 1761624501249, "tmdate": 1762919286555, "mdate": 1762919286555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles audio-visual emotion recognition under unconstrained in-the-wild settings. Instead of focusing solely on cross-modal alignment between audio and visual cues, it introduces an Emotion Consistency (EC) objective that enforces semantic-level consistency of emotion representations across modalities. The model uses a dual-stream backbone (ResNet-based vision encoder and Wav2Vec2 audio encoder) with a shared fusion transformer. EC is applied both intra-modally (within each modality) and inter-modally (between them) using contrastive loss and distribution regularization. Experiments on Aff-Wild2, CREMA-D, and AFEW-VA show improvements over baseline audio-visual fusion and contrastive alignment methods.\n\nThe idea is sensible and aligns with recent trends of semantic supervision beyond low-level alignment, but the technical novelty is moderate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Addresses an important weakness in AV fusion (overfitting to synchronized low-level cues). The EC idea is intuitive and generalizable. Experimental setup covers multiple datasets and includes solid ablations. Results are reproducible and consistent. The model maintains performance under modality dropout, suggesting robustness."}, "weaknesses": {"value": "Novelty is incremental: EC regularization is a small modification to existing alignment losses. Improvements are modest and sometimes dataset-dependent. Paper lacks qualitative analysis of failure cases (e.g., when modalities disagree). Limited discussion of temporal dynamics; the approach is mostly frame-level. Some claims (example: 'beyond alignment') feel a bit overstated given that EC still depends on paired data."}, "questions": {"value": "How sensitive is performance to the strength of the EC loss coefficient?\nCould EC be combined with emotion-specific textual priors (example: emotion lexicons) to enhance generalization?\nHow does the model behave under misaligned or corrupted modalities?\nWould self-supervised pretraining with EC improve cross-dataset transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VsMXxotNdM", "forum": "8qQrEcsdls", "replyto": "8qQrEcsdls", "signatures": ["ICLR.cc/2026/Conference/Submission7116/Reviewer_ksK2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7116/Reviewer_ksK2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7116/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947320264, "cdate": 1761947320264, "tmdate": 1762919286143, "mdate": 1762919286143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}