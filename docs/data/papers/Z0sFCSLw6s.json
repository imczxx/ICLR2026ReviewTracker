{"id": "Z0sFCSLw6s", "number": 20287, "cdate": 1758304469318, "mdate": 1759896986041, "content": {"title": "Cross-Batch Gradient Consistency for Adaptive Loss Balancing in Knowledge Distillation", "abstract": "Knowledge distillation (KD) is a widely used approach for compressing large neural networks into compact student models by combining supervised learning with teacher-guided alignment. While recent studies have attempted to improve KD through adaptive weighting between the supervised and distillation objectives, most existing methods determine weights solely from gradients computed on a single mini-batch. This batch-local perspective neglects the crucial requirement that student updates should generalize across unseen data, often resulting in gradient conflicts, unstable training dynamics, and suboptimal performance. In this work, we introduce a cross-batch dynamic weighting framework for KD that explicitly incorporates generalization signals beyond the current batch. At each iteration, we leverage an auxiliary batch as a proxy for unseen data, compute its supervised gradient as a reference, and solve a lightweight quadratic program to adaptively select weights that align the combined update direction with this reference. To further stabilize optimization, we normalize task gradients and introduce a scaling mechanism that balances their magnitudes while maintaining computational efficiency. Extensive experiments on standard benchmarks demonstrate that our approach consistently outperforms fixed-weight and batch-local adaptive baselines, leading to more stable optimization and superior student performance. These results highlight the importance of cross-batch consistency in KD and establish our method as a principled and effective strategy for dynamic loss balancing.", "tldr": "", "keywords": ["Knowledge Distillation", "Dynamic Weighting"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/208bc3cdee394b2420856a0fe60d5a7ff54228c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a cross-batch dynamic weighting framework for knowledge distillation (KD), which adaptively balances the cross-entropy (CE) and KL-divergence losses by leveraging an auxiliary batch to approximate unseen data. The method formulates weight selection as a lightweight quadratic program to align the combined gradient direction with the CE gradient from the auxiliary batch, supplemented by gradient normalization and scaling for stability. Experiments on CIFAR-100 and TinyImageNet show consistent improvements over fixed-weight and batch-local adaptive baselines across multiple teacher-student architectures and distillation methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The cross-batch perspective for dynamic weighting in KD is a fresh and principled approach, distinguishing it from prior batch-local methods.\n- Consistent improvements are shown across multiple distillation methods and teacher-student pairs on CIFAR-100 and TinyImageNet."}, "weaknesses": {"value": "- In lines 127-130, the claim that gradient conflicts between CE and KL lead to optimization instability is not validated. No theoretical and empirical analysis is provided to support this motivation.\n- In lines 153-156, the relationship between the objective “prevent the weighted update from overly aligning with the auxiliary signal” and the minimization of $(g_w^\\top \\mathcal{G} _{ce})^2$ is not clearly explained. \n- In line 184, the notation shifts from $u = [g{ce}^\\top \\mathcal{G}{ce}, g{kl}^\\top \\mathcal{G}{ce}]$ to the claim that $uu^\\top$ is positive semidefinite with rank one. This seems tangential to the optimization of $u^\\top w$, and the emphasis on $uu^\\top$ is confusing.\n- No experiments on large-scale pretrained models (e.g., Qwen, DeepSeek) are conducted, which are highly relevant for real-world KD applications.\n- Only two image classification datasets (CIFAR-100 and TinyImageNet) are used, both relatively small. To strengthen the claim of generalization, larger-scale datasets like ImageNet should be included. Besides, to robustly demonstrate the method's generality, validation should be extended to other critical domains, particularly natural language processing. Knowledge distillation is extensively used for compressing large language models, and the community would benefit from seeing the method's efficacy on text classification, language understanding, or generation tasks.\n- The auxiliary batch is drawn from the same training set. After the first epoch, all batches have been seen, undermining the claim that the auxiliary batch represents “unseen data.”"}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VSoSYtQipP", "forum": "Z0sFCSLw6s", "replyto": "Z0sFCSLw6s", "signatures": ["ICLR.cc/2026/Conference/Submission20287/Reviewer_nuig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20287/Reviewer_nuig"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880229516, "cdate": 1761880229516, "tmdate": 1762933757732, "mdate": 1762933757732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address a relevant problem of how to enhance the adaptive weighting between the distillation and supervised objectives in the KD setup. Keeping generalization in mind they extend this weighting from computing the weights for a single minibatch by relying only on that batch to utilizing other batches as a proxy for the unseen data. Now to find the the weights for a given batch they just consider the weights which don't align much with the gradient due to the cross entropy loss from other batches so that the updates are consistent across batches. They do this by a standard quadratic minimization setup keeping the student model fixed. Furthermore, they also rescale the weights for each of the two tasks inversely w.r.t. to the gradient of that task to normalize the task contributions. Fianlly they do a weighted sum between the standard unbiased gradient from the two tasksm and this new gradient where the weight is controlled by an additional hyperparameter. They then attach this lightweight module to the existing KD methods and analyze the performance with various teacher-student architectures on the CIFAR-100 dataset. They also compare their method with other dynamic weighing approaches on this dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The overall idea seems intuitive and simple to implement with limited overhead in terms of making the updates consistent across the batches and making the distillation process training stable. This sort of cross-batch training can serve as a good proxy for keeping generalization in mind for the student model when focusing on the distillation and classification tasks."}, "weaknesses": {"value": "Very limited experimental results are provided where the authors have only considered the CIFAR-100 dataset which is not sufficient for to understand the durability of the proposed scheme. Secondly the gains on this CIFAR-100 are not significant and fail to convince the usability of this extra overhead in terms of standard unbiased gradient. Similarly the method is not very convincing as against other dynamic weighing methods."}, "questions": {"value": "There are no major questions, the approach is simple and intuitive but the experimental results are not convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FVDYvDrMeI", "forum": "Z0sFCSLw6s", "replyto": "Z0sFCSLw6s", "signatures": ["ICLR.cc/2026/Conference/Submission20287/Reviewer_sk4j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20287/Reviewer_sk4j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957972288, "cdate": 1761957972288, "tmdate": 1762933757069, "mdate": 1762933757069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method called Cross-Batch Gradient Consolidation (CBGC) for improving Post-Training Quantization (PTQ) of deep neural networks. PTQ methods aim to compress pretrained models without full retraining, but often suffer from performance drops, especially at low bit-widths (e.g., 4-bit). The authors observe that batch size affects the stability of PTQ, particularly due to gradient noise in weight optimization. CBGC addresses this by aggregating gradient statistics across multiple batches, leading to more stable weight updates in PTQ. The method is implemented on top of existing quantization-aware optimization, and tested across various models and datasets (e.g., ResNet, ViT on ImageNet). CBGC shows consistent improvements in quantization accuracy, especially at lower bit-widths."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear problem motivation**: The paper targets a real limitation in current PTQ methods—the instability caused by small calibration batch sizes. The observation that noisy gradients harm quantization is well-motivated and experimentally supported.\n\n2. **Simple and practical solution**: CBGC is conceptually simple and easy to integrate into existing PTQ pipelines. It does not require architectural changes or full fine-tuning, making it applicable in low-resource or deployment settings.\n\n3. **Consistent performance gains**: The method is evaluated across diverse models (CNNs and Transformers) and datasets. CBGC consistently improves accuracy under 4-bit and 8-bit quantization, often by noticeable margins compared to standard baselines."}, "weaknesses": {"value": "1. **Limited theoretical analysis**: While the empirical findings are solid, the paper lacks deeper theoretical analysis of why cross-batch gradient averaging stabilizes PTQ. It would benefit from more formal grounding or links to optimization literature.\n\n2. **No study of efficiency trade-offs**: CBGC introduces extra computation by requiring multiple batches for update steps, but the paper does not quantify its overhead in terms of runtime or memory compared to standard PTQ.\n\n3. **Few ablations on consolidation strategy**: There is limited exploration of different strategies for cross-batch selection (e.g., random vs. sequential batches, batch count sensitivity). The paper could better clarify how sensitive the method is to these design choices."}, "questions": {"value": "N.A. But I suggest the following exps to strengthen the paper:\n\n1. Report runtime and memory increase introduced by CBGC to justify its practicality.\n\n2. uantitatively measure how CBGC reduces gradient variance compared to single-batch baselines.\n\n3. Vary number of batches used for consolidation and compare alternative selection strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "htpcCjKfTe", "forum": "Z0sFCSLw6s", "replyto": "Z0sFCSLw6s", "signatures": ["ICLR.cc/2026/Conference/Submission20287/Reviewer_cpDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20287/Reviewer_cpDv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050593986, "cdate": 1762050593986, "tmdate": 1762933756288, "mdate": 1762933756288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new dynamic loss-weighting framework for KD, addressing a key limitation of existing adaptive KD strategies: they rely only on current-batch gradients, which can cause gradient conflicts and unstable optimization. The proposed method introduces a cross-batch consistency mechanism: at each iteration, gradients of CE and KL are computed on the current batch, while a reference CE gradient is extracted from an auxiliary batch to act as a proxy. The method selects CE/KL loss weights by solving a tiny quadratic program to minimize inconsistency with the auxiliary CE gradient. Additional gradient normalization and a scaling mechanism (CAGrad-inspired) are used to stabilize optimization.\n\nThe authors conduct experiments on CIFAR-100 and TinyImageNet. Results show improvements when integrating the method into various distillation techniques (KD, RKD, AT, KDSVD, PKT) across multiple teacher–student architectures. Comparisons with MTL-based dynamic weighting methods (MGDA, FairGrad, PCGrad) also demonstrate good performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies why batch-local weighting is insufficient: CE and KL gradients often misalign, and optimizing on a single batch provides no guarantee of generalization. Figures and experiments (e.g., Fig. 2) nicely illustrate how sensitive KD is to loss-weight choices.\n\n2. The paper properly positions CE/KL balancing as a two-task optimization problem. The comparison against MGDA, PCGrad, FairGrad is valuable, and the analysis in Sec. 3.4 strengthens the argument that CE must be prioritized."}, "weaknesses": {"value": "1. All results are on CIFAR-100 (main) and TinyImageNet (small additional). Modern KD papers typically evaluate on: ImageNet-1K / Larger teachers (ResNet-152, DeiT-B, ViT-B) / Stronger students (MobileNetV3, EfficientNet) .The absence of ImageNet-scale experiments makes it difficult to judge robustness.\n\n2. The ablation study is not comprehensive (section 3.4). The method contains several components, but their individual roles are not fully isolated. In section 3.4, ablation only covers CE-priority vs KL-priority, while some other studies are missing:  (1) What is the effect of quadratic programming alone? (2) What if we replace \\lambda-scaling with a simpler normalization? (3) How sensitive is performance to the hyperparameter \\alpha and \\lambda?\n\nGiven the simplicity of the method, more detailed ablations would increase confidence."}, "questions": {"value": "1. How sensitive is the method to the specific auxiliary batch? Have the authors tried using multiple past batches or a moving average?\n\n2. How does computation scale when applying the method to larger models (e.g., ViT)? (The only mention of the cost is “negligible” (line 250) with no real computational analysis. Given lack of ImageNet/Vit experiments, I would like the authors to discuss more on this point.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "htpcCjKfTe", "forum": "Z0sFCSLw6s", "replyto": "Z0sFCSLw6s", "signatures": ["ICLR.cc/2026/Conference/Submission20287/Reviewer_cpDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20287/Reviewer_cpDv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050593986, "cdate": 1762050593986, "tmdate": 1763653149901, "mdate": 1763653149901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}