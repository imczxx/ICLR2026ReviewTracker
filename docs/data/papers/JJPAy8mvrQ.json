{"id": "JJPAy8mvrQ", "number": 538, "cdate": 1756744998024, "mdate": 1759898254737, "content": {"title": "SelectLLM – Calibrating LLMs for Selective Prediction: Balancing Coverage and Risk", "abstract": "Despite the impressive capabilities of large language models (LLMs), their outputs often exhibit inconsistent correctness and unreliable factual accuracy. In high-stakes domains, overconfident yet incorrect predictions can lead to serious consequences, highlighting the need for robust uncertainty estimation. To address this, we introduce SelectLLM, an end-to-end method designed to enhance the ability of LLMs to recognize and express uncertainty effectively. By integrating selective prediction into finetuning, SelectLLM optimizes model performance over the covered domain, achieving a more balanced trade-off between predictive coverage and utility.  Experimental results on TriviaQA, CommonsenseQA and MedConceptsQA show that SelectLLM significantly outperforms standard baselines, improving abstention behaviour while maintaining high accuracy.", "tldr": "", "keywords": ["LLM Confidence; Trustworthy AI; LLM Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/287d3fdcda148caef9db54f03ae47cb8d1b25fa9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SelectLLM, a method for selective prediction that allows LLMs to abstain from answering when uncertain, thereby balancing predictive risk and coverage. SelectLLM employs a dual-head architecture with separate decoding and selection heads. It is jointly fine-tuned using DPO for utility and a custom loss function for calibrated abstention. Extensive experiments on multiple QA benchmarks and LLMs show that SelectLLM outperforms existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the critical problem of enabling LLMs to abstain when uncertain, which is fundamental for their safe deployment in high-stakes applications. The motivation is well-defined, and the proposed method offers a viable solution. The dual-head architecture, which decouples the generation task from the confidence estimation, is an elegant design choice."}, "weaknesses": {"value": "1. The related work section omits several highly relevant papers on uncertainty quantification and selective prediction for LLMs, such as [1-3]. The paper doesn't provide citations for the LLMs used in the experiments, including Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.2, Qwen2.5-14B-Instruct, and DeepSeek-v3.\n\n2. The paper introduces several key hyperparameters without adequate analysis. An ablation study on the modified risk terms in Eq.3 is mentioned (lines 270-271) but is not present in the appendix or main text. This lack of analysis makes it hard to understand why the method works and limits its claimed reliability.\n\n3.  The evaluation of SelectLLM is confined to QA datasets. The paper provides no evidence or  discussion on whether the method generalizes to other tasks, such as summarization and open-ended generation. \n\n**References**:\n\n[1] Uncertainty-aware Language Modeling for Selective Question Answering.  (Yang, et al., Arxiv 2023)\n\n[2] Improving the reliability of large language models by leveraging uncertainty-aware in-context learning.  (Yang, et al., Arxiv 2023)\n\n[3] Uncertainty in language models: assessment through rank-calibration.  (Huang, et al., EMNLP 2024)"}, "questions": {"value": "1.  Regarding the tone-confidence metric:\n\n - How would SelectLLM's performance be affected if a different or less powerful LLM were used to generate the tone-confidence preference labels instead of DeepSeek-v3?\n - Can the authors substantiate the reliability of the tone-confidence used to validate calibration in Section 5.4? \n\n2.  Regarding hyperparameters and implementation details:\n\n -   How sensitive are the results to the target coverage $c$ and the regularization hyperparameter $\\lambda$? Can you provide risk-coverage curves for varying values of these hyperparameters?\n -   What is the specific architecture of the selection head $g(·)$ (e.g., a linear layer, an MLP)? What are the additional training and inference costs associated with this head?\n -   What is the justification for fixing the loss weighting parameter $\\alpha$ to 0.5? Is there evidence that this choice is robust across different models and datasets?\n\n3.  Regarding additional experiments and ablations:\n\n -   Could you provide the ablation study mentioned in the main paper (lines 270-271) that demonstrates the empirical impact of the additional risk terms introduced in Eq.3?\n -   Which component of SelectLLM is most critical to its performance gains? Is it the separate selection head, the explicit coverage constraint $c$ in the loss function, the modified risk formulation, or their combination?\n -   How might SelectLLM be adapted for generative tasks beyond QA? Have you performed any qualitative experiments on tasks like summarization or open-ended generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C98GqWN6nN", "forum": "JJPAy8mvrQ", "replyto": "JJPAy8mvrQ", "signatures": ["ICLR.cc/2026/Conference/Submission538/Reviewer_tA2u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission538/Reviewer_tA2u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553452270, "cdate": 1761553452270, "tmdate": 1762915542645, "mdate": 1762915542645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a confidence-head training strategy combined with the DPO loss. Their method is training a different head, which is for predicting the confidence and DPO loss for the generation. They evaluate their method on various datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Confidence Estimation is a timely and important topic."}, "weaknesses": {"value": "- The paper is poorly written: The novelty is not properly presented. Some technical terms are misused, such as reward function at line 259. Sections are not separated properly: Section 4 only has one subsection, so why do you have a subsection?\n- In line 256, the proposed loss addition to DPO is exactly the same loss in DPO because. logx-logy = log(x/y).\n- Why do you use an additional model to set the ground truth confidence?\n - If the abstention decision is solely based on the confidence score coming from the confidence head, why do you combine it with DPO preference tuning?\n- What is the purpose of defining the expected coverage? Do you evaluate it on your experiments?\n- Please report TP/TN ratio rather than the actual magnitude.\n- What is the novelty/new perspective of this paper with respect to the current literature?\n- Citation of MARS on line 308 is wrong."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jma4dKLFnB", "forum": "JJPAy8mvrQ", "replyto": "JJPAy8mvrQ", "signatures": ["ICLR.cc/2026/Conference/Submission538/Reviewer_DpC2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission538/Reviewer_DpC2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591971110, "cdate": 1761591971110, "tmdate": 1762915542530, "mdate": 1762915542530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for training a confidence-estimation head in addition to the standard autoregressive decoding loss for an LLM. The paper proposes to use this confidence-estimation head to gate whether the model abstains from providing an answer. The loss function that is used to train the model is a combination of a DPO loss and a ‘select’ loss that attempts to maintain a target global coverage (i.e. abstention vs non-abstention) rate. The authors show that their approach achieves higher performance (defined by the sum of the rate of true positives and true negatives) than a range of prior methods, on three Q&A datasets, and across three open-source models.\n\nOverall, in my view, there are several missing details in this paper, that make it difficult to properly evaluate. I hope that the authors are able to provide some of these details, as well as comprehensively answer my set of questions in the weaknesses below."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Strong performance on relevant metrics (TRUTH score).\n2. Extensive set of baselines that are compared to."}, "weaknesses": {"value": "There are several weaknesses in the paper, some of which I think are quite serious:\n\n1. Line 270 states that “In the appendix, we include an ablation study to demonstrate the effectiveness of the two additional terms”, but there is no such appendix included.\n2. There are missing experimental details, including hyperparameters such as learning rate, training epochs, optimizer. Most crucial is the missing batch size, because:\n3. It is not clear how the empirical coverage (line 277) is calculated. Presumably this is not recomputed on the entire dataset at every training iteration. If it is estimated by the empirical coverage within a batch, then it is crucial to have a high enough batch size for this to be a low variance estimator; and I would like to have seen an ablation w.r.t. batch size.\n4. What is the target coverage rate that is used in the experiments? I did not see this detailed anywhere.\n5. It is also not clear to me exactly how the dataset is constructed. What constitutes a preferred and a dispreferred response? The statement is that a threshold of 0.7 is used and any response assigned a confidence score above that is considered ‘accepted’ and those below ‘rejected’. In which case, how are the pairs of {preferred, dispreferred} constructed precisely? If the pairing does not matter (e.g. it is done at random), then the use of DPO is not the most suitable choice to use; general margin maximisation algorithms such as [KTO](https://arxiv.org/abs/2402.01306) may be more appropriate.\n6. The paper makes extensive reference to ‘human preferences’, yet, the experiments use a strong LLM (DeepSeek v3) to mark the confidence of the responses. The motivation of this design choice is not discussed in the paper at all. The use of this strong LLM labeller suggests that the method primarily benefits from distillation of confidence from a much stronger LLM.\n7. Related to point 6) above, a standard approach of fine-tuning with LoRA to calibrate the LLM directly (as done in e.g. [[1]](https://arxiv.org/abs/2207.05221), [[2]](https://proceedings.neurips.cc/paper_files/paper/2024/file/9c20f16b05f5e5e70fa07e2a4364b80e-Paper-Conference.pdf) should be compared to the method used. It is claimed that the latter is done but there are no details given on the methodology used there, so it is impossible to judge if it is a fair comparison to the proposed method.\n8. The discussion of and adjustment to the DPO loss function due to the possibility of the preferred responses’ likelihoods being reduced by the standard loss function would benefit from reference to prior works such as [[3]](https://arxiv.org/abs/2404.12358), [[4]](https://arxiv.org/abs/2404.04626), [[5]](https://arxiv.org/pdf/2402.13228), which all identify the issue as well as provide suggested ameliorations."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lo6BXl6wJe", "forum": "JJPAy8mvrQ", "replyto": "JJPAy8mvrQ", "signatures": ["ICLR.cc/2026/Conference/Submission538/Reviewer_KetN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission538/Reviewer_KetN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594590219, "cdate": 1761594590219, "tmdate": 1762915542342, "mdate": 1762915542342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SelectLLM, an end-to-end method to calibrate LLMs for selective prediction by adding a selection head and training to balance coverage and risk, claiming strong results on TriviaQA, CommonsenseQA, MedConceptsQA. \n\nIt begins with framing the risk–coverage trade-off in terms of four different outcome types (accept , reject) x  (correct, incorrect). It then motivates the need and pitfalls of abstention, and positions the proposed methods as optimizing that trade-off. The high level idea is to attach a selection head that reads the last hidden state of the question to produce a question-level confidence. The decoding head is trained with DPO, the selection head is optimized for risk–coverage, some illustrative cases are shown in table 1. Moving to more details into the method, the idea is to formalize coverage as proportion answered and risk as error rate over answered set, and then suggests DPO training with pairwise preferences, with the goal of a target coverage x and to abstain if the confidence is below a threshold. The selection head outputs confidence (between 0 and 1) from the last question token state. The loss is a combination of a DPO loss and a select loss. A modified empirical selective risk is defined with some additional terms (line 271), using a reward that prevents DPO from decreasing both probabilities. The two losses are combined with a detault weight of 0.5 (i.e. takes a simple average). \n\nFor experiments, the following base models are chosen: Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.2, Qwen2.5-14B-Instruct; QLoRA rank-16. The proposed method is compared against base, LACIE (DPO), LARS, MARS, TokenSAR, P(True), Semantic Entropy. The metrics used are TP/TN/Precision/Recall/Coverage, and TRUTH = TP+TN (upper bound 1000). For score-based methods, tune the threshold on validation to maximize TRUTH, then apply to test; non-score baselines accept anything unless model explicitly refuses. It is reported that SelectLLM substantially increases TN and Precision at the cost of Coverage/Recall; TRUTH is best across models/datasets. Some experiments for OOD are also reported."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper has a clear practical goal and contribution -- that of using train-time selective prediction for LLMs in a principled way, not just test-time thresholding.\n\nThe architecture proposed is simple and general. It has a single selection head with minimal changes needed to the decoding pipeline. \n\nThe results show consistent empirical gains in TRUTH and precision across three base models and two ID datasets. The OOD results are also encouraging."}, "weaknesses": {"value": "This is not really a weakness but cross my mind. Confidence uses only the question’s last token hidden state, it ignores evidence in the generated answer or early decoding signals. These can be highly informative for difficulty/uncertainty. While I understand that the authors made a choice, it may systematically miss cases where uncertainty emerges during generation (such as in multi-hop). Further, the authors argue that token probs are miscalibrated so a separate head is needed. However, then that head is trained without using token-level uncertainty or decoding statistics. I wonder if calibration performance could be better if so. \n\nSlight consistency issue: Earlier in the paper coverage is defined as the fraction answered. However, later, the empirical coverage is defined as the average of g(h), where g outputs a confidence score between 0 and 1 (not a binary accept indicator). This makes the empirical coverage an average score, not the fraction answered. This causes a slight consistency mismatch in what is argued and e.g. in the constraint terms (equation 4). \n\nThere seems to be a circularity or leakage risk in the validation of confidence. Training pairs are constructed using DeepSeek-v3 \"tone-confidence\" thresholds and fallbacks. But later, SelectLLM’s confidence is validated by comparing to the same tone-confidence distribution (Fig. 3). This is not an independent validation signal. The risk is that it could be teaching the model to mimic tone signals and then \"validating\" that mimicry. For such empirical work this is a chicken and egg problem, but it would at least be useful to spell out. \n\nUsing LDPO (pairwise margin) inside the selective risk is an indirect surrogate for correctness and might misalign with the risk–coverage target, especially OOD. (pages 5–6)"}, "questions": {"value": "See above. \n\nIn addition. \n\nWhy restrict g() to the question state only? Did you test variants that pool answer states or incorporate token-entropy/semantic entropy/variance across samples? If tested, please share,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O9fySPRQbl", "forum": "JJPAy8mvrQ", "replyto": "JJPAy8mvrQ", "signatures": ["ICLR.cc/2026/Conference/Submission538/Reviewer_atUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission538/Reviewer_atUF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027316589, "cdate": 1762027316589, "tmdate": 1762915542206, "mdate": 1762915542206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}