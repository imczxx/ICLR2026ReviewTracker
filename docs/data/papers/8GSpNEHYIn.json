{"id": "8GSpNEHYIn", "number": 6684, "cdate": 1757992286303, "mdate": 1763350947637, "content": {"title": "Deep Thinking on Out-Of-Distribution Data: How can we know when a model is overthinking?", "abstract": "Deep thinking models, a class of recurrent architectures, can generalize from easy to hard examples by allocating more computation during inference. \nWhile effective in logical reasoning tasks, their potential for test-time adaptation in computer vision under out-of-distribution (OOD) data remains underexplored. \nThis work investigates deep thinking as a test-time scaling strategy for object recognition under distributed shift settings. \nWe show that while thinking longer can improve performance, it also introduces the risk of overthinking, where excessive computation damages accuracy.\nTo address this, we propose a self-supervised proxy task that dynamically detects overthinking and approximates the peak accuracy without requiring ground-truth labels.  \nAcross multiple OOD object-recognition benchmarks, deep thinking with our proxy delivers performance gains and accuracy close to peak while avoiding overthinking-related drops.", "tldr": "In this paper, we explore the extrapolation capacity of Deep Thinking Models on Object Recognition Task, and propose a novel method using self supervise learning to detect overthinking of the models on Out-of-Distribution Shift.", "keywords": ["Overthinking", "Test Time Scaling", "Distribution Shift", "Self-Supervised Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72232072c0d91908baa6097c862c745e57bae8f1.pdf", "supplementary_material": "/attachment/d76ef335f1d0d464442e63fe710086f11d6b0db0.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyses how existing recurrent models with flexible compute can outperform higher parameterized models (aka resnet, ) in ood settings like classification on datasets like cifar etc, and its corrupted variants."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores the extrapolation capabilities of deep thinking architectures, particularly when they are allowed to think in the latent space for a long number of iterations.  \n- Authors show that ACT leads to underthinking which can be solved by their proposed SSL approach."}, "weaknesses": {"value": "I have several concerns about the paper as noted below:\n- concerns on novelty: it seems that the paper combines two well known ideas 1) the fact that recurrent-thinking models can lead to strong OOD performance, if dedicated more compute during inference (original paper titled Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach). 2) test-time-training. for eg, the efficiency of rotation prediction task as a surrogate ssl task is already well known(TTT-MAE). \n- the application of ttt + recurrent thinking seems interesting, however, i believe that that it still raises some novelty concerns for me. \n- conv-gru/conv-ligru: fig 2 led me to believe that convli-gru is a novel architecture , and should perform better than conv-gru. however, it appears that convli-gru underperforms in table 1. so i am not clear on why convli-gru is proposed as a method in the first place. \n- needs more results on other datasets: the paper reports results on cifar10. however, following hendryks et al, and existing literature in test-time-training, results on bigger datasets like imagenet-v2, imagenetsketch, imagenet-a, imagenet, and 10 fine-grained datasets should be reported. \n- algorithm 2 (Step 4) seems largely similar to Avi's paper(original paper titled Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach). , and is already a well known step in the theory of recurrent models. \n-  assumption A1, A2 (lines 208-211) appears a big limitation of method: how can one know that for a given task, dataset, and model, which ssl task will hold a direct correlation with the downstream accuracy. The premise of the paper lies on this assumption, so it would be beneficial to provide some analysis on when this assumption might hold. \n- what happens if the network is ran till t= infinity ?"}, "questions": {"value": "please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1FN4eQpYjU", "forum": "8GSpNEHYIn", "replyto": "8GSpNEHYIn", "signatures": ["ICLR.cc/2026/Conference/Submission6684/Reviewer_pdNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6684/Reviewer_pdNM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761453559985, "cdate": 1761453559985, "tmdate": 1762918981426, "mdate": 1762918981426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a formalism for studying _overthinking_ in recurrent _deep-thinking_ vision models under covariate, showing that test accuracy peaks and then decreases as the number of _thinking_ iterations increases. This aligns with prior observations on algorithmic tasks, but is shown repeatedly through multiple experiments on CIFAR-C and Tiny-ImageNet-C (eg: Fig 1, Fig 6). To assess overthinking, the authors introduce a label-free test-time selection rule (Algorithm 1) via the self-supervised auxiliary task of rotation prediction. They then demonstrate the usefulness of the proposed rule by direct methodological contrast against ACT and norm thresholds (Fig 7, Table 3) and further show that iterative test-time scaling can help OOD robustness with fewer parameters as compared to feed-forward models (Tables 1, 2, 4, 5)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The key idea behind the selection rule of the paper is to pick the iteration step to stop at that corresponds to peak accuracy on the auxiliary task, with the main contribution being that it's label-free unlike existing inference-time adaptation methods. This is simple, and potentially domain-agnostic for inference-time adaptation without labels, unlike existing methods such as ACT and norm-thresholds, which the proposed method is either competitive or stronger than while being label-free.\n- From my limited knowledge, only _Bansal et al. (2022)_ actually demonstrated accuracy peaking and then declining with further recurrent steps, and exclusively on synthetic algorithmic reasoning tasks (e.g., addition, sorting). None of the vision-related works they cite (e.g., (Graves, 2016; Veerabadran et al., 2023; Mathur et al., 2025) a) report such degradation. If true, this position can be strengthened by explicitly clarifying that the previous algorithmic tasks denote symbolic sequence computations and not ML datasets."}, "weaknesses": {"value": "- The central assumption (stated in A.1 and A.2) is untested: the curves in Figs 6 and 11 are only qualitative. Plus, the naming is misleading: they are NOT showing correlations, they are only reporting accuracies. Nowhere in the paper could I find the correlation coefficients, associated confidence intervals, variation under or outlier (failure) cases where rotation might be ambiguous. Concrete way to address this issue: report Pearson/Spearman correlation coefficients between stepwise auxiliary and main accuracies for each corruption (mean ± s.d. over seeds) with any comments on where it fails.\n- I am unclear on the details on training parity between the models: were BN, RandAugment, EMA, label smoothing etc used for training the feed-forward CNNs from scratch, was ViT trained with augmentations? These are important since these architectural changes are known to achieve higher CIFAR-C accuracy and show better OOD robustness. So, this would make the claim of Conv-GRU/Conv-LiGRU convincing.\n- It is also not clear if the recurrent models are benefitting from extra compute at inference time, can you show a comparison that is FLOPs-matched? \n- In Lemma 1, how do we know that the task loss shrinks multiplicatively? Doesn't this need to be derived from the dynamics? We need the state update map to shrink thus and then be able to make a statement about the loss if F is derived from the gradient of the loss. The lemma seems to be confusing hidden-state dynamics with the functional used to measure performance and hence skipping an important step. Also some explanation of the \"bounded by non-monotonic\" loss function in this context would be helpful. It is possible I am missing something and I am happy to reconsider in that case else linking the dynamics and the loss functional would make the lemma meaningful.\n- When looking at the curve comparing accuracy and norm difference (such as Fig 3), the connection with overthinking is less conspicuous. The ranges of the x axes across these plots (for eg vs. Fig 6) are vastly different (40 vs 100 where peak is at 25). Is it possible to make the comparison clearer such as by having consistent regions of overthinking? \n- The y axes for the two plots shown side by side in Figure 7 have very different scale: each unit means 10 for the plot on the left, and 5 for the plot on the right. For the argument about the unhalted model surpassing ACT on STL-10 to be more convincing, having similar scales will help.\n\n\nI am happy to reconsider my score if my issues and questions can be addressed."}, "questions": {"value": "- Re: the auxiliary task of rotation prediction, from my understanding, the paper implicitly assumes that both the main and the auxiliary tasks depend on similar representation quality whereas for some classes, the rotation prediction task might be intrinsically ambiguous (such as a bottle, ball, donut, frog, ship etc). For these, rotation prediction accuracy curve may not peak at the same iteration as the main classification curve-- it might saturate early or fluctuate after a certain number of iterations. Do you observe this, and how relevant is it towards the central assumption of the paper? Are there positive correlation coefficients between the two tasks for classes with rotational symmetry as well? Both CIFAR-C and Tiny-ImageNet-C consist of such classes. Concretely, can you report per-class correlation showing trends between asymmetric and symmetric classes?\n- To test the scope of the OOD claim beyond narrow, _artificially corrupted_ benchmarks such as CIFAR-C and Tiny-ImageNet-C that capture low-level pixel perturbations but not true domain or semantic shifts, what do you think about testing on benchmarks such WILDS, PACS etc? This would reveal whether the rotation-proxy criterion and iterative test-time scaling extend to _real distribution shifts_ rather than only to synthetic corruptions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hMP8JETN5L", "forum": "8GSpNEHYIn", "replyto": "8GSpNEHYIn", "signatures": ["ICLR.cc/2026/Conference/Submission6684/Reviewer_XTzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6684/Reviewer_XTzY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757988965, "cdate": 1761757988965, "tmdate": 1762918981030, "mdate": 1762918981030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies deep thinking (recurrent models that improve by running more iterations at inference) in object recognition under OOD shifts. It identifies overthinking (accuracy drops after peak) and proposes a self-supervised proxy task (rotation prediction) to detect the optimal stopping point without test labels. The method is lightweight, avoids underthinking from prior halting rules, and shows strong gains on CIFAR-C, Tiny ImageNet-C, and STL10."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is interesting. Overthinking is real and under-addressed in vision OOD. Prior halting (ACT, norm-threshold) causes underthinking, limiting extrapolation.\n\n- Using rotation prediction as a proxy is simple, label-free at test time, and doesn’t require parameter updates.\n\n- Despite simple, the proposed method show strong empirical results in discussed setups."}, "weaknesses": {"value": "- The method could fails if main and auxiliary tasks decouple under certain OOD shifts (e.g., texture, style, or adversarial). In practice, the models might need to deal with many OOD shifts after deployment. Additional ablation on alternative proxies or failure cases are needed.\n\n- The corruption-based shifts is limited with only (CIFAR-C) and one domain shift (CIFAR→STL10). No semantic, style, or real-world OOD (e.g., ImageNet-R). No large-scale models (e.g., ViT-L, CLIP, DINO) or high-resolution inputs (224×224). This makes this paper as a proof-of-concept rather than a practical validation.\n\n- The computational cost is not well discussed. Running 100 iterations per sample is slow in practice. There is a lack of latency analysis, early-exit trade-offs, per-sample iteration stats.\n\n- The proxy must be trained jointly, not plug-and-play on pretrained models, limiting applicability to frozen backbones (e.g. foundation models). I wonder if we can do fine-tune the proxy task and main task from a pre-trained model rather than training them from scratch. By doing so, it would make this method more applicable since current computer vision research is largely based on pretrained backbones."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IK7afyvT6j", "forum": "8GSpNEHYIn", "replyto": "8GSpNEHYIn", "signatures": ["ICLR.cc/2026/Conference/Submission6684/Reviewer_j6qK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6684/Reviewer_j6qK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841696656, "cdate": 1761841696656, "tmdate": 1762918980542, "mdate": 1762918980542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}