{"id": "KJ0ktHAuds", "number": 23632, "cdate": 1758346566351, "mdate": 1759896803996, "content": {"title": "Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning", "abstract": "Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.4x improvement in sample efficiency and can solve tasks that the baseline fails to learn, demonstrated across a suite of manipulation tasks from the ManiSkill3 benchmark, all without modifying the underlying algorithm or hyperparameters.", "tldr": "We present a return-guided contrastive learning framework for visual RL that trains spatial attention from outcome differences, improving performance without modifying architectures or tuning base RL algorithms.", "keywords": ["Reinforcement Learning", "Robot Learning", "Contrastive Learning", "Robotics", "Gaze"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee8b43135c8380f4c68fef3d21e76ad2ed627784.pdf", "supplementary_material": "/attachment/797ad072f4d59ae3aa0278524b0bfffa1b78d91a.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a framework for visual reinforcement learning that trains a foveal attention mechanism using triplet data, where two similar feature representations correspond to different returns. The foveal attention module is optimized through return-guided contrastive learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is engaging, presenting an intuitive yet effective idea that is both interesting and easy to follow.\n\n2. The authors conduct extensive experiments, demonstrating the strong performance of the proposed framework and its high flexibility across different algorithms."}, "weaknesses": {"value": "The authors may benefit from providing additional implementation details and a more thorough discussion of the framework’s limitations.\n\n1. Clarification on implementation and data mining: I would appreciate a more detailed explanation of Sections 3.3.2 and 4.1–4.2, where the authors discuss return-guided triplet mining and the model implementation. The proposed method appears to rely heavily on datasets that provide dense reward signals and a large number of feature pairs with similar representations but different returns, which may not be readily available in many practical settings. More details on the data-mining process, dataset characteristics, and implementation specifics would help readers better understand the method’s applicability. \nAdditionally, the experiments include a baseline model (“Foveal attention” without contrastive loss). It would be helpful to elaborate on this model’s setup to clarify the standalone impact of the contrastive loss component.\n\n2. Extension of the gaze module: The current gaze module focuses on a single focal point, which may limit its generalizability. The authors are encouraged to discuss whether the framework could be extended to handle multiple focal locations. If such an extension is feasible, it would be useful to describe the required adaptations; if not, clarifying the bottlenecks or constraints preventing this would strengthen the discussion.\n\n3. Data availability and robustness: A key limitation, also noted by the authors, concerns the availability of return signals or, more broadly, the scarcity of triplet-mining data. It would be valuable to provide insights into how much effective data is required for training, and how data scarcity might influence performance or stability. This discussion would help readers understand the framework’s robustness under less ideal data conditions."}, "questions": {"value": "see comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z0CoUfiohe", "forum": "KJ0ktHAuds", "replyto": "KJ0ktHAuds", "signatures": ["ICLR.cc/2026/Conference/Submission23632/Reviewer_eqoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23632/Reviewer_eqoC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615637978, "cdate": 1761615637978, "tmdate": 1762942738398, "mdate": 1762942738398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an approach to learn a foveated attention mechanism where attention is encouraged to focus on task-relevant information. To achieve this, the authors propose using a Gaussian parametric attention mechanism paired with a contrastive loss where positive and negative samples are mined based on visual similarity and return differences. For that the authors use FAISS to store tuples of observations and returns, and then for each anchor select the positives and negatives based on their corresponding return classification based on a threshold. In simulated experiments, the approach is shown to improve sample efficiency and performance over model-free on-policy and off-policy algoriths such as PPO and SAC."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and enjoyable to read\n- The proposed approach makes a lot of sense and is sound\n- The proposed approach is very simple and can be easily plugged into most vision-based RL frameworks\n- The results are promising, the proposed approach seems to yield a consistent improvement over SAC and PPO without strongly impacting wall-clock time, which could have been a concern given the use of retrieval methods such as FAISS\n- Ablations illustrate the role of different components and the sensitivity to different hyperparameters that were introduced in this work"}, "weaknesses": {"value": "- On the method' side, one detail does not make full sense to me. I do not understand why the positives are chosen to have the higher returns, the anchor might belong to the low-return set and in that case the latter should be treated as positives.\n- The main weakness of this work lies in its evaluation. Specifically, the baselines considered in this work fail to position the contribution well. Let me elaborate. While the proposed framework seems to be quite helpfull when using vanilla PPO or SAC, it would be interesting to see if it has any effect on stronger visual RL methods such as CURL or more recent methods. Without such a baseline, it is unclear whether the proposed framework is even needed. In theory, the proposed method should help no matter what baseline is used, but this needs to be validated and well-studied to understand what level of task complexity is needed for this to become apparent.\n- Another missing experiment is to understand the role of this plugin  if the encoders are pretrained either large-scale on images or in a first stage with random data from the target environment\n- (optional) what would strengthen the paper a lot and increase its impact is to also test the method on a model-based RL baseline, but I understand this can be time-expensive"}, "questions": {"value": "- Can you explain the choice of positive and negative sampling in your contrastive learning setup (refer to my first comment under weaknesses)?\n- how would your method perform when augmenting stronger vision-based RL baselines such as CURL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XOcfMxJh2h", "forum": "KJ0ktHAuds", "replyto": "KJ0ktHAuds", "signatures": ["ICLR.cc/2026/Conference/Submission23632/Reviewer_6vmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23632/Reviewer_6vmd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841855931, "cdate": 1761841855931, "tmdate": 1762942738167, "mdate": 1762942738167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Gaze on the Prize, a visual reinforcement learning framework that enhances sample efficiency and stability by integrating a learnable foveal attention mechanism inspired by human visual focus. Instead of processing all image features equally, the method guides attention toward task-relevant regions using a self-supervised signal derived from return differences—comparing states that lead to high versus low rewards. Through return-guided contrastive learning, the system learns to emphasize visual features that distinguish success from failure, forming contrastive triplets based on outcome differences. Without changing the base RL algorithms or hyperparameters, this approach yields up to 2.4× higher sample efficiency and enables agents to solve manipulation tasks in the ManiSkill3 benchmark that standard methods fail to learn."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a biologically inspired, learnable foveal attention mechanism that enhances both the interpretability and efficiency of visual reinforcement learning. \n2. The paper provides a flexible, plug-and-play framework that can seamlessly integrate with various RL algorithms to consistently improve performance across manipulation tasks.\n3. The paper is well-written, presenting a straight-forward idea with conceptual clarity and a logical narrative that is easy to follow."}, "weaknesses": {"value": "1. One potential drawback of the proposed method lies in that it assumes that there are a sufficient number of clearly distinguishable positive and negative samples. However, in some difficult or sparse-reward tasks, there can be very few positive samples in the replay buffer, which may affect the performance of the policy."}, "questions": {"value": "1. I'm curious about why the episode return instead of step reward is used to guide the contrastive learning, since one-step reward seems a better indicator for the one-step observation.\n2. What will happen if the task involves two objects? For example, if the task is to pick a cube into a box, will the attention mechanism always capture both cube and box, or first focus on cube and then box?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e5VJk9LvKz", "forum": "KJ0ktHAuds", "replyto": "KJ0ktHAuds", "signatures": ["ICLR.cc/2026/Conference/Submission23632/Reviewer_LYqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23632/Reviewer_LYqW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925666839, "cdate": 1761925666839, "tmdate": 1762942737934, "mdate": 1762942737934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a lightweight \"gaze\" head that predicts an anisotropic Gaussian to reweight spatial CNN features, trained with a return-guided triplet loss mined from a separate buffer of detached features, and combined with a standard RL loss (PPO/SAC). On ManiSkill3 manipulation tasks, the method learns faster and shows larger gains under cluttered visuals."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, interpretable, drop-in attention module with few parameters and human-readable heatmaps.\n\n- Clear optimization split: contrastive loss updates the gaze head; RL gradients train the encoder/policy.\n\n- Works with both off-policy algorithm and off-policy algorithm\n\n- Useful ablations (e.g., auxiliary buffer, update frequency) and some wall-clock considerations."}, "weaknesses": {"value": "1. Missing strong pixel-RL baselines. The paper omits standard data-augmentation and contrastive baselines (RAD, DrQ-v2, SVEA, CURL) that are now expected in visual control comparisons. Without these, it is unclear whether the gains stem from the proposed attention or simply from adding any robust representation trick.\n2. Insufficient OOD evaluation. Claims about distraction robustness/generalization are not tested on canonical benchmarks with held-out domains and graded shifts (DCS; DMC-GB/GB2). Results limited to ManiSkill3 and custom clutter do not substitute for these protocols. \n\n3. Sensitivity to reward structure. The return-guided triplet objective relies on reward spread; sparse-reward regimes may weaken positives/negatives. A value- or TRP-based auxiliary could help, but this is not explored. ([ojs.aaai.org][3])\n5. The narrative hints at generalization, but the experiments emphasize sample-efficiency on manipulation. Without in-distribution to OOD gap reporting, distraction-intensity curves, or cross-background transfer, the generalization claim is under-supported.\n6. Related-work positioning. Attention/masking approaches targeting distraction robustness: MaDi; saliency-guided methods like SGQN and SGFD; segmentation-assisted FTD are not compared and deserve deeper discussion. ([ifaamas.org][4])\n\n\nReferences: \n* Laskin, M., et al. (2020). Reinforcement Learning with Augmented Data (RAD). NeurIPS.[1]\n* Stone, A., Ramirez, O., & Jonschkowski, R. (2021). The Distracting Control Suite. [2]\n* Wang, S., Wu, Z., Hu, X., Wang, J., Lin, Y., & Lv, K. (2024). What Effects the Generalization in Visual RL: Policy Consistency with Truncated Return Prediction (TRP). [3]\n* Grooten, B., et al. (2024). MaDi: Learning to Mask Distractions for Generalization in Visual Deep RL [4]\n* Laskin, M., Srinivas, A., & Abbeel, P. (2020). CURL: Contrastive Unsupervised Representations for Reinforcement Learning. ICML. [5]\n* Yarats, D., Fergus, R., Lazaric, A., & Pinto, L. (2021). Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning (DrQ-v2) [6]\n* Hansen, N., Su, H., & Wang, X. (2021). SVEA: Stabilizing Deep Q-Learning with ConvNets and ViTs under Data Augmentation. NeurIPS. [7]\n* Hansen, N., & Wang, X. (DMC-GB) and Almuzairee, A., et al. (DMC-GB2). DMControl Generalization Benchmarks. [8] \n* Bertoin, D., Zouitine, A., Zouitine, M., & Rachelson, E. (2022). Look where you look! Saliency-guided Q-networks for generalization in visual RL (SGQN). NeurIPS. [9]\n* Chen, C., Xu, J., Liao, W., Ding, H., Zhang, Z., Yu, Y., & Zhao, R. (2024). Focus-Then-Decide: Segmentation-Assisted Reinforcement Learning (FTD). [10]\n* Huang, S., Sun, Y., Hu, J., Guo, S., Chen, H., Chang, Y., Sun, L., & Yang, B. (2023). Learning Generalizable Agents via Saliency-Guided Features Decorrelation (SGFD).  [11]\n* Tao, S., et al. (2024). ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI. arXiv:2410.00425. [12]"}, "questions": {"value": "- How does the method compare to RAD, DrQ-v2, SVEA, and CURL under their recommended settings? Please report both ID performance and ID and OOD gaps, for example you can use the Distracting Control Suite.\n- If reward spread is small, could you form positives/negatives using value estimates or TRP targets? Any preliminary results?\n- Please provide a compute table (SPS and wall-clock) for baseline CNN, +gaze (no contrast), and full method, including buffer maintenance and mining time, across several tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5dAiHMFccy", "forum": "KJ0ktHAuds", "replyto": "KJ0ktHAuds", "signatures": ["ICLR.cc/2026/Conference/Submission23632/Reviewer_iVWx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23632/Reviewer_iVWx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939091035, "cdate": 1761939091035, "tmdate": 1762942737676, "mdate": 1762942737676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}