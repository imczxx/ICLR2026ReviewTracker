{"id": "e4xANXjA9W", "number": 11969, "cdate": 1758204930362, "mdate": 1759897541929, "content": {"title": "Robustness in the Face of Partial Identifiability in Reward Learning", "abstract": "In Reward Learning (ReL), we are given feedback on an unknown target reward, and the goal is to use this information to recover it in order to carry out some downstream application, e.g., planning. When the feedback is not informative enough, the target reward is only partially identifiable, i.e., there exists a set of rewards, called the feasible set, that are equally plausible candidates for the target reward. In these cases, the ReL algorithm might recover a reward function different from the target reward, possibly leading to a failure in the application. In this paper, we introduce a general ReL framework that permits to quantify the drop in \"performance\" suffered in the considered application because of identifiability issues. Building on this, we propose a robust approach to address the identifiability problem in a principled way, by maximizing the \"performance\" with respect to the worst-case reward in the feasible set. We then develop Rob-ReL, a ReL algorithm that applies this robust approach to the subset of ReL problems aimed at assessing a preference between two policies, and we provide theoretical guarantees on sample and iteration complexity for Rob-ReL. We conclude with some numerical simulations to illustrate the setting and empirically characterize Rob-ReL.", "tldr": "We propose to tackle the identifiability problem in reward learning with a robust approach.", "keywords": ["Inverse Reinforcement Learning", "Reward Learning", "Preference Based Reinforcement Learning", "Theory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4566f55cc2e4169455b4e8f370cc5e6f1a82cdc.pdf", "supplementary_material": "/attachment/32eab0a45dfe4dd2b3225ccaee2c9e2ba73b4964.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a unified framework for robust reward learning in partially identifiable settings. Given some form of feedback (e.g., preferences, demonstrations), the method constructs a feasible set of reward functions consistent with observed data and then optimizes the worst case over this set (for a downstream decision such as a policy). The formulation aims to generalize across multiple feedback modalities and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The framework unifies various feedback types (e.g., pairwise comparisons, demonstrations) and downstream objectives under one minmax formalism.\n+ The worst-case formulation in (7) is intuitive and could be more realistic than assuming a point estimate of the reward."}, "weaknesses": {"value": "- The general minmax structure resembles many existing approaches in IRL, robust MDPs, and RLHF (e.g., distributionally robust RL, reward uncertainty sets). The paper does not sufficiently delineate what is new in (2) beyond framing and unification.\n\n- The paper needs sharper theoretical distinctions (e.g., identifiability analysis, sample complexity bounds, or formal generalization of previous frameworks). Without this the contribution risks being incremental.\n\n- The approach requires estimating or approximating policy-induced dynamics and state-action visitation measures, which can be computationally prohibitive and may not be scalable to realistic domains.\n\n- The proposed method replaces one set of unknowns (the reward) with multiple estimated quantities (transition probabilities, occupancy measures, feasible reward bounds), which may not improve robustness in practice.\n\n- The experiments are limited to small toy domains (even with the ones in the appendix). The paper needs results on more standard RL or preference-learning benchmarks (e.g., MuJoCo or D4RL tasks) to demonstrate practical value. The lack of scalability experiments weakens the claimed generality.\n\n- It is unclear whether ablation or sensitivity studies were conducted to assess dependence on feasible set or uncertainty size.\n\n- The paper does not discuss how many trajectories or feedback samples are required to characterize the feasible reward set. It remains also unclear how the method handles poor coverage or unobserved states which is an important concern when identifiability is partial.\n\n- The framework implicitly assumes access to accurate simulators or transition estimates, which limits realism in practical RLHF settings."}, "questions": {"value": "1) How does (2) differ theoretically or algorithmically from robust IRL or minimax RLHF formulations?\n2) How sensitive is performance to the feasible reward set?\n3) Can the approach handle partially observed dynamics or limited coverage?\n4) What is the empirical complexity compared to baselines? how does it scale with state or trajectory length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "en6J66Bc4B", "forum": "e4xANXjA9W", "replyto": "e4xANXjA9W", "signatures": ["ICLR.cc/2026/Conference/Submission11969/Reviewer_Kgr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11969/Reviewer_Kgr5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664779640, "cdate": 1761664779640, "tmdate": 1762922967024, "mdate": 1762922967024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for reward learning from preferences and demonstrations. The authors try to quantify the effect of  identifiability issues of the reward, and propose a minimax approach to optimize the worst-case scenario over the feasible reward set. They provide Rob-ReL, an algorithm for policy preference assessment, providing theoretical guarantees on sample and computation complexity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear formalization of the problem. \n- The paper is well written and organized in a clear progression\n- Clear sample-complexity results for their algorithm with illustrative numerical results"}, "weaknesses": {"value": "- A big limitation of this work is the limited number of applications. The authors provide a general framework, but then they limit themselves to a single scenario. This leaves the overall framework not properly tested, and severely limits the contribution of the paper.\n- Another limitation, is that while the authors provide a general framework, the theoretical insights seem to be limited. For example, when solving a minimax game the equilibrium may not be a pure equilibrium; however, the authors do not seem to discuss this issue. Depending on the properties of ${\\cal X}_g$, the loss, etc...we may have different situations. In the application they propose the solution they get seems a pure one (because it's estimating a scalar), but i would expect mixed solutions to appear depending on the problem (e.g., when ${\\cal X}_g=\\Pi$). While the authors do an effort at quantifying the error (eq 3), it is not very clear what are the properties of this minimax problem (which depends on the set of rewards, loss, etc.). \n\n- Theorem 5.3: the dependency on $\\xi$ seems quite large\n- It is not clear why the chosen application (estimating policy-value differences) is an interesting one\n\n- While the paper is well written, there is still lot of notation and it is hard to follow the proofs.\n\n- The method is only tested on a simple problem, while larger problems are untested."}, "questions": {"value": "Please see the weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VVtwIkmSTd", "forum": "e4xANXjA9W", "replyto": "e4xANXjA9W", "signatures": ["ICLR.cc/2026/Conference/Submission11969/Reviewer_kR8k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11969/Reviewer_kR8k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778172757, "cdate": 1761778172757, "tmdate": 1762922966629, "mdate": 1762922966629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of partial identifiability in reward learning (ReL), a consequence of when the provided feedback is insufficient and does not allow for the identification of the target reward $r^\\*$. Instead of traditional approaches, the authors propose a new framework that incorporates robust optimization to minimize the loss tied to the worst possible value for $r^*$ within some feasible set."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The formalization, explanation, and clarity of the ReL problem as a pair $(\\mathcal{F},g)$ is very well written, general, relevant, and well positioned against related work. A key piece is how the ReL problem is reformulated to finding the optimal object $x\\in\\mathcal{X}_g$ for some application $g$, when given the uncertainty set of rewards $\\mathcal{R_F}$.\n- The metric used to calculate the loss, $\\mathcal{I}_{\\mathcal{F},g}$, or how \"uninformative\" $\\mathcal{F}$ is for application $g$, is very useful as it effectively allows for the principled comparison of various feedback sets and/or applications.\n- The algorithm instantiation and associated analysis is novel. A key piece that facilitates this is in Proposition 5.1 where the minimax problem is simplified into two convex problem."}, "weaknesses": {"value": "- While the authors present a powerful, general framework, they do not address tractability concerns. The limitations section significantly downplays this fact and speaks broadly on it.\n- The experiments, though useful in demonstrating/verifying the author's theoretical, do not have any baselines to compare against. For example, in section 4, existing approaches are discussed. It would be interesting to see how the proposed algorithm compares against these non-robust baselines."}, "questions": {"value": "- Can you expand on what you mean on lines 240-241 and/or provide a reference speaking towards a practical example?\n- You make the assumption \"that the feasible set $\\mathcal{R}_\\mathcal{F}$ contains a strictly feasible reward $\\bar{r}.$ How realistic is this in practice?\n- You make the assumption \"that the feasible set $\\mathcal{R}_\\mathcal{F}$ contains a strictly feasible reward $\\bar{r}.$ How realistic is this in practice?\n- Consider changing the colored text in equation 7 so that it matches the blue in Lemma E.2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "30fTHsbEcp", "forum": "e4xANXjA9W", "replyto": "e4xANXjA9W", "signatures": ["ICLR.cc/2026/Conference/Submission11969/Reviewer_tEva"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11969/Reviewer_tEva"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762271475077, "cdate": 1762271475077, "tmdate": 1762922966294, "mdate": 1762922966294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the fundamental problem of partial identifiability in Reward Learning (ReL), where limited feedback makes it impossible to uniquely recover the target reward function. The authors propose a quantitative framework that allows measuring performance degradation due to identifiability issues, moving beyond prior qualitative approaches. They introduce a robust minimax approach that optimizes for the worst-case reward in the feasible set and develop Rob-ReL, an algorithm for policy preference assessment problems with provable sample and iteration complexity guarantees. The work combines demonstrations, trajectory comparisons, and newly introduced policy comparison feedback in a mixed offline-online setting. Theoretical analysis shows polynomial complexity in relevant problem parameters, and numerical experiments illustrate the approach on a low-dimensional navigation task."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**Rigorous Theoretical Analysis.** Theorem 5.3 provides polynomial sample and iteration complexity bounds under reasonable assumptions (Slater's condition). The proof technique combining visitation distribution estimation errors with primal-dual subgradient convergence is sound. The use of RF-Express for minimax-optimal reward-free exploration is appropriate.\n\n**Clear Presentation and Organization.** The paper is well-structured with motivation, framework, approach, algorithm, and theory presented logically. The illustrative example in Section 6 effectively conveys the main ideas visually."}, "weaknesses": {"value": "**Limited treatment of function approximation.** The tabular setting with explicit state-action representation limits applicability to high-dimensional problems. While the authors mention neural network parameterization in related work, Rob-ReL does not incorporate function approximation"}, "questions": {"value": "How does the method scale to continuous state-action spaces with function approximation? \nIs there a path toward extending Rob-ReL to deep RL settings, perhaps using neural network reward parameterization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YtAMG1YT2x", "forum": "e4xANXjA9W", "replyto": "e4xANXjA9W", "signatures": ["ICLR.cc/2026/Conference/Submission11969/Reviewer_mwRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11969/Reviewer_mwRB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762283538084, "cdate": 1762283538084, "tmdate": 1762922965853, "mdate": 1762922965853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a new framework, robust reward learning, that tackles the known problem of partial identifiability in reward learning. The framework incorporates several important forms of data for reward learning (including trajectories from an optimal policy, trajectory comparisons, and policy comparisons) and considers several potential downstream applications (including imitation, finding an optimal policy, and learning to compare preferences). The paper proposes to use worst-case performance in a downstream application, across all rewards compatible with the training data, as a metric for success.\n\nThe paper provides an algorithm for solving robust reward learning problems and provides theoretical guarantees on the sample and computational complexities. They also run some experiments in a small RL setting to demonstrate their algorithm works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I think the paper has a number of strengths:\n1. The paper addresses an *important problem*:  improving the safety of reward learning, which is a salient problem for modern AI deployments.\n2. The *novel formulation* of robustness for reward learning adds upon prior work addressing this problem. The uninformativeness measure is an interesting way to quantify the difficulty of doing reward learning.\n3. The formulation is *very general* and explicitly considers three important kinds of reward learning feedback. (Prior work, e.g. Skalse et al. (2023) only considered two kinds).\n4. A *(somewhat) tractable algorithm* for solving the novel problem is introduced (Rob-Rel), and theoretical guarantees on sample complexity and time complexity (at least in terms of the number of iterations) are given. Further, by using duality, the paper avoided any dependence of time complexity on the size of the feasible reward set.\n5. The paper *thoroughly, clearly, and fairly considers existing work*. In particular, tables 1 and 2 give clear relationships to broad prior work, and appendix *A.2* gives a thorough comparison to Skalse et al. (2023) which helped me to understand its contribution."}, "weaknesses": {"value": "### Scalability\nThe proposed algorithm is polynomial in the size of the state space. This is a limited weakness, as it is common to some of the prior literature.\n\nHowever, some reward learning methods have been shown to work in realistic and large or continuous state spaces. For example, Christiano et al.'s (2017) reward learning has been applied to text settings (large, discrete state spaces) and physics simulations (continuous state spaces). Laidlaw et al. (2025) applied CIRL to a large game.\n\nI think the impact of this work would be significantly improved if evidence was given to suggest the framework and algorithm were scalable. If the algorithm is not scalable, it would be helpful to describe these limitations.\n\n### Practicality of worst-case return\nThe proposed algorithm maximizes worst-case robustness relative to a feasible set. As the authors themselves suggest on line 241, there may be some reward learning problems where it is infeasible to get an acceptable worst-case reward. (In these cases, I think the uninformativeness could be a helpful metric for measuring this infeasibility, as noted in the strengths above). I expect that, in many real-world applications with large feasible reward spaces, many reward learning problems will be infeasible.\nWhile it is not reasonable for the paper to solve this problem, I do believe it is a notable limitation, relative to e.g. a Bayesian approach that only tries to minimize expected loss over a posterior reward distribution.  Evidence that robust reward learning is feasible in real-world environments might improve the paper.\n\n\n### Clarity\nI found the paper to be quite difficult to parse in a number of places, although I think the paper is relatively clear given its technical density. Three potential points for improvement might be:\n1. Reduce the introduction of abbreviations: \"ReL\", \"IL\", \"PBIRL\", and other abbreviations could be far easier to parse if set out plainly.\n2. Reduce the introduction of notation, or where possible, explain in plain English what terms mean. Section 5, in particular, introduces a whole range of new notation that is hard to keep track of and could be better supported with natural language explanations.\n3. (Minor) Throughout the paper, citations are given without bracketing, where bracketing would be much clearer. For example, lines 214, 121 and 105.\n\n\n### References\n* Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. arXiv preprint arXiv:1706.03741.\n* Laidlaw, C., Bronstein, E., Guo, T., Feng, D., Berglund, L., Svegliato, J., Russell, S., & Dragan, A. (2025). AssistanceZero: Scalably solving assistance games. arXiv preprint arXiv:2504.07091."}, "questions": {"value": "Can the authors provide any insight about whether worst-case reward learning can scale as well as alternative approaches (such as cooperative inverse RL, or reinforcement learning from human feedback)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PElcKsswMW", "forum": "e4xANXjA9W", "replyto": "e4xANXjA9W", "signatures": ["ICLR.cc/2026/Conference/Submission11969/Reviewer_PqUe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11969/Reviewer_PqUe"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762871317778, "cdate": 1762871317778, "tmdate": 1762922965376, "mdate": 1762922965376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}