{"id": "P5heo2n6Vq", "number": 12828, "cdate": 1758210614694, "mdate": 1759897482473, "content": {"title": "MCPlanner: Multi-Scale Consistency Planning for Offline Reinforcement Learning", "abstract": "Planning for long-horizon tasks is a significant challenge, often addressed with complex hierarchical methods that rely on multiple, independently trained models. These hierarchical approaches can be brittle and incur coherence issues. In this work, we introduce Multi-scale Consistency Planner ($\\textbf{MCPlanner}$), a novel framework that leverages the unique properties of Generalized Consistency Trajectory Models (GCTMs) to create a fluid and unified planning hierarchy. Unlike prior generative models which are limited to mappings from noise to data, GCTMs can learn a direct, fully-traversable ODE path between arbitrary data distributions. This crucial capability allows MCPlanner to unify high-level and low-level planning within a single model. Instead of training separate high-level and low-level planners, MCPlanner employs a single GCTM trained on end-to-end expert trajectories. At inference time, a seamless hierarchy emerges: coarse, long-horizon plans are generated by querying the model at a sparse temporal resolution, while dense, fine-grained motions are synthesized by querying the same model on the continuous path between these coarse waypoints. Our approach obviates the need for discrete hierarchical structures, offering a more elegant, efficient, and controllable solution to long-horizon planning. Furthermore, our experimental results demonstrate that MCPlanner achieves state-of-the-art performance across $35$ challenging tasks on OGBench benchmark, by consistently outperforming prior approaches.", "tldr": "", "keywords": ["Generative models", "Long-horizon planning", "Reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31a4e38f1b6d9ebbc991f9b5494646a3e7b456dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MCPlanner, a multi-scale planning framework for offline goal-conditioned reinforcement learning (RL). The idea is to replace the traditional separated hierarchical structure (high-level and low-level polices) with a single unified model based on the Generalized Consistency Trajectory Model (GCTM). Comprehensive experiments on the OGBench benchmark show that MCPlanner consistently outperforms state-of-the-art diffusion-based and hierarchical planners."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a conceptually novel approach to long-horizon planning in offline RL by unifying hierarchical diffusion planning into a single flow-based model (GCTM). The introduction of a multi-scale consistency loss and conditional optimal-transport coupling represents a creative and principled extension of flow-matching ideas to hierarchical planning.\n\n* The experiments are extensive across 35 tasks in the OGBench benchmark, including both locomotion and manipulation domains. Empirical results consistently demonstrate superior performance over previous hierarchical diffusion methods."}, "weaknesses": {"value": "### 1. Clarity and Accessibility :\n\nThe paper is not easy to follow, particularly in the methodological exposition. Some sections could benefit from clearer explanations and more intuitive motivation. For instance, Figure 2 is difficult to interpret and lacks sufficient textual explanation in the main body. The authors should explicitly describe how querying the same GCTM across different time scales results in the emergence of hierarchical behavior. Furthermore, the discussion about why separately trained high-level and low-level planners lead to lack of coherence and consistency remains underdeveloped. Providing a more concrete analysis (or illustrative example) of how the two-model approach fails to maintain consistency would greatly improve readability and conceptual clarity.\n\n### 2. Unclear causal link between performance and claimed coherence : \n\nAlthough the experiments show that MCPlanner achieves better performance than baselines, and that removing  L_MS leads to degradation,\nit is not clearly demonstrated that these gains directly stem from improved coherence or consistency. Additional analysis, such as qualitative visualizations of subgoal alignment or temporal consistency metrics, would strengthen the causal argument between the multi-scale objective and the resulting performance. \n\n### 3. Inference-time advantage and fairness concerns :\n\nWhile the paper reports consistent superiority over diffusion-based baselines, MCPlanner’s inference pipeline includes candidate generation, scoring, and latent refinement, effectively adding test-time optimization and selection steps. These additional computations likely improve success rates but were not applied to baseline methods, making the comparison potentially unfair in terms of compute and inference strategy.\nIndeed, Figure 3 shows that when the number of trajectory candidates is reduced to N=1, MCPlanner’s performance drops substantially, approaching that of baseline planners. This suggests that part of the reported gain may come from the inference-time candidate search rather than the proposed architecture itself.\n\n### 4. Lack of quantitative analysis on computational efficiency :\n\nThe paper claims that the unified GCTM design reduces training and inference cost compared to two-model hierarchies, yet provides no quantitative evidence, such as runtime, GPU-hour, or FLOPs per trajectory. Given that GCTM integration and OT coupling introduce additional computation (e.g., Sinkhorn iterations, multi-time evaluations), a fair and transparent assessment of computational cost is needed to justify the efficiency claims."}, "questions": {"value": "Please provide your response to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8hGt5wVDmr", "forum": "P5heo2n6Vq", "replyto": "P5heo2n6Vq", "signatures": ["ICLR.cc/2026/Conference/Submission12828/Reviewer_yiK8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12828/Reviewer_yiK8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909950290, "cdate": 1761909950290, "tmdate": 1762923631817, "mdate": 1762923631817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses long-horizon planning in offline reinforcement learning, focusing on multi-scale planning. The authors propose a multi-scale planner built upon Generalized Consistency Trajectory Models (GCTM). Unlike hierarchical planners that require separate models across temporal scales, this method integrates all resolutions into a single unified model, aiming for improved efficiency and coherence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for tackling multi-scale planning in offline RL is clearly presented and well-justified, connecting the need for temporal abstraction with the limitations of current planners.\n\n2. The paper is easy to follow, self-contained, and includes well-written preliminaries that make it accessible to a broad ICLR audience."}, "weaknesses": {"value": "1. The motivation for using GCTM as the specific backbone is insufficiently justified. While GCTMs are relevant to consistency-based generative modeling, the paper does not explain why they are better suited for multi-scale planning than diffusion-based planning.\n\n2. The comparative evaluation lacks a controlled ablation isolating the effect of the proposed multi-scale mechanism. Current results compare against planners that differ in both backbone and architecture, making it unclear whether the gains stem from multi-scale or model-specific characteristics. As a suggestion, authors can include a GCTM variant without the multi-scale mechanism.\n\n3. The novelty is somewhat limited. Multi-scale temporal modeling has been previously explored in diffusion-based planners ([1], [2]). The contribution thus appears incremental, extending known ideas to a new architecture (GCTM) without clear conceptual gain."}, "questions": {"value": "1. The text under the images in Figure 2 is ambiguous. At first glance, it seems to correspond to textual input, but the method is not applied to text-based environments. Could you clarify what this text represents?\n\n2. In Table 1, the reported score for cube-quadruple-play-v0 is zero across all baselines. Could the authors elaborate the reason?\n\n3. The authors attribute the gap between two-level hierarchy and the proposed multi-scale variant to \"coherence gap and compounding errors\". Could you provide quantitative or qualitative evidence supporting this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h2wS7CcmjA", "forum": "P5heo2n6Vq", "replyto": "P5heo2n6Vq", "signatures": ["ICLR.cc/2026/Conference/Submission12828/Reviewer_e2iD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12828/Reviewer_e2iD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951426662, "cdate": 1761951426662, "tmdate": 1762923631584, "mdate": 1762923631584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MCPlanner, a unified planning framework built on Generalized Consistency Trajectory Models (GCTMs) that replaces the usual two-model hierarchy with a single model queried at multiple temporal resolutions. Specifically, this work proposes: (i) a multi-scale consistency objective to align long jumps with compositions of short jumps; and (ii) conditional optimal-transport coupling to straighten the learned flow and stabilize coarse integration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method has the advantages of generating (1) coarse intermediate “*subgoals*” very quickly; (2) fine-grained motion on interval level.\n\nThis work proposes a new entropy regularized OT coupling between batches of expert trajectories and their corresponding deterministic trajectory priors to reduce the curvature of the FM ODE.\n\nAblation study demonstrated the effectiveness of the design choice of the proposed method."}, "weaknesses": {"value": "Although this paper proposed some modifications over GCTM, the overall technical novelty and contribution in terms of consistency models are just incremental as the key technique used is based on existing training objectives.\n\nThe central topic of the paper is more on goal achieving in offline planning tasks like navigation or locomotion. There is no reward function used in the formulation or the methodology. So this paper is strongly related to offline RL where offline policy improvement is the key objective. Therefore, the paper is closer to trajectory planning than offline RL.\n\nIt would be better to compare with non-diffusion based methods like hierarchical RL.\n\nThere are some confusions over the notations, for example, T is used for trajectory length and is also used for diffusion/FW denoising steps.\n\nIt seems that the method cannot handle test-time environmental uncertainty and errors as the planning seems to be done before execution. Can the proposed method plan in close loop in a receding horizon fashion?"}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ncnY69vZV3", "forum": "P5heo2n6Vq", "replyto": "P5heo2n6Vq", "signatures": ["ICLR.cc/2026/Conference/Submission12828/Reviewer_Wbtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12828/Reviewer_Wbtz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998034768, "cdate": 1761998034768, "tmdate": 1762923631289, "mdate": 1762923631289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}