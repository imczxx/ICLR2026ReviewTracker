{"id": "Q4LtkLD3r8", "number": 19633, "cdate": 1758297847532, "mdate": 1759897029228, "content": {"title": "Random Feature Spiking Neural Networks", "abstract": "Spiking Neural Networks (\\textit{SNN}s) as Machine Learning (\\textit{ML}) models have recently received a lot of attention as a potentially more energy-efficient alternative to conventional Artificial Neural Networks. The non-differentiability and sparsity of the spiking mechanism can make these models very difficult to train with algorithms based on propagating gradients through the spiking non-linearity. We address this problem by adapting the paradigm of Random Feature Methods (\\textit{RFM}s) from Artificial Neural Networks (\\textit{ANN}s) to Spike Response Model (\\textit{SRM}) \\textit{SNN}s. This approach allows training of \\textit{SNN}s without approximation of the spike function gradient. Concretely, we propose a novel data-driven, fast, high-performance, and interpretable algorithm for end-to-end training of \\textit{SNN}s inspired by the \\textit{SWIM} algorithm for \\textit{RFM}-\\textit{ANN}s, which we coin \\textit{S-SWIM}. We provide a thorough theoretical discussion and supplementary numerical experiments showing that \\textit{S-SWIM} can reach high accuracies on time series forecasting as a standalone strategy and serve as an effective initialisation strategy before gradient-based training. Additional ablation studies show that our proposed method performs better than random sampling of network weights.", "tldr": "We propose a data-driven random feature algorithm for SNNs to be used as initialisation or standalone training strategy.", "keywords": ["Spiking Neural Networks", "Random Feature Methods", "Time Series Forecasting"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d33c9c8b899432f1f16a394ae3809494b9403e4b.pdf", "supplementary_material": "/attachment/3b46808745770b0e749b47c9c8be482306759999.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes S-SWIM, a gradient-free method to train SNNs for time series forecasting. Experimental results have shown its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The S-SWIM method is novel for SNN training. Especially, it does not need gradient backpropagation."}, "weaknesses": {"value": "1. This paper is difficult to understand. I recommend the authors to draw an illustration figure of the S-SWIM method to help the readers better understand it.\n2. The paper is written like a technical report rather than a paper. For a paper, the authors should propose the scientific problem this paper aims to address, then introduce how the method addresses this problem. The whole part of Section 3 introduces the technical details without holistic intuition. Also the experimental settings (baselines, training method details, etc.) are vague in Section 4.\n3. The baselines in Table 1 are not cited. I am not familiar with these works, as far as I know, Spikformer and QKFormer are designed for vision tasks, how do the authors use them in time series forecasting tasks? In addition, it seems that works in 'SNNs for Time Series Forecasting' in Related work are not included in this table."}, "questions": {"value": "1. There lacks the explanation for Eq (1)(2)(3). I recommend the authors to introduce $\\phi$ and $\\eta$ in Eq (2)(3) (spike response kernel and reset kernel). Besides, what is PSPK, RfKin line 133? A full name should be given. Where do the spike-cost $c$ appear in the equations?\n2. In line 157, it seems that $\\tau_{max}$ is not continuous at $O=H$. This design is a bit weird, does it have any intuitions?\n3. In Eq. (6)(7), what is $-3 \\cdot s_t$ in the normal distribution? Normal distributions only have two parameters in common.\n4. Authors have said that S-SWIM is much faster than SGD. Where is the experimental data to support this conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YpIRFA3mCP", "forum": "Q4LtkLD3r8", "replyto": "Q4LtkLD3r8", "signatures": ["ICLR.cc/2026/Conference/Submission19633/Reviewer_k7CL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19633/Reviewer_k7CL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761307107613, "cdate": 1761307107613, "tmdate": 1762931484047, "mdate": 1762931484047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Random Feature Spiking (RFS) method to enable efficient supervised learning in Spiking Neural Networks (SNNs) by treating membrane dynamics as random projections. The authors offer theoretical guarantees showing consistency to SGD-based weight updates and demonstrate preliminary experimental evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides sufficient theoretical justification for the proposed method.\n2. A promising solution for enabling global weight updates in SNNs similar to SGD."}, "weaknesses": {"value": "1. Since the paper states conceptual relationships to SWIM, but does not empirically or algorithmically contrast against it, it remains unclear that whether temporal parameters meaningfully impact learning quality, and whether RFS is fundamentally more scalable or simply a variant of existing methods.\n2. Experimental evaluation is insufficient to validate claims\nThe paper contains only one dataset and one architecture configuration, with missing details such as:\nnumber of synapses / learned parameters in neurons,\nSpike-based computation metrics (SynOps / NeuOps).\n3. Writing clarity and notation consistency need improvement\nNumerous grammatical mistakes, unclear expressions, and inconsistent mathematical notation negatively affect readability. Some expressions lack necessary subscripts or arguments, making it harder to follow derivations.\n4. Presentation misses a comprehensive contextual comparison\nThe related-work section briefly lists methods but does not sufficiently articulate technical distinctions from:\nLocal learning rules like e-prop [1]\nSparse BP [2,3]\nCurrent efficient training methods like gradient estimation [4] and integer spikes [5]\n5. Conclusion overstates findings relative to evidence\nThe claims of “better credit assignment” remain theoretical without adequate practical demonstration.\n\n[1] Bellec, G., Scherr, F., Subramoney, A. et al. A solution to the learning dilemma for recurrent networks of spiking neurons. Nat Commun 11, 3625 (2020).\n[2] Meng, Qingyan, et al. \"Towards memory-and time-efficient backpropagation for training spiking neural networks.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023.\n[3] Perez-Nieves, Nicolas, and Dan Goodman. \"Sparse spiking gradient descent.\" Advances in Neural Information Processing Systems 34 (2021): 11795-11808.\n[4] Meng, Qingyan, et al. \"Training high-performance low-latency spiking neural networks by differentiation on spike representation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n[5] Luo, Xinhao, et al. \"Integer-valued training and spike-driven inference spiking neural network for high-performance and energy-efficient object detection.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "see in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z9txzpDBDm", "forum": "Q4LtkLD3r8", "replyto": "Q4LtkLD3r8", "signatures": ["ICLR.cc/2026/Conference/Submission19633/Reviewer_w4Dg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19633/Reviewer_w4Dg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063892610, "cdate": 1762063892610, "tmdate": 1762931483260, "mdate": 1762931483260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a gradient-free training method for spiking neural networks (SNNs) based on the spike response model (SRM), leveraging the random feature method. It introduces a sampling strategy for data points, a weight design scheme for temporal parameters, and an approach for handling output delays."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "**S1.** This paper addresses an important topic — exploring how to train spiking neural networks (SNNs) without relying on gradient-based methods.\n\n**S2.** The incorporation of the random feature method into SNN training is novel, and the experimental results demonstrate its effectiveness to a certain extent."}, "weaknesses": {"value": "**W1.** The paper introduces a large number of notations, which makes it somewhat difficult to follow. In particular, Definition 3.1 employs multiple types of brackets—{}, () and []—without clear distinction (e.g.,$\\phi{x}, f(t), and \\zeta[v]$), which can be confusing. Moreover, the use of the symbol “!=” should be clarified. Since I am an emergency reviewer with limited time to thoroughly read the paper, this issue significantly affects the readability and comprehension of the technical content."}, "questions": {"value": "**Q1.** According to Figure 1, why do S-SWIM-trained networks fine-tuned with SGD (i.e., SGD-1) require less training time than the S-SWIM-trained networks themselves in some cases?\n\n**Q2.** What is the architecture of the network trained using S-SWIM? Since S-SWIM models each neuron individually, the training time may increase linearly with the number of neurons. Could the authors clarify this relationship?\n\n**Q3.** How sensitive is S-SWIM to the number of random features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LyEtdE7q97", "forum": "Q4LtkLD3r8", "replyto": "Q4LtkLD3r8", "signatures": ["ICLR.cc/2026/Conference/Submission19633/Reviewer_SPAj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19633/Reviewer_SPAj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19633/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762951763553, "cdate": 1762951763553, "tmdate": 1762951763553, "mdate": 1762951763553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes S-SWIM, a gradient-free training algorithm for Spike Response Model (SRM) spiking neural networks based on Random Feature Methods (RFMs) and inspired by the SWIM algorithm previously developed for ANNs. The key idea is to construct hidden-layer weights and temporal parameters (delays, kernel widths) in a data-driven way so that pairs of inputs with similar signals but dissimilar targets are separated in function space, and then to solve a linear problem for the output layer. This avoids backpropagating gradients through the non-differentiable spike function and thus avoids surrogate gradients entirely.\n\nExperiments focus on time-series forecasting on four standard datasets (METR-LA, PEMS-BAY, Solar, Electricity) with multiple prediction horizons. The authors compare: (a) S-SWIM alone, (b) surrogate-gradient SGD (Adam), and (c) S-SWIM used as initialization followed by SGD, and report RSE metrics and training times. S-SWIM often matches or outperforms pure SGD on these forecasting tasks and is reported to be one to three orders of magnitude faster in training time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper cleanly adapts data-driven random feature sampling from SWIM to a Spike Response Model SNN setting, including temporal parameters (delays, kernel supports) as part of the random feature construction. This is more principled than purely random ELM-style initialisation and fits well with the SNN temporal structure. \n\nS-SWIM provides a surrogate-gradient-free alternative, addressing a real pain point in SNN training (biased surrogates, gradient instability, heavy compute/memory cost). All learning is done via sampling, linear algebra, and correlation analysis over spike trains and kernel responses."}, "weaknesses": {"value": "All experiments are on tabular / numerical time-series datasets (METR-LA, PEMS-BAY, Solar, Electricity) with RSE metrics. \n\nThere are no experiments on canonical SNN benchmarks such as neuromorphic image datasets (e.g., DVS variants), frame-based image classification (MNIST/CIFAR-like), audio/speech spiking tasks, or text/event-driven tasks.\n\nGiven the broad claims (“general fast training algorithm for SNNs”, “potentially more energy-efficient alternative to ANNs”) and the heavy emphasis on the generality of the SRM parameterization, the fact that only one task family (time-series forecasting) is actually tested makes the empirical validation feel incomplete. A reader cannot tell whether S-SWIM meaningfully helps in other common SNN application domains such as vision or speech.\n\nThe paper focuses on a relatively specific shallow feed-forward architecture (the “shallow network case”) and does not empirically validate S-SWIM on deeper networks, even though the method is claimed to be applicable to them.\n\nThere is no evaluation on classification tasks or tasks with spike-valued outputs, although the theory discusses them. This accentuates the feeling that the method is only demonstrated in the easiest setting for S-SWIM: continuous regression with L2 loss."}, "questions": {"value": "Overall, this is a technically solid and well-written paper with a genuinely interesting idea: bringing modern data-driven random-feature sampling into the SNN world to obtain a fully gradient-free training scheme with good training-time savings on time-series forecasting benchmarks. The mathematical treatment and modular algorithm design are clear strengths.\n\nHowever, from a conference-acceptance point of view, the experimental validation feels too narrow. The method is only demonstrated on numerical time-series forecasting, with no experiments on images, videos, text, or speech, and no demonstration on classification or neuromorphic benchmarks. Given the broad claims and ambitious framing, this limited evaluation undermines the impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "768V7mHbK5", "forum": "Q4LtkLD3r8", "replyto": "Q4LtkLD3r8", "signatures": ["ICLR.cc/2026/Conference/Submission19633/Reviewer_WHnL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19633/Reviewer_WHnL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19633/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763001239140, "cdate": 1763001239140, "tmdate": 1763001239140, "mdate": 1763001239140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}